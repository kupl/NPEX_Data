[
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1397. NullPointerException observed during task failures. Contributed by Amareshwari Sriramadasu.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@938385 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/b2b37a17e5fc5b138c1621dae28dc62740835164",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/4772f72d369f224dc1e96844609d29282189e57a",
        "bug_id": "hadoop-mapreduce_1",
        "file": [
            {
                "sha": "de052bb671acd5c9efbe9ac07b092c4861e99181",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b2b37a17e5fc5b138c1621dae28dc62740835164/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b2b37a17e5fc5b138c1621dae28dc62740835164/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=b2b37a17e5fc5b138c1621dae28dc62740835164",
                "patch": "@@ -596,6 +596,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1612. job conf file is not accessible from job history web page.\n     (Ravi Gummadi and Sreekanth Ramakrishnan via vinodkv)\n \n+    MAPREDUCE-1397. NullPointerException observed during task failures.\n+    (Amareshwari Sriramadasu via vinodkv)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "957cbf0614b0cdcc7bf4d88b5666c30bc22d9f1b",
                "filename": "src/java/org/apache/hadoop/mapred/JvmManager.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/JvmManager.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/JvmManager.java",
                "status": "modified",
                "changes": 34,
                "additions": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JvmManager.java?ref=b2b37a17e5fc5b138c1621dae28dc62740835164",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.mapred.TaskController.TaskControllerContext;\n import org.apache.hadoop.mapred.TaskTracker.TaskInProgress;\n+import org.apache.hadoop.mapreduce.TaskType;\n import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;\n import org.apache.hadoop.util.Shell.ShellCommandExecutor;\n import org.apache.hadoop.util.StringUtils;\n@@ -42,9 +43,9 @@\n   public static final Log LOG =\n     LogFactory.getLog(JvmManager.class);\n \n-  JvmManagerForType mapJvmManager;\n+  private JvmManagerForType mapJvmManager;\n \n-  JvmManagerForType reduceJvmManager;\n+  private JvmManagerForType reduceJvmManager;\n   \n   public JvmEnv constructJvmEnv(List<String> setup, Vector<String>vargs,\n       File stdout,File stderr,long logSize, File workDir, \n@@ -58,6 +59,15 @@ public JvmManager(TaskTracker tracker) {\n     reduceJvmManager = new JvmManagerForType(tracker.getMaxCurrentReduceTasks(),\n         false, tracker);\n   }\n+\n+  JvmManagerForType getJvmManagerForType(TaskType type) {\n+    if (type.equals(TaskType.MAP)) {\n+      return mapJvmManager;\n+    } else if (type.equals(TaskType.REDUCE)) {\n+      return reduceJvmManager;\n+    }\n+    return null;\n+  }\n   \n   public void stop() {\n     mapJvmManager.stop();\n@@ -168,7 +178,7 @@ static void deleteWorkDir(TaskTracker tracker, Task task) throws IOException {\n           tracker.getTaskController()));\n   }\n \n-  private static class JvmManagerForType {\n+  static class JvmManagerForType {\n     //Mapping from the JVM IDs to running Tasks\n     Map <JVMId,TaskRunner> jvmToRunningTask = \n       new HashMap<JVMId, TaskRunner>();\n@@ -263,10 +273,15 @@ synchronized public void taskKilled(TaskRunner tr) {\n     synchronized public void killJvm(JVMId jvmId) {\n       JvmRunner jvmRunner;\n       if ((jvmRunner = jvmIdToRunner.get(jvmId)) != null) {\n-        jvmRunner.kill();\n+        killJvmRunner(jvmRunner);\n       }\n     }\n     \n+    private synchronized void killJvmRunner(JvmRunner jvmRunner) {\n+      jvmRunner.kill();\n+      removeJvm(jvmRunner.jvmId);\n+    }\n+\n     synchronized void dumpStack(TaskRunner tr) {\n       JVMId jvmId = runningTaskToJvm.get(tr);\n       if (null != jvmId) {\n@@ -286,7 +301,7 @@ synchronized public void stop() {\n       List <JvmRunner> list = new ArrayList<JvmRunner>();\n       list.addAll(jvmIdToRunner.values());\n       for (JvmRunner jvm : list) {\n-        jvm.kill();\n+        killJvmRunner(jvm);\n       }\n     }\n     \n@@ -350,7 +365,7 @@ private synchronized void reapJvm(\n       if (spawnNewJvm) {\n         if (runnerToKill != null) {\n           LOG.info(\"Killing JVM: \" + runnerToKill.jvmId);\n-          runnerToKill.kill();\n+          killJvmRunner(runnerToKill);\n         }\n         spawnNewJvm(jobId, env, t);\n         return;\n@@ -412,7 +427,7 @@ synchronized private void updateOnJvmExit(JVMId jvmId,\n       }\n     }\n \n-    private class JvmRunner extends Thread {\n+    class JvmRunner extends Thread {\n       JvmEnv env;\n       volatile boolean killed = false;\n       volatile int numTasksRan;\n@@ -472,9 +487,8 @@ public void runChild(JvmEnv env) {\n        * Kills the process. Also kills its subprocesses if the process(root of subtree\n        * of processes) is created using setsid.\n        */\n-      public void kill() {\n+      synchronized void kill() {\n         if (!killed) {\n-          killed = true;\n           TaskController controller = tracker.getTaskController();\n           // Check inital context before issuing a kill to prevent situations\n           // where kill is issued before task is launched.\n@@ -490,7 +504,7 @@ public void kill() {\n             LOG.info(String.format(\"JVM Not killed %s but just removed\", jvmId\n                 .toString()));\n           }\n-          removeJvm(jvmId);\n+          killed = true;\n         }\n       }\n ",
                "deletions": 10
            },
            {
                "sha": "219a953591dd2ed471be442b780150e4edf75550",
                "filename": "src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "status": "modified",
                "changes": 21,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/TaskRunner.java?ref=b2b37a17e5fc5b138c1621dae28dc62740835164",
                "patch": "@@ -226,14 +226,7 @@ public Void run() throws IOException {\n       errorInfo = getVMEnvironment(errorInfo, workDir, conf, env,\n                                    taskid, logSize);\n \n-      jvmManager.launchJvm(this, \n-          jvmManager.constructJvmEnv(setup,vargs,stdout,stderr,logSize, \n-              workDir, env, conf));\n-      synchronized (lock) {\n-        while (!done) {\n-          lock.wait();\n-        }\n-      }\n+      launchJvmAndWait(setup, vargs, stdout, stderr, logSize, workDir, env);\n       tracker.getTaskTrackerInstrumentation().reportTaskEnd(t.getTaskID());\n       if (exitCodeSet) {\n         if (!killed && exitCode != 0) {\n@@ -277,6 +270,18 @@ public Void run() throws IOException {\n     }\n   }\n \n+  void launchJvmAndWait(List<String> setup, Vector<String> vargs, File stdout,\n+      File stderr, long logSize, File workDir, Map<String, String> env)\n+      throws InterruptedException {\n+    jvmManager.launchJvm(this, jvmManager.constructJvmEnv(setup, vargs, stdout,\n+        stderr, logSize, workDir, env, conf));\n+    synchronized (lock) {\n+      while (!done) {\n+        lock.wait();\n+      }\n+    }\n+  }\n+\n   /**\n    * Prepare the log files for the task\n    * ",
                "deletions": 8
            },
            {
                "sha": "bb0ce3ded263b76abe364250d15f031c537c5056",
                "filename": "src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b2b37a17e5fc5b138c1621dae28dc62740835164/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "status": "modified",
                "changes": 19,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/TaskTracker.java?ref=b2b37a17e5fc5b138c1621dae28dc62740835164",
                "patch": "@@ -258,7 +258,7 @@\n   private MapEventsFetcherThread mapEventsFetcher;\n   int workerThreads;\n   CleanupQueue directoryCleanupThread;\n-  volatile JvmManager jvmManager;\n+  private volatile JvmManager jvmManager;\n   UserLogCleaner taskLogCleanupThread;\n   private TaskMemoryManagerThread taskMemoryManager;\n   private boolean taskMemoryManagerEnabled = true;\n@@ -2064,7 +2064,12 @@ private long getFreeSpace() throws IOException {\n   public JvmManager getJvmManagerInstance() {\n     return jvmManager;\n   }\n-  \n+\n+  // called from unit test  \n+  void setJvmManagerInstance(JvmManager jvmManager) {\n+    this.jvmManager = jvmManager;\n+  }\n+\n   private void addToTaskQueue(LaunchTaskAction action) {\n     if (action.getTask().isMapTask()) {\n       mapLauncher.addToTaskQueue(action);\n@@ -3721,6 +3726,16 @@ int getMaxCurrentReduceTasks() {\n     return maxReduceSlots;\n   }\n \n+  //called from unit test\n+  synchronized void setMaxMapSlots(int mapSlots) {\n+    maxMapSlots = mapSlots;\n+  }\n+\n+  //called from unit test\n+  synchronized void setMaxReduceSlots(int reduceSlots) {\n+    maxReduceSlots = reduceSlots;\n+  }\n+\n   /**\n    * Is the TaskMemoryManager Enabled on this system?\n    * @return true if enabled, false otherwise.",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-962. Fix a NullPointerException while killing task process trees. Contributed by Ravi Gummadi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@833002 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/1615886bbd9ece187b7754d1a138fb7074a955c8",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/80540f15210c86c549e0ffe9854ce152806d499f",
        "bug_id": "hadoop-mapreduce_2",
        "file": [
            {
                "sha": "1c473e8de64a12b4d3cced2670d4bb939610b1be",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "patch": "@@ -854,3 +854,7 @@ Release 0.21.0 - Unreleased\n \n     MAPREDUCE-1163. Remove unused, hard-coded paths from libhdfs. (Allen\n     Wittenauer via cdouglas)\n+\n+    MAPREDUCE-962. Fix a NullPointerException while killing task process \n+    trees. (Ravi Gummadi via yhemanth)\n+    ",
                "deletions": 0
            },
            {
                "sha": "c3ced7964bd06e643a49a25279f4e217d1c3965d",
                "filename": "src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "status": "modified",
                "changes": 29,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "patch": "@@ -228,12 +228,19 @@ public boolean isAnyProcessInTreeAlive() {\n \n   /** Verify that the given process id is same as its process group id.\n    * @param pidStr Process id of the to-be-verified-process\n+   * @param procfsDir  Procfs root dir\n    */\n-  private static boolean assertPidPgrpidForMatch(String pidStr) {\n+  static boolean checkPidPgrpidForMatch(String pidStr, String procfsDir) {\n     Integer pId = Integer.parseInt(pidStr);\n     // Get information for this process\n     ProcessInfo pInfo = new ProcessInfo(pId);\n-    pInfo = constructProcessInfo(pInfo);\n+    pInfo = constructProcessInfo(pInfo, procfsDir);\n+    if (pInfo == null) {\n+      // process group leader may have finished execution, but we still need to\n+      // kill the subProcesses in the process group.\n+      return true;\n+    }\n+\n     //make sure that pId and its pgrpId match\n     if (!pInfo.getPgrpId().equals(pId)) {\n       LOG.warn(\"Unexpected: Process with PID \" + pId +\n@@ -258,7 +265,7 @@ public static void assertAndDestroyProcessGroup(String pgrpId, long interval,\n                        boolean inBackground)\n          throws IOException {\n     // Make sure that the pid given is a process group leader\n-    if (!assertPidPgrpidForMatch(pgrpId)) {\n+    if (!checkPidPgrpidForMatch(pgrpId, PROCFS)) {\n       throw new IOException(\"Process with PID \" + pgrpId  +\n                           \" is not a process group leader.\");\n     }\n@@ -390,15 +397,6 @@ private static Integer getValidPID(String pid) {\n     return processList;\n   }\n \n-  /**\n-   * \n-   * Construct the ProcessInfo using the process' PID and procfs and return the\n-   * same. Returns null on failing to read from procfs,\n-   */\n-  private static ProcessInfo constructProcessInfo(ProcessInfo pinfo) {\n-    return constructProcessInfo(pinfo, PROCFS);\n-  }\n-\n   /**\n    * Construct the ProcessInfo using the process' PID and procfs rooted at the\n    * specified directory and return the same. It is provided mainly to assist\n@@ -422,6 +420,8 @@ private static ProcessInfo constructProcessInfo(ProcessInfo pinfo,\n       in = new BufferedReader(fReader);\n     } catch (FileNotFoundException f) {\n       // The process vanished in the interim!\n+      LOG.warn(\"The process \" + pinfo.getPid()\n+          + \" may have finished in the interim.\");\n       return ret;\n     }\n \n@@ -436,6 +436,11 @@ private static ProcessInfo constructProcessInfo(ProcessInfo pinfo,\n             .parseInt(m.group(4)), Integer.parseInt(m.group(5)), Long\n             .parseLong(m.group(7)));\n       }\n+      else {\n+        LOG.warn(\"Unexpected: procfs stat file is not in the expected format\"\n+            + \" for process with pid \" + pinfo.getPid());\n+        ret = null;\n+      }\n     } catch (IOException io) {\n       LOG.warn(\"Error reading the stream \" + io);\n       ret = null;",
                "deletions": 12
            },
            {
                "sha": "cc785062e3c6ffd1baa3537f64d4fda3ec0580f1",
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "status": "modified",
                "changes": 28,
                "additions": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "patch": "@@ -422,6 +422,34 @@ public void testVMemForOlderProcesses() throws IOException {\n     }\n   }\n \n+  /**\n+   * Verifies ProcfsBasedProcessTree.checkPidPgrpidForMatch() in case of\n+   * 'constructProcessInfo() returning null' by not writing stat file for the\n+   * mock process\n+   * @throws IOException if there was a problem setting up the\n+   *                      fake procfs directories or files.\n+   */\n+  public void testDestroyProcessTree() throws IOException {\n+    // test process\n+    String pid = \"100\";\n+    // create the fake procfs root directory. \n+    File procfsRootDir = new File(TEST_ROOT_DIR, \"proc\");\n+\n+    try {\n+      setupProcfsRootDir(procfsRootDir);\n+      \n+      // crank up the process tree class.\n+      ProcfsBasedProcessTree processTree = new ProcfsBasedProcessTree(\n+                        pid, true, 100L, procfsRootDir.getAbsolutePath());\n+\n+      // Let us not create stat file for pid 100.\n+      assertTrue(ProcfsBasedProcessTree.checkPidPgrpidForMatch(\n+                            pid, procfsRootDir.getAbsolutePath()));\n+    } finally {\n+      FileUtil.fullyDelete(procfsRootDir);\n+    }\n+  }\n+  \n   /**\n    * Test the correctness of process-tree dump.\n    * ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "HADOOP-6243. Fixes a NullPointerException in handling deprecated keys. Contributed by Sreekanth Ramakrishnan.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@812473 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/b6811c7890779b03642b69bb1071357b981f2297",
        "bug_id": "hadoop-mapreduce_3",
        "file": [
            {
                "sha": "797181ea1efbe2343f27b6beb383e35fde9adb1b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=b6bbd1093e682d3d4cdc4f608faa0a43cad71df4",
                "patch": "@@ -539,3 +539,7 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-764. TypedBytesInput's readRaw() does not preserve custom type\n     codes. (Klaas Bosteels via tomwhite)\n+\n+    HADOOP-6243. Fixes a NullPointerException in handling deprecated keys.\n+    (Sreekanth Ramakrishnan via yhemanth)\n+",
                "deletions": 0
            },
            {
                "sha": "3293cf619ddd9042952f355439dea1070f8cbeac",
                "filename": "lib/hadoop-core-0.21.0-dev.jar",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/lib/hadoop-core-0.21.0-dev.jar",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/lib/hadoop-core-0.21.0-dev.jar",
                "status": "modified",
                "changes": 0,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/lib/hadoop-core-0.21.0-dev.jar?ref=b6bbd1093e682d3d4cdc4f608faa0a43cad71df4",
                "deletions": 0
            },
            {
                "sha": "b70ed7e93f16a702896ea33613e4bf349ea68fee",
                "filename": "lib/hadoop-core-test-0.21.0-dev.jar",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/lib/hadoop-core-test-0.21.0-dev.jar",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/b6bbd1093e682d3d4cdc4f608faa0a43cad71df4/lib/hadoop-core-test-0.21.0-dev.jar",
                "status": "modified",
                "changes": 0,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/lib/hadoop-core-test-0.21.0-dev.jar?ref=b6bbd1093e682d3d4cdc4f608faa0a43cad71df4",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1813. NPE in PipeMapred.MRErrorThread. Contributed by Ravi Gummadi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@953670 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/c2e41ceb1541ce550119256a82661c704f54e4df",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/2fb120dd1326b2b28c346f36d6097ee1dbe22d68",
        "bug_id": "hadoop-mapreduce_4",
        "file": [
            {
                "sha": "7895d3f98e1e5fae2bfe0a51ea4b3338b634bb3c",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -84,6 +84,8 @@ Trunk (unreleased changes)\n     MAPREDUCE-1505. Create RPC client on job submission, not in cstr of Job\n     instance. (Dick King via cdouglas)\n \n+    MAPREDUCE-1813. NPE in PipeMapred.MRErrorThread. (Ravi Gummadi via vinodkv)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "f66001b2b898a5f713fc57ba10494e7df79e513c",
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -222,8 +222,6 @@ public void configure(JobConf job) {\n       clientErr_ = new DataInputStream(new BufferedInputStream(sim.getErrorStream()));\n       startTime_ = System.currentTimeMillis();\n \n-      errThread_ = new MRErrorThread();\n-      errThread_.start();\n     } catch (IOException e) {\n       logStackTrace(e);\n       LOG.error(\"configuration exception\", e);\n@@ -338,7 +336,9 @@ void startOutputThreads(OutputCollector output, Reporter reporter)\n     outReader_ = createOutputReader();\n     outThread_ = new MROutputThread(outReader_, output, reporter);\n     outThread_.start();\n+    errThread_ = new MRErrorThread();\n     errThread_.setReporter(reporter);\n+    errThread_.start();\n   }\n   \n   void waitOutputThreads() throws IOException {",
                "deletions": 2
            },
            {
                "sha": "da2790b861e50c93c7e06927dd68cb026b71a20c",
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "status": "modified",
                "changes": 2,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -25,8 +25,6 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.util.ReflectionUtils;\n-\n public class PipeMapRunner<K1, V1, K2, V2> extends MapRunner<K1, V1, K2, V2> {\n   public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,\n                   Reporter reporter)",
                "deletions": 2
            },
            {
                "sha": "0678a8759d0fe0bd6710f0d7fcea23ce6d7d0644",
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2fb120dd1326b2b28c346f36d6097ee1dbe22d68/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2fb120dd1326b2b28c346f36d6097ee1dbe22d68/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "status": "removed",
                "changes": 122,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java?ref=2fb120dd1326b2b28c346f36d6097ee1dbe22d68",
                "patch": "@@ -1,122 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.streaming;\n-\n-import org.junit.Test;\n-\n-import java.io.*;\n-\n-import org.apache.hadoop.fs.FileUtil;\n-\n-/**\n- * This class tests hadoopStreaming in MapReduce local mode by giving\n- * empty input to mapper and the mapper generates nonempty output. Since map()\n- * is not called at all, output thread was not getting created and the mapper\n- * was hanging forever. Now this issue is solved. Similarly reducer is also\n- * checked for task completion with empty input and nonempty output.\n- */\n-public class TestStreamingEmptyInpNonemptyOut\n-{\n-\n-  protected File INPUT_FILE = new File(\"emptyInputFile.txt\");\n-  protected File OUTPUT_DIR = new File(\"out\");\n-  protected File SCRIPT_FILE = new File(\"perlScript.pl\");\n-\n-  protected String map = \"perlScript.pl\";\n-  protected String reduce = \"org.apache.hadoop.mapred.lib.IdentityReducer\";\n-  protected String script = \"#!/usr/bin/perl\\nfor($count = 1500; $count >= 1; $count--) {print \\\"$count \\\";}\";\n-\n-  private StreamJob job;\n-\n-  public TestStreamingEmptyInpNonemptyOut() throws IOException\n-  {\n-    UtilTest utilTest = new UtilTest(getClass().getName());\n-    utilTest.checkUserDir();\n-    utilTest.redirectIfAntJunit();\n-  }\n-\n-  protected void createInputAndScript() throws IOException\n-  {\n-    DataOutputStream out = new DataOutputStream(\n-                           new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\n-    out.close();\n-\n-    out = new DataOutputStream(\n-          new FileOutputStream(SCRIPT_FILE.getAbsoluteFile()));\n-    out.write(script.getBytes(\"UTF-8\"));\n-    out.close();\n-  }\n-\n-  protected String[] genArgs() {\n-    return new String[] {\n-      \"-input\", INPUT_FILE.getAbsolutePath(),\n-      \"-output\", OUTPUT_DIR.getAbsolutePath(),\n-      \"-mapper\", map,\n-      \"-reducer\", reduce,\n-      //\"-verbose\",\n-      //\"-jobconf\", \"stream.debug=set\"\n-      \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\",\n-      \"-jobconf\", \"stream.tmpdir=\"+System.getProperty(\"test.build.data\",\"/tmp\")\n-    };\n-  }\n-\n-  @Test\n-  public void testEmptyInputNonemptyOutput() throws IOException\n-  {\n-    try {\n-      try {\n-        OUTPUT_DIR.getAbsoluteFile().delete();\n-      } catch (Exception e) {\n-      }\n-\n-      createInputAndScript();\n-      boolean mayExit = false;\n-\n-      // During tests, the default Configuration will use a local mapred\n-      // So don't specify -config or -cluster.\n-      // First let us test if mapper doesn't hang for empty i/p and nonempty o/p\n-      job = new StreamJob(genArgs(), mayExit);      \n-      job.go();\n-      File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\n-      outFile.delete();\n-\n-      // Now let us test if reducer doesn't hang for empty i/p and nonempty o/p\n-      map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\n-      reduce = \"perlScript.pl\";\n-      job = new StreamJob(genArgs(), mayExit);      \n-      job.go();\n-      outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\n-      outFile.delete();\n-    } finally {\n-      try {\n-        INPUT_FILE.delete();\n-        SCRIPT_FILE.delete();\n-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\n-      } catch (IOException e) {\n-        e.printStackTrace();\n-      }\n-    }\n-  }\n-\n-  public static void main(String[]args) throws Exception\n-  {\n-    new TestStreaming().testCommandLine();\n-  }\n-\n-}",
                "deletions": 122
            },
            {
                "sha": "0b353b5ae8bdd2a85c1d277adf217f527fbdd9e2",
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "status": "modified",
                "changes": 291,
                "additions": 253,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -22,88 +22,303 @@\n import java.io.IOException;\n import java.io.File;\n \n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n import static org.junit.Assert.*;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.Counters;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapred.TaskID;\n+import org.apache.hadoop.mapred.TaskLog;\n import org.apache.hadoop.mapred.TaskReport;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n \n /**\n- * Tests for the ability of a streaming task to set the status\n- * by writing \"reporter:status:\" lines to stderr. Uses MiniMR\n- * since the local jobtracker doesn't track status.\n+ * Tests if mapper/reducer with empty/nonempty input works properly if\n+ * reporting is done using lines like \"reporter:status:\" and\n+ * \"reporter:counter:\" before map()/reduce() method is called.\n+ * Validates the task's log of STDERR if messages are written to stderr before\n+ * map()/reduce() is called.\n+ * Also validates job output.\n+ * Uses MiniMR since the local jobtracker doesn't track task status. \n  */\n public class TestStreamingStatus {\n-  private static String TEST_ROOT_DIR =\n-    new File(System.getProperty(\"test.build.data\",\"/tmp\"))\n+  protected static String TEST_ROOT_DIR =\n+    new File(System.getProperty(\"test.build.data\",\"/tmp\"),\n+    TestStreamingStatus.class.getSimpleName())\n     .toURI().toString().replace(' ', '+');\n   protected String INPUT_FILE = TEST_ROOT_DIR + \"/input.txt\";\n   protected String OUTPUT_DIR = TEST_ROOT_DIR + \"/out\";\n   protected String input = \"roses.are.red\\nviolets.are.blue\\nbunnies.are.pink\\n\";\n-  protected String map = StreamUtil.makeJavaCommand(StderrApp.class, new String[]{\"3\", \"0\", \"0\", \"true\"});\n+  protected String map = null;\n+  protected String reduce = null;\n \n-  protected String[] genArgs(int jobtrackerPort) {\n+  protected String scriptFile = TEST_ROOT_DIR + \"/perlScript.pl\";\n+  protected String scriptFileName = new Path(scriptFile).toUri().getPath();\n+\n+\n+  String expectedStderr = \"my error msg before consuming input\\n\" +\n+      \"my error msg after consuming input\\n\";\n+  String expectedOutput = null;// inited in setUp()\n+  String expectedStatus = \"before consuming input\";\n+\n+  // This script does the following\n+  // (a) setting task status before reading input\n+  // (b) writing to stderr before reading input and after reading input\n+  // (c) writing to stdout before reading input\n+  // (d) incrementing user counter before reading input and after reading input\n+  // Write lines to stdout before reading input{(c) above} is to validate\n+  // the hanging task issue when input to task is empty(because of not starting\n+  // output thread).\n+  protected String script =\n+    \"#!/usr/bin/perl\\n\" +\n+    \"print STDERR \\\"reporter:status:\" + expectedStatus + \"\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"my error msg before consuming input\\\\n\\\";\\n\" +\n+    \"for($count = 1500; $count >= 1; $count--) {print STDOUT \\\"$count \\\";}\" +\n+    \"while(<STDIN>) {chomp;}\\n\" +\n+    \"print STDERR \\\"my error msg after consuming input\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\\\n\\\";\\n\";\n+\n+  MiniMRCluster mr = null;\n+  FileSystem fs = null;\n+  JobConf conf = null;\n+\n+  /**\n+   * Start the cluster and create input file before running the actual test.\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void setUp() throws IOException {\n+    conf = new JobConf();\n+    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);\n+\n+    mr = new MiniMRCluster(1, \"file:///\", 3, null , null, conf);\n+\n+    Path inFile = new Path(INPUT_FILE);\n+    fs = inFile.getFileSystem(mr.createJobConf());\n+    clean(fs);\n+\n+    buildExpectedJobOutput();\n+  }\n+\n+  /**\n+   * Kill the cluster after the test is done.\n+   */\n+  @After\n+  public void tearDown() {\n+    if (fs != null) { clean(fs); }\n+    if (mr != null) { mr.shutdown(); }\n+  }\n+\n+  // Updates expectedOutput to have the expected job output as a string\n+  void buildExpectedJobOutput() {\n+    if (expectedOutput == null) {\n+      expectedOutput = \"\";\n+      for(int i = 1500; i >= 1; i--) {\n+        expectedOutput = expectedOutput.concat(Integer.toString(i) + \" \");\n+      }\n+      expectedOutput = expectedOutput.trim();\n+    }\n+  }\n+\n+  // Create empty/nonempty input file.\n+  // Create script file with the specified content.\n+  protected void createInputAndScript(boolean isEmptyInput,\n+      String script) throws IOException {\n+    makeInput(fs, isEmptyInput ? \"\" : input);\n+\n+    // create script file\n+    DataOutputStream file = fs.create(new Path(scriptFileName));\n+    file.writeBytes(script);\n+    file.close();\n+  }\n+\n+  protected String[] genArgs(int jobtrackerPort, String mapper, String reducer)\n+  {\n     return new String[] {\n       \"-input\", INPUT_FILE,\n       \"-output\", OUTPUT_DIR,\n-      \"-mapper\", map,\n+      \"-mapper\", mapper,\n+      \"-reducer\", reducer,\n       \"-jobconf\", MRJobConfig.NUM_MAPS + \"=1\",\n-      \"-jobconf\", MRJobConfig.NUM_REDUCES + \"=0\",      \n+      \"-jobconf\", MRJobConfig.NUM_REDUCES + \"=1\",\n       \"-jobconf\", MRJobConfig.PRESERVE_FAILED_TASK_FILES + \"=true\",\n-      \"-jobconf\", \"stream.tmpdir=\"+System.getProperty(\"test.build.data\",\"/tmp\"),\n+      \"-jobconf\", \"stream.tmpdir=\" + new Path(TEST_ROOT_DIR).toUri().getPath(),\n       \"-jobconf\", JTConfig.JT_IPC_ADDRESS + \"=localhost:\"+jobtrackerPort,\n       \"-jobconf\", \"fs.default.name=file:///\"\n     };\n   }\n-  \n-  public void makeInput(FileSystem fs) throws IOException {\n+\n+  // create input file with the given content\n+  public void makeInput(FileSystem fs, String input) throws IOException {\n     Path inFile = new Path(INPUT_FILE);\n     DataOutputStream file = fs.create(inFile);\n     file.writeBytes(input);\n     file.close();\n   }\n \n-  public void clean(FileSystem fs) {\n+  // Delete output directory\n+  protected void deleteOutDir(FileSystem fs) {\n     try {\n       Path outDir = new Path(OUTPUT_DIR);\n       fs.delete(outDir, true);\n     } catch (Exception e) {}\n+  }\n+\n+  // Delete input file, script file and output directory\n+  public void clean(FileSystem fs) {\n+    deleteOutDir(fs);\n     try {\n-      Path inFile = new Path(INPUT_FILE);    \n-      fs.delete(inFile, false);\n-    } catch (Exception e) {}\n+      Path file = new Path(INPUT_FILE);\n+      if (fs.exists(file)) {\n+        fs.delete(file, false);\n+      }\n+      file = new Path(scriptFile);\n+      if (fs.exists(file)) {\n+        fs.delete(file, false);\n+      }\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+    }\n   }\n-  \n+\n+  /**\n+   * Check if mapper/reducer with empty/nonempty input works properly if\n+   * reporting is done using lines like \"reporter:status:\" and\n+   * \"reporter:counter:\" before map()/reduce() method is called.\n+   * Validate the task's log of STDERR if messages are written\n+   * to stderr before map()/reduce() is called.\n+   * Also validate job output.\n+   *\n+   * @throws IOException\n+   */\n   @Test\n-  public void testStreamingStatus() throws Exception {\n-    MiniMRCluster mr = null;\n-    FileSystem fs = null;\n-    JobConf conf = new JobConf();\n-    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);\n-    try {\n-      mr = new MiniMRCluster(1, \"file:///\", 3, null , null, conf);\n+  public void testReporting() throws Exception {\n+    testStreamJob(false);// nonempty input\n+    testStreamJob(true);// empty input\n+  }\n+\n+  /**\n+   * Run a streaming job with the given script as mapper and validate.\n+   * Run another streaming job with the given script as reducer and validate.\n+   *\n+   * @param isEmptyInput Should the input to the script be empty ?\n+   * @param script The content of the script that will run as the streaming task\n+   */\n+  private void testStreamJob(boolean isEmptyInput)\n+      throws IOException {\n+\n+      createInputAndScript(isEmptyInput, script);\n \n-      Path inFile = new Path(INPUT_FILE);\n-      fs = inFile.getFileSystem(mr.createJobConf());\n+      // Check if streaming mapper works as expected\n+      map = scriptFileName;\n+      reduce = \"/bin/cat\";\n+      runStreamJob(TaskType.MAP, isEmptyInput);\n+      deleteOutDir(fs);\n+\n+      // Check if streaming reducer works as expected.\n+      map = \"/bin/cat\";\n+      reduce = scriptFileName;\n+      runStreamJob(TaskType.REDUCE, isEmptyInput);\n       clean(fs);\n-      makeInput(fs);\n-      \n-      StreamJob job = new StreamJob();\n-      int failed = job.run(genArgs(mr.getJobTrackerPort()));\n-      assertEquals(0, failed);\n-\n-      TaskReport[] reports = job.jc_.getMapTaskReports(job.jobId_);\n-      assertEquals(1, reports.length);\n-      assertEquals(\"starting echo > sort\", reports[0].getState());\n-    } finally {\n-      if (fs != null) { clean(fs); }\n-      if (mr != null) { mr.shutdown(); }\n+  }\n+\n+  // Run streaming job for the specified input file, mapper and reducer and\n+  // (1) Validate if the job succeeds.\n+  // (2) Validate if user counter is incremented properly for the cases of\n+  //   (a) nonempty input to map\n+  //   (b) empty input to map and\n+  //   (c) nonempty input to reduce\n+  // (3) Validate task status for the cases of (2)(a),(2)(b),(2)(c).\n+  //     Because empty input to reduce task => reporter is dummy and ignores\n+  //     all \"reporter:status\" and \"reporter:counter\" lines. \n+  // (4) Validate stderr of task of given task type.\n+  // (5) Validate job output\n+  void runStreamJob(TaskType type, boolean isEmptyInput) throws IOException {\n+    boolean mayExit = false;\n+    StreamJob job = new StreamJob(genArgs(\n+        mr.getJobTrackerPort(), map, reduce), mayExit);\n+    int returnValue = job.go();\n+    assertEquals(0, returnValue);\n+\n+    // If input to reducer is empty, dummy reporter(which ignores all\n+    // reporting lines) is set for MRErrorThread in waitOutputThreads(). So\n+    // expectedCounterValue is 0 for empty-input-to-reducer case.\n+    // Output of reducer is also empty for empty-input-to-reducer case.\n+    int expectedCounterValue = 0;\n+    if (type == TaskType.MAP || !isEmptyInput) {\n+      validateTaskStatus(job, type);\n+      // output is from \"print STDOUT\" statements in perl script\n+      validateJobOutput(job.getConf());\n+      expectedCounterValue = 2;\n+    }\n+    validateUserCounter(job, expectedCounterValue);\n+    validateTaskStderr(job, type);\n+\n+    deleteOutDir(fs);\n+  }\n+\n+  // validate task status of task of given type(validates 1st task of that type)\n+  void validateTaskStatus(StreamJob job, TaskType type) throws IOException {\n+    // Map Task has 2 phases: map, sort\n+    // Reduce Task has 3 phases: copy, sort, reduce\n+    String finalPhaseInTask;\n+    TaskReport[] reports;\n+    if (type == TaskType.MAP) {\n+      reports = job.jc_.getMapTaskReports(job.jobId_);\n+      finalPhaseInTask = \"sort\";\n+    } else {// reduce task\n+      reports = job.jc_.getReduceTaskReports(job.jobId_);\n+      finalPhaseInTask = \"reduce\";\n     }\n+    assertEquals(1, reports.length);\n+    assertEquals(expectedStatus + \" > \" + finalPhaseInTask,\n+        reports[0].getState());\n   }\n+\n+  // Validate the job output\n+  void validateJobOutput(Configuration conf)\n+      throws IOException {\n+\n+    String output = MapReduceTestUtil.readOutput(\n+        new Path(OUTPUT_DIR), conf).trim();\n+\n+    assertTrue(output.equals(expectedOutput));\n+  }\n+\n+  // Validate stderr task log of given task type(validates 1st\n+  // task of that type).\n+  void validateTaskStderr(StreamJob job, TaskType type)\n+      throws IOException {\n+    TaskAttemptID attemptId =\n+        new TaskAttemptID(new TaskID(job.jobId_, type, 0), 0);\n+\n+    String log = MapReduceTestUtil.readTaskLog(TaskLog.LogName.STDERR,\n+        attemptId, false);\n+\n+    // trim() is called on expectedStderr here because the method\n+    // MapReduceTestUtil.readTaskLog() returns trimmed String.\n+    assertTrue(log.equals(expectedStderr.trim()));\n+  }\n+\n+  // Validate if user counter is incremented properly\n+  void validateUserCounter(StreamJob job, int expectedCounterValue)\n+      throws IOException {\n+    Counters counters = job.running_.getCounters();\n+    assertEquals(expectedCounterValue, counters.findCounter(\n+        \"myOwnCounterGroup\", \"myOwnCounter\").getValue());\n+  }\n+\n }",
                "deletions": 38
            },
            {
                "sha": "c8c92e6c64131840ab6e102c49eb78130d89a31e",
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -25,7 +25,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.util.Shell;\n \n import org.junit.Test;\n@@ -131,7 +131,7 @@ private void runStreamJobAndValidateEnv() throws IOException {\n     assertEquals(\"StreamJob failed.\", 0, returnStatus);\n     \n     // validate environment variables set for the child(script) of java process\n-    String env = TestMiniMRWithDFS.readOutput(outputPath, mr.createJobConf());\n+    String env = MapReduceTestUtil.readOutput(outputPath, mr.createJobConf());\n     long logSize = USERLOG_LIMIT_KB * 1024;\n     assertTrue(\"environment set for child is wrong\", env.contains(\"INFO,TLA\")\n                && env.contains(\"-Dhadoop.tasklog.taskid=attempt_\")",
                "deletions": 2
            },
            {
                "sha": "854507946d865e16924d1c3f71979990b9b12fd2",
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -26,8 +26,8 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.util.StringUtils;\n \n@@ -122,7 +122,7 @@ private void runProgram(String memLimit) throws IOException {\n     boolean mayExit = false;\n     StreamJob job = new StreamJob(genArgs(memLimit), mayExit);\n     job.go();\n-    String output = TestMiniMRWithDFS.readOutput(outputPath,\n+    String output = MapReduceTestUtil.readOutput(outputPath,\n                                         mr.createJobConf());\n     assertEquals(\"output is wrong\", SET_MEMORY_LIMIT,\n                                     output.trim());",
                "deletions": 2
            },
            {
                "sha": "83eed74993dc4259ef593b89be2a3e8f28a5ade8",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n \n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n@@ -217,7 +218,7 @@ private String launchWordCount(JobConf conf,\n     conf.setNumMapTasks(numMaps);\n     conf.setNumReduceTasks(numReduces);\n     JobClient.runJob(conf);\n-    return TestMiniMRWithDFS.readOutput(outDir, conf);\n+    return MapReduceTestUtil.readOutput(outDir, conf);\n   }\n \n }",
                "deletions": 1
            },
            {
                "sha": "239c239230e5a836902fb0b1e380ec2542e7c49a",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.hadoop.fs.*;\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.lib.*;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionHelper;\n import org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection;\n \n@@ -86,7 +87,7 @@ public static void launch() throws Exception {\n     //\n     boolean success = true;\n     Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\n-    String outdata = TestMiniMRWithDFS.readOutput(outPath,job);\n+    String outdata = MapReduceTestUtil.readOutput(outPath,job);\n \n     assertEquals(expectedOutput.toString(),outdata);\n     fs.delete(OUTPUT_DIR, true);",
                "deletions": 1
            },
            {
                "sha": "b32d5263c95e455eb1206a209b677c9c22c71d81",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n /**\n@@ -93,7 +94,7 @@ public static TestResult launchWordCount(JobConf conf,\n     // Check if the Job Tracker system dir is propogated to client\n     assertFalse(sysDir.contains(\"/tmp/subru/mapred/system\"));\n     assertTrue(sysDir.contains(\"custom\"));\n-    return new TestResult(job, TestMiniMRWithDFS.readOutput(outDir, conf));\n+    return new TestResult(job, MapReduceTestUtil.readOutput(outDir, conf));\n   }\n \n  static void runWordCount(MiniMRCluster mr, JobConf jobConf, String sysDir) ",
                "deletions": 1
            },
            {
                "sha": "2788a373402071023400e964d8b331a6dad0ba59",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.io.WritableUtils;\n import org.apache.hadoop.mapred.MRCaching.TestResult;\n import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.TaskCounter;\n import org.apache.hadoop.mapreduce.TestMapReduceLocal;\n import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n@@ -125,7 +126,7 @@ private void runCustomFormats(MiniMRCluster mr) throws IOException {\n     try {\n       JobClient.runJob(job);\n       String result = \n-        TestMiniMRWithDFS.readOutput(outDir, job);\n+        MapReduceTestUtil.readOutput(outDir, job);\n       assertEquals(\"output\", (\"aunt annie\\t1\\n\" +\n                               \"bumble boat\\t4\\n\" +\n                               \"crocodile pants\\t0\\n\" +",
                "deletions": 1
            },
            {
                "sha": "4b90a675b56828e1a3d2a58007973686b814afa6",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "status": "modified",
                "changes": 28,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapreduce.MRConfig;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.TaskCounter;\n import org.apache.hadoop.mapreduce.TaskType;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -97,32 +98,7 @@ public static TestResult launchWordCount(JobConf conf,\n     conf.setNumMapTasks(numMaps);\n     conf.setNumReduceTasks(numReduces);\n     RunningJob job = JobClient.runJob(conf);\n-    return new TestResult(job, readOutput(outDir, conf));\n-  }\n-\n-  public static String readOutput(Path outDir, \n-                                  JobConf conf) throws IOException {\n-    FileSystem fs = outDir.getFileSystem(conf);\n-    StringBuffer result = new StringBuffer();\n-    {\n-      \n-      Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,\n-                                   new Utils.OutputFileUtils\n-                                            .OutputFilesFilter()));\n-      for(int i=0; i < fileList.length; ++i) {\n-        LOG.info(\"File list[\" + i + \"]\" + \": \"+ fileList[i]);\n-        BufferedReader file = \n-          new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\n-        String line = file.readLine();\n-        while (line != null) {\n-          result.append(line);\n-          result.append(\"\\n\");\n-          line = file.readLine();\n-        }\n-        file.close();\n-      }\n-    }\n-    return result.toString();\n+    return new TestResult(job, MapReduceTestUtil.readOutput(outDir, conf));\n   }\n \n   /**",
                "deletions": 26
            },
            {
                "sha": "6da96ce22bd052d23e9cc397056db8282e14f4af",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.*;\n import org.apache.hadoop.mapred.lib.*;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n+\n import junit.framework.TestCase;\n import java.io.*;\n import java.util.*;\n@@ -107,7 +109,7 @@ public static void launch() throws Exception {\n     //\n     boolean success = true;\n     Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\n-    String outdata = TestMiniMRWithDFS.readOutput(outPath,job);\n+    String outdata = MapReduceTestUtil.readOutput(outPath,job);\n     System.out.println(\"full out data:\");\n     System.out.println(outdata.toString());\n     outdata = outdata.substring(0, expectedOutput.toString().length());",
                "deletions": 1
            },
            {
                "sha": "2df117b107891ae50b99f1fd642ea89fcc298b2e",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -38,9 +38,9 @@\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n import org.apache.hadoop.mapred.RunningJob;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n import org.apache.hadoop.mapred.Utils;\n import org.apache.hadoop.mapred.Counters.Counter;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -199,7 +199,7 @@ static void runProgram(MiniMRCluster mr, MiniDFSCluster dfs,\n     for (Path p:FileUtil.stat2Paths(dfs.getFileSystem().listStatus(outputPath,\n     \t\t                        new Utils.OutputFileUtils\n     \t\t                                 .OutputFilesFilter()))) {\n-      results.add(TestMiniMRWithDFS.readOutput(p, job));\n+      results.add(MapReduceTestUtil.readOutput(p, job));\n     }\n     assertEquals(\"number of reduces is wrong\", \n                  expectedResults.length, results.size());",
                "deletions": 2
            },
            {
                "sha": "65462b9ca66ed1161086e0eeaee2c1b792dbbf53",
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -396,7 +396,8 @@ public Counter getCounter(String group, String name) {\n       }\n     };\n   }\n-  \n+\n+  // Return output of MR job by reading from the given output directory\n   public static String readOutput(Path outDir, Configuration conf) \n       throws IOException {\n     FileSystem fs = outDir.getFileSystem(conf);",
                "deletions": 1
            },
            {
                "sha": "f24dffe2655c6129377ae953d3f50eafa03581bf",
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "status": "modified",
                "changes": 23,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.Utils;\n import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n@@ -112,7 +113,7 @@ public static void launch() throws Exception {\n     // original one.  Remember, we need to ignore zero-count items\n     // in the original key.\n     //\n-    String outdata = readOutput(OUTPUT_DIR, conf);\n+    String outdata = MapReduceTestUtil.readOutput(OUTPUT_DIR, conf);\n     System.out.println(\"full out data:\");\n     System.out.println(outdata.toString());\n     outdata = outdata.substring(0, expectedOutput.toString().length());\n@@ -121,26 +122,6 @@ public static void launch() throws Exception {\n     fs.delete(OUTPUT_DIR, true);\n     fs.delete(INPUT_DIR, true);\n   }\n-\n-  public static String readOutput(Path outDir, Configuration conf) \n-    throws IOException {\n-    FileSystem fs = outDir.getFileSystem(conf);\n-    StringBuffer result = new StringBuffer();\n-    Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,\n-                        new Utils.OutputFileUtils.OutputFilesFilter()));\n-    for(int i=0; i < fileList.length; ++i) {\n-      BufferedReader file = \n-        new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\n-      String line = file.readLine();\n-      while (line != null) {\n-        result.append(line);\n-        result.append(\"\\n\");\n-        line = file.readLine();\n-      }\n-      file.close();\n-    }\n-    return result.toString();\n-  }\n   \n   /**\n    * Launches all the tasks in order.",
                "deletions": 21
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. Contributed by Robert Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1130994 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/0727aa6e8cb2499562ae1d35f7a4e5a923083571",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/34fc2ec906ba5602d4a65b0bb2ec9fc7c4cb855f",
        "bug_id": "hadoop-mapreduce_5",
        "file": [
            {
                "sha": "493d13037e920b46fb0b710e743a3eaf1d5f3b8f",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "patch": "@@ -163,6 +163,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. (Robert Evans via\n+    acmurthy) \n+\n     MAPREDUCE-2531. Fixed jobcontrol to downgrade JobID. (Robert Evans via\n     acmurthy) \n ",
                "deletions": 0
            },
            {
                "sha": "245e362866fe446422f09c9425a1f3e6651e7879",
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "status": "modified",
                "changes": 34,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "patch": "@@ -576,6 +576,8 @@ public RunningJob getJob(String jobid) throws IOException {\n     return getJob(JobID.forName(jobid));\n   }\n   \n+  private static final TaskReport[] EMPTY_TASK_REPORTS = new TaskReport[0];\n+  \n   /**\n    * Get the information of the current state of the map tasks of a job.\n    * \n@@ -584,9 +586,16 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */\n   public TaskReport[] getMapTaskReports(JobID jobId) throws IOException {\n+    return getTaskReports(jobId, TaskType.MAP);\n+  }\n+  \n+  private TaskReport[] getTaskReports(JobID jobId, TaskType type) throws IOException {\n     try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.MAP));\n+      Job j = cluster.getJob(jobId);\n+      if(j == null) {\n+        return EMPTY_TASK_REPORTS;\n+      }\n+      return TaskReport.downgradeArray(j.getTaskReports(type));\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n@@ -606,12 +615,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getReduceTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.REDUCE));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.REDUCE);\n   }\n \n   /**\n@@ -622,12 +626,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getCleanupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_CLEANUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_CLEANUP);\n   }\n \n   /**\n@@ -638,12 +637,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getSetupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_SETUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_SETUP);\n   }\n \n   ",
                "deletions": 20
            },
            {
                "sha": "11873c14233814e67b9a766c486e4f8b775edd10",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "status": "added",
                "changes": 94,
                "additions": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.junit.Test;\n+\n+public class JobClientUnitTest {\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testMapTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getMapTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testReduceTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getReduceTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testSetupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getSetupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testCleanupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getCleanupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1127444 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/64b55c391e197a8a72d40da7be50fdcb9f8748b9",
        "bug_id": "hadoop-mapreduce_6",
        "file": [
            {
                "sha": "267bbd5f6761ad691d7e255f6a8f1b766e31c5d1",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -232,6 +232,8 @@ Trunk (unreleased changes)\n     MAPREDUCE-2495. exit() the TaskTracker when the distributed cache cleanup\n     thread dies. (Robert Joseph Evans via cdouglas)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n \n Release 0.22.0 - Unreleased\n ",
                "deletions": 0
            },
            {
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "status": "modified",
                "changes": 15,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "deletions": 6
            },
            {
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "deletions": 1
            },
            {
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "deletions": 1
            },
            {
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "status": "added",
                "changes": 44,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "d84f41a547b710bcec7bebea5f1ed74266917589",
                "filename": "src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "patch": "@@ -129,7 +129,7 @@ public void testFailedTaskJobStatus()\n     }\n     Assert.assertTrue(\"Task has not been started for 1 min.\", counter != 60);\n \n-    NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+    NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n     TaskID tID = TaskID.downgrade(taskInfo.getTaskID());\n     TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);\n     networkJob.killTask(taskAttID, false);\n@@ -245,7 +245,7 @@ public void testDirCleanupAfterTaskKilled()\n       filesStatus = ttClient.listStatus(localTaskDir, true);\n       if (filesStatus.length > 0) {\n         isTempFolderExists = true;\n-        NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+        NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n         networkJob.killTask(taskAttID, false);\n         break;\n       }\n@@ -558,7 +558,7 @@ public void testAllTaskAttemptKill() throws Exception {\n             taskIdKilled = taskid.toString();\n             taskAttemptID = new TaskAttemptID(taskid, i);\n             LOG.info(\"taskAttemptid going to be killed is : \" + taskAttemptID);\n-            (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+            (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                 taskAttemptID, true);\n             checkTaskCompletionEvent(taskAttemptID, jInfo);\n             break;\n@@ -568,7 +568,7 @@ public void testAllTaskAttemptKill() throws Exception {\n               LOG\n                   .info(\"taskAttemptid going to be killed is : \"\n                       + taskAttemptID);\n-              (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+              (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                   taskAttemptID, true);\n               checkTaskCompletionEvent(taskAttemptID, jInfo);\n               break;\n@@ -611,7 +611,7 @@ public void checkTaskCompletionEvent(\n     int count = 0;\n     while (!match) {\n       TaskCompletionEvent[] taskCompletionEvents =\n-          jobClient.new NetworkedJob(jInfo.getStatus())\n+          new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)\n               .getTaskCompletionEvents(0);\n       for (TaskCompletionEvent taskCompletionEvent : taskCompletionEvents) {\n         if ((taskCompletionEvent.getTaskAttemptId().toString())",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1125578 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/921c7920150466cb545dca153e05f0edeacd3754",
        "bug_id": "hadoop-mapreduce_7",
        "file": [
            {
                "sha": "e1eb16d5a672f5955bcf791150d2caaf76949320",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "patch": "@@ -214,6 +214,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-2518. The t flag is missing in distcp help message.  (Wei Yongjun\n     via szetszwo)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "status": "modified",
                "changes": 15,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "deletions": 6
            },
            {
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "deletions": 1
            },
            {
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "deletions": 1
            },
            {
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "status": "added",
                "changes": 44,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-2317. Fix a NPE in HadoopArchives.  Contributed by Devaraj K\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1096022 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/11131d72040c250f897a8d1c130b83ce1b065050",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/ac3445f84c3699f5032374587c3c80087b598e0f",
        "bug_id": "hadoop-mapreduce_8",
        "file": [
            {
                "sha": "9f26015d60ab79f23ebb727a7c2cdc543fbbf831",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "patch": "@@ -2198,3 +2198,5 @@ Release 0.21.0 - 2010-08-13\n \n     MAPREDUCE-1856. Extract a subset of tests for smoke (DOA) validation (cos)\n \n+    MAPREDUCE-2317. Fix a NPE in HadoopArchives.  (Devaraj K via szetszwo)\n+",
                "deletions": 0
            },
            {
                "sha": "41d149ea952f98e877d56de2deb3c5ba482ae120",
                "filename": "src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "status": "modified",
                "changes": 20,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/tools/org/apache/hadoop/tools/HadoopArchives.java?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "patch": "@@ -367,16 +367,18 @@ private void writeTopLevelDirs(SequenceFile.Writer srcWriter,\n         }\n         else {\n           Path parent = p.getParent();\n-          if (allpaths.containsKey(parent.toString())) {\n-            HashSet<String> children = allpaths.get(parent.toString());\n-            children.add(p.getName());\n-          }\n-          else {\n-            HashSet<String> children = new HashSet<String>();\n-            children.add(p.getName());\n-            allpaths.put(parent.toString(), children);\n+          if (null != parent) {\n+            if (allpaths.containsKey(parent.toString())) {\n+              HashSet<String> children = allpaths.get(parent.toString());\n+              children.add(p.getName());\n+            } \n+            else {\n+              HashSet<String> children = new HashSet<String>();\n+              children.add(p.getName());\n+              allpaths.put(parent.toString(), children);\n+            }\n+            parents.add(parent);\n           }\n-          parents.add(parent);\n         }\n       }\n       justDirs = parents;",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-2034. TestSubmitJob triggers NPE instead of permissions error. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1033668 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/dc0a8497e9073732de2d70fea437eb96a0bbd735",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/1085c204f5f73949b6a1f7cc8f085d91b3a2e666",
        "bug_id": "hadoop-mapreduce_9",
        "file": [
            {
                "sha": "0cb361dace0644dd8043be92c9d50f260b788f9e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/dc0a8497e9073732de2d70fea437eb96a0bbd735/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/dc0a8497e9073732de2d70fea437eb96a0bbd735/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=dc0a8497e9073732de2d70fea437eb96a0bbd735",
                "patch": "@@ -383,6 +383,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-2179. Fix RaidBlockSender compilation failure. (Ramkumar Vadali\n     via schen)\n \n+    MAPREDUCE-2034. TestSubmitJob triggers NPE instead of permissions error.\n+    (Todd Lipcon via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   NEW FEATURES",
                "deletions": 0
            },
            {
                "sha": "39a92cefd9134e1796f5ae671c30844ccb68906d",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/dc0a8497e9073732de2d70fea437eb96a0bbd735/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/dc0a8497e9073732de2d70fea437eb96a0bbd735/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "status": "modified",
                "changes": 21,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java?ref=dc0a8497e9073732de2d70fea437eb96a0bbd735",
                "patch": "@@ -259,31 +259,30 @@ public RunningJob run() throws IOException {\n         getDFSClient(conf_other, user2);\n \n       // try accessing mapred.system.dir/jobid/*\n-      boolean failed = false;\n       try {\n-        Path path = new Path(new URI(jt.getSystemDir()).getPath());\n+        String path = new URI(jt.getSystemDir()).getPath();\n         LOG.info(\"Try listing the mapred-system-dir as the user (\" \n                  + user2.getUserName() + \")\");\n-        client.getListing(\n-            path.toString(), HdfsFileStatus.EMPTY_NAME, false);\n+        client.getListing(path, HdfsFileStatus.EMPTY_NAME, false);\n+        fail(\"JobTracker system dir is accessible to others\");\n       } catch (IOException ioe) {\n-        failed = true;\n+        assertTrue(ioe.toString(),\n+          ioe.toString().contains(\"Permission denied\"));\n       }\n-      assertTrue(\"JobTracker system dir is accessible to others\", failed);\n       // try accessing ~/.staging/jobid/*\n-      failed = false;\n       JobInProgress jip = jt.getJob(id);\n       Path jobSubmitDirpath = \n         new Path(jip.getJobConf().get(\"mapreduce.job.dir\"));\n       try {\n         LOG.info(\"Try accessing the job folder for job \" + id + \" as the user (\" \n                  + user2.getUserName() + \")\");\n-        client.getListing(\n-            jobSubmitDirpath.toString(), HdfsFileStatus.EMPTY_NAME, false);\n+        client.getListing(jobSubmitDirpath.toUri().getPath(),\n+          HdfsFileStatus.EMPTY_NAME, false);\n+        fail(\"User's staging folder is accessible to others\");\n       } catch (IOException ioe) {\n-        failed = true;\n+        assertTrue(ioe.toString(),\n+          ioe.toString().contains(\"Permission denied\"));\n       }\n-      assertTrue(\"User's staging folder is accessible to others\", failed);\n       UtilsForTests.signalTasks(dfs, fs, true, mapSignalFile.toString(), \n       reduceSignalFile.toString());\n       // wait for job to be done",
                "deletions": 11
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1621. Fixes NPE in TextOutputReader.getLastOutput if it has never read any output. Contributed by Amareshwari Sriramadasu\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@964752 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/a8047bca029795d32551379ab5f29f26f1d3ed23",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/a4cef9ce651810793595fabce583cbc208c16b22",
        "bug_id": "hadoop-mapreduce_10",
        "file": [
            {
                "sha": "0769aa2c1a6515fcfdc7c110b9850f49cd7c28df",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/a8047bca029795d32551379ab5f29f26f1d3ed23/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/a8047bca029795d32551379ab5f29f26f1d3ed23/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=a8047bca029795d32551379ab5f29f26f1d3ed23",
                "patch": "@@ -170,6 +170,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1865. Rumen should also support jobhistory files generated using\n     trunk. (Amar Kamat via amareshwari)\n \n+    MAPREDUCE-1621. Fixes NPE in TextOutputReader.getLastOutput if it has never\n+    read any output. (amareshwari)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "2ffb62c27a8393238526db018f16e71434fdb60a",
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java?ref=a8047bca029795d32551379ab5f29f26f1d3ed23",
                "patch": "@@ -68,7 +68,11 @@ public BytesWritable getCurrentValue() throws IOException {\n \n   @Override\n   public String getLastOutput() {\n-    return new BytesWritable(bytes).toString();\n+    if (bytes != null) {\n+      return new BytesWritable(bytes).toString();\n+    } else {\n+      return null;\n+    }\n   }\n \n   private int readLength() throws IOException {",
                "deletions": 1
            },
            {
                "sha": "06c05bc9ef795575dea20ecb81e18cbb82811fd1",
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TextOutputReader.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TextOutputReader.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TextOutputReader.java",
                "status": "modified",
                "changes": 12,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TextOutputReader.java?ref=a8047bca029795d32551379ab5f29f26f1d3ed23",
                "patch": "@@ -83,10 +83,14 @@ public Text getCurrentValue() throws IOException {\n \n   @Override\n   public String getLastOutput() {\n-    try {\n-      return new String(bytes, \"UTF-8\");\n-    } catch (UnsupportedEncodingException e) {\n-      return \"<undecodable>\";\n+    if (bytes != null) {\n+      try {\n+        return new String(bytes, \"UTF-8\");\n+      } catch (UnsupportedEncodingException e) {\n+        return \"<undecodable>\";\n+      }\n+    } else {\n+      return null;\n     }\n   }\n ",
                "deletions": 4
            },
            {
                "sha": "e5526ef6f717eb7c958e540b9733afb633c31d1e",
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/a8047bca029795d32551379ab5f29f26f1d3ed23/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java?ref=a8047bca029795d32551379ab5f29f26f1d3ed23",
                "patch": "@@ -70,7 +70,11 @@ public TypedBytesWritable getCurrentValue() throws IOException {\n \n   @Override\n   public String getLastOutput() {\n-    return new TypedBytesWritable(bytes).toString();\n+    if (bytes != null) {\n+      return new TypedBytesWritable(bytes).toString();\n+    } else {\n+      return null;\n+    }\n   }\n \n }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1863. Fix NPE in Rumen when processing null CDF for failed task attempts. Contributed by Amar Kamat\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@958841 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/1a84d983db2623af64c7d1b4c2cfcff276873609",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/d3fc6b3d90714c96988009bc5edcc29be800dbb0",
        "bug_id": "hadoop-mapreduce_11",
        "file": [
            {
                "sha": "46294b6aa914b6c135c86cae792cd25bbbe2f4e5",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "patch": "@@ -123,6 +123,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1876. Fixes TaskAttemptStartedEvent to correctly log event type\n     for all task types. (Amar Kamat via amareshwari)\n \n+    MAPREDUCE-1863. Fix NPE in Rumen when processing null CDF for failed task\n+    attempts. (Amar Kamat via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "c58d801d8bdef43c51254a86ba04b562e257fd53",
                "filename": "src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "patch": "@@ -214,7 +214,7 @@ public void testHadoop20JHParser() throws Exception {\n             lfs.getUri(), lfs.getWorkingDirectory());\n \n     final Path rootInputPath = new Path(rootInputDir, \"rumen/small-trace-test\");\n-    final Path tempDir = new Path(rootTempDir, \"TestRumenViaDispatch\");\n+    final Path tempDir = new Path(rootTempDir, \"TestHadoop20JHParser\");\n     lfs.delete(tempDir, true);\n \n     final Path inputPath = new Path(rootInputPath, \"v20-single-input-log.gz\");",
                "deletions": 1
            },
            {
                "sha": "fdf9b353d38d5a0cdee8e94f601ec5de9bd16c2b",
                "filename": "src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "status": "modified",
                "changes": 0,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 0
            },
            {
                "sha": "8ca1f06c60e952914aeaaca74d379b3796dd3b35",
                "filename": "src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "status": "modified",
                "changes": 0,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 0
            },
            {
                "sha": "2877eb7f06c61208a3588191ab6b9d92e9ddabfd",
                "filename": "src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "status": "modified",
                "changes": 2,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "patch": "@@ -357,8 +357,6 @@ public LoggedJob build() {\n         100);\n     result.setSuccessfulReduceAttemptCDF(succReduce);\n \n-    result.setFailedMapAttemptCDFs(null);\n-\n     long totalSuccessfulAttempts = 0L;\n     long maxTriesToSucceed = 0L;\n ",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1707. TaskRunner can get NPE in getting ugi from TaskTracker. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@940740 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/1f471a2232362ae29911a6ca50dd4cd330faa225",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/77be52c3293b52e329fd08baee7969b8e27ce943",
        "bug_id": "hadoop-mapreduce_12",
        "file": [
            {
                "sha": "ba5fd8006ab37795cc780e75a98ef49216e23d22",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/CHANGES.txt",
                "status": "modified",
                "changes": 13,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "patch": "@@ -2,6 +2,19 @@ Hadoop MapReduce Change Log\n \n Trunk (unreleased changes)\n \n+  INCOMPATIBLE CHANGES\n+\n+  NEW FEATURES\n+\n+  IMPROVEMENTS\n+\n+  OPTIMIZATIONS\n+\n+  BUG FIXES\n+\n+    MAPREDUCE-1707. TaskRunner can get NPE in getting ugi from TaskTracker.\n+    (Vinod Kumar Vavilapalli)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "02ae6f88b2e2eefd17bc9cded4b449adc160d224",
                "filename": "src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "status": "modified",
                "changes": 9,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/TaskRunner.java?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "patch": "@@ -177,9 +177,7 @@ public final void run() {\n \n       // We don't create any symlinks yet, so presence/absence of workDir\n       // actually on the file system doesn't matter.\n-      UserGroupInformation ugi = \n-        tracker.getRunningJob(t.getJobID()).getUGI();\n-      ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+      tip.getUGI().doAs(new PrivilegedExceptionAction<Void>() {\n         public Void run() throws IOException {\n           taskDistributedCacheManager = \n             tracker.getTrackerDistributedCacheManager()\n@@ -256,8 +254,9 @@ public Void run() throws IOException {\n       }\n     } finally {\n       try{\n-        taskDistributedCacheManager.release();\n-\n+        if (taskDistributedCacheManager != null) {\n+          taskDistributedCacheManager.release();\n+        }\n       }catch(IOException ie){\n         LOG.warn(\"Error releasing caches : Cache files might not have been cleaned up\");\n       }",
                "deletions": 5
            },
            {
                "sha": "ac5d47519db14c93fc1d90edf57ae14011730fae",
                "filename": "src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "status": "modified",
                "changes": 21,
                "additions": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/TaskTracker.java?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "patch": "@@ -1156,9 +1156,11 @@ private void localizeJobJarFile(String user, JobID jobId, FileSystem localFs,\n     }\n   }\n \n-  private void launchTaskForJob(TaskInProgress tip, JobConf jobConf) throws IOException{\n+  private void launchTaskForJob(TaskInProgress tip, JobConf jobConf,\n+      UserGroupInformation ugi) throws IOException {\n     synchronized (tip) {\n       tip.setJobConf(jobConf);\n+      tip.setUGI(ugi);\n       tip.launchTask();\n     }\n   }\n@@ -2232,7 +2234,8 @@ private TaskInProgress registerTask(LaunchTaskAction action,\n   void startNewTask(TaskInProgress tip) {\n     try {\n       RunningJob rjob = localizeJob(tip);\n-      launchTaskForJob(tip, new JobConf(rjob.jobConf)); \n+      // Localization is done. Neither rjob.jobConf nor rjob.ugi can be null\n+      launchTaskForJob(tip, new JobConf(rjob.jobConf), rjob.ugi); \n     } catch (Throwable e) {\n       String msg = (\"Error initializing \" + tip.getTask().getTaskID() + \n                     \":\\n\" + StringUtils.stringifyException(e));\n@@ -2373,7 +2376,19 @@ public void run() {\n     private String debugCommand;\n     private volatile boolean slotTaken = false;\n     private TaskLauncher launcher;\n-        \n+\n+    // The ugi of the user who is running the job. This contains all the tokens\n+    // too which will be populated during job-localization\n+    private UserGroupInformation ugi;\n+\n+    UserGroupInformation getUGI() {\n+      return ugi;\n+    }\n+\n+    void setUGI(UserGroupInformation userUGI) {\n+      ugi = userUGI;\n+    }\n+\n     /**\n      */\n     public TaskInProgress(Task task, JobConf conf) {",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1508. Protect against NPE in TestMultipleLevelCaching. Contributed by Aaron Kimball\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@925554 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/03bf4679e2705ffd70452a1b7b0e29b058cae290",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/67166032772b591062b187be3e284cc5ed8f2ac2",
        "bug_id": "hadoop-mapreduce_13",
        "file": [
            {
                "sha": "0c5c780698ceb83acf0a4dff998dba0bc609b0c1",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/03bf4679e2705ffd70452a1b7b0e29b058cae290/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/03bf4679e2705ffd70452a1b7b0e29b058cae290/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=03bf4679e2705ffd70452a1b7b0e29b058cae290",
                "patch": "@@ -471,6 +471,9 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-1615. Fix compilation of TestSubmitJob. (cdouglas)\n \n+    MAPREDUCE-1508. Protect against NPE in TestMultipleLevelCaching. (Aaron\n+    Kimball via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "6b767fc12380f79f27cb27b1beabfed9d1802e24",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/03bf4679e2705ffd70452a1b7b0e29b058cae290/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/03bf4679e2705ffd70452a1b7b0e29b058cae290/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "status": "modified",
                "changes": 7,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java?ref=03bf4679e2705ffd70452a1b7b0e29b058cae290",
                "patch": "@@ -119,8 +119,11 @@ private void testCachingAtLevel(int level) throws IOException {\n     \t\t  testName, mr, fileSys, inDir, outputPath, 1, 1, 0, 0);\n       mr.shutdown();\n     } finally {\n-      fileSys.delete(inDir, true);\n-      fileSys.delete(outputPath, true);\n+      if (null != fileSys) {\n+        // inDir, outputPath only exist if fileSys is valid.\n+        fileSys.delete(inDir, true);\n+        fileSys.delete(outputPath, true);\n+      }\n       if (dfs != null) { \n         dfs.shutdown(); \n       }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1490. Fix a NPE that could occur during instantiation and\ninitialization of the DistributedRaidFileSystem. \n(Rodrigo Schmidt via dhruba)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@909993 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/408e44ac2074c503caf21aa73113dc48584619bb",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/2e646c225c5dd696c345f1c1fd9190f0b35b165d",
        "bug_id": "hadoop-mapreduce_14",
        "file": [
            {
                "sha": "c1c8ef9bb508a8e5b749842085dbc5d0e0dbec6b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/408e44ac2074c503caf21aa73113dc48584619bb/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/408e44ac2074c503caf21aa73113dc48584619bb/CHANGES.txt",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "patch": "@@ -341,6 +341,12 @@ Trunk (unreleased changes)\n     MAPREDUCE-1358. Avoid false positives in OutputLogFilter. (Todd Lipcon via\n     cdouglas)\n \n+    MAPREDUCE-1490. Fix a NullPointerException that could occur during \n+    instantiation and initialization of the DistributedRaidFileSystem. \n+    (Rodrigo Schmidt via dhruba)\n+\n+    cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "72cda6b9596cae00f5081babb7652897dabbcfaa",
                "filename": "src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "status": "modified",
                "changes": 10,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "patch": "@@ -65,9 +65,17 @@\n   /* Initialize a Raid FileSystem\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n-    super.initialize(name, conf);\n     this.conf = conf;\n \n+    Class<?> clazz = conf.getClass(\"fs.raid.underlyingfs.impl\",\n+        DistributedFileSystem.class);\n+    if (clazz == null) {\n+      throw new IOException(\"No FileSystem for fs.raid.underlyingfs.impl.\");\n+    }\n+    \n+    this.fs = (FileSystem)ReflectionUtils.newInstance(clazz, null); \n+    super.initialize(name, conf);\n+    \n     String alt = conf.get(\"hdfs.raid.locations\");\n     \n     // If no alternates are specified, then behave absolutely same as ",
                "deletions": 1
            },
            {
                "sha": "dab43e2c776a0206a126f685db1ca572eb96d726",
                "filename": "src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "status": "modified",
                "changes": 13,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.PrintWriter;\n+import java.net.URI;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.GregorianCalendar;\n@@ -177,10 +178,14 @@ public void testRaidDfs() throws Exception {\n \n       // filter all filesystem calls from client\n       Configuration clientConf = new Configuration(conf);\n-      clientConf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.dfs.DistributedRaidFileSystem\");\n-      DistributedRaidFileSystem raidfs = new DistributedRaidFileSystem(dfs);\n-      raidfs.initialize(dfs.getUri(), clientConf);\n-\n+      clientConf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedRaidFileSystem\");\n+      clientConf.set(\"fs.raid.underlyingfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+      URI dfsUri = dfs.getUri();\n+      FileSystem.closeAll();\n+      FileSystem raidfs = FileSystem.get(dfsUri, clientConf);\n+      \n+      assertTrue(\"raidfs not an instance of DistributedRaidFileSystem\",raidfs instanceof DistributedRaidFileSystem);\n+      \n       // corrupt first block of file\n       LOG.info(\"Corrupt first block of file\");\n       corruptBlock(file1, locations.get(0).getBlock());",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1313. Fix NPE in Sqoop when table with null fields uses escape\nduring import. Contributed by Aaron Kimball\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@902674 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/c2f3e11399ff6810b16491c01b96dbcb9279eee9",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/385b5e06b5121ae939a3e340e396bc1fa3472309",
        "bug_id": "hadoop-mapreduce_15",
        "file": [
            {
                "sha": "08688d0e4c2579b084284068677dad6da40f2333",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "patch": "@@ -241,6 +241,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1388. Move the HDFS RAID package from HDFS to MAPREDUCE.\n     (Eli Collins via dhruba)\n \n+    MAPREDUCE-1313. Fix NPE in Sqoop when table with null fields uses escape\n+    during import. (Aaron Kimball via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "deletions": 0
            },
            {
                "sha": "a1b6742133b49da13448a6a94d4b7cc87c2bf9cb",
                "filename": "src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "patch": "@@ -57,6 +57,10 @@ public static final String escapeAndEnclose(String str, String escape, String en\n     boolean escapingLegal = (null != escape && escape.length() > 0 && !escape.equals(\"\\000\"));\n     String withEscapes;\n \n+    if (null == str) {\n+      return null;\n+    }\n+\n     if (escapingLegal) {\n       // escaping is legal. Escape any instances of the escape char itself\n       withEscapes = str.replace(escape, escape + escape);",
                "deletions": 0
            },
            {
                "sha": "31c5167d4e0d5523ca5c903e6fc5f620d72868e9",
                "filename": "src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "status": "modified",
                "changes": 17,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "patch": "@@ -58,6 +58,8 @@\n     args.add(HsqldbTestServer.getUrl());\n     args.add(\"--num-mappers\");\n     args.add(\"1\");\n+    args.add(\"--escaped-by\");\n+    args.add(\"\\\\\");\n \n     return args.toArray(new String[0]);\n   }\n@@ -86,9 +88,18 @@ public void setUp() {\n     // create two tables.\n     this.expectedStrings.add(\"A winner\");\n     this.expectedStrings.add(\"is you!\");\n+    this.expectedStrings.add(null);\n \n+    int i = 0;\n     for (String expectedStr: this.expectedStrings) {\n-      this.createTableForColType(\"VARCHAR(32) PRIMARY KEY\", \"'\" + expectedStr + \"'\");\n+      String wrappedStr = null;\n+      if (expectedStr != null) {\n+        wrappedStr = \"'\" + expectedStr + \"'\";\n+      }\n+\n+      String [] types = { \"INT NOT NULL PRIMARY KEY\", \"VARCHAR(32)\" };\n+      String [] vals = { Integer.toString(i++) , wrappedStr };\n+      this.createTableWithColTypes(types, vals);\n       this.tableNames.add(this.getTableName());\n       this.removeTableDir();\n       incrementTableNum();\n@@ -100,13 +111,15 @@ public void testMultiTableImport() throws IOException {\n     runImport(argv);\n \n     Path warehousePath = new Path(this.getWarehouseDir());\n+    int i = 0;\n     for (String tableName : this.tableNames) {\n       Path tablePath = new Path(warehousePath, tableName);\n       Path filePath = new Path(tablePath, \"part-m-00000\");\n \n       // dequeue the expected value for this table. This\n       // list has the same order as the tableNames list.\n-      String expectedVal = this.expectedStrings.get(0);\n+      String expectedVal = Integer.toString(i++) + \",\"\n+          + this.expectedStrings.get(0);\n       this.expectedStrings.remove(0);\n \n       BufferedReader reader = new BufferedReader(",
                "deletions": 2
            },
            {
                "sha": "661a9ac98bb52a2dac6de286bb35a63c81ea1c78",
                "filename": "src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "patch": "@@ -37,6 +37,10 @@ public void testAllEmpty() {\n   public void testNullArgs() {\n     String result = FieldFormatter.escapeAndEnclose(\"\", null, null, null, false);\n     assertEquals(\"\", result);\n+\n+    char [] encloseFor = { '\\\"' };\n+    assertNull(FieldFormatter.escapeAndEnclose(null, \"\\\\\", \"\\\"\", encloseFor,\n+        false));\n   }\n \n   public void testBasicStr() {",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-754. Fix NPE in expiry thread when a TT is lost. Contributed by Amar Kamat.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@888431 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/da589c34298a140cea761b17e7c8b3946ae5b97c",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/e38a2babaed31d7ca85523b36be78845daf2aef2",
        "bug_id": "hadoop-mapreduce_16",
        "file": [
            {
                "sha": "30feefe5f363d9ac2ae63d31de5291b59cc5a267",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "patch": "@@ -966,3 +966,6 @@ Release 0.21.0 - Unreleased\n     MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent\n     queue. (V.V.Chaitanya Krishna via yhemanth)\n \n+    MAPREDUCE-754. Fix NPE in expiry thread when a TT is lost. (Amar Kamat \n+    via sharad)\n+",
                "deletions": 0
            },
            {
                "sha": "b07e2c0b1afb265140b3f69ccef41d31594f9ef5",
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "status": "modified",
                "changes": 24,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "patch": "@@ -804,17 +804,21 @@ boolean shouldAssignTasksToTracker(String hostName, long now) {\n     private void removeHostCapacity(String hostName) {\n       synchronized (taskTrackers) {\n         // remove the capacity of trackers on this host\n+        int numTrackersOnHost = 0;\n         for (TaskTrackerStatus status : getStatusesOnHost(hostName)) {\n           int mapSlots = status.getMaxMapSlots();\n           totalMapTaskCapacity -= mapSlots;\n           int reduceSlots = status.getMaxReduceSlots();\n           totalReduceTaskCapacity -= reduceSlots;\n+          ++numTrackersOnHost;\n           getInstrumentation().addBlackListedMapSlots(\n               mapSlots);\n           getInstrumentation().addBlackListedReduceSlots(\n               reduceSlots);\n         }\n-        incrBlackListedTrackers(uniqueHostsMap.remove(hostName));\n+        // remove the host\n+        uniqueHostsMap.remove(hostName);\n+        incrBlackListedTrackers(numTrackersOnHost);\n       }\n     }\n     \n@@ -2468,12 +2472,14 @@ boolean updateTaskTrackerStatus(String trackerName,\n         taskTrackers.remove(trackerName);\n         Integer numTaskTrackersInHost = \n           uniqueHostsMap.get(oldStatus.getHost());\n-        numTaskTrackersInHost --;\n-        if (numTaskTrackersInHost > 0)  {\n-          uniqueHostsMap.put(oldStatus.getHost(), numTaskTrackersInHost);\n-        }\n-        else {\n-          uniqueHostsMap.remove(oldStatus.getHost());\n+        if (numTaskTrackersInHost != null) {\n+          numTaskTrackersInHost --;\n+          if (numTaskTrackersInHost > 0)  {\n+            uniqueHostsMap.put(oldStatus.getHost(), numTaskTrackersInHost);\n+          }\n+          else {\n+            uniqueHostsMap.remove(oldStatus.getHost());\n+          }\n         }\n       }\n     }\n@@ -3841,8 +3847,8 @@ synchronized void decommissionNodes(Set<String> hosts)\n           Set<TaskTracker> trackers = hostnameToTaskTracker.remove(host);\n           if (trackers != null) {\n             for (TaskTracker tracker : trackers) {\n-              LOG.info(\"Decommission: Losing tracker \" + tracker + \n-                       \" on host \" + host);\n+              LOG.info(\"Decommission: Losing tracker \" \n+                       + tracker.getTrackerName() + \" on host \" + host);\n               removeTracker(tracker);\n             }\n             trackersDecommissioned += trackers.size();",
                "deletions": 9
            },
            {
                "sha": "09a9fd12440f0832042150aa76f2d03471b84b04",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "patch": "@@ -63,8 +63,10 @@\n     }\n     @Override\n     public ClusterStatus getClusterStatus(boolean detailed) {\n-      return new ClusterStatus(trackers.length,\n-          0, 0, 0, 0, totalSlots/2, totalSlots/2, JobTracker.State.RUNNING, 0);\n+      return new ClusterStatus(\n+          taskTrackers().size() - getBlacklistedTrackerCount(),\n+          getBlacklistedTrackerCount(), 0, 0, 0, totalSlots/2, totalSlots/2, \n+           JobTracker.State.RUNNING, 0);\n     }\n \n     public void setNumSlots(int totalSlots) {",
                "deletions": 2
            },
            {
                "sha": "ef38fee1aa24a51b742076df716832cff3cafc3c",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "status": "modified",
                "changes": 137,
                "additions": 137,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobInProgress;\n import org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker;\n import org.apache.hadoop.mapred.UtilsForTests.FakeClock;\n+import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n /**\n@@ -47,6 +48,7 @@ protected void setUp() throws Exception {\n     conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n     conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n     conf.setLong(JTConfig.JT_TRACKER_EXPIRY_INTERVAL, 1000);\n+    conf.set(JTConfig.JT_MAX_TRACKER_BLACKLISTS, \"1\");\n     jobTracker = new FakeJobTracker(conf, (clock = new FakeClock()), trackers);\n     jobTracker.startExpireTrackersThread();\n   }\n@@ -91,4 +93,139 @@ public void testLostTracker() throws IOException {\n     job.finishTask(tid[1]);\n     \n   }\n+  \n+  /**\n+   * Test whether the tracker gets blacklisted after its lost.\n+   */\n+  public void testLostTrackerBeforeBlacklisting() throws Exception {\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[0]);\n+    TaskAttemptID[] tid = new TaskAttemptID[3];\n+    JobConf conf = new JobConf();\n+    conf.setNumMapTasks(1);\n+    conf.setNumReduceTasks(1);\n+    conf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    conf.set(JobContext.SETUP_CLEANUP_NEEDED, \"false\");\n+    FakeJobInProgress job = new FakeJobInProgress(conf, jobTracker);\n+    job.initTasks();\n+    job.setClusterSize(4);\n+    \n+    // Tracker 0 gets the map task\n+    tid[0] = job.findMapTask(trackers[0]);\n+\n+    job.finishTask(tid[0]);\n+\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    \n+    // lose the tracker\n+    clock.advance(1100);\n+    jobTracker.checkExpiredTrackers();\n+    assertFalse(\"Tracker 0 not lost\", \n+        jobTracker.getClusterStatus(false).getActiveTrackerNames()\n+                  .contains(trackers[0]));\n+    \n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 0, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    \n+    // Tracker 1 establishes contact with JT \n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[1]);\n+    \n+    // Tracker1 should get assigned the lost map task\n+    tid[1] =  job.findMapTask(trackers[1]);\n+\n+    assertNotNull(\"Map Task from Lost Tracker did not get reassigned\", tid[1]);\n+    \n+    assertEquals(\"Task ID of reassigned map task does not match\",\n+        tid[0].getTaskID().toString(), tid[1].getTaskID().toString());\n+    \n+    // finish the map task\n+    job.finishTask(tid[1]);\n+\n+    // finish the reduce task\n+    tid[2] =  job.findReduceTask(trackers[1]);\n+    job.finishTask(tid[2]);\n+    \n+    // check if job is successful\n+    assertEquals(\"Job not successful\", \n+                 JobStatus.SUCCEEDED, job.getStatus().getRunState());\n+    \n+    // check if the tracker is lost\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count .. since we lost one blacklisted tracker\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+  }\n+\n+  /**\n+   * Test whether the tracker gets lost after its blacklisted.\n+   */\n+  public void testLostTrackerAfterBlacklisting() throws Exception {\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[0]);\n+    clock.advance(600);\n+    TaskAttemptID[] tid = new TaskAttemptID[2];\n+    JobConf conf = new JobConf();\n+    conf.setNumMapTasks(1);\n+    conf.setNumReduceTasks(0);\n+    conf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    conf.set(JobContext.SETUP_CLEANUP_NEEDED, \"false\");\n+    FakeJobInProgress job = new FakeJobInProgress(conf, jobTracker);\n+    job.initTasks();\n+    job.setClusterSize(4);\n+    \n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.taskTrackers().size());\n+    \n+    // Tracker 0 gets the map task\n+    tid[0] = job.findMapTask(trackers[0]);\n+    // Fail the task\n+    job.failTask(tid[0]);\n+    \n+    // Tracker 1 establishes contact with JT\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[1]);\n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jobTracker.taskTrackers().size());\n+    \n+    // Tracker 1 gets the map task\n+    tid[1] = job.findMapTask(trackers[1]);\n+    // Finish the task and also the job\n+    job.finishTask(tid[1]);\n+\n+    // check if job is successful\n+    assertEquals(\"Job not successful\", \n+                 JobStatus.SUCCEEDED, job.getStatus().getRunState());\n+    \n+    // check if the trackers 1 got blacklisted\n+    assertTrue(\"Tracker 0 not blacklisted\", \n+               jobTracker.getBlacklistedTrackers()[0].getTaskTrackerName()\n+                 .equals(trackers[0]));\n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jobTracker.taskTrackers().size());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                1, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+    // Advance clock. Tracker 0 should be lost\n+    clock.advance(500);\n+    jobTracker.checkExpiredTrackers();\n+    \n+    // check if the task tracker is lost\n+    assertFalse(\"Tracker 0 not lost\", \n+            jobTracker.getClusterStatus(false).getActiveTrackerNames()\n+                      .contains(trackers[0]));\n+    \n+    // check if the lost tracker has removed from the jobtracker\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.taskTrackers().size());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+  }\n }\n\\ No newline at end of file",
                "deletions": 0
            },
            {
                "sha": "f14c3a49e666072eb7989fae06878fb29046dd56",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "status": "modified",
                "changes": 88,
                "additions": 88,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "patch": "@@ -32,7 +32,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableComparable;\n import org.apache.hadoop.ipc.RPC;\n+import org.apache.hadoop.mapred.lib.IdentityReducer;\n+import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.UnixUserGroupInformation;\n@@ -377,4 +381,88 @@ public void testMRRefreshRecommissioning() throws IOException {\n     \n     stopCluster();\n   }\n+  \n+  // Mapper that fails once for the first time\n+  static class FailOnceMapper extends MapReduceBase implements\n+      Mapper<WritableComparable, Writable, WritableComparable, Writable> {\n+\n+    private boolean shouldFail = false;\n+    public void map(WritableComparable key, Writable value,\n+        OutputCollector<WritableComparable, Writable> out, Reporter reporter)\n+        throws IOException {\n+\n+      if (shouldFail) {\n+        throw new RuntimeException(\"failing map\");\n+      }\n+    }\n+    \n+    @Override\n+    public void configure(JobConf conf) {\n+      TaskAttemptID id = TaskAttemptID.forName(conf.get(\"mapred.task.id\"));\n+      shouldFail = id.getId() == 0 && id.getTaskID().getId() == 0; \n+    }\n+  }\n+  \n+  /**\n+   * Check refreshNodes for decommissioning blacklisted nodes. \n+   */\n+  public void testBlacklistedNodeDecommissioning() throws Exception {\n+    LOG.info(\"Testing blacklisted node decommissioning\");\n+\n+    Configuration conf = new Configuration();\n+    conf.set(JTConfig.JT_MAX_TRACKER_BLACKLISTS, \"1\");\n+\n+    startCluster(2, 1, 0, conf);\n+    \n+    assertEquals(\"Trackers not up\", 2,\n+           mr.getJobTrackerRunner().getJobTracker().getActiveTrackers().length);\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jt.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jt.getClusterStatus(false).getBlacklistedTrackers());\n+\n+    // run a failing job to blacklist the tracker\n+    JobConf jConf = mr.createJobConf();\n+    jConf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    jConf.setJobName(\"test-job-fail-once\");\n+    jConf.setMapperClass(FailOnceMapper.class);\n+    jConf.setReducerClass(IdentityReducer.class);\n+    jConf.setNumMapTasks(1);\n+    jConf.setNumReduceTasks(0);\n+    \n+    RunningJob job = \n+      UtilsForTests.runJob(jConf, new Path(\"in\"), new Path(\"out\"));\n+    job.waitForCompletion();\n+    \n+    // check if the tracker is lost\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jt.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                1, jt.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+    // find the tracker to decommission\n+    String hostToDecommission = \n+      JobInProgress.convertTrackerNameToHostName(\n+          jt.getBlacklistedTrackers()[0].getTaskTrackerName());\n+    LOG.info(\"Decommissioning host \" + hostToDecommission);\n+    \n+    Set<String> decom = new HashSet<String>(1);\n+    decom.add(hostToDecommission);\n+    jt.decommissionNodes(decom);\n+ \n+    // check the cluster status and tracker size\n+    assertEquals(\"Tracker is not lost upon host decommissioning\", \n+                 1, jt.getClusterStatus(false).getTaskTrackers());\n+    assertEquals(\"Blacklisted tracker count incorrect in cluster status after \"\n+                 + \"decommissioning\", \n+                 0, jt.getClusterStatus(false).getBlacklistedTrackers());\n+    assertEquals(\"Tracker is not lost upon host decommissioning\", \n+                 1, jt.taskTrackers().size());\n+    \n+    stopCluster();\n+  }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent queue. Contributed by V.V.Chaitanya Krishna.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@888257 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/9c08b79db0aa747720b5794ad67cba9187d29371",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/ddc67bf61e875e82b47ffb998172f5062791586d",
        "bug_id": "hadoop-mapreduce_17",
        "file": [
            {
                "sha": "385924c8cfabac25911a0372a8c58acbc643c1df",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "patch": "@@ -960,3 +960,7 @@ Release 0.21.0 - Unreleased\n \n     MAPREDUCE-1161. Remove ineffective synchronization in NotificationTestCase.\n     (Owen O'Malley via cdouglas)\n+\n+    MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent\n+    queue. (V.V.Chaitanya Krishna via yhemanth)\n+",
                "deletions": 0
            },
            {
                "sha": "3ec640566492e9af0e17a0d348b52d43e1617fe4",
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "status": "modified",
                "changes": 12,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "patch": "@@ -1001,8 +1001,12 @@ public Path getSystemDir() {\n   \n   public JobStatus[] getJobsFromQueue(String queueName) throws IOException {\n     try {\n+      QueueInfo queue = cluster.getQueue(queueName);\n+      if (queue == null) {\n+        return null;\n+      }\n       org.apache.hadoop.mapreduce.JobStatus[] stats = \n-        cluster.getQueue(queueName).getJobStatuses();\n+        queue.getJobStatuses();\n       JobStatus[] ret = new JobStatus[stats.length];\n       for (int i = 0 ; i < stats.length; i++ ) {\n         ret[i] = JobStatus.downgrade(stats[i]);\n@@ -1022,7 +1026,11 @@ public Path getSystemDir() {\n    */\n   public JobQueueInfo getQueueInfo(String queueName) throws IOException {\n     try {\n-      return new JobQueueInfo(cluster.getQueue(queueName));\n+      QueueInfo queueInfo = cluster.getQueue(queueName);\n+      if (queueInfo != null) {\n+        return new JobQueueInfo(queueInfo);\n+      }\n+      return null;\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }",
                "deletions": 2
            },
            {
                "sha": "129b175966e1fda1d229e1117f2ed5c64b8411ab",
                "filename": "src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "patch": "@@ -175,6 +175,11 @@ private void displayQueueList() throws IOException {\n   private void displayQueueInfo(String queue, boolean showJobs)\n       throws IOException {\n     JobQueueInfo jobQueueInfo = jc.getQueueInfo(queue);\n+    \n+    if (jobQueueInfo == null) {\n+      System.out.println(\"Queue \\\"\" + queue + \"\\\" does not exist.\");\n+      return;\n+    }\n     printJobQueueInfo(jobQueueInfo, new PrintWriter(System.out));\n     if (showJobs && (jobQueueInfo.getChildren() == null ||\n         jobQueueInfo.getChildren().size() == 0)) {",
                "deletions": 0
            },
            {
                "sha": "6686c97fb3dede682ea457b0a2b577795a5746d2",
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "patch": "@@ -3966,7 +3966,9 @@ public JobQueueInfo getQueueInfo(String queue) throws IOException {\n   @Override\n   public QueueInfo getQueue(String queue) throws IOException {\n     JobQueueInfo jqueue = queueManager.getJobQueueInfo(queue);\n-    jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    if (jqueue != null) {\n+      jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    }\n     return jqueue;\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "94735fe317e52351eb3e6335c25534af225b676b",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "status": "modified",
                "changes": 40,
                "additions": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "patch": "@@ -17,20 +17,33 @@\n  */\n package org.apache.hadoop.mapred;\n \n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.CONFIG;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.checkForConfigFile;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createDocument;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createSimpleDocumentWithAcls;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.miniMRCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.setUpCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.writeToFile;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n+import java.io.IOException;\n import java.io.StringWriter;\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.junit.After;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.QueueInfo;\n import org.junit.Test;\n+import org.w3c.dom.Document;\n \n public class TestJobQueueClient {\n   @Test\n   public void testQueueOrdering() throws Exception {\n-    System.out.println(\"in test queue ordering\");\n     // create some sample queues in a hierarchy..\n     JobQueueInfo[] roots = new JobQueueInfo[2];\n     roots[0] = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -53,7 +66,6 @@ public void testQueueOrdering() throws Exception {\n   \n   @Test\n   public void testQueueInfoPrinting() throws Exception {\n-    System.out.println(\"in test queue info printing\");\n     // create a test queue with children.\n     // create some sample queues in a hierarchy..\n     JobQueueInfo root = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -76,4 +88,24 @@ public void testQueueInfoPrinting() throws Exception {\n     \n     assertEquals(sb.toString(), writer.toString());\n   }\n-}\n\\ No newline at end of file\n+  \n+  @Test\n+  public void testGetQueue() throws Exception {\n+    checkForConfigFile();\n+    Document doc = createDocument();\n+    createSimpleDocumentWithAcls(doc, \"true\");\n+    writeToFile(doc, CONFIG);\n+    Configuration conf = new Configuration();\n+    conf.addResource(CONFIG);\n+    setUpCluster(conf);\n+    JobClient jc = new JobClient(miniMRCluster.createJobConf());\n+    // test for existing queue\n+    QueueInfo queueInfo = jc.getQueueInfo(\"q1\");\n+    assertEquals(\"q1\",queueInfo.getQueueName());\n+    // try getting a non-existing queue\n+    queueInfo = jc.getQueueInfo(\"queue\");\n+    assertNull(queueInfo);\n+\n+    new File(CONFIG).delete();\n+  }\n+}",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1007. Fix NPE in CapacityTaskScheduler.getJobs(). Contributed by V.V.Chaitanya Krishna.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@882470 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/978e6a360cdafc522eb5233895040e2d3f3a4b21",
        "bug_id": "hadoop-mapreduce_18",
        "file": [
            {
                "sha": "4f47403239cb191f7bc0b68b787dece264504bdc",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "patch": "@@ -893,3 +893,7 @@ Release 0.21.0 - Unreleased\n     cdouglas)\n \n     MAPREDUCE-915. The debug scripts are run as the job user. (ddas)\n+\n+    MAPREDUCE-1007. Fix NPE in CapacityTaskScheduler.getJobs(). \n+    (V.V.Chaitanya Krishna via sharad)\n+",
                "deletions": 0
            },
            {
                "sha": "04c6f2dd2d8a7ca507c05a09fe26e2d2a6b0c1ee",
                "filename": "src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "patch": "@@ -989,13 +989,17 @@ private synchronized void updateContextObjects(int mapClusterCapacity,\n   @Override\n   public synchronized Collection<JobInProgress> getJobs(String queueName) {\n     Collection<JobInProgress> jobCollection = new ArrayList<JobInProgress>();\n+    JobQueue jobQueue = jobQueuesManager.getJobQueue(queueName);\n+    if (jobQueue == null) {\n+      return jobCollection;\n+    }\n     Collection<JobInProgress> runningJobs =\n-      jobQueuesManager.getJobQueue(queueName).getRunningJobs();\n+      jobQueue.getRunningJobs();\n     if (runningJobs != null) {\n       jobCollection.addAll(runningJobs);\n     }\n     Collection<JobInProgress> waitingJobs = \n-      jobQueuesManager.getJobQueue(queueName).getWaitingJobs();\n+      jobQueue.getWaitingJobs();\n     Collection<JobInProgress> tempCollection = new ArrayList<JobInProgress>();\n     if(waitingJobs != null) {\n       tempCollection.addAll(waitingJobs);",
                "deletions": 2
            },
            {
                "sha": "c93616490d52d9a735dc58ab626bc32c228420f8",
                "filename": "src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "status": "modified",
                "changes": 96,
                "additions": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;\n import static org.apache.hadoop.mapred.CapacityTestUtils.*;\n \n+import java.io.File;\n import java.io.IOException;\n import java.util.*;\n \n@@ -36,6 +37,11 @@\n   static final Log LOG =\n     LogFactory.getLog(org.apache.hadoop.mapred.TestCapacityScheduler.class);\n \n+  String queueConfigPath =\n+    System.getProperty(\"test.build.extraconf\", \"build/test/extraconf\");\n+  File queueConfigFile =\n+    new File(queueConfigPath, QueueManager.QUEUE_CONF_FILE_NAME);\n+\n   private static int jobCounter;\n \n   private ControlledInitializationPoller controlledInitializationPoller;\n@@ -341,65 +347,55 @@ public void testJobFinished() throws Exception {\n     taskTrackerManager.finishTask(\"attempt_test_0002_m_000003_0\", j2);\n   }\n \n-  // basic tests, should be able to submit to queues\n-  public void testSubmitToQueues() throws Exception {\n-    // set up some queues\n-    String[] qs = {\"default\", \"q2\"};\n-    taskTrackerManager.addQueues(qs);\n-    ArrayList<FakeQueueInfo> queues = new ArrayList<FakeQueueInfo>();\n-    queues.add(new FakeQueueInfo(\"default\", 50.0f, true, 25));\n-    queues.add(new FakeQueueInfo(\"q2\", 50.0f, true, 25));\n-\n-\n-    taskTrackerManager.setFakeQueues(queues);\n+  /**\n+   * tests the submission of jobs to container and job queues\n+   * @throws Exception\n+   */\n+  public void testJobSubmission() throws Exception {\n+    JobQueueInfo[] queues = TestQueueManagerRefresh.getSimpleQueueHierarchy();\n+\n+    queues[0].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(100));\n+    queues[1].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(50));\n+    queues[2].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(50));\n+\n+    // write the configuration file\n+    QueueManagerTestUtils.writeQueueConfigurationFile(\n+        queueConfigFile.getAbsolutePath(), new JobQueueInfo[] { queues[0] });\n+    setUp(1, 4, 4);\n+    // use the queues from the config file.\n+    taskTrackerManager.setQueueManager(new QueueManager());\n     scheduler.start();\n \n-    // submit a job with no queue specified. It should be accepted\n-    // and given to the default queue. \n-    JobInProgress j = taskTrackerManager.submitJobAndInit(JobStatus.PREP, \n-                                                    10, 10, null, \"u1\");\n-    // when we ask for tasks, we should get them for the job submitted\n-    Map<String, String> expectedTaskStrings = new HashMap<String, String>();\n-    expectedTaskStrings.put(CapacityTestUtils.MAP, \n-                            \"attempt_test_0001_m_000001_0 on tt1\");\n-    expectedTaskStrings.put(CapacityTestUtils.REDUCE, \n-                            \"attempt_test_0001_r_000001_0 on tt1\");\n-    checkMultipleTaskAssignment(taskTrackerManager, scheduler, \n-                                      \"tt1\", expectedTaskStrings);\n-\n-    // submit another job, to a different queue\n-    j = taskTrackerManager.submitJobAndInit(JobStatus.PREP, 10, 10, \"q2\", \"u1\");\n-    // now when we get tasks, it should be from the second job\n-    expectedTaskStrings.clear();\n-    expectedTaskStrings.put(CapacityTestUtils.MAP,\n-                              \"attempt_test_0002_m_000001_0 on tt2\");\n-    expectedTaskStrings.put(CapacityTestUtils.REDUCE,\n-                              \"attempt_test_0002_r_000001_0 on tt2\");\n-    checkMultipleTaskAssignment(taskTrackerManager, scheduler, \n-                                  \"tt2\", expectedTaskStrings);\n-  }\n+    // submit a job to the container queue\n+    try {\n+      taskTrackerManager.submitJobAndInit(JobStatus.PREP, 20, 0,\n+          queues[0].getQueueName(), \"user\");\n+      fail(\"Jobs are being able to be submitted to the container queue\");\n+    } catch (Exception e) {\n+      assertTrue(scheduler.getJobs(queues[0].getQueueName()).isEmpty());\n+    }\n \n-  public void testGetJobs() throws Exception {\n-    // need only one queue\n-    String[] qs = {\"default\"};\n-    taskTrackerManager.addQueues(qs);\n-    ArrayList<FakeQueueInfo> queues = new ArrayList<FakeQueueInfo>();\n-    queues.add(new FakeQueueInfo(\"default\", 100.0f, true, 100));\n+    FakeJobInProgress job = taskTrackerManager.submitJobAndInit(JobStatus.PREP,\n+        1, 0, queues[1].getQueueName(), \"user\");\n+    assertEquals(1, scheduler.getJobs(queues[1].getQueueName()).size());\n+    assertTrue(scheduler.getJobs(queues[1].getQueueName()).contains(job));\n \n+    // check if the job is submitted\n+    checkAssignment(taskTrackerManager, scheduler, \"tt1\", \n+    \"attempt_test_0002_m_000001_0 on tt1\");\n \n-    taskTrackerManager.setFakeQueues(queues);\n-    scheduler.start();\n+    // test for getJobs\n     HashMap<String, ArrayList<FakeJobInProgress>> subJobsList =\n-      taskTrackerManager.submitJobs(1, 4, \"default\");\n+      taskTrackerManager.submitJobs(1, 4, queues[2].getQueueName());\n \n     JobQueuesManager mgr = scheduler.jobQueuesManager;\n-\n-    while (mgr.getJobQueue(\"default\").getWaitingJobs().size() < 4) {\n-      Thread.sleep(1);\n-    }\n     //Raise status change events for jobs submitted.\n-    raiseStatusChangeEvents(mgr);\n-    Collection<JobInProgress> jobs = scheduler.getJobs(\"default\");\n+    raiseStatusChangeEvents(mgr, queues[2].getQueueName());\n+    Collection<JobInProgress> jobs =\n+      scheduler.getJobs(queues[2].getQueueName());\n \n     assertTrue(\n       \"Number of jobs returned by scheduler is wrong\"",
                "deletions": 50
            },
            {
                "sha": "fe271dea3d208e5e0c054ac6db07092ea25767fb",
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "patch": "@@ -3962,7 +3962,10 @@ public QueueInfo getQueue(String queue) throws IOException {\n \n   public org.apache.hadoop.mapreduce.JobStatus[] getJobsFromQueue(String queue) \n       throws IOException {\n-    Collection<JobInProgress> jips = taskScheduler.getJobs(queue);\n+    Collection<JobInProgress> jips = null;\n+    if (queueManager.getLeafQueueNames().contains(queue)) {\n+      jips = taskScheduler.getJobs(queue);\n+    }\n     return getJobStatus(jips,false);\n   }\n   ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are  \nscheduled but not running. Contributed by Todd Lipcon.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@830821 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/a98327008f5b124724f3577bde06f2b4ddec6a18",
        "bug_id": "hadoop-mapreduce_19",
        "file": [
            {
                "sha": "0571389832cb58552b200f56fa0479f7524b073d",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "patch": "@@ -37,6 +37,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are\n+    scheduled but not running. (Todd Lipcon via matei)\n+\n     MAPREDUCE-1014. Fix the libraries for common and hdfs. (omalley)\n \n     MAPREDUCE-1111. JT Jetty UI not working if we run mumak.sh ",
                "deletions": 0
            },
            {
                "sha": "ae0930c46581cef366f6fcd59d3ca7408b5383e3",
                "filename": "src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "patch": "@@ -831,7 +831,11 @@ protected int tasksToPreempt(PoolSchedulable sched, long curTime) {\n     List<TaskStatus> statuses = new ArrayList<TaskStatus>();\n     for (TaskInProgress tip: tips) {\n       for (TaskAttemptID id: tip.getActiveTasks().keySet()) {\n-        statuses.add(tip.getTaskStatus(id));\n+        TaskStatus stat = tip.getTaskStatus(id);\n+        // status is null when the task has been scheduled but not yet running\n+        if (stat != null) {\n+          statuses.add(stat);\n+        }\n       }\n     }\n     return statuses;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\nis closed cause a NPE. Contributed by Jothi Padmanabhan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@816454 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/c82125320349962f05edc66d819c00db2681db97",
        "bug_id": "hadoop-mapreduce_20",
        "file": [
            {
                "sha": "2464a02711e43999d7b52749b7bc0dfc84e75831",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "patch": "@@ -629,3 +629,6 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-971. distcp does not always remove distcp.tmp.dir. (Aaron Kimball\n     via tomwhite)\n+\n+    MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\n+    is closed cause a NPE. (Jothi Padmanabhan via cdouglas)",
                "deletions": 0
            },
            {
                "sha": "77644667b2c2acaa1e2fc6bf9f25fd566d132a65",
                "filename": "src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "status": "modified",
                "changes": 53,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "patch": "@@ -21,7 +21,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Date;\n+import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n import java.util.List;\n@@ -55,7 +55,8 @@\n   final Log LOG = LogFactory.getLog(JobHistory.class);\n \n   private long jobHistoryBlockSize;\n-  private Map<JobID, MetaInfo> fileMap;\n+  private final Map<JobID, MetaInfo> fileMap =\n+    Collections.<JobID,MetaInfo>synchronizedMap(new HashMap<JobID,MetaInfo>());\n   private ThreadPoolExecutor executor = null;\n   static final FsPermission HISTORY_DIR_PERMISSION =\n     FsPermission.createImmutable((short) 0750); // rwxr-x---\n@@ -115,8 +116,6 @@ public void init(JobTracker jt, JobConf conf, String hostname,\n           3 * 1024 * 1024);\n     \n     jobTracker = jt;\n-    \n-    fileMap = new HashMap<JobID, MetaInfo> ();\n   }\n   \n   /** Initialize the done directory and start the history cleaner thread */\n@@ -305,40 +304,28 @@ public void setupEventWriter(JobID jobId, JobConf jobConf)\n   /** Close the event writer for this id */\n   public void closeWriter(JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.close();\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.closeWriter();\n+      }\n     } catch (IOException e) {\n       LOG.info(\"Error closing writer for JobID: \" + id);\n     }\n   }\n \n-\n-  /**\n-   * Get the JsonEventWriter for the specified Job Id\n-   * @param jobId\n-   * @return\n-   * @throws IOException if a writer is not available\n-   */\n-  private EventWriter getWriter(final JobID jobId) throws IOException {\n-    EventWriter writer = null;\n-    MetaInfo mi = fileMap.get(jobId);\n-    if (mi == null || (writer = mi.getEventWriter()) == null) {\n-      throw new IOException(\"History File does not exist for JobID\");\n-    }\n-    return writer;\n-  }\n-\n   /**\n    * Method to log the specified event\n    * @param event The event to log\n    * @param id The Job ID of the event\n    */\n   public void logEvent(HistoryEvent event, JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.write(event);\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.writeEvent(event);\n+      }\n     } catch (IOException e) {\n-      LOG.error(\"Error creating writer, \" + e.getMessage());\n+      LOG.error(\"Error Logging event, \" + e.getMessage());\n     }\n   }\n \n@@ -388,7 +375,7 @@ private void moveOldFiles() throws IOException {\n   \n   private void moveToDone(final JobID id) {\n     final List<Path> paths = new ArrayList<Path>();\n-    MetaInfo metaInfo = fileMap.get(id);\n+    final MetaInfo metaInfo = fileMap.get(id);\n     if (metaInfo == null) {\n       LOG.info(\"No file for job-history with \" + id + \" found in cache!\");\n       return;\n@@ -456,7 +443,19 @@ private String getUserName(JobConf jobConf) {\n \n     Path getHistoryFile() { return historyFile; }\n     Path getConfFile() { return confFile; }\n-    EventWriter getEventWriter() { return writer; }\n+\n+    synchronized void closeWriter() throws IOException {\n+      if (writer != null) {\n+        writer.close();\n+      }\n+      writer = null;\n+    }\n+\n+    synchronized void writeEvent(HistoryEvent event) throws IOException {\n+      if (writer != null) {\n+        writer.write(event);\n+      }\n+    }\n   }\n \n   /**",
                "deletions": 27
            },
            {
                "sha": "5dc445ec0633fe0f9529f9376fb55ef007a9afcc",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "status": "added",
                "changes": 116,
                "additions": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "patch": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapred;\n+\n+import java.io.IOException;\n+\n+import junit.framework.TestCase;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapreduce.Counters;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistory;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser;\n+import org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.TaskFinishedEvent;\n+\n+/**\n+ * Unit test to test if the JobHistory writer/parser is able to handle\n+ * values with special characters\n+ * This test also tests if the job history module is able to gracefully\n+ * ignore events after the event writer is closed\n+ *\n+ */\n+public class TestJobHistoryParsing  extends TestCase {\n+\n+  public void testHistoryParsing() throws IOException {\n+    // open a test history file\n+    Path historyDir = new Path(System.getProperty(\"test.build.data\", \".\"),\n+                                \"history\");\n+    JobConf conf = new JobConf();\n+    conf.set(\"hadoop.job.history.location\", historyDir.toString());\n+    FileSystem fs = FileSystem.getLocal(new JobConf());\n+\n+    // Some weird strings\n+    String username = \"user\";\n+    String weirdJob = \"Value has \\n new line \\n and \" +\n+                    \"dot followed by new line .\\n in it +\" +\n+                    \"ends with escape\\\\\";\n+    String weirdPath = \"Value has characters: \" +\n+                    \"`1234567890-=qwertyuiop[]\\\\asdfghjkl;'zxcvbnm,./\" +\n+                    \"~!@#$%^&*()_+QWERTYUIOP{}|ASDFGHJKL:\\\"'ZXCVBNM<>?\" +\n+                    \"\\t\\b\\n\\f\\\"\\n in it\";\n+\n+    conf.setUser(username);\n+\n+    MiniMRCluster mr = null;\n+    mr = new MiniMRCluster(2, \"file:///\", 3, null, null, conf);\n+\n+    JobTracker jt = mr.getJobTrackerRunner().getJobTracker();\n+    JobHistory jh = jt.getJobHistory();\n+\n+    jh.init(jt, conf, \"localhost\", 1234);\n+    JobID jobId = JobID.forName(\"job_200809171136_0001\");\n+    jh.setupEventWriter(jobId, conf);\n+    JobSubmittedEvent jse =\n+      new JobSubmittedEvent(jobId, weirdJob, username, 12345, weirdPath);\n+    jh.logEvent(jse, jobId);\n+\n+    JobFinishedEvent jfe =\n+      new JobFinishedEvent(jobId, 12346, 1, 1, 0, 0, new Counters());\n+    jh.logEvent(jfe, jobId);\n+    jh.closeWriter(jobId);\n+\n+    // Try to write one more event now, should not fail\n+    TaskID tid = TaskID.forName(\"task_200809171136_0001_m_000002\");\n+    TaskFinishedEvent tfe =\n+      new TaskFinishedEvent(tid, 0, TaskType.MAP, \"\", null);\n+    boolean caughtException = false;\n+\n+    try {\n+      jh.logEvent(tfe, jobId);\n+    } catch (Exception e) {\n+      caughtException = true;\n+    }\n+\n+    assertFalse(\"Writing an event after closing event writer is not handled\",\n+        caughtException);\n+\n+    String historyFileName = jobId.toString() + \"_\" + username;\n+    Path historyFilePath = new Path (historyDir.toString(),\n+      historyFileName);\n+\n+    System.out.println(\"History File is \" + historyFilePath.toString());\n+\n+    JobHistoryParser parser =\n+      new JobHistoryParser(fs, historyFilePath);\n+\n+    JobHistoryParser.JobInfo jobInfo = parser.parse();\n+\n+    assertTrue (jobInfo.getUsername().equals(username));\n+    assertTrue(jobInfo.getJobname().equals(weirdJob));\n+    assertTrue(jobInfo.getJobConfPath().equals(weirdPath));\n+\n+    if (mr != null) {\n+      mr.shutdown();\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-968. NPE in distcp encountered when placing _logs directory on S3FileSystem. Contributed by Aaron Kimball.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@814713 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/75ad1dc5aaf9e120a0d79a938c69a771babeb36a",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/942b34cc27d7a93f7f4c37760c6f96fe8bce460a",
        "bug_id": "hadoop-mapreduce_21",
        "file": [
            {
                "sha": "fc28e8a07ee36789459db773fead1912ee9922cb",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/75ad1dc5aaf9e120a0d79a938c69a771babeb36a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/75ad1dc5aaf9e120a0d79a938c69a771babeb36a/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=75ad1dc5aaf9e120a0d79a938c69a771babeb36a",
                "patch": "@@ -582,3 +582,5 @@ Trunk (unreleased changes)\n     HADOOP-6243. Fixes a NullPointerException in handling deprecated keys.\n     (Sreekanth Ramakrishnan via yhemanth)\n \n+    MAPREDUCE-968. NPE in distcp encountered when placing _logs directory on\n+    S3FileSystem. (Aaron Kimball via tomwhite)",
                "deletions": 0
            },
            {
                "sha": "e630dc114219f21038439c0b3cc9a17fb10b9005",
                "filename": "src/tools/org/apache/hadoop/tools/DistCp.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/75ad1dc5aaf9e120a0d79a938c69a771babeb36a/src/tools/org/apache/hadoop/tools/DistCp.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/75ad1dc5aaf9e120a0d79a938c69a771babeb36a/src/tools/org/apache/hadoop/tools/DistCp.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/tools/org/apache/hadoop/tools/DistCp.java?ref=75ad1dc5aaf9e120a0d79a938c69a771babeb36a",
                "patch": "@@ -1062,6 +1062,12 @@ private static boolean setup(Configuration conf, JobConf jobConf,\n       String filename = \"_distcp_logs_\" + randomId;\n       if (!dstExists || !dstIsDir) {\n         Path parent = args.dst.getParent();\n+        if (null == parent) {\n+          // If dst is '/' on S3, it might not exist yet, but dst.getParent()\n+          // will return null. In this case, use '/' as its own parent to prevent\n+          // NPE errors below.\n+          parent = args.dst;\n+        }\n         if (!dstfs.exists(parent)) {\n           dstfs.mkdirs(parent);\n         }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-658. Replace NPE in distcp with a meaningful error message when\nthe source path does not exist. Contributed by Ravi Gummadi\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@789237 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/10db66a425d2584382e721896e1f9ad0f5ca6876",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/e0dad7f96ec78f0ea36ef8b02561e973af9766cf",
        "bug_id": "hadoop-mapreduce_22",
        "file": [
            {
                "sha": "bd3061387a2739741f381c514524569396508158",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/10db66a425d2584382e721896e1f9ad0f5ca6876/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/10db66a425d2584382e721896e1f9ad0f5ca6876/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=10db66a425d2584382e721896e1f9ad0f5ca6876",
                "patch": "@@ -55,3 +55,6 @@ Trunk (unreleased changes)\n     (Amar Kamat via sharad)\n \n     MAPREDUCE-179. Update progress in new RecordReaders. (cdouglas)\n+\n+    MAPREDUCE-658. Replace NPE in distcp with a meaningful error message when\n+    the source path does not exist. (Ravi Gummadi via cdouglas)",
                "deletions": 0
            },
            {
                "sha": "0c104a38f88f739e439c4bcc3ea5813599e21829",
                "filename": "src/tools/org/apache/hadoop/tools/DistCp.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/10db66a425d2584382e721896e1f9ad0f5ca6876/src/tools/org/apache/hadoop/tools/DistCp.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/10db66a425d2584382e721896e1f9ad0f5ca6876/src/tools/org/apache/hadoop/tools/DistCp.java",
                "status": "modified",
                "changes": 10,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/tools/org/apache/hadoop/tools/DistCp.java?ref=10db66a425d2584382e721896e1f9ad0f5ca6876",
                "patch": "@@ -545,6 +545,14 @@ public void map(LongWritable key,\n                           StringUtils.stringifyException(e);\n         out.collect(null, new Text(sfailure));\n         LOG.info(sfailure);\n+        if (e instanceof FileNotFoundException) {\n+          final String s = \"Possible Cause for failure: Either the filesystem \"\n+                           + srcstat.getPath().getFileSystem(job)\n+                           + \" is not accessible or the file is deleted\";\n+          LOG.error(s);\n+          out.collect(null, new Text(s));\n+        }\n+\n         try {\n           for (int i = 0; i < 3; ++i) {\n             try {\n@@ -623,7 +631,7 @@ private static void checkSrcPath(Configuration conf, List<Path> srcPaths\n       FileSystem fs = p.getFileSystem(conf);\n       FileStatus[] inputs = fs.globStatus(p);\n       \n-      if(inputs.length > 0) {\n+      if(inputs != null && inputs.length > 0) {\n         for (FileStatus onePath: inputs) {\n           unglobbed.add(onePath.getPath());\n         }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hadoop-mapreduce",
        "message": "MAPREDUCE-913. TaskRunner crashes with NPE resulting in held up slots, UNINITIALIZED tasks and hung TaskTracker. Contributed by Amareshwari Sriramadasu and Sreekanth Ramakrishnan.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@950485 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-mapreduce/commit/661f09463aa06f2e3b808fbc360d35f114b95a44",
        "parent": "https://github.com/apache/hadoop-mapreduce/commit/ac557915ad5905a424acc9f166bd154fbf3acb5e",
        "bug_id": "hadoop-mapreduce_23",
        "file": [
            {
                "sha": "3f651ddeb64410b64f79c35f384806689996f768",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/CHANGES.txt?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "patch": "@@ -1638,3 +1638,7 @@ Release 0.21.0 - Unreleased\n     (Dick King and Amareshwari Sriramadasu via tomwhite)\n \n     MAPREDUCE-118. Fix Job.getJobID(). (Amareshwari Sriramadasu via sharad)\n+\n+    MAPREDUCE-913. TaskRunner crashes with NPE resulting in held up slots,\n+    UNINITIALIZED tasks and hung TaskTracker. (Amareshwari Sriramadasu and\n+    Sreekanth Ramakrishnan via vinodkv)",
                "deletions": 0
            },
            {
                "sha": "c3d2bdc4afbf88cdc416ffddabd365cb6adf2dbd",
                "filename": "src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "status": "modified",
                "changes": 89,
                "additions": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/java/org/apache/hadoop/mapred/TaskTracker.java?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "patch": "@@ -2093,7 +2093,16 @@ private void addToTaskQueue(LaunchTaskAction action) {\n       reduceLauncher.addToTaskQueue(action);\n     }\n   }\n-  \n+\n+  // This method is called from unit tests\n+  int getFreeSlots(boolean isMap) {\n+    if (isMap) {\n+      return mapLauncher.numFreeSlots.get();\n+    } else {\n+      return reduceLauncher.numFreeSlots.get();\n+    }\n+  }\n+\n   class TaskLauncher extends Thread {\n     private IntWritable numFreeSlots;\n     private final int maxSlots;\n@@ -2657,8 +2666,11 @@ public boolean wasKilled() {\n      */\n     void reportTaskFinished(boolean commitPending) {\n       if (!commitPending) {\n-        taskFinished();\n-        releaseSlot();\n+        try {\n+          taskFinished(); \n+        } finally {\n+          releaseSlot();\n+        }\n       }\n       notifyTTAboutTaskCompletion();\n     }\n@@ -2728,7 +2740,15 @@ public void taskFinished() {\n             setTaskFailState(true);\n             // call the script here for the failed tasks.\n             if (debugCommand != null) {\n-              runDebugScript();\n+              try {\n+                runDebugScript();\n+              } catch (Exception e) {\n+                String msg =\n+                    \"Debug-script could not be run successfully : \"\n+                        + StringUtils.stringifyException(e);\n+                LOG.warn(msg);\n+                reportDiagnosticInfo(msg);\n+              }\n             }\n           }\n           taskStatus.setProgress(0.0f);\n@@ -2749,14 +2769,17 @@ public void taskFinished() {\n       if (needCleanup) {\n         removeTaskFromJob(task.getJobID(), this);\n       }\n-      try {\n-        cleanup(needCleanup);\n-      } catch (IOException ie) {\n-      }\n \n+      cleanup(needCleanup);\n     }\n-    \n-    private void runDebugScript() {\n+\n+    /**\n+     * Run the debug-script now. Because debug-script can be user code, we use\n+     * {@link TaskController} to execute the debug script.\n+     * \n+     * @throws IOException\n+     */\n+    private void runDebugScript() throws IOException {\n       String taskStdout =\"\";\n       String taskStderr =\"\";\n       String taskSyslog =\"\";\n@@ -2774,23 +2797,14 @@ private void runDebugScript() {\n         taskSyslog = FileUtil\n             .makeShellPath(TaskLog.getRealTaskLogFileLocation(task.getTaskID(),\n                 task.isTaskCleanupTask(), TaskLog.LogName.SYSLOG));\n-      } catch(IOException e){\n-        LOG.warn(\"Exception finding task's stdout/err/syslog files\");\n-      }\n-      File workDir = null;\n-      try {\n-        workDir =\n-            new File(lDirAlloc.getLocalPathToRead(\n-                TaskTracker.getLocalTaskDir(task.getUser(), task\n-                    .getJobID().toString(), task.getTaskID()\n-                    .toString(), task.isTaskCleanupTask())\n-                    + Path.SEPARATOR + MRConstants.WORKDIR,\n-                localJobConf).toString());\n-      } catch (IOException e) {\n-        LOG.warn(\"Working Directory of the task \" + task.getTaskID() +\n-                        \" doesnt exist. Caught exception \" +\n-                  StringUtils.stringifyException(e));\n-      }\n+      } catch(Exception e){\n+        LOG.warn(\"Exception finding task's stdout/err/syslog files\", e);\n+      }\n+      File workDir = new File(lDirAlloc.getLocalPathToRead(\n+          TaskTracker.getLocalTaskDir(task.getUser(), task.getJobID()\n+              .toString(), task.getTaskID().toString(), task\n+              .isTaskCleanupTask())\n+              + Path.SEPARATOR + MRConstants.WORKDIR, localJobConf).toString());\n       // Build the command  \n       File stdout = TaskLog.getTaskLogFile(task.getTaskID(), task\n           .isTaskCleanupTask(), TaskLog.LogName.DEBUGOUT);\n@@ -2820,21 +2834,10 @@ private void runDebugScript() {\n       context.stdout = stdout;\n       context.workDir = workDir;\n       context.task = task;\n-      try {\n-        getTaskController().runDebugScript(context);\n-        // add all lines of debug out to diagnostics\n-        try {\n-          int num = localJobConf.getInt(MRJobConfig.TASK_DEBUGOUT_LINES,\n-              -1);\n-          addDiagnostics(FileUtil.makeShellPath(stdout),num,\n-              \"DEBUG OUT\");\n-        } catch(IOException ioe) {\n-          LOG.warn(\"Exception in add diagnostics!\");\n-        }\n-      } catch (IOException ie) {\n-        LOG.warn(\"runDebugScript failed with: \" + StringUtils.\n-                                              stringifyException(ie));\n-      }\n+      getTaskController().runDebugScript(context);\n+      // add the lines of debug out to diagnostics\n+      int num = localJobConf.getInt(MRJobConfig.TASK_DEBUGOUT_LINES, -1);\n+      addDiagnostics(FileUtil.makeShellPath(stdout), num, \"DEBUG OUT\");\n     }\n \n     /**\n@@ -2998,7 +3001,7 @@ private synchronized void mapOutputLost(String failure\n      * otherwise the current working directory of the task \n      * i.e. &lt;taskid&gt;/work is cleaned up.\n      */\n-    void cleanup(boolean needCleanup) throws IOException {\n+    void cleanup(boolean needCleanup) {\n       TaskAttemptID taskId = task.getTaskID();\n       LOG.debug(\"Cleaning up \" + taskId);\n ",
                "deletions": 43
            },
            {
                "sha": "3bfbc681844c8af1caea38518ddd38a6981f6c84",
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "blob_url": "https://github.com/apache/hadoop-mapreduce/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "raw_url": "https://github.com/apache/hadoop-mapreduce/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "status": "added",
                "changes": 115,
                "additions": 115,
                "contents_url": "https://api.github.com/repos/apache/hadoop-mapreduce/contents/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "patch": "@@ -0,0 +1,115 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapred;\n+\n+import java.io.File;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.ClusterMetrics;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Regression test for MAPREDUCE-913\n+ */\n+public class TestTaskTrackerSlotManagement {\n+\n+  private static final Path TEST_DIR = new Path(System.getProperty(\n+      \"test.build.data\", \"/tmp\"), \"tt_slots\");\n+  private static final String CACHE_FILE_PATH = new Path(TEST_DIR, \"test.txt\")\n+      .toString();\n+\n+  /**\n+   * Test-setup. Create the cache-file.\n+   * \n+   * @throws Exception\n+   */\n+  @Before\n+  public void setUp() throws Exception {\n+    new File(TEST_DIR.toString()).mkdirs();\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.createNewFile();\n+  }\n+\n+  /**\n+   * Test-cleanup. Remove the cache-file.\n+   * \n+   * @throws Exception\n+   */\n+  @After\n+  public void tearDown() throws Exception {\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.delete();\n+    new File(TEST_DIR.toString()).delete();\n+  }\n+\n+  /**\n+   * Test case to test addition of free slot when the job fails localization due\n+   * to cache file being modified after the job has started running.\n+   * \n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFreeingOfTaskSlots() throws Exception {\n+    // Start a cluster with no task tracker.\n+    MiniMRCluster mrCluster = new MiniMRCluster(0, \"file:///\", 1);\n+    Configuration conf = mrCluster.createJobConf();\n+    Cluster cluster = new Cluster(conf);\n+    // set the debug script so that TT tries to launch the debug\n+    // script for failed tasks.\n+    conf.set(JobContext.MAP_DEBUG_SCRIPT, \"/bin/echo\");\n+    conf.set(JobContext.REDUCE_DEBUG_SCRIPT, \"/bin/echo\");\n+    Job j = MapReduceTestUtil.createJob(conf, new Path(TEST_DIR, \"in\"),\n+        new Path(TEST_DIR, \"out\"), 0, 0);\n+    // Add the local filed created to the cache files of the job\n+    j.addCacheFile(new URI(CACHE_FILE_PATH));\n+    j.setMaxMapAttempts(1);\n+    j.setMaxReduceAttempts(1);\n+    // Submit the job and return immediately.\n+    // Job submit now takes care setting the last\n+    // modified time of the cache file.\n+    j.submit();\n+    // Look up the file and modify the modification time.\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.setLastModified(0L);\n+    // Start up the task tracker after the time has been changed.\n+    mrCluster.startTaskTracker(null, null, 0, 1);\n+    // Now wait for the job to fail.\n+    j.waitForCompletion(false);\n+    Assert.assertFalse(\"Job successfully completed.\", j.isSuccessful());\n+\n+    ClusterMetrics metrics = cluster.getClusterStatus();\n+    // validate number of slots in JobTracker\n+    Assert.assertEquals(0, metrics.getOccupiedMapSlots());\n+    Assert.assertEquals(0, metrics.getOccupiedReduceSlots());\n+\n+    // validate number of slots in TaskTracker\n+    TaskTracker tt = mrCluster.getTaskTrackerRunner(0).getTaskTracker();\n+    Assert.assertEquals(metrics.getMapSlotCapacity(), tt.getFreeSlots(true));\n+    Assert.assertEquals(metrics.getReduceSlotCapacity(), tt.getFreeSlots(false));\n+\n+  }\n+}",
                "deletions": 0
            }
        ]
    }
]