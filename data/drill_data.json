[
    {
        "commit": "https://github.com/apache/drill/commit/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
        "file": [
            {
                "patch": "@@ -406,8 +406,19 @@ private void addDataToTable(Packet packet, int networkType, RowSetLoader rowWrit\n     srcMacAddressWriter.setString(packet.getEthernetSource());\n     dstMacAddressWriter.setString(packet.getEthernetDestination());\n \n-    dstIPWriter.setString(packet.getDst_ip().getHostAddress());\n-    srcIPWriter.setString(packet.getSrc_ip().getHostAddress());\n+    String destinationIp = packet.getDestinationIpAddressString();\n+    if (destinationIp == null) {\n+      dstIPWriter.setNull();\n+    } else {\n+      dstIPWriter.setString(destinationIp);\n+    }\n+\n+    String sourceIp = packet.getSourceIpAddressString();\n+    if (sourceIp == null) {\n+      srcIPWriter.setNull();\n+    } else {\n+      srcIPWriter.setString(sourceIp);\n+    }\n     srcPortWriter.setInt(packet.getSrc_port());\n     dstPortWriter.setInt(packet.getDst_port());\n     packetLengthWriter.setInt(packet.getPacketLength());",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "status": "modified",
                "changes": 15,
                "deletions": 2,
                "sha": "4aeff2f359d8c846918052c4c691ad0480ed2b15",
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8"
            },
            {
                "patch": "@@ -183,6 +183,24 @@ public InetAddress getDst_ip() {\n     return getIPAddress(false);\n   }\n \n+  public String getSourceIpAddressString() {\n+    InetAddress address = getSrc_ip();\n+    if (address == null) {\n+      return null;\n+    } else {\n+      return address.getHostAddress();\n+    }\n+  }\n+\n+  public String getDestinationIpAddressString() {\n+    InetAddress address = getDst_ip();\n+    if (address == null) {\n+      return null;\n+    } else {\n+      return address.getHostAddress();\n+    }\n+  }\n+\n   public String getEthernetSource() {\n     return getEthernetAddress(PacketConstants.ETHER_SRC_OFFSET);\n   }",
                "additions": 18,
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "status": "modified",
                "changes": 18,
                "deletions": 0,
                "sha": "148f94fa6c5ca1c4236a7af47f014685003ae9a1",
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8"
            },
            {
                "patch": "@@ -99,4 +99,20 @@ public void testAggregateQuery() throws Exception {\n       .baselineValues(true, 16L)\n       .go();\n   }\n-}\n\\ No newline at end of file\n+\n+  @Test\n+  public void testArpPcapFile() throws Exception {\n+    String sql = \"SELECT src_ip, dst_ip FROM cp.`store/pcap/arpWithNullIP.pcap` WHERE src_port=1\";\n+    testBuilder()\n+      .sqlQuery(sql)\n+      .ordered()\n+      .baselineColumns(\"src_ip\", \"dst_ip\")\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .go();\n+  }\n+}",
                "additions": 17,
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "status": "modified",
                "changes": 18,
                "deletions": 1,
                "sha": "3b878c3b8141e951d019f336303b5164c84e65e0",
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "status": "added",
                "changes": 0,
                "deletions": 0,
                "sha": "1485811381a24acdded5f35af97b215e904075ac",
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "filename": "exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8"
            }
        ],
        "bug_id": "drill_1",
        "parent": "https://github.com/apache/drill/commit/5339fc23eb1b177bbc20ed74637e6b11e3ffa803",
        "message": "DRILL-7485: NPE on PCAP Batch Reader\n\ncloses #1932",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/e477480e7b9626abc8efd70914d3bfd4321b7258",
        "file": [
            {
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.drill.exec.record.MaterializedField;\n import org.apache.drill.exec.record.RecordBatch;\n import org.apache.drill.exec.record.TypedFieldId;\n+import org.apache.drill.exec.server.options.OptionManager;\n import org.apache.drill.exec.store.ColumnExplorer;\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.drill.exec.vector.complex.FieldIdUtil;\n@@ -90,8 +91,9 @@ public StatisticsAggBatch(StatisticsAggregate popConfig, RecordBatch incoming,\n   /*\n    * Returns whether the given column is an implicit column\n    */\n-  private boolean isImplicitFileColumn(MaterializedField mf) {\n-    return implicitFileColumnsMap.get(SchemaPath.getSimplePath(mf.getName()).toString()) != null;\n+  private boolean isImplicitFileOrPartitionColumn(MaterializedField mf, OptionManager optionManager) {\n+    return implicitFileColumnsMap.get(SchemaPath.getSimplePath(mf.getName()).toString()) != null ||\n+       ColumnExplorer.isPartitionColumn(optionManager, SchemaPath.getSimplePath(mf.getName()));\n   }\n \n   /*\n@@ -191,7 +193,7 @@ protected StreamingAggregator createAggregatorInternal()\n           expr = ValueExpressions.getChar(DrillStatsTable.getMapper().writeValueAsString(mf.getType()), 0);\n         }\n         // Ignore implicit columns\n-        if (!isImplicitFileColumn(mf)) {\n+        if (!isImplicitFileOrPartitionColumn(mf, incoming.getContext().getOptions())) {\n           createNestedKeyColumn(\n               parent,\n               SchemaPath.getSimplePath(mf.getName()).toString(),\n@@ -213,7 +215,7 @@ protected StreamingAggregator createAggregatorInternal()\n       for (MaterializedField mf : incoming.getSchema()) {\n         // Check stats collection is only being done for supported data-types. Complex types\n         // such as MAP, LIST are not supported!\n-        if (isColMinorTypeValid(mf) && !isImplicitFileColumn(mf)) {\n+        if (isColMinorTypeValid(mf) && !isImplicitFileOrPartitionColumn(mf, incoming.getContext().getOptions())) {\n           List<LogicalExpression> args = Lists.newArrayList();\n           args.add(SchemaPath.getSimplePath(mf.getName()));\n           LogicalExpression call = FunctionCallFactory.createExpression(func, args);",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "status": "modified",
                "changes": 10,
                "deletions": 4,
                "sha": "40fc445ec6dc16846ce4b490187747353e6bad17",
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258"
            },
            {
                "patch": "@@ -26,14 +26,14 @@\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.module.SimpleModule;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import org.apache.calcite.rel.RelNode;\n import org.apache.calcite.rel.RelVisitor;\n import org.apache.calcite.rel.core.TableScan;\n-import org.apache.drill.common.exceptions.DrillRuntimeException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.types.TypeProtos;\n@@ -196,7 +196,7 @@ public Histogram getHistogram(String col) {\n    * @param context - Query context\n    * @throws Exception\n    */\n-  public void materialize(final DrillTable table, final QueryContext context) throws IOException {\n+  public void materialize(final DrillTable table, final QueryContext context) {\n     if (materialized) {\n       return;\n     }\n@@ -223,8 +223,11 @@ public void materialize(final DrillTable table, final QueryContext context) thro\n         materialized = true;\n       }\n     } catch (IOException ex) {\n-      logger.warn(\"Failed to read the stats file.\", ex);\n-      throw ex;\n+      if (ex instanceof FileNotFoundException) {\n+        logger.debug(String.format(\"Did not find statistics file %s\", tablePath.toString()), ex);\n+      } else {\n+        logger.debug(String.format(\"Error trying to read statistics table %s\", tablePath.toString()), ex);\n+      }\n     }\n   }\n \n@@ -247,16 +250,15 @@ public void visit(RelNode node, int ordinal, RelNode parent) {\n       if (node instanceof TableScan) {\n         try {\n           final DrillTable drillTable = Utilities.getDrillTable(node.getTable());\n-          final DrillStatsTable statsTable = drillTable.getStatsTable();\n-          if (statsTable != null) {\n+          final DrillStatsTable statsTable = drillTable != null ? drillTable.getStatsTable() : null;\n+          if (drillTable != null && statsTable != null) {\n             statsTable.materialize(drillTable, context);\n-          } else {\n-            throw new DrillRuntimeException(\n-                String.format(\"Failed to find the stats for table [%s] in schema [%s]\",\n-                    node.getTable().getQualifiedName(), node.getTable().getRelOptSchema()));\n+          } else if (drillTable != null) {\n+            logger.debug(String.format(\"Unable to find statistics table info for table [%s] in schema [%s]\",\n+                node.getTable().getQualifiedName(), node.getTable().getRelOptSchema()));\n           }\n         } catch (Exception e) {\n-          // Log a warning and proceed. We don't want to fail a query.\n+          // Something unexpected happened! Log a warning and proceed. We don't want to fail the query.\n           logger.warn(\"Failed to materialize the stats. Continuing without stats.\", e);\n         }\n       }",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "status": "modified",
                "changes": 24,
                "deletions": 11,
                "sha": "65b0b6421e3ea1614e690ebb123b21d26ad89b71",
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258"
            },
            {
                "patch": "@@ -170,6 +170,7 @@ public void testAnalyzeSupportedFormats() throws Exception {\n     }\n   }\n \n+  @Ignore(\"For 1.16.0, we do not plan to support statistics on dir columns\")\n   @Test\n   public void testAnalyzePartitionedTables() throws Exception {\n     //Computing statistics on columns, dir0, dir1\n@@ -330,7 +331,6 @@ public void testUseStatistics() throws Exception {\n     PlanTestBase.testPlanWithAttributesMatchingPatterns(query, expectedPlan11, new String[]{});\n   }\n \n-  @Ignore(\"Fails intermittently. Enable after fixing the issue.\")\n   @Test\n   public void testWithMetadataCaching() throws Exception {\n     test(\"ALTER SESSION SET `planner.slice_target` = 1\");",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "30d23b3d9816456199b0c1b4c3408b7f127b1d75",
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258"
            }
        ],
        "bug_id": "drill_2",
        "parent": "https://github.com/apache/drill/commit/d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
        "message": "DRILL-7076: Fix NPE in StatsMaterializationVisitor\n\ncloses #1722",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/bf1bdec6069f6fdd2132608450357edea47d328c",
        "file": [
            {
                "patch": "@@ -458,7 +458,7 @@\n     <dependency>\n       <groupId>nl.basjes.parse.httpdlog</groupId>\n       <artifactId>httpdlog-parser</artifactId>\n-      <version>2.4</version>\n+      <version>5.2</version>\n       <exclusions>\n         <exclusion>\n           <groupId>commons-codec</groupId>",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "a341ab9bba688c4556a1ed21ca323a6097e92d0d",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/pom.xml",
                "filename": "exec/java-exec/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/pom.xml?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.httpd;\n+\n+import com.fasterxml.jackson.annotation.JsonInclude;\n+import com.fasterxml.jackson.annotation.JsonTypeName;\n+import org.apache.drill.common.logical.FormatPluginConfig;\n+\n+@JsonTypeName(\"httpd\")\n+@JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+public class HttpdLogFormatConfig implements FormatPluginConfig {\n+\n+  public String logFormat;\n+  public String timestampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n+\n+  /**\n+   * @return the log formatting string.  This string is the config string from httpd.conf or similar config file.\n+   */\n+  public String getLogFormat() {\n+    return logFormat;\n+  }\n+\n+  public void setLogFormat(String format) {\n+    this.logFormat = format;\n+  }\n+\n+  /**\n+   * @return the timestampFormat\n+   */\n+  public String getTimestampFormat() {\n+    return timestampFormat;\n+  }\n+\n+  /**\n+   * Sets the time stamp format\n+   * @param timestamp\n+   */\n+  public void setTimestampFormat(String timestamp) {\n+    this.timestampFormat = timestamp;\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    int result = logFormat != null ? logFormat.hashCode() : 0;\n+    result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n+    return result;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n+\n+    if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n+      return false;\n+    }\n+    return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n+  }\n+}",
                "additions": 80,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "status": "added",
                "changes": 80,
                "deletions": 0,
                "sha": "c4c34b6750c5e44520101673bb3539d4723190a2",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -18,17 +18,16 @@\n package org.apache.drill.exec.store.httpd;\n \n import java.io.IOException;\n+import java.util.HashMap;\n import java.util.List;\n \n-import com.fasterxml.jackson.annotation.JsonInclude;\n import nl.basjes.parse.core.exceptions.DissectionFailure;\n import nl.basjes.parse.core.exceptions.InvalidDissectorException;\n import nl.basjes.parse.core.exceptions.MissingDissectorsException;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.ops.FragmentContext;\n@@ -55,26 +54,23 @@\n import org.apache.hadoop.mapred.LineRecordReader;\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.mapred.TextInputFormat;\n-\n-import com.fasterxml.jackson.annotation.JsonTypeName;\n-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n-import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n+import java.util.Collections;\n import java.util.Map;\n import org.apache.drill.exec.store.RecordReader;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class HttpdLogFormatPlugin extends EasyFormatPlugin<HttpdLogFormatPlugin.HttpdLogFormatConfig> {\n+public class HttpdLogFormatPlugin extends EasyFormatPlugin<HttpdLogFormatConfig> {\n \n   private static final Logger LOG = LoggerFactory.getLogger(HttpdLogFormatPlugin.class);\n   private static final String PLUGIN_EXTENSION = \"httpd\";\n   private static final int VECTOR_MEMORY_ALLOCATION = 4095;\n \n   public HttpdLogFormatPlugin(final String name, final DrillbitContext context, final Configuration fsConf,\n-      final StoragePluginConfig storageConfig, final HttpdLogFormatConfig formatConfig) {\n+                              final StoragePluginConfig storageConfig, final HttpdLogFormatConfig formatConfig) {\n \n     super(name, context, fsConf, storageConfig, formatConfig, true, false, true, true,\n-        Lists.newArrayList(PLUGIN_EXTENSION), PLUGIN_EXTENSION);\n+            Collections.singletonList(PLUGIN_EXTENSION), PLUGIN_EXTENSION);\n   }\n \n   @Override\n@@ -92,55 +88,6 @@ public void writeStatistics(TableStatistics statistics, FileSystem fs, Path stat\n     throw new UnsupportedOperationException(\"unimplemented\");\n   }\n \n-  /**\n-   * This class is a POJO to hold the configuration for the HttpdLogFormat Parser. This is automatically\n-   * serialized/deserialized from JSON format.\n-   */\n-  @JsonTypeName(PLUGIN_EXTENSION) @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n-  public static class HttpdLogFormatConfig implements FormatPluginConfig {\n-\n-    public String logFormat;\n-    public String timestampFormat;\n-\n-    /**\n-     * @return the logFormat\n-     */\n-    public String getLogFormat() {\n-      return logFormat;\n-    }\n-\n-    /**\n-     * @return the timestampFormat\n-     */\n-    public String getTimestampFormat() {\n-      return timestampFormat;\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      int result = logFormat != null ? logFormat.hashCode() : 0;\n-      result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n-      return result;\n-    }\n-\n-    @Override\n-    public boolean equals(Object o) {\n-      if (this == o) {\n-        return true;\n-      }\n-      if (o == null || getClass() != o.getClass()) {\n-        return false;\n-      }\n-\n-      HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n-\n-      if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n-        return false;\n-      }\n-      return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n-    }\n-  }\n-\n   /**\n    * This class performs the work for the plugin. This is where all logic goes to read records. In this case httpd logs\n    * are lines terminated with a new line character.\n@@ -169,11 +116,15 @@ public HttpdLogRecordReader(final FragmentContext context, final DrillFileSystem\n      * @return Map with Drill field names as a key and Parser Field names as a value\n      */\n     private Map<String, String> makeParserFields() {\n-      final Map<String, String> fieldMapping = Maps.newHashMap();\n+      Map<String, String> fieldMapping = new HashMap<>();\n       for (final SchemaPath sp : getColumns()) {\n-        final String drillField = sp.getRootSegment().getPath();\n-        final String parserField = HttpdParser.parserFormattedFieldName(drillField);\n-        fieldMapping.put(drillField, parserField);\n+        String drillField = sp.getRootSegment().getPath();\n+        try {\n+          String parserField = HttpdParser.parserFormattedFieldName(drillField);\n+          fieldMapping.put(drillField, parserField);\n+        } catch (Exception e) {\n+          LOG.info(\"Putting field: \" + drillField + \" into map\", e);\n+        }\n       }\n       return fieldMapping;\n     }\n@@ -187,10 +138,11 @@ public void setup(final OperatorContext context, final OutputMutator output) thr\n          */\n         final Map<String, String> fieldMapping = !isStarQuery() ? makeParserFields() : null;\n         writer = new VectorContainerWriter(output);\n+\n         parser = new HttpdParser(writer.rootAsMap(), context.getManagedBuffer(),\n-            HttpdLogFormatPlugin.this.getConfig().getLogFormat(),\n-            HttpdLogFormatPlugin.this.getConfig().getTimestampFormat(),\n-            fieldMapping);\n+                HttpdLogFormatPlugin.this.getConfig().getLogFormat(),\n+                HttpdLogFormatPlugin.this.getConfig().getTimestampFormat(),\n+                fieldMapping);\n \n         final Path path = fs.makeQualified(work.getPath());\n         FileSplit split = new FileSplit(path, work.getStart(), work.getLength(), new String[]{\"\"});\n@@ -200,23 +152,21 @@ public void setup(final OperatorContext context, final OutputMutator output) thr\n         job.setInputFormat(inputFormat.getClass());\n         lineReader = (LineRecordReader) inputFormat.getRecordReader(split, job, Reporter.NULL);\n         lineNumber = lineReader.createKey();\n-      }\n-      catch (NoSuchMethodException | MissingDissectorsException | InvalidDissectorException e) {\n+      } catch (NoSuchMethodException | MissingDissectorsException | InvalidDissectorException e) {\n         throw handleAndGenerate(\"Failure creating HttpdParser\", e);\n-      }\n-      catch (IOException e) {\n+      } catch (IOException e) {\n         throw handleAndGenerate(\"Failure creating HttpdRecordReader\", e);\n       }\n     }\n \n     private RuntimeException handleAndGenerate(final String s, final Exception e) {\n       throw UserException.dataReadError(e)\n-          .message(s + \"\\n%s\", e.getMessage())\n-          .addContext(\"Path\", work.getPath())\n-          .addContext(\"Split Start\", work.getStart())\n-          .addContext(\"Split Length\", work.getLength())\n-          .addContext(\"Local Line Number\", lineNumber.get())\n-          .build(LOG);\n+              .message(s + \"\\n%s\", e.getMessage())\n+              .addContext(\"Path\", work.getPath())\n+              .addContext(\"Split Start\", work.getStart())\n+              .addContext(\"Split Length\", work.getLength())\n+              .addContext(\"Local Line Number\", lineNumber.get())\n+              .build(LOG);\n     }\n \n     /**\n@@ -241,8 +191,7 @@ public int next() {\n         writer.setValueCount(recordCount);\n \n         return recordCount;\n-      }\n-      catch (DissectionFailure | InvalidDissectorException | MissingDissectorsException | IOException e) {\n+      } catch (DissectionFailure | InvalidDissectorException | MissingDissectorsException | IOException e) {\n         throw handleAndGenerate(\"Failure while parsing log record.\", e);\n       }\n     }\n@@ -253,19 +202,18 @@ public void close() throws Exception {\n         if (lineReader != null) {\n           lineReader.close();\n         }\n-      }\n-      catch (IOException e) {\n+      } catch (IOException e) {\n         LOG.warn(\"Failure while closing Httpd reader.\", e);\n       }\n     }\n \n     @Override\n     public String toString() {\n       return \"HttpdLogRecordReader[Path=\" + work.getPath()\n-          + \", Start=\" + work.getStart()\n-          + \", Length=\" + work.getLength()\n-          + \", Line=\" + lineNumber.get()\n-          + \"]\";\n+              + \", Start=\" + work.getStart()\n+              + \", Length=\" + work.getLength()\n+              + \", Line=\" + lineNumber.get()\n+              + \"]\";\n     }\n   }\n ",
                "additions": 31,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "status": "modified",
                "changes": 114,
                "deletions": 83,
                "sha": "4785da13137999417d0ad7e6ad9dbccd6648f09d",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -20,31 +20,43 @@\n import org.apache.drill.shaded.guava.com.google.common.base.Charsets;\n import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n import io.netty.buffer.DrillBuf;\n+\n import java.util.EnumSet;\n+import java.util.HashMap;\n import java.util.Map;\n+\n import nl.basjes.parse.core.Casts;\n import nl.basjes.parse.core.Parser;\n import org.apache.drill.exec.vector.complex.writer.BaseWriter.MapWriter;\n import org.apache.drill.exec.vector.complex.writer.BigIntWriter;\n import org.apache.drill.exec.vector.complex.writer.Float8Writer;\n import org.apache.drill.exec.vector.complex.writer.VarCharWriter;\n+import org.apache.drill.exec.vector.complex.writer.TimeStampWriter;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+\n public class HttpdLogRecord {\n \n   private static final Logger LOG = LoggerFactory.getLogger(HttpdLogRecord.class);\n   private final Map<String, VarCharWriter> strings = Maps.newHashMap();\n   private final Map<String, BigIntWriter> longs = Maps.newHashMap();\n   private final Map<String, Float8Writer> doubles = Maps.newHashMap();\n+  private final Map<String, TimeStampWriter> times = new HashMap<>();\n   private final Map<String, MapWriter> wildcards = Maps.newHashMap();\n   private final Map<String, String> cleanExtensions = Maps.newHashMap();\n   private final Map<String, MapWriter> startedWildcards = Maps.newHashMap();\n   private final Map<String, MapWriter> wildcardWriters = Maps.newHashMap();\n+  private final SimpleDateFormat dateFormatter;\n   private DrillBuf managedBuffer;\n+  private String timeFormat;\n \n-  public HttpdLogRecord(final DrillBuf managedBuffer) {\n+  public HttpdLogRecord(final DrillBuf managedBuffer, final String timeFormat) {\n     this.managedBuffer = managedBuffer;\n+    this.timeFormat = timeFormat;\n+    this.dateFormatter = new SimpleDateFormat(this.timeFormat);\n   }\n \n   /**\n@@ -66,7 +78,7 @@ private DrillBuf buf(final int size) {\n     return managedBuffer;\n   }\n \n-  private void writeString(final VarCharWriter writer, final String value) {\n+  private void writeString(VarCharWriter writer, String value) {\n     final byte[] stringBytes = value.getBytes(Charsets.UTF_8);\n     final DrillBuf stringBuffer = buf(stringBytes.length);\n     stringBuffer.clear();\n@@ -82,14 +94,13 @@ private void writeString(final VarCharWriter writer, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final String value) {\n+  public void set(String field, String value) {\n     if (value != null) {\n       final VarCharWriter w = strings.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as string: {}\", field, value);\n         writeString(w, value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'string' writer found for field: {}\", field);\n       }\n     }\n@@ -103,19 +114,47 @@ public void set(final String field, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final Long value) {\n+  public void set(String field, Long value) {\n     if (value != null) {\n       final BigIntWriter w = longs.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as long: {}\", field, value);\n         w.writeBigInt(value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'long' writer found for field: {}\", field);\n       }\n     }\n   }\n \n+  /**\n+   * This method is referenced and called via reflection. This is added as a parsing target for the parser. It will get\n+   * called when the value of a log field is a timesstamp data type.\n+   *\n+   * @param field name of field\n+   * @param value value of field\n+   */\n+  @SuppressWarnings(\"unused\")\n+  public void setTimestamp(String field, String value) {\n+    if (value != null) {\n+      //Convert the date string into a long\n+      long ts = 0;\n+      try {\n+        Date d = this.dateFormatter.parse(value);\n+        ts = d.getTime();\n+      } catch (Exception e) {\n+        //If the date formatter does not successfully create a date, the timestamp will fall back to zero\n+        //Do not throw exception\n+      }\n+      final TimeStampWriter tw = times.get(field);\n+      if (tw != null) {\n+        LOG.trace(\"Parsed field: {}, as time: {}\", field, value);\n+        tw.writeTimeStamp(ts);\n+      } else {\n+        LOG.warn(\"No 'timestamp' writer found for field: {}\", field);\n+      }\n+    }\n+  }\n+\n   /**\n    * This method is referenced and called via reflection. This is added as a parsing target for the parser. It will get\n    * called when the value of a log field is a Double data type.\n@@ -124,14 +163,13 @@ public void set(final String field, final Long value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final Double value) {\n+  public void set(String field, Double value) {\n     if (value != null) {\n       final Float8Writer w = doubles.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as double: {}\", field, value);\n         w.writeFloat8(value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'double' writer found for field: {}\", field);\n       }\n     }\n@@ -146,7 +184,7 @@ public void set(final String field, final Double value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final String value) {\n+  public void setWildcard(String field, String value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as string: {}\", field, value);\n@@ -164,7 +202,7 @@ public void setWildcard(final String field, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final Long value) {\n+  public void setWildcard(String field, Long value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as long: {}\", field, value);\n@@ -182,7 +220,7 @@ public void setWildcard(final String field, final Long value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final Double value) {\n+  public void setWildcard(String field, Double value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as double: {}\", field, value);\n@@ -199,7 +237,7 @@ public void setWildcard(final String field, final Double value) {\n    * @param field like HTTP.URI:request.firstline.uri.query.old where 'old' is one of many different parameter names.\n    * @return the writer to be used for this field.\n    */\n-  private MapWriter getWildcardWriter(final String field) {\n+  private MapWriter getWildcardWriter(String field) {\n     MapWriter writer = startedWildcards.get(field);\n     if (writer == null) {\n       for (Map.Entry<String, MapWriter> entry : wildcards.entrySet()) {\n@@ -212,7 +250,7 @@ private MapWriter getWildcardWriter(final String field) {\n            * unsafe characters in it.\n            */\n           if (!cleanExtensions.containsKey(field)) {\n-            final String extension = field.substring(root.length() + 1, field.length());\n+            final String extension = field.substring(root.length() + 1);\n             final String cleanExtension = HttpdParser.drillFormattedFieldName(extension);\n             cleanExtensions.put(field, cleanExtension);\n             LOG.debug(\"Added extension: field='{}' with cleanExtension='{}'\", field, cleanExtension);\n@@ -255,6 +293,10 @@ private MapWriter getWildcardWriter(final String field) {\n     return doubles;\n   }\n \n+  public Map<String, TimeStampWriter> getTimes() {\n+    return times;\n+  }\n+\n   /**\n    * This record will be used with a single parser. For each field that is to be parsed a setter will be called. It\n    * registers a setter method for each field being parsed. It also builds the data writers to hold the data beings\n@@ -281,21 +323,23 @@ public void addField(final Parser<HttpdLogRecord> parser, final MapWriter mapWri\n       parser.addParseTarget(this.getClass().getMethod(\"setWildcard\", String.class, Double.class), parserFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"setWildcard\", String.class, Long.class), parserFieldName);\n       wildcards.put(cleanName, mapWriter.map(drillFieldName));\n-    }\n-    else if (type.contains(Casts.DOUBLE)) {\n+    } else if (type.contains(Casts.DOUBLE)) {\n       LOG.debug(\"Adding DOUBLE parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, Double.class), parserFieldName);\n       doubles.put(parserFieldName, mapWriter.float8(drillFieldName));\n-    }\n-    else if (type.contains(Casts.LONG)) {\n+    } else if (type.contains(Casts.LONG)) {\n       LOG.debug(\"Adding LONG parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, Long.class), parserFieldName);\n       longs.put(parserFieldName, mapWriter.bigInt(drillFieldName));\n-    }\n-    else {\n+    } else {\n       LOG.debug(\"Adding STRING parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n-      parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, String.class), parserFieldName);\n-      strings.put(parserFieldName, mapWriter.varChar(drillFieldName));\n+      if (parserFieldName.startsWith(\"TIME.STAMP:\")) {\n+        parser.addParseTarget(this.getClass().getMethod(\"setTimestamp\", String.class, String.class), parserFieldName);\n+        times.put(parserFieldName, mapWriter.timeStamp(drillFieldName));\n+      } else {\n+        parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, String.class), parserFieldName);\n+        strings.put(parserFieldName, mapWriter.varChar(drillFieldName));\n+      }\n     }\n   }\n }\n\\ No newline at end of file",
                "additions": 68,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "status": "modified",
                "changes": 92,
                "deletions": 24,
                "sha": "95917cb6e3c0da8f00823875e6f6c6e0d1302369",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -45,112 +45,262 @@\n   private final Parser<HttpdLogRecord> parser;\n   private final HttpdLogRecord record;\n \n-    public static final HashMap<String, String> LOGFIELDS = new HashMap<String, String>();\n-    static\n-    {\n-        LOGFIELDS.put(\"request_receive_time_weekyear__utc\", \"TIME_YEAR:request_receive_time_weekyear__utc\");\n-        LOGFIELDS.put(\"request_referer_ref\", \"HTTP_REF:request_referer_ref\");\n-        LOGFIELDS.put(\"request_referer_protocol\", \"HTTP_PROTOCOL:request_referer_protocol\");\n-        LOGFIELDS.put(\"request_receive_time_timezone\", \"TIME_ZONE:request_receive_time_timezone\");\n-        LOGFIELDS.put(\"connection_client_host\", \"IP:connection_client_host\");\n-        LOGFIELDS.put(\"connection_client_ip\", \"IP:connection_client_ip\");\n-        LOGFIELDS.put(\"connection_client_peerip\", \"IP:connection_client_peerip\");\n-        LOGFIELDS.put(\"connection_server_ip\", \"IP:connection_server_ip\");\n-        LOGFIELDS.put(\"request_receive_time_day\", \"TIME_DAY:request_receive_time_day\");\n-        LOGFIELDS.put(\"request_receive_time_minute__utc\", \"TIME_MINUTE:request_receive_time_minute__utc\");\n-        LOGFIELDS.put(\"request_referer_query_$\", \"STRING:request_referer_query_$\");\n-        LOGFIELDS.put(\"request_receive_time_millisecond__utc\", \"TIME_MILLISECOND:request_receive_time_millisecond__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri_port\", \"HTTP_PORT:request_firstline_uri_port\");\n-        LOGFIELDS.put(\"request_referer_userinfo\", \"HTTP_USERINFO:request_referer_userinfo\");\n-        LOGFIELDS.put(\"request_receive_time_second__utc\", \"TIME_SECOND:request_receive_time_second__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri_protocol\", \"HTTP_PROTOCOL:request_firstline_uri_protocol\");\n-        LOGFIELDS.put(\"request_receive_time_month\", \"TIME_MONTH:request_receive_time_month\");\n-        LOGFIELDS.put(\"request_firstline_uri_query\", \"HTTP_QUERYSTRING:request_firstline_uri_query\");\n-        LOGFIELDS.put(\"request_firstline_uri_path\", \"HTTP_PATH:request_firstline_uri_path\");\n-        LOGFIELDS.put(\"request_receive_time_hour__utc\", \"TIME_HOUR:request_receive_time_hour__utc\");\n-        LOGFIELDS.put(\"request_receive_time_monthname\", \"TIME_MONTHNAME:request_receive_time_monthname\");\n-        LOGFIELDS.put(\"request_receive_time_year__utc\", \"TIME_YEAR:request_receive_time_year__utc\");\n-        LOGFIELDS.put(\"request_receive_time_second\", \"TIME_SECOND:request_receive_time_second\");\n-        LOGFIELDS.put(\"request_referer\", \"HTTP_URI:request_referer\");\n-        LOGFIELDS.put(\"request_receive_time_monthname__utc\", \"TIME_MONTHNAME:request_receive_time_monthname__utc\");\n-        LOGFIELDS.put(\"request_referer_path\", \"HTTP_PATH:request_referer_path\");\n-        LOGFIELDS.put(\"request_receive_time_weekyear\", \"TIME_YEAR:request_receive_time_weekyear\");\n-        LOGFIELDS.put(\"request_firstline_protocol\", \"HTTP_PROTOCOL:request_firstline_protocol\");\n-        LOGFIELDS.put(\"request_referer_port\", \"HTTP_PORT:request_referer_port\");\n-        LOGFIELDS.put(\"request_receive_time_minute\", \"TIME_MINUTE:request_receive_time_minute\");\n-        LOGFIELDS.put(\"request_status_last\", \"STRING:request_status_last\");\n-        LOGFIELDS.put(\"request_receive_time_hour\", \"TIME_HOUR:request_receive_time_hour\");\n-        LOGFIELDS.put(\"request_firstline_protocol_version\", \"HTTP_PROTOCOL_VERSION:request_firstline_protocol_version\");\n-        LOGFIELDS.put(\"request_receive_time\", \"TIME_STAMP:request_receive_time\");\n-        LOGFIELDS.put(\"request_firstline_method\", \"HTTP_METHOD:request_firstline_method\");\n-        LOGFIELDS.put(\"request_receive_time_epoch\", \"TIME_EPOCH:request_receive_time_epoch\");\n-        LOGFIELDS.put(\"request_receive_time_weekofweekyear\", \"TIME_WEEK:request_receive_time_weekofweekyear\");\n-        LOGFIELDS.put(\"request_firstline_uri_host\", \"HTTP_HOST:request_firstline_uri_host\");\n-        LOGFIELDS.put(\"request_referer_query\", \"HTTP_QUERYSTRING:request_referer_query\");\n-        LOGFIELDS.put(\"request_firstline_uri_userinfo\", \"HTTP_USERINFO:request_firstline_uri_userinfo\");\n-        LOGFIELDS.put(\"response_body_bytes\", \"BYTES:response_body_bytes\");\n-        LOGFIELDS.put(\"response_body_bytesclf\", \"BYTES:response_body_bytesclf\");\n-        LOGFIELDS.put(\"request_referer_host\", \"HTTP_HOST:request_referer_host\");\n-        LOGFIELDS.put(\"request_receive_time_weekofweekyear__utc\", \"TIME_WEEK:request_receive_time_weekofweekyear__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri\", \"HTTP_URI:request_firstline_uri\");\n-        LOGFIELDS.put(\"request_firstline_uri_ref\", \"HTTP_REF:request_firstline_uri_ref\");\n-        LOGFIELDS.put(\"request_receive_time_year\", \"TIME_YEAR:request_receive_time_year\");\n-        LOGFIELDS.put(\"request_firstline\", \"HTTP_FIRSTLINE:request_firstline\");\n-        LOGFIELDS.put(\"request_user-agent\", \"HTTP_USERAGENT:request_user-agent\");\n-        LOGFIELDS.put(\"request_cookies\", \"HTTP_COOKIE:request_cookies\");\n-        LOGFIELDS.put(\"server_process_time\", \"MICROSECONDS:server_process_time\");\n-        LOGFIELDS.put(\"request_cookies_$\", \"HTTP_COOKIE:request_cookies_$\");\n-        LOGFIELDS.put(\"server_environment_$\", \"VARIABLE:server_environment_$\");\n-        LOGFIELDS.put(\"server_filename\", \"FILENAME:server_filename\");\n-        LOGFIELDS.put(\"request_protocol\", \"PROTOCOL:request_protocol\");\n-        LOGFIELDS.put(\"request_header_\", \"HTTP_HEADER:request_header_\");\n-        LOGFIELDS.put(\"connection_keepalivecount\", \"NUMBER:connection_keepalivecount\");\n-        LOGFIELDS.put(\"connection_client_logname\", \"NUMBER:connection_client_logname\");\n-        LOGFIELDS.put(\"request_errorlogid\", \"STRING:request_errorlogid\");\n-        LOGFIELDS.put(\"request_method\", \"HTTP_METHOD:request_method\");\n-        LOGFIELDS.put(\"server_module_note_$\", \"STRING:server_module_note_$\");\n-        LOGFIELDS.put(\"response_header_$\", \"HTTP_HEADER:response_header_$\");\n-        LOGFIELDS.put(\"request_server_port_canonical\", \"PORT:request_server_port_canonical\");\n-        LOGFIELDS.put(\"connection_server_port_canonical\", \"PORT:connection_server_port_canonical\");\n-        LOGFIELDS.put(\"connection_server_port\", \"PORT:connection_server_port\");\n-        LOGFIELDS.put(\"connection_client_port\", \"PORT:connection_client_port\");\n-        LOGFIELDS.put(\"connection_server_child_processid\", \"NUMBER:connection_server_child_processid\");\n-        LOGFIELDS.put(\"connection_server_child_threadid\", \"NUMBER:connection_server_child_threadid\");\n-        LOGFIELDS.put(\"connection_server_child_hexthreadid\", \"NUMBER:connection_server_child_hexthreadid\");\n-        LOGFIELDS.put(\"request_querystring\", \"HTTP_QUERYSTRING:request_querystring\");\n-        LOGFIELDS.put(\"request_handler\", \"STRING:request_handler\");\n-        LOGFIELDS.put(\"request_status_original\", \"STRING:request_status_original\");\n-        LOGFIELDS.put(\"request_status_last\", \"STRING:request_status_last\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec\", \"TIME_EPOCH:request_receive_time_begin_msec\");\n-        LOGFIELDS.put(\"request_receive_time_end_msec\", \"TIME_EPOCH:request_receive_time_end_msec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec\", \"TIME_EPOCH_USEC:request_receive_time_begin_usec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec\", \"TIME_EPOCH_USEC:request_receive_time_begin_usec\");\n-        LOGFIELDS.put(\"request_receive_time_end_usec\", \"TIME_EPOCH_USEC:request_receive_time_end_usec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec_frac\", \"TIME_EPOCH:request_receive_time_begin_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec_frac\", \"TIME_EPOCH:request_receive_time_begin_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_end_msec_frac\", \"TIME_EPOCH:request_receive_time_end_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request_receive_time_begin_usec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request.receive.time.begin.usec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_end_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request_receive_time_end_usec_frac\");\n-        LOGFIELDS.put(\"response_server_processing_time\", \"SECONDS:response_server_processing_time\");\n-        LOGFIELDS.put(\"connection_client_user\", \"STRING:connection_client_user\");\n-        LOGFIELDS.put(\"request_urlpath\", \"URI:request_urlpath\");\n-        LOGFIELDS.put(\"connection_server_name_canonical\", \"STRING:connection_server_name_canonical\");\n-        LOGFIELDS.put(\"connection_server_name\", \"STRING:connection_server_name\");\n-        LOGFIELDS.put(\"response_connection_status\", \"HTTP_CONNECTSTATUS:response_connection_status\");\n-        LOGFIELDS.put(\"request_bytes\", \"BYTES:request_bytes\");\n-        LOGFIELDS.put(\"response_bytes\", \"BYTES:response_bytes\");\n-    }\n+  public static final HashMap<String, String> LOGFIELDS = new HashMap<String, String>();\n \n-    //Map map = Collections.synchronizedMap(LOGFIELDS);\n+  static {\n+    LOGFIELDS.put(\"connection.client.ip\", \"IP:connection.client.ip\");\n+    LOGFIELDS.put(\"connection.client.ip.last\", \"IP:connection.client.ip.last\");\n+    LOGFIELDS.put(\"connection.client.ip.original\", \"IP:connection.client.ip.original\");\n+    LOGFIELDS.put(\"connection.client.ip.last\", \"IP:connection.client.ip.last\");\n+    LOGFIELDS.put(\"connection.client.peerip\", \"IP:connection.client.peerip\");\n+    LOGFIELDS.put(\"connection.client.peerip.last\", \"IP:connection.client.peerip.last\");\n+    LOGFIELDS.put(\"connection.client.peerip.original\", \"IP:connection.client.peerip.original\");\n+    LOGFIELDS.put(\"connection.client.peerip.last\", \"IP:connection.client.peerip.last\");\n+    LOGFIELDS.put(\"connection.server.ip\", \"IP:connection.server.ip\");\n+    LOGFIELDS.put(\"connection.server.ip.last\", \"IP:connection.server.ip.last\");\n+    LOGFIELDS.put(\"connection.server.ip.original\", \"IP:connection.server.ip.original\");\n+    LOGFIELDS.put(\"connection.server.ip.last\", \"IP:connection.server.ip.last\");\n+    LOGFIELDS.put(\"response.body.bytes\", \"BYTES:response.body.bytes\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTES:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytes.original\", \"BYTES:response.body.bytes.original\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTES:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytesclf\", \"BYTES:response.body.bytesclf\");\n+    LOGFIELDS.put(\"response.body.bytes\", \"BYTESCLF:response.body.bytes\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTESCLF:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytes.original\", \"BYTESCLF:response.body.bytes.original\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTESCLF:response.body.bytes.last\");\n+    LOGFIELDS.put(\"request.cookies.foobar\", \"HTTP.COOKIE:request.cookies.foobar\");\n+    LOGFIELDS.put(\"server.environment.foobar\", \"VARIABLE:server.environment.foobar\");\n+    LOGFIELDS.put(\"server.filename\", \"FILENAME:server.filename\");\n+    LOGFIELDS.put(\"server.filename.last\", \"FILENAME:server.filename.last\");\n+    LOGFIELDS.put(\"server.filename.original\", \"FILENAME:server.filename.original\");\n+    LOGFIELDS.put(\"server.filename.last\", \"FILENAME:server.filename.last\");\n+    LOGFIELDS.put(\"connection.client.host\", \"IP:connection.client.host\");\n+    LOGFIELDS.put(\"connection.client.host.last\", \"IP:connection.client.host.last\");\n+    LOGFIELDS.put(\"connection.client.host.original\", \"IP:connection.client.host.original\");\n+    LOGFIELDS.put(\"connection.client.host.last\", \"IP:connection.client.host.last\");\n+    LOGFIELDS.put(\"request.protocol\", \"PROTOCOL:request.protocol\");\n+    LOGFIELDS.put(\"request.protocol.last\", \"PROTOCOL:request.protocol.last\");\n+    LOGFIELDS.put(\"request.protocol.original\", \"PROTOCOL:request.protocol.original\");\n+    LOGFIELDS.put(\"request.protocol.last\", \"PROTOCOL:request.protocol.last\");\n+    LOGFIELDS.put(\"request.header.foobar\", \"HTTP.HEADER:request.header.foobar\");\n+    LOGFIELDS.put(\"request.trailer.foobar\", \"HTTP.TRAILER:request.trailer.foobar\");\n+    LOGFIELDS.put(\"connection.keepalivecount\", \"NUMBER:connection.keepalivecount\");\n+    LOGFIELDS.put(\"connection.keepalivecount.last\", \"NUMBER:connection.keepalivecount.last\");\n+    LOGFIELDS.put(\"connection.keepalivecount.original\", \"NUMBER:connection.keepalivecount.original\");\n+    LOGFIELDS.put(\"connection.keepalivecount.last\", \"NUMBER:connection.keepalivecount.last\");\n+    LOGFIELDS.put(\"connection.client.logname\", \"NUMBER:connection.client.logname\");\n+    LOGFIELDS.put(\"connection.client.logname.last\", \"NUMBER:connection.client.logname.last\");\n+    LOGFIELDS.put(\"connection.client.logname.original\", \"NUMBER:connection.client.logname.original\");\n+    LOGFIELDS.put(\"connection.client.logname.last\", \"NUMBER:connection.client.logname.last\");\n+    LOGFIELDS.put(\"request.errorlogid\", \"STRING:request.errorlogid\");\n+    LOGFIELDS.put(\"request.errorlogid.last\", \"STRING:request.errorlogid.last\");\n+    LOGFIELDS.put(\"request.errorlogid.original\", \"STRING:request.errorlogid.original\");\n+    LOGFIELDS.put(\"request.errorlogid.last\", \"STRING:request.errorlogid.last\");\n+    LOGFIELDS.put(\"request.method\", \"HTTP.METHOD:request.method\");\n+    LOGFIELDS.put(\"request.method.last\", \"HTTP.METHOD:request.method.last\");\n+    LOGFIELDS.put(\"request.method.original\", \"HTTP.METHOD:request.method.original\");\n+    LOGFIELDS.put(\"request.method.last\", \"HTTP.METHOD:request.method.last\");\n+    LOGFIELDS.put(\"server.module_note.foobar\", \"STRING:server.module_note.foobar\");\n+    LOGFIELDS.put(\"response.header.foobar\", \"HTTP.HEADER:response.header.foobar\");\n+    LOGFIELDS.put(\"response.trailer.foobar\", \"HTTP.TRAILER:response.trailer.foobar\");\n+    LOGFIELDS.put(\"request.server.port.canonical\", \"PORT:request.server.port.canonical\");\n+    LOGFIELDS.put(\"request.server.port.canonical.last\", \"PORT:request.server.port.canonical.last\");\n+    LOGFIELDS.put(\"request.server.port.canonical.original\", \"PORT:request.server.port.canonical.original\");\n+    LOGFIELDS.put(\"request.server.port.canonical.last\", \"PORT:request.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port.canonical\", \"PORT:connection.server.port.canonical\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.last\", \"PORT:connection.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.original\", \"PORT:connection.server.port.canonical.original\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.last\", \"PORT:connection.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port\", \"PORT:connection.server.port\");\n+    LOGFIELDS.put(\"connection.server.port.last\", \"PORT:connection.server.port.last\");\n+    LOGFIELDS.put(\"connection.server.port.original\", \"PORT:connection.server.port.original\");\n+    LOGFIELDS.put(\"connection.server.port.last\", \"PORT:connection.server.port.last\");\n+    LOGFIELDS.put(\"connection.client.port\", \"PORT:connection.client.port\");\n+    LOGFIELDS.put(\"connection.client.port.last\", \"PORT:connection.client.port.last\");\n+    LOGFIELDS.put(\"connection.client.port.original\", \"PORT:connection.client.port.original\");\n+    LOGFIELDS.put(\"connection.client.port.last\", \"PORT:connection.client.port.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid\", \"NUMBER:connection.server.child.processid\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid.original\", \"NUMBER:connection.server.child.processid.original\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid\", \"NUMBER:connection.server.child.processid\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid.original\", \"NUMBER:connection.server.child.processid.original\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.threadid\", \"NUMBER:connection.server.child.threadid\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.last\", \"NUMBER:connection.server.child.threadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.original\", \"NUMBER:connection.server.child.threadid.original\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.last\", \"NUMBER:connection.server.child.threadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid\", \"NUMBER:connection.server.child.hexthreadid\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.last\", \"NUMBER:connection.server.child.hexthreadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.original\", \"NUMBER:connection.server.child.hexthreadid.original\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.last\", \"NUMBER:connection.server.child.hexthreadid.last\");\n+    LOGFIELDS.put(\"request.querystring\", \"HTTP.QUERYSTRING:request.querystring\");\n+    LOGFIELDS.put(\"request.querystring.last\", \"HTTP.QUERYSTRING:request.querystring.last\");\n+    LOGFIELDS.put(\"request.querystring.original\", \"HTTP.QUERYSTRING:request.querystring.original\");\n+    LOGFIELDS.put(\"request.querystring.last\", \"HTTP.QUERYSTRING:request.querystring.last\");\n+    LOGFIELDS.put(\"request.firstline\", \"HTTP.FIRSTLINE:request.firstline\");\n+    LOGFIELDS.put(\"request.firstline.original\", \"HTTP.FIRSTLINE:request.firstline.original\");\n+    LOGFIELDS.put(\"request.firstline.original\", \"HTTP.FIRSTLINE:request.firstline.original\");\n+    LOGFIELDS.put(\"request.firstline.last\", \"HTTP.FIRSTLINE:request.firstline.last\");\n+    LOGFIELDS.put(\"request.handler\", \"STRING:request.handler\");\n+    LOGFIELDS.put(\"request.handler.last\", \"STRING:request.handler.last\");\n+    LOGFIELDS.put(\"request.handler.original\", \"STRING:request.handler.original\");\n+    LOGFIELDS.put(\"request.handler.last\", \"STRING:request.handler.last\");\n+    LOGFIELDS.put(\"request.status\", \"STRING:request.status\");\n+    LOGFIELDS.put(\"request.status.original\", \"STRING:request.status.original\");\n+    LOGFIELDS.put(\"request.status.original\", \"STRING:request.status.original\");\n+    LOGFIELDS.put(\"request.status.last\", \"STRING:request.status.last\");\n+    LOGFIELDS.put(\"request.receive.time\", \"TIME.STAMP:request.receive.time\");\n+    LOGFIELDS.put(\"request.receive.time.last\", \"TIME.STAMP:request.receive.time.last\");\n+    LOGFIELDS.put(\"request.receive.time.original\", \"TIME.STAMP:request.receive.time.original\");\n+    LOGFIELDS.put(\"request.receive.time.last\", \"TIME.STAMP:request.receive.time.last\");\n+    LOGFIELDS.put(\"request.receive.time.year\", \"TIME.YEAR:request.receive.time.year\");\n+    LOGFIELDS.put(\"request.receive.time.begin.year\", \"TIME.YEAR:request.receive.time.begin.year\");\n+    LOGFIELDS.put(\"request.receive.time.end.year\", \"TIME.YEAR:request.receive.time.end.year\");\n+    LOGFIELDS.put(\"request.receive.time.sec\", \"TIME.SECONDS:request.receive.time.sec\");\n+    LOGFIELDS.put(\"request.receive.time.sec\", \"TIME.SECONDS:request.receive.time.sec\");\n+    LOGFIELDS.put(\"request.receive.time.sec.original\", \"TIME.SECONDS:request.receive.time.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.sec.last\", \"TIME.SECONDS:request.receive.time.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec\", \"TIME.SECONDS:request.receive.time.begin.sec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.last\", \"TIME.SECONDS:request.receive.time.begin.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.original\", \"TIME.SECONDS:request.receive.time.begin.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.last\", \"TIME.SECONDS:request.receive.time.begin.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec\", \"TIME.SECONDS:request.receive.time.end.sec\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.last\", \"TIME.SECONDS:request.receive.time.end.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.original\", \"TIME.SECONDS:request.receive.time.end.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.last\", \"TIME.SECONDS:request.receive.time.end.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec\", \"TIME.EPOCH:request.receive.time.begin.msec\");\n+    LOGFIELDS.put(\"request.receive.time.msec\", \"TIME.EPOCH:request.receive.time.msec\");\n+    LOGFIELDS.put(\"request.receive.time.msec.last\", \"TIME.EPOCH:request.receive.time.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.msec.original\", \"TIME.EPOCH:request.receive.time.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.msec.last\", \"TIME.EPOCH:request.receive.time.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec\", \"TIME.EPOCH:request.receive.time.begin.msec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.last\", \"TIME.EPOCH:request.receive.time.begin.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.original\", \"TIME.EPOCH:request.receive.time.begin.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.last\", \"TIME.EPOCH:request.receive.time.begin.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec\", \"TIME.EPOCH:request.receive.time.end.msec\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.last\", \"TIME.EPOCH:request.receive.time.end.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.original\", \"TIME.EPOCH:request.receive.time.end.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.last\", \"TIME.EPOCH:request.receive.time.end.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec\");\n+    LOGFIELDS.put(\"request.receive.time.usec\", \"TIME.EPOCH.USEC:request.receive.time.usec\");\n+    LOGFIELDS.put(\"request.receive.time.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec\", \"TIME.EPOCH.USEC:request.receive.time.end.usec\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac\", \"TIME.EPOCH:request.receive.time.begin.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac\", \"TIME.EPOCH:request.receive.time.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.last\", \"TIME.EPOCH:request.receive.time.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.original\", \"TIME.EPOCH:request.receive.time.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.last\", \"TIME.EPOCH:request.receive.time.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac\", \"TIME.EPOCH:request.receive.time.begin.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.last\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.original\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.last\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac\", \"TIME.EPOCH:request.receive.time.end.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.last\", \"TIME.EPOCH:request.receive.time.end.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.original\", \"TIME.EPOCH:request.receive.time.end.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.last\", \"TIME.EPOCH:request.receive.time.end.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac\", \"FRAC:request.receive.time.begin.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac\", \"FRAC:request.receive.time.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.last\", \"FRAC:request.receive.time.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.original\", \"FRAC:request.receive.time.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.last\", \"FRAC:request.receive.time.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac\", \"FRAC:request.receive.time.begin.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.last\", \"FRAC:request.receive.time.begin.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.original\", \"FRAC:request.receive.time.begin.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.last\", \"FRAC:request.receive.time.begin.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac\", \"FRAC:request.receive.time.end.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.last\", \"FRAC:request.receive.time.end.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.original\", \"FRAC:request.receive.time.end.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.last\", \"FRAC:request.receive.time.end.usec_frac.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"SECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"SECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"server.process.time\", \"MICROSECONDS:server.process.time\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MICROSECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MICROSECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MICROSECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MICROSECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MILLISECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MILLISECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MILLISECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MILLISECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"SECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"SECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"connection.client.user\", \"STRING:connection.client.user\");\n+    LOGFIELDS.put(\"connection.client.user.last\", \"STRING:connection.client.user.last\");\n+    LOGFIELDS.put(\"connection.client.user.original\", \"STRING:connection.client.user.original\");\n+    LOGFIELDS.put(\"connection.client.user.last\", \"STRING:connection.client.user.last\");\n+    LOGFIELDS.put(\"request.urlpath\", \"URI:request.urlpath\");\n+    LOGFIELDS.put(\"request.urlpath.original\", \"URI:request.urlpath.original\");\n+    LOGFIELDS.put(\"request.urlpath.original\", \"URI:request.urlpath.original\");\n+    LOGFIELDS.put(\"request.urlpath.last\", \"URI:request.urlpath.last\");\n+    LOGFIELDS.put(\"connection.server.name.canonical\", \"STRING:connection.server.name.canonical\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.last\", \"STRING:connection.server.name.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.original\", \"STRING:connection.server.name.canonical.original\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.last\", \"STRING:connection.server.name.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.name\", \"STRING:connection.server.name\");\n+    LOGFIELDS.put(\"connection.server.name.last\", \"STRING:connection.server.name.last\");\n+    LOGFIELDS.put(\"connection.server.name.original\", \"STRING:connection.server.name.original\");\n+    LOGFIELDS.put(\"connection.server.name.last\", \"STRING:connection.server.name.last\");\n+    LOGFIELDS.put(\"response.connection.status\", \"HTTP.CONNECTSTATUS:response.connection.status\");\n+    LOGFIELDS.put(\"response.connection.status.last\", \"HTTP.CONNECTSTATUS:response.connection.status.last\");\n+    LOGFIELDS.put(\"response.connection.status.original\", \"HTTP.CONNECTSTATUS:response.connection.status.original\");\n+    LOGFIELDS.put(\"response.connection.status.last\", \"HTTP.CONNECTSTATUS:response.connection.status.last\");\n+    LOGFIELDS.put(\"request.bytes\", \"BYTES:request.bytes\");\n+    LOGFIELDS.put(\"request.bytes.last\", \"BYTES:request.bytes.last\");\n+    LOGFIELDS.put(\"request.bytes.original\", \"BYTES:request.bytes.original\");\n+    LOGFIELDS.put(\"request.bytes.last\", \"BYTES:request.bytes.last\");\n+    LOGFIELDS.put(\"response.bytes\", \"BYTES:response.bytes\");\n+    LOGFIELDS.put(\"response.bytes.last\", \"BYTES:response.bytes.last\");\n+    LOGFIELDS.put(\"response.bytes.original\", \"BYTES:response.bytes.original\");\n+    LOGFIELDS.put(\"response.bytes.last\", \"BYTES:response.bytes.last\");\n+    LOGFIELDS.put(\"total.bytes\", \"BYTES:total.bytes\");\n+    LOGFIELDS.put(\"total.bytes.last\", \"BYTES:total.bytes.last\");\n+    LOGFIELDS.put(\"total.bytes.original\", \"BYTES:total.bytes.original\");\n+    LOGFIELDS.put(\"total.bytes.last\", \"BYTES:total.bytes.last\");\n+    LOGFIELDS.put(\"request.cookies\", \"HTTP.COOKIES:request.cookies\");\n+    LOGFIELDS.put(\"request.cookies.last\", \"HTTP.COOKIES:request.cookies.last\");\n+    LOGFIELDS.put(\"request.cookies.original\", \"HTTP.COOKIES:request.cookies.original\");\n+    LOGFIELDS.put(\"request.cookies.last\", \"HTTP.COOKIES:request.cookies.last\");\n+    LOGFIELDS.put(\"response.cookies\", \"HTTP.SETCOOKIES:response.cookies\");\n+    LOGFIELDS.put(\"response.cookies.last\", \"HTTP.SETCOOKIES:response.cookies.last\");\n+    LOGFIELDS.put(\"response.cookies.original\", \"HTTP.SETCOOKIES:response.cookies.original\");\n+    LOGFIELDS.put(\"response.cookies.last\", \"HTTP.SETCOOKIES:response.cookies.last\");\n+    LOGFIELDS.put(\"request.user-agent\", \"HTTP.USERAGENT:request.user-agent\");\n+    LOGFIELDS.put(\"request.user-agent.last\", \"HTTP.USERAGENT:request.user-agent.last\");\n+    LOGFIELDS.put(\"request.user-agent.original\", \"HTTP.USERAGENT:request.user-agent.original\");\n+    LOGFIELDS.put(\"request.user-agent.last\", \"HTTP.USERAGENT:request.user-agent.last\");\n+    LOGFIELDS.put(\"request.referer\", \"HTTP.URI:request.referer\");\n+    LOGFIELDS.put(\"request.referer.last\", \"HTTP.URI:request.referer.last\");\n+    LOGFIELDS.put(\"request.referer.original\", \"HTTP.URI:request.referer.original\");\n+    LOGFIELDS.put(\"request.referer.last\", \"HTTP.URI:request.referer.last\");\n+  }\n \n   public HttpdParser(final MapWriter mapWriter, final DrillBuf managedBuffer, final String logFormat,\n-      final String timestampFormat, final Map<String, String> fieldMapping)\n-      throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n+                     final String timestampFormat, final Map<String, String> fieldMapping)\n+          throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n \n     Preconditions.checkArgument(logFormat != null && !logFormat.trim().isEmpty(), \"logFormat cannot be null or empty\");\n \n-    this.record = new HttpdLogRecord(managedBuffer);\n+    this.record = new HttpdLogRecord(managedBuffer, timestampFormat);\n     this.parser = new HttpdLoglineParser<>(HttpdLogRecord.class, logFormat, timestampFormat);\n \n     setupParser(mapWriter, logFormat, fieldMapping);\n@@ -167,7 +317,6 @@ public HttpdParser(final MapWriter mapWriter, final DrillBuf managedBuffer, fina\n    * We do not expose the underlying parser or the record which is used to manage the writers.\n    *\n    * @param line log line to tear apart.\n-   *\n    * @throws DissectionFailure\n    * @throws InvalidDissectorException\n    * @throws MissingDissectorsException\n@@ -181,7 +330,7 @@ public void parse(final String line) throws DissectionFailure, InvalidDissectorE\n    * In order to define a type remapping the format of the field configuration will look like: <br/>\n    * HTTP.URI:request.firstline.uri.query.[parameter name] <br/>\n    *\n-   * @param parser Add type remapping to this parser instance.\n+   * @param parser    Add type remapping to this parser instance.\n    * @param fieldName request.firstline.uri.query.[parameter_name]\n    * @param fieldType HTTP.URI, etc..\n    */\n@@ -198,11 +347,17 @@ private void addTypeRemapping(final Parser<HttpdLogRecord> parser, final String\n    * @param drillFieldName name to be cleansed.\n    * @return\n    */\n-  public static String parserFormattedFieldName(final String drillFieldName) {\n-      String tempFieldName;\n-      tempFieldName = LOGFIELDS.get(drillFieldName);\n-      return tempFieldName.replace(SAFE_WILDCARD, PARSER_WILDCARD).replaceAll(SAFE_SEPARATOR, \".\").replaceAll(\"\\\\.\\\\.\", \"_\");\n+  public static String parserFormattedFieldName(String drillFieldName) {\n \n+    //The Useragent fields contain a dash which causes potential problems if the field name is not escaped properly\n+    //This removes the dash\n+    if (drillFieldName.contains(\"useragent\")) {\n+      drillFieldName = drillFieldName.replace(\"useragent\", \"user-agent\");\n+    }\n+\n+    String tempFieldName;\n+    tempFieldName = LOGFIELDS.get(drillFieldName);\n+    return tempFieldName.replace(SAFE_WILDCARD, PARSER_WILDCARD).replaceAll(SAFE_SEPARATOR, \".\").replaceAll(\"\\\\.\\\\.\", \"_\");\n   }\n \n   /**\n@@ -213,19 +368,24 @@ public static String parserFormattedFieldName(final String drillFieldName) {\n    * @param parserFieldName name to be cleansed.\n    * @return\n    */\n-  public static String drillFormattedFieldName(final String parserFieldName) {\n+  public static String drillFormattedFieldName(String parserFieldName) {\n \n-      if (parserFieldName.contains(\":\") ) {\n-        String[] fieldPart= parserFieldName.split(\":\");\n-        return fieldPart[1].replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n-        }\n-    else{\n+    //The Useragent fields contain a dash which causes potential problems if the field name is not escaped properly\n+    //This removes the dash\n+    if (parserFieldName.contains(\"user-agent\")) {\n+      parserFieldName = parserFieldName.replace(\"user-agent\", \"useragent\");\n+    }\n+\n+    if (parserFieldName.contains(\":\")) {\n+      String[] fieldPart = parserFieldName.split(\":\");\n+      return fieldPart[1].replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n+    } else {\n       return parserFieldName.replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n     }\n   }\n \n   private void setupParser(final MapWriter mapWriter, final String logFormat, final Map<String, String> fieldMapping)\n-      throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n+          throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n \n     /**\n      * If the user has selected fields, then we will use them to configure the parser because this would be the most\n@@ -236,8 +396,7 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n     if (fieldMapping != null && !fieldMapping.isEmpty()) {\n       LOG.debug(\"Using fields defined by user.\");\n       requestedPaths = fieldMapping;\n-    }\n-    else {\n+    } else {\n       /**\n        * Use all possible paths that the parser has determined from the specified log format.\n        */\n@@ -255,7 +414,6 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n      */\n     Parser<Object> dummy = new HttpdLoglineParser<>(Object.class, logFormat);\n     dummy.addParseTarget(String.class.getMethod(\"indexOf\", String.class), allParserPaths);\n-\n     for (final Map.Entry<String, String> entry : requestedPaths.entrySet()) {\n       final EnumSet<Casts> casts;\n \n@@ -270,15 +428,13 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n \n         final String[] pieces = entry.getValue().split(\":\");\n         addTypeRemapping(parser, pieces[1], pieces[0]);\n-\n         casts = Casts.STRING_ONLY;\n-      }\n-      else {\n+      } else {\n         casts = dummy.getCasts(entry.getValue());\n       }\n \n       LOG.debug(\"Setting up drill field: {}, parser field: {}, which casts as: {}\", entry.getKey(), entry.getValue(), casts);\n       record.addField(parser, mapWriter, casts, entry.getValue(), entry.getKey());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "additions": 276,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "status": "modified",
                "changes": 396,
                "deletions": 120,
                "sha": "5d3d7c0d3b5cd18f013d1fd4984ec9a4bf677c06",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -1,50 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.drill.exec.store.httpd;\n-\n-import io.netty.buffer.DrillBuf;\n-import java.util.Map;\n-import org.apache.drill.exec.vector.complex.writer.BaseWriter.MapWriter;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-public class HttpdParserTest {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(HttpdParserTest.class);\n-\n-  private void runTest(String logFormat, String logLine) throws Exception {\n-    MapWriter mapWriter = null;\n-    DrillBuf managedBuffer = null;\n-    Map<String, String> configuredFields = null;\n-    HttpdParser parser = new HttpdParser(mapWriter, managedBuffer, logFormat, null, configuredFields);\n-    parser.parse(logLine);\n-  }\n-\n-//  @Test\n-  public void testFirstPattern() throws Exception {\n-    LOG.info(\"testFirstPattern\");\n-//    final String format = \"common\";\n-//    final String format = \"%h %l %u %t \\\"%r\\\" %>s %b\";\n-    final String format = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n-    final String line = \"127.0.0.1 [31/Dec/2012:23:49:41 +0100] \"\n-        + \"\\\"GET /foo HTTP/1.1\\\" 200 \"\n-        + \"1213 \\\"http://localhost/index.php?mies=wim\\\"\";\n-    runTest(format, line);\n-  }\n-\n-}\n\\ No newline at end of file",
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/2364b02175bec69cee2f9ceb4e52e1333da39f70/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "status": "removed",
                "changes": 50,
                "deletions": 50,
                "sha": "3897136a144d2805996c62c521e92f17588f3985",
                "blob_url": "https://github.com/apache/drill/blob/2364b02175bec69cee2f9ceb4e52e1333da39f70/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java?ref=2364b02175bec69cee2f9ceb4e52e1333da39f70"
            },
            {
                "patch": "@@ -1,178 +1,178 @@\n {\n   \"storage\":{\n-    dfs: {\n-      type: \"file\",\n-      connection: \"file:///\",\n-      workspaces: {\n+    \"dfs\": {\n+      \"type\": \"file\",\n+      \"connection\": \"file:///\",\n+      \"workspaces\": {\n         \"root\" : {\n-          location: \"/\",\n-          writable: false,\n-          allowAccessOutsideWorkspace: false\n+          \"location\": \"/\",\n+          \"writable\": false,\n+          \"allowAccessOutsideWorkspace\": false\n         },\n         \"tmp\" : {\n-          location: \"/tmp\",\n-          writable: true,\n-          allowAccessOutsideWorkspace: false\n+          \"location\": \"/tmp\",\n+          \"writable\": true,\n+          \"allowAccessOutsideWorkspace\": false\n         }\n       },\n-      formats: {\n+      \"formats\": {\n         \"psv\" : {\n-          type: \"text\",\n-          extensions: [ \"tbl\" ],\n-          delimiter: \"|\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tbl\" ],\n+          \"delimiter\": \"|\"\n         },\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-          type: \"text\",\n-          extensions: [ \"tsv\" ],\n-          delimiter: \"\\t\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tsv\" ],\n+          \"delimiter\": \"\\t\"\n         },\n         \"httpd\" : {\n-          type: \"httpd\",\n-          logFormat: \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\"\n-          /* timestampFormat: \"dd/MMM/yyyy:HH:mm:ss ZZ\" */\n+          \"type\": \"httpd\",\n+          \"logFormat\": \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\",\n+          \"timestampFormat\": \"dd/MMM/yyyy:HH:mm:ss ZZ\"\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"pcap\" : {\n-          type: \"pcap\"\n+          \"type\": \"pcap\"\n         },\n         \"pcapng\" : {\n-          type: \"pcapng\"\n+          \"type\": \"pcapng\"\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"sequencefile\": {\n-          type : \"sequencefile\",\n-          extensions: [ \"seq\" ]\n+          \"type\": \"sequencefile\",\n+          \"extensions\": [ \"seq\" ]\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         },\n         \"image\" : {\n-          type: \"image\",\n-          extensions: [\n+          \"type\": \"image\",\n+          \"extensions\": [\n             \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\",\n             \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\",\n             \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\",\n             \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\"\n           ]\n         }\n       },\n-      enabled : true\n+      \"enabled\" : true\n     },\n \n-    s3: {\n-      type: \"file\",\n-      connection: \"s3a://my.bucket.location.com\",\n-      config : {\n+    \"s3\": {\n+      \"type\": \"file\",\n+      \"connection\": \"s3a://my.bucket.location.com\",\n+      \"config\" : {\n         \"fs.s3a.access.key\": \"ID\",\n         \"fs.s3a.secret.key\": \"SECRET\"\n       },\n-      workspaces: {\n+      \"workspaces\": {\n         \"root\" : {\n-          location: \"/\",\n-          writable: false\n+          \"location\": \"/\",\n+          \"writable\": false\n         },\n         \"tmp\" : {\n-          location: \"/tmp\",\n-          writable: true\n+          \"location\": \"/tmp\",\n+          \"writable\": true\n         }\n       },\n-      formats: {\n+      \"formats\": {\n         \"psv\" : {\n-          type: \"text\",\n-          extensions: [ \"tbl\" ],\n-          delimiter: \"|\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tbl\" ],\n+          \"delimiter\": \"|\"\n         },\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-          type: \"text\",\n-          extensions: [ \"tsv\" ],\n-          delimiter: \"\\t\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tsv\" ],\n+          \"delimiter\": \"\\t\"\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"sequencefile\": {\n-          type : \"sequencefile\",\n-          extensions: [ \"seq\" ]\n+          \"type\": \"sequencefile\",\n+          \"extensions\": [ \"seq\" ]\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         }\n       },\n-      enabled : false\n+      \"enabled\" : false\n     },\n \n-    cp: {\n-      type: \"file\",\n-      connection: \"classpath:///\",\n-      formats: {\n+    \"cp\": {\n+      \"type\": \"file\",\n+      \"connection\": \"classpath:///\",\n+      \"formats\": {\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-                  type: \"text\",\n-                  extensions: [ \"tsv\" ],\n-                  delimiter: \"\\t\"\n+                  \"type\": \"text\",\n+                  \"extensions\": [ \"tsv\" ],\n+                  \"delimiter\": \"\\t\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         },\n         \"image\" : {\n-          type: \"image\",\n-          extensions: [\n+          \"type\": \"image\",\n+          \"extensions\": [\n             \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\",\n             \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\",\n             \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\",\n             \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\"\n           ]\n         }\n       },\n-      enabled : true\n+      \"enabled\" : true\n     }\n   }\n }",
                "additions": 89,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "status": "modified",
                "changes": 178,
                "deletions": 89,
                "sha": "afcf53d969d3e27554632b8daa67a6b87cc6b057",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "filename": "exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -93,9 +93,9 @@ public void testPcap() throws Exception {\n \n   @Test\n   public void testHttpd() throws Exception {\n-    String path = \"store/httpd/dfs-bootstrap.httpd\";\n+    String path = \"store/httpd/dfs-test-bootstrap-test.httpd\";\n     dirTestWatcher.copyResourceToRoot(Paths.get(path));\n-    String logFormat = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n+    String logFormat = \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\";\n     String timeStampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n     testPhysicalPlanSubmission(\n         String.format(\"select * from dfs.`%s`\", path),",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "b4c8b146a6a96bdc0c6217f4311bc620d1ff429d",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.httpd;\n+\n+import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.types.TypeProtos.MinorType;\n+import org.apache.drill.exec.record.BatchSchema;\n+import org.apache.drill.exec.record.metadata.SchemaBuilder;\n+import org.apache.drill.exec.rpc.RpcException;\n+import org.apache.drill.exec.server.Drillbit;\n+import org.apache.drill.exec.store.StoragePluginRegistry;\n+import org.apache.drill.exec.store.dfs.FileSystemConfig;\n+import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n+import org.apache.drill.test.BaseDirTestWatcher;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterTest;\n+import org.apache.drill.test.rowSet.RowSet;\n+import org.apache.drill.test.rowSet.RowSetUtilities;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import java.time.LocalDateTime;\n+import java.util.HashMap;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestHTTPDLogReader extends ClusterTest {\n+\n+  @ClassRule\n+  public static final BaseDirTestWatcher dirTestWatcher = new BaseDirTestWatcher();\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));\n+    defineHTTPDPlugin();\n+  }\n+\n+  private static void defineHTTPDPlugin() throws ExecutionSetupException {\n+\n+    // Create an instance of the regex config.\n+    // Note: we can\"t use the \".log\" extension; the Drill .gitignore\n+    // file ignores such files, so they\"ll never get committed. Instead,\n+    // make up a fake suffix.\n+    HttpdLogFormatConfig sampleConfig = new HttpdLogFormatConfig();\n+    sampleConfig.setLogFormat(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\");\n+\n+    // Define a temporary format plugin for the \"cp\" storage plugin.\n+    Drillbit drillbit = cluster.drillbit();\n+    final StoragePluginRegistry pluginRegistry = drillbit.getContext().getStorage();\n+    final FileSystemPlugin plugin = (FileSystemPlugin) pluginRegistry.getPlugin(\"cp\");\n+    final FileSystemConfig pluginConfig = (FileSystemConfig) plugin.getConfig();\n+    pluginConfig.getFormats().put(\"sample\", sampleConfig);\n+    pluginRegistry.createOrUpdate(\"cp\", pluginConfig, false);\n+  }\n+\n+  @Test\n+  public void testDateField() throws RpcException {\n+    String sql = \"SELECT `request_receive_time` FROM cp.`httpd/hackers-access-small.httpd` LIMIT 5\";\n+    RowSet results = client.queryBuilder().sql(sql).rowSet();\n+\n+    BatchSchema expectedSchema = new SchemaBuilder()\n+            .addNullable(\"request_receive_time\", MinorType.TIMESTAMP)\n+            .build();\n+    RowSet expected = client.rowSetBuilder(expectedSchema)\n+            .addRow(1445742685000L)\n+            .addRow(1445742686000L)\n+            .addRow(1445742687000L)\n+            .addRow(1445743471000L)\n+            .addRow(1445743472000L)\n+            .build();\n+\n+    RowSetUtilities.verify(expected, results);\n+  }\n+\n+  @Test\n+  public void testSelectColumns() throws Exception {\n+    String sql = \"SELECT request_referer_ref,\\n\" +\n+            \"request_receive_time_last_time,\\n\" +\n+            \"request_firstline_uri_protocol,\\n\" +\n+            \"request_receive_time_microsecond,\\n\" +\n+            \"request_receive_time_last_microsecond__utc,\\n\" +\n+            \"request_firstline_original_protocol,\\n\" +\n+            \"request_firstline_original_uri_host,\\n\" +\n+            \"request_referer_host,\\n\" +\n+            \"request_receive_time_month__utc,\\n\" +\n+            \"request_receive_time_last_minute,\\n\" +\n+            \"request_firstline_protocol_version,\\n\" +\n+            \"request_receive_time_time__utc,\\n\" +\n+            \"request_referer_last_ref,\\n\" +\n+            \"request_receive_time_last_timezone,\\n\" +\n+            \"request_receive_time_last_weekofweekyear,\\n\" +\n+            \"request_referer_last,\\n\" +\n+            \"request_receive_time_minute,\\n\" +\n+            \"connection_client_host_last,\\n\" +\n+            \"request_receive_time_last_millisecond__utc,\\n\" +\n+            \"request_firstline_original_uri,\\n\" +\n+            \"request_firstline,\\n\" +\n+            \"request_receive_time_nanosecond,\\n\" +\n+            \"request_receive_time_last_millisecond,\\n\" +\n+            \"request_receive_time_day,\\n\" +\n+            \"request_referer_port,\\n\" +\n+            \"request_firstline_original_uri_port,\\n\" +\n+            \"request_receive_time_year,\\n\" +\n+            \"request_receive_time_last_date,\\n\" +\n+            \"request_receive_time_last_time__utc,\\n\" +\n+            \"request_receive_time_last_hour__utc,\\n\" +\n+            \"request_firstline_original_protocol_version,\\n\" +\n+            \"request_firstline_original_method,\\n\" +\n+            \"request_receive_time_last_year__utc,\\n\" +\n+            \"request_firstline_uri,\\n\" +\n+            \"request_referer_last_host,\\n\" +\n+            \"request_receive_time_last_minute__utc,\\n\" +\n+            \"request_receive_time_weekofweekyear,\\n\" +\n+            \"request_firstline_uri_userinfo,\\n\" +\n+            \"request_receive_time_epoch,\\n\" +\n+            \"connection_client_logname,\\n\" +\n+            \"response_body_bytes,\\n\" +\n+            \"request_receive_time_nanosecond__utc,\\n\" +\n+            \"request_firstline_protocol,\\n\" +\n+            \"request_receive_time_microsecond__utc,\\n\" +\n+            \"request_receive_time_hour,\\n\" +\n+            \"request_firstline_uri_host,\\n\" +\n+            \"request_referer_last_port,\\n\" +\n+            \"request_receive_time_last_epoch,\\n\" +\n+            \"request_receive_time_last_weekyear__utc,\\n\" +\n+            \"request_useragent,\\n\" +\n+            \"request_receive_time_weekyear,\\n\" +\n+            \"request_receive_time_timezone,\\n\" +\n+            \"response_body_bytesclf,\\n\" +\n+            \"request_receive_time_last_date__utc,\\n\" +\n+            \"request_receive_time_millisecond__utc,\\n\" +\n+            \"request_referer_last_protocol,\\n\" +\n+            \"request_status_last,\\n\" +\n+            \"request_firstline_uri_query,\\n\" +\n+            \"request_receive_time_minute__utc,\\n\" +\n+            \"request_firstline_original_uri_protocol,\\n\" +\n+            \"request_referer_query,\\n\" +\n+            \"request_receive_time_date,\\n\" +\n+            \"request_firstline_uri_port,\\n\" +\n+            \"request_receive_time_last_second__utc,\\n\" +\n+            \"request_referer_last_userinfo,\\n\" +\n+            \"request_receive_time_last_second,\\n\" +\n+            \"request_receive_time_last_monthname__utc,\\n\" +\n+            \"request_firstline_method,\\n\" +\n+            \"request_receive_time_last_month__utc,\\n\" +\n+            \"request_receive_time_millisecond,\\n\" +\n+            \"request_receive_time_day__utc,\\n\" +\n+            \"request_receive_time_year__utc,\\n\" +\n+            \"request_receive_time_weekofweekyear__utc,\\n\" +\n+            \"request_receive_time_second,\\n\" +\n+            \"request_firstline_original_uri_ref,\\n\" +\n+            \"connection_client_logname_last,\\n\" +\n+            \"request_receive_time_last_year,\\n\" +\n+            \"request_firstline_original_uri_path,\\n\" +\n+            \"connection_client_host,\\n\" +\n+            \"request_firstline_original_uri_query,\\n\" +\n+            \"request_referer_userinfo,\\n\" +\n+            \"request_receive_time_last_monthname,\\n\" +\n+            \"request_referer_path,\\n\" +\n+            \"request_receive_time_monthname,\\n\" +\n+            \"request_receive_time_last_month,\\n\" +\n+            \"request_referer_last_query,\\n\" +\n+            \"request_firstline_uri_ref,\\n\" +\n+            \"request_receive_time_last_day,\\n\" +\n+            \"request_receive_time_time,\\n\" +\n+            \"request_receive_time_last_weekofweekyear__utc,\\n\" +\n+            \"request_useragent_last,\\n\" +\n+            \"request_receive_time_last_weekyear,\\n\" +\n+            \"request_receive_time_last_microsecond,\\n\" +\n+            \"request_firstline_original,\\n\" +\n+            \"request_referer_last_path,\\n\" +\n+            \"request_receive_time_month,\\n\" +\n+            \"request_receive_time_last_day__utc,\\n\" +\n+            \"request_referer,\\n\" +\n+            \"request_referer_protocol,\\n\" +\n+            \"request_receive_time_monthname__utc,\\n\" +\n+            \"response_body_bytes_last,\\n\" +\n+            \"request_receive_time,\\n\" +\n+            \"request_receive_time_last_nanosecond,\\n\" +\n+            \"request_firstline_uri_path,\\n\" +\n+            \"request_firstline_original_uri_userinfo,\\n\" +\n+            \"request_receive_time_date__utc,\\n\" +\n+            \"request_receive_time_last,\\n\" +\n+            \"request_receive_time_last_nanosecond__utc,\\n\" +\n+            \"request_receive_time_last_hour,\\n\" +\n+            \"request_receive_time_hour__utc,\\n\" +\n+            \"request_receive_time_second__utc,\\n\" +\n+            \"connection_client_user_last,\\n\" +\n+            \"request_receive_time_weekyear__utc,\\n\" +\n+            \"connection_client_user\\n\" +\n+            \"FROM cp.`httpd/hackers-access-small.httpd`\\n\" +\n+            \"LIMIT 1\";\n+\n+    testBuilder()\n+            .sqlQuery(sql)\n+            .unOrdered()\n+            .baselineColumns(\"request_referer_ref\", \"request_receive_time_last_time\", \"request_firstline_uri_protocol\", \"request_receive_time_microsecond\", \"request_receive_time_last_microsecond__utc\", \"request_firstline_original_protocol\", \"request_firstline_original_uri_host\", \"request_referer_host\", \"request_receive_time_month__utc\", \"request_receive_time_last_minute\", \"request_firstline_protocol_version\", \"request_receive_time_time__utc\", \"request_referer_last_ref\", \"request_receive_time_last_timezone\", \"request_receive_time_last_weekofweekyear\", \"request_referer_last\", \"request_receive_time_minute\", \"connection_client_host_last\", \"request_receive_time_last_millisecond__utc\", \"request_firstline_original_uri\", \"request_firstline\", \"request_receive_time_nanosecond\", \"request_receive_time_last_millisecond\", \"request_receive_time_day\", \"request_referer_port\", \"request_firstline_original_uri_port\", \"request_receive_time_year\", \"request_receive_time_last_date\", \"request_receive_time_last_time__utc\", \"request_receive_time_last_hour__utc\", \"request_firstline_original_protocol_version\", \"request_firstline_original_method\", \"request_receive_time_last_year__utc\", \"request_firstline_uri\", \"request_referer_last_host\", \"request_receive_time_last_minute__utc\", \"request_receive_time_weekofweekyear\", \"request_firstline_uri_userinfo\", \"request_receive_time_epoch\", \"connection_client_logname\", \"response_body_bytes\", \"request_receive_time_nanosecond__utc\", \"request_firstline_protocol\", \"request_receive_time_microsecond__utc\", \"request_receive_time_hour\", \"request_firstline_uri_host\", \"request_referer_last_port\", \"request_receive_time_last_epoch\", \"request_receive_time_last_weekyear__utc\", \"request_useragent\", \"request_receive_time_weekyear\", \"request_receive_time_timezone\", \"response_body_bytesclf\", \"request_receive_time_last_date__utc\", \"request_receive_time_millisecond__utc\", \"request_referer_last_protocol\", \"request_status_last\", \"request_firstline_uri_query\", \"request_receive_time_minute__utc\", \"request_firstline_original_uri_protocol\", \"request_referer_query\", \"request_receive_time_date\", \"request_firstline_uri_port\", \"request_receive_time_last_second__utc\", \"request_referer_last_userinfo\", \"request_receive_time_last_second\", \"request_receive_time_last_monthname__utc\", \"request_firstline_method\", \"request_receive_time_last_month__utc\", \"request_receive_time_millisecond\", \"request_receive_time_day__utc\", \"request_receive_time_year__utc\", \"request_receive_time_weekofweekyear__utc\", \"request_receive_time_second\", \"request_firstline_original_uri_ref\", \"connection_client_logname_last\", \"request_receive_time_last_year\", \"request_firstline_original_uri_path\", \"connection_client_host\", \"request_firstline_original_uri_query\", \"request_referer_userinfo\", \"request_receive_time_last_monthname\", \"request_referer_path\", \"request_receive_time_monthname\", \"request_receive_time_last_month\", \"request_referer_last_query\", \"request_firstline_uri_ref\", \"request_receive_time_last_day\", \"request_receive_time_time\", \"request_receive_time_last_weekofweekyear__utc\", \"request_useragent_last\", \"request_receive_time_last_weekyear\", \"request_receive_time_last_microsecond\", \"request_firstline_original\", \"request_referer_last_path\", \"request_receive_time_month\", \"request_receive_time_last_day__utc\", \"request_referer\", \"request_referer_protocol\", \"request_receive_time_monthname__utc\", \"response_body_bytes_last\", \"request_receive_time\", \"request_receive_time_last_nanosecond\", \"request_firstline_uri_path\", \"request_firstline_original_uri_userinfo\", \"request_receive_time_date__utc\", \"request_receive_time_last\", \"request_receive_time_last_nanosecond__utc\", \"request_receive_time_last_hour\", \"request_receive_time_hour__utc\", \"request_receive_time_second__utc\", \"connection_client_user_last\", \"request_receive_time_weekyear__utc\", \"connection_client_user\")\n+            .baselineValues(null, \"04:11:25\", null, 0L, 0L, \"HTTP\", null, \"howto.basjes.nl\", 10L, 11L, \"1.1\", \"03:11:25\", null, null, 43L, \"http://howto.basjes.nl/\", 11L, \"195.154.46.135\", 0L, \"/linux/doing-pxe-without-dhcp-control\", \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\", 0L, 0L, 25L, null, null, 2015L, \"2015-10-25\", \"03:11:25\", 3L, \"1.1\", \"GET\", 2015L, \"/linux/doing-pxe-without-dhcp-control\", \"howto.basjes.nl\", 11L, 43L, null, 1445742685000L, null, 24323L, 0L, \"HTTP\", 0L, 4L, null, null, 1445742685000L, 2015L, \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\", 2015L, null, 24323L, \"2015-10-25\", 0L, \"http\", \"200\", \"\", 11L, null, \"\", \"2015-10-25\", null, 25L, null, 25L, \"October\", \"GET\", 10L, 0L, 25L, 2015L, 43L, 25L, null, null, 2015L, \"/linux/doing-pxe-without-dhcp-control\", \"195.154.46.135\", \"\", null, \"October\", \"/\", \"October\", 10L, \"\", null, 25L, \"04:11:25\", 43L, \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\", 2015L, 0L, \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\", \"/\", 10L, 25L, \"http://howto.basjes.nl/\", \"http\", \"October\", 24323L, LocalDateTime.parse(\"2015-10-25T03:11:25\"), 0L, \"/linux/doing-pxe-without-dhcp-control\", null, \"2015-10-25\", LocalDateTime.parse(\"2015-10-25T03:11:25\"), 0L, 4L, 3L, 25L, null, 2015L, null)\n+            .go();\n+  }\n+\n+\n+  @Test\n+  public void testCount() throws Exception {\n+    String sql = \"SELECT COUNT(*) FROM cp.`httpd/hackers-access-small.httpd`\";\n+    long result = client.queryBuilder().sql(sql).singletonLong();\n+    assertEquals(10, result);\n+  }\n+\n+  @Test\n+  public void testStar() throws Exception {\n+    String sql = \"SELECT * FROM cp.`httpd/hackers-access-small.httpd` LIMIT 1\";\n+\n+    testBuilder()\n+            .sqlQuery(sql)\n+            .unOrdered()\n+            .baselineColumns(\"request_referer_ref\",\"request_receive_time_last_time\",\"request_firstline_uri_protocol\",\"request_receive_time_microsecond\",\"request_receive_time_last_microsecond__utc\",\"request_firstline_original_uri_query_$\",\"request_firstline_original_protocol\",\"request_firstline_original_uri_host\",\"request_referer_host\",\"request_receive_time_month__utc\",\"request_receive_time_last_minute\",\"request_firstline_protocol_version\",\"request_receive_time_time__utc\",\"request_referer_last_ref\",\"request_receive_time_last_timezone\",\"request_receive_time_last_weekofweekyear\",\"request_referer_last\",\"request_receive_time_minute\",\"connection_client_host_last\",\"request_receive_time_last_millisecond__utc\",\"request_firstline_original_uri\",\"request_firstline\",\"request_receive_time_nanosecond\",\"request_receive_time_last_millisecond\",\"request_receive_time_day\",\"request_referer_port\",\"request_firstline_original_uri_port\",\"request_receive_time_year\",\"request_receive_time_last_date\",\"request_referer_query_$\",\"request_receive_time_last_time__utc\",\"request_receive_time_last_hour__utc\",\"request_firstline_original_protocol_version\",\"request_firstline_original_method\",\"request_receive_time_last_year__utc\",\"request_firstline_uri\",\"request_referer_last_host\",\"request_receive_time_last_minute__utc\",\"request_receive_time_weekofweekyear\",\"request_firstline_uri_userinfo\",\"request_receive_time_epoch\",\"connection_client_logname\",\"response_body_bytes\",\"request_receive_time_nanosecond__utc\",\"request_firstline_protocol\",\"request_receive_time_microsecond__utc\",\"request_receive_time_hour\",\"request_firstline_uri_host\",\"request_referer_last_port\",\"request_receive_time_last_epoch\",\"request_receive_time_last_weekyear__utc\",\"request_receive_time_weekyear\",\"request_receive_time_timezone\",\"response_body_bytesclf\",\"request_receive_time_last_date__utc\",\"request_useragent_last\",\"request_useragent\",\"request_receive_time_millisecond__utc\",\"request_referer_last_protocol\",\"request_status_last\",\"request_firstline_uri_query\",\"request_receive_time_minute__utc\",\"request_firstline_original_uri_protocol\",\"request_referer_query\",\"request_receive_time_date\",\"request_firstline_uri_port\",\"request_receive_time_last_second__utc\",\"request_referer_last_userinfo\",\"request_receive_time_last_second\",\"request_receive_time_last_monthname__utc\",\"request_firstline_method\",\"request_receive_time_last_month__utc\",\"request_receive_time_millisecond\",\"request_receive_time_day__utc\",\"request_receive_time_year__utc\",\"request_receive_time_weekofweekyear__utc\",\"request_receive_time_second\",\"request_firstline_original_uri_ref\",\"connection_client_logname_last\",\"request_receive_time_last_year\",\"request_firstline_original_uri_path\",\"connection_client_host\",\"request_referer_last_query_$\",\"request_firstline_original_uri_query\",\"request_referer_userinfo\",\"request_receive_time_last_monthname\",\"request_referer_path\",\"request_receive_time_monthname\",\"request_receive_time_last_month\",\"request_referer_last_query\",\"request_firstline_uri_ref\",\"request_receive_time_last_day\",\"request_receive_time_time\",\"request_receive_time_last_weekofweekyear__utc\",\"request_receive_time_last_weekyear\",\"request_receive_time_last_microsecond\",\"request_firstline_original\",\"request_firstline_uri_query_$\",\"request_referer_last_path\",\"request_receive_time_month\",\"request_receive_time_last_day__utc\",\"request_referer\",\"request_referer_protocol\",\"request_receive_time_monthname__utc\",\"response_body_bytes_last\",\"request_receive_time\",\"request_receive_time_last_nanosecond\",\"request_firstline_uri_path\",\"request_firstline_original_uri_userinfo\",\"request_receive_time_date__utc\",\"request_receive_time_last\",\"request_receive_time_last_nanosecond__utc\",\"request_receive_time_last_hour\",\"request_receive_time_hour__utc\",\"request_receive_time_second__utc\",\"connection_client_user_last\",\"request_receive_time_weekyear__utc\",\"connection_client_user\")\n+            .baselineValues(null,\"04:11:25\",null,0L,0L,new HashMap<>(),\"HTTP\",null,\"howto.basjes.nl\",10L,11L,\"1.1\",\"03:11:25\",null,null,43L,\"http://howto.basjes.nl/\",11L,\"195.154.46.135\",0L,\"/linux/doing-pxe-without-dhcp-control\",\"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\",0L,0L,25L,null,null,2015L,\"2015-10-25\",new HashMap<>(),\"03:11:25\",3L,\"1.1\",\"GET\",2015L,\"/linux/doing-pxe-without-dhcp-control\",\"howto.basjes.nl\",11L,43L,null,1445742685000L,null,24323L,0L,\"HTTP\",0L,4L,null,null,1445742685000L,2015L,2015L,null,24323L,\"2015-10-25\",\"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\",\"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\",0L,\"http\",\"200\",\"\",11L,null,\"\",\"2015-10-25\",null,25L,null,25L,\"October\",\"GET\",10L,0L,25L,2015L,43L,25L,null,null,2015L,\"/linux/doing-pxe-without-dhcp-control\",\"195.154.46.135\",new HashMap<>(),\"\",null,\"October\",\"/\",\"October\",10L,\"\",null,25L,\"04:11:25\",43L,2015L,0L,\"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\",new HashMap<>(),\"/\",10L,25L,\"http://howto.basjes.nl/\",\"http\",\"October\",24323L,LocalDateTime.parse(\"2015-10-25T03:11:25\"),0L,\"/linux/doing-pxe-without-dhcp-control\",null,\"2015-10-25\",LocalDateTime.parse(\"2015-10-25T03:11:25\"),0L,4L,3L,25L,null,2015L,null)\n+            .go();\n+  }\n+}",
                "additions": 237,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "status": "added",
                "changes": 237,
                "deletions": 0,
                "sha": "ac85a920db82769ba8ae8970d3eb0db29ea1cb4f",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            },
            {
                "patch": "@@ -0,0 +1,10 @@\n+195.154.46.135 - - [25/Oct/2015:04:11:25 +0100] \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\" 200 24323 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+23.95.237.180 - - [25/Oct/2015:04:11:26 +0100] \"GET /join_form HTTP/1.0\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+23.95.237.180 - - [25/Oct/2015:04:11:27 +0100] \"POST /join_form HTTP/1.1\" 302 9093 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+158.222.5.157 - - [25/Oct/2015:04:24:31 +0100] \"GET /join_form HTTP/1.0\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:32 +0100] \"POST /join_form HTTP/1.1\" 302 9093 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:37 +0100] \"GET /acl_users/credentials_cookie_auth/require_login?came_from=http%3A//howto.basjes.nl/join_form HTTP/1.1\" 200 10716 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:39 +0100] \"GET /login_form HTTP/1.1\" 200 10543 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:41 +0100] \"POST /login_form HTTP/1.1\" 200 16810 \"http://howto.basjes.nl/login_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+5.39.5.5 - - [25/Oct/2015:04:32:22 +0100] \"GET /join_form HTTP/1.1\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:34.0) Gecko/20100101 Firefox/34.0\"\n+180.180.64.16 - - [25/Oct/2015:04:34:37 +0100] \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\" 200 24323 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "status": "added",
                "changes": 10,
                "deletions": 0,
                "sha": "98af53288f09c7dcab8d5f85680f80741a112fb6",
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "filename": "exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd?ref=bf1bdec6069f6fdd2132608450357edea47d328c"
            }
        ],
        "bug_id": "drill_3",
        "parent": "https://github.com/apache/drill/commit/2364b02175bec69cee2f9ceb4e52e1333da39f70",
        "message": "DRILL-7021: HTTPD Throws NPE and Doesn't Recognize Timeformat",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
        "file": [
            {
                "patch": "@@ -55,7 +55,7 @@ public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext)\n   // return the memory computed for a physical operator on a drillbitendpoint.\n   public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {\n     return (endpoint, operator) -> {\n-      if (planHasMemory) {\n+      if (!planHasMemory) {\n         return operators.get(endpoint).get(operator);\n       }\n       else {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "5cd4a09e420e52dc6e84ff3d29df126b10feb239",
                "blob_url": "https://github.com/apache/drill/blob/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java?ref=d29a5b1749a77b863be3eac1423f4ecf6d244ebf"
            },
            {
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.drill.exec.proto.UserProtos;\n import org.apache.drill.exec.rpc.user.UserSession;\n import org.apache.drill.exec.server.DrillbitContext;\n+import org.apache.drill.exec.work.foreman.rm.EmbeddedQueryQueue;\n import org.apache.drill.shaded.guava.com.google.common.collect.Iterables;\n import org.apache.drill.test.ClientFixture;\n import org.apache.drill.test.ClusterFixture;\n@@ -224,4 +225,18 @@ public void TestTwoMajorFragmentWithSortyProjectAndScan() throws Exception {\n     parallelizer.adjustMemory(planningSet, createSet(planningSet.getRootWrapper()), activeEndpoints);\n     assertTrue(\"memory requirement is different\", Iterables.all(resources.entrySet(), (e) -> e.getValue().getMemory() == 481490));\n   }\n+\n+  @Test\n+  public void TestZKBasedQueue() throws Exception {\n+    String sql = \"select * from cp.`employee.json`\";\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher).configProperty(EmbeddedQueryQueue.ENABLED, true);\n+\n+    try (ClusterFixture cluster = builder.build();\n+         ClientFixture client = cluster.clientFixture()) {\n+      client\n+        .queryBuilder()\n+        .sql(sql)\n+        .run();\n+    }\n+  }\n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "status": "modified",
                "changes": 15,
                "deletions": 0,
                "sha": "891b4a6a2262ce01dd85a70fdcfeaf96426d37fe",
                "blob_url": "https://github.com/apache/drill/blob/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java?ref=d29a5b1749a77b863be3eac1423f4ecf6d244ebf"
            }
        ],
        "bug_id": "drill_4",
        "parent": "https://github.com/apache/drill/commit/54384a992a0742aeab23afa82c7b7f4adcd388d3",
        "message": "DRILL-7146: Query failing with NPE when ZK queue is enabled.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/624634d88ab870e3edb1648743097242522e58d6",
        "file": [
            {
                "patch": "@@ -129,21 +129,6 @@\n \n   private File tmpJavaScriptDir;\n \n-  public File getTmpJavaScriptDir() {\n-    if (tmpJavaScriptDir == null) {\n-      tmpJavaScriptDir = org.apache.drill.shaded.guava.com.google.common.io.Files.createTempDir();\n-      tmpJavaScriptDir.deleteOnExit();\n-      //Perform All auto generated files at this point\n-      try {\n-        generateOptionsDescriptionJSFile();\n-        generateFunctionJS();\n-      } catch (IOException e) {\n-        logger.error(\"Unable to create temp dir for JavaScripts. {}\", e);\n-      }\n-    }\n-    return tmpJavaScriptDir;\n-  }\n-\n   /**\n    * Create Jetty based web server.\n    *\n@@ -246,8 +231,8 @@ private ServletContextHandler createServletContextHandler(final boolean authEnab\n     //Add Local path resource (This will allow access to dynamically created files like JavaScript)\n     final ServletHolder dynamicHolder = new ServletHolder(\"dynamic\", DefaultServlet.class);\n     //Skip if unable to get a temp directory (e.g. during Unit tests)\n-    if (getTmpJavaScriptDir() != null) {\n-      dynamicHolder.setInitParameter(\"resourceBase\", getTmpJavaScriptDir().getAbsolutePath());\n+    if (getOrCreateTmpJavaScriptDir() != null) {\n+      dynamicHolder.setInitParameter(\"resourceBase\", getOrCreateTmpJavaScriptDir().getAbsolutePath());\n       dynamicHolder.setInitParameter(\"dirAllowed\", \"true\");\n       dynamicHolder.setInitParameter(\"pathInfoOnly\", \"true\");\n       servletContextHandler.addServlet(dynamicHolder, \"/dynamic/*\");\n@@ -467,43 +452,61 @@ public void close() throws Exception {\n     if (embeddedJetty != null) {\n       embeddedJetty.stop();\n     }\n-    //Deleting temp directory\n-    FileUtils.deleteDirectory(getTmpJavaScriptDir());\n+    // Deleting temp directory\n+    FileUtils.deleteQuietly(tmpJavaScriptDir);\n   }\n \n+  /**\n+   * Creates if not exists, and returns File for temporary Javascript directory\n+   * @return File handle\n+   */\n+  public File getOrCreateTmpJavaScriptDir() {\n+    if (tmpJavaScriptDir == null && this.drillbit.getContext() != null) {\n+      tmpJavaScriptDir = org.apache.drill.shaded.guava.com.google.common.io.Files.createTempDir();\n+      // Perform All auto generated files at this point\n+      try {\n+        generateOptionsDescriptionJSFile();\n+        generateFunctionJS();\n+      } catch (IOException e) {\n+        logger.error(\"Unable to create temp dir for JavaScripts. {}\", e);\n+      }\n+    }\n+    return tmpJavaScriptDir;\n+  }\n+\n+\n   /**\n    * Generate Options Description JavaScript to serve http://drillhost/options ACE library search features\n    * @throws IOException\n    */\n   private void generateOptionsDescriptionJSFile() throws IOException {\n-    //Obtain list of Options & their descriptions\n+    // Obtain list of Options & their descriptions\n     OptionManager optionManager = this.drillbit.getContext().getOptionManager();\n     OptionList publicOptions = optionManager.getPublicOptionList();\n     List<OptionValue> options = new ArrayList<>(publicOptions);\n-    //Add internal options\n+    // Add internal options\n     OptionList internalOptions = optionManager.getInternalOptionList();\n     options.addAll(internalOptions);\n     Collections.sort(options);\n     int numLeftToWrite = options.size();\n \n-    //Template source Javascript file\n+    // Template source Javascript file\n     InputStream optionsDescripTemplateStream = Resource.newClassPathResource(OPTIONS_DESCRIBE_TEMPLATE_JS).getInputStream();\n-    //Generated file\n-    File optionsDescriptionFile = new File(getTmpJavaScriptDir(), OPTIONS_DESCRIBE_JS);\n+    // Generated file\n+    File optionsDescriptionFile = new File(getOrCreateTmpJavaScriptDir(), OPTIONS_DESCRIBE_JS);\n     final String file_content_footer = \"};\";\n-    optionsDescriptionFile.deleteOnExit();\n-    //Create a copy of a template and write with that!\n+    // Create a copy of a template and write with that!\n     java.nio.file.Files.copy(optionsDescripTemplateStream, optionsDescriptionFile.toPath());\n     logger.info(\"Will write {} descriptions to {}\", numLeftToWrite, optionsDescriptionFile.getAbsolutePath());\n \n     try (BufferedWriter writer = new BufferedWriter(new FileWriter(optionsDescriptionFile, true))) {\n-      //Iterate through options\n+      // Iterate through options\n       for (OptionValue option : options) {\n         numLeftToWrite--;\n         String optionName = option.getName();\n         OptionDescription optionDescription = optionManager.getOptionDefinition(optionName).getValidator().getOptionDescription();\n         if (optionDescription != null) {\n-          //Note: We don't need to worry about short descriptions for WebUI, since they will never be explicitly accessed from the map\n+          // Note: We don't need to worry about short descriptions for WebUI, since they will never be explicitly accessed from the map\n           writer.append(\"  \\\"\").append(optionName).append(\"\\\" : \\\"\")\n           .append(StringEscapeUtils.escapeEcmaScript(optionDescription.getDescription()))\n           .append( numLeftToWrite > 0 ? \"\\\",\" : \"\\\"\");\n@@ -521,14 +524,14 @@ private void generateOptionsDescriptionJSFile() throws IOException {\n    * @throws IOException\n    */\n   private void generateFunctionJS() throws IOException {\n-    //Naturally ordered set of function names\n+    // Naturally ordered set of function names\n     TreeSet<String> functionSet = new TreeSet<>();\n-    //Extracting ONLY builtIn functions (i.e those already available)\n+    // Extracting ONLY builtIn functions (i.e those already available)\n     List<FunctionHolder> builtInFuncHolderList = this.drillbit.getContext().getFunctionImplementationRegistry().getLocalFunctionRegistry()\n         .getAllJarsWithFunctionsHolders().get(LocalFunctionRegistry.BUILT_IN);\n \n-    //Build List of 'usable' functions (i.e. functions that start with an alphabet and can be autocompleted by the ACE library)\n-    //Example of 'unusable' functions would be operators like '<', '!'\n+    // Build List of 'usable' functions (i.e. functions that start with an alphabet and can be autocompleted by the ACE library)\n+    // Example of 'unusable' functions would be operators like '<', '!'\n     int skipCount = 0;\n     for (FunctionHolder builtInFunctionHolder : builtInFuncHolderList) {\n       String name = builtInFunctionHolder.getName();\n@@ -541,22 +544,21 @@ private void generateFunctionJS() throws IOException {\n     }\n     logger.debug(\"{} functions will not be available in WebUI\", skipCount);\n \n-    //Generated file\n-    File functionsListFile = new File(getTmpJavaScriptDir(), ACE_MODE_SQL_JS);\n-    functionsListFile.deleteOnExit();\n-    //Template source Javascript file\n+    // Generated file\n+    File functionsListFile = new File(getOrCreateTmpJavaScriptDir(), ACE_MODE_SQL_JS);\n+    // Template source Javascript file\n     try (InputStream aceModeSqlTemplateStream = Resource.newClassPathResource(ACE_MODE_SQL_TEMPLATE_JS).getInputStream()) {\n-      //Create a copy of a template and write with that!\n+      // Create a copy of a template and write with that!\n       java.nio.file.Files.copy(aceModeSqlTemplateStream, functionsListFile.toPath());\n     }\n \n-    //Construct String\n+    // Construct String\n     String funcListString = String.join(\"|\", functionSet);\n \n     Path path = Paths.get(functionsListFile.getPath());\n     try (Stream<String> lines = Files.lines(path)) {\n       List <String> replaced =\n-          lines //Replacing first occurrence\n+          lines // Replacing first occurrence\n             .map(line -> line.replaceFirst(DRILL_FUNCTIONS_PLACEHOLDER, funcListString))\n             .collect(Collectors.toList());\n       Files.write(path, replaced);",
                "additions": 41,
                "raw_url": "https://github.com/apache/drill/raw/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "status": "modified",
                "changes": 80,
                "deletions": 39,
                "sha": "cdde4aa3ea9dc880d0e5beee78fc0829191741f6",
                "blob_url": "https://github.com/apache/drill/blob/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java?ref=624634d88ab870e3edb1648743097242522e58d6"
            },
            {
                "patch": "@@ -17,11 +17,13 @@\n  */\n package org.apache.drill.test;\n \n+import org.apache.commons.lang3.reflect.FieldUtils;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n import org.apache.drill.exec.server.Drillbit;\n+import org.apache.drill.exec.server.rest.WebServer;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n@@ -30,17 +32,21 @@\n import org.junit.rules.TestRule;\n \n import java.io.BufferedWriter;\n+import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n import java.io.PrintWriter;\n+import java.lang.reflect.Field;\n import java.net.HttpURLConnection;\n import java.net.URL;\n import java.nio.file.Path;\n import java.util.Collection;\n \n import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n @Category({SlowTest.class})\n@@ -205,6 +211,42 @@ public void testDrillbitWithSamePortContainsShutdownThread() throws Exception {\n     }\n   }\n \n+  @Test // DRILL-7056\n+  public void testDrillbitTempDir() throws Exception {\n+    File originalDrillbitTempDir = null;\n+    ClusterFixtureBuilder fixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher).withLocalZk()\n+        .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)\n+        .configProperty(ExecConstants.INITIAL_USER_PORT, QueryTestUtil.getFreePortNumber(31170, 300))\n+        .configProperty(ExecConstants.INITIAL_BIT_PORT, QueryTestUtil.getFreePortNumber(31180, 300));\n+\n+    try (ClusterFixture fixture = fixtureBuilder.build();\n+        Drillbit twinDrillbitOnSamePort = new Drillbit(fixture.config(),\n+            fixtureBuilder.configBuilder().getDefinitions(), fixture.serviceSet())) {\n+      // Assert preconditions :\n+      //      1. First drillbit instance should be started normally\n+      //      2. Second instance startup should fail, because ports are occupied by the first one\n+      Drillbit originalDrillbit = fixture.drillbit();\n+      assertNotNull(\"First drillbit instance should be initialized\", originalDrillbit);\n+      originalDrillbitTempDir = getWebServerTempDirPath(originalDrillbit);\n+      assertTrue(\"First drillbit instance should have a temporary Javascript dir initialized\", originalDrillbitTempDir.exists());\n+      try {\n+        twinDrillbitOnSamePort.run();\n+        fail(\"Invocation of 'twinDrillbitOnSamePort.run()' should throw UserException\");\n+      } catch (UserException userEx) {\n+        assertThat(userEx.getMessage(), containsString(\"RESOURCE ERROR: Drillbit could not bind to port\"));\n+      }\n+    }\n+    // Verify deletion\n+    assertFalse(\"First drillbit instance should have a temporary Javascript dir deleted\", originalDrillbitTempDir.exists());\n+  }\n+\n+  private static File getWebServerTempDirPath(Drillbit drillbit) throws IllegalAccessException {\n+    Field webServerField = FieldUtils.getField(drillbit.getClass(), \"webServer\", true);\n+    WebServer webServerHandle = (WebServer) FieldUtils.readField(webServerField, drillbit, true);\n+    File webServerTempDirPath = webServerHandle.getOrCreateTmpJavaScriptDir();\n+    return webServerTempDirPath;\n+  }\n+\n   private static boolean waitAndAssertDrillbitCount(ClusterFixture cluster, int zkRefresh) throws InterruptedException {\n \n     while (true) {",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "status": "modified",
                "changes": 42,
                "deletions": 0,
                "sha": "d74f1e691f9f3bcb506612c5c5f291a27d23cd58",
                "blob_url": "https://github.com/apache/drill/blob/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java?ref=624634d88ab870e3edb1648743097242522e58d6"
            }
        ],
        "bug_id": "drill_5",
        "parent": "https://github.com/apache/drill/commit/5abcd88642e224beb8252185f938a5e42387b18e",
        "message": "DRILL-7056: Drill fails with NPE when starting in distributed mode & 31010 port is used\ncloses #1656",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef",
        "file": [
            {
                "patch": "@@ -114,8 +114,13 @@ public RecordBatchMemoryManager getRecordBatchMemoryManager() {\n \n     @Override\n     public void update() {\n+      update(incoming);\n+    }\n+\n+    @Override\n+    public void update(RecordBatch incomingRecordBatch) {\n       // Get sizing information for the batch.\n-      setRecordBatchSizer(new RecordBatchSizer(incoming));\n+      setRecordBatchSizer(new RecordBatchSizer(incomingRecordBatch));\n \n       int fieldId = 0;\n       int newOutgoingRowWidth = 0;",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "status": "modified",
                "changes": 7,
                "deletions": 1,
                "sha": "ba928ae8f2dbff276c67d3c859cb96458f4fac8c",
                "blob_url": "https://github.com/apache/drill/blob/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java?ref=c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef"
            },
            {
                "patch": "@@ -584,6 +584,11 @@ public AggOutcome doWork() {\n         currentBatchRecordCount = incoming.getRecordCount(); // initialize for first non empty batch\n         // Calculate the number of partitions based on actual incoming data\n         delayedSetup();\n+        // Update the record batch manager since this is the first batch with data; we need to\n+        // perform the update before any processing.\n+        // NOTE - We pass the incoming record batch explicitly because it could be a spilled record (different\n+        //        from the instance owned by the HashAggBatch).\n+        outgoing.getRecordBatchMemoryManager().update(incoming);\n       }\n \n       //\n@@ -666,7 +671,9 @@ public AggOutcome doWork() {\n           // remember EMIT, but continue like handling OK\n \n         case OK:\n-          outgoing.getRecordBatchMemoryManager().update();\n+          // NOTE - We pass the incoming record batch explicitly because it could be a spilled record (different\n+          //        from the instance owned by the HashAggBatch).\n+          outgoing.getRecordBatchMemoryManager().update(incoming);\n \n           currentBatchRecordCount = incoming.getRecordCount(); // size of next batch\n ",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "status": "modified",
                "changes": 9,
                "deletions": 1,
                "sha": "4bbfa05a16e56ad4b6b738555fa6adf69b5bb71e",
                "blob_url": "https://github.com/apache/drill/blob/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java?ref=c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef"
            },
            {
                "patch": "@@ -154,6 +154,9 @@ public void update(int inputIndex) {\n \n   public void update() {};\n \n+  public void update(RecordBatch recordBatch) {\n+  }\n+\n   public void update(RecordBatch recordBatch, int index) {\n     // Get sizing information for the batch.\n     setRecordBatchSizer(index, new RecordBatchSizer(recordBatch));",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordBatchMemoryManager.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "2372be2c900e3f6776d90f21f6ce5abcbebf0b49",
                "blob_url": "https://github.com/apache/drill/blob/c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordBatchMemoryManager.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordBatchMemoryManager.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordBatchMemoryManager.java?ref=c58735a04e0ad6dcff15ee35bbbc27bdd7a14aef"
            }
        ],
        "bug_id": "drill_6",
        "parent": "https://github.com/apache/drill/commit/2162986ce50f566cb80151d97b05fb5557d8d2ab",
        "message": "DRILL-6622: Fixed a NullPointerException in a query with Union\n\ncloses #1391",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/788eb7effe72d838427d42fabbf167fa424bf88c",
        "file": [
            {
                "patch": "@@ -87,7 +87,7 @@ export args\n \n # Set default scheduling priority\n DRILL_NICENESS=${DRILL_NICENESS:-0}\n-GRACEFUL_FILE=$DRILL_PID_DIR/$GRACEFUL_SIGFILE\n+GRACEFUL_FILE=$DRILL_HOME/$GRACEFUL_SIGFILE\n \n waitForProcessEnd()\n {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/788eb7effe72d838427d42fabbf167fa424bf88c/distribution/src/resources/drillbit.sh",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "404c6f86d48d69e9bd8e247ecbc460f1adc5883e",
                "blob_url": "https://github.com/apache/drill/blob/788eb7effe72d838427d42fabbf167fa424bf88c/distribution/src/resources/drillbit.sh",
                "filename": "distribution/src/resources/drillbit.sh",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/distribution/src/resources/drillbit.sh?ref=788eb7effe72d838427d42fabbf167fa424bf88c"
            },
            {
                "patch": "@@ -18,8 +18,9 @@\n package org.apache.drill.exec.server;\n \n import java.io.IOException;\n-import java.nio.file.FileSystems;\n+import java.nio.file.InvalidPathException;\n import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.nio.file.StandardWatchEventKinds;\n import java.nio.file.WatchEvent;\n import java.nio.file.WatchKey;\n@@ -373,18 +374,33 @@ public void run () {\n       }\n     }\n \n+    /*\n+     * Poll for the graceful file, if the file is found cloase the drillbit. In case if the DRILL_HOME path is not\n+     * set, graceful shutdown will not be supported from the command line.\n+     */\n     private void pollShutdown(Drillbit drillbit) throws IOException, InterruptedException {\n-      final Path drillPidDirPath = FileSystems.getDefault().getPath(System.getenv(\"DRILL_PID_DIR\"));\n-      final String gracefulFileName = System.getenv(\"GRACEFUL_SIGFILE\");\n+      final String drillHome = System.getenv(\"DRILL_HOME\");\n+      final String gracefulFile = System.getenv(\"GRACEFUL_SIGFILE\");\n+      final Path drillHomePath;\n+      if (drillHome == null || gracefulFile == null) {\n+        logger.warn(\"Cannot access graceful file. Graceful shutdown from command line will not be supported.\");\n+        return;\n+      }\n+      try {\n+        drillHomePath = Paths.get(drillHome);\n+      } catch (InvalidPathException e) {\n+        logger.warn(\"Cannot access graceful file. Graceful shutdown from command line will not be supported.\");\n+        return;\n+      }\n       boolean triggered_shutdown = false;\n       WatchKey wk = null;\n-      try (final WatchService watchService = FileSystems.getDefault().newWatchService()) {\n-        drillPidDirPath.register(watchService, StandardWatchEventKinds.ENTRY_MODIFY, StandardWatchEventKinds.ENTRY_CREATE);\n+      try (final WatchService watchService = drillHomePath.getFileSystem().newWatchService()) {\n+        drillHomePath.register(watchService, StandardWatchEventKinds.ENTRY_MODIFY, StandardWatchEventKinds.ENTRY_CREATE);\n         while (!triggered_shutdown) {\n           wk = watchService.take();\n           for (WatchEvent<?> event : wk.pollEvents()) {\n             final Path changed = (Path) event.context();\n-            if (changed != null && changed.endsWith(gracefulFileName)) {\n+            if (changed != null && changed.endsWith(gracefulFile)) {\n               drillbit.interruptPollShutdown = false;\n               triggered_shutdown = true;\n               drillbit.close();",
                "additions": 22,
                "raw_url": "https://github.com/apache/drill/raw/788eb7effe72d838427d42fabbf167fa424bf88c/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "status": "modified",
                "changes": 28,
                "deletions": 6,
                "sha": "4c595a2aa600cf307892ffcc6e80afbc65a58bc3",
                "blob_url": "https://github.com/apache/drill/blob/788eb7effe72d838427d42fabbf167fa424bf88c/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java?ref=788eb7effe72d838427d42fabbf167fa424bf88c"
            }
        ],
        "bug_id": "drill_7",
        "parent": "https://github.com/apache/drill/commit/75b9a788df6d6ea5e73cfedff9e2b47acc27b684",
        "message": "DRILL-6877: NPE when starting Drillbit\n\ncloses #1560",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/9dec5144f669b1fb97208a0b9c848af14b7cbccf",
        "file": [
            {
                "patch": "@@ -98,7 +98,7 @@\n   private DrillbitStateManager stateManager;\n   private boolean quiescentMode;\n   private boolean forcefulShutdown = false;\n-  GracefulShutdownThread gracefulShutdownThread;\n+  private GracefulShutdownThread gracefulShutdownThread;\n   private boolean interruptPollShutdown = true;\n \n   public void setQuiescentMode(boolean quiescentMode) {\n@@ -196,6 +196,7 @@ public int getWebServerPort() {\n   public void run() throws Exception {\n     final Stopwatch w = Stopwatch.createStarted();\n     logger.debug(\"Startup begun.\");\n+    gracefulShutdownThread = new GracefulShutdownThread(this, new StackTrace());\n     coord.start(10000);\n     stateManager.setState(DrillbitState.ONLINE);\n     storeProvider.start();\n@@ -222,7 +223,6 @@ public void run() throws Exception {\n     drillbitContext.startRM();\n \n     Runtime.getRuntime().addShutdownHook(new ShutdownThread(this, new StackTrace()));\n-    gracefulShutdownThread = new GracefulShutdownThread(this, new StackTrace());\n     gracefulShutdownThread.start();\n     logger.info(\"Startup completed ({} ms).\", w.elapsed(TimeUnit.MILLISECONDS));\n   }\n@@ -470,6 +470,11 @@ public DrillbitContext getContext() {\n     return manager.getContext();\n   }\n \n+  @VisibleForTesting\n+  public GracefulShutdownThread getGracefulShutdownThread() {\n+    return gracefulShutdownThread;\n+  }\n+\n   public static void main(final String[] cli) throws DrillbitStartupException {\n     final StartupOptions options = StartupOptions.parse(cli);\n     start(options);",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "status": "modified",
                "changes": 9,
                "deletions": 2,
                "sha": "8d413977f5c2587eea26ab0b438e17836ee54eb6",
                "blob_url": "https://github.com/apache/drill/blob/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java?ref=9dec5144f669b1fb97208a0b9c848af14b7cbccf"
            },
            {
                "patch": "@@ -17,9 +17,11 @@\n  */\n package org.apache.drill.test;\n import org.apache.drill.categories.SlowTest;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n import org.apache.drill.exec.server.Drillbit;\n+import org.apache.hadoop.net.ServerSocketUtil;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n@@ -36,6 +38,9 @@\n import java.nio.file.Path;\n import java.util.Collection;\n \n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.fail;\n \n @Category({SlowTest.class})\n@@ -174,6 +179,44 @@ public void testRestApiShutdown() throws Exception {\n     }\n   }\n \n+  @Test // DRILL-6912\n+  public void gracefulShutdownThreadShouldBeInitializedBeforeClosingDrillbit() throws Exception {\n+    Drillbit drillbit = null;\n+    Drillbit drillbitWithSamePort = null;\n+\n+    int userPort = ServerSocketUtil.getPort(31170, 300);\n+    int bitPort = ServerSocketUtil.getPort(31180, 300);\n+    ClusterFixtureBuilder fixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher).withLocalZk()\n+        .configProperty(ExecConstants.INITIAL_USER_PORT, userPort)\n+        .configProperty(ExecConstants.INITIAL_BIT_PORT, bitPort);\n+    try (ClusterFixture clusterFixture = fixtureBuilder.build()) {\n+      drillbit = clusterFixture.drillbit();\n+\n+      // creating another drillbit instance using same config\n+      drillbitWithSamePort = new Drillbit(clusterFixture.config(), fixtureBuilder.configBuilder().getDefinitions(),\n+          clusterFixture.serviceSet());\n+\n+      try {\n+        drillbitWithSamePort.run();\n+        fail(\"drillbitWithSamePort.run() should throw UserException\");\n+      } catch (UserException e) {\n+        // it's expected that second drillbit can't be started because port is busy\n+        assertThat(e.getMessage(), containsString(\"RESOURCE ERROR: Drillbit could not bind to port\"));\n+      }\n+    } finally {\n+      // preconditions\n+      assertNotNull(drillbit);\n+      assertNotNull(drillbitWithSamePort);\n+      assertNotNull(\"gracefulShutdownThread should be initialized, otherwise NPE will be thrown from close()\",\n+          drillbit.getGracefulShutdownThread());\n+      // main test case\n+      assertNotNull(\"gracefulShutdownThread should be initialized, otherwise NPE will be thrown from close()\",\n+          drillbitWithSamePort.getGracefulShutdownThread());\n+      drillbit.close();\n+      drillbitWithSamePort.close();\n+    }\n+  }\n+\n   private static boolean waitAndAssertDrillbitCount(ClusterFixture cluster, int zkRefresh) throws InterruptedException {\n \n     while (true) {",
                "additions": 43,
                "raw_url": "https://github.com/apache/drill/raw/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "status": "modified",
                "changes": 43,
                "deletions": 0,
                "sha": "9d649ba452130d42ccbd16741f10e822ba74ca52",
                "blob_url": "https://github.com/apache/drill/blob/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java?ref=9dec5144f669b1fb97208a0b9c848af14b7cbccf"
            }
        ],
        "bug_id": "drill_8",
        "parent": "https://github.com/apache/drill/commit/1df935f8f4ed0635ceb38f42fc5b5387c49bdf73",
        "message": "DRILL-6912: NPE when other drillbit is already running\n\ncloses #1577",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
        "file": [
            {
                "patch": "@@ -31,7 +31,7 @@\n   <name>contrib/jdbc-storage-plugin</name>\n \n   <properties>\n-    <mysql.connector.version>5.1.36</mysql.connector.version>\n+    <mysql.connector.version>8.0.13</mysql.connector.version>\n     <derby.database.name>drill_derby_test</derby.database.name>\n     <mysql.database.name>drill_mysql_test</mysql.database.name>\n     <skipTests>false</skipTests>\n@@ -62,13 +62,13 @@\n     <dependency>\n       <groupId>org.apache.derby</groupId>\n       <artifactId>derbyclient</artifactId>\n-      <version>10.11.1.1</version>\n+      <version>10.14.2.0</version>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.derby</groupId>\n       <artifactId>derbynet</artifactId>\n-      <version>10.11.1.1</version>\n+      <version>10.14.2.0</version>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n@@ -104,7 +104,7 @@\n         <!-- Because the JDBC tests are somewhat heavyweight, we only run them in the 'verify' phase -->\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-failsafe-plugin</artifactId>\n-        <version>2.22.0</version>\n+        <version>2.22.1</version>\n         <configuration>\n           <forkCount combine.self=\"override\">1</forkCount>\n           <systemPropertyVariables>\n@@ -128,7 +128,7 @@\n         <!-- Allows us to reserve ports for external servers that we will launch  -->\n         <groupId>org.codehaus.mojo</groupId>\n         <artifactId>build-helper-maven-plugin</artifactId>\n-        <version>1.9.1</version>\n+        <version>3.0.0</version>\n         <executions>\n           <execution>\n             <id>reserve-network-port</id>\n@@ -147,7 +147,7 @@\n       </plugin>\n       <plugin>\n         <artifactId>maven-dependency-plugin</artifactId>\n-        <version>2.8</version>\n+        <version>3.1.1</version>\n         <executions>\n           <execution>\n             <goals>",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/pom.xml",
                "status": "modified",
                "changes": 12,
                "deletions": 6,
                "sha": "5bad6b111b1736dbf0c6fa635f64a7827db5ada5",
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/pom.xml",
                "filename": "contrib/storage-jdbc/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/pom.xml?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6"
            },
            {
                "patch": "@@ -22,9 +22,14 @@\n import org.apache.drill.exec.expr.fn.impl.DateUtility;\n import org.apache.drill.exec.proto.UserBitShared;\n \n+import org.apache.drill.exec.util.StoragePluginTestUtils;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n+import java.math.BigDecimal;\n+import java.nio.file.Paths;\n+\n import static org.junit.Assert.assertEquals;\n \n /**\n@@ -33,107 +38,114 @@\n @Category(JdbcStorageTest.class)\n public class TestJdbcPluginWithDerbyIT extends PlanTestBase {\n \n+  private static final String TABLE_PATH = \"jdbcmulti/\";\n+  private static final String TABLE_NAME = String.format(\"%s.`%s`\", StoragePluginTestUtils.DFS_PLUGIN_NAME, TABLE_PATH);\n+\n+  @BeforeClass\n+  public static void copyData() throws Exception {\n+    dirTestWatcher.copyResourceToRoot(Paths.get(TABLE_PATH));\n+  }\n+\n   @Test\n   public void testCrossSourceMultiFragmentJoin() throws Exception {\n-    testNoResult(\"USE derby\");\n     testNoResult(\"SET `planner.slice_target` = 1\");\n-    String query = \"select x.person_id, y.salary from DRILL_DERBY_TEST.PERSON x \"\n-        + \"join dfs.`${WORKING_PATH}/src/test/resources/jdbcmulti/` y on x.person_id = y.person_id \";\n-    test(query);\n+    test(\"select x.person_id, y.salary from derby.drill_derby_test.person x \"\n+        + \"join %s y on x.person_id = y.person_id \", TABLE_NAME);\n   }\n \n   @Test\n   public void validateResult() throws Exception {\n-\n     // Skip date, time, and timestamp types since derby mangles these due to improper timezone support.\n     testBuilder()\n-            .sqlQuery(\n-                    \"select PERSON_ID, FIRST_NAME, LAST_NAME, ADDRESS, CITY, STATE, ZIP, JSON, BIGINT_FIELD, SMALLINT_FIELD, \" +\n-                            \"NUMERIC_FIELD, BOOLEAN_FIELD, DOUBLE_FIELD, FLOAT_FIELD, REAL_FIELD, TIME_FIELD, TIMESTAMP_FIELD, \" +\n-                            \"DATE_FIELD, CLOB_FIELD from derby.DRILL_DERBY_TEST.PERSON\")\n-            .ordered()\n-            .baselineColumns(\"PERSON_ID\", \"FIRST_NAME\", \"LAST_NAME\", \"ADDRESS\", \"CITY\", \"STATE\", \"ZIP\", \"JSON\",\n-                    \"BIGINT_FIELD\", \"SMALLINT_FIELD\", \"NUMERIC_FIELD\", \"BOOLEAN_FIELD\", \"DOUBLE_FIELD\",\n-                    \"FLOAT_FIELD\", \"REAL_FIELD\", \"TIME_FIELD\", \"TIMESTAMP_FIELD\", \"DATE_FIELD\", \"CLOB_FIELD\")\n-            .baselineValues(1, \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\",   \"Philadelphia\",     \"PA\",\n-                            19107, \"{ a : 5, b : 6 }\",            123456L,         1, 10.01, false, 1.0, 1.1, 111.00,\n-                            DateUtility.parseLocalTime(\"13:00:01.0\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2012-02-29\"), \"some clob data 1\")\n-            .baselineValues(2, \"first_name_2\", \"last_name_2\", \"One Ferry Building\",         \"San Francisco\",    \"CA\",\n-                            94111, \"{ foo : \\\"abc\\\" }\",            95949L,         2, 20.02, true, 2.0, 2.1, 222.00,\n-                            DateUtility.parseLocalTime(\"23:59:59.0\"),  DateUtility.parseLocalDateTime(\"1999-09-09 23:59:59.0\"), DateUtility.parseLocalDate(\"1999-09-09\"), \"some more clob data\")\n-            .baselineValues(3, \"first_name_3\", \"last_name_3\", \"176 Bowery\",                 \"New York\",         \"NY\",\n-                            10012, \"{ z : [ 1, 2, 3 ] }\",           45456L,        3, 30.04, true, 3.0, 3.1, 333.00,\n-                            DateUtility.parseLocalTime(\"11:34:21.0\"),  DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2011-10-30\"), \"clobber\")\n-            .baselineValues(4, null, null, \"2 15th St NW\", \"Washington\", \"DC\", 20007, \"{ z : { a : 1, b : 2, c : 3 } \" +\n-                    \"}\", -67L, 4, 40.04, false, 4.0, 4.1, 444.00, DateUtility.parseLocalTime(\"16:00:01.0\"), DateUtility.parseLocalDateTime(\"2015-06-01 16:00:01.0\"),  DateUtility.parseLocalDate(\"2015-06-01\"), \"xxx\")\n-            .baselineValues(5, null, null, null, null, null, null, null, null, null, null, null, null, null, null,\n-                            null, null, null, null)\n-            .build().run();\n+        .sqlQuery(\n+            \"select person_id, first_name, last_name, address, city, state, zip, json, bigint_field, smallint_field, \" +\n+                \"numeric_field, boolean_field, double_field, float_field, real_field, time_field, timestamp_field, \" +\n+                \"date_field, clob_field from derby.`drill_derby_test`.person\")\n+        .ordered()\n+        .baselineColumns(\"person_id\", \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\", \"json\",\n+            \"bigint_field\", \"smallint_field\", \"numeric_field\", \"boolean_field\", \"double_field\", \"float_field\",\n+            \"real_field\", \"time_field\", \"timestamp_field\", \"date_field\", \"clob_field\")\n+        .baselineValues(1, \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\",   \"Philadelphia\",     \"PA\", 19107,\n+            \"{ a : 5, b : 6 }\", 123456L, 1, new BigDecimal(\"10.01\"), false, 1.0, 1.1, 111.00,\n+            DateUtility.parseLocalTime(\"13:00:01.0\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"),\n+            DateUtility.parseLocalDate(\"2012-02-29\"), \"some clob data 1\")\n+        .baselineValues(2, \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n+            \"{ foo : \\\"abc\\\" }\", 95949L, 2, new BigDecimal(\"20.02\"), true, 2.0, 2.1, 222.00,\n+            DateUtility.parseLocalTime(\"23:59:59.0\"),  DateUtility.parseLocalDateTime(\"1999-09-09 23:59:59.0\"),\n+            DateUtility.parseLocalDate(\"1999-09-09\"), \"some more clob data\")\n+        .baselineValues(3, \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012, \"{ z : [ 1, 2, 3 ] }\",\n+            45456L, 3, new BigDecimal(\"30.04\"), true, 3.0, 3.1, 333.00, DateUtility.parseLocalTime(\"11:34:21.0\"),\n+            DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2011-10-30\"), \"clobber\")\n+        .baselineValues(4, null, null, \"2 15th St NW\", \"Washington\", \"DC\", 20007, \"{ z : { a : 1, b : 2, c : 3 } }\",\n+            -67L, 4, new BigDecimal(\"40.04\"), false, 4.0, 4.1, 444.00, DateUtility.parseLocalTime(\"16:00:01.0\"),\n+            DateUtility.parseLocalDateTime(\"2015-06-01 16:00:01.0\"),  DateUtility.parseLocalDate(\"2015-06-01\"), \"xxx\")\n+        .baselineValues(5, null, null, null, null, null, null, null, null, null, null, null, null, null, null,\n+            null, null, null, null)\n+        .go();\n   }\n \n   @Test\n   public void pushdownJoin() throws Exception {\n     testNoResult(\"use derby\");\n-    String query = \"select x.person_id from (select person_id from DRILL_DERBY_TEST.PERSON) x \"\n-            + \"join (select person_id from DRILL_DERBY_TEST.PERSON) y on x.person_id = y.person_id \";\n+    String query = \"select x.person_id from (select person_id from derby.drill_derby_test.person) x \"\n+            + \"join (select person_id from derby.drill_derby_test.person) y on x.person_id = y.person_id \";\n     testPlanMatchingPatterns(query, new String[]{}, new String[]{\"Join\"});\n   }\n \n   @Test\n   public void pushdownJoinAndFilterPushDown() throws Exception {\n     final String query = \"select * from \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON e\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON s\\n\" +\n-            \"ON e.FIRST_NAME = s.FIRST_NAME\\n\" +\n-            \"WHERE e.LAST_NAME > 'hello'\";\n+        \"derby.drill_derby_test.person e\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person s\\n\" +\n+        \"ON e.FIRST_NAME = s.FIRST_NAME\\n\" +\n+        \"WHERE e.LAST_NAME > 'hello'\";\n \n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n   @Test\n   public void pushdownAggregation() throws Exception {\n-    final String query = \"select count(*) from derby.DRILL_DERBY_TEST.PERSON\";\n+    final String query = \"select count(*) from derby.drill_derby_test.person\";\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Aggregate\" });\n   }\n \n   @Test\n   public void pushdownDoubleJoinAndFilter() throws Exception {\n     final String query = \"select * from \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON e\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON s\\n\" +\n-            \"ON e.PERSON_ID = s.PERSON_ID\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON ed\\n\" +\n-            \"ON e.PERSON_ID = ed.PERSON_ID\\n\" +\n-            \"WHERE s.FIRST_NAME > 'abc' and ed.FIRST_NAME > 'efg'\";\n+        \"derby.drill_derby_test.person e\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person s\\n\" +\n+        \"ON e.person_ID = s.person_ID\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person ed\\n\" +\n+        \"ON e.person_ID = ed.person_ID\\n\" +\n+        \"WHERE s.first_name > 'abc' and ed.first_name > 'efg'\";\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n   @Test\n   public void showTablesDefaultSchema() throws Exception {\n-    testNoResult(\"use derby\");\n-    assertEquals(1, testRunAndPrint(UserBitShared.QueryType.SQL, \"show tables like 'PERSON'\"));\n+    test(\"use derby\");\n+    assertEquals(1, testRunAndPrint(UserBitShared.QueryType.SQL, \"show tables like 'person'\"));\n   }\n \n   @Test\n   public void describe() throws Exception {\n-    testNoResult(\"use derby\");\n-    assertEquals(19, testRunAndPrint(UserBitShared.QueryType.SQL, \"describe PERSON\"));\n+    test(\"use derby\");\n+    assertEquals(19, testRunAndPrint(UserBitShared.QueryType.SQL, \"describe drill_derby_test.person\"));\n   }\n \n   @Test\n   public void ensureDrillFunctionsAreNotPushedDown() throws Exception {\n     // This should verify that we're not trying to push CONVERT_FROM into the JDBC storage plugin. If were pushing\n     // this function down, the SQL query would fail.\n-    testNoResult(\"select CONVERT_FROM(JSON, 'JSON') from derby.DRILL_DERBY_TEST.PERSON where PERSON_ID = 4\");\n+    testNoResult(\"select CONVERT_FROM(JSON, 'JSON') from derby.drill_derby_test.person where person_ID = 4\");\n   }\n \n   @Test\n   public void pushdownFilter() throws Exception {\n-    testNoResult(\"use derby\");\n-    String query = \"select * from DRILL_DERBY_TEST.PERSON where PERSON_ID = 1\";\n+    String query = \"select * from derby.drill_derby_test.person where person_ID = 1\";\n     testPlanMatchingPatterns(query, new String[]{}, new String[]{\"Filter\"});\n   }\n }",
                "additions": 62,
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "status": "modified",
                "changes": 112,
                "deletions": 50,
                "sha": "65a1ea564eec5f90ef0a17383fff948e26e235b1",
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6"
            },
            {
                "patch": "@@ -24,6 +24,8 @@\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n+import java.math.BigDecimal;\n+\n /**\n  * JDBC storage plugin tests against MySQL.\n  * Note: it requires libaio.so library in the system\n@@ -35,77 +37,77 @@\n   public void validateResult() throws Exception {\n \n     testBuilder()\n-            .sqlQuery(\n-                    \"select person_id, \" +\n-                            \"first_name, last_name, address, city, state, zip, \" +\n-                            \"bigint_field, smallint_field, numeric_field, \" +\n-                            \"boolean_field, double_field, float_field, real_field, \" +\n-                            \"date_field, datetime_field, year_field, time_field, \" +\n-                            \"json, text_field, tiny_text_field, medium_text_field, long_text_field, \" +\n-                            \"blob_field, bit_field, enum_field \" +\n-                    \"from mysql.`drill_mysql_test`.person\")\n-            .ordered()\n-            .baselineColumns(\"person_id\",\n-                    \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\",\n-                    \"bigint_field\", \"smallint_field\", \"numeric_field\",\n-                    \"boolean_field\",\n-                    \"double_field\", \"float_field\", \"real_field\",\n-                    \"date_field\", \"datetime_field\", \"year_field\", \"time_field\",\n-                    \"json\", \"text_field\", \"tiny_text_field\", \"medium_text_field\", \"long_text_field\",\n-                    \"blob_field\", \"bit_field\", \"enum_field\")\n-            .baselineValues(1,\n-                    \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\", \"Philadelphia\", \"PA\", 19107,\n-                    123456789L, 1, 10.01,\n-                    false,\n-                    1.0, 1.1, 1.2,\n-                    DateUtility.parseLocalDate(\"2012-02-29\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"13:00:01.0\"),\n-                    \"{ a : 5, b : 6 }\",\n-                    \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n-                    \"xxx\",\n-                    \"a medium piece of text\",\n-                    \"a longer piece of text this is going on.....\",\n-                    \"this is a test\".getBytes(),\n-                    true, \"XXX\")\n-            .baselineValues(2,\n-                    \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n-                    45456767L, 3, 30.04,\n-                    true,\n-                    3.0, 3.1, 3.2,\n-                    DateUtility.parseLocalDate(\"2011-10-30\"), DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"11:34:21.0\"),\n-                    \"{ z : [ 1, 2, 3 ] }\",\n-                    \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n-                    \"abc\",\n-                    \"a medium piece of text 2\",\n-                    \"somewhat more text\",\n-                    \"this is a test 2\".getBytes(),\n-                    false, \"YYY\")\n-            .baselineValues(3,\n-                    \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012,\n-                    123090L, -3, 55.12,\n-                    false,\n-                    5.0, 5.1, 5.55,\n-                    DateUtility.parseLocalDate(\"2015-06-01\"), DateUtility.parseLocalDateTime(\"2015-09-22 15:46:10.0\"), DateUtility.parseLocalDate(\"1901-01-01\"), DateUtility.parseLocalTime(\"16:00:01.0\"),\n-                    \"{ [ a, b, c ] }\",\n-                    \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\",\n-                    \"abc\",\n-                    \"a medium piece of text 3\",\n-                    \"somewhat more text\",\n-                    \"this is a test 3\".getBytes(),\n-                    true, \"ZZZ\")\n-            .baselineValues(5,\n-                    null, null, null, null, null, null,\n-                    null, null, null,\n-                    null,\n-                    null, null, null,\n-                    null, null, null, null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null, \"XXX\")\n-                  .build().run();\n+        .sqlQuery(\n+            \"select person_id, \" +\n+                \"first_name, last_name, address, city, state, zip, \" +\n+                \"bigint_field, smallint_field, numeric_field, \" +\n+                \"boolean_field, double_field, float_field, real_field, \" +\n+                \"date_field, datetime_field, year_field, time_field, \" +\n+                \"json, text_field, tiny_text_field, medium_text_field, long_text_field, \" +\n+                \"blob_field, bit_field, enum_field \" +\n+            \"from mysql.`drill_mysql_test`.person\")\n+        .ordered()\n+        .baselineColumns(\"person_id\",\n+            \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\",\n+            \"bigint_field\", \"smallint_field\", \"numeric_field\",\n+            \"boolean_field\",\n+            \"double_field\", \"float_field\", \"real_field\",\n+            \"date_field\", \"datetime_field\", \"year_field\", \"time_field\",\n+            \"json\", \"text_field\", \"tiny_text_field\", \"medium_text_field\", \"long_text_field\",\n+            \"blob_field\", \"bit_field\", \"enum_field\")\n+        .baselineValues(1,\n+            \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\", \"Philadelphia\", \"PA\", 19107,\n+            123456789L, 1, new BigDecimal(\"10.01\"),\n+            false,\n+            1.0, 1.1, 1.2,\n+            DateUtility.parseLocalDate(\"2012-02-29\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"13:00:01.0\"),\n+            \"{ a : 5, b : 6 }\",\n+            \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n+            \"xxx\",\n+            \"a medium piece of text\",\n+            \"a longer piece of text this is going on.....\",\n+            \"this is a test\".getBytes(),\n+            true, \"XXX\")\n+        .baselineValues(2,\n+            \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n+            45456767L, 3, new BigDecimal(\"30.04\"),\n+            true,\n+            3.0, 3.1, 3.2,\n+            DateUtility.parseLocalDate(\"2011-10-30\"), DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"11:34:21.0\"),\n+            \"{ z : [ 1, 2, 3 ] }\",\n+            \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n+            \"abc\",\n+            \"a medium piece of text 2\",\n+            \"somewhat more text\",\n+            \"this is a test 2\".getBytes(),\n+            false, \"YYY\")\n+        .baselineValues(3,\n+            \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012,\n+            123090L, -3, new BigDecimal(\"55.12\"),\n+            false,\n+            5.0, 5.1, 5.55,\n+            DateUtility.parseLocalDate(\"2015-06-01\"), DateUtility.parseLocalDateTime(\"2015-09-22 15:46:10.0\"), DateUtility.parseLocalDate(\"1901-01-01\"), DateUtility.parseLocalTime(\"16:00:01.0\"),\n+            \"{ [ a, b, c ] }\",\n+            \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\",\n+            \"abc\",\n+            \"a medium piece of text 3\",\n+            \"somewhat more text\",\n+            \"this is a test 3\".getBytes(),\n+            true, \"ZZZ\")\n+        .baselineValues(5,\n+            null, null, null, null, null, null,\n+            null, null, null,\n+            null,\n+            null, null, null,\n+            null, null, null, null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null, \"XXX\")\n+            .go();\n   }\n \n   @Test\n@@ -132,4 +134,12 @@ public void testPhysicalPlanSubmission() throws Exception {\n     testPhysicalPlanExecutionBasedOnQuery(\"select * from mysql.`drill_mysql_test`.person\");\n   }\n \n+  @Test\n+  public void emptyOutput() throws Exception {\n+    String query = \"select * from mysql.`drill_mysql_test`.person e limit 0\";\n+\n+    test(query);\n+  }\n+\n+\n }",
                "additions": 81,
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "status": "modified",
                "changes": 152,
                "deletions": 71,
                "sha": "c8396b5097501351efadbbfa220ffa3340fd55c1",
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6"
            },
            {
                "patch": "@@ -52,7 +52,12 @@ public RelNode visit(TableScan scan) {\n     DrillTable unwrap;\n     unwrap = scan.getTable().unwrap(DrillTable.class);\n     if (unwrap == null) {\n-      unwrap = scan.getTable().unwrap(DrillTranslatableTable.class).getDrillTable();\n+      DrillTranslatableTable drillTranslatableTable = scan.getTable().unwrap(DrillTranslatableTable.class);\n+      if (drillTranslatableTable == null) {\n+        contains = true; // it rejects single mode.\n+        return scan;\n+      }\n+      unwrap = drillTranslatableTable.getDrillTable();\n     }\n \n     try {",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "status": "modified",
                "changes": 7,
                "deletions": 1,
                "sha": "90cc178fea551f0f8a6a7969a24cb0073575f600",
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6"
            }
        ],
        "bug_id": "drill_9",
        "parent": "https://github.com/apache/drill/commit/6a990c7eb928b00311935e107427c8420258fd6c",
        "message": "DRILL-6850: JDBC integration tests failures\n\n- Fix RDBMS integration tests (expected decimal output and testCrossSourceMultiFragmentJoin)\n- Update libraries versions\n- Resolve NPE for empty result",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724",
        "file": [
            {
                "patch": "@@ -31,21 +31,29 @@\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(FieldIdUtil.class);\n \n   public static TypedFieldId getFieldIdIfMatchesUnion(UnionVector unionVector, TypedFieldId.Builder builder, boolean addToBreadCrumb, PathSegment seg) {\n-    if (seg.isNamed()) {\n-      ValueVector v = unionVector.getMap();\n-      if (v != null) {\n-        return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n-      } else {\n-        return null;\n+    if (seg != null) {\n+      if (seg.isNamed()) {\n+        ValueVector v = unionVector.getMap();\n+        if (v != null) {\n+          return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n+        } else {\n+          return null;\n+        }\n+      } else if (seg.isArray()) {\n+        ValueVector v = unionVector.getList();\n+        if (v != null) {\n+          return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n+        } else {\n+          return null;\n+        }\n       }\n-    } else if (seg.isArray()) {\n-      ValueVector v = unionVector.getList();\n-      if (v != null) {\n-        return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n-      } else {\n-        return null;\n+    } else {\n+      if (addToBreadCrumb) {\n+        builder.intermediateType(unionVector.getField().getType());\n       }\n+      return builder.finalType(unionVector.getField().getType()).build();\n     }\n+\n     return null;\n   }\n ",
                "additions": 20,
                "raw_url": "https://github.com/apache/drill/raw/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "status": "modified",
                "changes": 32,
                "deletions": 12,
                "sha": "6e72b6e50e644e86b496bbc1f87a8d86892c6a91",
                "blob_url": "https://github.com/apache/drill/blob/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java?ref=b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724"
            },
            {
                "patch": "@@ -35,6 +35,8 @@\n import java.util.List;\n import java.util.zip.GZIPOutputStream;\n \n+import org.apache.drill.exec.util.JsonStringHashMap;\n+import org.apache.drill.exec.util.Text;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.util.DrillFileUtils;\n@@ -720,4 +722,30 @@ public void testFieldWithDots() throws Exception {\n       .baselineValues(\"1\", \"2\", \"1\", null, \"a\")\n       .go();\n   }\n+\n+  @Test // DRILL-6020\n+  public void testUntypedPathWithUnion() throws Exception {\n+    String fileName = \"table.json\";\n+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(new File(dirTestWatcher.getRootDir(), fileName)))) {\n+      writer.write(\"{\\\"rk\\\": {\\\"a\\\": {\\\"b\\\": \\\"1\\\"}}}\");\n+      writer.write(\"{\\\"rk\\\": {\\\"a\\\": \\\"2\\\"}}\");\n+    }\n+\n+    JsonStringHashMap<String, Text> map = new JsonStringHashMap<>();\n+    map.put(\"b\", new Text(\"1\"));\n+\n+    try {\n+      testBuilder()\n+        .sqlQuery(\"select t.rk.a as a from dfs.`%s` t\", fileName)\n+        .ordered()\n+        .optionSettingQueriesForTestQuery(\"alter session set `exec.enable_union_type`=true\")\n+        .baselineColumns(\"a\")\n+        .baselineValues(map)\n+        .baselineValues(\"2\")\n+        .go();\n+\n+    } finally {\n+      testNoResult(\"alter session reset `exec.enable_union_type`\");\n+    }\n+  }\n }",
                "additions": 28,
                "raw_url": "https://github.com/apache/drill/raw/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "status": "modified",
                "changes": 28,
                "deletions": 0,
                "sha": "da1cddba5003fc1c57cfd35f8fd078449c9b65c4",
                "blob_url": "https://github.com/apache/drill/blob/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java?ref=b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724"
            }
        ],
        "bug_id": "drill_10",
        "parent": "https://github.com/apache/drill/commit/ce80da857d1b28af7619f8402ffe1e4e3c833e1c",
        "message": "DRILL-6020: Fix NullPointerException when querying JSON untyped path with Union setting on\n\ncloses #1068",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
        "file": [
            {
                "patch": "@@ -60,8 +60,9 @@\n import org.apache.drill.exec.planner.logical.DrillWindowRule;\n import org.apache.drill.exec.planner.logical.partition.ParquetPruneScanRule;\n import org.apache.drill.exec.planner.logical.partition.PruneScanRule;\n+import org.apache.drill.exec.planner.logical.ConvertCountToDirectScanRule;\n import org.apache.drill.exec.planner.physical.AnalyzePrule;\n-import org.apache.drill.exec.planner.physical.ConvertCountToDirectScan;\n+import org.apache.drill.exec.planner.physical.ConvertCountToDirectScanPrule;\n import org.apache.drill.exec.planner.physical.LateralJoinPrule;\n import org.apache.drill.exec.planner.physical.DirectScanPrule;\n import org.apache.drill.exec.planner.physical.FilterPrule;\n@@ -474,8 +475,10 @@ static RuleSet getDirPruneScanRules(OptimizerRulesContext optimizerRulesContext)\n         .add(\n             PruneScanRule.getDirFilterOnProject(optimizerRulesContext),\n             PruneScanRule.getDirFilterOnScan(optimizerRulesContext),\n-            PruneScanRule.getConvertAggScanToValuesRule(optimizerRulesContext)\n-        )\n+            PruneScanRule.getConvertAggScanToValuesRule(optimizerRulesContext),\n+            ConvertCountToDirectScanRule.AGG_ON_PROJ_ON_SCAN,\n+            ConvertCountToDirectScanRule.AGG_ON_SCAN\n+          )\n         .build();\n \n     return RuleSets.ofList(pruneRules);\n@@ -501,8 +504,8 @@ static RuleSet getJoinPermRules(OptimizerRulesContext optimizerRulesContext) {\n   static RuleSet getPhysicalRules(OptimizerRulesContext optimizerRulesContext) {\n     final List<RelOptRule> ruleList = new ArrayList<>();\n     final PlannerSettings ps = optimizerRulesContext.getPlannerSettings();\n-    ruleList.add(ConvertCountToDirectScan.AGG_ON_PROJ_ON_SCAN);\n-    ruleList.add(ConvertCountToDirectScan.AGG_ON_SCAN);\n+    ruleList.add(ConvertCountToDirectScanPrule.AGG_ON_PROJ_ON_SCAN);\n+    ruleList.add(ConvertCountToDirectScanPrule.AGG_ON_SCAN);\n     ruleList.add(SortConvertPrule.INSTANCE);\n     ruleList.add(SortPrule.INSTANCE);\n     ruleList.add(ProjectPrule.INSTANCE);",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "status": "modified",
                "changes": 13,
                "deletions": 5,
                "sha": "0c89c1bf2737fb9c56797db2f7aef96335297a19",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.common;\n+\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rel.type.RelDataTypeFieldImpl;\n+import org.apache.calcite.rel.type.RelRecordType;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * A utility class that contains helper functions used by rules that convert COUNT(*) and COUNT(col)\n+ * aggregates (no group-by) to DirectScan\n+ */\n+public class CountToDirectScanUtils {\n+\n+  /**\n+   * Checks if aggregate call contains star or non-null expression:\n+   * <pre>\n+   * count(*)  == >  empty arg  ==>  rowCount\n+   * count(Not-null-input) ==> rowCount\n+   * </pre>\n+   *\n+   * @param aggregateCall aggregate call\n+   * @param aggregate aggregate relation expression\n+   * @return true of aggregate call contains star or non-null expression\n+   */\n+  public static boolean containsStarOrNotNullInput(AggregateCall aggregateCall, Aggregate aggregate) {\n+    return aggregateCall.getArgList().isEmpty() ||\n+        (aggregateCall.getArgList().size() == 1 &&\n+            !aggregate.getInput().getRowType().getFieldList().get(aggregateCall.getArgList().get(0)).getType().isNullable());\n+  }\n+\n+  /**\n+   * For each aggregate call creates field based on its name with bigint type.\n+   * Constructs record type for created fields.\n+   *\n+   * @param aggregateRel aggregate relation expression\n+   * @param fieldNames field names\n+   * @return record type\n+   */\n+  public static RelDataType constructDataType(Aggregate aggregateRel, Collection<String> fieldNames) {\n+    List<RelDataTypeField> fields = new ArrayList<>();\n+    int fieldIndex = 0;\n+    for (String name : fieldNames) {\n+      RelDataTypeField field = new RelDataTypeFieldImpl(\n+          name,\n+          fieldIndex++,\n+          aggregateRel.getCluster().getTypeFactory().createSqlType(SqlTypeName.BIGINT));\n+      fields.add(field);\n+    }\n+    return new RelRecordType(fields);\n+  }\n+\n+  /**\n+   * Builds schema based on given field names.\n+   * Type for each schema is set to long.class.\n+   *\n+   * @param fieldNames field names\n+   * @return schema\n+   */\n+  public static LinkedHashMap<String, Class<?>> buildSchema(List<String> fieldNames) {\n+    LinkedHashMap<String, Class<?>> schema = new LinkedHashMap<>();\n+    for (String fieldName: fieldNames) {\n+      schema.put(fieldName, long.class);\n+    }\n+    return schema;\n+  }\n+\n+  /**\n+   * For each field creates row expression.\n+   *\n+   * @param rowType row type\n+   * @return list of row expressions\n+   */\n+  public static List<RexNode> prepareFieldExpressions(RelDataType rowType) {\n+    List<RexNode> expressions = new ArrayList<>();\n+    for (int i = 0; i < rowType.getFieldCount(); i++) {\n+      expressions.add(RexInputRef.of(i, rowType));\n+    }\n+    return expressions;\n+  }\n+\n+}",
                "additions": 108,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "status": "added",
                "changes": 108,
                "deletions": 0,
                "sha": "2b5755f5c8f652a8ffad49c24866d5c3c1cbd8d6",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.drill.exec.planner.logical.DrillRelFactories;\n import org.apache.drill.exec.planner.logical.DrillTable;\n import org.apache.drill.exec.planner.logical.FieldsReWriterUtil;\n+import org.apache.drill.exec.planner.logical.DrillTranslatableTable;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n import org.apache.drill.exec.resolver.TypeCastRules;\n import org.apache.drill.exec.util.Utilities;\n@@ -647,4 +648,16 @@ public static boolean analyzeSimpleEquiJoin(Join join, int[] joinFieldOrdinals)\n       }\n     }\n   }\n+\n+  public static DrillTable getDrillTable(final TableScan scan) {\n+    DrillTable drillTable = null;\n+    drillTable = scan.getTable().unwrap(DrillTable.class);\n+    if (drillTable == null) {\n+      DrillTranslatableTable transTable = scan.getTable().unwrap(DrillTranslatableTable.class);\n+      if (transTable != null) {\n+        drillTable = transTable.getDrillTable();\n+      }\n+    }\n+    return drillTable;\n+  }\n }",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "status": "modified",
                "changes": 13,
                "deletions": 0,
                "sha": "531c2b85d635758e9b2f59729b13ecbb52953de9",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.logical;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptRuleOperand;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.core.TableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.common.logical.FormatPluginConfig;\n+\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.ScanStats;\n+import org.apache.drill.exec.planner.common.CountToDirectScanUtils;\n+import org.apache.drill.exec.planner.common.DrillRelOptUtil;\n+\n+import org.apache.drill.exec.planner.physical.PlannerSettings;\n+import org.apache.drill.exec.store.ColumnExplorer;\n+import org.apache.drill.exec.store.dfs.DrillFileSystem;\n+import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n+import org.apache.drill.exec.store.dfs.FormatSelection;\n+import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n+import org.apache.drill.exec.store.direct.MetadataDirectGroupScan;\n+import org.apache.drill.exec.store.parquet.ParquetFormatConfig;\n+import org.apache.drill.exec.store.parquet.ParquetReaderConfig;\n+import org.apache.drill.exec.store.parquet.metadata.Metadata;\n+import org.apache.drill.exec.store.parquet.metadata.Metadata_V4;\n+import org.apache.drill.exec.store.pojo.DynamicPojoRecordReader;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.LinkedHashMap;\n+import java.util.Set;\n+\n+/**\n+ * <p> This rule is a logical planning counterpart to a corresponding <b>ConvertCountToDirectScanPrule</b>\n+ * physical rule\n+ * </p>\n+ * <p>\n+ * This rule will convert <b>\" select count(*)  as mycount from table \"</b>\n+ * or <b>\" select count(not-nullable-expr) as mycount from table \"</b> into\n+ * <pre>\n+ *    Project(mycount)\n+ *         \\\n+ *    DirectGroupScan ( PojoRecordReader ( rowCount ))\n+ *</pre>\n+ * or <b>\" select count(column) as mycount from table \"</b> into\n+ * <pre>\n+ *      Project(mycount)\n+ *           \\\n+ *            DirectGroupScan (PojoRecordReader (columnValueCount))\n+ *</pre>\n+ * Rule can be applied if query contains multiple count expressions.\n+ * <b>\" select count(column1), count(column2), count(*) from table \"</b>\n+ * </p>\n+ *\n+ * <p>\n+ * The rule utilizes the Parquet Metadata Cache's summary information to retrieve the total row count\n+ * and the per-column null count.  As such, the rule is only applicable for Parquet tables and only if the\n+ * metadata cache has been created with the summary information.\n+ * </p>\n+ */\n+public class ConvertCountToDirectScanRule extends RelOptRule {\n+\n+  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScanRule(\n+      RelOptHelper.some(Aggregate.class,\n+                        RelOptHelper.some(Project.class,\n+                            RelOptHelper.any(TableScan.class))), \"Agg_on_proj_on_scan\");\n+\n+  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScanRule(\n+      RelOptHelper.some(Aggregate.class,\n+                            RelOptHelper.any(TableScan.class)), \"Agg_on_scan\");\n+\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScanRule.class);\n+\n+  protected ConvertCountToDirectScanRule(RelOptRuleOperand rule, String id) {\n+    super(rule, \"ConvertCountToDirectScanRule:\" + id);\n+  }\n+\n+  @Override\n+  public void onMatch(RelOptRuleCall call) {\n+    final Aggregate agg = (Aggregate) call.rel(0);\n+    final TableScan scan = (TableScan) call.rel(call.rels.length - 1);\n+    final Project project = call.rels.length == 3 ? (Project) call.rel(1) : null;\n+\n+    // Qualifying conditions for rule:\n+    //    1) There's no GroupBY key,\n+    //    2) Agg is not a DISTINCT agg\n+    //    3) Additional checks are done further below ..\n+    if (agg.getGroupCount() > 0 ||\n+        agg.containsDistinctCall()) {\n+      return;\n+    }\n+\n+    DrillTable drillTable = DrillRelOptUtil.getDrillTable(scan);\n+\n+    if (drillTable == null) {\n+      logger.debug(\"Rule does not apply since an eligible drill table instance was not found.\");\n+      return;\n+    }\n+\n+    Object selection = drillTable.getSelection();\n+\n+    if (!(selection instanceof FormatSelection)) {\n+      logger.debug(\"Rule does not apply since only Parquet file format is eligible.\");\n+      return;\n+    }\n+\n+    PlannerSettings settings = call.getPlanner().getContext().unwrap(PlannerSettings.class);\n+\n+    //  Rule is applicable only if the statistics for row count and null count are available from the metadata,\n+    FormatSelection formatSelection = (FormatSelection) selection;\n+    Pair<Boolean, Metadata_V4.MetadataSummary> status = checkMetadataForScanStats(settings, drillTable, formatSelection);\n+\n+    if (!status.getLeft()) {\n+      logger.debug(\"Rule does not apply since MetadataSummary metadata was not found.\");\n+      return;\n+    }\n+\n+    Metadata_V4.MetadataSummary metadataSummary = status.getRight();\n+    Map<String, Long> result = collectCounts(settings, metadataSummary, agg, scan, project);\n+    logger.trace(\"Calculated the following aggregate counts: \", result);\n+\n+    // if counts could not be determined, rule won't be applied\n+    if (result.isEmpty()) {\n+      logger.debug(\"Rule does not apply since one or more COUNTs could not be determined from metadata.\");\n+      return;\n+    }\n+\n+    List<Path> fileList =\n+            ImmutableList.of(Metadata.getSummaryFileName(formatSelection.getSelection().getSelectionRoot()));\n+\n+    final RelDataType scanRowType = CountToDirectScanUtils.constructDataType(agg, result.keySet());\n+\n+    final DynamicPojoRecordReader<Long> reader = new DynamicPojoRecordReader<>(\n+        CountToDirectScanUtils.buildSchema(scanRowType.getFieldNames()),\n+        Collections.singletonList((List<Long>) new ArrayList<>(result.values())));\n+\n+    final ScanStats scanStats = new ScanStats(ScanStats.GroupScanProperty.EXACT_ROW_COUNT, 1, 1, scanRowType.getFieldCount());\n+    final MetadataDirectGroupScan directScan = new MetadataDirectGroupScan(reader, fileList, scanStats, true);\n+\n+    final DrillDirectScanRel newScan = new DrillDirectScanRel(scan.getCluster(), scan.getTraitSet().plus(DrillRel.DRILL_LOGICAL),\n+      directScan, scanRowType);\n+\n+    final DrillProjectRel newProject = new DrillProjectRel(agg.getCluster(), agg.getTraitSet().plus(DrillRel.DRILL_LOGICAL),\n+      newScan, CountToDirectScanUtils.prepareFieldExpressions(scanRowType), agg.getRowType());\n+\n+    call.transformTo(newProject);\n+  }\n+\n+  private Pair<Boolean, Metadata_V4.MetadataSummary> checkMetadataForScanStats(PlannerSettings settings, DrillTable drillTable,\n+                                                                               FormatSelection formatSelection) {\n+\n+    // Currently only support metadata rowcount stats for Parquet tables\n+    FormatPluginConfig formatConfig = formatSelection.getFormat();\n+    if (!((formatConfig instanceof ParquetFormatConfig)\n+      || ((formatConfig instanceof NamedFormatPluginConfig)\n+      && ((NamedFormatPluginConfig) formatConfig).name.equals(\"parquet\")))) {\n+      return new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+    }\n+\n+    FileSystemPlugin plugin = (FileSystemPlugin) drillTable.getPlugin();\n+    DrillFileSystem fs = null;\n+    try {\n+       fs = new DrillFileSystem(plugin.getFormatPlugin(formatSelection.getFormat()).getFsConf());\n+    } catch (IOException e) {\n+      logger.warn(\"Unable to create the file system object for retrieving statistics from metadata cache file \", e);\n+      return new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+    }\n+\n+    // check if the cacheFileRoot has been set: this is needed because after directory pruning, the\n+    // cacheFileRoot could have been changed and not be the same as the original selectionRoot\n+    Path selectionRoot = formatSelection.getSelection().getCacheFileRoot() != null ?\n+            formatSelection.getSelection().getCacheFileRoot() :\n+            formatSelection.getSelection().getSelectionRoot();\n+\n+    ParquetReaderConfig parquetReaderConfig= ParquetReaderConfig.builder()\n+            .withFormatConfig((ParquetFormatConfig) formatConfig)\n+            .withOptions(settings.getOptions())\n+            .build();\n+\n+    Metadata_V4.MetadataSummary metadataSummary = Metadata.getSummary(fs, selectionRoot, false, parquetReaderConfig);\n+\n+    return metadataSummary != null ? new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(true, metadataSummary) :\n+      new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+  }\n+\n+  /**\n+   * Collects counts for each aggregation call by using the metadata summary information\n+   * Will return empty result map if was not able to determine count for at least one aggregation call.\n+   *\n+   * For each aggregate call will determine if count can be calculated. Collects counts only for COUNT function.\n+   *   1. First, we get the total row count from the metadata summary.\n+   *   2. For COUNT(*) and COUNT(<non null column>) and COUNT(<implicit column>), the count = total row count\n+   *   3. For COUNT(nullable column), count = (total row count - column's null count)\n+   *   4. Also count can not be calculated for parition columns.\n+   *\n+   * @param settings planner options\n+   * @param metadataSummary metadata summary containing row counts and column counts\n+   * @param agg aggregate relational expression\n+   * @param scan scan relational expression\n+   * @param project project relational expression\n+   * @return result map where key is count column name, value is count value\n+   */\n+  private Map<String, Long> collectCounts(PlannerSettings settings, Metadata_V4.MetadataSummary metadataSummary,\n+                                          Aggregate agg, TableScan scan, Project project) {\n+    final Set<String> implicitColumnsNames = ColumnExplorer.initImplicitFileColumns(settings.getOptions()).keySet();\n+    final long totalRecordCount = metadataSummary.getTotalRowCount();\n+    final LinkedHashMap<String, Long> result = new LinkedHashMap<>();\n+\n+    for (int i = 0; i < agg.getAggCallList().size(); i++) {\n+      AggregateCall aggCall = agg.getAggCallList().get(i);\n+      long cnt;\n+\n+      // rule can be applied only for count function, return empty counts\n+      if (!\"count\".equalsIgnoreCase(aggCall.getAggregation().getName()) ) {\n+        return ImmutableMap.of();\n+      }\n+\n+      if (CountToDirectScanUtils.containsStarOrNotNullInput(aggCall, agg)) {\n+        cnt = totalRecordCount;\n+\n+      } else if (aggCall.getArgList().size() == 1) {\n+        // count(columnName) ==> Agg ( Scan )) ==> columnValueCount\n+        int index = aggCall.getArgList().get(0);\n+\n+        if (project != null) {\n+          // project in the middle of Agg and Scan : Only when input of AggCall is a RexInputRef in Project, we find the index of Scan's field.\n+          // For instance,\n+          // Agg - count($0)\n+          //  \\\n+          //  Proj - Exp={$1}\n+          //    \\\n+          //   Scan (col1, col2).\n+          // return count of \"col2\" in Scan's metadata, if found.\n+          if (!(project.getProjects().get(index) instanceof RexInputRef)) {\n+            return ImmutableMap.of(); // do not apply for all other cases.\n+          }\n+\n+          index = ((RexInputRef) project.getProjects().get(index)).getIndex();\n+        }\n+\n+        String columnName = scan.getRowType().getFieldNames().get(index).toLowerCase();\n+\n+        // for implicit column count will be the same as total record count\n+        if (implicitColumnsNames.contains(columnName)) {\n+          cnt = totalRecordCount;\n+        } else {\n+          SchemaPath simplePath = SchemaPath.getSimplePath(columnName);\n+\n+          if (ColumnExplorer.isPartitionColumn(settings.getOptions(), simplePath)) {\n+            return ImmutableMap.of();\n+          }\n+\n+          Metadata_V4.ColumnTypeMetadata_v4 columnMetadata = metadataSummary.getColumnTypeInfo(new Metadata_V4.ColumnTypeMetadata_v4.Key(simplePath));\n+\n+         if (columnMetadata == null || columnMetadata.totalNullCount == GroupScan.NO_COLUMN_STATS) {\n+            // if column stats is not available don't apply this rule, return empty counts\n+            return ImmutableMap.of();\n+          } else {\n+           // count of a nullable column = (total row count - column's null count)\n+           cnt = totalRecordCount - columnMetadata.totalNullCount;\n+         }\n+\n+        }\n+      } else {\n+        return ImmutableMap.of();\n+      }\n+\n+      String name = \"count\" + i + \"$\" + (aggCall.getName() == null ? aggCall.toString() : aggCall.getName());\n+      result.put(name, cnt);\n+    }\n+\n+    return ImmutableMap.copyOf(result);\n+  }\n+\n+}",
                "additions": 305,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "status": "added",
                "changes": 305,
                "deletions": 0,
                "sha": "63206c52473c7876cff714663d723b69e1458e4d",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -26,6 +26,7 @@\n import java.util.concurrent.TimeUnit;\n import java.util.regex.Pattern;\n \n+import org.apache.drill.exec.planner.common.DrillRelOptUtil;\n import org.apache.drill.exec.util.DrillFileSystemUtil;\n import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;\n import org.apache.calcite.adapter.enumerable.EnumerableTableScan;\n@@ -65,8 +66,6 @@\n import org.apache.drill.exec.planner.logical.DrillRel;\n import org.apache.drill.exec.planner.logical.DrillRelFactories;\n import org.apache.drill.exec.planner.logical.DrillScanRel;\n-import org.apache.drill.exec.planner.logical.DrillTable;\n-import org.apache.drill.exec.planner.logical.DrillTranslatableTable;\n import org.apache.drill.exec.planner.logical.DrillValuesRel;\n import org.apache.drill.exec.planner.logical.RelOptHelper;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n@@ -172,7 +171,7 @@ protected void doOnMatch(RelOptRuleCall call, Filter filterRel, Project projectR\n     PartitionDescriptor descriptor = getPartitionDescriptor(settings, scanRel);\n     final BufferAllocator allocator = optimizerContext.getAllocator();\n \n-    final Object selection = getDrillTable(scanRel).getSelection();\n+    final Object selection = DrillRelOptUtil.getDrillTable(scanRel).getSelection();\n     MetadataContext metaContext = null;\n     if (selection instanceof FormatSelection) {\n          metaContext = ((FormatSelection)selection).getSelection().getMetaContext();\n@@ -541,18 +540,9 @@ protected OptimizerRulesContext getOptimizerRulesContext() {\n \n   public abstract PartitionDescriptor getPartitionDescriptor(PlannerSettings settings, TableScan scanRel);\n \n-  private static DrillTable getDrillTable(final TableScan scan) {\n-    DrillTable drillTable;\n-    drillTable = scan.getTable().unwrap(DrillTable.class);\n-    if (drillTable == null) {\n-      drillTable = scan.getTable().unwrap(DrillTranslatableTable.class).getDrillTable();\n-    }\n-    return drillTable;\n-  }\n-\n   private static boolean isQualifiedDirPruning(final TableScan scan) {\n     if (scan instanceof EnumerableTableScan) {\n-      final Object selection = getDrillTable(scan).getSelection();\n+      final Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n       if (selection instanceof FormatSelection\n           && ((FormatSelection)selection).supportDirPruning()) {\n         return true;  // Do directory-based pruning in Calcite logical\n@@ -657,7 +647,7 @@ public void onMatch(RelOptRuleCall call) {\n       logger.debug(\"Beginning file partition pruning, pruning class: {}\", pruningClassName);\n       Stopwatch totalPruningTime = logger.isDebugEnabled() ? Stopwatch.createStarted() : null;\n \n-      Object selection = getDrillTable(scan).getSelection();\n+      Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n       MetadataContext metaContext = null;\n       FileSelection fileSelection = null;\n       if (selection instanceof FormatSelection) {\n@@ -779,7 +769,7 @@ public void onMatch(RelOptRuleCall call) {\n \n     private static boolean isQualifiedFilePruning(final TableScan scan) {\n       if (scan instanceof EnumerableTableScan) {\n-        Object selection = getDrillTable(scan).getSelection();\n+        Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n         return selection instanceof FormatSelection;\n       } else if (scan instanceof DrillScanRel) {\n         GroupScan groupScan = ((DrillScanRel) scan).getGroupScan();",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "status": "modified",
                "changes": 20,
                "deletions": 15,
                "sha": "723cd084f4e5cf7d47d81bb2ded7e9fd0833af9a",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -17,38 +17,32 @@\n  */\n package org.apache.drill.exec.planner.physical;\n \n+import java.util.List;\n import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.Iterator;\n import java.util.LinkedHashMap;\n-import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.Collections;\n \n-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n import org.apache.calcite.plan.RelOptRule;\n import org.apache.calcite.plan.RelOptRuleCall;\n import org.apache.calcite.plan.RelOptRuleOperand;\n-import org.apache.calcite.rel.core.AggregateCall;\n import org.apache.calcite.rel.type.RelDataType;\n-import org.apache.calcite.rel.type.RelDataTypeField;\n-import org.apache.calcite.rel.type.RelDataTypeFieldImpl;\n-import org.apache.calcite.rel.type.RelRecordType;\n import org.apache.calcite.rex.RexInputRef;\n-import org.apache.calcite.rex.RexNode;\n-import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.calcite.rel.core.AggregateCall;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.exec.physical.base.GroupScan;\n import org.apache.drill.exec.physical.base.ScanStats;\n import org.apache.drill.exec.planner.logical.DrillAggregateRel;\n import org.apache.drill.exec.planner.logical.DrillProjectRel;\n import org.apache.drill.exec.planner.logical.DrillScanRel;\n import org.apache.drill.exec.planner.logical.RelOptHelper;\n-import org.apache.drill.exec.store.ColumnExplorer;\n+import org.apache.drill.exec.planner.common.CountToDirectScanUtils;\n \n+import org.apache.drill.exec.store.ColumnExplorer;\n import org.apache.drill.exec.store.direct.MetadataDirectGroupScan;\n import org.apache.drill.exec.store.pojo.DynamicPojoRecordReader;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n \n /**\n  * <p>\n@@ -74,23 +68,30 @@\n  * obtained from parquet row group info. This will save the cost to\n  * scan the whole parquet files.\n  * </p>\n+ *\n+ * <p>\n+ *     NOTE: This rule is a physical planning counterpart to a similar ConvertCountToDirectScanRule\n+ *     logical rule. However, while the logical rule relies on the Parquet metadata cache's Summary\n+ *     aggregates, this rule is applicable if the exact row count is available from the GroupScan\n+ *     regardless of where that stat came from. Hence, it is more general, with the trade-off that the\n+ *     GroupScan relies on the fully expanded list of row groups to compute the aggregate row count.\n+ * </p>\n  */\n-public class ConvertCountToDirectScan extends Prule {\n+public class ConvertCountToDirectScanPrule extends Prule {\n \n-  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScan(\n+  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScanPrule(\n       RelOptHelper.some(DrillAggregateRel.class,\n                         RelOptHelper.some(DrillProjectRel.class,\n                             RelOptHelper.any(DrillScanRel.class))), \"Agg_on_proj_on_scan\");\n \n-  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScan(\n+  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScanPrule(\n       RelOptHelper.some(DrillAggregateRel.class,\n                             RelOptHelper.any(DrillScanRel.class)), \"Agg_on_scan\");\n \n-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScan.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScanPrule.class);\n \n-  /** Creates a SplunkPushDownRule. */\n-  protected ConvertCountToDirectScan(RelOptRuleOperand rule, String id) {\n-    super(rule, \"ConvertCountToDirectScan:\" + id);\n+  protected ConvertCountToDirectScanPrule(RelOptRuleOperand rule, String id) {\n+    super(rule, \"ConvertCountToDirectScanPrule:\" + id);\n   }\n \n   @Override\n@@ -119,20 +120,20 @@ public void onMatch(RelOptRuleCall call) {\n       return;\n     }\n \n-    final RelDataType scanRowType = constructDataType(agg, result.keySet());\n+    final RelDataType scanRowType = CountToDirectScanUtils.constructDataType(agg, result.keySet());\n \n     final DynamicPojoRecordReader<Long> reader = new DynamicPojoRecordReader<>(\n-        buildSchema(scanRowType.getFieldNames()),\n+        CountToDirectScanUtils.buildSchema(scanRowType.getFieldNames()),\n         Collections.singletonList((List<Long>) new ArrayList<>(result.values())));\n \n     final ScanStats scanStats = new ScanStats(ScanStats.GroupScanProperty.EXACT_ROW_COUNT, 1, 1, scanRowType.getFieldCount());\n-    final GroupScan directScan = new MetadataDirectGroupScan(reader, oldGrpScan.getFiles(), scanStats);\n+    final GroupScan directScan = new MetadataDirectGroupScan(reader, oldGrpScan.getFiles(), scanStats, false);\n \n     final DirectScanPrel newScan = DirectScanPrel.create(scan, scan.getTraitSet().plus(Prel.DRILL_PHYSICAL)\n         .plus(DrillDistributionTrait.SINGLETON), directScan, scanRowType);\n \n     final ProjectPrel newProject = new ProjectPrel(agg.getCluster(), agg.getTraitSet().plus(Prel.DRILL_PHYSICAL)\n-        .plus(DrillDistributionTrait.SINGLETON), newScan, prepareFieldExpressions(scanRowType), agg.getRowType());\n+        .plus(DrillDistributionTrait.SINGLETON), newScan, CountToDirectScanUtils.prepareFieldExpressions(scanRowType), agg.getRowType());\n \n     call.transformTo(newProject);\n   }\n@@ -165,7 +166,7 @@ public void onMatch(RelOptRuleCall call) {\n         return ImmutableMap.of();\n       }\n \n-      if (containsStarOrNotNullInput(aggCall, agg)) {\n+      if (CountToDirectScanUtils.containsStarOrNotNullInput(aggCall, agg)) {\n         cnt = totalRecordCount;\n \n       } else if (aggCall.getArgList().size() == 1) {\n@@ -217,72 +218,4 @@ public void onMatch(RelOptRuleCall call) {\n     return ImmutableMap.copyOf(result);\n   }\n \n-  /**\n-   * Checks if aggregate call contains star or non-null expression:\n-   * <pre>\n-   * count(*)  == >  empty arg  ==>  rowCount\n-   * count(Not-null-input) ==> rowCount\n-   * </pre>\n-   *\n-   * @param aggregateCall aggregate call\n-   * @param aggregate aggregate relation expression\n-   * @return true of aggregate call contains star or non-null expression\n-   */\n-  private boolean containsStarOrNotNullInput(AggregateCall aggregateCall, DrillAggregateRel aggregate) {\n-    return aggregateCall.getArgList().isEmpty() ||\n-        (aggregateCall.getArgList().size() == 1 &&\n-            !aggregate.getInput().getRowType().getFieldList().get(aggregateCall.getArgList().get(0)).getType().isNullable());\n-  }\n-\n-  /**\n-   * For each aggregate call creates field based on its name with bigint type.\n-   * Constructs record type for created fields.\n-   *\n-   * @param aggregateRel aggregate relation expression\n-   * @param fieldNames field names\n-   * @return record type\n-   */\n-  private RelDataType constructDataType(DrillAggregateRel aggregateRel, Collection<String> fieldNames) {\n-    List<RelDataTypeField> fields = new ArrayList<>();\n-    Iterator<String> filedNamesIterator = fieldNames.iterator();\n-    int fieldIndex = 0;\n-    while (filedNamesIterator.hasNext()) {\n-      RelDataTypeField field = new RelDataTypeFieldImpl(\n-          filedNamesIterator.next(),\n-          fieldIndex++,\n-          aggregateRel.getCluster().getTypeFactory().createSqlType(SqlTypeName.BIGINT));\n-      fields.add(field);\n-    }\n-    return new RelRecordType(fields);\n-  }\n-\n-  /**\n-   * Builds schema based on given field names.\n-   * Type for each schema is set to long.class.\n-   *\n-   * @param fieldNames field names\n-   * @return schema\n-   */\n-  private LinkedHashMap<String, Class<?>> buildSchema(List<String> fieldNames) {\n-    LinkedHashMap<String, Class<?>> schema = new LinkedHashMap<>();\n-    for (String fieldName: fieldNames) {\n-      schema.put(fieldName, long.class);\n-    }\n-    return schema;\n-  }\n-\n-  /**\n-   * For each field creates row expression.\n-   *\n-   * @param rowType row type\n-   * @return list of row expressions\n-   */\n-  private List<RexNode> prepareFieldExpressions(RelDataType rowType) {\n-    List<RexNode> expressions = new ArrayList<>();\n-    for (int i = 0; i < rowType.getFieldCount(); i++) {\n-      expressions.add(RexInputRef.of(i, rowType));\n-    }\n-    return expressions;\n-  }\n-\n }",
                "additions": 25,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "previous_filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScan.java",
                "status": "renamed",
                "changes": 117,
                "deletions": 92,
                "sha": "4176950608484b695edcb34f653389d1886ed924",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -38,21 +38,19 @@\n public class MetadataDirectGroupScan extends DirectGroupScan {\n \n   private final Collection<Path> files;\n+  private boolean usedMetadataSummaryFile = false;\n \n-  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files) {\n-    super(reader);\n-    this.files = files;\n-  }\n-\n-  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files, ScanStats stats) {\n+  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files, ScanStats stats,\n+                                 boolean usedMetadataSummaryFile) {\n     super(reader, stats);\n     this.files = files;\n+    this.usedMetadataSummaryFile = usedMetadataSummaryFile;\n   }\n \n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     assert children == null || children.isEmpty();\n-    return new MetadataDirectGroupScan(reader, files, stats);\n+    return new MetadataDirectGroupScan(reader, files, stats, usedMetadataSummaryFile);\n   }\n \n   @Override\n@@ -78,6 +76,7 @@ public String getDigest() {\n       StringBuilder builder = new StringBuilder();\n       builder.append(\"files = \").append(files).append(\", \");\n       builder.append(\"numFiles = \").append(files.size()).append(\", \");\n+      builder.append(\"usedMetadataSummaryFile = \").append(usedMetadataSummaryFile).append(\", \");\n       return builder.append(super.getDigest()).toString();\n     }\n     return super.getDigest();",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "status": "modified",
                "changes": 13,
                "deletions": 7,
                "sha": "4fea4564c481e01a5adfe81a65ecd83fb69930a7",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -796,12 +796,12 @@ private boolean getallColumnsInteresting(FileSystem fs, Path metadataParentDir,\n     return metadataSummary.isAllColumnsInteresting();\n   }\n \n-  private static Path getSummaryFileName(Path metadataParentDir) {\n+  public static Path getSummaryFileName(Path metadataParentDir) {\n     Path summaryFile = new Path(metadataParentDir, METADATA_SUMMARY_FILENAME);\n     return summaryFile;\n   }\n \n-  private static Path getDirFileName(Path metadataParentDir) {\n+  public static Path getDirFileName(Path metadataParentDir) {\n     Path metadataDirFile = new Path(metadataParentDir, METADATA_DIRECTORIES_FILENAME);\n     return metadataDirFile;\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "2b0581c39516cc22c2915f10d291ab29b3b16799",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            },
            {
                "patch": "@@ -163,4 +163,79 @@ public void ensureCorrectCountWithMissingStatistics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testCountsWithMetadataCacheSummary() throws Exception {\n+    test(\"use dfs.tmp\");\n+    String tableName = \"parquet_table_counts\";\n+\n+    try {\n+      test(String.format(\"create table `%s/1` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/2` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/3` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/4` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+\n+      test(\"refresh table metadata %s\", tableName);\n+\n+      String sql = String.format(\"select\\n\" +\n+              \"count(*) as star_count,\\n\" +\n+              \"count(col_int) as int_column_count,\\n\" +\n+              \"count(col_vrchr) as vrchr_column_count\\n\" +\n+              \"from %s\", tableName);\n+\n+      int expectedNumFiles = 1;\n+      String numFilesPattern = \"numFiles = \" + expectedNumFiles;\n+      String usedMetaSummaryPattern = \"usedMetadataSummaryFile = true\";\n+      String recordReaderPattern = \"DynamicPojoRecordReader\";\n+\n+      testPlanMatchingPatterns(sql, new String[]{numFilesPattern, usedMetaSummaryPattern, recordReaderPattern});\n+\n+      testBuilder()\n+              .sqlQuery(sql)\n+              .unOrdered()\n+              .baselineColumns(\"star_count\", \"int_column_count\", \"vrchr_column_count\")\n+              .baselineValues(24L, 8L, 12L)\n+              .go();\n+\n+    } finally {\n+      test(\"drop table if exists %s\", tableName);\n+    }\n+  }\n+\n+  @Test\n+  public void testCountsWithMetadataCacheSummaryAndDirPruning() throws Exception {\n+    test(\"use dfs.tmp\");\n+    String tableName = \"parquet_table_counts\";\n+\n+    try {\n+      test(String.format(\"create table `%s/1` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/2` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/3` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/4` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+\n+      test(\"refresh table metadata %s\", tableName);\n+\n+      String sql = String.format(\"select\\n\" +\n+              \"count(*) as star_count,\\n\" +\n+              \"count(col_int) as int_column_count,\\n\" +\n+              \"count(col_vrchr) as vrchr_column_count\\n\" +\n+              \"from %s where dir0 = 1 \", tableName);\n+\n+      int expectedNumFiles = 1;\n+      String numFilesPattern = \"numFiles = \" + expectedNumFiles;\n+      String usedMetaSummaryPattern = \"usedMetadataSummaryFile = false\";\n+      String recordReaderPattern = \"DynamicPojoRecordReader\";\n+\n+      testPlanMatchingPatterns(sql, new String[]{numFilesPattern, usedMetaSummaryPattern, recordReaderPattern});\n+\n+      testBuilder()\n+              .sqlQuery(sql)\n+              .unOrdered()\n+              .baselineColumns(\"star_count\", \"int_column_count\", \"vrchr_column_count\")\n+              .baselineValues(6L, 2L, 3L)\n+              .go();\n+\n+    } finally {\n+      test(\"drop table if exists %s\", tableName);\n+    }\n+  }\n }",
                "additions": 75,
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "status": "modified",
                "changes": 75,
                "deletions": 0,
                "sha": "d18ed45277aa7f75328394d23d9bb6fd71629261",
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad"
            }
        ],
        "bug_id": "drill_11",
        "parent": "https://github.com/apache/drill/commit/f3022eacebe63fe4d7a4a9e22a8c398e88e42588",
        "message": "DRILL-7064: Leverage the summary metadata for plain COUNT aggregates.\n\nAdd unit test\n\nModify MetadataDirectGroupScan to track summary file information and use in unit test.\n\nConflicts:\n\texec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java\n\nFix NPE for DrillTable to account for non-eligible tables.\n\nFix bug with direct scan after directory pruning.  Add unit test.\n\nAddress review comments.\n\ncloses #1736",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/0ed56833f4a1e6dedf33289d9bcd968e28555b31",
        "file": [
            {
                "patch": "@@ -62,6 +62,8 @@\n   private int remainderIndex = 0;\n   private int recordCount;\n   private MaterializedField unnestFieldMetadata;\n+  // Reference of TypedFieldId for Unnest column. It's always set in schemaChanged method and later used by others\n+  private TypedFieldId unnestTypedFieldId;\n   private final UnnestMemoryManager memoryManager;\n \n   public enum Metric implements MetricDef {\n@@ -95,12 +97,8 @@ public void update() {\n       // Get sizing information for the batch.\n       setRecordBatchSizer(new RecordBatchSizer(incoming));\n \n-      final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n-      final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n-\n       // Get column size of unnest column.\n-\n-      RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(field.getName());\n+      RecordBatchSizer.ColumnSize columnSize = getRecordBatchSizer().getColumn(unnestFieldMetadata.getName());\n \n       final int rowIdColumnSize = TypeHelper.getSize(rowIdVector.getField().getType());\n \n@@ -213,22 +211,15 @@ public IterOutcome innerNext() {\n       container.zeroVectors();\n       // Check if schema has changed\n       if (lateral.getRecordIndex() == 0) {\n-        boolean hasNewSchema = schemaChanged();\n-        stats.batchReceived(0, incoming.getRecordCount(), hasNewSchema);\n-        if (hasNewSchema) {\n-          try {\n+        try {\n+          boolean hasNewSchema = schemaChanged();\n+          stats.batchReceived(0, incoming.getRecordCount(), hasNewSchema);\n+          if (hasNewSchema) {\n             setupNewSchema();\n             hasRemainder = true;\n             memoryManager.update();\n-          } catch (SchemaChangeException ex) {\n-            kill(false);\n-            logger.error(\"Failure during query\", ex);\n-            context.getExecutorState().fail(ex);\n-            return IterOutcome.STOP;\n-          }\n-          return OK_NEW_SCHEMA;\n-        } else { // Unnest field schema didn't changed but new left empty/nonempty batch might come with OK_NEW_SCHEMA\n-          try {\n+            return OK_NEW_SCHEMA;\n+          } else { // Unnest field schema didn't changed but new left empty/nonempty batch might come with OK_NEW_SCHEMA\n             // This means even though there is no schema change for unnest field the reference of unnest field\n             // ValueVector must have changed hence we should just refresh the transfer pairs and keep output vector\n             // same as before. In case when new left batch is received with SchemaChange but was empty Lateral will\n@@ -237,19 +228,18 @@ public IterOutcome innerNext() {\n             // pair. It should do for each new left incoming batch.\n             resetUnnestTransferPair();\n             container.zeroVectors();\n-          } catch (SchemaChangeException ex) {\n-            kill(false);\n-            logger.error(\"Failure during query\", ex);\n-            context.getExecutorState().fail(ex);\n-            return IterOutcome.STOP;\n-          }\n-        } // else\n-        unnest.resetGroupIndex();\n-        memoryManager.update();\n+          } // else\n+          unnest.resetGroupIndex();\n+          memoryManager.update();\n+        } catch (SchemaChangeException ex) {\n+          kill(false);\n+          logger.error(\"Failure during query\", ex);\n+          context.getExecutorState().fail(ex);\n+          return IterOutcome.STOP;\n+        }\n       }\n       return doWork();\n     }\n-\n   }\n \n     @Override\n@@ -259,11 +249,10 @@ public VectorContainer getOutgoingContainer() {\n \n   @SuppressWarnings(\"resource\")\n   private void setUnnestVector() {\n-    final TypedFieldId typedFieldId = incoming.getValueVectorId(popConfig.getColumn());\n-    final MaterializedField field = incoming.getSchema().getColumn(typedFieldId.getFieldIds()[0]);\n+    final MaterializedField field = incoming.getSchema().getColumn(unnestTypedFieldId.getFieldIds()[0]);\n     final RepeatedValueVector vector;\n     final ValueVector inVV =\n-        incoming.getValueAccessorById(field.getValueClass(), typedFieldId.getFieldIds()).getValueVector();\n+        incoming.getValueAccessorById(field.getValueClass(), unnestTypedFieldId.getFieldIds()).getValueVector();\n \n     if (!(inVV instanceof RepeatedValueVector)) {\n       if (incoming.getRecordCount() != 0) {\n@@ -333,10 +322,11 @@ protected IterOutcome doWork() {\n    * the end of one of the other vectors while we are copying the data of the other vectors alongside each new unnested\n    * value coming out of the repeated field.)\n    */\n-  @SuppressWarnings(\"resource\") private TransferPair getUnnestFieldTransferPair(FieldReference reference) {\n-    final TypedFieldId fieldId = incoming.getValueVectorId(popConfig.getColumn());\n-    final Class<?> vectorClass = incoming.getSchema().getColumn(fieldId.getFieldIds()[0]).getValueClass();\n-    final ValueVector unnestField = incoming.getValueAccessorById(vectorClass, fieldId.getFieldIds()).getValueVector();\n+  @SuppressWarnings(\"resource\")\n+  private TransferPair getUnnestFieldTransferPair(FieldReference reference) {\n+    final int[] typeFieldIds = unnestTypedFieldId.getFieldIds();\n+    final Class<?> vectorClass = incoming.getSchema().getColumn(typeFieldIds[0]).getValueClass();\n+    final ValueVector unnestField = incoming.getValueAccessorById(vectorClass, typeFieldIds).getValueVector();\n \n     TransferPair tp = null;\n     if (unnestField instanceof RepeatedMapVector) {\n@@ -398,9 +388,9 @@ protected boolean setupNewSchema() throws SchemaChangeException {\n    *\n    * @return true if the schema has changed, false otherwise\n    */\n-  private boolean schemaChanged() {\n-    final TypedFieldId fieldId = incoming.getValueVectorId(popConfig.getColumn());\n-    final MaterializedField thisField = incoming.getSchema().getColumn(fieldId.getFieldIds()[0]);\n+  private boolean schemaChanged() throws SchemaChangeException {\n+    unnestTypedFieldId = checkAndGetUnnestFieldId();\n+    final MaterializedField thisField = incoming.getSchema().getColumn(unnestTypedFieldId.getFieldIds()[0]);\n     final MaterializedField prevField = unnestFieldMetadata;\n     Preconditions.checkNotNull(thisField);\n \n@@ -440,6 +430,17 @@ private void updateStats() {\n \n   }\n \n+  private TypedFieldId checkAndGetUnnestFieldId() throws SchemaChangeException {\n+    final TypedFieldId fieldId = incoming.getValueVectorId(popConfig.getColumn());\n+    if (fieldId == null) {\n+      throw new SchemaChangeException(String.format(\"Unnest column %s not found inside the incoming record batch. \" +\n+          \"This may happen if a wrong Unnest column name is used in the query. Please rerun query after fixing that.\",\n+        popConfig.getColumn()));\n+    }\n+\n+    return fieldId;\n+  }\n+\n   @Override\n   public void close() {\n     updateStats();",
                "additions": 39,
                "raw_url": "https://github.com/apache/drill/raw/0ed56833f4a1e6dedf33289d9bcd968e28555b31/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/unnest/UnnestRecordBatch.java",
                "status": "modified",
                "changes": 77,
                "deletions": 38,
                "sha": "a00fae67bd5b7f9ff780bd73c15081a9b8acfd3c",
                "blob_url": "https://github.com/apache/drill/blob/0ed56833f4a1e6dedf33289d9bcd968e28555b31/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/unnest/UnnestRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/unnest/UnnestRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/unnest/UnnestRecordBatch.java?ref=0ed56833f4a1e6dedf33289d9bcd968e28555b31"
            }
        ],
        "bug_id": "drill_12",
        "parent": "https://github.com/apache/drill/commit/b8376cce07dc6b5fe2b3408a4b4a88c3b21816dd",
        "message": "DRILL-6694: NPE in UnnestRecordBatch when query uses a column name not present in data\n\ncloses #1434",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/aa127b70b1e46f7f4aa19881f25eda583627830a",
        "file": [
            {
                "patch": "@@ -32,10 +32,12 @@\n import org.apache.drill.exec.physical.PhysicalPlan;\n import org.apache.drill.exec.planner.sql.DirectPlan;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.store.AbstractSchema;\n import org.apache.drill.exec.store.StoragePlugin;\n import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n import org.apache.drill.exec.store.dfs.FileSystemSchemaFactory;\n import org.apache.drill.exec.store.dfs.WorkspaceConfig;\n+import org.apache.drill.exec.work.foreman.ForemanSetupException;\n \n import java.util.List;\n import java.util.Map;\n@@ -68,33 +70,38 @@ public SerializableString getEscapeSequence(int i) {\n \n \n   @Override\n-  public PhysicalPlan getPlan(SqlNode sqlNode) {\n-    SqlIdentifier schema = ((SqlDescribeSchema) sqlNode).getSchema();\n-    SchemaPlus drillSchema = SchemaUtilites.findSchema(config.getConverter().getDefaultSchema(), schema.names);\n-\n-    if (drillSchema != null) {\n-      StoragePlugin storagePlugin;\n-      try {\n-        storagePlugin = context.getStorage().getPlugin(schema.names.get(0));\n-      } catch (ExecutionSetupException e) {\n-        throw new DrillRuntimeException(\"Failure while retrieving storage plugin\", e);\n+  public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {\n+    SqlIdentifier schema = unwrap(sqlNode, SqlDescribeSchema.class).getSchema();\n+    SchemaPlus schemaPlus = SchemaUtilites.findSchema(config.getConverter().getDefaultSchema(), schema.names);\n+\n+    if (schemaPlus == null) {\n+      throw UserException.validationError()\n+        .message(\"Invalid schema name [%s]\", Joiner.on(\".\").join(schema.names))\n+        .build(logger);\n+    }\n+\n+    StoragePlugin storagePlugin;\n+    try {\n+      AbstractSchema drillSchema = SchemaUtilites.unwrapAsDrillSchemaInstance(schemaPlus);\n+      storagePlugin = context.getStorage().getPlugin(drillSchema.getSchemaPath().get(0));\n+      if (storagePlugin == null) {\n+        throw new DrillRuntimeException(String.format(\"Unable to find storage plugin with the following name [%s].\",\n+          drillSchema.getSchemaPath().get(0)));\n       }\n-      String properties;\n-      try {\n-        final Map configMap = mapper.convertValue(storagePlugin.getConfig(), Map.class);\n-        if (storagePlugin instanceof FileSystemPlugin) {\n-          transformWorkspaces(schema.names, configMap);\n-        }\n-        properties = mapper.writeValueAsString(configMap);\n-      } catch (JsonProcessingException e) {\n-        throw new DrillRuntimeException(\"Error while trying to convert storage config to json string\", e);\n+    } catch (ExecutionSetupException e) {\n+      throw new DrillRuntimeException(\"Failure while retrieving storage plugin\", e);\n+    }\n+\n+    try {\n+      Map configMap = mapper.convertValue(storagePlugin.getConfig(), Map.class);\n+      if (storagePlugin instanceof FileSystemPlugin) {\n+        transformWorkspaces(schema.names, configMap);\n       }\n+      String properties = mapper.writeValueAsString(configMap);\n       return DirectPlan.createDirectPlan(context, new DescribeSchemaResult(Joiner.on(\".\").join(schema.names), properties));\n+    } catch (JsonProcessingException e) {\n+      throw new DrillRuntimeException(\"Error while trying to convert storage config to json string\", e);\n     }\n-\n-    throw UserException.validationError()\n-          .message(String.format(\"Invalid schema name [%s]\", Joiner.on(\".\").join(schema.names)))\n-          .build(logger);\n   }\n \n   /**",
                "additions": 30,
                "raw_url": "https://github.com/apache/drill/raw/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "status": "modified",
                "changes": 53,
                "deletions": 23,
                "sha": "bb51ef0b6238c71110e9356606ed529cada0b715",
                "blob_url": "https://github.com/apache/drill/blob/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java?ref=aa127b70b1e46f7f4aa19881f25eda583627830a"
            },
            {
                "patch": "@@ -382,10 +382,16 @@ public void describeSchemaSyntax() throws Exception {\n     test(\"describe database dfs.`default`\");\n   }\n \n+  @Test\n+  public void describePartialSchema() throws Exception {\n+    test(\"use dfs\");\n+    test(\"describe schema tmp\");\n+  }\n+\n   @Test\n   public void describeSchemaOutput() throws Exception {\n     final List<QueryDataBatch> result = testSqlWithResults(\"describe schema dfs.tmp\");\n-    assertTrue(result.size() == 1);\n+    assertEquals(1, result.size());\n     final QueryDataBatch batch = result.get(0);\n     final RecordBatchLoader loader = new RecordBatchLoader(getDrillbitContext().getAllocator());\n     loader.load(batch.getHeader().getDef(), batch.getData());",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "status": "modified",
                "changes": 8,
                "deletions": 1,
                "sha": "e0ed2fb0faa81c1bac7bc1c3a5a42d363dd6f612",
                "blob_url": "https://github.com/apache/drill/blob/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java?ref=aa127b70b1e46f7f4aa19881f25eda583627830a"
            }
        ],
        "bug_id": "drill_13",
        "parent": "https://github.com/apache/drill/commit/6823a8f241a58a4ae1dc1c6e84ba6a9920cc1643",
        "message": "DRILL-6523: Fix NPE for describe of partial schema\n\ncloses #1332",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/4848a56447c7733cae018358a8a814db66d58853",
        "file": [
            {
                "patch": "@@ -211,6 +211,12 @@ public void waitForGracePeriod() {\n     exitLatch.awaitUninterruptibly(gracePeriod);\n   }\n \n+  private void updateState(State state) {\n+    if ( registrationHandle != null) {\n+      coord.update(registrationHandle, state);\n+    }\n+  }\n+\n   /*\n \n    */\n@@ -228,14 +234,14 @@ public synchronized void close() {\n     }\n     final Stopwatch w = Stopwatch.createStarted();\n     logger.debug(\"Shutdown begun.\");\n-    registrationHandle = coord.update(registrationHandle, State.QUIESCENT);\n+    updateState(State.QUIESCENT);\n     stateManager.setState(DrillbitState.GRACE);\n     waitForGracePeriod();\n     stateManager.setState(DrillbitState.DRAINING);\n     // wait for all the in-flight queries to finish\n     manager.waitToExit(forcefulShutdown);\n     //safe to exit\n-    registrationHandle = coord.update(registrationHandle, State.OFFLINE);\n+    updateState(State.OFFLINE);\n     stateManager.setState(DrillbitState.OFFLINE);\n     if(quiescentMode == true) {\n       return;",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/4848a56447c7733cae018358a8a814db66d58853/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "status": "modified",
                "changes": 10,
                "deletions": 2,
                "sha": "626a551ebcbb636cdc16604fc9608eb9aa3eac9f",
                "blob_url": "https://github.com/apache/drill/blob/4848a56447c7733cae018358a8a814db66d58853/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java?ref=4848a56447c7733cae018358a8a814db66d58853"
            }
        ],
        "bug_id": "drill_14",
        "parent": "https://github.com/apache/drill/commit/2601cdd33e0685f59a7bf2ac72541bd9dcaaa18f",
        "message": "DRILL-6302: Fixed NPE in Drillbit close method\n\ncloses #1217",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/1355bfddb1c76462366e28ff7f98fdb6823b4b2a",
        "file": [
            {
                "patch": "@@ -95,8 +95,14 @@ public IterOutcome innerNext() {\n             return upstream;\n \n           case NOT_YET:\n-          case NONE:\n             break;\n+          case NONE:\n+            if (schema != null) {\n+              // Schema is for the output batch schema which is setup in setupNewSchema(). Since the output\n+              // schema is fixed ((Fragment(VARCHAR), Number of records written (BIGINT)) we should set it\n+              // up even with 0 records for it to be reported back to the client.\n+              break;\n+            }\n \n           case OK_NEW_SCHEMA:\n             setupNewSchema();",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "status": "modified",
                "changes": 8,
                "deletions": 1,
                "sha": "65d0c54d1c020da199f0a85ee1a035b5fa78f210",
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a"
            },
            {
                "patch": "@@ -230,7 +230,8 @@ private void newSchema() throws IOException {\n     // Its value is likely below Integer.MAX_VALUE (2GB), although rowGroupSize is a long type.\n     // Therefore this size is cast to int, since allocating byte array in under layer needs to\n     // limit the array size in an int scope.\n-    int initialBlockBufferSize = max(MINIMUM_BUFFER_SIZE, blockSize / this.schema.getColumns().size() / 5);\n+    int initialBlockBufferSize = this.schema.getColumns().size() > 0 ?\n+        max(MINIMUM_BUFFER_SIZE, blockSize / this.schema.getColumns().size() / 5) : MINIMUM_BUFFER_SIZE;\n     // We don't want this number to be too small either. Ideally, slightly bigger than the page size,\n     // but not bigger than the block buffer\n     int initialPageBufferSize = max(MINIMUM_BUFFER_SIZE, min(pageSize + pageSize / 10, initialBlockBufferSize));",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "0e40c9e360b3f183bf9ba7c35e822e3f495f8d8a",
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a"
            },
            {
                "patch": "@@ -315,6 +315,16 @@ public void testCreateTableIfNotExistsWhenTableWithSameNameDoesNotExist() throws\n     }\n   }\n \n+  @Test\n+  public void testCTASWithEmptyJson() throws Exception {\n+    final String newTblName = \"tbl4444\";\n+    try {\n+      test(String.format(\"CREATE TABLE %s.%s AS SELECT * FROM cp.`project/pushdown/empty.json`\", DFS_TMP_SCHEMA, newTblName));\n+    } finally {\n+      test(\"DROP TABLE IF EXISTS %s.%s\", DFS_TMP_SCHEMA, newTblName);\n+    }\n+  }\n+\n   private static void ctasErrorTestHelper(final String ctasSql, final String expErrorMsg) throws Exception {\n     final String createTableSql = String.format(ctasSql, \"testTableName\");\n     errorMsgTestHelper(createTableSql, expErrorMsg);",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "2e7c052fce7ca3840c897eda232e8ef48147a3e0",
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a"
            }
        ],
        "bug_id": "drill_15",
        "parent": "https://github.com/apache/drill/commit/18766950c640c6963ffd1c94224c4a984bedd3c1",
        "message": "DRILL-3964 : Fix NPE in WriterRecordBatch when 0 rows\n\ncloses #1290",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/a4712c24101e7131a1161a5e79a619326e330890",
        "file": [
            {
                "patch": "@@ -1774,7 +1774,7 @@\n       <properties>\n         <alt-hadoop>mapr</alt-hadoop>\n         <rat.excludeSubprojects>true</rat.excludeSubprojects>\n-        <hive.version>1.2.0-mapr-1608</hive.version>\n+        <hive.version>1.2.0-mapr-1707</hive.version>\n         <hbase.version>1.1.1-mapr-1602-m7-5.2.0</hbase.version>\n         <hadoop.version>2.7.0-mapr-1607</hadoop.version>\n       </properties>",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/a4712c24101e7131a1161a5e79a619326e330890/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "31f0f6955932af90a2d090a2aa6685692de6182f",
                "blob_url": "https://github.com/apache/drill/blob/a4712c24101e7131a1161a5e79a619326e330890/pom.xml",
                "filename": "pom.xml",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/pom.xml?ref=a4712c24101e7131a1161a5e79a619326e330890"
            }
        ],
        "bug_id": "drill_16",
        "parent": "https://github.com/apache/drill/commit/fd58e2944b4d03538ed981359940ef486279b626",
        "message": "DRILL-5906: java.lang.NullPointerException while quering Hive ORC tables on MapR cluster\n\n- Upgrade drill to 1.2.0-mapr-1707 hive.version.\n\ncloses #1010",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/c8a78409d9472b84eaa7fba719842c08302c81c3",
        "file": [
            {
                "patch": "@@ -137,6 +137,7 @@ public void output() {\n   @Override\n   public void reset() {\n     value = new ObjectHolder();\n+    value.obj = new org.apache.drill.exec.expr.fn.impl.DrillByteArray();\n     init.value = 0;\n     nonNullCount.value = 0;\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "11b20b1c8276d53381a59951cdf4316c23e4bf71",
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "filename": "exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java?ref=c8a78409d9472b84eaa7fba719842c08302c81c3"
            },
            {
                "patch": "@@ -19,7 +19,6 @@\n \n import org.apache.drill.BaseTestQuery;\n import org.apache.drill.PlanTestBase;\n-import org.apache.drill.common.types.TypeProtos;\n import org.apache.drill.common.util.TestTools;\n import org.junit.Ignore;\n import org.junit.Test;\n@@ -457,4 +456,8 @@ public void testGroupBySystemFuncFileSystemTable() throws Exception {\n         .build().run();\n   }\n \n+  @Test\n+  public void test4443() throws Exception {\n+    test(\"SELECT MIN(columns[1]) FROM dfs_test.`%s/agg/4443.csv` GROUP BY columns[0]\", TEST_RES_PATH);\n+  }\n }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "43e206e5dd870260e5796041efd0ad917d926f5f",
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java?ref=c8a78409d9472b84eaa7fba719842c08302c81c3"
            },
            {
                "patch": "@@ -0,0 +1,2 @@\n+a,1\n+b,2\n\\ No newline at end of file",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/resources/agg/4443.csv",
                "status": "added",
                "changes": 2,
                "deletions": 0,
                "sha": "bf85e264a48b2d4395980f7529e5d6aa71ae61fc",
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/resources/agg/4443.csv",
                "filename": "exec/java-exec/src/test/resources/agg/4443.csv",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/agg/4443.csv?ref=c8a78409d9472b84eaa7fba719842c08302c81c3"
            }
        ],
        "bug_id": "drill_17",
        "parent": "https://github.com/apache/drill/commit/447b093cd2b05bfeae001844a7e3573935e84389",
        "message": "DRILL-4443: MIN/MAX on VARCHAR throw a NullPointerException",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/a26fbec13134f249e258be6735b82cf09ab1f406",
        "file": [
            {
                "patch": "@@ -29,6 +29,9 @@\n import freemarker.cache.WebappTemplateLoader;\n import freemarker.core.HTMLOutputFormat;\n import freemarker.template.Configuration;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n import org.apache.drill.common.config.DrillConfig;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.memory.BufferAllocator;\n@@ -108,10 +111,17 @@ public DrillRestServer(final WorkManager workManager, final ServletContext servl\n     provider.setMapper(workManager.getContext().getLpPersistence().getMapper());\n     register(provider);\n \n+    // Get an EventExecutor out of the BitServer EventLoopGroup to notify listeners for WebUserConnection. For\n+    // actual connections between Drillbits this EventLoopGroup is used to handle network related events. Though\n+    // there is no actual network connection associated with WebUserConnection but we need a CloseFuture in\n+    // WebSessionResources, so we are using EvenExecutor from network EventLoopGroup pool.\n+    final EventExecutor executor = workManager.getContext().getBitLoopGroup().next();\n+\n     register(new AbstractBinder() {\n       @Override\n       protected void configure() {\n         bind(workManager).to(WorkManager.class);\n+        bind(executor).to(EventExecutor.class);\n         bind(workManager.getContext().getLpPersistence().getMapper()).to(ObjectMapper.class);\n         bind(workManager.getContext().getStoreProvider()).to(PersistentStoreProvider.class);\n         bind(workManager.getContext().getStorage()).to(StoragePluginRegistry.class);\n@@ -159,6 +169,9 @@ private Configuration getFreemarkerConfiguration(ServletContext servletContext)\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -204,9 +217,15 @@ public WebUserConnection provide() {\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_RESERVATION),\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_MAXIMUM));\n \n+        // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+        // listener to known about channel close event from underlying layer. We use this future to notify Foreman\n+        // listeners when the Web session (not connection) between Web Client and WebServer is closed. This will help\n+        // Foreman to cancel all the running queries for this Web Client.\n+        final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n         // Create a WebSessionResource instance which owns the lifecycle of all the session resources.\n-        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed.\n-        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession);\n+        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed\n+        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession, closeFuture);\n         session.setAttribute(WebSessionResources.class.getSimpleName(), webSessionResources);\n       }\n       // Create a new WebUserConnection for the request\n@@ -227,6 +246,9 @@ public void dispose(WebUserConnection instance) {\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -260,8 +282,15 @@ public WebUserConnection provide() {\n         logger.trace(\"Failed to get the remote address of the http session request\", ex);\n       }\n \n-      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator,\n-              remoteAddress, drillUserSession);\n+      // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+      // listener to known about channel close event from underlying layer.\n+      //\n+      // The invocation of this close future is no-op as it will be triggered after query completion in unsecure case.\n+      // But we need this close future as it's expected by Foreman.\n+      final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress,\n+          drillUserSession, closeFuture);\n \n       // Create a AnonWenUserConnection for this request\n       return new AnonWebUserConnection(webSessionResources);",
                "additions": 33,
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "status": "modified",
                "changes": 37,
                "deletions": 4,
                "sha": "15458478df958d7923a2ad968cf754c2808d4ee8",
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406"
            },
            {
                "patch": "@@ -19,7 +19,6 @@\n package org.apache.drill.exec.server.rest;\n \n import io.netty.channel.ChannelPromise;\n-import io.netty.channel.DefaultChannelPromise;\n import org.apache.drill.common.AutoCloseables;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.rpc.ChannelClosedException;\n@@ -43,11 +42,12 @@\n \n   private ChannelPromise closeFuture;\n \n-  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress, UserSession userSession) {\n+  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress,\n+                      UserSession userSession, ChannelPromise closeFuture) {\n     this.allocator = allocator;\n     this.remoteAddress = remoteAddress;\n     this.webUserSession = userSession;\n-    closeFuture = new DefaultChannelPromise(null);\n+    this.closeFuture = closeFuture;\n   }\n \n   public UserSession getSession() {\n@@ -68,16 +68,20 @@ public SocketAddress getRemoteAddress() {\n \n   @Override\n   public void close() {\n-\n     try {\n       AutoCloseables.close(webUserSession, allocator);\n     } catch (Exception ex) {\n       logger.error(\"Failure while closing the session resources\", ex);\n     }\n \n-    // Set the close future associated with this session.\n+    // Notify all the listeners of this closeFuture for failure events so that listeners can do cleanup related to this\n+    // WebSession. This will be called after every query execution by AnonymousWebUserConnection::cleanupSession and\n+    // for authenticated user it is called when session is invalidated.\n+    // For authenticated user it will cancel the in-flight queries based on session invalidation. Whereas for\n+    // unauthenticated user it's a no-op since there is no session associated with it. We don't have mechanism currently\n+    // to call this close future upon Http connection close.\n     if (closeFuture != null) {\n-      closeFuture.setFailure(new ChannelClosedException(\"Http Session of the user is closed.\"));\n+      closeFuture.setFailure(new ChannelClosedException(\"Http connection is closed by Web Client\"));\n       closeFuture = null;\n     }\n   }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "status": "modified",
                "changes": 16,
                "deletions": 6,
                "sha": "2ca457c02e142e6ca6c53d1d37398436690484ac",
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406"
            },
            {
                "patch": "@@ -42,9 +42,14 @@\n import java.util.Set;\n \n /**\n- * WebUserConnectionWrapper which represents the UserClientConnection for the WebUser submitting the query. It provides\n- * access to the UserSession executing the query. There is no actual physical channel corresponding to this connection\n- * wrapper.\n+ * WebUserConnectionWrapper which represents the UserClientConnection between WebServer and Foreman, for the WebUser\n+ * submitting the query. It provides access to the UserSession executing the query. There is no actual physical\n+ * channel corresponding to this connection wrapper.\n+ *\n+ * It returns a close future with no actual underlying {@link io.netty.channel.Channel} associated with it but do have an\n+ * EventExecutor out of BitServer EventLoopGroup. Since there is no actual connection established using this class,\n+ * hence the close event will never be fired by underlying layer and close future is set only when the\n+ * {@link WebSessionResources} are closed.\n  */\n \n public class WebUserConnection extends AbstractDisposableUserClientConnection implements ConnectionThrottle {",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "status": "modified",
                "changes": 11,
                "deletions": 3,
                "sha": "f46b5e5e69b4758bb3866e84965213feefdd4648",
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406"
            },
            {
                "patch": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.server.rest;\n+\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n+import io.netty.util.concurrent.Future;\n+import io.netty.util.concurrent.GenericFutureListener;\n+import org.apache.drill.exec.memory.BufferAllocator;\n+import org.apache.drill.exec.rpc.TransportCheck;\n+import org.apache.drill.exec.rpc.user.UserSession;\n+import org.junit.Test;\n+\n+import java.net.SocketAddress;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+/**\n+ * Validates {@link WebSessionResources} close works as expected w.r.t {@link io.netty.channel.AbstractChannel.CloseFuture}\n+ * associated with it.\n+ */\n+public class WebSessionResourcesTest {\n+  //private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WebSessionResourcesTest.class);\n+\n+  private WebSessionResources webSessionResources;\n+\n+  private boolean listenerComplete;\n+\n+  private CountDownLatch latch;\n+\n+  private EventExecutor executor;\n+\n+  // A close listener added in close future in one of the test to see if it's invoked correctly.\n+  private class TestClosedListener implements GenericFutureListener<Future<Void>> {\n+    @Override\n+    public void operationComplete(Future<Void> future) throws Exception {\n+      listenerComplete = true;\n+      latch.countDown();\n+    }\n+  }\n+\n+  /**\n+   * Validates {@link WebSessionResources#close()} throws NPE when closefuture passed to WebSessionResources doesn't\n+   * have a valid channel and EventExecutor associated with it.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithNullExecutor() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      fail();\n+    } catch (Exception e) {\n+      assertTrue(e instanceof NullPointerException);\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and other parameters.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithValidExecutor() throws Exception {\n+    try {\n+      EventExecutor mockExecutor = mock(EventExecutor.class);\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mockExecutor);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      verify(mockExecutor).inEventLoop();\n+      verify(mockExecutor).execute(any(Runnable.class));\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+      assertTrue(!listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates double call to {@link WebSessionResources#close()} doesn't throw any exception.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDoubleClose() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mock(EventExecutor.class));\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      webSessionResources.close();\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and {@link TestClosedListener}\n+   * getting invoked which is added to the close future.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testCloseWithListener() throws Exception {\n+    try {\n+      // Assign latch, executor and closeListener for this test case\n+      GenericFutureListener<Future<Void>> closeListener = new TestClosedListener();\n+      latch = new CountDownLatch(1);\n+      executor = TransportCheck.createEventLoopGroup(1, \"Test-Thread\").next();\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      // create WebSessionResources with above ChannelPromise to notify listener\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class),\n+          mock(UserSession.class), closeFuture);\n+\n+      // Add the Test Listener to close future\n+      assertTrue(!listenerComplete);\n+      closeFuture.addListener(closeListener);\n+\n+      // Close the WebSessionResources\n+      webSessionResources.close();\n+\n+      // Verify the states\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      // Since listener will be invoked so test should not wait forever\n+      latch.await();\n+      assertTrue(listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    } finally {\n+      listenerComplete = false;\n+      executor.shutdownGracefully();\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "additions": 168,
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "status": "added",
                "changes": 168,
                "deletions": 0,
                "sha": "bb990de6ef28abac7e9c862a508784b7997a9dcb",
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406"
            }
        ],
        "bug_id": "drill_18",
        "parent": "https://github.com/apache/drill/commit/8eda4d7749c129c692f9e57db4c2a755a9139052",
        "message": "DRILL-5874: NPE in AnonWebUserConnection.cleanupSession()\n\ncloses #993",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
        "file": [
            {
                "patch": "@@ -1,2 +1,8 @@\n drill-mapr-plugin\n =================\n+By default all the tests in contrib/format-maprdb are disabled.\n+To enable and run these tests please use -Pmapr profile to\n+compile and execute the tests.\n+\n+Here is an example of the mvn command to use to run these tests.\n+mvn install -Dtests=cluster -Pmapr",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "a94a7cb012bddb432f97bd276621adcf4b292642",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "filename": "contrib/format-maprdb/README.md",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/README.md?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            },
            {
                "patch": "@@ -32,6 +32,12 @@\n   public boolean ignoreSchemaChange = false;\n   public boolean readAllNumbersAsDouble = false;\n   public boolean disableCountOptimization = false;\n+  /* This flag is a switch to do special handling in case of\n+   * no columns in the query exists in the maprdb table. This flag\n+   * can get deprecated once it is observed that this special handling\n+   * is not regressing performance of reading maprdb table.\n+   */\n+  public boolean nonExistentFieldSupport = true;\n \n   @Override\n   public int hashCode() {\n@@ -40,6 +46,7 @@ public int hashCode() {\n     result = 31 * result + (ignoreSchemaChange ? 1231 : 1237);\n     result = 31 * result + (readAllNumbersAsDouble ? 1231 : 1237);\n     result = 31 * result + (disableCountOptimization ? 1231 : 1237);\n+    result = 31 * result + (nonExistentFieldSupport ? 1231 : 1237);\n     return result;\n   }\n \n@@ -56,6 +63,8 @@ protected boolean impEquals(Object obj) {\n       return false;\n     } else if (disableCountOptimization != other.disableCountOptimization) {\n       return false;\n+    } else if (nonExistentFieldSupport != other.nonExistentFieldSupport) {\n+      return false;\n     }\n     return true;\n   }\n@@ -76,6 +85,8 @@ public boolean isEnablePushdown() {\n     return enablePushdown;\n   }\n \n+  public boolean isNonExistentFieldSupport() { return nonExistentFieldSupport; }\n+\n   public boolean isIgnoreSchemaChange() {\n     return ignoreSchemaChange;\n   }",
                "additions": 11,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "status": "modified",
                "changes": 11,
                "deletions": 0,
                "sha": "ad153fe7fa5e7f44628b084483f50b1258ad4048",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            },
            {
                "patch": "@@ -26,6 +26,7 @@\n import java.util.List;\n import java.util.Set;\n import java.util.Stack;\n+import java.util.Collections;\n import java.util.concurrent.TimeUnit;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n@@ -44,6 +45,7 @@\n import org.apache.drill.exec.util.Utilities;\n import org.apache.drill.exec.vector.BaseValueVector;\n import org.apache.drill.exec.vector.complex.impl.MapOrListWriterImpl;\n+import org.apache.drill.exec.vector.complex.fn.JsonReaderUtils;\n import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;\n import org.ojai.DocumentReader;\n import org.ojai.DocumentReader.EventType;\n@@ -95,6 +97,7 @@\n   private final boolean allTextMode;\n   private final boolean ignoreSchemaChange;\n   private final boolean disableCountOptimization;\n+  private final boolean nonExistentColumnsProjection;\n \n   public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n       MapRDBFormatPluginConfig formatPluginConfig,\n@@ -119,6 +122,7 @@ public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n     allTextMode = formatPluginConfig.isAllTextMode();\n     ignoreSchemaChange = formatPluginConfig.isIgnoreSchemaChange();\n     disablePushdown = !formatPluginConfig.isEnablePushdown();\n+    nonExistentColumnsProjection = formatPluginConfig.isNonExistentFieldSupport();\n   }\n \n   @Override\n@@ -230,6 +234,9 @@ public int next() {\n       }\n     }\n \n+    if (nonExistentColumnsProjection && recordCount > 0) {\n+      JsonReaderUtils.ensureAtLeastOneField(vectorWriter, getColumns(), allTextMode, Collections.EMPTY_LIST);\n+    }\n     vectorWriter.setValueCount(recordCount);\n     logger.debug(\"Took {} ms to get {} records\", watch.elapsed(TimeUnit.MILLISECONDS), recordCount);\n     return recordCount;",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "status": "modified",
                "changes": 7,
                "deletions": 0,
                "sha": "113b3adb7d98b56762feffa397cfadbbf2c8b0f6",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            },
            {
                "patch": "@@ -57,6 +57,16 @@ public void testSelectId() throws Exception {\n     runSQLAndVerifyCount(sql, 10);\n   }\n \n+  @Test\n+  public void testSelectNonExistentColumns() throws Exception {\n+    setColumnWidths(new int[] {23});\n+    final String sql = \"SELECT\\n\"\n+            + \"  something\\n\"\n+            + \"FROM\\n\"\n+            + \"  hbase.business business limit 5\";\n+    runSQLAndVerifyCount(sql, 5);\n+  }\n+\n   @Test\n   public void testKVGen() throws Exception {\n     setColumnWidths(new int[] {21, 10, 6});",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "26f54b837a1ff1e0ba1a4412aa9ee2567976e08c",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "filename": "contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            },
            {
                "patch": "@@ -104,71 +104,7 @@ public JsonReader(DrillBuf managedBuf, List<SchemaPath> columns,\n   @SuppressWarnings(\"resource\")\n   @Override\n   public void ensureAtLeastOneField(ComplexWriter writer) {\n-    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n-    List<PathSegment> fieldPathList = Lists.newArrayList();\n-    BitSet emptyStatus = new BitSet(columns.size());\n-\n-    // first pass: collect which fields are empty\n-    for (int i = 0; i < columns.size(); i++) {\n-      SchemaPath sp = columns.get(i);\n-      PathSegment fieldPath = sp.getRootSegment();\n-      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n-      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n-        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n-        fieldPath = fieldPath.getChild();\n-      }\n-      writerList.add(fieldWriter);\n-      fieldPathList.add(fieldPath);\n-      if (fieldWriter.isEmptyMap()) {\n-        emptyStatus.set(i, true);\n-      }\n-      if (i == 0 && !allTextMode) {\n-        // when allTextMode is false, there is not much benefit to producing all\n-        // the empty\n-        // fields; just produce 1 field. The reason is that the type of the\n-        // fields is\n-        // unknown, so if we produce multiple Integer fields by default, a\n-        // subsequent batch\n-        // that contains non-integer fields will error out in any case. Whereas,\n-        // with\n-        // allTextMode true, we are sure that all fields are going to be treated\n-        // as varchar,\n-        // so it makes sense to produce all the fields, and in fact is necessary\n-        // in order to\n-        // avoid schema change exceptions by downstream operators.\n-        break;\n-      }\n-\n-    }\n-\n-    // second pass: create default typed vectors corresponding to empty fields\n-    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n-    // may be\n-    // shared by multiple fields whereas we want to keep track of all fields\n-    // independently,\n-    // so we rely on the emptyStatus.\n-    for (int j = 0; j < fieldPathList.size(); j++) {\n-      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n-      PathSegment fieldPath = fieldPathList.get(j);\n-      if (emptyStatus.get(j)) {\n-        if (allTextMode) {\n-          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n-        } else {\n-          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n-        }\n-      }\n-    }\n-\n-    for (ListWriter field : emptyArrayWriters) {\n-      // checks that array has not been initialized\n-      if (field.getValueCapacity() == 0) {\n-        if (allTextMode) {\n-          field.varChar();\n-        } else {\n-          field.integer();\n-        }\n-      }\n-    }\n+    JsonReaderUtils.ensureAtLeastOneField(writer, columns, allTextMode, emptyArrayWriters);\n   }\n \n   public void setSource(int start, int end, DrillBuf buf) throws IOException {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "status": "modified",
                "changes": 66,
                "deletions": 65,
                "sha": "4ffbb26e01eb31852334203ef61bca5af592f18f",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            },
            {
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.vector.complex.fn;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.drill.common.expression.PathSegment;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.vector.complex.writer.BaseWriter;\n+\n+import java.util.BitSet;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public class JsonReaderUtils {\n+\n+  public static void ensureAtLeastOneField(BaseWriter.ComplexWriter writer,\n+                                    Collection<SchemaPath> columns,\n+                                    boolean allTextMode,\n+                                    List<BaseWriter.ListWriter> emptyArrayWriters) {\n+\n+    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n+    List<PathSegment> fieldPathList = Lists.newArrayList();\n+    BitSet emptyStatus = new BitSet(columns.size());\n+    int i = 0;\n+\n+    // first pass: collect which fields are empty\n+    for (SchemaPath sp : columns) {\n+      PathSegment fieldPath = sp.getRootSegment();\n+      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n+      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n+        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n+        fieldPath = fieldPath.getChild();\n+      }\n+      writerList.add(fieldWriter);\n+      fieldPathList.add(fieldPath);\n+      if (fieldWriter.isEmptyMap()) {\n+        emptyStatus.set(i, true);\n+      }\n+      if (i == 0 && !allTextMode) {\n+        // when allTextMode is false, there is not much benefit to producing all\n+        // the empty fields; just produce 1 field. The reason is that the type of the\n+        // fields is unknown, so if we produce multiple Integer fields by default, a\n+        // subsequent batch that contains non-integer fields will error out in any case.\n+        // Whereas, with allTextMode true, we are sure that all fields are going to be\n+        // treated as varchar, so it makes sense to produce all the fields, and in fact\n+        // is necessary in order to avoid schema change exceptions by downstream operators.\n+        break;\n+      }\n+      i++;\n+    }\n+\n+    // second pass: create default typed vectors corresponding to empty fields\n+    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n+    // may be shared by multiple fields whereas we want to keep track of all fields\n+    // independently, so we rely on the emptyStatus.\n+    for (int j = 0; j < fieldPathList.size(); j++) {\n+      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n+      PathSegment fieldPath = fieldPathList.get(j);\n+      if (emptyStatus.get(j)) {\n+        if (allTextMode) {\n+          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n+        } else {\n+          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n+        }\n+      }\n+    }\n+\n+    for (BaseWriter.ListWriter field : emptyArrayWriters) {\n+      // checks that array has not been initialized\n+      if (field.getValueCapacity() == 0) {\n+        if (allTextMode) {\n+          field.varChar();\n+        } else {\n+          field.integer();\n+        }\n+      }\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "additions": 94,
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "status": "added",
                "changes": 94,
                "deletions": 0,
                "sha": "775be023783401a8367debd885eda277fe3fa78e",
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9"
            }
        ],
        "bug_id": "drill_19",
        "parent": "https://github.com/apache/drill/commit/4a718a0bd728ae02b502ac93620d132f0f6e1b6c",
        "message": "DRILL-5864: Selecting a non-existing field from a MapR-DB JSON table fails with NPE.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/06e1522b5ddf7e15d49921be1d9323f1e09273b0",
        "file": [
            {
                "patch": "@@ -93,8 +93,9 @@ protected DrillConnectionImpl(DriverImpl driver, AvaticaFactory factory,\n     super(driver, factory, url, info);\n \n     // Initialize transaction-related settings per Drill behavior.\n-    super.setTransactionIsolation( TRANSACTION_NONE );\n-    super.setAutoCommit( true );\n+    super.setTransactionIsolation(TRANSACTION_NONE);\n+    super.setAutoCommit(true);\n+    super.setReadOnly(false);\n \n     this.config = new DrillConnectionConfig(info);\n ",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "0e4726d994b4a2864c9d0945d0bd1b1f5d049b47",
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -17,9 +17,9 @@\n  */\n package org.apache.drill.jdbc;\n \n-import org.apache.drill.jdbc.Driver;\n \n import static org.hamcrest.CoreMatchers.*;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n@@ -42,6 +42,7 @@\n /**\n  * Test for Drill's implementation of Connection's methods (other than\n  * main transaction-related methods in {@link ConnectionTransactionMethodsTest}).\n+ * TODO: When here will be more tests, they should be sorted according to the {@link Connection} methods order\n  */\n public class ConnectionTest extends JdbcTestBase {\n \n@@ -332,4 +333,9 @@ public void testSetNetworkTimeoutRejectsBadExecutorValue() throws SQLException {\n     }\n   }\n \n+  @Test\n+  public void testIsReadOnly() throws Exception {\n+    assertFalse(connection.isReadOnly());\n+  }\n+\n }",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "status": "modified",
                "changes": 10,
                "deletions": 2,
                "sha": "09b75a66c856b064c991b9cdca88d70b3da9f38e",
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0"
            },
            {
                "patch": "@@ -35,7 +35,7 @@\n     <dep.guava.version>18.0</dep.guava.version>\n     <forkCount>2</forkCount>\n     <parquet.version>1.8.1-drill-r0</parquet.version>\n-    <calcite.version>1.4.0-drill-r20</calcite.version>\n+    <calcite.version>1.4.0-drill-r21</calcite.version>\n     <janino.version>2.7.6</janino.version>\n     <sqlline.version>1.1.9-drill-r7</sqlline.version>\n     <jackson.version>2.7.1</jackson.version>",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "5bde19b8b5becf372e4351f0ccf0b4f870b87d1e",
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/pom.xml",
                "filename": "pom.xml",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/pom.xml?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0"
            }
        ],
        "bug_id": "drill_20",
        "parent": "https://github.com/apache/drill/commit/d3718a62e2315a601615db8803cdfdcc3cedab82",
        "message": "DRILL-5413: DrillConnectionImpl.isReadOnly() throws NullPointerException\n\nchange is in CALCITE-843.\nupdate drill's calcite version to 1.4.0-drill-r21\n\nclose #806",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/e57514aad09985cd48ee5d3bf39f21af3ceaf510",
        "file": [
            {
                "patch": "@@ -71,14 +71,32 @@ public MapRDBSubScan(String userName, MapRDBFormatPlugin formatPlugin,\n     this.tableType = tableType;\n   }\n \n+\n+  @JsonProperty(\"formatPluginConfig\")\n+  public MapRDBFormatPluginConfig getFormatPluginConfig() {\n+    return (MapRDBFormatPluginConfig) formatPlugin.getConfig();\n+  }\n+\n+  @JsonProperty(\"storageConfig\")\n+  public StoragePluginConfig getStorageConfig(){\n+    return formatPlugin.getStorageConfig();\n+  }\n+\n+  @JsonProperty(\"regionScanSpecList\")\n   public List<MapRDBSubScanSpec> getRegionScanSpecList() {\n     return regionScanSpecList;\n   }\n \n+  @JsonProperty(\"columns\")\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n+  @JsonProperty(\"tableType\")\n+  public String getTableType() {\n+    return tableType;\n+  }\n+\n   @Override\n   public boolean isExecutable() {\n     return false;\n@@ -105,18 +123,9 @@ public int getOperatorType() {\n     return 1001;\n   }\n \n-  public String getTableType() {\n-    return tableType;\n-  }\n-\n   @JsonIgnore\n   public MapRDBFormatPlugin getFormatPlugin() {\n     return formatPlugin;\n   }\n \n-  @JsonIgnore\n-  public MapRDBFormatPluginConfig getFormatPluginConfig() {\n-    return (MapRDBFormatPluginConfig) formatPlugin.getConfig();\n-  }\n-\n }",
                "additions": 18,
                "raw_url": "https://github.com/apache/drill/raw/e57514aad09985cd48ee5d3bf39f21af3ceaf510/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "status": "modified",
                "changes": 27,
                "deletions": 9,
                "sha": "95471863660c2445be2c6160dc563e6dcf4451b6",
                "blob_url": "https://github.com/apache/drill/blob/e57514aad09985cd48ee5d3bf39f21af3ceaf510/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java?ref=e57514aad09985cd48ee5d3bf39f21af3ceaf510"
            }
        ],
        "bug_id": "drill_21",
        "parent": "https://github.com/apache/drill/commit/6829af095ac5bb78ed3df874f078891e61ef7721",
        "message": "DRILL-5763: Fix NPE during MapRDBSubScan deserialization\n\nclose apache/drill#931",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/3ad9a42b0741c509b625f21854bede70fca16cae",
        "file": [
            {
                "patch": "@@ -348,8 +348,11 @@ public IterOutcome innerNext() {\n                 logger.info(\"Merging spills\");\n                 spilledBatchGroups.addFirst(mergeAndSpill(spilledBatchGroups));\n               }\n-              spilledBatchGroups.add(mergeAndSpill(batchGroups));\n-              batchesSinceLastSpill = 0;\n+              final BatchGroup merged = mergeAndSpill(batchGroups);\n+              if (merged != null) { // make sure we don't add null to spilledBatchGroups\n+                spilledBatchGroups.add(merged);\n+                batchesSinceLastSpill = 0;\n+              }\n             }\n             long t = w.elapsed(TimeUnit.MICROSECONDS);\n //          logger.debug(\"Took {} us to sort {} records\", t, count);\n@@ -417,8 +420,12 @@ public IterOutcome innerNext() {\n //        logger.debug(\"Took {} us to sort {} records\", t, sv4.getTotalCount());\n         container.buildSchema(SelectionVectorMode.FOUR_BYTE);\n       } else {\n-        spilledBatchGroups.add(mergeAndSpill(batchGroups));\n+        final BatchGroup merged = mergeAndSpill(batchGroups);\n+        if (merged != null) {\n+          spilledBatchGroups.add(merged);\n+        }\n         batchGroups.addAll(spilledBatchGroups);\n+        spilledBatchGroups = null; // no need to cleanup spilledBatchGroups, all it's batches are in batchGroups now\n         logger.warn(\"Starting to merge. {} batch groups. Current allocated memory: {}\", batchGroups.size(), oContext.getAllocator().getAllocatedMemory());\n         VectorContainer hyperBatch = constructHyperBatch(batchGroups);\n         createCopier(hyperBatch, batchGroups, container, false);",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/3ad9a42b0741c509b625f21854bede70fca16cae/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "status": "modified",
                "changes": 13,
                "deletions": 3,
                "sha": "d9866bbfe8412a6dada02f7b81f0e76a5480123c",
                "blob_url": "https://github.com/apache/drill/blob/3ad9a42b0741c509b625f21854bede70fca16cae/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java?ref=3ad9a42b0741c509b625f21854bede70fca16cae"
            }
        ],
        "bug_id": "drill_22",
        "parent": "https://github.com/apache/drill/commit/b0e9e7c085a3106a612e400cd1732b6ec6267268",
        "message": "DRILL-3707: Fix for DRILL-3616 can cause a NullPointerException in ExternalSort cleanup\n\nThis closes #130",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f",
        "file": [
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -21,10 +21,10 @@\n \n import com.fasterxml.jackson.annotation.JsonInclude;\n import com.fasterxml.jackson.annotation.JsonInclude.Include;\n-import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n \n-@JsonTypeName(\"maprdb\")  @JsonInclude(Include.NON_DEFAULT)\n+@JsonTypeName(\"maprdb\")\n+@JsonInclude(Include.NON_DEFAULT)\n public class MapRDBFormatPluginConfig extends TableFormatPluginConfig {\n \n   public boolean allTextMode = false;\n@@ -35,17 +35,22 @@\n \n   @Override\n   public int hashCode() {\n-    return 53;\n+    int result = (allTextMode ? 1231 : 1237);\n+    result = 31 * result + (enablePushdown ? 1231 : 1237);\n+    result = 31 * result + (ignoreSchemaChange ? 1231 : 1237);\n+    result = 31 * result + (readAllNumbersAsDouble ? 1231 : 1237);\n+    result = 31 * result + (disableCountOptimization ? 1231 : 1237);\n+    return result;\n   }\n \n   @Override\n   protected boolean impEquals(Object obj) {\n-    MapRDBFormatPluginConfig other = (MapRDBFormatPluginConfig)obj;\n+    MapRDBFormatPluginConfig other = (MapRDBFormatPluginConfig) obj;\n     if (readAllNumbersAsDouble != other.readAllNumbersAsDouble) {\n       return false;\n     } else if (allTextMode != other.allTextMode) {\n       return false;\n-    } else if (isIgnoreSchemaChange() != other.isIgnoreSchemaChange()) {\n+    } else if (ignoreSchemaChange != other.ignoreSchemaChange) {\n       return false;\n     } else if (enablePushdown != other.enablePushdown) {\n       return false;\n@@ -63,40 +68,16 @@ public boolean isAllTextMode() {\n     return allTextMode;\n   }\n \n-  @JsonProperty(\"allTextMode\")\n-  public void setAllTextMode(boolean mode) {\n-    allTextMode = mode;\n-  }\n-\n-  @JsonProperty(\"disableCountOptimization\")\n-  public void setDisableCountOptimization(boolean mode) {\n-    disableCountOptimization = mode;\n-  }\n-\n-  public boolean shouldDisableCountOptimization() {\n+  public boolean disableCountOptimization() {\n     return disableCountOptimization;\n   }\n \n-  @JsonProperty(\"readAllNumbersAsDouble\")\n-  public void setReadAllNumbersAsDouble(boolean read) {\n-    readAllNumbersAsDouble = read;\n-  }\n-\n   public boolean isEnablePushdown() {\n     return enablePushdown;\n   }\n \n-  @JsonProperty(\"enablePushdown\")\n-  public void setEnablePushdown(boolean enablePushdown) {\n-    this.enablePushdown = enablePushdown;\n-  }\n-\n   public boolean isIgnoreSchemaChange() {\n     return ignoreSchemaChange;\n   }\n \n-  public void setIgnoreSchemaChange(boolean ignoreSchemaChange) {\n-    this.ignoreSchemaChange = ignoreSchemaChange;\n-  }\n-\n }",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "status": "modified",
                "changes": 43,
                "deletions": 31,
                "sha": "50a67b4ad27b77434d707e5628f4d4d35eecf545",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -34,7 +34,7 @@\n import com.google.common.collect.Lists;\n \n public class MapRDBScanBatchCreator implements BatchCreator<MapRDBSubScan>{\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MapRDBScanBatchCreator.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MapRDBScanBatchCreator.class);\n \n   @Override\n   public ScanBatch getBatch(FragmentContext context, MapRDBSubScan subScan, List<RecordBatch> children) throws ExecutionSetupException {\n@@ -43,13 +43,16 @@ public ScanBatch getBatch(FragmentContext context, MapRDBSubScan subScan, List<R\n     for(MapRDBSubScanSpec scanSpec : subScan.getRegionScanSpecList()){\n       try {\n         if (BinaryTableGroupScan.TABLE_BINARY.equals(subScan.getTableType())) {\n-          readers.add(new HBaseRecordReader(subScan.getFormatPlugin().getConnection(),\n-              getHBaseSubScanSpec(scanSpec), subScan.getColumns(), context));\n+          readers.add(new HBaseRecordReader(\n+              subScan.getFormatPlugin().getConnection(),\n+              getHBaseSubScanSpec(scanSpec),\n+              subScan.getColumns(),\n+              context));\n         } else {\n           readers.add(new MaprDBJsonRecordReader(scanSpec, subScan.getFormatPluginConfig(), subScan.getColumns(), context));\n         }\n-      } catch (Exception e1) {\n-        throw new ExecutionSetupException(e1);\n+      } catch (Exception e) {\n+        throw new ExecutionSetupException(e);\n       }\n     }\n     return new ScanBatch(subScan, context, readers.iterator());",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBScanBatchCreator.java",
                "status": "modified",
                "changes": 15,
                "deletions": 6,
                "sha": "d4a3f06b6099cac7f632f30b78e6763ebd473638",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBScanBatchCreator.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBScanBatchCreator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBScanBatchCreator.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -20,6 +20,7 @@\n import java.util.Iterator;\n import java.util.List;\n \n+import com.fasterxml.jackson.annotation.JsonIgnore;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.StoragePluginConfig;\n@@ -28,11 +29,9 @@\n import org.apache.drill.exec.physical.base.PhysicalVisitor;\n import org.apache.drill.exec.physical.base.SubScan;\n import org.apache.drill.exec.store.StoragePluginRegistry;\n-import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n import com.fasterxml.jackson.annotation.JsonCreator;\n-import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.google.common.base.Preconditions;\n@@ -41,40 +40,32 @@\n // Class containing information for reading a single HBase region\n @JsonTypeName(\"maprdb-sub-scan\")\n public class MapRDBSubScan extends AbstractBase implements SubScan {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MapRDBSubScan.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MapRDBSubScan.class);\n \n-  @JsonProperty\n-  public final StoragePluginConfig storageConfig;\n-  @JsonIgnore\n-  private final MapRDBFormatPluginConfig formatPluginConfig;\n-  private final FileSystemPlugin storagePlugin;\n+  private final MapRDBFormatPlugin formatPlugin;\n   private final List<MapRDBSubScanSpec> regionScanSpecList;\n   private final List<SchemaPath> columns;\n   private final String tableType;\n \n-  private final MapRDBFormatPlugin formatPlugin;\n-\n   @JsonCreator\n-  public MapRDBSubScan(@JacksonInject StoragePluginRegistry registry,\n+  public MapRDBSubScan(@JacksonInject StoragePluginRegistry engineRegistry,\n                        @JsonProperty(\"userName\") String userName,\n                        @JsonProperty(\"formatPluginConfig\") MapRDBFormatPluginConfig formatPluginConfig,\n-                       @JsonProperty(\"storageConfig\") StoragePluginConfig storage,\n+                       @JsonProperty(\"storageConfig\") StoragePluginConfig storageConfig,\n                        @JsonProperty(\"regionScanSpecList\") List<MapRDBSubScanSpec> regionScanSpecList,\n                        @JsonProperty(\"columns\") List<SchemaPath> columns,\n                        @JsonProperty(\"tableType\") String tableType) throws ExecutionSetupException {\n-    this(userName, formatPluginConfig,\n-        (FileSystemPlugin) registry.getPlugin(storage),\n-        storage, regionScanSpecList, columns, tableType);\n+    this(userName,\n+        (MapRDBFormatPlugin) engineRegistry.getFormatPlugin(storageConfig, formatPluginConfig),\n+        regionScanSpecList,\n+        columns,\n+        tableType);\n   }\n \n-  public MapRDBSubScan(String userName, MapRDBFormatPluginConfig formatPluginConfig, FileSystemPlugin storagePlugin, StoragePluginConfig storageConfig,\n+  public MapRDBSubScan(String userName, MapRDBFormatPlugin formatPlugin,\n       List<MapRDBSubScanSpec> maprSubScanSpecs, List<SchemaPath> columns, String tableType) {\n     super(userName);\n-    this.storageConfig = storageConfig;\n-    this.storagePlugin = storagePlugin;\n-    this.formatPluginConfig = formatPluginConfig;\n-    this.formatPlugin = (MapRDBFormatPlugin) storagePlugin.getFormatPlugin(formatPluginConfig);\n-\n+    this.formatPlugin = formatPlugin;\n     this.regionScanSpecList = maprSubScanSpecs;\n     this.columns = columns;\n     this.tableType = tableType;\n@@ -101,7 +92,7 @@ public boolean isExecutable() {\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new MapRDBSubScan(getUserName(), formatPluginConfig, storagePlugin, storageConfig, regionScanSpecList, columns, tableType);\n+    return new MapRDBSubScan(getUserName(), formatPlugin, regionScanSpecList, columns, tableType);\n   }\n \n   @Override\n@@ -118,13 +109,14 @@ public String getTableType() {\n     return tableType;\n   }\n \n-  public MapRDBFormatPluginConfig getFormatPluginConfig() {\n-    return formatPluginConfig;\n-  }\n-\n   @JsonIgnore\n   public MapRDBFormatPlugin getFormatPlugin() {\n     return formatPlugin;\n   }\n \n+  @JsonIgnore\n+  public MapRDBFormatPluginConfig getFormatPluginConfig() {\n+    return (MapRDBFormatPluginConfig) formatPlugin.getConfig();\n+  }\n+\n }",
                "additions": 19,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "status": "modified",
                "changes": 46,
                "deletions": 27,
                "sha": "98335f3abdc18e4b2ecd3645ad76356e8e12d568",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBSubScan.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -129,7 +129,7 @@ private void init() {\n         tableStats = new MapRDBTableStats(getHBaseConf(), hbaseScanSpec.getTableName());\n       }\n       boolean foundStartRegion = false;\n-      regionsToScan = new TreeMap<TabletFragmentInfo, String>();\n+      regionsToScan = new TreeMap<>();\n       List<HRegionLocation> regionLocations = locator.getAllRegionLocations();\n       for (HRegionLocation regionLocation : regionLocations) {\n         HRegionInfo regionInfo = regionLocation.getRegionInfo();\n@@ -178,8 +178,7 @@ public MapRDBSubScan getSpecificScan(int minorFragmentId) {\n     assert minorFragmentId < endpointFragmentMapping.size() : String.format(\n         \"Mappings length [%d] should be greater than minor fragment id [%d] but it isn't.\", endpointFragmentMapping.size(),\n         minorFragmentId);\n-    return new MapRDBSubScan(getUserName(), formatPluginConfig, getStoragePlugin(), getStoragePlugin().getConfig(),\n-        endpointFragmentMapping.get(minorFragmentId), columns, TABLE_BINARY);\n+    return new MapRDBSubScan(getUserName(), formatPlugin, endpointFragmentMapping.get(minorFragmentId), columns, TABLE_BINARY);\n   }\n \n   @Override",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/binary/BinaryTableGroupScan.java",
                "status": "modified",
                "changes": 7,
                "deletions": 4,
                "sha": "282b84852fea83395b91b4998e1e977a9e6be19c",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/binary/BinaryTableGroupScan.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/binary/BinaryTableGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/binary/BinaryTableGroupScan.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -37,7 +37,6 @@\n import org.apache.drill.exec.store.mapr.db.MapRDBFormatPluginConfig;\n import org.apache.drill.exec.store.mapr.db.MapRDBGroupScan;\n import org.apache.drill.exec.store.mapr.db.MapRDBSubScan;\n-import org.apache.drill.exec.store.mapr.db.MapRDBTableStats;\n import org.apache.drill.exec.store.mapr.db.TabletFragmentInfo;\n import org.apache.hadoop.conf.Configuration;\n import org.codehaus.jackson.annotate.JsonCreator;\n@@ -180,8 +179,7 @@ public MapRDBSubScan getSpecificScan(int minorFragmentId) {\n     assert minorFragmentId < endpointFragmentMapping.size() : String.format(\n         \"Mappings length [%d] should be greater than minor fragment id [%d] but it isn't.\", endpointFragmentMapping.size(),\n         minorFragmentId);\n-    return new MapRDBSubScan(getUserName(), formatPluginConfig, getStoragePlugin(), getStoragePlugin().getConfig(),\n-        endpointFragmentMapping.get(minorFragmentId), columns, TABLE_JSON);\n+    return new MapRDBSubScan(getUserName(), formatPlugin, endpointFragmentMapping.get(minorFragmentId), columns, TABLE_JSON);\n   }\n \n   @Override",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/JsonTableGroupScan.java",
                "status": "modified",
                "changes": 6,
                "deletions": 4,
                "sha": "a1d7f9abff133498d753a49831207df9cee2420a",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/JsonTableGroupScan.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/JsonTableGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/JsonTableGroupScan.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -38,7 +38,6 @@\n import org.apache.drill.exec.ops.OperatorContext;\n import org.apache.drill.exec.ops.OperatorStats;\n import org.apache.drill.exec.physical.impl.OutputMutator;\n-import org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType;\n import org.apache.drill.exec.store.AbstractRecordReader;\n import org.apache.drill.exec.store.mapr.db.MapRDBFormatPluginConfig;\n import org.apache.drill.exec.store.mapr.db.MapRDBSubScanSpec;\n@@ -112,7 +111,7 @@ public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n       condition = com.mapr.db.impl.ConditionImpl.parseFrom(ByteBufs.wrap(serializedFilter));\n     }\n \n-    disableCountOptimization = formatPluginConfig.shouldDisableCountOptimization();\n+    disableCountOptimization = formatPluginConfig.disableCountOptimization();\n     setColumns(projectedColumns);\n     unionEnabled = context.getOptions().getOption(ExecConstants.ENABLE_UNION_TYPE);\n     readNumbersAsDouble = formatPluginConfig.isReadAllNumbersAsDouble();",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 3,
                "sha": "5921249698c4ab952d4df1f88277f2241cccbb49",
                "blob_url": "https://github.com/apache/drill/blob/fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java?ref=fd7fba6c56fb19c6fbf3074ce16b7c97d1aef63f"
            }
        ],
        "bug_id": "drill_23",
        "parent": "https://github.com/apache/drill/commit/8b5642353505d1001d7ec3590a07ad1144ecf7f3",
        "message": "DRILL-5714: Fix NPE when mapr-db plugin is used in table function\n\nclose #902",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/58e4cec9a913e381ef0e96b072c31e34085277b3",
        "file": [
            {
                "patch": "@@ -160,7 +160,7 @@ private void init() {\n       statsCalculator = new TableStatsCalculator(conn, hbaseScanSpec, storagePlugin.getContext().getConfig(), storagePluginConfig);\n \n       boolean foundStartRegion = false;\n-      regionsToScan = new TreeMap<HRegionInfo, ServerName>();\n+      regionsToScan = new TreeMap<>();\n       for (HRegionLocation regionLocation : regionLocations) {\n         HRegionInfo regionInfo = regionLocation.getRegionInfo();\n         if (!foundStartRegion && hbaseScanSpec.getStartRow() != null && hbaseScanSpec.getStartRow().length != 0 && !regionInfo.containsRow(hbaseScanSpec.getStartRow())) {\n@@ -338,8 +338,7 @@ public HBaseSubScan getSpecificScan(int minorFragmentId) {\n     assert minorFragmentId < endpointFragmentMapping.size() : String.format(\n         \"Mappings length [%d] should be greater than minor fragment id [%d] but it isn't.\", endpointFragmentMapping.size(),\n         minorFragmentId);\n-    return new HBaseSubScan(getUserName(), storagePlugin, storagePluginConfig,\n-        endpointFragmentMapping.get(minorFragmentId), columns);\n+    return new HBaseSubScan(getUserName(), storagePlugin, endpointFragmentMapping.get(minorFragmentId), columns);\n   }\n \n   @Override",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "status": "modified",
                "changes": 5,
                "deletions": 3,
                "sha": "11782987ac4deeb9c245b346dcc276f68c379556",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "filename": "contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -24,7 +24,6 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractBase;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n import org.apache.drill.exec.physical.base.PhysicalVisitor;\n@@ -49,44 +48,43 @@\n public class HBaseSubScan extends AbstractBase implements SubScan {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HBaseSubScan.class);\n \n-  @JsonProperty\n-  public final HBaseStoragePluginConfig storage;\n-  @JsonIgnore\n   private final HBaseStoragePlugin hbaseStoragePlugin;\n   private final List<HBaseSubScanSpec> regionScanSpecList;\n   private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public HBaseSubScan(@JacksonInject StoragePluginRegistry registry,\n                       @JsonProperty(\"userName\") String userName,\n-                      @JsonProperty(\"storage\") StoragePluginConfig storage,\n+                      @JsonProperty(\"hbaseStoragePluginConfig\") HBaseStoragePluginConfig hbaseStoragePluginConfig,\n                       @JsonProperty(\"regionScanSpecList\") LinkedList<HBaseSubScanSpec> regionScanSpecList,\n                       @JsonProperty(\"columns\") List<SchemaPath> columns) throws ExecutionSetupException {\n-    super(userName);\n-    hbaseStoragePlugin = (HBaseStoragePlugin) registry.getPlugin(storage);\n-    this.regionScanSpecList = regionScanSpecList;\n-    this.storage = (HBaseStoragePluginConfig) storage;\n-    this.columns = columns;\n+    this(userName,\n+        (HBaseStoragePlugin) registry.getPlugin(hbaseStoragePluginConfig),\n+        regionScanSpecList,\n+        columns);\n   }\n \n-  public HBaseSubScan(String userName, HBaseStoragePlugin plugin, HBaseStoragePluginConfig config,\n-      List<HBaseSubScanSpec> regionInfoList, List<SchemaPath> columns) {\n+  public HBaseSubScan(String userName,\n+                      HBaseStoragePlugin hbaseStoragePlugin,\n+                      List<HBaseSubScanSpec> regionInfoList,\n+                      List<SchemaPath> columns) {\n     super(userName);\n-    hbaseStoragePlugin = plugin;\n-    storage = config;\n+    this.hbaseStoragePlugin = hbaseStoragePlugin;\n     this.regionScanSpecList = regionInfoList;\n     this.columns = columns;\n   }\n \n-  public List<HBaseSubScanSpec> getRegionScanSpecList() {\n-    return regionScanSpecList;\n+  @JsonProperty\n+  public HBaseStoragePluginConfig getHbaseStoragePluginConfig() {\n+    return hbaseStoragePlugin.getConfig();\n   }\n \n-  @JsonIgnore\n-  public HBaseStoragePluginConfig getStorageConfig() {\n-    return storage;\n+  @JsonProperty\n+  public List<HBaseSubScanSpec> getRegionScanSpecList() {\n+    return regionScanSpecList;\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n@@ -109,7 +107,7 @@ public HBaseStoragePlugin getStorageEngine(){\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new HBaseSubScan(getUserName(), hbaseStoragePlugin, storage, regionScanSpecList, columns);\n+    return new HBaseSubScan(getUserName(), hbaseStoragePlugin, regionScanSpecList, columns);\n   }\n \n   @Override",
                "additions": 18,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "status": "modified",
                "changes": 38,
                "deletions": 20,
                "sha": "bd179fbac35cd649a3187aa127f73b22f02dbd03",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "filename": "contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.categories.HbaseStorageTest;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\n@@ -109,4 +110,9 @@ public void testSelectFromSchema() throws Exception {\n     runHBaseSQLVerifyCount(\"SELECT row_key\\n\"\n         + \" FROM hbase.TestTableNullStr t WHERE row_key='a1'\", 1);\n   }\n+\n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from hbase.TestTableNullStr\");\n+  }\n }",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "0d5349943d7c25754a34970b06630e006615b5c2",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "filename": "contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -64,7 +64,7 @@ public HivePartitionDescriptor(@SuppressWarnings(\"unused\") final PlannerSettings\n     this.scanRel = scanRel;\n     this.managedBuffer = managedBuffer.reallocIfNeeded(256);\n     this.defaultPartitionValue = defaultPartitionValue;\n-    for (HiveTableWrapper.FieldSchemaWrapper wrapper : ((HiveScan) scanRel.getGroupScan()).hiveReadEntry.table.partitionKeys) {\n+    for (HiveTableWrapper.FieldSchemaWrapper wrapper : ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry().table.partitionKeys) {\n       partitionMap.put(wrapper.name, i);\n       i++;\n     }\n@@ -88,7 +88,7 @@ public int getMaxHierarchyLevel() {\n \n   @Override\n   public String getBaseTableLocation() {\n-    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).hiveReadEntry;\n+    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry();\n     return origEntry.table.getTable().getSd().getLocation();\n   }\n \n@@ -97,7 +97,7 @@ public void populatePartitionVectors(ValueVector[] vectors, List<PartitionLocati\n                                        BitSet partitionColumnBitSet, Map<Integer, String> fieldNameMap) {\n     int record = 0;\n     final HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n-    final Map<String, String> partitionNameTypeMap = hiveScan.hiveReadEntry.table.getPartitionNameTypeMap();\n+    final Map<String, String> partitionNameTypeMap = hiveScan.getHiveReadEntry().table.getPartitionNameTypeMap();\n     for(PartitionLocation partitionLocation: partitions) {\n       for(int partitionColumnIndex : BitSets.toIter(partitionColumnBitSet)){\n         final String hiveType = partitionNameTypeMap.get(fieldNameMap.get(partitionColumnIndex));\n@@ -126,7 +126,7 @@ public void populatePartitionVectors(ValueVector[] vectors, List<PartitionLocati\n   public TypeProtos.MajorType getVectorType(SchemaPath column, PlannerSettings plannerSettings) {\n     HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n     String partitionName = column.getAsNamePart().getName();\n-    Map<String, String> partitionNameTypeMap = hiveScan.hiveReadEntry.table.getPartitionNameTypeMap();\n+    Map<String, String> partitionNameTypeMap = hiveScan.getHiveReadEntry().table.getPartitionNameTypeMap();\n     String hiveType = partitionNameTypeMap.get(partitionName);\n     PrimitiveTypeInfo primitiveTypeInfo = (PrimitiveTypeInfo) TypeInfoUtils.getTypeInfoFromTypeString(hiveType);\n \n@@ -143,7 +143,7 @@ public Integer getIdIfValid(String name) {\n   @Override\n   protected void createPartitionSublists() {\n     List<PartitionLocation> locations = new LinkedList<>();\n-    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).hiveReadEntry;\n+    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry();\n     for (Partition partition: origEntry.getPartitions()) {\n       locations.add(new HivePartitionLocation(partition.getValues(), partition.getSd().getLocation()));\n     }\n@@ -165,7 +165,7 @@ public TableScan createTableScan(List<PartitionLocation> newPartitions, boolean\n \n   private GroupScan createNewGroupScan(List<PartitionLocation> newPartitionLocations) throws ExecutionSetupException {\n     HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n-    HiveReadEntry origReadEntry = hiveScan.hiveReadEntry;\n+    HiveReadEntry origReadEntry = hiveScan.getHiveReadEntry();\n     List<HiveTableWrapper.HivePartitionWrapper> oldPartitions = origReadEntry.partitions;\n     List<HiveTableWrapper.HivePartitionWrapper> newPartitions = Lists.newLinkedList();\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "status": "modified",
                "changes": 14,
                "deletions": 7,
                "sha": "2d2bb6c6984b617de482dba38127771aad088816",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -97,16 +97,16 @@ public boolean matches(RelOptRuleCall call) {\n \n     final HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n     final HiveConf hiveConf = hiveScan.getHiveConf();\n-    final HiveTableWithColumnCache hiveTable = hiveScan.hiveReadEntry.getTable();\n+    final HiveTableWithColumnCache hiveTable = hiveScan.getHiveReadEntry().getTable();\n \n     final Class<? extends InputFormat<?,?>> tableInputFormat =\n-        getInputFormatFromSD(HiveUtilities.getTableMetadata(hiveTable), hiveScan.hiveReadEntry, hiveTable.getSd(),\n+        getInputFormatFromSD(HiveUtilities.getTableMetadata(hiveTable), hiveScan.getHiveReadEntry(), hiveTable.getSd(),\n             hiveConf);\n     if (tableInputFormat == null || !tableInputFormat.equals(MapredParquetInputFormat.class)) {\n       return false;\n     }\n \n-    final List<HivePartitionWrapper> partitions = hiveScan.hiveReadEntry.getHivePartitionWrappers();\n+    final List<HivePartitionWrapper> partitions = hiveScan.getHiveReadEntry().getHivePartitionWrappers();\n     if (partitions == null) {\n       return true;\n     }\n@@ -116,7 +116,7 @@ public boolean matches(RelOptRuleCall call) {\n     for (HivePartitionWrapper partition : partitions) {\n       final StorageDescriptor partitionSD = partition.getPartition().getSd();\n       Class<? extends InputFormat<?, ?>> inputFormat = getInputFormatFromSD(\n-          HiveUtilities.getPartitionMetadata(partition.getPartition(), hiveTable), hiveScan.hiveReadEntry, partitionSD,\n+          HiveUtilities.getPartitionMetadata(partition.getPartition(), hiveTable), hiveScan.getHiveReadEntry(), partitionSD,\n           hiveConf);\n       if (inputFormat == null || !inputFormat.equals(tableInputFormat)) {\n         return false;\n@@ -172,7 +172,7 @@ public void onMatch(RelOptRuleCall call) {\n       final PlannerSettings settings = PrelUtil.getPlannerSettings(call.getPlanner());\n       final String partitionColumnLabel = settings.getFsPartitionColumnLabel();\n \n-      final Table hiveTable = hiveScan.hiveReadEntry.getTable();\n+      final Table hiveTable = hiveScan.getHiveReadEntry().getTable();\n       checkForUnsupportedDataTypes(hiveTable);\n \n       final Map<String, String> partitionColMapping =\n@@ -245,8 +245,8 @@ private DrillScanRel createNativeScanRel(final Map<String, String> partitionColM\n     final HiveDrillNativeParquetScan nativeHiveScan =\n         new HiveDrillNativeParquetScan(\n             hiveScan.getUserName(),\n-            hiveScan.hiveReadEntry,\n-            hiveScan.storagePlugin,\n+            hiveScan.getHiveReadEntry(),\n+            hiveScan.getStoragePlugin(),\n             nativeScanCols,\n             null);\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "status": "modified",
                "changes": 14,
                "deletions": 7,
                "sha": "a7322454bc1b55ee80778fd062ec7df091711f7a",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -41,16 +41,16 @@\n \n   @JsonCreator\n   public HiveDrillNativeParquetScan(@JsonProperty(\"userName\") String userName,\n-                                    @JsonProperty(\"hive-table\") HiveReadEntry hiveReadEntry,\n-                                    @JsonProperty(\"storage-plugin\") String storagePluginName,\n+                                    @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n+                                    @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig,\n                                     @JsonProperty(\"columns\") List<SchemaPath> columns,\n                                     @JacksonInject StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n-    super(userName, hiveReadEntry, storagePluginName, columns, pluginRegistry);\n+    super(userName, hiveReadEntry, hiveStoragePluginConfig, columns, pluginRegistry);\n   }\n \n-  public HiveDrillNativeParquetScan(String userName, HiveReadEntry hiveReadEntry, HiveStoragePlugin storagePlugin,\n+  public HiveDrillNativeParquetScan(String userName, HiveReadEntry hiveReadEntry, HiveStoragePlugin hiveStoragePlugin,\n       List<SchemaPath> columns, HiveMetadataProvider metadataProvider) throws ExecutionSetupException {\n-    super(userName, hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    super(userName, hiveReadEntry, hiveStoragePlugin, columns, metadataProvider);\n   }\n \n   public HiveDrillNativeParquetScan(final HiveScan hiveScan) {\n@@ -91,7 +91,7 @@ public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) thro\n \n   @Override\n   public HiveScan clone(HiveReadEntry hiveReadEntry) throws ExecutionSetupException {\n-    return new HiveDrillNativeParquetScan(getUserName(), hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    return new HiveDrillNativeParquetScan(getUserName(), hiveReadEntry, getStoragePlugin(), getColumns(), getMetadataProvider());\n   }\n \n   @Override\n@@ -103,12 +103,12 @@ public GroupScan clone(List<SchemaPath> columns) {\n \n   @Override\n   public String toString() {\n-    final List<HivePartitionWrapper> partitions = hiveReadEntry.getHivePartitionWrappers();\n+    final List<HivePartitionWrapper> partitions = getHiveReadEntry().getHivePartitionWrappers();\n     int numPartitions = partitions == null ? 0 : partitions.size();\n-    return \"HiveDrillNativeParquetScan [table=\" + hiveReadEntry.getHiveTableWrapper()\n-        + \", columns=\" + columns\n+    return \"HiveDrillNativeParquetScan [table=\" + getHiveReadEntry().getHiveTableWrapper()\n+        + \", columns=\" + getColumns()\n         + \", numPartitions=\" + numPartitions\n         + \", partitions= \" + partitions\n-        + \", inputDirectories=\" + metadataProvider.getInputDirectories(hiveReadEntry) + \"]\";\n+        + \", inputDirectories=\" + getMetadataProvider().getInputDirectories(getHiveReadEntry()) + \"]\";\n   }\n }",
                "additions": 11,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "status": "modified",
                "changes": 22,
                "deletions": 11,
                "sha": "202bd435e1c31515853542e6170f91fa573aa855",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -42,9 +42,9 @@ public HiveDrillNativeParquetSubScan(@JacksonInject StoragePluginRegistry regist\n                                        @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n                                        @JsonProperty(\"splitClasses\") List<String> splitClasses,\n                                        @JsonProperty(\"columns\") List<SchemaPath> columns,\n-                                       @JsonProperty(\"storagePluginName\") String pluginName)\n+                                       @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig)\n       throws IOException, ExecutionSetupException, ReflectiveOperationException {\n-    super(registry, userName, splits, hiveReadEntry, splitClasses, columns, pluginName);\n+    super(registry, userName, splits, hiveReadEntry, splitClasses, columns, hiveStoragePluginConfig);\n   }\n \n   public HiveDrillNativeParquetSubScan(final HiveSubScan subScan)",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "2129ed454234dec2123e4f86df7221eed0d66d39",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -61,41 +61,36 @@\n \n   private static int HIVE_SERDE_SCAN_OVERHEAD_FACTOR_PER_COLUMN = 20;\n \n-  @JsonProperty(\"hive-table\")\n-  public HiveReadEntry hiveReadEntry;\n+  private final HiveStoragePlugin hiveStoragePlugin;\n+  private final HiveReadEntry hiveReadEntry;\n+  private final HiveMetadataProvider metadataProvider;\n \n-  @JsonIgnore\n-  public HiveStoragePlugin storagePlugin;\n-\n-  @JsonProperty(\"columns\")\n-  public List<SchemaPath> columns;\n-\n-  @JsonIgnore\n-  protected final HiveMetadataProvider metadataProvider;\n-\n-  @JsonIgnore\n   private List<List<LogicalInputSplit>> mappings;\n+  private List<LogicalInputSplit> inputSplits;\n \n-  @JsonIgnore\n-  protected List<LogicalInputSplit> inputSplits;\n+  protected List<SchemaPath> columns;\n \n   @JsonCreator\n   public HiveScan(@JsonProperty(\"userName\") final String userName,\n-                  @JsonProperty(\"hive-table\") final HiveReadEntry hiveReadEntry,\n-                  @JsonProperty(\"storage-plugin\") final String storagePluginName,\n+                  @JsonProperty(\"hiveReadEntry\") final HiveReadEntry hiveReadEntry,\n+                  @JsonProperty(\"hiveStoragePluginConfig\") final HiveStoragePluginConfig hiveStoragePluginConfig,\n                   @JsonProperty(\"columns\") final List<SchemaPath> columns,\n                   @JacksonInject final StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n-    this(userName, hiveReadEntry, (HiveStoragePlugin) pluginRegistry.getPlugin(storagePluginName), columns, null);\n+    this(userName,\n+        hiveReadEntry,\n+        (HiveStoragePlugin) pluginRegistry.getPlugin(hiveStoragePluginConfig),\n+        columns,\n+        null);\n   }\n \n-  public HiveScan(final String userName, final HiveReadEntry hiveReadEntry, final HiveStoragePlugin storagePlugin,\n+  public HiveScan(final String userName, final HiveReadEntry hiveReadEntry, final HiveStoragePlugin hiveStoragePlugin,\n       final List<SchemaPath> columns, final HiveMetadataProvider metadataProvider) throws ExecutionSetupException {\n     super(userName);\n     this.hiveReadEntry = hiveReadEntry;\n     this.columns = columns;\n-    this.storagePlugin = storagePlugin;\n+    this.hiveStoragePlugin = hiveStoragePlugin;\n     if (metadataProvider == null) {\n-      this.metadataProvider = new HiveMetadataProvider(userName, hiveReadEntry, storagePlugin.getHiveConf());\n+      this.metadataProvider = new HiveMetadataProvider(userName, hiveReadEntry, hiveStoragePlugin.getHiveConf());\n     } else {\n       this.metadataProvider = metadataProvider;\n     }\n@@ -105,26 +100,47 @@ public HiveScan(final HiveScan that) {\n     super(that);\n     this.columns = that.columns;\n     this.hiveReadEntry = that.hiveReadEntry;\n-    this.storagePlugin = that.storagePlugin;\n+    this.hiveStoragePlugin = that.hiveStoragePlugin;\n     this.metadataProvider = that.metadataProvider;\n   }\n \n   public HiveScan clone(final HiveReadEntry hiveReadEntry) throws ExecutionSetupException {\n-    return new HiveScan(getUserName(), hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    return new HiveScan(getUserName(), hiveReadEntry, hiveStoragePlugin, columns, metadataProvider);\n+  }\n+\n+  @JsonProperty\n+  public HiveReadEntry getHiveReadEntry() {\n+    return hiveReadEntry;\n   }\n \n+  @JsonProperty\n+  public HiveStoragePluginConfig getHiveStoragePluginConfig() {\n+    return hiveStoragePlugin.getConfig();\n+  }\n+\n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n-  protected List<LogicalInputSplit> getInputSplits() {\n+  @JsonIgnore\n+  public HiveStoragePlugin getStoragePlugin() {\n+    return hiveStoragePlugin;\n+  }\n+\n+  protected HiveMetadataProvider getMetadataProvider() {\n+    return metadataProvider;\n+  }\n+\n+  private List<LogicalInputSplit> getInputSplits() {\n     if (inputSplits == null) {\n       inputSplits = metadataProvider.getInputSplits(hiveReadEntry);\n     }\n \n     return inputSplits;\n   }\n \n+\n   @Override\n   public void applyAssignments(final List<CoordinationProtos.DrillbitEndpoint> endpoints) {\n     mappings = new ArrayList<>();\n@@ -160,7 +176,7 @@ public SubScan getSpecificScan(final int minorFragmentId) throws ExecutionSetupE\n       }\n \n       final HiveReadEntry subEntry = new HiveReadEntry(hiveReadEntry.getTableWrapper(), parts);\n-      return new HiveSubScan(getUserName(), encodedInputSplits, subEntry, splitTypes, columns, storagePlugin);\n+      return new HiveSubScan(getUserName(), encodedInputSplits, subEntry, splitTypes, columns, hiveStoragePlugin);\n     } catch (IOException | ReflectiveOperationException e) {\n       throw new ExecutionSetupException(e);\n     }\n@@ -174,7 +190,7 @@ public int getMaxParallelizationWidth() {\n   @Override\n   public List<EndpointAffinity> getOperatorAffinity() {\n     final Map<String, DrillbitEndpoint> endpointMap = new HashMap<>();\n-    for (final DrillbitEndpoint endpoint : storagePlugin.getContext().getBits()) {\n+    for (final DrillbitEndpoint endpoint : hiveStoragePlugin.getContext().getBits()) {\n       endpointMap.put(endpoint.getAddress(), endpoint);\n       logger.debug(\"endpoing address: {}\", endpoint.getAddress());\n     }\n@@ -285,7 +301,7 @@ public boolean supportsPartitionFilterPushdown() {\n \n   @JsonIgnore\n   public HiveConf getHiveConf() {\n-    return storagePlugin.getHiveConf();\n+    return hiveStoragePlugin.getHiveConf();\n   }\n \n   @JsonIgnore",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "status": "modified",
                "changes": 68,
                "deletions": 26,
                "sha": "11d47f304cbb150fc5769ae2b93c6310ece75121",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -24,6 +24,7 @@\n import java.util.List;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.google.common.collect.ImmutableSet;\n import org.apache.commons.codec.binary.Base64;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n@@ -40,26 +41,20 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n-import com.google.common.collect.Iterators;\n import com.google.common.io.ByteArrayDataInput;\n import com.google.common.io.ByteStreams;\n \n @JsonTypeName(\"hive-sub-scan\")\n public class HiveSubScan extends AbstractBase implements SubScan {\n-  protected HiveReadEntry hiveReadEntry;\n \n-  @JsonIgnore\n-  protected List<List<InputSplit>> inputSplits = new ArrayList<>();\n-  @JsonIgnore\n-  protected HiveTableWithColumnCache table;\n-  @JsonIgnore\n-  protected List<HivePartition> partitions;\n-  @JsonIgnore\n-  protected HiveStoragePlugin storagePlugin;\n-\n-  private List<List<String>> splits;\n-  private List<String> splitClasses;\n-  protected List<SchemaPath> columns;\n+  private final HiveReadEntry hiveReadEntry;\n+  private final List<List<InputSplit>> inputSplits = new ArrayList<>();\n+  private final HiveStoragePlugin hiveStoragePlugin;\n+  private final List<List<String>> splits;\n+  private final List<String> splitClasses;\n+  private final HiveTableWithColumnCache table;\n+  private final List<HivePartition> partitions;\n+  private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public HiveSubScan(@JacksonInject StoragePluginRegistry registry,\n@@ -68,13 +63,22 @@ public HiveSubScan(@JacksonInject StoragePluginRegistry registry,\n                      @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n                      @JsonProperty(\"splitClasses\") List<String> splitClasses,\n                      @JsonProperty(\"columns\") List<SchemaPath> columns,\n-                     @JsonProperty(\"storagePluginName\") String pluginName)\n+                     @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig)\n       throws IOException, ExecutionSetupException, ReflectiveOperationException {\n-    this(userName, splits, hiveReadEntry, splitClasses, columns, (HiveStoragePlugin)registry.getPlugin(pluginName));\n-  }\n-\n-  public HiveSubScan(final String userName, final List<List<String>> splits, final HiveReadEntry hiveReadEntry,\n-      final List<String> splitClasses, final List<SchemaPath> columns, final HiveStoragePlugin plugin)\n+    this(userName,\n+        splits,\n+        hiveReadEntry,\n+        splitClasses,\n+        columns,\n+        (HiveStoragePlugin) registry.getPlugin(hiveStoragePluginConfig));\n+  }\n+\n+  public HiveSubScan(final String userName,\n+                     final List<List<String>> splits,\n+                     final HiveReadEntry hiveReadEntry,\n+                      final List<String> splitClasses,\n+                     final List<SchemaPath> columns,\n+                     final HiveStoragePlugin hiveStoragePlugin)\n     throws IOException, ReflectiveOperationException {\n     super(userName);\n     this.hiveReadEntry = hiveReadEntry;\n@@ -83,66 +87,61 @@ public HiveSubScan(final String userName, final List<List<String>> splits, final\n     this.splits = splits;\n     this.splitClasses = splitClasses;\n     this.columns = columns;\n-    this.storagePlugin = plugin;\n+    this.hiveStoragePlugin = hiveStoragePlugin;\n \n     for (int i = 0; i < splits.size(); i++) {\n       inputSplits.add(deserializeInputSplit(splits.get(i), splitClasses.get(i)));\n     }\n   }\n \n-  @JsonProperty(\"storagePluginName\")\n-  @SuppressWarnings(\"unused\")\n-  public String getStoragePluginName() {\n-    return storagePlugin.getName();\n-  }\n-\n-  @JsonIgnore\n-  public HiveStoragePlugin getStoragePlugin() {\n-    return storagePlugin;\n-  }\n-\n+  @JsonProperty\n   public List<List<String>> getSplits() {\n     return splits;\n   }\n \n-  public HiveTableWithColumnCache getTable() {\n-    return table;\n-  }\n-\n-  public List<HivePartition> getPartitions() {\n-    return partitions;\n+  @JsonProperty\n+  public HiveReadEntry getHiveReadEntry() {\n+    return hiveReadEntry;\n   }\n \n+  @JsonProperty\n   public List<String> getSplitClasses() {\n     return splitClasses;\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n+  @JsonProperty\n+  public HiveStoragePluginConfig getHiveStoragePluginConfig() {\n+    return hiveStoragePlugin.getConfig();\n+  }\n+\n+  @JsonIgnore\n+  public HiveTableWithColumnCache getTable() {\n+    return table;\n+  }\n+\n+  @JsonIgnore\n+  public List<HivePartition> getPartitions() {\n+    return partitions;\n+  }\n+\n+  @JsonIgnore\n   public List<List<InputSplit>> getInputSplits() {\n     return inputSplits;\n   }\n \n-  public HiveReadEntry getHiveReadEntry() {\n-    return hiveReadEntry;\n+  @JsonIgnore\n+  public HiveStoragePlugin getStoragePlugin() {\n+    return hiveStoragePlugin;\n   }\n \n-  public static List<InputSplit> deserializeInputSplit(List<String> base64, String className) throws IOException, ReflectiveOperationException{\n-    Constructor<?> constructor = Class.forName(className).getDeclaredConstructor();\n-    if (constructor == null) {\n-      throw new ReflectiveOperationException(\"Class \" + className + \" does not implement a default constructor.\");\n-    }\n-    constructor.setAccessible(true);\n-    List<InputSplit> splits = new ArrayList<>();\n-    for (String str : base64) {\n-      InputSplit split = (InputSplit) constructor.newInstance();\n-      ByteArrayDataInput byteArrayDataInput = ByteStreams.newDataInput(Base64.decodeBase64(str));\n-      split.readFields(byteArrayDataInput);\n-      splits.add(split);\n-    }\n-    return splits;\n+  @JsonIgnore\n+  public HiveConf getHiveConf() {\n+    return hiveStoragePlugin.getHiveConf();\n   }\n \n   @Override\n@@ -153,24 +152,37 @@ public HiveReadEntry getHiveReadEntry() {\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {\n     try {\n-      return new HiveSubScan(getUserName(), splits, hiveReadEntry, splitClasses, columns, storagePlugin);\n+      return new HiveSubScan(getUserName(), splits, hiveReadEntry, splitClasses, columns, hiveStoragePlugin);\n     } catch (IOException | ReflectiveOperationException e) {\n       throw new ExecutionSetupException(e);\n     }\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n-    return Iterators.emptyIterator();\n+    return ImmutableSet.<PhysicalOperator>of().iterator();\n   }\n \n   @Override\n   public int getOperatorType() {\n     return CoreOperatorType.HIVE_SUB_SCAN_VALUE;\n   }\n \n-  @JsonIgnore\n-  public HiveConf getHiveConf() {\n-    return storagePlugin.getHiveConf();\n+  private static List<InputSplit> deserializeInputSplit(List<String> base64, String className)\n+      throws IOException, ReflectiveOperationException{\n+    Constructor<?> constructor = Class.forName(className).getDeclaredConstructor();\n+    if (constructor == null) {\n+      throw new ReflectiveOperationException(\"Class \" + className + \" does not implement a default constructor.\");\n+    }\n+    constructor.setAccessible(true);\n+    List<InputSplit> splits = new ArrayList<>();\n+    for (String str : base64) {\n+      InputSplit split = (InputSplit) constructor.newInstance();\n+      ByteArrayDataInput byteArrayDataInput = ByteStreams.newDataInput(Base64.decodeBase64(str));\n+      split.readFields(byteArrayDataInput);\n+      splits.add(split);\n+    }\n+    return splits;\n   }\n+\n }",
                "additions": 71,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "status": "modified",
                "changes": 130,
                "deletions": 59,
                "sha": "8ca8647cb1d7950f559b7b84f75e0a9a62ffd484",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -19,6 +19,7 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.categories.HiveStorageTest;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.common.exceptions.UserRemoteException;\n@@ -100,7 +101,6 @@ public void convertFromOnHiveBinaryType() throws Exception {\n   /**\n    * Test to ensure Drill reads the all supported types correctly both normal fields (converted to Nullable types) and\n    * partition fields (converted to Required types).\n-   * @throws Exception\n    */\n   @Test\n   public void readAllSupportedHiveDataTypes() throws Exception {\n@@ -558,6 +558,11 @@ public void testNonAsciiStringLiterals() throws Exception {\n         .go();\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from hive.kv\");\n+  }\n+\n   private void verifyColumnsMetadata(List<UserProtos.ResultColumnMetadata> columnsList, Map<String, Integer> expectedResult) {\n     for (UserProtos.ResultColumnMetadata columnMetadata : columnsList) {\n       assertTrue(\"Column should be present in result set\", expectedResult.containsKey(columnMetadata.getColumnName()));",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "status": "modified",
                "changes": 7,
                "deletions": 1,
                "sha": "c2412ad7e7f4fc18d7e10f9fad42760c24227970",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -127,4 +127,9 @@ public void pushdownJoinAndFilterPushDown() throws Exception {\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    testPhysicalPlanExecutionBasedOnQuery(\"select * from mysql.`drill_mysql_test`.person\");\n+  }\n+\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "7b8c21acc0dc2f783ddccc03deed2361099841a4",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -69,47 +69,49 @@\n   private static final long MSG_SIZE = 1024;\n \n   private final KafkaStoragePlugin kafkaStoragePlugin;\n-  private final KafkaStoragePluginConfig kafkaStoragePluginConfig;\n-  private List<SchemaPath> columns;\n   private final KafkaScanSpec kafkaScanSpec;\n \n+  private List<SchemaPath> columns;\n   private List<PartitionScanWork> partitionWorkList;\n   private ListMultimap<Integer, PartitionScanWork> assignments;\n   private List<EndpointAffinity> affinities;\n \n   @JsonCreator\n   public KafkaGroupScan(@JsonProperty(\"userName\") String userName,\n-      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n-      @JsonProperty(\"columns\") List<SchemaPath> columns, @JsonProperty(\"scanSpec\") KafkaScanSpec scanSpec,\n-      @JacksonInject StoragePluginRegistry pluginRegistry) {\n-    this(userName, kafkaStoragePluginConfig, columns, scanSpec, (KafkaStoragePlugin) pluginRegistry);\n+                        @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n+                        @JsonProperty(\"columns\") List<SchemaPath> columns,\n+                        @JsonProperty(\"kafkaScanSpec\") KafkaScanSpec scanSpec,\n+                        @JacksonInject StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n+    this(userName,\n+        (KafkaStoragePlugin) pluginRegistry.getPlugin(kafkaStoragePluginConfig),\n+        columns,\n+        scanSpec);\n   }\n \n   public KafkaGroupScan(KafkaStoragePlugin kafkaStoragePlugin, KafkaScanSpec kafkaScanSpec, List<SchemaPath> columns) {\n     super(StringUtils.EMPTY);\n     this.kafkaStoragePlugin = kafkaStoragePlugin;\n-    this.kafkaStoragePluginConfig = (KafkaStoragePluginConfig) kafkaStoragePlugin.getConfig();\n     this.columns = columns;\n     this.kafkaScanSpec = kafkaScanSpec;\n     init();\n   }\n \n-  public KafkaGroupScan(String userName, KafkaStoragePluginConfig kafkaStoragePluginConfig, List<SchemaPath> columns,\n-      KafkaScanSpec kafkaScanSpec, KafkaStoragePlugin pluginRegistry) {\n+  public KafkaGroupScan(String userName,\n+                        KafkaStoragePlugin kafkaStoragePlugin,\n+                        List<SchemaPath> columns,\n+                        KafkaScanSpec kafkaScanSpec) {\n     super(userName);\n-    this.kafkaStoragePluginConfig = kafkaStoragePluginConfig;\n+    this.kafkaStoragePlugin = kafkaStoragePlugin;\n     this.columns = columns;\n     this.kafkaScanSpec = kafkaScanSpec;\n-    this.kafkaStoragePlugin = pluginRegistry;\n     init();\n   }\n \n   public KafkaGroupScan(KafkaGroupScan that) {\n     super(that);\n-    this.kafkaStoragePluginConfig = that.kafkaStoragePluginConfig;\n+    this.kafkaStoragePlugin = that.kafkaStoragePlugin;\n     this.columns = that.columns;\n     this.kafkaScanSpec = that.kafkaScanSpec;\n-    this.kafkaStoragePlugin = that.kafkaStoragePlugin;\n     this.partitionWorkList = that.partitionWorkList;\n     this.assignments = that.assignments;\n   }\n@@ -242,7 +244,7 @@ public KafkaSubScan getSpecificScan(int minorFragmentId) {\n           work.getBeginOffset(), work.getLatestOffset()));\n     }\n \n-    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, kafkaStoragePluginConfig, columns, scanSpecList);\n+    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, columns, scanSpecList);\n   }\n \n   @Override\n@@ -291,18 +293,18 @@ public GroupScan clone(List<SchemaPath> columns) {\n     return clone;\n   }\n \n-  @JsonProperty(\"kafkaStoragePluginConfig\")\n-  public KafkaStoragePluginConfig getStorageConfig() {\n-    return this.kafkaStoragePluginConfig;\n+  @JsonProperty\n+  public KafkaStoragePluginConfig getKafkaStoragePluginConfig() {\n+    return kafkaStoragePlugin.getConfig();\n   }\n \n   @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n-  @JsonProperty(\"kafkaScanSpec\")\n-  public KafkaScanSpec getScanSpec() {\n+  @JsonProperty\n+  public KafkaScanSpec getKafkaScanSpec() {\n     return kafkaScanSpec;\n   }\n ",
                "additions": 21,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "status": "modified",
                "changes": 40,
                "deletions": 19,
                "sha": "9cf575b3451bac5e0f75409e2c27368ded82adbf",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "filename": "contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -41,34 +41,31 @@\n @JsonTypeName(\"kafka-partition-scan\")\n public class KafkaSubScan extends AbstractBase implements SubScan {\n \n-  @JsonProperty\n-  private final KafkaStoragePluginConfig KafkaStoragePluginConfig;\n-\n-  @JsonIgnore\n   private final KafkaStoragePlugin kafkaStoragePlugin;\n   private final List<SchemaPath> columns;\n-  private final List<KafkaSubScanSpec> partitions;\n+  private final List<KafkaSubScanSpec> partitionSubScanSpecList;\n \n   @JsonCreator\n-  public KafkaSubScan(@JacksonInject StoragePluginRegistry registry, @JsonProperty(\"userName\") String userName,\n-      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n-      @JsonProperty(\"columns\") List<SchemaPath> columns,\n-      @JsonProperty(\"partitionSubScanSpecList\") LinkedList<KafkaSubScanSpec> partitions)\n+  public KafkaSubScan(@JacksonInject StoragePluginRegistry registry,\n+                      @JsonProperty(\"userName\") String userName,\n+                      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n+                      @JsonProperty(\"columns\") List<SchemaPath> columns,\n+                      @JsonProperty(\"partitionSubScanSpecList\") LinkedList<KafkaSubScanSpec> partitionSubScanSpecList)\n       throws ExecutionSetupException {\n-    super(userName);\n-    this.KafkaStoragePluginConfig = kafkaStoragePluginConfig;\n-    this.columns = columns;\n-    this.partitions = partitions;\n-    this.kafkaStoragePlugin = (KafkaStoragePlugin) registry.getPlugin(kafkaStoragePluginConfig);\n+    this(userName,\n+        (KafkaStoragePlugin) registry.getPlugin(kafkaStoragePluginConfig),\n+        columns,\n+        partitionSubScanSpecList);\n   }\n \n-  public KafkaSubScan(String userName, KafkaStoragePlugin plugin, KafkaStoragePluginConfig kafkStoragePluginConfig,\n-      List<SchemaPath> columns, List<KafkaSubScanSpec> partitionSubScanSpecList) {\n+  public KafkaSubScan(String userName,\n+                      KafkaStoragePlugin kafkaStoragePlugin,\n+                      List<SchemaPath> columns,\n+                      List<KafkaSubScanSpec> partitionSubScanSpecList) {\n     super(userName);\n+    this.kafkaStoragePlugin = kafkaStoragePlugin;\n     this.columns = columns;\n-    this.KafkaStoragePluginConfig = kafkStoragePluginConfig;\n-    this.kafkaStoragePlugin = plugin;\n-    this.partitions = partitionSubScanSpecList;\n+    this.partitionSubScanSpecList = partitionSubScanSpecList;\n   }\n \n   @Override\n@@ -79,31 +76,32 @@ public KafkaSubScan(String userName, KafkaStoragePlugin plugin, KafkaStoragePlug\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, KafkaStoragePluginConfig, columns,\n-        partitions);\n+    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, columns, partitionSubScanSpecList);\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n     return Collections.emptyIterator();\n   }\n \n-  @JsonIgnore\n+  @JsonProperty\n   public KafkaStoragePluginConfig getKafkaStoragePluginConfig() {\n-    return KafkaStoragePluginConfig;\n-  }\n-\n-  @JsonIgnore\n-  public KafkaStoragePlugin getKafkaStoragePlugin() {\n-    return kafkaStoragePlugin;\n+    return kafkaStoragePlugin.getConfig();\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n+  @JsonProperty\n   public List<KafkaSubScanSpec> getPartitionSubScanSpecList() {\n-    return partitions;\n+    return partitionSubScanSpecList;\n+  }\n+\n+  @JsonIgnore\n+  public KafkaStoragePlugin getKafkaStoragePlugin() {\n+    return kafkaStoragePlugin;\n   }\n \n   @Override",
                "additions": 27,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "status": "modified",
                "changes": 56,
                "deletions": 29,
                "sha": "468f766a9685cdd010c68ea449b2d437357aeccc",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "filename": "contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.drill.exec.store.kafka;\n \n-import java.util.Arrays;\n import java.util.Collections;\n import java.util.Map;\n import java.util.Set;\n@@ -39,7 +38,7 @@\n \n   @Test\n   public void testSqlQueryOnInvalidTopic() throws Exception {\n-    String queryString = String.format(QueryConstants.MSG_SELECT_QUERY, QueryConstants.INVALID_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.INVALID_TOPIC);\n     try {\n       testBuilder().sqlQuery(queryString).unOrdered().baselineRecords(Collections.<Map<String, Object>> emptyList())\n           .build().run();\n@@ -51,7 +50,7 @@ public void testSqlQueryOnInvalidTopic() throws Exception {\n \n   @Test\n   public void testResultCount() throws Exception {\n-    String queryString = String.format(QueryConstants.MSG_SELECT_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.JSON_TOPIC);\n     runKafkaSQLVerifyCount(queryString, TestKafkaSuit.NUM_JSON_MSG);\n   }\n \n@@ -60,27 +59,27 @@ public void testPartitionMinOffset() throws Exception {\n     // following kafka.tools.GetOffsetShell for earliest as -2\n     Map<TopicPartition, Long> startOffsetsMap = fetchOffsets(-2);\n \n-    String queryString = String.format(QueryConstants.MIN_OFFSET_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MIN_OFFSET_QUERY, TestQueryConstants.JSON_TOPIC);\n     testBuilder().sqlQuery(queryString).unOrdered().baselineColumns(\"minOffset\")\n-        .baselineValues(startOffsetsMap.get(new TopicPartition(QueryConstants.JSON_TOPIC, 0))).go();\n+        .baselineValues(startOffsetsMap.get(new TopicPartition(TestQueryConstants.JSON_TOPIC, 0))).go();\n   }\n \n   @Test\n   public void testPartitionMaxOffset() throws Exception {\n     // following kafka.tools.GetOffsetShell for latest as -1\n     Map<TopicPartition, Long> endOffsetsMap = fetchOffsets(-1);\n \n-    String queryString = String.format(QueryConstants.MAX_OFFSET_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MAX_OFFSET_QUERY, TestQueryConstants.JSON_TOPIC);\n     testBuilder().sqlQuery(queryString).unOrdered().baselineColumns(\"maxOffset\")\n-        .baselineValues(endOffsetsMap.get(new TopicPartition(QueryConstants.JSON_TOPIC, 0))-1).go();\n+        .baselineValues(endOffsetsMap.get(new TopicPartition(TestQueryConstants.JSON_TOPIC, 0))-1).go();\n   }\n \n   private Map<TopicPartition, Long> fetchOffsets(int flag) {\n     KafkaConsumer<byte[], byte[]> kafkaConsumer = new KafkaConsumer<>(storagePluginConfig.getKafkaConsumerProps(),\n         new ByteArrayDeserializer(), new ByteArrayDeserializer());\n \n     Map<TopicPartition, Long> offsetsMap = Maps.newHashMap();\n-    kafkaConsumer.subscribe(Arrays.asList(QueryConstants.JSON_TOPIC));\n+    kafkaConsumer.subscribe(Collections.singletonList(TestQueryConstants.JSON_TOPIC));\n     // based on KafkaConsumer JavaDoc, seekToBeginning/seekToEnd functions\n     // evaluates lazily, seeking to the\n     // first/last offset in all partitions only when poll(long) or\n@@ -110,4 +109,10 @@ public void testPartitionMaxOffset() throws Exception {\n     return offsetsMap;\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.JSON_TOPIC);\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n+  }\n+\n }",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "status": "modified",
                "changes": 21,
                "deletions": 8,
                "sha": "ce9eb9984e3ad136a2fde21909121bcae05c2c0c",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -49,7 +49,7 @@ public void setUp() {\n     consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n     consumerProps.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"4\");\n     kafkaConsumer = new KafkaConsumer<>(consumerProps);\n-    subScanSpec = new KafkaSubScanSpec(QueryConstants.JSON_TOPIC, 0, 0, TestKafkaSuit.NUM_JSON_MSG);\n+    subScanSpec = new KafkaSubScanSpec(TestQueryConstants.JSON_TOPIC, 0, 0, TestKafkaSuit.NUM_JSON_MSG);\n   }\n \n   @After",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "4a155963fa3f1548739b91203afbed69df54c1b8",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -72,15 +72,15 @@ public static void initKafka() throws Exception {\n         Properties topicProps = new Properties();\n         zkClient = new ZkClient(embeddedKafkaCluster.getZkServer().getConnectionString(), SESSION_TIMEOUT, CONN_TIMEOUT, ZKStringSerializer$.MODULE$);\n         ZkUtils zkUtils = new ZkUtils(zkClient, new ZkConnection(embeddedKafkaCluster.getZkServer().getConnectionString()), false);\n-        AdminUtils.createTopic(zkUtils, QueryConstants.JSON_TOPIC, 1, 1, topicProps, RackAwareMode.Disabled$.MODULE$);\n+        AdminUtils.createTopic(zkUtils, TestQueryConstants.JSON_TOPIC, 1, 1, topicProps, RackAwareMode.Disabled$.MODULE$);\n \n         org.apache.kafka.common.requests.MetadataResponse.TopicMetadata fetchTopicMetadataFromZk = AdminUtils\n-            .fetchTopicMetadataFromZk(QueryConstants.JSON_TOPIC, zkUtils);\n+            .fetchTopicMetadataFromZk(TestQueryConstants.JSON_TOPIC, zkUtils);\n         logger.info(\"Topic Metadata: \" + fetchTopicMetadataFromZk);\n \n         KafkaMessageGenerator generator = new KafkaMessageGenerator(embeddedKafkaCluster.getKafkaBrokerList(),\n             StringSerializer.class);\n-        generator.populateJsonMsgIntoKafka(QueryConstants.JSON_TOPIC, NUM_JSON_MSG);\n+        generator.populateJsonMsgIntoKafka(TestQueryConstants.JSON_TOPIC, NUM_JSON_MSG);\n       }\n       initCount.incrementAndGet();\n       runningSuite = true;",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "ed0174755ad7833cbcb0bcf21990f481c5af189a",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -17,24 +17,24 @@\n  */\n package org.apache.drill.exec.store.kafka;\n \n-public interface QueryConstants {\n+public interface TestQueryConstants {\n \n   // Kafka Server Prop Constants\n-  public static final String BROKER_DELIM = \",\";\n-  public final String LOCAL_HOST = \"127.0.0.1\";\n+  String BROKER_DELIM = \",\";\n+  String LOCAL_HOST = \"127.0.0.1\";\n \n   // ZK\n-  public final static String ZK_TMP = \"zk_tmp\";\n-  public final static int TICK_TIME = 500;\n-  public final static int MAX_CLIENT_CONNECTIONS = 100;\n+  String ZK_TMP = \"zk_tmp\";\n+  int TICK_TIME = 500;\n+  int MAX_CLIENT_CONNECTIONS = 100;\n \n-  public static final String JSON_TOPIC = \"drill-json-topic\";\n-  public static final String AVRO_TOPIC = \"drill-avro-topic\";\n-  public static final String INVALID_TOPIC = \"invalid-topic\";\n+  String JSON_TOPIC = \"drill-json-topic\";\n+  String AVRO_TOPIC = \"drill-avro-topic\";\n+  String INVALID_TOPIC = \"invalid-topic\";\n \n   // Queries\n-  public static final String MSG_COUNT_QUERY = \"select count(*) from kafka.`%s`\";\n-  public static final String MSG_SELECT_QUERY = \"select * from kafka.`%s`\";\n-  public static final String MIN_OFFSET_QUERY = \"select MIN(kafkaMsgOffset) as minOffset from kafka.`%s`\";\n-  public static final String MAX_OFFSET_QUERY = \"select MAX(kafkaMsgOffset) as maxOffset from kafka.`%s`\";\n+  String MSG_COUNT_QUERY = \"select count(*) from kafka.`%s`\";\n+  String MSG_SELECT_QUERY = \"select * from kafka.`%s`\";\n+  String MIN_OFFSET_QUERY = \"select MIN(kafkaMsgOffset) as minOffset from kafka.`%s`\";\n+  String MAX_OFFSET_QUERY = \"select MAX(kafkaMsgOffset) as maxOffset from kafka.`%s`\";\n }",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "previous_filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/QueryConstants.java",
                "status": "renamed",
                "changes": 26,
                "deletions": 13,
                "sha": "057af7eb9bcbf971afa909461c1495aff3922a1b",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -26,7 +26,7 @@\n \n import org.apache.drill.exec.ZookeeperHelper;\n import org.apache.drill.exec.store.kafka.KafkaStoragePluginConfig;\n-import org.apache.drill.exec.store.kafka.QueryConstants;\n+import org.apache.drill.exec.store.kafka.TestQueryConstants;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.slf4j.Logger;\n@@ -35,7 +35,7 @@\n import kafka.server.KafkaConfig;\n import kafka.server.KafkaServerStartable;\n \n-public class EmbeddedKafkaCluster implements QueryConstants {\n+public class EmbeddedKafkaCluster implements TestQueryConstants {\n   private static final Logger logger = LoggerFactory.getLogger(EmbeddedKafkaCluster.class);\n   private List<KafkaServerStartable> brokers;\n   private final ZookeeperHelper zkHelper;",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "663e0e47a86aa513e8ece2aafcd915b7533bdd86",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -19,7 +19,6 @@\n \n import java.io.IOException;\n import java.util.Collection;\n-import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n \n@@ -45,7 +44,6 @@\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.ImmutableList;\n import org.apache.drill.exec.store.schedule.AffinityCreator;\n import org.apache.drill.exec.store.schedule.AssignmentCreator;\n import org.apache.drill.exec.store.schedule.CompleteWork;\n@@ -59,10 +57,10 @@\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(KuduGroupScan.class);\n   private static final long DEFAULT_TABLET_SIZE = 1000;\n \n-  private KuduStoragePluginConfig storagePluginConfig;\n+  private KuduStoragePlugin kuduStoragePlugin;\n   private List<SchemaPath> columns;\n   private KuduScanSpec kuduScanSpec;\n-  private KuduStoragePlugin storagePlugin;\n+\n   private boolean filterPushedDown = false;\n   private List<KuduWork> kuduWorkList = Lists.newArrayList();\n   private ListMultimap<Integer,KuduWork> assignments;\n@@ -71,31 +69,31 @@\n \n   @JsonCreator\n   public KuduGroupScan(@JsonProperty(\"kuduScanSpec\") KuduScanSpec kuduScanSpec,\n-                        @JsonProperty(\"storage\") KuduStoragePluginConfig storagePluginConfig,\n+                        @JsonProperty(\"kuduStoragePluginConfig\") KuduStoragePluginConfig kuduStoragePluginConfig,\n                         @JsonProperty(\"columns\") List<SchemaPath> columns,\n                         @JacksonInject StoragePluginRegistry pluginRegistry) throws IOException, ExecutionSetupException {\n-    this((KuduStoragePlugin) pluginRegistry.getPlugin(storagePluginConfig), kuduScanSpec, columns);\n+    this((KuduStoragePlugin) pluginRegistry.getPlugin(kuduStoragePluginConfig), kuduScanSpec, columns);\n   }\n \n-  public KuduGroupScan(KuduStoragePlugin storagePlugin, KuduScanSpec scanSpec,\n-      List<SchemaPath> columns) {\n+  public KuduGroupScan(KuduStoragePlugin kuduStoragePlugin,\n+                       KuduScanSpec kuduScanSpec,\n+                       List<SchemaPath> columns) {\n     super((String) null);\n-    this.storagePlugin = storagePlugin;\n-    this.storagePluginConfig = storagePlugin.getConfig();\n-    this.kuduScanSpec = scanSpec;\n+    this.kuduStoragePlugin = kuduStoragePlugin;\n+    this.kuduScanSpec = kuduScanSpec;\n     this.columns = columns == null || columns.size() == 0? ALL_COLUMNS : columns;\n     init();\n   }\n \n   private void init() {\n     String tableName = kuduScanSpec.getTableName();\n-    Collection<DrillbitEndpoint> endpoints = storagePlugin.getContext().getBits();\n+    Collection<DrillbitEndpoint> endpoints = kuduStoragePlugin.getContext().getBits();\n     Map<String,DrillbitEndpoint> endpointMap = Maps.newHashMap();\n     for (DrillbitEndpoint endpoint : endpoints) {\n       endpointMap.put(endpoint.getAddress(), endpoint);\n     }\n     try {\n-      List<LocatedTablet> locations = storagePlugin.getClient().openTable(tableName).getTabletsLocations(10000);\n+      List<LocatedTablet> locations = kuduStoragePlugin.getClient().openTable(tableName).getTabletsLocations(10000);\n       for (LocatedTablet tablet : locations) {\n         KuduWork work = new KuduWork(tablet.getPartition().getPartitionKeyStart(), tablet.getPartition().getPartitionKeyEnd());\n         for (Replica replica : tablet.getReplicas()) {\n@@ -153,10 +151,9 @@ public int compareTo(CompleteWork o) {\n    */\n   private KuduGroupScan(KuduGroupScan that) {\n     super(that);\n+    this.kuduStoragePlugin = that.kuduStoragePlugin;\n     this.columns = that.columns;\n     this.kuduScanSpec = that.kuduScanSpec;\n-    this.storagePlugin = that.storagePlugin;\n-    this.storagePluginConfig = that.storagePluginConfig;\n     this.filterPushedDown = that.filterPushedDown;\n     this.kuduWorkList = that.kuduWorkList;\n     this.assignments = that.assignments;\n@@ -204,7 +201,7 @@ public KuduSubScan getSpecificScan(int minorFragmentId) {\n       scanSpecList.add(new KuduSubScanSpec(getTableName(), work.getPartitionKeyStart(), work.getPartitionKeyEnd()));\n     }\n \n-    return new KuduSubScan(storagePlugin, storagePluginConfig, scanSpecList, this.columns);\n+    return new KuduSubScan(kuduStoragePlugin, scanSpecList, this.columns);\n   }\n \n   // KuduStoragePlugin plugin, KuduStoragePluginConfig config,\n@@ -224,7 +221,7 @@ public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n \n   @JsonIgnore\n   public KuduStoragePlugin getStoragePlugin() {\n-    return storagePlugin;\n+    return kuduStoragePlugin;\n   }\n \n   @JsonIgnore\n@@ -244,9 +241,9 @@ public String toString() {\n         + columns + \"]\";\n   }\n \n-  @JsonProperty(\"storage\")\n-  public KuduStoragePluginConfig getStorageConfig() {\n-    return this.storagePluginConfig;\n+  @JsonProperty\n+  public KuduStoragePluginConfig getKuduStoragePluginConfig() {\n+    return kuduStoragePlugin.getConfig();\n   }\n \n   @JsonProperty",
                "additions": 18,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "status": "modified",
                "changes": 39,
                "deletions": 21,
                "sha": "7bddf18617e8360fd468033d7ec61ba967c09041",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "filename": "contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -21,9 +21,9 @@\n import java.util.LinkedList;\n import java.util.List;\n \n+import com.google.common.collect.ImmutableSet;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractBase;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n import org.apache.drill.exec.physical.base.PhysicalVisitor;\n@@ -37,49 +37,40 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.Iterators;\n \n // Class containing information for reading a single Kudu tablet\n-@JsonTypeName(\"kudu-tablet-scan\")\n+@JsonTypeName(\"kudu-sub-scan\")\n public class KuduSubScan extends AbstractBase implements SubScan {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(KuduSubScan.class);\n \n-  @JsonProperty\n-  public final KuduStoragePluginConfig storage;\n-\n-\n   private final KuduStoragePlugin kuduStoragePlugin;\n   private final List<KuduSubScanSpec> tabletScanSpecList;\n   private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public KuduSubScan(@JacksonInject StoragePluginRegistry registry,\n-                      @JsonProperty(\"storage\") StoragePluginConfig storage,\n-      @JsonProperty(\"tabletScanSpecList\") LinkedList<KuduSubScanSpec> tabletScanSpecList,\n+                      @JsonProperty(\"kuduStoragePluginConfig\") KuduStoragePluginConfig kuduStoragePluginConfig,\n+                      @JsonProperty(\"tabletScanSpecList\") LinkedList<KuduSubScanSpec> tabletScanSpecList,\n                       @JsonProperty(\"columns\") List<SchemaPath> columns) throws ExecutionSetupException {\n     super((String) null);\n-    kuduStoragePlugin = (KuduStoragePlugin) registry.getPlugin(storage);\n+    kuduStoragePlugin = (KuduStoragePlugin) registry.getPlugin(kuduStoragePluginConfig);\n     this.tabletScanSpecList = tabletScanSpecList;\n-    this.storage = (KuduStoragePluginConfig) storage;\n     this.columns = columns;\n   }\n \n-  public KuduSubScan(KuduStoragePlugin plugin, KuduStoragePluginConfig config,\n-      List<KuduSubScanSpec> tabletInfoList, List<SchemaPath> columns) {\n+  public KuduSubScan(KuduStoragePlugin plugin, List<KuduSubScanSpec> tabletInfoList, List<SchemaPath> columns) {\n     super((String) null);\n-    kuduStoragePlugin = plugin;\n-    storage = config;\n+    this.kuduStoragePlugin = plugin;\n     this.tabletScanSpecList = tabletInfoList;\n     this.columns = columns;\n   }\n \n-  public List<KuduSubScanSpec> getTabletScanSpecList() {\n-    return tabletScanSpecList;\n+  public KuduStoragePluginConfig getKuduStoragePluginConfig() {\n+    return kuduStoragePlugin.getConfig();\n   }\n \n-  @JsonIgnore\n-  public KuduStoragePluginConfig getStorageConfig() {\n-    return storage;\n+  public List<KuduSubScanSpec> getTabletScanSpecList() {\n+    return tabletScanSpecList;\n   }\n \n   public List<SchemaPath> getColumns() {\n@@ -104,12 +95,12 @@ public KuduStoragePlugin getStorageEngine(){\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new KuduSubScan(kuduStoragePlugin, storage, tabletScanSpecList, columns);\n+    return new KuduSubScan(kuduStoragePlugin, tabletScanSpecList, columns);\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n-    return Iterators.emptyIterator();\n+    return ImmutableSet.<PhysicalOperator>of().iterator();\n   }\n \n   public static class KuduSubScanSpec {\n@@ -143,7 +134,7 @@ public String getTableName() {\n \n   @Override\n   public int getOperatorType() {\n-    return CoreOperatorType.HBASE_SUB_SCAN_VALUE;\n+    return CoreOperatorType.KUDU_SUB_SCAN_VALUE;\n   }\n \n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "status": "modified",
                "changes": 39,
                "deletions": 24,
                "sha": "ca577e753f265fbad1f31eb98ba58b33d7f4ecfb",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "filename": "contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -17,6 +17,7 @@\n  */\n package org.apache.drill.store.kudu;\n \n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.categories.KuduStorageTest;\n import org.junit.Ignore;\n@@ -44,6 +45,11 @@ public void testCreate() throws Exception {\n     test(\"create table kudu.regions as select 1, * from sys.options limit 1\");\n     test(\"select * from kudu.regions\");\n     test(\"drop table kudu.regions\");\n+  }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from kudu.demo\");\n   }\n+\n }",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "status": "modified",
                "changes": 8,
                "deletions": 1,
                "sha": "4e1c7fd5f745b8833e911d39744e1756160c1be7",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "filename": "contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -82,4 +82,11 @@ public void testUnShardedDBInShardedClusterWithGroupByProjectionAndFilter() thro\n         DONUTS_DB, DONUTS_COLLECTION);\n     runMongoSQLVerifyCount(queryString, 5);\n   }\n+\n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = String.format(TEST_BOOLEAN_FILTER_QUERY_TEMPLATE1,\n+        EMPLOYEE_DB, EMPINFO_COLLECTION);\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n+  }\n }",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "status": "modified",
                "changes": 7,
                "deletions": 0,
                "sha": "8d0064f2f948d1909c01c0e54773c68e5fd10998",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "filename": "contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -174,10 +174,9 @@ public void testBasicQueryWithNonExistentTableName() throws Exception {\n   }\n \n   @Test\n-  public void testPhysicalPlanExecutionBasedOnQuery() throws Exception {\n-    String query = \"EXPLAIN PLAN for select * from openTSDB.`(metric=warp.speed.test, start=47y-ago, aggregator=sum)`\";\n-    String plan = getPlanInString(query, JSON_FORMAT);\n-    Assert.assertEquals(18, testPhysical(plan));\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = \"select * from openTSDB.`(metric=warp.speed.test, start=47y-ago, aggregator=sum)`\";\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n   }\n \n   @Test",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "status": "modified",
                "changes": 7,
                "deletions": 4,
                "sha": "27ca09c61aeaa539c4c84eee2204786a2cd5dd9d",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "filename": "contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -521,6 +521,10 @@ private FragmentState(int index, int value) {\n      * <code>KAFKA_SUB_SCAN = 38;</code>\n      */\n     KAFKA_SUB_SCAN(38, 38),\n+    /**\n+     * <code>KUDU_SUB_SCAN = 39;</code>\n+     */\n+    KUDU_SUB_SCAN(39, 39),\n     ;\n \n     /**\n@@ -679,6 +683,10 @@ private FragmentState(int index, int value) {\n      * <code>KAFKA_SUB_SCAN = 38;</code>\n      */\n     public static final int KAFKA_SUB_SCAN_VALUE = 38;\n+    /**\n+     * <code>KUDU_SUB_SCAN = 39;</code>\n+     */\n+    public static final int KUDU_SUB_SCAN_VALUE = 39;\n \n \n     public final int getNumber() { return value; }\n@@ -724,6 +732,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 36: return AVRO_SUB_SCAN;\n         case 37: return PCAP_SUB_SCAN;\n         case 38: return KAFKA_SUB_SCAN;\n+        case 39: return KUDU_SUB_SCAN;\n         default: return null;\n       }\n     }\n@@ -24104,7 +24113,7 @@ public Builder clearStatus() {\n       \"agmentState\\022\\013\\n\\007SENDING\\020\\000\\022\\027\\n\\023AWAITING_ALL\" +\n       \"OCATION\\020\\001\\022\\013\\n\\007RUNNING\\020\\002\\022\\014\\n\\010FINISHED\\020\\003\\022\\r\\n\\t\" +\n       \"CANCELLED\\020\\004\\022\\n\\n\\006FAILED\\020\\005\\022\\032\\n\\026CANCELLATION_\" +\n-      \"REQUESTED\\020\\006*\\204\\006\\n\\020CoreOperatorType\\022\\021\\n\\rSING\" +\n+      \"REQUESTED\\020\\006*\\227\\006\\n\\020CoreOperatorType\\022\\021\\n\\rSING\" +\n       \"LE_SENDER\\020\\000\\022\\024\\n\\020BROADCAST_SENDER\\020\\001\\022\\n\\n\\006FIL\" +\n       \"TER\\020\\002\\022\\022\\n\\016HASH_AGGREGATE\\020\\003\\022\\r\\n\\tHASH_JOIN\\020\\004\" +\n       \"\\022\\016\\n\\nMERGE_JOIN\\020\\005\\022\\031\\n\\025HASH_PARTITION_SENDE\" +\n@@ -24123,11 +24132,11 @@ public Builder clearStatus() {\n       \"X_TO_JSON\\020\\037\\022\\025\\n\\021PRODUCER_CONSUMER\\020 \\022\\022\\n\\016HB\",\n       \"ASE_SUB_SCAN\\020!\\022\\n\\n\\006WINDOW\\020\\\"\\022\\024\\n\\020NESTED_LOO\" +\n       \"P_JOIN\\020#\\022\\021\\n\\rAVRO_SUB_SCAN\\020$\\022\\021\\n\\rPCAP_SUB_\" +\n-      \"SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCAN\\020&*g\\n\\nSaslStatus\" +\n-      \"\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\\nSASL_START\\020\\001\\022\\024\\n\\020SA\" +\n-      \"SL_IN_PROGRESS\\020\\002\\022\\020\\n\\014SASL_SUCCESS\\020\\003\\022\\017\\n\\013SA\" +\n-      \"SL_FAILED\\020\\004B.\\n\\033org.apache.drill.exec.pro\" +\n-      \"toB\\rUserBitSharedH\\001\"\n+      \"SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCAN\\020&\\022\\021\\n\\rKUDU_SUB_S\" +\n+      \"CAN\\020\\'*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\" +\n+      \"\\nSASL_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRESS\\020\\002\\022\\020\\n\\014S\" +\n+      \"ASL_SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B.\\n\\033org.ap\" +\n+      \"ache.drill.exec.protoB\\rUserBitSharedH\\001\"\n     };\n     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =\n       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "status": "modified",
                "changes": 21,
                "deletions": 6,
                "sha": "6ae9deb4cc878376071a7ae0570136b788e42c0f",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -60,7 +60,8 @@\n     NESTED_LOOP_JOIN(35),\n     AVRO_SUB_SCAN(36),\n     PCAP_SUB_SCAN(37),\n-    KAFKA_SUB_SCAN(38);\n+    KAFKA_SUB_SCAN(38),\n+    KUDU_SUB_SCAN(39);\n     \n     public final int number;\n     \n@@ -117,6 +118,7 @@ public static CoreOperatorType valueOf(int number)\n             case 36: return AVRO_SUB_SCAN;\n             case 37: return PCAP_SUB_SCAN;\n             case 38: return KAFKA_SUB_SCAN;\n+            case 39: return KUDU_SUB_SCAN;\n             default: return null;\n         }\n     }",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "71595f72e99c06cc76733c14cb46d77308d832d2",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            },
            {
                "patch": "@@ -326,6 +326,7 @@ enum CoreOperatorType {\n   AVRO_SUB_SCAN = 36;\n   PCAP_SUB_SCAN = 37;\n   KAFKA_SUB_SCAN = 38;\n+  KUDU_SUB_SCAN = 39;\n }\n \n /* Registry that contains list of jars, each jar contains its name and list of function signatures.",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/protobuf/UserBitShared.proto",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "b13f0591894a9e49a238c2cdbdd9fb7298da9c18",
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/protobuf/UserBitShared.proto",
                "filename": "protocol/src/main/protobuf/UserBitShared.proto",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/protobuf/UserBitShared.proto?ref=58e4cec9a913e381ef0e96b072c31e34085277b3"
            }
        ],
        "bug_id": "drill_24",
        "parent": "https://github.com/apache/drill/commit/58f3b10464f5a3b969ce19705750cb163be9b287",
        "message": "DRILL-6130: Fix NPE during physical plan submission for various storage plugins\n\n1. Fixed ser / de issues for Hive, Kafka, Hbase plugins.\n2. Added physical plan submission unit test for all storage plugins in contrib module.\n3. Refactoring.\n\ncloses #1108",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/f88a73c9e7a75f2d08cc54188816f591b003eff4",
        "file": [
            {
                "patch": "@@ -22,10 +22,8 @@\n import org.apache.drill.exec.expr.annotations.FunctionTemplate;\n import org.apache.drill.exec.expr.annotations.Output;\n import org.apache.drill.exec.expr.annotations.Param;\n-import org.apache.drill.exec.expr.annotations.Workspace;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n-import javax.crypto.Cipher;\n import javax.inject.Inject;\n \n public class CryptoFunctions {\n@@ -271,34 +269,25 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.ENCRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String encryptedText = \"\";\n       try {\n-        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(\"UTF-8\")));\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.ENCRYPT_MODE, secretKey);\n+        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(java.nio.charset.StandardCharsets.UTF_8)));\n       } catch (Exception e) {\n         //Exceptions are ignored\n       }\n@@ -331,33 +320,24 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.DECRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String decryptedText = \"\";\n       try {\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.DECRYPT_MODE, secretKey);\n         decryptedText = new String(cipher.doFinal(javax.xml.bind.DatatypeConverter.parseBase64Binary(input)));\n       } catch (Exception e) {\n         //Exceptions are ignored",
                "additions": 19,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "status": "modified",
                "changes": 58,
                "deletions": 39,
                "sha": "f914fb99a371b72f88072d0a8cf121d308909e26",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            },
            {
                "patch": "@@ -24,6 +24,8 @@\n import org.apache.drill.exec.expr.annotations.Param;\n import org.apache.drill.exec.expr.holders.BigIntHolder;\n import org.apache.drill.exec.expr.holders.BitHolder;\n+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;\n+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n import javax.inject.Inject;\n@@ -50,18 +52,15 @@ public void setup() {\n \n \n     public void eval() {\n-\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n \n-      int result = 0;\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      if (utils.getInfo().isInRange(ipString)) {\n-        result = 1;\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().isInRange(ipString) ? 1 : 0;\n+      } catch (IllegalArgumentException e) {\n+        // return false in case of invalid input\n       }\n-\n-      out.value = result;\n     }\n   }\n \n@@ -76,17 +75,20 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      out.value = utils.getInfo().getAddressCountLong();\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().getAddressCountLong();\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null in case of invalid input\n+      }\n     }\n \n   }\n@@ -101,7 +103,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -110,16 +112,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getBroadcastAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getBroadcastAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -134,7 +139,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -143,16 +148,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getNetmask();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getNetmask();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -167,7 +175,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -176,16 +184,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getLowAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getLowAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -200,7 +211,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -209,24 +220,27 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getHighAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getHighAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n   }\n \n   /**\n    * This function encodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_encode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urlencodeFunction implements DrillSimpleFunc {\n+  public static class UrlEncodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -261,7 +275,7 @@ public void eval() {\n    * This function decodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_decode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urldecodeFunction implements DrillSimpleFunc {\n+  public static class UrlDecodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -308,27 +322,20 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-\n     public void setup() {\n     }\n \n \n     public void eval() {\n       StringBuilder result = new StringBuilder(15);\n-\n       long inputInt = in.value;\n-\n       for (int i = 0; i < 4; i++) {\n-\n         result.insert(0, Long.toString(inputInt & 0xff));\n-\n         if (i < 3) {\n           result.insert(0, '.');\n         }\n-\n         inputInt = inputInt >> 8;\n       }\n-\n       String outputValue = result.toString();\n \n       out.buffer = buffer;\n@@ -356,13 +363,26 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n \n       String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      if (ipAddressInArray.length < 3) {\n+        return;\n+      }\n \n-      int[] octets = new int[3];\n-\n-      for (int i = 0; i < 3; i++) {\n-        octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+      // only first two octets are needed for the check\n+      int[] octets = new int[2];\n+      for (int i = 0; i < 2; i++) {\n+        try {\n+          octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return false\n+          return;\n+        }\n       }\n \n       int result = 0;\n@@ -392,28 +412,34 @@ public void eval() {\n     VarCharHolder inputTextA;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        String[] ipAddressInArray = ipString.split(\"\\\\.\");\n-\n-        long result = 0;\n-        for (int i = 0; i < ipAddressInArray.length; i++) {\n-          int power = 3 - i;\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n+\n+      String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      long result = 0;\n+      for (int i = 0; i < ipAddressInArray.length; i++) {\n+        int power = 3 - i;\n+        try {\n           int ip = Integer.parseInt(ipAddressInArray[i]);\n           result += ip * Math.pow(256, power);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return null\n+          return;\n         }\n-\n-        out.value = result;\n       }\n+      out.value = result;\n+      out.isSet = 1;\n     }\n   }\n \n@@ -435,18 +461,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValid(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValid(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -465,21 +481,10 @@ public void eval() {\n     public void setup() {\n     }\n \n-\n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet4Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet4Address(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -500,18 +505,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet6Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet6Address(ipString) ? 1 : 0;\n     }\n   }\n }\n\\ No newline at end of file",
                "additions": 109,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "status": "modified",
                "changes": 223,
                "deletions": 114,
                "sha": "0dbaf87a1db236afc6433d1221be74991e526925",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            },
            {
                "patch": "@@ -383,6 +383,7 @@ public void eval() {\n \n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String outputString = new org.apache.commons.codec.language.DoubleMetaphone().doubleMetaphone(input);\n+      outputString = outputString == null ? \"\" : outputString;\n \n       out.buffer = buffer;\n       out.start = 0;",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "66ab0da92aefe46b80d107f7a31d11662d9a7342",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            },
            {
                "patch": "@@ -17,14 +17,23 @@\n  */\n package org.apache.drill.exec.udfs;\n \n-import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestCryptoFunctions extends BaseTestQuery {\n+public class TestCryptoFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testMD5() throws Exception {\n@@ -74,23 +83,48 @@ public void testSHA512() throws Exception {\n \n   @Test\n   public void testAESEncrypt() throws Exception {\n-    final String query = \"select aes_encrypt('testing', 'secret_key') as encrypted FROM (VALUES(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_encrypt('testing', 'secret_key') as encrypted from (values(1))\")\n       .ordered()\n       .baselineColumns(\"encrypted\")\n       .baselineValues(\"ICf+zdOrLitogB8HUDru0w==\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt(cast(null as varchar), 'secret_key') as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt('testing', cast (null as varchar)) as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n \n   @Test\n   public void testAESDecrypt() throws Exception {\n-    final String query = \"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\")\n       .ordered()\n       .baselineColumns(\"decrypt\")\n       .baselineValues(\"testing\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt(cast(null as varchar), 'secret_key') as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', cast(null as varchar)) as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n }\n\\ No newline at end of file",
                "additions": 40,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "status": "modified",
                "changes": 46,
                "deletions": 6,
                "sha": "d382e96856186d676c21a1d9ad4e74f95610622e",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            },
            {
                "patch": "@@ -19,23 +19,44 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseTestQuery;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestNetworkFunctions extends BaseTestQuery {\n+public class TestNetworkFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testInetAton() throws Exception {\n-    final String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n-    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(Long.parseLong(\"3232235521\")).go();\n+    String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(3232235521L).go();\n+\n+    query = \"select inet_aton('192.168.0') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton('') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton(cast(null as varchar)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n   }\n \n   @Test\n   public void testInetNtoa() throws Exception {\n-    final String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n+    String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select inet_ntoa(cast(null as int)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -46,32 +67,68 @@ public void testInNetwork() throws Exception {\n \n   @Test\n   public void testNotInNetwork() throws Exception {\n-    final String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net FROM (values(1))\";\n+    String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('10.10.10.10', '') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('', '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network(cast(null as varchar), '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n+\n+    query = \"select in_network('10.10.10.10', cast(null as varchar)) as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n   public void testBroadcastAddress() throws Exception {\n-    final String query = \"select broadcast_address( '192.168.0.0/28' ) AS broadcast_address FROM (values(1))\";\n+    String query = \"select broadcast_address('192.168.0.0/28') as broadcast_address from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues(\"192.168.0.15\").go();\n+\n+    query = \"select broadcast_address('192.168.') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n+\n+    query = \"select broadcast_address('') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testNetmask() throws Exception {\n-    final String query = \"select netmask('192.168.0.0/28') AS netmask FROM (values(1))\";\n+    String query = \"select netmask('192.168.0.0/28') as netmask from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues(\"255.255.255.240\").go();\n+\n+    query = \"select netmask('192222') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n+\n+    query = \"select netmask('') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testLowAddress() throws Exception {\n-    final String query = \"SELECT low_address('192.168.0.0/28') AS low FROM (values(1))\";\n+    String query = \"select low_address('192.168.0.0/28') as low from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testHighAddress() throws Exception {\n-    final String query = \"SELECT high_address('192.168.0.0/28') AS high FROM (values(1))\";\n+    String query = \"select high_address('192.168.0.0/28') as high from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues(\"192.168.0.14\").go();\n+\n+    query = \"select high_address('192.168.0.') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n+\n+    query = \"select high_address('') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -88,8 +145,20 @@ public void testDecodeUrl() throws Exception {\n \n   @Test\n   public void testNotPrivateIP() throws Exception {\n-    final String query = \"SELECT is_private_ip('8.8.8.8') AS is_private_ip FROM (values(1))\";\n+    String query = \"select is_private_ip('8.8.8.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('8.A.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('192.168') as is_private_ip from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip(cast(null as varchar)) as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -100,8 +169,17 @@ public void testPrivateIP() throws Exception {\n \n   @Test\n   public void testNotValidIP() throws Exception {\n-    final String query = \"SELECT is_valid_IP('258.257.234.23') AS is_valid_IP FROM (values(1))\";\n+    String query = \"select is_valid_IP('258.257.234.23') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('258.257.2') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('') as is_valid_IP from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP(cast(null as varchar)) as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -112,8 +190,17 @@ public void testIsValidIP() throws Exception {\n \n   @Test\n   public void testNotValidIPv4() throws Exception {\n-    final String query = \"SELECT is_valid_IPv4( '192.168.0.257') AS is_valid_IP4 FROM (values(1))\";\n+    String query = \"select is_valid_IPv4('192.168.0.257') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('192123') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('') as is_valid_IP4 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4(cast(null as varchar)) as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -130,8 +217,32 @@ public void testIsValidIPv6() throws Exception {\n \n   @Test\n   public void testNotValidIPv6() throws Exception {\n-    final String query = \"SELECT is_valid_IPv6('1050:0:0:0:5:600:300c:326g') AS is_valid_IP6 FROM (values(1))\";\n+    String query = \"select is_valid_IPv6('1050:0:0:0:5:600:300c:326g') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('1050:0:0:0:5:600_AAA') as is_valid_IP6 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6(cast(null as varchar)) as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues((Boolean) null).go();\n+  }\n+\n+  @Test\n+  public void testAddressCount() throws Exception {\n+    String query = \"select address_count('192.168.0.1/30') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues(2L).go();\n+\n+    query = \"select address_count('192.168') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('192.168.0.1/100') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n   }\n \n }\n\\ No newline at end of file",
                "additions": 125,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "status": "modified",
                "changes": 139,
                "deletions": 14,
                "sha": "349e09794b015c97a3567a58b3e35739120d283d",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            },
            {
                "patch": "@@ -19,12 +19,10 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseDirTestWatcher;\n import org.apache.drill.test.ClusterFixture;\n import org.apache.drill.test.ClusterFixtureBuilder;\n import org.apache.drill.test.ClusterTest;\n import org.junit.BeforeClass;\n-import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n@@ -33,9 +31,6 @@\n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n public class TestPhoneticFunctions extends ClusterTest {\n \n-  @Rule\n-  public final BaseDirTestWatcher baseDirTestWatcher = new BaseDirTestWatcher();\n-\n   @BeforeClass\n   public static void setup() throws Exception {\n     ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n@@ -112,5 +107,10 @@ public void testDoubleMetaphone() throws Exception {\n         .sql(\"SELECT double_metaphone('Phoenix') AS meta FROM (VALUES(1))\")\n         .singletonString();\n     assertEquals(\"FNKS\", result);\n+\n+    result = queryBuilder()\n+        .sql(\"SELECT double_metaphone('') AS meta FROM (VALUES(1))\")\n+        .singletonString();\n+    assertEquals(\"\", result);\n   }\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "status": "modified",
                "changes": 10,
                "deletions": 5,
                "sha": "64fb6daa0b124dc9c0af21c208140f891b2f2703",
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4"
            }
        ],
        "bug_id": "drill_25",
        "parent": "https://github.com/apache/drill/commit/ea83672edc75fe379a4e19764232b10f5199d06e",
        "message": "DRILL-6705: Fix various failures in Crypto / Network / Phonetic functions when invalid input is given\n\n1. aes_decrypt / aes_ecrypt - moved cyper init part into eval method since it not a constant and can be different for each input\n2. double_metaphone - fixed NPE when given string is empty\n3. in_network / address_count / broadcast_address / netmask / low_address / high_address / - fixed IllegalArgumentException in case of invalid input\n4. is_private_ip / inet_aton - fixed ArrayIndexOutOfBoundsException / NumberFormatException in case of invalid input\n5. is_valid_IP / is_valid_IPv4 / is_valid_IPv6 - removed unnecessary checks\n6. Added appropriate unit tests\n\ncloses #1443",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
        "file": [
            {
                "patch": "@@ -17,6 +17,12 @@\n  */\n <@pp.dropOutputFile />\n \n+<#macro reassignHolder>\n+        previous.buffer = buf.reallocIfNeeded(length);\n+        previous.buffer.setBytes(0, in.buffer, in.start, length);\n+        previous.end = length;\n+</#macro>\n+\n \n <@pp.changeOutputFile name=\"/org/apache/drill/exec/expr/fn/impl/GNewValueFunctions.java\" />\n <#include \"/@includes/license.ftl\" />\n@@ -39,28 +45,51 @@\n  */\n public class GNewValueFunctions {\n <#list vv.types as type>\n-<#if type.major == \"Fixed\" || type.major = \"Bit\">\n-\n <#list type.minor as minor>\n <#list vv.modes as mode>\n   <#if mode.name != \"Repeated\">\n \n <#if !minor.class.startsWith(\"Decimal28\") && !minor.class.startsWith(\"Decimal38\") && !minor.class.startsWith(\"Interval\")>\n @SuppressWarnings(\"unused\")\n-@FunctionTemplate(name = \"newPartitionValue\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls=NullHandling.INTERNAL)\n-public static class NewValue${minor.class}${mode.prefix} implements DrillSimpleFunc{\n+@FunctionTemplate(name = \"newPartitionValue\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = NullHandling.INTERNAL)\n+public static class NewValue${minor.class}${mode.prefix} implements DrillSimpleFunc {\n \n   @Param ${mode.prefix}${minor.class}Holder in;\n   @Workspace ${mode.prefix}${minor.class}Holder previous;\n   @Workspace Boolean initialized;\n   @Output BitHolder out;\n+  <#if type.major == \"VarLen\">\n+  @Inject DrillBuf buf;\n+  </#if>\n \n   public void setup() {\n     initialized = false;\n+    <#if type.major == \"VarLen\">\n+    previous.buffer = buf;\n+    previous.start = 0;\n+    </#if>\n   }\n \n-  <#if mode.name == \"Required\">\n   public void eval() {\n+  <#if mode.name == \"Required\">\n+  <#if type.major == \"VarLen\">\n+    int length = in.end - in.start;\n+\n+    if (initialized) {\n+      if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n+          previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n+        out.value = 0;\n+      } else {\n+        <@reassignHolder/>\n+        out.value = 1;\n+      }\n+    } else {\n+      <@reassignHolder/>\n+      out.value = 1;\n+      initialized = true;\n+    }\n+  </#if>\n+  <#if type.major == \"Fixed\" || type.major == \"Bit\">\n     if (initialized) {\n       if (in.value == previous.value) {\n         out.value = 0;\n@@ -73,10 +102,36 @@ public void eval() {\n       out.value = 1;\n       initialized = true;\n     }\n-  }\n   </#if>\n+  </#if> <#-- mode.name == \"Required\" -->\n+\n   <#if mode.name == \"Optional\">\n-  public void eval() {\n+  <#if type.major == \"VarLen\">\n+    int length = in.isSet == 0 ? 0 : in.end - in.start;\n+\n+    if (initialized) {\n+      if (previous.isSet == 0 && in.isSet == 0) {\n+        out.value = 0;\n+      } else if (previous.isSet != 0 && in.isSet != 0 && org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n+          previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n+        out.value = 0;\n+      } else {\n+        if (in.isSet == 1) {\n+          <@reassignHolder/>\n+        }\n+        previous.isSet = in.isSet;\n+        out.value = 1;\n+      }\n+    } else {\n+      if (in.isSet == 1) {\n+        <@reassignHolder/>\n+      }\n+      previous.isSet = in.isSet;\n+      out.value = 1;\n+      initialized = true;\n+    }\n+  </#if>\n+  <#if type.major == \"Fixed\" || type.major == \"Bit\">\n     if (initialized) {\n       if (in.isSet == 0 && previous.isSet == 0) {\n         out.value = 0;\n@@ -93,14 +148,14 @@ public void eval() {\n       out.value = 1;\n       initialized = true;\n     }\n-  }\n   </#if>\n+  </#if> <#-- mode.name == \"Optional\" -->\n+  }\n }\n </#if> <#-- minor.class.startWith -->\n \n </#if> <#-- mode.name -->\n </#list>\n </#list>\n-</#if> <#-- type.major -->\n </#list>\n }",
                "additions": 64,
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "status": "modified",
                "changes": 73,
                "deletions": 9,
                "sha": "5591d669c4bedbb053d5acbf9385d255b62ca7f6",
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "filename": "exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611"
            },
            {
                "patch": "@@ -1,209 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.drill.exec.store;\n-\n-import io.netty.buffer.DrillBuf;\n-import org.apache.drill.exec.expr.DrillSimpleFunc;\n-import org.apache.drill.exec.expr.annotations.FunctionTemplate;\n-import org.apache.drill.exec.expr.annotations.FunctionTemplate.NullHandling;\n-import org.apache.drill.exec.expr.annotations.Output;\n-import org.apache.drill.exec.expr.annotations.Param;\n-import org.apache.drill.exec.expr.annotations.Workspace;\n-import org.apache.drill.exec.expr.holders.BitHolder;\n-import org.apache.drill.exec.expr.holders.IntHolder;\n-import org.apache.drill.exec.expr.holders.NullableVarBinaryHolder;\n-import org.apache.drill.exec.expr.holders.NullableVarCharHolder;\n-import org.apache.drill.exec.expr.holders.VarBinaryHolder;\n-import org.apache.drill.exec.expr.holders.VarCharHolder;\n-\n-import javax.inject.Inject;\n-\n-/**\n- *  The functions are similar to those created through FreeMarker template for fixed types. There is not much benefit to\n- *  using code generation for generating the functions for variable length types, so simply doing them by hand.\n- */\n-public class NewValueFunction {\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarChar implements DrillSimpleFunc {\n-\n-    @Param VarCharHolder in;\n-    @Workspace VarCharHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.end - in.start;\n-\n-      if (initialized) {\n-        if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n-          out.value = 0;\n-        } else {\n-          previous.buffer = buf.reallocIfNeeded(length);\n-          previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-          previous.end = in.end - in.start;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarCharNullable implements DrillSimpleFunc {\n-\n-    @Param NullableVarCharHolder in;\n-    @Workspace NullableVarCharHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.isSet == 0 ? 0 : in.end - in.start;\n-\n-      if (initialized) {\n-        if (previous.isSet == 0 && in.isSet == 0 ||\n-            (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n-                previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0)) {\n-          out.value = 0;\n-        } else {\n-          if (in.isSet == 1) {\n-            previous.buffer = buf.reallocIfNeeded(length);\n-            previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-            previous.end = in.end - in.start;\n-          }\n-          previous.isSet = in.isSet;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        previous.isSet = 1;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarBinary implements DrillSimpleFunc {\n-\n-    @Param VarBinaryHolder in;\n-    @Workspace VarBinaryHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.end - in.start;\n-\n-      if (initialized) {\n-        if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n-          out.value = 0;\n-        } else {\n-          previous.buffer = buf.reallocIfNeeded(length);\n-          previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-          previous.end = in.end - in.start;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarBinaryNullable implements DrillSimpleFunc {\n-\n-    @Param NullableVarBinaryHolder in;\n-    @Workspace NullableVarBinaryHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.isSet == 0 ? 0 : in.end - in.start;\n-\n-      if (initialized) {\n-        if (previous.isSet == 0 && in.isSet == 0 ||\n-            (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n-                previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0)) {\n-          out.value = 0;\n-        } else {\n-          if (in.isSet == 1) {\n-            previous.buffer = buf.reallocIfNeeded(length);\n-            previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-            previous.end = in.end - in.start;\n-          }\n-          previous.isSet = in.isSet;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        previous.isSet = 1;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-}",
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/15b021f1f4c73c08c0443fd9cae0221ae43877ba/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "status": "removed",
                "changes": 209,
                "deletions": 209,
                "sha": "fedb4733a5fb0d62e0121c1d2a23f54085395e27",
                "blob_url": "https://github.com/apache/drill/blob/15b021f1f4c73c08c0443fd9cae0221ae43877ba/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java?ref=15b021f1f4c73c08c0443fd9cae0221ae43877ba"
            },
            {
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.types.TypeProtos;\n import org.apache.drill.common.util.TestTools;\n+import org.apache.drill.exec.proto.UserBitShared;\n+import org.apache.drill.exec.rpc.user.QueryDataBatch;\n import org.junit.Ignore;\n import org.junit.Test;\n \n@@ -285,41 +287,51 @@ public void stddevEmptyNonexistentNullableInput() throws Exception {\n \n   }\n   @Test\n-  public void minEmptyNonnullableInput() throws Exception {\n-    // test min function on required type\n-    String query = \"select \" +\n-        \"min(bool_col) col1, min(int_col) col2, min(bigint_col) col3, min(float4_col) col4, min(float8_col) col5, \" +\n-        \"min(date_col) col6, min(time_col) col7, min(timestamp_col) col8, min(interval_year_col) col9, \" +\n-        \"min(varhcar_col) col10 \" +\n-        \"from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n-\n-    testBuilder()\n-        .sqlQuery(query)\n-        .unOrdered()\n-        .baselineColumns(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\")\n-        .baselineValues(null, null, null, null, null, null, null, null, null, null)\n-        .go();\n-  }\n+  public void minMaxEmptyNonNullableInput() throws Exception {\n+    // test min and max functions on required type\n+\n+    final QueryDataBatch result = testSqlWithResults(\"select * from cp.`parquet/alltypes_required.parquet` limit 0\")\n+        .get(0);\n+\n+    final Map<String, StringBuilder> functions = Maps.newHashMap();\n+    functions.put(\"min\", new StringBuilder());\n+    functions.put(\"max\", new StringBuilder());\n+\n+    final Map<String, Object> resultingValues = Maps.newHashMap();\n+    for (UserBitShared.SerializedField field : result.getHeader().getDef().getFieldList()) {\n+      final String fieldName = field.getNamePart().getName();\n+      // Only COUNT aggregate function supported for Boolean type\n+      if (fieldName.equals(\"col_bln\")) {\n+        continue;\n+      }\n+      resultingValues.put(String.format(\"`%s`\", fieldName), null);\n+      for (Map.Entry<String, StringBuilder> function : functions.entrySet()) {\n+        function.getValue()\n+            .append(function.getKey())\n+            .append(\"(\")\n+            .append(fieldName)\n+            .append(\") \")\n+            .append(fieldName)\n+            .append(\",\");\n+      }\n+    }\n+    result.release();\n \n-  @Test\n-  public void maxEmptyNonnullableInput() throws Exception {\n+    final String query = \"select %s from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n+    final List<Map<String, Object>> baselineRecords = Lists.newArrayList();\n+    baselineRecords.add(resultingValues);\n \n-    // test max function\n-    final String query = \"select \" +\n-        \"max(int_col) col1, max(bigint_col) col2, max(float4_col) col3, max(float8_col) col4, \" +\n-        \"max(date_col) col5, max(time_col) col6, max(timestamp_col) col7, max(interval_year_col) col8, \" +\n-        \"max(varhcar_col) col9 \" +\n-        \"from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n+    for (StringBuilder selectBody : functions.values()) {\n+      selectBody.setLength(selectBody.length() - 1);\n \n-    testBuilder()\n-        .sqlQuery(query)\n-        .unOrdered()\n-        .baselineColumns(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\")\n-        .baselineValues(null, null, null, null, null, null, null, null, null)\n-        .go();\n+      testBuilder()\n+          .sqlQuery(query, selectBody.toString())\n+          .unOrdered()\n+          .baselineRecords(baselineRecords)\n+          .go();\n+    }\n   }\n \n-\n   /*\n    * Streaming agg on top of a filter produces wrong results if the first two batches are filtered out.\n    * In the below test we have three files in the input directory and since the ordering of reading",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "status": "modified",
                "changes": 72,
                "deletions": 30,
                "sha": "36ee1b91659ed517b239c777c65e874ec111582b",
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611"
            },
            {
                "patch": "@@ -17,11 +17,15 @@\n  */\n package org.apache.drill.exec.sql;\n \n+import com.google.common.collect.Maps;\n import org.apache.commons.io.FileUtils;\n import org.apache.drill.BaseTestQuery;\n+import org.apache.drill.exec.proto.UserBitShared;\n+import org.apache.drill.exec.rpc.user.QueryDataBatch;\n import org.junit.Test;\n \n import java.io.File;\n+import java.util.Map;\n \n public class TestCTAS extends BaseTestQuery {\n   @Test // DRILL-2589\n@@ -125,8 +129,7 @@ public void ctasPartitionWithEmptyList() throws Exception {\n     try {\n       final String ctasQuery = String.format(\"CREATE TABLE %s.%s PARTITION BY AS SELECT * from cp.`region.json`\", TEMP_SCHEMA, newTblName);\n \n-      errorMsgTestHelper(ctasQuery,\n-          String.format(\"PARSE ERROR: Encountered \\\"AS\\\"\"));\n+      errorMsgTestHelper(ctasQuery,\"PARSE ERROR: Encountered \\\"AS\\\"\");\n     } finally {\n       FileUtils.deleteQuietly(new File(getDfsTestTmpSchemaLocation(), newTblName));\n     }\n@@ -238,6 +241,41 @@ public void ctasWithPartition() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testPartitionByForAllTypes() throws Exception {\n+    final String location = \"partitioned_tables_with_nulls\";\n+    final String ctasQuery = \"create table %s partition by (%s) as %s\";\n+    final String tablePath = \"%s.`%s/%s_%s`\";\n+\n+    // key - new table suffix, value - data query\n+    final Map<String, String> variations = Maps.newHashMap();\n+    variations.put(\"required\", \"select * from cp.`parquet/alltypes_required.parquet`\");\n+    variations.put(\"optional\", \"select * from cp.`parquet/alltypes_optional.parquet`\");\n+    variations.put(\"nulls_only\", \"select * from cp.`parquet/alltypes_optional.parquet` where %s is null\");\n+\n+    try {\n+      final QueryDataBatch result = testSqlWithResults(\"select * from cp.`parquet/alltypes_required.parquet` limit 0\").get(0);\n+      for (UserBitShared.SerializedField field : result.getHeader().getDef().getFieldList()) {\n+        final String fieldName = field.getNamePart().getName();\n+\n+        for (Map.Entry<String, String> variation : variations.entrySet()) {\n+          final String table = String.format(tablePath, TEMP_SCHEMA, location, fieldName, variation.getKey());\n+          final String dataQuery = String.format(variation.getValue(), fieldName);\n+          test(ctasQuery, table, fieldName, dataQuery, fieldName);\n+          testBuilder()\n+              .sqlQuery(\"select * from %s\", table)\n+              .unOrdered()\n+              .sqlBaselineQuery(dataQuery)\n+              .build()\n+              .run();\n+        }\n+      }\n+      result.release();\n+    } finally {\n+      FileUtils.deleteQuietly(new File(getDfsTestTmpSchemaLocation(), location));\n+    }\n+  }\n+\n   private static void ctasErrorTestHelper(final String ctasSql, final String expErrorMsg) throws Exception {\n     final String createTableSql = String.format(ctasSql, TEMP_SCHEMA, \"testTableName\");\n     errorMsgTestHelper(createTableSql, expErrorMsg);",
                "additions": 40,
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "status": "modified",
                "changes": 42,
                "deletions": 2,
                "sha": "9d9b403ede37975644385638d80e1562e326adaf",
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "status": "added",
                "changes": 0,
                "deletions": 0,
                "sha": "53f5fa19d2f1761b93878424301d242466eba272",
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "filename": "exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "status": "modified",
                "changes": 0,
                "deletions": 0,
                "sha": "efc6add0cb583674bb2310aeca75c41d179a653d",
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "filename": "exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611"
            }
        ],
        "bug_id": "drill_26",
        "parent": "https://github.com/apache/drill/commit/15b021f1f4c73c08c0443fd9cae0221ae43877ba",
        "message": "DRILL-5039: NPE - CTAS PARTITION BY (<char-type-column>)\n\nclose apache/drill#706",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/99eba727fe2a38d3bf15dd5c3fc390adfdabd1bb",
        "file": [
            {
                "patch": "@@ -189,7 +189,7 @@ public void applyAssignments(List<DrillbitEndpoint> incomingEndpoints) {\n     PriorityQueue<List<MapRDBSubScanSpec>> minHeap = new PriorityQueue<List<MapRDBSubScanSpec>>(numSlots, LIST_SIZE_COMPARATOR);\n     PriorityQueue<List<MapRDBSubScanSpec>> maxHeap = new PriorityQueue<List<MapRDBSubScanSpec>>(numSlots, LIST_SIZE_COMPARATOR_REV);\n     for(List<MapRDBSubScanSpec> listOfScan : endpointFragmentMapping.values()) {\n-      if (listOfScan.size() < minPerEndpointSlot) {\n+      if (listOfScan.size() <= minPerEndpointSlot) {\n         minHeap.offer(listOfScan);\n       } else if (listOfScan.size() > minPerEndpointSlot){\n         maxHeap.offer(listOfScan);",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/99eba727fe2a38d3bf15dd5c3fc390adfdabd1bb/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBGroupScan.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "2de30e301ba793424efaada8c404117e76b4307b",
                "blob_url": "https://github.com/apache/drill/blob/99eba727fe2a38d3bf15dd5c3fc390adfdabd1bb/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBGroupScan.java",
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBGroupScan.java?ref=99eba727fe2a38d3bf15dd5c3fc390adfdabd1bb"
            }
        ],
        "bug_id": "drill_27",
        "parent": "https://github.com/apache/drill/commit/25b213cd39ec192bebd043770a8131c410699421",
        "message": "DRILL-5395: Query on MapR-DB table fails with NPE due to an issue with assignment logic closes #803",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
        "file": [
            {
                "patch": "@@ -150,7 +150,11 @@ public boolean apply(@Nullable FileStatus status) {\n     logger.debug(\"FileSelection.minusDirectories() took {} ms, numFiles: {}\",\n         timer.elapsed(TimeUnit.MILLISECONDS), total);\n \n-    fileSel.setExpanded();\n+    // fileSel will be null if we query an empty folder\n+    if (fileSel != null) {\n+      fileSel.setExpanded();\n+    }\n+\n     return fileSel;\n   }\n ",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "5b4813ab6c4e9b852237a5552522de064f93c012",
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a"
            },
            {
                "patch": "@@ -27,6 +27,7 @@\n import java.util.Set;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.logical.StoragePluginConfig;\n@@ -568,6 +569,7 @@ public long getRowCount() {\n    * @return file selection read from cache\n    *\n    * @throws IOException\n+   * @throws UserException when the updated selection is empty, this happens if the user selects an empty folder.\n    */\n   private FileSelection\n   initFromMetadataCache(FileSelection selection, Path metaFilePath) throws IOException {\n@@ -580,7 +582,8 @@ public long getRowCount() {\n     List<String> fileNames = Lists.newArrayList();\n     List<FileStatus> fileStatuses = selection.getStatuses(fs);\n \n-    if (fileStatuses.size() == 1 && fileStatuses.get(0).isDirectory()) {\n+    final Path first = fileStatuses.get(0).getPath();\n+    if (fileStatuses.size() == 1 && selection.getSelectionRoot().equals(first.toString())) {\n       // we are selecting all files from selection root. Expand the file list from the cache\n       for (Metadata.ParquetFileMetadata file : parquetTableMetadata.getFiles()) {\n         fileNames.add(file.getPath());\n@@ -590,7 +593,7 @@ public long getRowCount() {\n       // we need to expand the files from fileStatuses\n       for (FileStatus status : fileStatuses) {\n         if (status.isDirectory()) {\n-          //TODO read the metadata cache files in parallel\n+          //TODO [DRILL-4496] read the metadata cache files in parallel\n           final Path metaPath = new Path(status.getPath(), Metadata.METADATA_FILENAME);\n           final Metadata.ParquetTableMetadataBase metadata = Metadata.readBlockMeta(fs, metaPath.toString());\n           for (Metadata.ParquetFileMetadata file : metadata.getFiles()) {\n@@ -606,6 +609,11 @@ public long getRowCount() {\n       fileSet = Sets.newHashSet(fileNames);\n     }\n \n+    if (fileNames.isEmpty()) {\n+      // no files were found, most likely we tried to query some empty sub folders\n+      throw UserException.validationError().message(\"The table you tried to query is empty\").build(logger);\n+    }\n+\n     // when creating the file selection, set the selection root in the form /a/b instead of\n     // file:/a/b.  The reason is that the file names above have been created in the form\n     // /a/b/c.parquet and the format of the selection root must match that of the file names",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "status": "modified",
                "changes": 12,
                "deletions": 2,
                "sha": "47172cc8a59ace5f016a4f9d451cd2f41784d1e9",
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a"
            },
            {
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.parquet;\n+\n+import org.apache.drill.BaseTestQuery;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestParquetGroupScan extends BaseTestQuery {\n+\n+  private void prepareTables(final String tableName, boolean refreshMetadata) throws Exception {\n+    // first create some parquet subfolders\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s`      AS SELECT employee_id FROM cp.`employee.json` LIMIT 1\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/501`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 2\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/502`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 4\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/503`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 8\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/504`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 16\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/505`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 32\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/60`   AS SELECT employee_id FROM cp.`employee.json` LIMIT 64\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/602`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 128\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6031` AS SELECT employee_id FROM cp.`employee.json` LIMIT 256\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6032` AS SELECT employee_id FROM cp.`employee.json` LIMIT 512\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6033` AS SELECT employee_id FROM cp.`employee.json` LIMIT 1024\", tableName);\n+\n+    // we need an empty subfolder `4376/20160401`\n+    // to do this we first create a table inside that subfolder\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6041/a` AS SELECT * FROM cp.`employee.json` LIMIT 1\", tableName);\n+    // then we delete the table, leaving the parent subfolder empty\n+    testNoResult(\"DROP TABLE   dfs_test.tmp.`%s/6041/a`\", tableName);\n+\n+    if (refreshMetadata) {\n+      // build the metadata cache file\n+      testNoResult(\"REFRESH TABLE METADATA dfs_test.tmp.`%s`\", tableName);\n+    }\n+  }\n+\n+  @Test\n+  public void testFix4376() throws Exception {\n+    prepareTables(\"4376_1\", true);\n+\n+    testBuilder()\n+      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_1/60*`\")\n+      .ordered()\n+      .baselineColumns(\"count\").baselineValues(1984L)\n+      .go();\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_2\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_2/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_3\", false);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_3/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_3/604*' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_4\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_4/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_5\", false);\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_5/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_5/6041' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+}",
                "additions": 120,
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "status": "added",
                "changes": 120,
                "deletions": 0,
                "sha": "21d62858f7a901a80fb4fd12266198f2559f0d83",
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a"
            },
            {
                "patch": "@@ -177,25 +177,6 @@ public void testFix4449() throws Exception {\n       .go();\n   }\n \n-  @Test\n-  public void testFix4376() throws Exception {\n-    // first create some parquet subfolders\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376`    AS SELECT * FROM cp.`employee.json` LIMIT 1\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/01` AS SELECT * FROM cp.`employee.json` LIMIT 2\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/02` AS SELECT * FROM cp.`employee.json` LIMIT 4\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/0`  AS SELECT * FROM cp.`employee.json` LIMIT 8\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/11` AS SELECT * FROM cp.`employee.json` LIMIT 16\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/12` AS SELECT * FROM cp.`employee.json` LIMIT 32\");\n-    // next, build the metadata cache file\n-    runSQL(\"REFRESH TABLE METADATA dfs_test.tmp.`4376`\");\n-\n-    testBuilder()\n-      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376/0*`\")\n-      .ordered()\n-      .baselineColumns(\"count\").baselineValues(15L)\n-      .go();\n-    }\n-\n   private void checkForMetadataFile(String table) throws Exception {\n     String tmpDir = getDfsTestTmpSchemaLocation();\n     String metaFile = Joiner.on(\"/\").join(tmpDir, table, Metadata.METADATA_FILENAME);",
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "status": "modified",
                "changes": 19,
                "deletions": 19,
                "sha": "4330c96471e1324572bac6283ef7f0a40b38eb37",
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a"
            }
        ],
        "bug_id": "drill_28",
        "parent": "https://github.com/apache/drill/commit/11fe8d7cdb1df4100cd48bcce1de0b2c3c5f983a",
        "message": "DRILL-4484: NPE when querying  empty directory",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/a459e4dbbc242fe0d06b221afe5f830780fc682e",
        "file": [
            {
                "patch": "@@ -196,7 +196,7 @@ public static DateCorruptionStatus detectCorruptDates(ParquetMetadata footer,\n     } else {\n       // Possibly an old, un-migrated Drill file, check the column statistics to see if min/max values look corrupt\n       // only applies if there is a date column selected\n-      if (createdBy.equals(\"parquet-mr\")) {\n+      if (createdBy == null || createdBy.equals(\"parquet-mr\")) {\n         // loop through parquet column metadata to find date columns, check for corrupt values\n         return checkForCorruptDateValuesInStatistics(footer, columns, autoCorrectCorruptDates);\n       } else {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/a459e4dbbc242fe0d06b221afe5f830780fc682e/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "767c98de8480cae7bd199ae06103bebdc05e0a27",
                "blob_url": "https://github.com/apache/drill/blob/a459e4dbbc242fe0d06b221afe5f830780fc682e/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java?ref=a459e4dbbc242fe0d06b221afe5f830780fc682e"
            }
        ],
        "bug_id": "drill_29",
        "parent": "https://github.com/apache/drill/commit/ee3489ce3b6e5ad53e5c3a59b6e2e4e50773c630",
        "message": "DRILL-5400: Fix NPE in corrupt date detection\n\nThis closes #646",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/d44b889ffebbb58939110776795489c052aac786",
        "file": [
            {
                "patch": "@@ -93,7 +93,7 @@ public void mark() {\n     // Release all batches before current batch. [0 to startBatchPosition).\n     final Map<Range<Long>,RecordBatchData> oldBatches = batches.subRangeMap(Range.closedOpen(0l, startBatchPosition)).asMapOfRanges();\n     for (Range<Long> range : oldBatches.keySet()) {\n-      oldBatches.get(range.lowerEndpoint()).clear();\n+      oldBatches.get(range).clear();\n     }\n     batches.remove(Range.closedOpen(0l, startBatchPosition));\n     markedInnerPosition = innerPosition;\n@@ -113,8 +113,9 @@ public void reset() {\n       }\n       innerPosition = markedInnerPosition;\n       outerPosition = markedOuterPosition;\n-      startBatchPosition = batches.getEntry(outerPosition).getKey().lowerEndpoint();\n-      innerRecordCount = (int)(batches.getEntry(outerPosition).getKey().upperEndpoint() - startBatchPosition);\n+      final Range<Long> markedBatchRange = batches.getEntry(outerPosition).getKey();\n+      startBatchPosition = markedBatchRange.lowerEndpoint();\n+      innerRecordCount = (int)(markedBatchRange.upperEndpoint() - startBatchPosition);\n       markedInnerPosition = -1;\n       markedOuterPosition = -1;\n     }\n@@ -133,9 +134,10 @@ public void forward(long delta) {\n     // Get vectors from new position.\n     container.transferIn(rbdNew.getContainer());\n     outerPosition = nextOuterPosition;\n-    startBatchPosition = batches.getEntry(outerPosition).getKey().lowerEndpoint();\n+    final Range<Long> markedBatchRange = batches.getEntry(outerPosition).getKey();\n+    startBatchPosition = markedBatchRange.lowerEndpoint();\n     innerPosition = (int)(outerPosition - startBatchPosition);\n-    innerRecordCount = (int)(batches.getEntry(outerPosition).getKey().upperEndpoint() - startBatchPosition);\n+    innerRecordCount = (int)(markedBatchRange.upperEndpoint() - startBatchPosition);\n   }\n \n   /**\n@@ -239,7 +241,9 @@ public long getOuterPosition() {\n \n   public int getCurrentPosition() {\n     Preconditions.checkArgument(initialized);\n-    Preconditions.checkArgument(innerPosition >= 0 && innerPosition < innerRecordCount);\n+    Preconditions.checkArgument(innerPosition >= 0 && innerPosition < innerRecordCount,\n+      String.format(\"innerPosition:%d, outerPosition:%d, innerRecordCount:%d, totalRecordCount:%d\",\n+        innerPosition, outerPosition, innerRecordCount, totalRecordCount));\n     return innerPosition;\n   }\n ",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/d44b889ffebbb58939110776795489c052aac786/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "status": "modified",
                "changes": 16,
                "deletions": 6,
                "sha": "7b5ec2889760089252cce73e6a88420881bf1955",
                "blob_url": "https://github.com/apache/drill/blob/d44b889ffebbb58939110776795489c052aac786/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java?ref=d44b889ffebbb58939110776795489c052aac786"
            }
        ],
        "bug_id": "drill_30",
        "parent": "https://github.com/apache/drill/commit/46c47a24f67e53146521d50062180c4aa902f687",
        "message": "DRILL-4109 Fix NPE in RecordIterator.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/2b5c52e57f7421ec1e8aca9cd8693e299fc436d6",
        "file": [
            {
                "patch": "@@ -174,7 +174,9 @@ rem sets an initial level instead of leaving it at whatever the Driver's default\n rem is.)\r\n rem Put our property specification before previous value of DRILL_SHELL_JAVA_OPTS\r\n rem so that it can still be overridden via DRILL_SHELL_JAVA_OPTS.\r\n-set DRILL_SHELL_JAVA_OPTS=-Dsqlline.isolation=TRANSACTION_NONE %DRILL_SHELL_JAVA_OPTS%\r\n+rem \r\n+rem This is not currently needed as the new SQLLine we are using doesn't isolate.\r\n+rem set DRILL_SHELL_JAVA_OPTS=-Dsqlline.isolation=TRANSACTION_NONE %DRILL_SHELL_JAVA_OPTS%\r\n \r\n set DRILL_SHELL_JAVA_OPTS=%DRILL_SHELL_JAVA_OPTS% -Dlog.path=\"%DRILL_LOG_DIR%\\sqlline.log\" -Dlog.query.path=\"%DRILL_LOG_DIR%\\sqlline_queries.log\"\r\n \r",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/2b5c52e57f7421ec1e8aca9cd8693e299fc436d6/distribution/src/resources/sqlline.bat",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "a0efdf1186a01362ffb1a2aa85cc6675e1d9dda3",
                "blob_url": "https://github.com/apache/drill/blob/2b5c52e57f7421ec1e8aca9cd8693e299fc436d6/distribution/src/resources/sqlline.bat",
                "filename": "distribution/src/resources/sqlline.bat",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/distribution/src/resources/sqlline.bat?ref=2b5c52e57f7421ec1e8aca9cd8693e299fc436d6"
            }
        ],
        "bug_id": "drill_31",
        "parent": "https://github.com/apache/drill/commit/e923ac54db6abddcae0d18fc7ed37f4b737b22b2",
        "message": "DRILL-3120: Windows startup throws NPE",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/e923ac54db6abddcae0d18fc7ed37f4b737b22b2",
        "file": [
            {
                "patch": "@@ -171,7 +171,7 @@ if NOT \"test%DRILL_CLASSPATH%\"==\"test\" set DRILL_CP=!DRILL_CP!;%DRILL_CLASSPATH%\n \r\n rem Override SQLLine's default initial transaction isolation level.  (SQLLine\r\n rem sets an initial level instead of leaving it at whatever the Driver's default\r\n-rem is.) \r\n+rem is.)\r\n rem Put our property specification before previous value of DRILL_SHELL_JAVA_OPTS\r\n rem so that it can still be overridden via DRILL_SHELL_JAVA_OPTS.\r\n set DRILL_SHELL_JAVA_OPTS=-Dsqlline.isolation=TRANSACTION_NONE %DRILL_SHELL_JAVA_OPTS%\r",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/e923ac54db6abddcae0d18fc7ed37f4b737b22b2/distribution/src/resources/sqlline.bat",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "05132e4c54e9e61f838546d842ce34ee22436201",
                "blob_url": "https://github.com/apache/drill/blob/e923ac54db6abddcae0d18fc7ed37f4b737b22b2/distribution/src/resources/sqlline.bat",
                "filename": "distribution/src/resources/sqlline.bat",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/distribution/src/resources/sqlline.bat?ref=e923ac54db6abddcae0d18fc7ed37f4b737b22b2"
            }
        ],
        "bug_id": "drill_32",
        "parent": "https://github.com/apache/drill/commit/fdfb1b26c42e90bb07eb74f687b4a7328e750edb",
        "message": "DRILL-3120: Windows startup throws NPE",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/4cfdb3b653ba4db664abc14c4b1d51e4cec5c668",
        "file": [
            {
                "patch": "@@ -236,22 +236,27 @@ public int compare(Node node1, Node node2) {\n \n       if (node.valueIndex == batchLoaders[node.batchId].getRecordCount() - 1) {\n         // reached the end of an incoming record batch\n+        RawFragmentBatch nextBatch = null;\n         try {\n-          incomingBatches[node.batchId] = fragProviders[node.batchId].getNext();\n+          nextBatch = fragProviders[node.batchId].getNext();\n+\n+          while (nextBatch != null && nextBatch.getHeader().getDef().getRecordCount() == 0) {\n+            nextBatch = fragProviders[node.batchId].getNext();\n+          }\n         } catch (IOException e) {\n           context.fail(e);\n           return IterOutcome.STOP;\n         }\n \n-        if (incomingBatches[node.batchId].getHeader().getIsLastBatch() ||\n-            incomingBatches[node.batchId].getHeader().getDef().getRecordCount() == 0) {\n+        incomingBatches[node.batchId] = nextBatch;\n+\n+        if (nextBatch == null) {\n           // batch is empty\n-          incomingBatches[node.batchId].release();\n           boolean allBatchesEmpty = true;\n \n           for (RawFragmentBatch batch : incomingBatches) {\n             // see if all batches are empty so we can return OK_* or NONE\n-            if (!batch.getHeader().getIsLastBatch()) {\n+            if (batch != null) {\n               allBatchesEmpty = false;\n               break;\n             }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/4cfdb3b653ba4db664abc14c4b1d51e4cec5c668/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "status": "modified",
                "changes": 15,
                "deletions": 5,
                "sha": "dcfe02f0937cbf76f8702e0355ef55096d45e16a",
                "blob_url": "https://github.com/apache/drill/blob/4cfdb3b653ba4db664abc14c4b1d51e4cec5c668/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java?ref=4cfdb3b653ba4db664abc14c4b1d51e4cec5c668"
            }
        ],
        "bug_id": "drill_33",
        "parent": "https://github.com/apache/drill/commit/0b1df5dfad13ab82ca95287eec2bf1ed960aee87",
        "message": "fix NPE in merging receiver",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/826fc5b9c2a6f044101967a9a2e49b20af2dae76",
        "file": [
            {
                "patch": "@@ -344,7 +344,10 @@ private void setValueCountAndPopulatePartitionVectors(int recordCount) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (Exception e) {\n       logger.warn(\"Failure while closing Hive Record reader.\", e);\n     }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "3c8b9ba419368f569cd463ee825354d859e9a073",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -144,7 +144,10 @@ public int next() {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception while closing stream.\", e);\n     }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "254e0d89b761020aa9a44beb8141df4921d1c59e",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -455,17 +455,22 @@ public void cleanup() {\n     // enable this for debugging when it is know that a whole file will be read\n     // limit kills upstream operators once it has enough records, so this assert will fail\n //    assert totalRecordsRead == footer.getBlocks().get(rowGroupIndex).getRowCount();\n-    for (ColumnReader column : columnStatuses) {\n-      column.clear();\n+    if (columnStatuses != null) {\n+      for (ColumnReader column : columnStatuses) {\n+        column.clear();\n+      }\n+      columnStatuses.clear();\n+      columnStatuses = null;\n     }\n-    columnStatuses.clear();\n \n     codecFactory.close();\n \n-    for (VarLengthColumn r : varLengthReader.columns) {\n-      r.clear();\n+    if (varLengthReader != null) {\n+      for (VarLengthColumn r : varLengthReader.columns) {\n+        r.clear();\n+      }\n+      varLengthReader.columns.clear();\n+      varLengthReader = null;\n     }\n-    varLengthReader.columns.clear();\n   }\n-\n }",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "status": "modified",
                "changes": 19,
                "deletions": 7,
                "sha": "2f07fb3527dbc4e4de25f8af07bba7c1c85d88f8",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -342,7 +342,10 @@ private int getPercentFilled() {\n   @Override\n   public void cleanup() {\n     try {\n-      pageReadStore.close();\n+      if (pageReadStore != null) {\n+        pageReadStore.close();\n+        pageReadStore = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Failure while closing PageReadStore\", e);\n     }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "4e7d6288f19ec48556d2532a4b81827b37754c80",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -231,7 +231,10 @@ public int find(Text text, byte delimiter, int start) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception closing reader: {}\", e);\n     }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "e25bd74084c25188d20b0cfd91a890fa816ede1d",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -59,6 +59,10 @@\n import com.google.common.base.Preconditions;\n import com.google.common.io.Resources;\n \n+import static org.hamcrest.core.StringContains.containsString;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertThat;\n+\n public class BaseTestQuery extends ExecTest {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BaseTestQuery.class);\n \n@@ -319,7 +323,7 @@ protected static void testNoResult(String query, Object... args) throws Exceptio\n \n   protected static void testNoResult(int interation, String query, Object... args) throws Exception {\n     query = String.format(query, args);\n-    logger.debug(\"Running query:\\n--------------\\n\"+query);\n+    logger.debug(\"Running query:\\n--------------\\n\" + query);\n     for (int i = 0; i < interation; i++) {\n       List<QueryDataBatch> results = client.runQuery(QueryType.SQL, query);\n       for (QueryDataBatch queryDataBatch : results) {\n@@ -364,6 +368,24 @@ protected static void testSqlFromFile(String file) throws Exception{\n     test(getFile(file));\n   }\n \n+  /**\n+   * Utility method which tests given query produces a {@link UserException} and the exception message contains\n+   * the given message.\n+   * @param testSqlQuery Test query\n+   * @param expectedErrorMsg Expected error message.\n+   */\n+  protected static void errorMsgTestHelper(final String testSqlQuery, final String expectedErrorMsg) throws Exception {\n+    UserException expException = null;\n+    try {\n+      test(testSqlQuery);\n+    } catch (final UserException ex) {\n+      expException = ex;\n+    }\n+\n+    assertNotNull(\"Expected a UserException\", expException);\n+    assertThat(expException.getMessage(), containsString(expectedErrorMsg));\n+  }\n+\n   public static String getFile(String resource) throws IOException{\n     URL url = Resources.getResource(resource);\n     if (url == null) {",
                "additions": 23,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "status": "modified",
                "changes": 24,
                "deletions": 1,
                "sha": "db1ed348a7edc7c1ee3b92491e34dbce7047d27a",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            },
            {
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.drill;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.util.FileUtils;\n+import org.apache.drill.exec.work.ExecErrorConstants;\n import org.apache.drill.exec.work.foreman.SqlUnsupportedException;\n import org.apache.drill.exec.work.foreman.UnsupportedDataTypeException;\n import org.apache.drill.exec.work.foreman.UnsupportedFunctionException;\n@@ -355,13 +356,15 @@ public void testFlattenWithinDistinct() throws Exception {\n     }\n   }\n \n-  @Test(expected =  UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalCasts() throws Exception {\n-    test(\"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\");\n+    final String query = \"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n \n-  @Test(expected = UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalFromParquet() throws Exception {\n-    test(\"select * from cp.`parquet/decimal_dictionary.parquet`\");\n+    final String query = \"select * from cp.`parquet/decimal_dictionary.parquet`\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n }\n\\ No newline at end of file",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "status": "modified",
                "changes": 11,
                "deletions": 4,
                "sha": "adbf653efdd5fe04489460ac3634e1352ad3bb00",
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76"
            }
        ],
        "bug_id": "drill_34",
        "parent": "https://github.com/apache/drill/commit/6076cc6435f7decb17856991c25dac77dfe0b203",
        "message": "DRILL-3017: Safeguard against NPEs in RecordReader.cleanup()s",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/8540450e179aa03f86ea204bdf1a1c064a90a46e",
        "file": [
            {
                "patch": "@@ -44,7 +44,7 @@ public V put(K key, V value){\n     Preconditions.checkNotNull(key);\n     Preconditions.checkNotNull(value);\n     HV oldValue = hzMap.put(getNewKey(key), getNewValue(value));\n-    return oldValue.get();\n+    return oldValue == null ? null : oldValue.get();\n   }\n   \n   public abstract HK getNewKey(K key);",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/8540450e179aa03f86ea204bdf1a1c064a90a46e/exec/java-exec/src/main/java/org/apache/drill/exec/cache/ProtoMap.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "1de1c4ee04b30c5a59332f0af0ab2ab31c6f9523",
                "blob_url": "https://github.com/apache/drill/blob/8540450e179aa03f86ea204bdf1a1c064a90a46e/exec/java-exec/src/main/java/org/apache/drill/exec/cache/ProtoMap.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/cache/ProtoMap.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/cache/ProtoMap.java?ref=8540450e179aa03f86ea204bdf1a1c064a90a46e"
            }
        ],
        "bug_id": "drill_35",
        "parent": "https://github.com/apache/drill/commit/4ccea48905cca4a822521c83ad2ff31eb62bfa06",
        "message": "DRILL-245: NPE in ProtoMap (hazel cache)",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
        "file": [
            {
                "patch": "@@ -807,6 +807,8 @@ private ExecConstants() {\n    */\n   public static final String ENABLE_ITERATOR_VALIDATION = \"drill.exec.debug.validate_iterators\";\n \n+  public static final String QUERY_ROWKEYJOIN_BATCHSIZE_KEY = \"exec.query.rowkeyjoin_batchsize\";\n+  public static final PositiveLongValidator QUERY_ROWKEYJOIN_BATCHSIZE = new PositiveLongValidator(QUERY_ROWKEYJOIN_BATCHSIZE_KEY, Long.MAX_VALUE, null);\n   /**\n    * When iterator validation is enabled, additionally validates the vectors in\n    * each batch passed to each iterator.",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "status": "modified",
                "changes": 2,
                "deletions": 0,
                "sha": "cb0fc5cf2cb3e0deec6335500a917d8a53e10099",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import java.util.List;\n+\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.common.logical.StoragePluginConfig;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.drill.exec.planner.index.IndexCollection;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n+import org.apache.drill.exec.store.AbstractStoragePlugin;\n+\n+public abstract class AbstractDbGroupScan extends AbstractGroupScan implements DbGroupScan {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractDbGroupScan.class);\n+\n+  private static final String ROW_KEY = \"_id\";\n+  private static final SchemaPath ROW_KEY_PATH = SchemaPath.getSimplePath(ROW_KEY);\n+\n+  public AbstractDbGroupScan(String userName) {\n+    super(userName);\n+  }\n+\n+  public AbstractDbGroupScan(AbstractDbGroupScan that) {\n+    super(that);\n+  }\n+\n+  public abstract AbstractStoragePlugin getStoragePlugin();\n+\n+  public abstract StoragePluginConfig getStorageConfig();\n+\n+  public abstract List<SchemaPath> getColumns();\n+\n+  @Override\n+  public boolean supportsSecondaryIndex() {\n+    return false;\n+  }\n+\n+  @Override\n+  public IndexCollection getSecondaryIndexCollection(RelNode scanrel) {\n+    return null;\n+  }\n+\n+  @Override\n+  public boolean supportsRestrictedScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isRestrictedScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public DbGroupScan getRestrictedScan(List<SchemaPath> columns) {\n+    return null;\n+  }\n+\n+  @Override\n+  public String getRowKeyName() {\n+    return ROW_KEY;\n+  }\n+\n+  @Override\n+  public SchemaPath getRowKeyPath() {\n+    return ROW_KEY_PATH;\n+  }\n+\n+  @Override\n+  public PartitionFunction getRangePartitionFunction(List<FieldReference> refList) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public PluginCost getPluginCostModel() {\n+    return null;\n+  }\n+}",
                "additions": 95,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "status": "added",
                "changes": 95,
                "deletions": 0,
                "sha": "42e4bb9ff052e103f40f9bd0e90a9ebcd235cf8f",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.drill.exec.physical.impl.join.RowKeyJoin;\n+\n+public abstract class AbstractDbSubScan extends AbstractSubScan implements DbSubScan {\n+\n+  public AbstractDbSubScan(String userName) {\n+    super(userName);\n+  }\n+\n+  public boolean isRestrictedSubScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public void addJoinForRestrictedSubScan(RowKeyJoin batch) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+}",
                "additions": 37,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "status": "added",
                "changes": 37,
                "deletions": 0,
                "sha": "caa583162eaf5db0e516c850a5e7f85584fef1e3",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -22,13 +22,15 @@\n import org.apache.drill.exec.physical.config.FlattenPOP;\n import org.apache.drill.exec.physical.config.HashAggregate;\n import org.apache.drill.exec.physical.config.HashPartitionSender;\n+import org.apache.drill.exec.physical.config.HashToRandomExchange;\n import org.apache.drill.exec.physical.config.IteratorValidator;\n import org.apache.drill.exec.physical.config.LateralJoinPOP;\n import org.apache.drill.exec.physical.config.Limit;\n import org.apache.drill.exec.physical.config.MergingReceiverPOP;\n import org.apache.drill.exec.physical.config.OrderedPartitionSender;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.config.Project;\n+import org.apache.drill.exec.physical.config.RangePartitionSender;\n import org.apache.drill.exec.physical.config.Screen;\n import org.apache.drill.exec.physical.config.SingleSender;\n import org.apache.drill.exec.physical.config.Sort;\n@@ -156,6 +158,16 @@ public T visitMergingReceiver(MergingReceiverPOP op, X value) throws E {\n     return visitReceiver(op, value);\n   }\n \n+  @Override\n+  public T visitHashPartitionSender(HashToRandomExchange op, X value) throws E {\n+    return visitExchange(op, value);\n+  }\n+\n+  @Override\n+  public T visitRangePartitionSender(RangePartitionSender op, X value) throws E {\n+    return visitSender(op, value);\n+  }\n+\n   @Override\n   public T visitBroadcastSender(BroadcastSender op, X value) throws E {\n     return visitSender(op, value);",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "status": "modified",
                "changes": 12,
                "deletions": 0,
                "sha": "ca82ca621a87822f6ad02becd82a1bbf47ac2cce",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.index.IndexCollection;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n+import org.apache.drill.exec.planner.index.Statistics;\n+\n+import java.util.List;\n+\n+/**\n+ * A DbGroupScan operator represents the scan associated with a database. The underlying\n+ * database may support secondary indexes, so there are interface methods for indexes.\n+ */\n+public interface DbGroupScan extends GroupScan {\n+\n+\n+  @JsonIgnore\n+  public boolean supportsSecondaryIndex();\n+\n+  /**\n+   * Get the index collection associated with this table if any\n+   */\n+  @JsonIgnore\n+  public IndexCollection getSecondaryIndexCollection(RelNode scan);\n+\n+  /**\n+   * Set the artificial row count after applying the {@link RexNode} condition\n+   * @param condition\n+   * @param count\n+   * @param capRowCount\n+   */\n+  @JsonIgnore\n+  public void setRowCount(RexNode condition, double count, double capRowCount);\n+\n+  /**\n+   * Get the row count after applying the {@link RexNode} condition\n+   * @param condition, filter to apply\n+   * @param scanRel, the current scan rel\n+   * @return row count post filtering\n+   */\n+  @JsonIgnore\n+  public double getRowCount(RexNode condition, RelNode scanRel);\n+\n+  /**\n+   * Get the statistics for this {@link DbGroupScan}\n+   * @return the {@link Statistics} for this Scan\n+   */\n+  @JsonIgnore\n+  public Statistics getStatistics();\n+\n+  public List<SchemaPath> getColumns();\n+\n+  public void setCostFactor(double sel);\n+\n+  @JsonIgnore\n+  boolean isIndexScan();\n+\n+  /**\n+   * Whether this DbGroupScan supports creating a restricted (skip) scan\n+   * @return true if restricted scan is supported, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean supportsRestrictedScan();\n+\n+  /**\n+   * Whether this DbGroupScan is itself a restricted scan\n+   * @return true if this DbGroupScan is itself a restricted scan, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean isRestrictedScan();\n+\n+  /**\n+   * If this DbGroupScan supports restricted scan, create a restricted scan from this DbGroupScan.\n+   * @param columns\n+   * @return a non-null DbGroupScan if restricted scan is supported, null otherwise\n+   */\n+  @JsonIgnore\n+  DbGroupScan getRestrictedScan(List<SchemaPath> columns);\n+\n+  @JsonIgnore\n+  String getRowKeyName();\n+\n+  @JsonIgnore\n+  String getIndexHint();\n+\n+  @JsonIgnore\n+  SchemaPath getRowKeyPath();\n+\n+  /**\n+   * Get a partition function instance for range based partitioning\n+   * @param refList a list of FieldReference exprs that are participating in the range partitioning\n+   * @return instance of a partitioning function\n+   */\n+  @JsonIgnore\n+  PartitionFunction getRangePartitionFunction(List<FieldReference> refList);\n+\n+  /**\n+   * Get the format plugin cost model. The cost model will provide cost factors such as seq. scan cost,\n+   * random scan cost, block size.\n+   * @return a PluginCost cost model\n+   */\n+  @JsonIgnore\n+  PluginCost getPluginCostModel();\n+\n+  @JsonIgnore\n+  boolean isFilterPushedDown();\n+}",
                "additions": 129,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "status": "added",
                "changes": 129,
                "deletions": 0,
                "sha": "e16fba1ff47f54956f033ab77feb275a22dd4a7f",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.drill.exec.physical.impl.join.RowKeyJoin;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+\n+\n+public interface DbSubScan extends SubScan {\n+\n+  /**\n+   * Whether this subscan is a restricted (skip) subscan\n+   * @return true if this subscan is a restricted subscan, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean isRestrictedSubScan();\n+\n+  /**\n+   * For a restricted sub-scan, this method allows associating a (hash)join instance.  A subscan within a minor\n+   * fragment must have a corresponding (hash)join batch instance from which it will retrieve its set of\n+   * rowkeys to perform the restricted scan.\n+   * @param batch\n+   */\n+  @JsonIgnore\n+  void addJoinForRestrictedSubScan(RowKeyJoin batch);\n+\n+}",
                "additions": 43,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "status": "added",
                "changes": 43,
                "deletions": 0,
                "sha": "874468d4e6851e1f46c87e41073dbcbafad494dc",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.index.Statistics;\n+\n+\n+import java.util.List;\n+\n+/**\n+ * An IndexGroupScan operator represents the scan associated with an Index.\n+ */\n+public interface IndexGroupScan extends GroupScan {\n+\n+  /**\n+   * Get the column ordinal of the rowkey column from the output schema of the IndexGroupScan\n+   * @return\n+   */\n+  @JsonIgnore\n+  public int getRowKeyOrdinal();\n+\n+  /**\n+   * Set the artificial row count after applying the {@link RexNode} condition\n+   * Mainly used for debugging\n+   * @param condition\n+   * @param count\n+   * @param capRowCount\n+   */\n+  @JsonIgnore\n+  public void setRowCount(RexNode condition, double count, double capRowCount);\n+\n+  /**\n+   * Get the row count after applying the {@link RexNode} condition\n+   * @param condition, filter to apply\n+   * @return row count post filtering\n+   */\n+  @JsonIgnore\n+  public double getRowCount(RexNode condition, RelNode scanRel);\n+\n+  /**\n+   * Set the statistics for {@link IndexGroupScan}\n+   * @param statistics\n+   */\n+  @JsonIgnore\n+  public void setStatistics(Statistics statistics);\n+\n+  @JsonIgnore\n+  public void setColumns(List<SchemaPath> columns);\n+\n+  @JsonIgnore\n+  public List<SchemaPath> getColumns();\n+\n+  @JsonIgnore\n+  public void setParallelizationWidth(int width);\n+\n+}\n\\ No newline at end of file",
                "additions": 76,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "status": "added",
                "changes": 76,
                "deletions": 0,
                "sha": "1047e829d0efa314e0ab8027c062cc201058fb7a",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -22,13 +22,15 @@\n import org.apache.drill.exec.physical.config.FlattenPOP;\n import org.apache.drill.exec.physical.config.HashAggregate;\n import org.apache.drill.exec.physical.config.HashPartitionSender;\n+import org.apache.drill.exec.physical.config.HashToRandomExchange;\n import org.apache.drill.exec.physical.config.IteratorValidator;\n import org.apache.drill.exec.physical.config.LateralJoinPOP;\n import org.apache.drill.exec.physical.config.Limit;\n import org.apache.drill.exec.physical.config.MergingReceiverPOP;\n import org.apache.drill.exec.physical.config.OrderedPartitionSender;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.config.Project;\n+import org.apache.drill.exec.physical.config.RangePartitionSender;\n import org.apache.drill.exec.physical.config.Screen;\n import org.apache.drill.exec.physical.config.SingleSender;\n import org.apache.drill.exec.physical.config.Sort;\n@@ -73,6 +75,8 @@\n   public RETURN visitOrderedPartitionSender(OrderedPartitionSender op, EXTRA value) throws EXCEP;\n   public RETURN visitUnorderedReceiver(UnorderedReceiver op, EXTRA value) throws EXCEP;\n   public RETURN visitMergingReceiver(MergingReceiverPOP op, EXTRA value) throws EXCEP;\n+  public RETURN visitHashPartitionSender(HashToRandomExchange op, EXTRA value) throws EXCEP;\n+  public RETURN visitRangePartitionSender(RangePartitionSender op, EXTRA value) throws EXCEP;\n   public RETURN visitBroadcastSender(BroadcastSender op, EXTRA value) throws EXCEP;\n   public RETURN visitScreen(Screen op, EXTRA value) throws EXCEP;\n   public RETURN visitSingleSender(SingleSender op, EXTRA value) throws EXCEP;",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "1bb1545c487a113b3efd6ce90f3cbcbed4628fcc",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -17,57 +17,57 @@\n  */\n package org.apache.drill.exec.physical.config;\n \n-import java.util.Collections;\n import java.util.List;\n \n import org.apache.drill.exec.physical.MinorFragmentEndpoint;\n import org.apache.drill.exec.physical.base.AbstractSender;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n-import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n+import org.apache.drill.exec.physical.base.PhysicalVisitor;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n \n-@JsonTypeName(\"range-sender\")\n-public class RangeSender extends AbstractSender{\n+@JsonTypeName(\"range-partition-sender\")\n+public class RangePartitionSender extends AbstractSender{\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RangeSender.class);\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RangePartitionSender.class);\n \n-  List<EndpointPartition> partitions;\n+  // The number of records in the outgoing batch. This is overriding the default value in Partitioner\n+  public static final int RANGE_PARTITION_OUTGOING_BATCH_SIZE = (1 << 12) - 1;\n+\n+  @JsonProperty(\"partitionFunction\")\n+  private PartitionFunction partitionFunction;\n \n   @JsonCreator\n-  public RangeSender(@JsonProperty(\"receiver-major-fragment\") int oppositeMajorFragmentId, @JsonProperty(\"child\") PhysicalOperator child, @JsonProperty(\"partitions\") List<EndpointPartition> partitions) {\n-    super(oppositeMajorFragmentId, child, Collections.<MinorFragmentEndpoint>emptyList());\n-    this.partitions = partitions;\n+  public RangePartitionSender(@JsonProperty(\"receiver-major-fragment\") int oppositeMajorFragmentId,\n+                              @JsonProperty(\"child\") PhysicalOperator child,\n+                              @JsonProperty(\"destinations\") List<MinorFragmentEndpoint> endpoints,\n+                              @JsonProperty(\"partitionFunction\") PartitionFunction partitionFunction) {\n+    super(oppositeMajorFragmentId, child, endpoints);\n+    this.partitionFunction = partitionFunction;\n   }\n \n   @Override\n   protected PhysicalOperator getNewWithChild(PhysicalOperator child) {\n-    return new RangeSender(oppositeMajorFragmentId, child, partitions);\n+    return new RangePartitionSender(oppositeMajorFragmentId, child, destinations, partitionFunction);\n   }\n \n-  public static class EndpointPartition{\n-    private final PartitionRange range;\n-    private final DrillbitEndpoint endpoint;\n+  @JsonProperty(\"partitionFunction\")\n+  public PartitionFunction getPartitionFunction() {\n+    return partitionFunction;\n+  }\n \n-    @JsonCreator\n-    public EndpointPartition(@JsonProperty(\"range\") PartitionRange range, @JsonProperty(\"endpoint\") DrillbitEndpoint endpoint) {\n-      super();\n-      this.range = range;\n-      this.endpoint = endpoint;\n-    }\n-    public PartitionRange getRange() {\n-      return range;\n-    }\n-    public DrillbitEndpoint getEndpoint() {\n-      return endpoint;\n-    }\n+  @Override\n+  public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {\n+    return physicalVisitor.visitRangePartitionSender(this, value);\n   }\n \n   @Override\n   public int getOperatorType() {\n-    return CoreOperatorType.RANGE_SENDER_VALUE;\n+    return CoreOperatorType.RANGE_PARTITION_SENDER_VALUE;\n   }\n+\n }",
                "additions": 26,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "previous_filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangeSender.java",
                "status": "renamed",
                "changes": 52,
                "deletions": 26,
                "sha": "0c0852a074f48a44c1056d67f49af818c78a5ba1",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -83,10 +83,14 @@\n   private final List<Map<String, String>> implicitColumnList;\n   private String currentReaderClassName;\n   private final RecordBatchStatsContext batchStatsContext;\n+\n   // Represents last outcome of next(). If an Exception is thrown\n   // during the method's execution a value IterOutcome.STOP will be assigned.\n   private IterOutcome lastOutcome;\n \n+  private List<RecordReader> readerList = null; // needed for repeatable scanners\n+  private boolean isRepeatableScan = false;     // needed for repeatable scanners\n+\n   /**\n    *\n    * @param context\n@@ -137,6 +141,15 @@ public ScanBatch(PhysicalOperator subScanConfig, FragmentContext context,\n         readers, Collections.<Map<String, String>> emptyList());\n   }\n \n+  public ScanBatch(PhysicalOperator subScanConfig, FragmentContext context,\n+                   List<RecordReader> readerList, boolean isRepeatableScan)\n+      throws ExecutionSetupException {\n+    this(context, context.newOperatorContext(subScanConfig),\n+        readerList, Collections.<Map<String, String>> emptyList());\n+    this.readerList = readerList;\n+    this.isRepeatableScan = isRepeatableScan;\n+  }\n+\n   @Override\n   public FragmentContext getContext() {\n     return context;\n@@ -255,7 +268,7 @@ private boolean getNextReaderIfHas() throws ExecutionSetupException {\n       return false;\n     }\n     currentReader = readers.next();\n-    if (readers.hasNext()) {\n+    if (!isRepeatableScan && readers.hasNext()) {\n       readers.remove();\n     }\n     implicitValues = implicitColumns.hasNext() ? implicitColumns.next() : null;",
                "additions": 14,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "status": "modified",
                "changes": 15,
                "deletions": 1,
                "sha": "5ccf1c093207e9c5eb869de56b981eab53fe213a",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.impl.join;\n+\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.drill.exec.record.AbstractRecordBatch.BatchState;\n+import org.apache.drill.exec.vector.ValueVector;\n+\n+/**\n+ * Interface for a row key join\n+ */\n+public interface RowKeyJoin {\n+\n+  /**\n+   * Enum for RowKeyJoin internal state.\n+   * Possible states are {INITIAL, PROCESSING, DONE}\n+   *\n+   * Initially RowKeyJoin will be at INITIAL state. Then the state will be transitioned\n+   * by the RestrictedJsonRecordReader to PROCESSING as soon as it processes the rows\n+   * related to RowKeys. Then RowKeyJoin algorithm sets to INITIAL state when leftStream has no data.\n+   * Basically RowKeyJoin calls leftStream multiple times depending upon the rightStream, hence\n+   * this transition from PROCESSING to INITIAL. If there is no data from rightStream or OutOfMemory\n+   * condition then the state is transitioned to DONE.\n+   */\n+  public enum RowKeyJoinState {\n+    INITIAL, PROCESSING, DONE;\n+  }\n+\n+  /**\n+   * Is the next batch of row keys ready to be returned\n+   * @return True if ready, false if not\n+   */\n+  public boolean hasRowKeyBatch();\n+\n+  /**\n+   * Get the next batch of row keys\n+   * @return a Pair whose left element is the ValueVector containing the row keys, right\n+   *    element is the number of row keys in this batch\n+   */\n+  public Pair<ValueVector, Integer> nextRowKeyBatch();\n+\n+\n+  /**\n+   * Get the current BatchState (this is useful when performing row key join)\n+   */\n+  public BatchState getBatchState();\n+\n+  /**\n+   * Set the BatchState (this is useful when performing row key join)\n+   * @param newState\n+   */\n+  public void setBatchState(BatchState newState);\n+\n+  /**\n+   * Set the RowKeyJoinState (this is useful for maintaining state for row key join algorithm)\n+   * @param newState\n+   */\n+  public void setRowKeyJoinState(RowKeyJoinState newState);\n+\n+  /**\n+   * Get the current RowKeyJoinState.\n+   */\n+  public RowKeyJoinState getRowKeyJoinState();\n+}",
                "additions": 79,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "status": "added",
                "changes": 79,
                "deletions": 0,
                "sha": "7b4dfcaaaaeddadd5a99e344fd047ea252a7fb41",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -396,4 +396,21 @@ public RexNode go(RexNode rex) {\n       }\n     }\n   }\n+\n+  public static boolean isProjectFlatten(RelNode project) {\n+\n+    assert project instanceof Project : \"Rel is NOT an instance of project!\";\n+\n+    for (RexNode rex : project.getChildExps()) {\n+      RexNode newExpr = rex;\n+      if (rex instanceof RexCall) {\n+        RexCall function = (RexCall) rex;\n+        String functionName = function.getOperator().getName();\n+        if (functionName.equalsIgnoreCase(\"flatten\") ) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n }",
                "additions": 17,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "status": "modified",
                "changes": 17,
                "deletions": 0,
                "sha": "b39328ea05fbcc7a4b3b4a43ad407d81f3564de8",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,79 @@\n+package org.apache.drill.exec.planner.cost;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+\n+import org.apache.drill.exec.physical.base.GroupScan;\n+\n+/**\n+ * PluginCost describes the cost factors to be used when costing for the specific storage/format plugin\n+ */\n+public interface PluginCost {\n+  org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(PluginCost.class);\n+\n+  /**\n+   * An interface to check if a parameter provided by user is valid or not.\n+   * @param <T> Type of the parameter.\n+   */\n+  interface CheckValid<T> {\n+    boolean isValid(T paramValue);\n+  }\n+\n+  /**\n+   * Class which checks whether the provided parameter value is greater than\n+   * or equals to a minimum limit.\n+   */\n+  class greaterThanEquals implements CheckValid<Integer> {\n+    private final Integer atleastEqualsTo;\n+    public greaterThanEquals(Integer atleast) {\n+      atleastEqualsTo = atleast;\n+    }\n+\n+    @Override\n+    public boolean isValid(Integer paramValue) {\n+      if (paramValue >= atleastEqualsTo &&\n+          paramValue <= Integer.MAX_VALUE) {\n+        return true;\n+      } else {\n+        logger.warn(\"Setting default value as the supplied parameter value is less than {}\", paramValue);\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * @return the average column size in bytes\n+   */\n+  int getAverageColumnSize(GroupScan scan);\n+\n+  /**\n+   * @return the block size in bytes\n+   */\n+  int getBlockSize(GroupScan scan);\n+\n+  /**\n+   * @return the sequential block read cost\n+   */\n+  int getSequentialBlockReadCost(GroupScan scan);\n+\n+  /**\n+   * @return the random block read cost\n+   */\n+  int getRandomBlockReadCost(GroupScan scan);\n+}",
                "additions": 79,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "status": "added",
                "changes": 79,
                "deletions": 0,
                "sha": "d765162fb62e17a4087245242dc15922ec4e8e96",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+/**\n+ * Abstract base class for Index collection (collection of Index descriptors)\n+ *\n+ */\n+public abstract class AbstractIndexCollection implements IndexCollection, Iterable<IndexDescriptor> {\n+\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractIndexCollection.class);\n+  /**\n+   * A set of indexes for a particular table\n+   */\n+  @JsonProperty\n+  protected List<IndexDescriptor> indexes;\n+\n+  public AbstractIndexCollection() {\n+    indexes = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public boolean addIndex(IndexDescriptor index) {\n+    return indexes.add(index);\n+  }\n+\n+  @Override\n+  public boolean removeIndex(IndexDescriptor index) {\n+    return indexes.remove(index);\n+  }\n+\n+  @Override\n+  public void clearAll() {\n+    indexes.clear();\n+  }\n+\n+  @Override\n+  public boolean supportsIndexSelection() {\n+    return false;\n+  }\n+\n+  @Override\n+  public double getRows(RexNode indexCondition) {\n+    throw new UnsupportedOperationException(\"getRows() not supported for this index collection.\");\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isColumnIndexed(SchemaPath path) {\n+    for (IndexDescriptor index : indexes) {\n+      if (index.getIndexColumnOrdinal(path) >= 0) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Iterator<IndexDescriptor> iterator() {\n+    return indexes.iterator();\n+  }\n+\n+}",
                "additions": 96,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "status": "added",
                "changes": 96,
                "deletions": 0,
                "sha": "9894b326322f6781dbc22743a5af7fbfe8b7448f",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import java.util.List;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+\n+/**\n+ * Abstract base class for an Index descriptor\n+ *\n+ */\n+public abstract class AbstractIndexDescriptor extends DrillIndexDefinition implements IndexDescriptor {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractIndexDescriptor .class);\n+\n+  public AbstractIndexDescriptor(List<LogicalExpression> indexCols,\n+                                 CollationContext indexCollationContext,\n+                                 List<LogicalExpression> nonIndexCols,\n+                                 List<LogicalExpression> rowKeyColumns,\n+                                 String indexName,\n+                                 String tableName,\n+                                 IndexType type,\n+                                 NullDirection nullsDirection) {\n+    super(indexCols, indexCollationContext, nonIndexCols, rowKeyColumns, indexName, tableName, type, nullsDirection);\n+  }\n+\n+  @Override\n+  public double getRows(RelNode scan, RexNode indexCondition) {\n+    throw new UnsupportedOperationException(\"getRows() not supported for this index.\");\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return false;\n+  }\n+\n+  @Override\n+  public IndexGroupScan getIndexGroupScan() {\n+    throw new UnsupportedOperationException(\"Group scan not supported for this index.\");\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return false;\n+  }\n+\n+  @Override\n+  public RelOptCost getCost(IndexProperties indexProps, RelOptPlanner planner,\n+      int numProjectedFields, GroupScan primaryGroupScan) {\n+    throw new UnsupportedOperationException(\"getCost() not supported for this index.\");\n+  }\n+}",
                "additions": 74,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "status": "added",
                "changes": 74,
                "deletions": 0,
                "sha": "f908ead4c90960be368d34b03902c0b6548c719e",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+\n+public abstract class AbstractIndexStatistics implements IndexStatistics {\n+\n+    protected static final Logger logger = LoggerFactory.getLogger(AbstractIndexStatistics.class);\n+    protected final RelNode input;\n+    protected final RexNode condition;\n+    protected final DrillTable table;\n+\n+    public AbstractIndexStatistics(RelNode input, RexNode condition, DrillTable table) {\n+            this.input = input;\n+            this.condition = condition;\n+            this.table = table;\n+    }\n+    public abstract double getRowCount();\n+\n+    public List<RelCollation> getCollations() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public RelDistribution getDistribution() {\n+        throw new UnsupportedOperationException();\n+    }\n+}",
                "additions": 51,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "status": "added",
                "changes": 51,
                "deletions": 0,
                "sha": "dfc0897ee2887e95faf86dac5f79a44edeac4619",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.drill.common.expression.LogicalExpression;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class CollationContext {\n+\n+  public final Map<LogicalExpression, RelFieldCollation> collationMap;\n+  public final List<RelFieldCollation> relFieldCollations;\n+\n+  public CollationContext(Map<LogicalExpression, RelFieldCollation> collationMap,\n+      List<RelFieldCollation> relFieldCollations) {\n+    this.collationMap = collationMap;\n+    this.relFieldCollations = relFieldCollations;\n+  }\n+\n+}",
                "additions": 37,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "status": "added",
                "changes": 37,
                "deletions": 0,
                "sha": "8260beea4bc2acc6b7d3b0a7ac985b7c1c66cdc9",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.Set;\n+\n+public class DrillIndexCollection extends AbstractIndexCollection {\n+  private final RelNode scan;  // physical scan rel corresponding to the primary table\n+\n+  public DrillIndexCollection(RelNode scanRel,\n+                               Set<DrillIndexDescriptor> indexes) {\n+    this.scan = scanRel;\n+    for (IndexDescriptor index : indexes) {\n+      super.addIndex(index);\n+    }\n+  }\n+\n+  private IndexDescriptor getIndexDescriptor() {\n+\n+    //XXX need a policy to pick the indexDesc to use instead of picking the first one.\n+    return this.indexes.iterator().next();\n+  }\n+\n+  @Override\n+  public boolean supportsIndexSelection() {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return true;\n+  }\n+\n+  @Override\n+  public double getRows(RexNode indexCondition) {\n+\n+    return getIndexDescriptor().getRows(scan, indexCondition);\n+  }\n+\n+  @Override\n+  public IndexGroupScan getGroupScan() {\n+    return getIndexDescriptor().getIndexGroupScan();\n+  }\n+\n+  @Override\n+  public IndexCollectionType getIndexCollectionType() {\n+    return IndexCollection.IndexCollectionType.EXTERNAL_SECONDARY_INDEX_COLLECTION;\n+  }\n+\n+}",
                "additions": 75,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "status": "added",
                "changes": 75,
                "deletions": 0,
                "sha": "0ea3d83e7db385638a20ab624b90ca76742a93c0",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelCollations;\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.drill.common.expression.CastExpression;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class DrillIndexDefinition implements IndexDefinition {\n+  /**\n+   * The indexColumns is the list of column(s) on which this index is created. If there is more than 1 column,\n+   * the order of the columns is important: index on {a, b} is not the same as index on {b, a}\n+   * NOTE: the indexed column could be of type columnfamily.column\n+   */\n+  @JsonProperty\n+  protected final List<LogicalExpression> indexColumns;\n+\n+  /**\n+   * nonIndexColumns: the list of columns that are included in the index as 'covering'\n+   * columns but are not themselves indexed.  These are useful for covering indexes where the\n+   * query request can be satisfied directly by the index and avoid accessing the table altogether.\n+   */\n+  @JsonProperty\n+  protected final List<LogicalExpression> nonIndexColumns;\n+\n+  @JsonIgnore\n+  protected final Set<LogicalExpression> allIndexColumns;\n+\n+  @JsonProperty\n+  protected final List<LogicalExpression> rowKeyColumns;\n+\n+  @JsonProperty\n+  protected final CollationContext indexCollationContext;\n+\n+  /**\n+   * indexName: name of the index that should be unique within the scope of a table\n+   */\n+  @JsonProperty\n+  protected final String indexName;\n+\n+  protected final String tableName;\n+\n+  @JsonProperty\n+  protected final IndexDescriptor.IndexType indexType;\n+\n+  @JsonProperty\n+  protected final NullDirection nullsDirection;\n+\n+  public DrillIndexDefinition(List<LogicalExpression> indexCols,\n+                              CollationContext indexCollationContext,\n+                              List<LogicalExpression> nonIndexCols,\n+                              List<LogicalExpression> rowKeyColumns,\n+                              String indexName,\n+                              String tableName,\n+                              IndexType type,\n+                              NullDirection nullsDirection) {\n+    this.indexColumns = indexCols;\n+    this.nonIndexColumns = nonIndexCols;\n+    this.rowKeyColumns = rowKeyColumns;\n+    this.indexName = indexName;\n+    this.tableName = tableName;\n+    this.indexType = type;\n+    this.allIndexColumns = Sets.newHashSet(indexColumns);\n+    this.allIndexColumns.addAll(nonIndexColumns);\n+    this.indexCollationContext = indexCollationContext;\n+    this.nullsDirection = nullsDirection;\n+\n+  }\n+\n+  @Override\n+  public int getIndexColumnOrdinal(LogicalExpression path) {\n+    int id = indexColumns.indexOf(path);\n+    return id;\n+  }\n+\n+  @Override\n+  public boolean isCoveringIndex(List<LogicalExpression> columns) {\n+    return allIndexColumns.containsAll(columns);\n+  }\n+\n+  @Override\n+  public boolean allColumnsIndexed(Collection<LogicalExpression> columns) {\n+    return columnsInIndexFields(columns, indexColumns);\n+  }\n+\n+  @Override\n+  public boolean someColumnsIndexed(Collection<LogicalExpression> columns) {\n+    return someColumnsInIndexFields(columns, indexColumns);\n+  }\n+\n+  public boolean pathExactIn(SchemaPath path, Collection<LogicalExpression> exprs) {\n+    for (LogicalExpression expr : exprs) {\n+      if (expr instanceof SchemaPath) {\n+        if (((SchemaPath) expr).toExpr().equals(path.toExpr())) {\n+          return true;\n+        }\n+      }\n+    }\n+\n+    return false;\n+  }\n+\n+  boolean castIsCompatible(CastExpression castExpr, Collection<LogicalExpression> indexFields) {\n+    for(LogicalExpression indexExpr : indexFields) {\n+      if(indexExpr.getClass() != castExpr.getClass()) {\n+        continue;\n+      }\n+      CastExpression indexCastExpr = (CastExpression)indexExpr;\n+      //we compare input using equals because we know we are comparing SchemaPath,\n+      //if we extend to support other expression, make sure the equals of that expression\n+      //is implemented properly, otherwise it will fall to identity comparison\n+      if ( !castExpr.getInput().equals(indexCastExpr.getInput()) ) {\n+          continue;\n+      }\n+\n+      if( castExpr.getMajorType().getMinorType() != indexCastExpr.getMajorType().getMinorType()) {\n+        continue;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  protected boolean columnsInIndexFields(Collection<LogicalExpression> columns, Collection<LogicalExpression> indexFields) {\n+    //we need to do extra check, so we could allow the case when query condition expression is not identical with indexed fields\n+    //and they still could use the index either by implicit cast or the difference is allowed, e.g. width of varchar\n+    for (LogicalExpression col : columns) {\n+      if (col instanceof CastExpression) {\n+        if (!castIsCompatible((CastExpression) col, indexFields)) {\n+          return false;\n+        }\n+      }\n+      else {\n+        if (!pathExactIn((SchemaPath)col, indexFields)) {\n+          return false;\n+        }\n+      }\n+    }\n+    return true;//indexFields.containsAll(columns);\n+  }\n+\n+  protected boolean someColumnsInIndexFields(Collection<LogicalExpression> columns,\n+      Collection<LogicalExpression> indexFields) {\n+\n+    //we need to do extra check, so we could allow the case when query condition expression is not identical with indexed fields\n+    //and they still could use the index either by implicit cast or the difference is allowed, e.g. width of varchar\n+    for (LogicalExpression col : columns) {\n+      if (col instanceof CastExpression) {\n+        if (castIsCompatible((CastExpression) col, indexFields)) {\n+          return true;\n+        }\n+      }\n+      else {\n+        if (pathExactIn((SchemaPath)col, indexFields)) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String columnsDesc = \" Index columns: \" + indexColumns.toString() + \" Non-Index columns: \" + nonIndexColumns.toString();\n+    String desc = \"Table: \" + tableName + \" Index: \" + indexName + columnsDesc;\n+    return desc;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null) {\n+      return false;\n+    }\n+    DrillIndexDefinition index1 = (DrillIndexDefinition) o;\n+    return tableName.equals(index1.tableName)\n+        && indexName.equals(index1.indexName)\n+        && indexType.equals(index1.indexType)\n+        && indexColumns.equals(index1.indexColumns);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    final int prime = 31;\n+    final String fullName = tableName + indexName;\n+    int result = 1;\n+    result = prime * result + fullName.hashCode();\n+    result = prime * result + indexType.hashCode();\n+\n+    return result;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public String getIndexName() {\n+    return indexName;\n+  }\n+\n+  @Override\n+  public String getTableName() {\n+    return tableName;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public IndexDescriptor.IndexType getIndexType() {\n+    return indexType;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getRowKeyColumns() {\n+    return this.rowKeyColumns;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getIndexColumns() {\n+    return this.indexColumns;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getNonIndexColumns() {\n+    return this.nonIndexColumns;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public RelCollation getCollation() {\n+    if (indexCollationContext != null) {\n+      return RelCollations.of(indexCollationContext.relFieldCollations);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public Map<LogicalExpression, RelFieldCollation> getCollationMap() {\n+    return indexCollationContext.collationMap;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public NullDirection getNullsOrderingDirection() {\n+    return nullsDirection;\n+  }\n+}",
                "additions": 278,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "status": "added",
                "changes": 278,
                "deletions": 0,
                "sha": "03c2a44c68ff66a8bdb7a0ed9088780071fd232b",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+public class DrillIndexDescriptor extends AbstractIndexDescriptor {\n+\n+  /**\n+   * The name of Drill's Storage Plugin on which the Index was stored\n+   */\n+  private String storage;\n+\n+  private DrillTable table;\n+\n+  public DrillIndexDescriptor(List<LogicalExpression> indexCols,\n+                              CollationContext indexCollationContext,\n+                              List<LogicalExpression> nonIndexCols,\n+                              List<LogicalExpression> rowKeyColumns,\n+                              String indexName,\n+                              String tableName,\n+                              IndexType type,\n+                              NullDirection nullsDirection) {\n+    super(indexCols, indexCollationContext, nonIndexCols, rowKeyColumns, indexName, tableName, type, nullsDirection);\n+  }\n+\n+  public DrillIndexDescriptor(DrillIndexDefinition def) {\n+    this(def.indexColumns, def.indexCollationContext, def.nonIndexColumns, def.rowKeyColumns, def.indexName,\n+        def.getTableName(), def.getIndexType(), def.nullsDirection);\n+  }\n+\n+  @Override\n+  public double getRows(RelNode scan, RexNode indexCondition) {\n+    //TODO: real implementation is to use Drill's stats implementation. for now return fake value 1.0\n+    return 1.0;\n+  }\n+\n+  @Override\n+  public IndexGroupScan getIndexGroupScan() {\n+    try {\n+      final DrillTable idxTable = getDrillTable();\n+      GroupScan scan = idxTable.getGroupScan();\n+\n+      if (!(scan instanceof IndexGroupScan)){\n+        logger.error(\"The Groupscan from table {} is not an IndexGroupScan\", idxTable.toString());\n+        return null;\n+      }\n+      return (IndexGroupScan)scan;\n+    }\n+    catch(IOException e) {\n+      logger.error(\"Error in getIndexGroupScan \", e);\n+    }\n+    return null;\n+  }\n+\n+  public void attach(String storageName, DrillTable inTable) {\n+    storage = storageName;\n+    setDrillTable(inTable);\n+  }\n+\n+  public void setStorageName(String storageName) {\n+    storage = storageName;\n+  }\n+\n+  public String getStorageName() {\n+    return storage;\n+  }\n+\n+  public void setDrillTable(DrillTable table) {\n+    this.table = table;\n+  }\n+\n+  public DrillTable getDrillTable() {\n+    return this.table;\n+  }\n+\n+  public FunctionalIndexInfo getFunctionalInfo() {\n+    return null;\n+  }\n+\n+  @Override\n+  public PluginCost getPluginCostModel() {\n+    return null;\n+  }\n+}",
                "additions": 110,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "status": "added",
                "changes": 110,
                "deletions": 0,
                "sha": "4da62c204f6d46b4b26eb3e48d25dc9c7ae98b38",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * FunctionalIndexInfo is to collect Functional fields in IndexDescriptor, derive information needed for index plan,\n+ * e.g. convert and rewrite filter, columns, and rowtype on index scan that involve functional index.\n+ * In case different store might have different way to rename expression in index table, we allow storage plugin\n+ */\n+public interface FunctionalIndexInfo {\n+\n+  /**\n+   * @return if this index has functional indexed field, return true\n+   */\n+  boolean hasFunctional();\n+\n+  /**\n+   * @return the IndexDescriptor this IndexInfo built from\n+   */\n+  IndexDescriptor getIndexDesc();\n+\n+  /**\n+   * getNewPath: for an original path, return new rename '$N' path, notice there could be multiple renamed paths\n+   * if the there are multiple functional indexes refer original path.\n+   * @param path\n+   * @return\n+   */\n+  SchemaPath getNewPath(SchemaPath path);\n+\n+  /**\n+   * return a plain field path if the incoming index expression 'expr' is replaced to be a plain field\n+   * @param expr suppose to be an indexed expression\n+   * @return the renamed schemapath in index table for the indexed expression\n+   */\n+  SchemaPath getNewPathFromExpr(LogicalExpression expr);\n+\n+  /**\n+   * @return the map of indexed expression --> the involved schema paths in a indexed expression\n+   */\n+  Map<LogicalExpression, Set<SchemaPath>> getPathsInFunctionExpr();\n+\n+  /**\n+   * @return the map between indexed expression and to-be-converted target expression for scan in index\n+   * e.g. cast(a.b as int) -> '$0'\n+   */\n+  Map<LogicalExpression, LogicalExpression> getExprMap();\n+\n+  /**\n+   * @return the set of all new field names for indexed functions in index\n+   */\n+  Set<SchemaPath> allNewSchemaPaths();\n+\n+  /**\n+   * @return the set of all schemaPath exist in functional index fields\n+   */\n+  Set<SchemaPath> allPathsInFunction();\n+\n+  /**\n+   * Whether this implementation( may be different per storage) support rewrite rewriting varchar equality expression,\n+   * e.g. cast(a.b as varchar(2)) = 'ca'  to LIKE expression: cast(a.b as varchar(2) LIKE 'ca%'\n+   */\n+  boolean supportEqualCharConvertToLike();\n+\n+}",
                "additions": 85,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "status": "added",
                "changes": 85,
                "deletions": 0,
                "sha": "a12dcc6c103dd576748c91031001798a8a69bf14",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.physical.base.DbGroupScan;\n+import org.apache.drill.exec.planner.physical.DrillDistributionTrait.DistributionField;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+import org.apache.drill.exec.planner.common.DrillProjectRelBase;\n+import java.util.List;\n+import java.util.Set;\n+\n+public interface IndexCallContext {\n+  DrillScanRelBase getScan();\n+\n+  DbGroupScan getGroupScan();\n+\n+  List<RelCollation> getCollationList();\n+\n+  RelCollation getCollation();\n+\n+  boolean hasLowerProject();\n+\n+  boolean hasUpperProject();\n+\n+  RelOptRuleCall getCall();\n+\n+  Set<LogicalExpression> getLeftOutPathsInFunctions();\n+\n+  RelNode getFilter();\n+\n+  IndexableExprMarker getOrigMarker();\n+\n+  List<LogicalExpression> getSortExprs();\n+\n+  DrillProjectRelBase getLowerProject();\n+\n+  DrillProjectRelBase getUpperProject();\n+\n+  void setLeftOutPathsInFunctions(Set<LogicalExpression> exprs);\n+\n+  List<SchemaPath> getScanColumns();\n+\n+  RexNode getFilterCondition();\n+\n+  RexNode getOrigCondition();\n+\n+  Sort getSort();\n+\n+  void createSortExprs();\n+\n+  RelNode getExchange();\n+\n+  List<DistributionField> getDistributionFields();\n+}",
                "additions": 76,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "status": "added",
                "changes": 76,
                "deletions": 0,
                "sha": "65788cb52ae3eb932a97e2d49c45d1d96aed348c",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+\n+// Interface used to describe an index collection\n+public interface IndexCollection extends Iterable<IndexDescriptor> {\n+  /**\n+   * Types of an index collections: NATIVE_SECONDARY_INDEX_COLLECTION, EXTERNAL_SECONDARY_INDEX_COLLECTION\n+   */\n+  public static enum IndexCollectionType {\n+    NATIVE_SECONDARY_INDEX_COLLECTION,\n+    EXTERNAL_SECONDARY_INDEX_COLLECTION\n+  };\n+\n+  /**\n+   * Add a new index to the collection. Return True if index was successfully added; False otherwise\n+   */\n+  public boolean addIndex(IndexDescriptor index);\n+\n+  /**\n+   * Remove an index (identified by table name and index name) from the collection.\n+   * Return True if index was successfully removed; False otherwise\n+   */\n+  public boolean removeIndex(IndexDescriptor index);\n+\n+  /**\n+   * Clears all entries from this index collection\n+   */\n+  public void clearAll();\n+\n+  /**\n+   * Get the type of this index based on {@link IndexCollectionType}\n+   * @return one of the values in {@link IndexCollectionType}\n+   */\n+  public IndexCollectionType getIndexCollectionType();\n+\n+  /**\n+   * Whether or not this index collection supports index selection (selecting an\n+   * appropriate index out of multiple candidates). Typically, external index collections\n+   * such as Elasticsearch already have this capability while native secondary index collection\n+   * may not have - in such cases, Drill needs to do the index selection.\n+   */\n+  public boolean supportsIndexSelection();\n+\n+  /**\n+   * Get the estimated row count for a single index condition\n+   * @param indexCondition The index condition (e.g index_col1 < 10 AND index_col2 = 'abc')\n+   * @return The estimated row count\n+   */\n+  public double getRows(RexNode indexCondition);\n+\n+  /**\n+   * Whether or not the index supports getting row count statistics\n+   * @return True if index supports getting row count, False otherwise\n+   */\n+  public boolean supportsRowCountStats();\n+\n+  /**\n+   * Whether or not the index supports full-text search (to allow pushing down such filters)\n+   * @return True if index supports full-text search, False otherwise\n+   */\n+  public boolean supportsFullTextSearch();\n+\n+  /**\n+   * If this IndexCollection exposes a single GroupScan, return the GroupScan instance. For external indexes\n+   * such as Elasticsearch, we may have a single GroupScan representing all the indexes contained\n+   * within that collection.  On the other hand, for native indexes, each separate index would\n+   * have its own GroupScan.\n+   * @return GroupScan for this IndexCollection if available, otherwise null\n+   */\n+  public IndexGroupScan getGroupScan();\n+\n+  /**\n+   * Check if the field name is the leading key of any of the indexes in this collection\n+   * @param path\n+   * @return True if an appropriate index is found, False otherwise\n+   */\n+  public boolean isColumnIndexed(SchemaPath path);\n+\n+}",
                "additions": 99,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "status": "added",
                "changes": 99,
                "deletions": 0,
                "sha": "9b4d170e07dc6d1153133aa6a51978d46f28ebb6",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.drill.common.expression.LogicalExpression;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+\n+// Interface used to define an index,\n+public interface IndexDefinition {\n+  /**\n+   * Types of an index: PRIMARY_KEY_INDEX, NATIVE_SECONDARY_INDEX, EXTERNAL_SECONDARY_INDEX\n+   */\n+  static enum IndexType {\n+    PRIMARY_KEY_INDEX,\n+    NATIVE_SECONDARY_INDEX,\n+    EXTERNAL_SECONDARY_INDEX\n+  };\n+\n+  /**\n+   * Check to see if the field name is an index column and if so return the ordinal position in the index\n+   * @param path The field path you want to compare to index column names.\n+   * @return Return ordinal of the indexed column if valid, otherwise return -1\n+   */\n+  int getIndexColumnOrdinal(LogicalExpression path);\n+\n+  /**\n+   * Get the name of the index\n+   */\n+  String getIndexName();\n+\n+  /**\n+   * Check if this index 'covers' all the columns specified in the supplied list of columns\n+   * @param columns\n+   * @return True for covering index, False for non-covering\n+   */\n+  boolean isCoveringIndex(List<LogicalExpression> columns);\n+\n+  /**\n+   * Check if this index have all the columns specified in the supplied list of columns indexed\n+   * @param columns\n+   * @return True if all fields are indexed, False for some or all fields is not indexed\n+   */\n+  boolean allColumnsIndexed(Collection<LogicalExpression> columns);\n+\n+  /**\n+   * Check if this index has some columns specified in the supplied list of columns indexed\n+   * @param columns\n+   * @return True if some fields are indexed, False if none of the fields are indexed\n+   */\n+  boolean someColumnsIndexed(Collection<LogicalExpression> columns);\n+\n+  /**\n+   * Get the list of columns (typically 1 column) that constitute the row key (primary key)\n+   * @return\n+   */\n+  List<LogicalExpression> getRowKeyColumns();\n+\n+  /**\n+   * Get the name of the table this index is associated with\n+   */\n+  String getTableName();\n+\n+  /**\n+   * Get the type of this index based on {@link IndexType}\n+   * @return one of the values in {@link IndexType}\n+   */\n+  IndexType getIndexType();\n+\n+\n+  List<LogicalExpression> getIndexColumns();\n+\n+  List<LogicalExpression> getNonIndexColumns();\n+\n+  RelCollation getCollation();\n+\n+  Map<LogicalExpression, RelFieldCollation> getCollationMap();\n+\n+  /**\n+   * Get the nulls ordering of this index\n+   * @return True, if nulls first. False otherwise\n+   */\n+  NullDirection getNullsOrderingDirection();\n+\n+}",
                "additions": 105,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "status": "added",
                "changes": 105,
                "deletions": 0,
                "sha": "995d23c5742ff12cee9b6940ddb1dfbf4be0e89d",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+\n+\n+/**\n+ * IndexDefinition + functions to access materialized index(index table/scan, etc)\n+ */\n+\n+public interface IndexDescriptor extends IndexDefinition {\n+\n+  /**\n+   * Get the estimated row count for a single index condition\n+   * @param input The rel node corresponding to the primary table\n+   * @param indexCondition The index condition (e.g index_col1 < 10 AND index_col2 = 'abc')\n+   * @return The estimated row count\n+   */\n+  double getRows(RelNode input, RexNode indexCondition);\n+\n+  /**\n+   * Whether or not the index supports getting row count statistics\n+   * @return True if index supports getting row count, False otherwise\n+   */\n+  boolean supportsRowCountStats();\n+\n+  /**\n+   * Get an instance of the group scan associated with this index descriptor\n+   * @return An instance of group scan for this index\n+   */\n+  IndexGroupScan getIndexGroupScan();\n+\n+  /**\n+   * Whether or not the index supports full-text search (to allow pushing down such filters)\n+   * @return True if index supports full-text search, False otherwise\n+   */\n+  boolean supportsFullTextSearch();\n+\n+  FunctionalIndexInfo getFunctionalInfo();\n+\n+  public RelOptCost getCost(IndexProperties indexProps, RelOptPlanner planner,\n+      int numProjectedFields, GroupScan primaryGroupScan);\n+\n+  public PluginCost getPluginCostModel();\n+\n+}",
                "additions": 68,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "status": "added",
                "changes": 68,
                "deletions": 0,
                "sha": "f355285cc22b9feb88b893c88852466559530d6e",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+public interface IndexDiscover {\n+    IndexCollection getTableIndex(String tableName);\n+}",
                "additions": 23,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "status": "added",
                "changes": 23,
                "deletions": 0,
                "sha": "309083b1bb9edf369395ce8e79aad2ef3c3d9ff8",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.exec.physical.base.AbstractDbGroupScan;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+import org.apache.drill.exec.planner.physical.ScanPrel;\n+import org.apache.calcite.rel.RelNode;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * IndexDiscoverBase is the layer to read index configurations of tables on storage plugins,\n+ * then based on the properties it collected, get the StoragePlugin from StoragePluginRegistry,\n+ * together with indexes information, build an IndexCollection\n+ */\n+public abstract class IndexDiscoverBase implements IndexDiscover {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(IndexDiscoverBase.class);\n+\n+  private AbstractDbGroupScan scan; // group scan corresponding to the primary table\n+  private RelNode scanRel;   // physical scan rel corresponding to the primary table\n+\n+  public IndexDiscoverBase(AbstractDbGroupScan inScan, DrillScanRelBase inScanPrel) {\n+    scan = inScan;\n+    scanRel = inScanPrel;\n+  }\n+\n+  public IndexDiscoverBase(AbstractDbGroupScan inScan, ScanPrel inScanPrel) {\n+    scan = inScan;\n+    scanRel = inScanPrel;\n+  }\n+\n+  public AbstractDbGroupScan getOriginalScan() {\n+    return scan;\n+  }\n+\n+  public RelNode getOriginalScanRel() {\n+    return scanRel;\n+  }\n+\n+  public IndexCollection getTableIndex(String tableName, String storageName, Collection<DrillIndexDefinition>  indexDefs ) {\n+    Set<DrillIndexDescriptor> idxSet = new HashSet<>();\n+    for (DrillIndexDefinition def : indexDefs) {\n+      DrillIndexDescriptor indexDescriptor = new DrillIndexDescriptor(def);\n+      materializeIndex(storageName, indexDescriptor);\n+    }\n+    return new DrillIndexCollection(getOriginalScanRel(), idxSet);\n+  }\n+\n+  public void materializeIndex(String storageName, DrillIndexDescriptor index) {\n+    index.setStorageName(storageName);\n+    index.setDrillTable(buildDrillTable(index));\n+  }\n+\n+  /**\n+   * When there is storageName in IndexDescriptor, get a DrillTable instance based on the\n+   * StorageName and other informaiton in idxDesc that helps identifying the table.\n+   * @param idxDesc\n+   * @return\n+   */\n+  public DrillTable getExternalDrillTable(IndexDescriptor idxDesc) {\n+    //XX: get table object for this index, index storage plugin should provide interface to get the DrillTable object\n+    return null;\n+  }\n+\n+  /**\n+   * Abstract function getDrillTable will be implemented the IndexDiscover within storage plugin(e.g. HBase, MaprDB)\n+   * since the implementations of AbstractStoragePlugin, IndexDescriptor and DrillTable in that storage plugin may have\n+   * the implement details.\n+   * @param idxDesc\n+\n+   * @return\n+   */\n+  public DrillTable buildDrillTable(IndexDescriptor idxDesc) {\n+    if(idxDesc.getIndexType() == IndexDescriptor.IndexType.EXTERNAL_SECONDARY_INDEX) {\n+      return getExternalDrillTable(idxDesc);\n+    }\n+    else {\n+      return getNativeDrillTable(idxDesc);\n+    }\n+  }\n+\n+  /**\n+   * When it is native index(index provided by native storage plugin),\n+   * the actual IndexDiscover should provide the implementation to get the DrillTable object of index,\n+   * Otherwise, we call IndexDiscoverable interface exposed from external storage plugin's SchemaFactory\n+   * to get the desired DrillTable.\n+   * @param idxDesc\n+   * @return\n+   */\n+  public abstract DrillTable getNativeDrillTable(IndexDescriptor idxDesc);\n+\n+}",
                "additions": 110,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "status": "added",
                "changes": 110,
                "deletions": 0,
                "sha": "fde2a32d2431850609479a976c3c9b97057323fd",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+\n+\n+/**\n+ * SchemaFactory of a storage plugin that can used to store index tables should expose this interface to allow\n+ * IndexDiscovers discovering the index table without adding dependency to the storage plugin.\n+ */\n+public interface IndexDiscoverable {\n+\n+  /**\n+   * return the found DrillTable with path (e.g. names={\"elasticsearch\", \"staffidx\", \"stjson\"})\n+   * @param discover\n+   * @param desc\n+   * @return\n+   */\n+    DrillTable findTable(IndexDiscover discover, DrillIndexDescriptor desc);\n+\n+}",
                "additions": 37,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "status": "added",
                "changes": 37,
                "deletions": 0,
                "sha": "dbf5edc82c5fcfc73da09440d546ad0387c12c9d",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+\n+import java.util.List;\n+\n+/**\n+ * Encapsulates one or more IndexProperties representing (non)covering or intersecting indexes. The encapsulated\n+ * IndexProperties are used to rank the index in comparison with other IndexGroups.\n+ */\n+public class IndexGroup {\n+  private List<IndexProperties> indexProps;\n+\n+  public IndexGroup() {\n+    indexProps = Lists.newArrayList();\n+  }\n+\n+  public boolean isIntersectIndex() {\n+    if (indexProps.size() > 1) {\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  public int numIndexes() {\n+    return indexProps.size();\n+  }\n+\n+  public void addIndexProp(IndexProperties prop) {\n+    indexProps.add(prop);\n+  }\n+\n+  public void addIndexProp(List<IndexProperties> prop) {\n+    indexProps.addAll(prop);\n+  }\n+\n+  public boolean removeIndexProp(IndexProperties prop) {\n+    return indexProps.remove(prop);\n+  }\n+\n+  public List<IndexProperties> getIndexProps() {\n+    return indexProps;\n+  }\n+}\n+",
                "additions": 63,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "status": "added",
                "changes": 63,
                "deletions": 0,
                "sha": "ea34ea585b53e58946418b322ca7a7353cec00e9",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+\n+import java.util.Map;\n+\n+/**\n+ * IndexProperties encapsulates the various metrics of a single index that are related to\n+ * the current query. These metrics are subsequently used to rank the index in comparison\n+ * with other indexes.\n+ */\n+public interface IndexProperties  {\n+\n+  void setProperties(Map<LogicalExpression, RexNode> prefixMap,\n+                            boolean satisfiesCollation,\n+                            RexNode indexColumnsRemainderFilter,\n+                            Statistics stats);\n+\n+  double getLeadingSelectivity();\n+\n+  double getRemainderSelectivity();\n+\n+  boolean isCovering();\n+\n+  double getTotalRows();\n+\n+  IndexDescriptor getIndexDesc();\n+\n+  DrillScanRelBase getPrimaryTableScan();\n+\n+  RexNode getTotalRemainderFilter();\n+\n+  boolean satisfiesCollation();\n+\n+  void setSatisfiesCollation(boolean satisfiesCollation);\n+\n+  RelOptCost getSelfCost(RelOptPlanner planner);\n+\n+  int numLeadingFilters();\n+\n+  double getAvgRowSize();\n+}\n+",
                "additions": 64,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "status": "added",
                "changes": 64,
                "deletions": 0,
                "sha": "cfdd6d0302c0a9b89adc89251131ccecf45f3ca7",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+\n+\n+import java.util.List;\n+\n+public interface IndexStatistics {\n+    /** Returns the approximate number of rows in the table. */\n+    double getRowCount();\n+\n+    /** Returns the collections of columns on which this table is sorted. */\n+    List<RelCollation> getCollations();\n+\n+    /** Returns the distribution of the data in query result table. */\n+    RelDistribution getDistribution();\n+\n+}",
                "additions": 36,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "status": "added",
                "changes": 36,
                "deletions": 0,
                "sha": "e71636973c8d6bc8d52be8df6746b0d63799fdd5",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,262 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexCorrelVariable;\n+import org.apache.calcite.rex.RexDynamicParam;\n+import org.apache.calcite.rex.RexFieldAccess;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexLiteral;\n+import org.apache.calcite.rex.RexLocalRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexOver;\n+import org.apache.calcite.rex.RexRangeRef;\n+import org.apache.calcite.rex.RexVisitorImpl;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.planner.logical.DrillOptiq;\n+import org.apache.drill.exec.planner.logical.DrillParseContext;\n+import org.apache.drill.exec.planner.physical.PrelUtil;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * The filter expressions that could be indexed\n+ * Other than SchemaPaths, which represent columns of a table and could be indexed,\n+ * we consider only function expressions, and specifically, CAST function.\n+ * To judge if an expression is indexable, we check these:\n+ * 1, this expression should be one operand of a comparison operator, one of SqlKind.COMPARISON:\n+ *      IN, EQUALS, NOT_EQUALS, LESS_THAN, GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN_OR_EQUAL\n+ * 2, the expression tree should contain at least one inputRef (which means this expression is a\n+ *     computation on top of at least one column), and if we have more than one indexable expressions\n+ *     are found from operands of comparison operator, we should not take any expression as indexable.\n+ *\n+ * 3, (LIMIT to one level function) the expression is a function call, and no nested function call underneath, except ITEM\n+ * 4, (LIMIT to CAST), the function call is a CAST\n+ */\n+public class IndexableExprMarker extends RexVisitorImpl<Boolean> {\n+\n+  //map of rexNode->converted LogicalExpression\n+  final Map<RexNode, LogicalExpression> desiredExpressions = Maps.newHashMap();\n+\n+  //the expressions in equality comparison\n+  final Map<RexNode, LogicalExpression> equalityExpressions = Maps.newHashMap();\n+\n+  //the expression found in non-equality comparison\n+  final Map<RexNode, LogicalExpression> notInEquality = Maps.newHashMap();\n+\n+  //for =(cast(a.b as VARCHAR(len)), 'abcd'), if the 'len' is less than the max length of casted field on index table,\n+  // we want to rewrite it to LIKE(cast(a.b as VARCHAR(len)), 'abcd%')\n+  //map equalOnCastChar: key is the equal operator, value is the operand (cast(a.b as VARCHAR(10)),\n+  final Map<RexNode, LogicalExpression> equalOnCastChar = Maps.newHashMap();\n+\n+  final private RelNode inputRel;\n+\n+  //flag current recursive call state: whether we are on a direct operand of comparison operator\n+  boolean directCompareOp = false;\n+\n+  RexCall contextCall = null;\n+\n+  DrillParseContext parserContext;\n+\n+  public IndexableExprMarker(RelNode inputRel) {\n+    super(true);\n+    this.inputRel = inputRel;\n+    parserContext = new DrillParseContext(PrelUtil.getPlannerSettings(inputRel.getCluster()));\n+  }\n+\n+  public Map<RexNode, LogicalExpression> getIndexableExpression() {\n+    return ImmutableMap.copyOf(desiredExpressions);\n+  }\n+\n+  public Map<RexNode, LogicalExpression> getEqualOnCastChar() {\n+    return ImmutableMap.copyOf(equalOnCastChar);\n+  }\n+\n+  /**\n+   * return the expressions that were only in equality condition _and_ only once. ( a.b = 'value' )\n+   * @return\n+   */\n+  public Set<LogicalExpression> getExpressionsOnlyInEquality() {\n+\n+    Set<LogicalExpression> onlyInEquality = Sets.newHashSet();\n+\n+    Set<LogicalExpression> notInEqSet = Sets.newHashSet();\n+\n+    Set<LogicalExpression> inEqMoreThanOnce = Sets.newHashSet();\n+\n+    notInEqSet.addAll(notInEquality.values());\n+\n+    for (LogicalExpression expr : equalityExpressions.values()) {\n+      //only process expr that is not in any non-equality condition(!notInEqSet.contains)\n+      if (!notInEqSet.contains(expr)) {\n+\n+        //expr appear in two and more equality conditions should be ignored too\n+        if (inEqMoreThanOnce.contains(expr)) {\n+          continue;\n+        }\n+\n+        //we already have recorded this expr in equality condition, move it to inEqMoreThanOnce\n+        if (onlyInEquality.contains(expr)) {\n+          inEqMoreThanOnce.add(expr);\n+          onlyInEquality.remove(expr);\n+          continue;\n+        }\n+\n+        //finally we could take this expr\n+        onlyInEquality.add(expr);\n+      }\n+    }\n+    return onlyInEquality;\n+  }\n+\n+  @Override\n+  public Boolean visitInputRef(RexInputRef rexInputRef) {\n+    return directCompareOp;\n+  }\n+\n+  public boolean containInputRef(RexNode rex) {\n+    if (rex instanceof RexInputRef) {\n+      return true;\n+    }\n+    if ((rex instanceof RexCall) && \"ITEM\".equals(((RexCall)rex).getOperator().getName())) {\n+      return true;\n+    }\n+    //TODO: use a visitor search recursively for inputRef, if found one return true\n+    return false;\n+  }\n+\n+  public boolean operandsAreIndexable(RexCall call) {\n+    SqlKind kind = call.getKind();\n+    boolean kindIsRight = (SqlKind.COMPARISON.contains(kind) || kind==SqlKind.LIKE || kind == SqlKind.SIMILAR);\n+\n+    if (!kindIsRight) {\n+      return false;\n+    }\n+\n+    int inputReference = 0;\n+    for (RexNode operand : call.operands) {\n+      //if for this operator, there are two operands and more have inputRef, which means it is something like:\n+      // a.b = a.c, instead of a.b ='hello', so this cannot apply index\n+      if (containInputRef(operand)) {\n+        inputReference++;\n+        if(inputReference>=2) {\n+          return false;\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public Boolean visitCall(RexCall call) {\n+    if (call.getKind() == SqlKind.NOT) {\n+      // Conditions under NOT are not indexable\n+      return false;\n+    }\n+    if (operandsAreIndexable(call)) {\n+      for (RexNode operand : call.operands) {\n+        directCompareOp = true;\n+        contextCall = call;\n+        boolean markIt = operand.accept(this);\n+        directCompareOp = false;\n+        contextCall = null;\n+        if (markIt) {\n+          LogicalExpression expr = DrillOptiq.toDrill(parserContext, inputRel, operand);\n+          desiredExpressions.put(operand, expr);\n+          if (call.getKind() == SqlKind.EQUALS) {\n+            equalityExpressions.put(operand, expr);\n+          }\n+          else {\n+            notInEquality.put(operand, expr);\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    //now we are handling a call directly under comparison e.g. <([call], literal)\n+    if (directCompareOp) {\n+      // if it is an item, or CAST function\n+      if (\"ITEM\".equals(call.getOperator().getName())) {\n+        return directCompareOp;\n+      }\n+      else if (call.getKind() == SqlKind.CAST) {\n+        //For now, we care only direct CAST: CAST's operand is a field(schemaPath),\n+        // either ITEM call(nested name) or inputRef\n+\n+        //cast as char/varchar in equals function\n+        if(contextCall != null && contextCall.getKind() == SqlKind.EQUALS\n+            && (call.getType().getSqlTypeName()== SqlTypeName.CHAR\n+                || call.getType().getSqlTypeName()==SqlTypeName.VARCHAR)) {\n+          equalOnCastChar.put(contextCall, DrillOptiq.toDrill(parserContext, inputRel, call));\n+        }\n+\n+        RexNode castOp = call.operands.get(0);\n+        if (castOp instanceof RexInputRef) {\n+          return true;\n+        }\n+        if ((castOp instanceof RexCall) && (\"ITEM\".equals(((RexCall)castOp).getOperator().getName()))) {\n+          return true;\n+        }\n+      }\n+    }\n+\n+    for (RexNode operand : call.operands) {\n+      boolean bret = operand.accept(this);\n+    }\n+    return false;\n+  }\n+\n+  public Boolean visitLocalRef(RexLocalRef localRef) {\n+    return false;\n+  }\n+\n+  public Boolean visitLiteral(RexLiteral literal) {\n+    return false;\n+  }\n+\n+  public Boolean visitOver(RexOver over) {\n+    return false;\n+  }\n+\n+  public Boolean visitCorrelVariable(RexCorrelVariable correlVariable) {\n+    return false;\n+  }\n+\n+  public Boolean visitDynamicParam(RexDynamicParam dynamicParam) {\n+    return false;\n+  }\n+\n+  public Boolean visitRangeRef(RexRangeRef rangeRef) {\n+    return false;\n+  }\n+\n+  public Boolean visitFieldAccess(RexFieldAccess fieldAccess) {\n+    final RexNode expr = fieldAccess.getReferenceExpr();\n+    return expr.accept(this);\n+  }\n+}",
                "additions": 262,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "status": "added",
                "changes": 262,
                "deletions": 0,
                "sha": "a1a6fc882a35e76cd473fc1e60ff1317865b8102",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.common.exceptions.DrillRuntimeException;\n+\n+public class InvalidIndexDefinitionException extends DrillRuntimeException {\n+  public InvalidIndexDefinitionException(String message) {\n+    super(message);\n+  }\n+\n+}",
                "additions": 27,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "status": "added",
                "changes": 27,
                "deletions": 0,
                "sha": "c17d09f000ff79a8a9c0ac56fd126781250dfd67",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+\n+public interface Statistics {\n+\n+  double ROWCOUNT_UNKNOWN = -1;\n+  //HUGE is same as DrillCostBase.HUGE\n+  double ROWCOUNT_HUGE = Double.MAX_VALUE;\n+  double AVG_ROWSIZE_UNKNOWN = -1;\n+  long AVG_COLUMN_SIZE = 10;\n+\n+  /** Returns whether statistics are available. Should be called prior to using the statistics\n+   */\n+  boolean isStatsAvailable();\n+\n+  /** Returns a unique index identifier\n+   *  @param idx - Index specified as a {@link IndexDescriptor}\n+   *  @return The unique index identifier\n+   */\n+  String buildUniqueIndexIdentifier(IndexDescriptor idx);\n+\n+  /** Returns the rowcount for the specified filter condition\n+   *  @param condition - Filter specified as a {@link RexNode}\n+   *  @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   *  @param scanRel - The current scan rel\n+   *  @return the rowcount for the filter\n+   */\n+  double getRowCount(RexNode condition, String tabIdxName, RelNode scanRel);\n+\n+  /** Returns the leading rowcount for the specified filter condition\n+   *  Leading rowcount means rowcount for filter condition only on leading index columns.\n+   *  @param condition - Filter specified as a {@link RexNode}\n+   *  @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   *  @param scanRel - The current scan rel\n+   *  @return the leading rowcount\n+   */\n+  double getLeadingRowCount(RexNode condition, String tabIdxName, RelNode scanRel);\n+\n+  /** Returns the average row size for the specified filter condition\n+   * @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   * @param isIndexScan - Whether the current rel is an index scan (false for primary table)\n+   */\n+  double getAvgRowSize(String tabIdxName, boolean isIndexScan);\n+\n+  boolean initialize(RexNode condition, DrillScanRelBase scanRel, IndexCallContext context);\n+}",
                "additions": 66,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "status": "added",
                "changes": 66,
                "deletions": 0,
                "sha": "2859102e21599b8b27106e7fb0cea8ba8ab88154",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+public interface StatisticsPayload {\n+  double getRowCount();\n+  double getLeadingRowCount();\n+  double getAvgRowSize();\n+}",
                "additions": 24,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "status": "added",
                "changes": 24,
                "deletions": 0,
                "sha": "6894e4fdbe61156213cffe33cc819e559434bb10",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -91,6 +91,10 @@ public void setOptions(SessionOptionManager options) {\n     this.options = options;\n   }\n \n+  public void setGroupScan(GroupScan scan) {\n+    this.scan = scan;\n+  }\n+\n   public GroupScan getGroupScan() throws IOException{\n     if (scan == null) {\n       if (selection instanceof FileSelection && ((FileSelection) selection).isEmptyDirectory()) {",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "ed9b32fe27bd89d5d067fbac8fc240ade70f2985",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.physical;\n+\n+import java.util.List;\n+\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.drill.exec.record.VectorWrapper;\n+\n+import com.fasterxml.jackson.annotation.JsonTypeInfo;\n+\n+@JsonTypeInfo(use=JsonTypeInfo.Id.CLASS, include=JsonTypeInfo.As.PROPERTY, property=\"@class\")\n+public interface PartitionFunction  {\n+\n+  /**\n+   * Return the list of FieldReferences that participate in the partitioning function\n+   * @return list of FieldReferences\n+   */\n+  List<FieldReference> getPartitionRefList();\n+\n+  /**\n+   * Setup method for the partitioning function\n+   * @param partitionKeys a list of partition columns on which range partitioning is needed\n+   */\n+  void setup(List<VectorWrapper<?>> partitionKeys);\n+\n+  /**\n+   * Evaluate a partitioning function for a particular row index and return the partition id\n+   * @param index the integer index into the partition keys vector for a specific 'row' of values\n+   * @param numPartitions the max number of partitions that are allowed\n+   * @return partition id, an integer value\n+   */\n+  int eval(int index, int numPartitions);\n+\n+  /**\n+   * Returns a FieldReference (LogicalExpression) for the partition function\n+   * @return FieldReference for the partition function\n+   */\n+  FieldReference getPartitionFieldRef();\n+\n+}",
                "additions": 56,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "status": "added",
                "changes": 56,
                "deletions": 0,
                "sha": "754c5d753e3dad3ea8b1ef910be2a9958d6f9d5b",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -80,7 +80,7 @@ protected AbstractRecordBatch(final T popConfig, final FragmentContext context,\n     }\n   }\n \n-  protected static enum BatchState {\n+  public static enum BatchState {\n     /** Need to build schema and return. */\n     BUILD_SCHEMA,\n     /** This is still the first data batch. */",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "cb790918a47500628d124b1c7687641962c0bd60",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -101,6 +101,11 @@ public void allocate(Map<String, ValueVector> vectorMap) throws OutOfMemoryExcep\n     }\n   }\n \n+  @Override\n+  public boolean hasNext() {\n+    return false;\n+  }\n+\n   protected List<SchemaPath> getDefaultColumnsToRead() {\n     return GroupScan.ALL_COLUMNS;\n   }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "1bbbe76b7c100868f925f5d260bac6fc29d7ccdb",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -42,6 +42,14 @@\n \n   void allocate(Map<String, ValueVector> vectorMap) throws OutOfMemoryException;\n \n+  /**\n+   * Check if the reader may have potentially more data to be read in subsequent iterations. Certain types of readers\n+   * such as repeatable readers can be invoked multiple times, so this method will allow ScanBatch to check with\n+   * the reader before closing it.\n+   * @return return true if there could potentially be more reads, false otherwise\n+   */\n+  boolean hasNext();\n+\n   /**\n    * Increments this record reader forward, writing via the provided output\n    * mutator into the output batch.",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "status": "modified",
                "changes": 8,
                "deletions": 0,
                "sha": "33b361c7b0d014c1a3fffd2cfec0733ad01d8871",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.util;\n+\n+\n+import org.apache.drill.shaded.guava.com.google.common.io.BaseEncoding;\n+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+import org.apache.drill.common.exceptions.DrillRuntimeException;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.physical.PlannerSettings;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+/**\n+ * This class provided utility methods to encode and decode a set of user specified\n+ * SchemaPaths to a set of encoded SchemaPaths with the following properties.\n+ * <ol>\n+ * <li>Valid Drill identifier as per its grammar with only one, root name segment.\n+ * <li>A single identifier can not exceed 1024 characters in length.\n+ * </ol>\n+ * <p>\n+ * Format of the encoded SchemaPath:\n+ * <blockquote><pre>$$ENC\\d\\dlt;base32 encoded input paths&gt;</pre></blockquote>\n+ * <p>\n+ * We use Base-32 over Base-64 because the later's charset includes '\\' and '+'.\n+ */\n+public class EncodedSchemaPathSet {\n+\n+  private static final int ESTIMATED_ENCODED_SIZE = 1024;\n+\n+  private static final String ENC_PREFIX = \"$$ENC\";\n+\n+  private static final String ENC_FORMAT_STRING = ENC_PREFIX + \"%02d%s\";\n+  private static final int ENC_PREFIX_SIZE = ENC_PREFIX.length() + \"00\".length();\n+  private static final int MAX_ENC_IDENTIFIER_SIZE = (PlannerSettings.DEFAULT_IDENTIFIER_MAX_LENGTH - ENC_PREFIX_SIZE);\n+  private static final int MAX_ENC_IDENTIFIER_COUNT = 100; // \"$$ENC00*...$$ENC99*\"\n+\n+  private static final BaseEncoding CODEC = BaseEncoding.base32().omitPadding(); // no-padding version\n+\n+  public static final String ENCODED_STAR_COLUMN = encode(\"*\")[0];\n+\n+  /*\n+   * Performance of various methods of encoding a Java String to UTF-8 keeps changing\n+   * between releases, hence we'll encapsulate the actual methods within these functions\n+   * and use them everywhere in Drill\n+   */\n+  private static final String UTF_8 = \"utf-8\";\n+\n+\n+  private static byte[] encodeUTF(String input) {\n+    try {\n+      return input.getBytes(UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  private static String decodeUTF(byte[] input) {\n+    try {\n+      return new String(input, UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  private static String decodeUTF(byte[] input, int offset, int length) {\n+    try {\n+      return new String(input, offset, length, UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  /**\n+   * Returns the encoded array of SchemaPath identifiers from the input array of SchemaPath.\n+   * <p>\n+   * The returned identifiers have the following properties:\n+   * <ul>\n+   *  <li>Each SchemaPath identifier in the array has only one single root NameSegment.</li>\n+   *  <li>Maximum length of each such identifier is equal to the maximum length of Drill identifier (currently 1024).</li>\n+   * </ul>\n+   * <p>\n+   * We take advantage of the fact that Java's modified utf-8 encoding can never contain\n+   * embedded null byte.\n+   * @see <a>http://docs.oracle.com/javase/8/docs/api/java/io/DataInput.html#modified-utf-8</a>\n+   */\n+  public static String[] encode(final String... schemaPaths) {\n+    Preconditions.checkArgument(schemaPaths != null && schemaPaths.length > 0,\n+        \"At least one schema path should be provided\");\n+\n+    NoCopyByteArrayOutputStream out = new NoCopyByteArrayOutputStream(ESTIMATED_ENCODED_SIZE);\n+    int bufOffset = 1; // 1st byte is NULL\n+    for (String schemaPath : schemaPaths) {\n+      out.write(0);\n+      out.write(encodeUTF(schemaPath));\n+    }\n+    out.close();\n+\n+    final int bufLen = out.size() - 1; // not counting the first NULL byte\n+    String encodedStr = CODEC.encode(out.getBuffer(), bufOffset, bufLen);\n+    assert !encodedStr.endsWith(\"=\") : String.format(\"Encoded string '%s' ends with '='\", encodedStr);\n+    return splitIdentifiers(encodedStr);\n+  }\n+\n+  public static boolean isEncodedSchemaPath(SchemaPath schemaPath) {\n+    return schemaPath != null && isEncodedSchemaPath(schemaPath.getRootSegment().getNameSegment().getPath());\n+  }\n+\n+  public static boolean isEncodedSchemaPath(String schemaPath) {\n+    return schemaPath != null && schemaPath.startsWith(ENC_PREFIX);\n+  }\n+\n+  /**\n+   * Returns the decoded Collection of SchemaPath from the input which\n+   * may contain a mix of encoded and non-encoded SchemaPaths.\n+   * <p>\n+   * The size of returned Collection is always equal to or greater than the\n+   * input array.\n+   * <p>\n+   * The non-encoded SchemaPaths are collated in the beginning to the returned\n+   * array, in the same order as that of the input array.\n+   */\n+  public static Collection<SchemaPath> decode(final Collection<SchemaPath> encodedPaths) {\n+    String[] schemaPathStrings = new String[encodedPaths.size()];\n+    Iterator<SchemaPath> encodedPathsItr = encodedPaths.iterator();\n+    for (int i = 0; i < schemaPathStrings.length; i++) {\n+      SchemaPath schemaPath = encodedPathsItr.next();\n+      if (schemaPath.getRootSegmentPath().startsWith(ENC_PREFIX)) {\n+        // encoded schema path contains only root segment\n+        schemaPathStrings[i] = schemaPath.getRootSegmentPath();\n+      } else {\n+        schemaPathStrings[i] = schemaPath.toExpr();\n+      }\n+    }\n+    String[] decodedStrings = decode(schemaPathStrings);\n+    if (decodedStrings == schemaPathStrings) {\n+      return encodedPaths; // return the original collection as no encoded SchemaPath was found\n+    } else {\n+      ImmutableList.Builder<SchemaPath> builder = new ImmutableList.Builder<>();\n+      for (String decodedString : decodedStrings) {\n+        if (\"*\".equals(decodedString) || \"`*`\".equals(decodedString)) {\n+          builder.add(SchemaPath.STAR_COLUMN);\n+        } else {\n+          builder.add(SchemaPath.parseFromString(decodedString));\n+        }\n+      }\n+      return builder.build();\n+    }\n+  }\n+\n+  /**\n+   * Returns the decoded array of SchemaPath strings from the input which\n+   * may contain a mix of encoded and non-encoded SchemaPaths.\n+   * <p>\n+   * The size of returned array is always equal to or greater than the\n+   * input array.\n+   * <p>\n+   * The non-encoded SchemaPaths are collated in the beginning to the returned\n+   * array, in the same order as that of the input array.\n+   */\n+  public static String[] decode(final String... encodedPaths) {\n+    Preconditions.checkArgument(encodedPaths != null && encodedPaths.length > 0,\n+        \"At least one encoded path should be provided\");\n+\n+    StringBuilder sb = new StringBuilder(ESTIMATED_ENCODED_SIZE);\n+\n+    // As the encoded schema path move across components, they could get reordered.\n+    // Sorting ensures that the original order is restored before concatenating the\n+    // components back to the full encoded String.\n+    Arrays.sort(encodedPaths);\n+\n+    List<String> decodedPathList = Lists.newArrayList();\n+    for (String encodedPath : encodedPaths) {\n+      if (encodedPath.startsWith(ENC_PREFIX)) {\n+        sb.append(encodedPath, ENC_PREFIX_SIZE, encodedPath.length());\n+      } else {\n+        decodedPathList.add(encodedPath);\n+      }\n+    }\n+\n+    if (sb.length() > 0) {\n+      byte[] decodedBytes;\n+      try {\n+        decodedBytes = CODEC.decode(sb);\n+      } catch (IllegalArgumentException e) {\n+        throw new DrillRuntimeException(String.format(\n+            \"Unable to decode the input strings as encoded schema paths:\\n%s\", Arrays.asList(encodedPaths)), e);\n+      }\n+\n+      int start = 0, index = 0;\n+      for (; index < decodedBytes.length; index++) {\n+        if (decodedBytes[index] == 0 && index - start > 0) {\n+          decodedPathList.add(decodeUTF(decodedBytes, start, index-start));\n+          start = index + 1;\n+        }\n+      }\n+      if (index - start > 0) {\n+        String lastSchemaPath = decodeUTF(decodedBytes, start, index-start).trim();\n+        if (!lastSchemaPath.isEmpty()) {\n+          decodedPathList.add(lastSchemaPath);\n+        }\n+      }\n+      return decodedPathList.toArray(new String[decodedPathList.size()]);\n+    } else {\n+      // original list did not have any encoded path, return as is\n+      return encodedPaths;\n+    }\n+  }\n+\n+  /**\n+   * Splits the input string so that the length of each encoded string,\n+   * including the signature prefix is less than or equal to MAX_DRILL_IDENTIFIER_SIZE.\n+   */\n+  private static String[] splitIdentifiers(String input) {\n+    if (input.length() < MAX_ENC_IDENTIFIER_SIZE) {\n+      return new String[] { String.format(ENC_FORMAT_STRING, 0, input) };\n+    }\n+    int splitsCount = (int) Math.ceil(input.length() / (double)MAX_ENC_IDENTIFIER_SIZE);\n+    if (splitsCount > MAX_ENC_IDENTIFIER_COUNT) {\n+      throw new DrillRuntimeException(String.format(\n+          \"Encoded size of the SchemaPath identifier '%s' exceeded maximum value.\", input));\n+    }\n+    String[] result = new String[splitsCount];\n+    for (int i = 0, startIdx = 0; i < result.length; i++, startIdx += MAX_ENC_IDENTIFIER_SIZE) {\n+      // TODO: see if we can avoid memcpy due to input.substring() call\n+      result[i] = String.format(ENC_FORMAT_STRING, i, input.substring(startIdx, Math.min(input.length(), startIdx + MAX_ENC_IDENTIFIER_SIZE)));\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Optimized version of Java's ByteArrayOutputStream which returns the underlying\n+   * byte array instead of making a copy\n+   */\n+  private static class NoCopyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public NoCopyByteArrayOutputStream(int size) {\n+      super(size);\n+    }\n+\n+    public byte[] getBuffer() {\n+      return buf;\n+    }\n+\n+    public int size() {\n+      return count;\n+    }\n+\n+    @Override\n+    public void write(int b) {\n+      super.write(b);\n+    }\n+\n+    @Override\n+    public void write(byte[] b) {\n+      super.write(b, 0, b.length);\n+    }\n+\n+    @Override\n+    public void close() {\n+      try {\n+        super.close();\n+      } catch (IOException e) {\n+        throw new DrillRuntimeException(e); // should never come to this\n+      }\n+    }\n+  }\n+\n+}",
                "additions": 291,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "status": "added",
                "changes": 291,
                "deletions": 0,
                "sha": "5f9eef80190be6ffd6d29e02096f66366f20dfb8",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -414,9 +414,9 @@ private FragmentState(int index, int value) {\n      */\n     UNORDERED_RECEIVER(11, 11),\n     /**\n-     * <code>RANGE_SENDER = 12;</code>\n+     * <code>RANGE_PARTITION_SENDER = 12;</code>\n      */\n-    RANGE_SENDER(12, 12),\n+    RANGE_PARTITION_SENDER(12, 12),\n     /**\n      * <code>SCREEN = 13;</code>\n      */\n@@ -593,6 +593,10 @@ private FragmentState(int index, int value) {\n      * <code>RUNTIME_FILTER = 56;</code>\n      */\n     RUNTIME_FILTER(56, 56),\n+    /**\n+     * <code>ROWKEY_JOIN = 57;</code>\n+     */\n+    ROWKEY_JOIN(57, 57),\n     ;\n \n     /**\n@@ -644,9 +648,9 @@ private FragmentState(int index, int value) {\n      */\n     public static final int UNORDERED_RECEIVER_VALUE = 11;\n     /**\n-     * <code>RANGE_SENDER = 12;</code>\n+     * <code>RANGE_PARTITION_SENDER = 12;</code>\n      */\n-    public static final int RANGE_SENDER_VALUE = 12;\n+    public static final int RANGE_PARTITION_SENDER_VALUE = 12;\n     /**\n      * <code>SCREEN = 13;</code>\n      */\n@@ -823,6 +827,10 @@ private FragmentState(int index, int value) {\n      * <code>RUNTIME_FILTER = 56;</code>\n      */\n     public static final int RUNTIME_FILTER_VALUE = 56;\n+    /**\n+     * <code>ROWKEY_JOIN = 57;</code>\n+     */\n+    public static final int ROWKEY_JOIN_VALUE = 57;\n \n \n     public final int getNumber() { return value; }\n@@ -841,7 +849,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 9: return ORDERED_PARTITION_SENDER;\n         case 10: return PROJECT;\n         case 11: return UNORDERED_RECEIVER;\n-        case 12: return RANGE_SENDER;\n+        case 12: return RANGE_PARTITION_SENDER;\n         case 13: return SCREEN;\n         case 14: return SELECTION_VECTOR_REMOVER;\n         case 15: return STREAMING_AGGREGATE;\n@@ -886,6 +894,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 54: return PARTITION_LIMIT;\n         case 55: return PCAPNG_SUB_SCAN;\n         case 56: return RUNTIME_FILTER;\n+        case 57: return ROWKEY_JOIN;\n         default: return null;\n       }\n     }\n@@ -24422,40 +24431,40 @@ public Builder clearStatus() {\n       \"TATEMENT\\020\\005*\\207\\001\\n\\rFragmentState\\022\\013\\n\\007SENDING\\020\" +\n       \"\\000\\022\\027\\n\\023AWAITING_ALLOCATION\\020\\001\\022\\013\\n\\007RUNNING\\020\\002\\022\" +\n       \"\\014\\n\\010FINISHED\\020\\003\\022\\r\\n\\tCANCELLED\\020\\004\\022\\n\\n\\006FAILED\\020\\005\" +\n-      \"\\022\\032\\n\\026CANCELLATION_REQUESTED\\020\\006*\\367\\010\\n\\020CoreOpe\" +\n+      \"\\022\\032\\n\\026CANCELLATION_REQUESTED\\020\\006*\\222\\t\\n\\020CoreOpe\" +\n       \"ratorType\\022\\021\\n\\rSINGLE_SENDER\\020\\000\\022\\024\\n\\020BROADCAS\" +\n       \"T_SENDER\\020\\001\\022\\n\\n\\006FILTER\\020\\002\\022\\022\\n\\016HASH_AGGREGATE\" +\n       \"\\020\\003\\022\\r\\n\\tHASH_JOIN\\020\\004\\022\\016\\n\\nMERGE_JOIN\\020\\005\\022\\031\\n\\025HAS\" +\n       \"H_PARTITION_SENDER\\020\\006\\022\\t\\n\\005LIMIT\\020\\007\\022\\024\\n\\020MERGI\" +\n       \"NG_RECEIVER\\020\\010\\022\\034\\n\\030ORDERED_PARTITION_SENDE\" +\n       \"R\\020\\t\\022\\013\\n\\007PROJECT\\020\\n\\022\\026\\n\\022UNORDERED_RECEIVER\\020\\013\",\n-      \"\\022\\020\\n\\014RANGE_SENDER\\020\\014\\022\\n\\n\\006SCREEN\\020\\r\\022\\034\\n\\030SELECT\" +\n-      \"ION_VECTOR_REMOVER\\020\\016\\022\\027\\n\\023STREAMING_AGGREG\" +\n-      \"ATE\\020\\017\\022\\016\\n\\nTOP_N_SORT\\020\\020\\022\\021\\n\\rEXTERNAL_SORT\\020\\021\" +\n-      \"\\022\\t\\n\\005TRACE\\020\\022\\022\\t\\n\\005UNION\\020\\023\\022\\014\\n\\010OLD_SORT\\020\\024\\022\\032\\n\\026\" +\n-      \"PARQUET_ROW_GROUP_SCAN\\020\\025\\022\\021\\n\\rHIVE_SUB_SCA\" +\n-      \"N\\020\\026\\022\\025\\n\\021SYSTEM_TABLE_SCAN\\020\\027\\022\\021\\n\\rMOCK_SUB_S\" +\n-      \"CAN\\020\\030\\022\\022\\n\\016PARQUET_WRITER\\020\\031\\022\\023\\n\\017DIRECT_SUB_\" +\n-      \"SCAN\\020\\032\\022\\017\\n\\013TEXT_WRITER\\020\\033\\022\\021\\n\\rTEXT_SUB_SCAN\" +\n-      \"\\020\\034\\022\\021\\n\\rJSON_SUB_SCAN\\020\\035\\022\\030\\n\\024INFO_SCHEMA_SUB\" +\n-      \"_SCAN\\020\\036\\022\\023\\n\\017COMPLEX_TO_JSON\\020\\037\\022\\025\\n\\021PRODUCER\",\n-      \"_CONSUMER\\020 \\022\\022\\n\\016HBASE_SUB_SCAN\\020!\\022\\n\\n\\006WINDO\" +\n-      \"W\\020\\\"\\022\\024\\n\\020NESTED_LOOP_JOIN\\020#\\022\\021\\n\\rAVRO_SUB_SC\" +\n-      \"AN\\020$\\022\\021\\n\\rPCAP_SUB_SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCA\" +\n-      \"N\\020&\\022\\021\\n\\rKUDU_SUB_SCAN\\020\\'\\022\\013\\n\\007FLATTEN\\020(\\022\\020\\n\\014L\" +\n-      \"ATERAL_JOIN\\020)\\022\\n\\n\\006UNNEST\\020*\\022,\\n(HIVE_DRILL_\" +\n-      \"NATIVE_PARQUET_ROW_GROUP_SCAN\\020+\\022\\r\\n\\tJDBC_\" +\n-      \"SCAN\\020,\\022\\022\\n\\016REGEX_SUB_SCAN\\020-\\022\\023\\n\\017MAPRDB_SUB\" +\n-      \"_SCAN\\020.\\022\\022\\n\\016MONGO_SUB_SCAN\\020/\\022\\017\\n\\013KUDU_WRIT\" +\n-      \"ER\\0200\\022\\026\\n\\022OPEN_TSDB_SUB_SCAN\\0201\\022\\017\\n\\013JSON_WRI\" +\n-      \"TER\\0202\\022\\026\\n\\022HTPPD_LOG_SUB_SCAN\\0203\\022\\022\\n\\016IMAGE_S\",\n-      \"UB_SCAN\\0204\\022\\025\\n\\021SEQUENCE_SUB_SCAN\\0205\\022\\023\\n\\017PART\" +\n-      \"ITION_LIMIT\\0206\\022\\023\\n\\017PCAPNG_SUB_SCAN\\0207\\022\\022\\n\\016RU\" +\n-      \"NTIME_FILTER\\0208*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNK\" +\n-      \"NOWN\\020\\000\\022\\016\\n\\nSASL_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRE\" +\n-      \"SS\\020\\002\\022\\020\\n\\014SASL_SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B\" +\n-      \".\\n\\033org.apache.drill.exec.protoB\\rUserBitS\" +\n-      \"haredH\\001\"\n+      \"\\022\\032\\n\\026RANGE_PARTITION_SENDER\\020\\014\\022\\n\\n\\006SCREEN\\020\\r\" +\n+      \"\\022\\034\\n\\030SELECTION_VECTOR_REMOVER\\020\\016\\022\\027\\n\\023STREAM\" +\n+      \"ING_AGGREGATE\\020\\017\\022\\016\\n\\nTOP_N_SORT\\020\\020\\022\\021\\n\\rEXTER\" +\n+      \"NAL_SORT\\020\\021\\022\\t\\n\\005TRACE\\020\\022\\022\\t\\n\\005UNION\\020\\023\\022\\014\\n\\010OLD_\" +\n+      \"SORT\\020\\024\\022\\032\\n\\026PARQUET_ROW_GROUP_SCAN\\020\\025\\022\\021\\n\\rHI\" +\n+      \"VE_SUB_SCAN\\020\\026\\022\\025\\n\\021SYSTEM_TABLE_SCAN\\020\\027\\022\\021\\n\\r\" +\n+      \"MOCK_SUB_SCAN\\020\\030\\022\\022\\n\\016PARQUET_WRITER\\020\\031\\022\\023\\n\\017D\" +\n+      \"IRECT_SUB_SCAN\\020\\032\\022\\017\\n\\013TEXT_WRITER\\020\\033\\022\\021\\n\\rTEX\" +\n+      \"T_SUB_SCAN\\020\\034\\022\\021\\n\\rJSON_SUB_SCAN\\020\\035\\022\\030\\n\\024INFO_\" +\n+      \"SCHEMA_SUB_SCAN\\020\\036\\022\\023\\n\\017COMPLEX_TO_JSON\\020\\037\\022\\025\",\n+      \"\\n\\021PRODUCER_CONSUMER\\020 \\022\\022\\n\\016HBASE_SUB_SCAN\\020\" +\n+      \"!\\022\\n\\n\\006WINDOW\\020\\\"\\022\\024\\n\\020NESTED_LOOP_JOIN\\020#\\022\\021\\n\\rA\" +\n+      \"VRO_SUB_SCAN\\020$\\022\\021\\n\\rPCAP_SUB_SCAN\\020%\\022\\022\\n\\016KAF\" +\n+      \"KA_SUB_SCAN\\020&\\022\\021\\n\\rKUDU_SUB_SCAN\\020\\'\\022\\013\\n\\007FLAT\" +\n+      \"TEN\\020(\\022\\020\\n\\014LATERAL_JOIN\\020)\\022\\n\\n\\006UNNEST\\020*\\022,\\n(H\" +\n+      \"IVE_DRILL_NATIVE_PARQUET_ROW_GROUP_SCAN\\020\" +\n+      \"+\\022\\r\\n\\tJDBC_SCAN\\020,\\022\\022\\n\\016REGEX_SUB_SCAN\\020-\\022\\023\\n\\017\" +\n+      \"MAPRDB_SUB_SCAN\\020.\\022\\022\\n\\016MONGO_SUB_SCAN\\020/\\022\\017\\n\" +\n+      \"\\013KUDU_WRITER\\0200\\022\\026\\n\\022OPEN_TSDB_SUB_SCAN\\0201\\022\\017\" +\n+      \"\\n\\013JSON_WRITER\\0202\\022\\026\\n\\022HTPPD_LOG_SUB_SCAN\\0203\\022\",\n+      \"\\022\\n\\016IMAGE_SUB_SCAN\\0204\\022\\025\\n\\021SEQUENCE_SUB_SCAN\" +\n+      \"\\0205\\022\\023\\n\\017PARTITION_LIMIT\\0206\\022\\023\\n\\017PCAPNG_SUB_SC\" +\n+      \"AN\\0207\\022\\022\\n\\016RUNTIME_FILTER\\0208\\022\\017\\n\\013ROWKEY_JOIN\\020\" +\n+      \"9*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\\nSAS\" +\n+      \"L_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRESS\\020\\002\\022\\020\\n\\014SASL_\" +\n+      \"SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B.\\n\\033org.apache\" +\n+      \".drill.exec.protoB\\rUserBitSharedH\\001\"\n     };\n     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =\n       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "status": "modified",
                "changes": 75,
                "deletions": 33,
                "sha": "2f5c3de214d87c151cc23d52769d691df258ed9d",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -34,7 +34,7 @@\n     ORDERED_PARTITION_SENDER(9),\n     PROJECT(10),\n     UNORDERED_RECEIVER(11),\n-    RANGE_SENDER(12),\n+    RANGE_PARTITION_SENDER(12),\n     SCREEN(13),\n     SELECTION_VECTOR_REMOVER(14),\n     STREAMING_AGGREGATE(15),\n@@ -78,7 +78,8 @@\n     SEQUENCE_SUB_SCAN(53),\n     PARTITION_LIMIT(54),\n     PCAPNG_SUB_SCAN(55),\n-    RUNTIME_FILTER(56);\n+    RUNTIME_FILTER(56),\n+    ROWKEY_JOIN(57);\n     \n     public final int number;\n     \n@@ -108,7 +109,7 @@ public static CoreOperatorType valueOf(int number)\n             case 9: return ORDERED_PARTITION_SENDER;\n             case 10: return PROJECT;\n             case 11: return UNORDERED_RECEIVER;\n-            case 12: return RANGE_SENDER;\n+            case 12: return RANGE_PARTITION_SENDER;\n             case 13: return SCREEN;\n             case 14: return SELECTION_VECTOR_REMOVER;\n             case 15: return STREAMING_AGGREGATE;\n@@ -153,6 +154,7 @@ public static CoreOperatorType valueOf(int number)\n             case 54: return PARTITION_LIMIT;\n             case 55: return PCAPNG_SUB_SCAN;\n             case 56: return RUNTIME_FILTER;\n+            case 57: return ROWKEY_JOIN;\n             default: return null;\n         }\n     }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "6138ad614b9f92288b6278ce10232d4d76c705e3",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            },
            {
                "patch": "@@ -300,7 +300,7 @@ enum CoreOperatorType {\n   ORDERED_PARTITION_SENDER = 9;\n   PROJECT = 10;\n   UNORDERED_RECEIVER = 11;\n-  RANGE_SENDER = 12;\n+  RANGE_PARTITION_SENDER = 12;\n   SCREEN = 13;\n   SELECTION_VECTOR_REMOVER = 14;\n   STREAMING_AGGREGATE = 15;\n@@ -345,6 +345,7 @@ enum CoreOperatorType {\n   PARTITION_LIMIT = 54;\n   PCAPNG_SUB_SCAN = 55;\n   RUNTIME_FILTER = 56;\n+  ROWKEY_JOIN = 57;\n }\n \n /* Registry that contains list of jars, each jar contains its name and list of function signatures.",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/protobuf/UserBitShared.proto",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "843c6d8c385b45c247e1a1b02084cd8490a91639",
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/protobuf/UserBitShared.proto",
                "filename": "protocol/src/main/protobuf/UserBitShared.proto",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/protobuf/UserBitShared.proto?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce"
            }
        ],
        "bug_id": "drill_36",
        "parent": "https://github.com/apache/drill/commit/61e8b464063299dc1f67445157a46c4939b0cace",
        "message": "DRILL-6381: (Part 1) Secondary Index framework\n\n\u00a0 1. Secondary Index planning interfaces and abstract classes like DBGroupScan, DbSubScan, IndexDecriptor etc.\n\u00a0 2. Statistics and Cost model interfaces/classes: PluginCost, Statistics, StatisticsPayload, AbstractIndexStatistics\n\u00a0 3. ScanBatch and RecordReader to support repeatable scan\n\u00a0 4. Secondary Index execution related interfaces: RangePartitionSender, RowKeyJoin, PartitionFunction\n  5. MD-3979: Query using cast index plan fails with NPE\n\nCo-authored-by: Aman Sinha <asinha@maprtech.com>\nCo-authored-by: chunhui-shi <cshi@maprtech.com>\nCo-authored-by: Gautam Parai <gparai@maprtech.com>\nCo-authored-by: Padma Penumarthy <ppenumar97@yahoo.com>\nCo-authored-by: Hanumath Rao Maduri <hmaduri@maprtech.com>\n\nConflicts:\n\texec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java\n\tprotocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java\n\tprotocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java\n\tprotocol/src/main/protobuf/UserBitShared.proto",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/84fa4f10112dc81539c9bf0dcc62c49b7810971d",
        "file": [
            {
                "patch": "@@ -53,7 +53,7 @@\n \n   \n   DrillSqlAggOperator(String name, int argCount) {\n-    super(name, SqlKind.OTHER_FUNCTION, DynamicReturnType.INSTANCE, null, new Checker(argCount), SqlFunctionCategory.USER_DEFINED_FUNCTION);\n+    super(name, new SqlIdentifier(name, SqlParserPos.ZERO), SqlKind.OTHER_FUNCTION, DynamicReturnType.INSTANCE, null, new Checker(argCount), SqlFunctionCategory.USER_DEFINED_FUNCTION);\n   }\n \n   @Override",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/84fa4f10112dc81539c9bf0dcc62c49b7810971d/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlAggOperator.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "b074ba0d5ae874ff663210f9fb873eb9b7f2a1cd",
                "blob_url": "https://github.com/apache/drill/blob/84fa4f10112dc81539c9bf0dcc62c49b7810971d/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlAggOperator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlAggOperator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlAggOperator.java?ref=84fa4f10112dc81539c9bf0dcc62c49b7810971d"
            },
            {
                "patch": "@@ -23,6 +23,8 @@\n import org.eigenbase.sql.SqlFunction;\n import org.eigenbase.sql.SqlFunctionCategory;\n import org.eigenbase.sql.SqlKind;\n+import org.eigenbase.sql.SqlIdentifier;\n+import org.eigenbase.sql.parser.SqlParserPos;\n import org.eigenbase.sql.type.SqlTypeName;\n import org.eigenbase.sql.validate.SqlValidator;\n import org.eigenbase.sql.validate.SqlValidatorScope;\n@@ -31,7 +33,7 @@\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DrillSqlOperator.class);\n \n   DrillSqlOperator(String name, int argCount) {\n-    super(name, SqlKind.OTHER_FUNCTION, DynamicReturnType.INSTANCE, null, new Checker(argCount), SqlFunctionCategory.USER_DEFINED_FUNCTION);\n+    super(new SqlIdentifier(name, SqlParserPos.ZERO), DynamicReturnType.INSTANCE, null, new Checker(argCount), null, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n   }\n \n   @Override",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/84fa4f10112dc81539c9bf0dcc62c49b7810971d/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlOperator.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "1f39c6e4c99b454793d8ee26656879e19e84ecfc",
                "blob_url": "https://github.com/apache/drill/blob/84fa4f10112dc81539c9bf0dcc62c49b7810971d/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlOperator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlOperator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillSqlOperator.java?ref=84fa4f10112dc81539c9bf0dcc62c49b7810971d"
            }
        ],
        "bug_id": "drill_37",
        "parent": "https://github.com/apache/drill/commit/946bdb5cc1ef360318f9b29c072faff43f194ef6",
        "message": "DRILL-550: Fix NPE while using custom Drill functions",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/4a83dae35f8782a9737b31723769724f3b274894",
        "file": [
            {
                "patch": "@@ -134,10 +134,11 @@ public IterOutcome next() {\n         }\n       }\n       \n-      if (builder == null)\n+      if (builder == null){\n         // builder may be null at this point if the first incoming batch is empty\n         return IterOutcome.NONE;\n-\n+      }\n+        \n       builder.build(context);\n       sv4 = builder.getSv4();\n ",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/4a83dae35f8782a9737b31723769724f3b274894/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "78811156466b7bdf9d4dbfcdda2fbb8b225b7320",
                "blob_url": "https://github.com/apache/drill/blob/4a83dae35f8782a9737b31723769724f3b274894/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java?ref=4a83dae35f8782a9737b31723769724f3b274894"
            }
        ],
        "bug_id": "drill_38",
        "parent": "https://github.com/apache/drill/commit/366bf8e4564333dd26b1f5672d4dd0fea28afacc",
        "message": "DRILL-293: Sort operator throws NPE on empty record batch",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/4ccea48905cca4a822521c83ad2ff31eb62bfa06",
        "file": [
            {
                "patch": "@@ -341,7 +341,11 @@ private void scanAndAssign (Multimap<Integer, ParquetRowGroupScan.RowGroupReadEn\n \n           endpointAssignments.put(minorFragmentId, rowGroupInfo.getRowGroupReadEntry());\n           logger.debug(\"Assigned rowGroup {} to minorFragmentId {} endpoint {}\", rowGroupInfo.getRowGroupIndex(), minorFragmentId, endpoints.get(minorFragmentId).getAddress());\n-          assignmentAffinityStats.update(bytesPerEndpoint.get(currentEndpoint) / rowGroupInfo.getLength());\n+          if (bytesPerEndpoint.get(currentEndpoint) != null) {\n+            assignmentAffinityStats.update(bytesPerEndpoint.get(currentEndpoint) / rowGroupInfo.getLength());\n+          } else {\n+            assignmentAffinityStats.update(0);\n+          }\n           iter.remove();\n           fragmentPointer = (minorFragmentId + 1) % endpoints.size();\n           break;",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/4ccea48905cca4a822521c83ad2ff31eb62bfa06/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "10fcdffff75976c13aaf4a28204d1f84b48d1df4",
                "blob_url": "https://github.com/apache/drill/blob/4ccea48905cca4a822521c83ad2ff31eb62bfa06/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=4ccea48905cca4a822521c83ad2ff31eb62bfa06"
            }
        ],
        "bug_id": "drill_39",
        "parent": "https://github.com/apache/drill/commit/c93a5a1dc396fc8378da77b62a9e61314fbc98b4",
        "message": "DRILL-244: NPE when updating assignment affinity metric",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/f1746c92ffbd74ef9622af294768372fd1676459",
        "file": [
            {
                "patch": "@@ -43,9 +43,10 @@\n   DrillbitContext context;\n   \n   public ServiceEngine(DrillbitContext context){\n+    this.context = context;\n     ByteBufAllocator allocator = context.getAllocator().getUnderlyingAllocator();\n-    userServer = new UserServer(allocator, new NioEventLoopGroup(1, new NamedThreadFactory(\"UserServer-\")), context);\n-    bitCom = new BitComImpl(context);\n+    this.userServer = new UserServer(allocator, new NioEventLoopGroup(1, new NamedThreadFactory(\"UserServer-\")), context);\n+    this.bitCom = new BitComImpl(context);\n   }\n   \n   public void start() throws DrillbitStartupException, InterruptedException{",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/f1746c92ffbd74ef9622af294768372fd1676459/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/service/ServiceEngine.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "5d83bdb480d8f2c86a82e77b88d4dddb48c7f257",
                "blob_url": "https://github.com/apache/drill/blob/f1746c92ffbd74ef9622af294768372fd1676459/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/service/ServiceEngine.java",
                "filename": "sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/service/ServiceEngine.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/service/ServiceEngine.java?ref=f1746c92ffbd74ef9622af294768372fd1676459"
            },
            {
                "patch": "@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+PROJECT_ROOT=../../../\n+\n+mvn dependency:build-classpath -f=$PROJECT_ROOT/pom.xml -Dmdep.outputFile=target/sh/cp.txt\n+CP=`cat $PROJECT_ROOT/target/sh/cp.txt`\n+CP=$CP:$PROJECT_ROOT/target/classes:$PROJECT_ROOT/target/test-classes\n+java -cp $CP org.apache.drill.exec.server.Drillbit\n+",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/f1746c92ffbd74ef9622af294768372fd1676459/sandbox/prototype/exec/java-exec/src/test/sh/runbit",
                "status": "added",
                "changes": 9,
                "deletions": 0,
                "sha": "10fc1d5422b7c92d4b900df2771353a68be8e620",
                "blob_url": "https://github.com/apache/drill/blob/f1746c92ffbd74ef9622af294768372fd1676459/sandbox/prototype/exec/java-exec/src/test/sh/runbit",
                "filename": "sandbox/prototype/exec/java-exec/src/test/sh/runbit",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/sandbox/prototype/exec/java-exec/src/test/sh/runbit?ref=f1746c92ffbd74ef9622af294768372fd1676459"
            }
        ],
        "bug_id": "drill_40",
        "parent": "https://github.com/apache/drill/commit/f3b2019365dab6abed84810173593152c9fb3ffa",
        "message": "Fix NPE, add runbit script to start up Drilbit.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/1b96174b1e5bafb13a873dd79f03467802d7c929",
        "file": [
            {
                "patch": "@@ -98,7 +98,8 @@ public Reader getReader(int rowOffset) throws InvalidAccessException{\n \n   @Override\n   public String getString(int rowOffset) throws InvalidAccessException{\n-    return getObject(rowOffset).toString();\n+    Object o = getObject(rowOffset);\n+    return o != null ? o.toString() : null;\n   }\n \n   @Override",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "e24f39c3d4a2b466201a37dffafe7ef6f9fe45d2",
                "blob_url": "https://github.com/apache/drill/blob/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java?ref=1b96174b1e5bafb13a873dd79f03467802d7c929"
            },
            {
                "patch": "@@ -62,6 +62,7 @@\n \n   private static Connection connection;\n   private static ResultSet testDataRow;\n+  private static ResultSet testDataRowWithNulls;\n \n   @BeforeClass\n   public static void setUpConnectionAndMetadataToCheck() throws SQLException {\n@@ -99,6 +100,33 @@ public static void setUpConnectionAndMetadataToCheck() throws SQLException {\n         + \"\\nLIMIT 1 \" );\n     // Note: Assertions must be enabled (as they have been so far in tests).\n     assertTrue( testDataRow.next() );\n+\n+    final Statement stmtForNulls = connection.createStatement();\n+    testDataRowWithNulls = stmtForNulls.executeQuery(\n+        \"\"\n+            +   \"SELECT  \"\n+            + \"\\n\"\n+            + \"\\n  CAST(null as boolean)                  AS  C_BOOLEAN_TRUE, \"\n+            // TODO(DRILL-2470): Uncomment when TINYINT is implemented:\n+            //+ \"\\n  CAST(  null AS TINYINT            ) AS  C_TINYINT_1, \"\n+            // TODO(DRILL-2470): Uncomment when SMALLINT is implemented:\n+            //+ \"\\n  CAST(  null AS SMALLINT           ) AS  C_SMALLINT_2, \"\n+            + \"\\n  CAST(  null AS INTEGER            ) AS  C_INTEGER_3, \"\n+            + \"\\n  CAST(  null AS BIGINT             ) AS  C_BIGINT_4, \"\n+            // TODO(DRILL-2683): Uncomment when REAL is implemented:\n+            //+ \"\\n  CAST(  null AS REAL             ) AS `C_REAL_5.5`, \"\n+            + \"\\n  CAST(  null AS DOUBLE PRECISION ) AS `C_DOUBLE_PREC._6.6`, \"\n+            + \"\\n  CAST(  null AS FLOAT            ) AS `C_FLOAT_7.7`, \"\n+            + \"\\n  CAST( null AS DECIMAL         ) AS `C_DECIMAL_10.10`, \"\n+            + \"\\n  CAST( null  AS DECIMAL         ) AS `C_DECIMAL_10.5`, \"\n+            + \"\\n  CAST( null AS DECIMAL(9,2)    ) AS `C_DECIMAL(9,2)_11.11`, \"\n+            + \"\\n  CAST( null AS DECIMAL(18,2)   ) AS `C_DECIMAL(18,2)_12.12`, \"\n+            + \"\\n  CAST( null AS DECIMAL(28,2)   ) AS `C_DECIMAL(28,2)_13.13`, \"\n+            + \"\\n  CAST( null AS DECIMAL(38,2)   ) AS `C_DECIMAL(38,2)_14.14`, \"\n+            + \"\\n  '' \"\n+            + \"\\nFROM (VALUES(1))\" );\n+    // Note: Assertions must be enabled (as they have been so far in tests).\n+    assertTrue( testDataRowWithNulls.next() );\n   }\n \n   @AfterClass\n@@ -492,43 +520,51 @@ public void test_getBigDecimal_handles_DECIMAL_2() throws SQLException {\n   @Test\n   public void test_getString_handles_TINYINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_TINYINT_1\" ), equalTo( \"1\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_TINYINT_1\" ), equalTo( null ) );\n   }\n \n   @Ignore( \"TODO(DRILL-2470): unignore when SMALLINT is implemented\" )\n   @Test\n   public void test_getString_handles_SMALLINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_SMALLINT_2\" ), equalTo( \"2\" ) );\n+    assertThat( testDataRowWithNulls.getString(\"C_SMALLINT_2\"), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_INTEGER() throws SQLException {\n     assertThat( testDataRow.getString( \"C_INTEGER_3\" ), equalTo( \"3\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_INTEGER_3\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_BIGINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_BIGINT_4\" ), equalTo( \"4\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_BIGINT_4\" ), equalTo( null ) );\n   }\n \n   @Ignore( \"TODO(DRILL-2683): unignore when REAL is implemented\" )\n   @Test\n   public void test_getString_handles_REAL() throws SQLException {\n     assertThat( testDataRow.getString( \"C_REAL_5.5\" ), equalTo( \"5.5????\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_REAL_5.5\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_DOUBLE() throws SQLException {\n     assertThat( testDataRow.getString( \"C_DOUBLE_PREC._6.6\" ), equalTo( \"6.6\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_DOUBLE_PREC._6.6\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_FLOAT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_FLOAT_7.7\" ), equalTo( \"7.7\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_FLOAT_7.7\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_DECIMAL() throws SQLException {\n     assertThat( testDataRow.getString( \"C_DECIMAL_10.10\" ), equalTo( \"10.1\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_DECIMAL_10.10\" ), equalTo( null ) );\n   }\n \n ",
                "additions": 36,
                "raw_url": "https://github.com/apache/drill/raw/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "status": "modified",
                "changes": 36,
                "deletions": 0,
                "sha": "e0e5232ee8df01d28d77c3e4f07d93aae2879d9d",
                "blob_url": "https://github.com/apache/drill/blob/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java?ref=1b96174b1e5bafb13a873dd79f03467802d7c929"
            }
        ],
        "bug_id": "drill_41",
        "parent": "https://github.com/apache/drill/commit/9ff947288f3214fe8e525e001d89a4f91b8b0728",
        "message": "DRILL-4128: Fix NPE when calling getString on a JDBC ResultSet when the type is not varchar",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c",
        "file": [
            {
                "patch": "@@ -60,6 +60,9 @@ private void doTransfers(){\n   }\n \n   public void filterBatch(int recordCount){\n+    if (recordCount == 0) {\n+      return;\n+    }\n     if (! outgoingSelectionVector.allocateNew(recordCount)) {\n       throw new UnsupportedOperationException(\"Unable to allocate filter batch\");\n     }",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "26f2657826dc2fdc2d36530c0c04709a06cab752",
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c"
            },
            {
                "patch": "@@ -43,6 +43,9 @@ public void setup(FragmentContext context, RecordBatch incoming, RecordBatch out\n \n   @Override\n   public void filterBatch(int recordCount){\n+    if (recordCount == 0) {\n+      return;\n+    }\n     int outPos = 0;\n     for (int i = 0; i < incomingSelectionVector.getCount(); i++) {\n       int index = incomingSelectionVector.get(i);",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "fd1f9e68c04a121cf5d6f3f385ea00e90093387a",
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c"
            },
            {
                "patch": "@@ -504,4 +504,18 @@ public void testFilterInSubqueryAndOutside() throws Exception {\n     assertEquals(expectedRecordCount, actualRecordCount);\n   }\n \n+  @Test // DRILL-1973\n+  public void testLimit0SubqueryWithFilter() throws Exception {\n+    String query1 = \"select * from (select sum(1) as x from  cp.`tpch/region.parquet` limit 0) WHERE x < 10\";\n+    String query2 = \"select * from (select sum(1) as x from  cp.`tpch/region.parquet` limit 0) WHERE (0 = 1)\";\n+    int actualRecordCount = 0;\n+    int expectedRecordCount = 0;\n+\n+    actualRecordCount = testSql(query1);\n+    assertEquals(expectedRecordCount, actualRecordCount);\n+\n+    actualRecordCount = testSql(query2);\n+    assertEquals(expectedRecordCount, actualRecordCount);\n+  }\n+\n }",
                "additions": 14,
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "status": "modified",
                "changes": 14,
                "deletions": 0,
                "sha": "a3b5ff19b00cdf9fd24f8ce6dedac3d129171515",
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c"
            }
        ],
        "bug_id": "drill_42",
        "parent": "https://github.com/apache/drill/commit/952114fcdb9b819b546bd0283d30e56027c3f831",
        "message": "DRILL-1973: Filter should check for record count before doing evaluation. Fixes NPE.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/ac6e913bfaea4b1f453f4b307b456e20dc341dfa",
        "file": [
            {
                "patch": "@@ -124,10 +124,8 @@ public IterOutcome innerNext() {\n \n   private void doTransfer() {\n     outRecordCount = current.getRecordCount();\n-    // skip empty batches\n-    if (outRecordCount == 0) {\n-      return;\n-    }\n+    // If the batch is empty we still need to set up the outgoing vectors otherwise the downstream operators will get\n+    // a NPE. SEE DRILL-1886\n     if (container.getSchema().getSelectionVectorMode() == BatchSchema.SelectionVectorMode.TWO_BYTE) {\n       this.sv = current.getSelectionVector2();\n     }",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/ac6e913bfaea4b1f453f4b307b456e20dc341dfa/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "status": "modified",
                "changes": 6,
                "deletions": 4,
                "sha": "99aec9202b3cf716650e55e752262699f961596d",
                "blob_url": "https://github.com/apache/drill/blob/ac6e913bfaea4b1f453f4b307b456e20dc341dfa/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java?ref=ac6e913bfaea4b1f453f4b307b456e20dc341dfa"
            }
        ],
        "bug_id": "drill_43",
        "parent": "https://github.com/apache/drill/commit/e10c2197b3884c9e6879462e0e1057aacadfa47f",
        "message": "DRILL-1886: Project on subquery containing a union all causes NPE",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/e2f57b46b97b1831c21dbfa07fd831abedf2020e",
        "file": [
            {
                "patch": "@@ -86,12 +86,12 @@ public Viewable update(@PathParam(\"name\") String name) throws JsonProcessingExce\n     map.put(\"config\", conf);\n     map.put(\"name\", name);\n     map.put(\"exists\", config != null);\n-    map.put(\"enabled\", config.isEnabled());\n+    map.put(\"enabled\", config != null && config.isEnabled());\n     return new Viewable(\"/rest/storage/update.ftl\", map);\n   }\n \n   @GET\n-  @Path(\"/{name}/enable/{val}\")\n+  @Path(\"/{name}/config/enable/{val}\")\n   @Produces(MediaType.TEXT_HTML)\n   public Response setEnable(@Context UriInfo uriInfo, @PathParam(\"name\") String name, @PathParam(\"val\") Boolean enable) throws ExecutionSetupException {\n     StoragePluginConfig config = findConfig(name);\n@@ -104,6 +104,14 @@ public Response setEnable(@Context UriInfo uriInfo, @PathParam(\"name\") String na\n     return Response.seeOther(uri).build();\n   }\n \n+  @GET\n+  @Path(\"/{name}/config/delete\")\n+  @Produces(MediaType.TEXT_HTML)\n+  public Viewable deleteConfig(@PathParam(\"name\") String name) {\n+    storage.deletePlugin(name);\n+    return new Viewable(\"/rest/status.ftl\", \"Deleted \" + name);\n+  }\n+\n   @GET\n   @Produces(MediaType.APPLICATION_JSON)\n   @Path(\"/{name}/config\")\n@@ -126,22 +134,13 @@ private StoragePluginConfig findConfig(String name) {\n \n   @POST\n   @Path(\"/config/update\")\n-  @Produces(MediaType.APPLICATION_JSON)\n+  @Produces(MediaType.TEXT_HTML)\n   @Consumes(\"application/x-www-form-urlencoded\")\n-  public JsonResult createTrackInJSON(@FormParam(\"name\") String name, @FormParam(\"config\") String storagePluginConfig)\n+  public Viewable createTrackInJSON(@FormParam(\"name\") String name, @FormParam(\"config\") String storagePluginConfig)\n       throws ExecutionSetupException, JsonParseException, JsonMappingException, IOException {\n     StoragePluginConfig config = mapper.readValue(new StringReader(storagePluginConfig), StoragePluginConfig.class);\n     storage.createOrUpdate(name, config, true);\n-    return r(\"success\");\n-  }\n-\n-  @POST\n-  @Path(\"/config/delete\")\n-  @Produces(MediaType.APPLICATION_JSON)\n-  @Consumes(\"application/x-www-form-urlencoded\")\n-  public JsonResult deleteConfig(@FormParam(\"name\") String name) {\n-    storage.deletePlugin(name);\n-    return r(\"success\");\n+    return new Viewable(\"/rest/status.ftl\", \"Updated \" + name);\n   }\n \n   private JsonResult r(String message) {",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java",
                "status": "modified",
                "changes": 27,
                "deletions": 14,
                "sha": "17ea72ca54bb5c457c74a58bb14c3a20aaa0554c",
                "blob_url": "https://github.com/apache/drill/blob/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/StorageResources.java?ref=e2f57b46b97b1831c21dbfa07fd831abedf2020e"
            },
            {
                "patch": "@@ -29,9 +29,9 @@\n           <td style=\"border:none;\">\n             <a class=\"btn btn-primary\" href=\"/storage/${plugin.name}/config/update\">Update</a>\n             <#if plugin.enabled>\n-              <a class=\"btn btn-default\" href=\"/storage/${plugin.name}/enable/false\">Disable</a>\n+              <a class=\"btn btn-default\" href=\"/storage/${plugin.name}/config/enable/false\">Disable</a>\n             <#else>\n-              <a class=\"btn btn-primary\" href=\"/storage/${plugin.name}/enable/true\">Enable</a>\n+              <a class=\"btn btn-primary\" href=\"/storage/${plugin.name}/config/enable/true\">Enable</a>\n             </#if>\n           </td>\n         </tr>\n@@ -49,7 +49,7 @@\n         function doSubmit() {\n           var name = document.getElementById(\"storageName\");\n           var form = document.getElementById(\"newStorage\");\n-          form.action = \"/storage/\" + name.value + \"/config/update?\";\n+          form.action = \"/storage/\" + name.value + \"/config/update\";\n           form.submit();\n         }\n       </script>",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/resources/rest/storage/list.ftl",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "3636fbb0de86053a9cdf5ca681ad7d90bd7cdaab",
                "blob_url": "https://github.com/apache/drill/blob/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/resources/rest/storage/list.ftl",
                "filename": "exec/java-exec/src/main/resources/rest/storage/list.ftl",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/rest/storage/list.ftl?ref=e2f57b46b97b1831c21dbfa07fd831abedf2020e"
            },
            {
                "patch": "@@ -28,17 +28,12 @@\n       <#if model.exists >Update<#else>Create</#if>\n     </button>\n     <#if model.enabled>\n-      <a class=\"btn btn-default\" href=\"/storage/${model.name}/enable/false\">Disable</a>\n+      <a class=\"btn btn-default\" href=\"/storage/${model.name}/config/enable/false\">Disable</a>\n     <#else>\n-      <a class=\"btn btn-primary\" href=\"/storage/${model.name}/enable/true\">Enable</a>\n+      <a class=\"btn btn-primary\" href=\"/storage/${model.name}/config/enable/true\">Enable</a>\n     </#if>\n-    <#if model.exists >\n-      <form role=\"form\" action=\"/storage/config/delete\" method=\"POST\">\n-        <input type=\"hidden\" name=\"name\" value=\"${model.name}\" />\n-        <button type=\"submit\" class=\"btn btn-default\" onclick=\"return confirm('Are you sure?')\">\n-        Delete\n-        </button>\n-      </form>\n+    <#if model.exists>\n+      <a class=\"btn btn-danger\" href=\"/storage/${model.name}/config/delete\">Delete</a>\n     </#if>\n   </form>\n </#macro>",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/resources/rest/storage/update.ftl",
                "status": "modified",
                "changes": 13,
                "deletions": 9,
                "sha": "f4b9e1271027a3227ec59606ce61b7ff354dc6eb",
                "blob_url": "https://github.com/apache/drill/blob/e2f57b46b97b1831c21dbfa07fd831abedf2020e/exec/java-exec/src/main/resources/rest/storage/update.ftl",
                "filename": "exec/java-exec/src/main/resources/rest/storage/update.ftl",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/rest/storage/update.ftl?ref=e2f57b46b97b1831c21dbfa07fd831abedf2020e"
            }
        ],
        "bug_id": "drill_44",
        "parent": "https://github.com/apache/drill/commit/219e4fa3f2ec5346ab2480cd5fece220638e80b4",
        "message": "DRILL-995: Removed inner form (which is not allowed). Fixed NPE and URLs.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e",
        "file": [
            {
                "patch": "@@ -54,7 +54,6 @@ public static void main(String args[]) throws Exception {\n       System.out.println(e.getMessage());\n       String[] valid = {\"-f\", \"file\"};\n       new JCommander(o, valid).usage();\n-      jc.usage();\n       System.exit(-1);\n     }\n     if (o.help) {",
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "status": "modified",
                "changes": 1,
                "deletions": 1,
                "sha": "7e8a4a26dca4f8d621836672e24b3adc7492c967",
                "blob_url": "https://github.com/apache/drill/blob/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java?ref=ed51b3f95ee4eaa3694629b1181b9e8cd13b932e"
            }
        ],
        "bug_id": "drill_45",
        "parent": "https://github.com/apache/drill/commit/3f92e56aeb768cac90495951befaa0f03f13372e",
        "message": "DRILL-559: Fix NPE in dumpcat when arguments are not provided",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/f8f12df99399fb0097ac4c1388d5ccce9a36d48d",
        "file": [
            {
                "patch": "@@ -119,11 +119,13 @@ protected void getRegionInfos() throws IOException {\n     for (ServerName sn : regionsMap.values()) {\n       String host = sn.getHostname();\n       DrillbitEndpoint ep = endpointMap.get(host);\n-      EndpointAffinity affinity = affinityMap.get(ep);\n-      if (affinity == null) {\n-        affinityMap.put(ep, new EndpointAffinity(ep, 1));\n-      } else {\n-        affinity.addAffinity(1);\n+      if (ep != null) {\n+        EndpointAffinity affinity = affinityMap.get(ep);\n+        if (affinity == null) {\n+          affinityMap.put(ep, new EndpointAffinity(ep, 1));\n+        } else {\n+          affinity.addAffinity(1);\n+        }\n       }\n     }\n     this.endpointAffinities = Lists.newArrayList(affinityMap.values());\n@@ -151,12 +153,17 @@ public void applyAssignments(List<DrillbitEndpoint> incomingEndpoints) {\n       Iterator<Integer> ints = Iterators.cycle(incomingEndpointMap.get(s));\n       mapIterator.put(s, ints);\n     }\n+    Iterator<Integer> nullIterator = Iterators.cycle(incomingEndpointMap.values());\n     for (HRegionInfo regionInfo : regionsMap.keySet()) {\n       logger.debug(\"creating read entry. start key: {} end key: {}\", Bytes.toStringBinary(regionInfo.getStartKey()), Bytes.toStringBinary(regionInfo.getEndKey()));\n       HBaseSubScan.HBaseSubScanReadEntry p = new HBaseSubScan.HBaseSubScanReadEntry(\n           tableName, Bytes.toStringBinary(regionInfo.getStartKey()), Bytes.toStringBinary(regionInfo.getEndKey()));\n       String host = regionsMap.get(regionInfo).getHostname();\n-      mappings.put(mapIterator.get(host).next(), p);\n+      Iterator<Integer> indexIterator = mapIterator.get(host);\n+      if (indexIterator == null) {\n+        indexIterator = nullIterator;\n+      }\n+      mappings.put(indexIterator.next(), p);\n     }\n   }\n ",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/f8f12df99399fb0097ac4c1388d5ccce9a36d48d/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "status": "modified",
                "changes": 19,
                "deletions": 6,
                "sha": "bb0adcc97d0230894b0484b26279ef03a26fdb42",
                "blob_url": "https://github.com/apache/drill/blob/f8f12df99399fb0097ac4c1388d5ccce9a36d48d/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "filename": "contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java?ref=f8f12df99399fb0097ac4c1388d5ccce9a36d48d"
            }
        ],
        "bug_id": "drill_46",
        "parent": "https://github.com/apache/drill/commit/ed5d7c91a6eeded6a5b0aec713e58bd2c654a87b",
        "message": "DRILL-483: Fix NPE when scanning table region on node without drillbit",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/7506cfbb5c8522d371c12dbdc2268d48a9449a48",
        "file": [
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -37,7 +37,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -105,13 +104,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n         FileSelection selection, FileSystemPlugin fsPlugin,\n         String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "status": "modified",
                "changes": 11,
                "deletions": 9,
                "sha": "fd6e59b5f45e1b5c2562134e79a2b03edb137c68",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -77,13 +77,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n       FileSelection selection, FileSystemPlugin fsPlugin,\n       String storageEngineName, String userName) throws IOException {\n     if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-      if (plugin.getName() != null) {\n-        NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-        namedConfig.name = plugin.getName();\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(namedConfig, selection));\n-      } else {\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n-      }\n+      return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n     }\n     return null;\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "status": "modified",
                "changes": 8,
                "deletions": 7,
                "sha": "65260739cac90ac91640c767e667de2ac3ca6f18",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -37,7 +37,6 @@\n  */\n public class FileSelection {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(FileSelection.class);\n-  private static final String PATH_SEPARATOR = System.getProperty(\"file.separator\");\n   private static final String WILD_CARD = \"*\";\n \n   private List<FileStatus> statuses;\n@@ -224,7 +223,7 @@ private static String commonPathForFiles(final List<String> files) {\n     int shortest = Integer.MAX_VALUE;\n     for (int i = 0; i < total; i++) {\n       final Path path = new Path(files.get(i));\n-      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(PATH_SEPARATOR);\n+      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(Path.SEPARATOR);\n       shortest = Math.min(shortest, folders[i].length);\n     }\n \n@@ -247,7 +246,7 @@ private static String commonPathForFiles(final List<String> files) {\n   private static String buildPath(final String[] path, final int folderIndex) {\n     final StringBuilder builder = new StringBuilder();\n     for (int i=0; i<folderIndex; i++) {\n-      builder.append(path[i]).append(PATH_SEPARATOR);\n+      builder.append(path[i]).append(Path.SEPARATOR);\n     }\n     builder.deleteCharAt(builder.length()-1);\n     return builder.toString();",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "status": "modified",
                "changes": 5,
                "deletions": 3,
                "sha": "6aff1dd019f9734cfe91f7b27fe372bd58cdbbda",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -133,15 +133,7 @@ public StoragePluginConfig getConfig() {\n   public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection, List<SchemaPath> columns)\n       throws IOException {\n     FormatSelection formatSelection = selection.getWith(lpPersistance, FormatSelection.class);\n-    FormatPlugin plugin;\n-    if (formatSelection.getFormat() instanceof NamedFormatPluginConfig) {\n-      plugin = formatCreator.getFormatPluginByName( ((NamedFormatPluginConfig) formatSelection.getFormat()).name);\n-    } else {\n-      plugin = formatPluginsByConfig.get(formatSelection.getFormat());\n-    }\n-    if (plugin == null) {\n-      plugin = formatCreator.newFormatPlugin(formatSelection.getFormat());\n-    }\n+    FormatPlugin plugin = getFormatPlugin(formatSelection.getFormat());\n     return plugin.getGroupScan(userName, formatSelection.getSelection(), columns);\n   }\n \n@@ -154,12 +146,23 @@ public FormatPlugin getFormatPlugin(String name) {\n     return formatCreator.getFormatPluginByName(name);\n   }\n \n+  /**\n+   * If format plugin configuration is for named format plugin, will return format plugin from pre-loaded list by name.\n+   * For other cases will try to find format plugin by its configuration, if not present will attempt to create one.\n+   *\n+   * @param config format plugin configuration\n+   * @return format plugin for given configuration if found, null otherwise\n+   */\n   public FormatPlugin getFormatPlugin(FormatPluginConfig config) {\n     if (config instanceof NamedFormatPluginConfig) {\n       return formatCreator.getFormatPluginByName(((NamedFormatPluginConfig) config).name);\n-    } else {\n-      return formatPluginsByConfig.get(config);\n     }\n+\n+    FormatPlugin plugin = formatPluginsByConfig.get(config);\n+    if (plugin == null) {\n+      plugin = formatCreator.newFormatPlugin(config);\n+    }\n+    return plugin;\n   }\n \n   @Override",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "status": "modified",
                "changes": 27,
                "deletions": 12,
                "sha": "5d382fe376fdc410c536ef994c5bdd33d29df87a",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -251,7 +251,14 @@ public boolean isOptional() {\n \n     @Override\n     public TranslatableTable apply(List<Object> arguments) {\n-      return new DrillTranslatableTable(schema.getDrillTable(new TableInstance(sig, arguments)));\n+      DrillTable drillTable = schema.getDrillTable(new TableInstance(sig, arguments));\n+      if (drillTable == null) {\n+        throw UserException\n+            .validationError()\n+            .message(\"Unable to find table [%s] in schema [%s]\", sig.name, schema.getFullSchemaName())\n+            .build(logger);\n+      }\n+      return new DrillTranslatableTable(drillTable);\n     }\n \n   }",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "status": "modified",
                "changes": 9,
                "deletions": 1,
                "sha": "6629fc4e2ec8a1f7a12edaa02b9e40ac13af3372",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -26,7 +26,6 @@\n import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractSubScan;\n import org.apache.drill.exec.store.StoragePluginRegistry;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.schedule.CompleteFileWork.FileWorkImpl;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n@@ -94,13 +93,7 @@ public StoragePluginConfig getStorageConfig(){\n \n   @JsonProperty(\"format\")\n   public FormatPluginConfig getFormatConfig(){\n-    if (formatPlugin.getName() != null) {\n-      NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-      namedConfig.name = formatPlugin.getName();\n-      return namedConfig;\n-    } else {\n-      return formatPlugin.getConfig();\n-    }\n+    return formatPlugin.getConfig();\n   }\n \n   @JsonProperty(\"columns\")",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "status": "modified",
                "changes": 11,
                "deletions": 9,
                "sha": "a6af1ac84894c81a74dfec9a92031864932ca228",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -1,21 +1,26 @@\n-\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n- * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n- * License. You may obtain a copy of the License at\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n  * http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n- * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n- * specific language governing permissions and limitations under the License.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.drill.exec.store.httpd;\n \n import java.io.IOException;\n import java.util.List;\n \n+import com.fasterxml.jackson.annotation.JsonInclude;\n import nl.basjes.parse.core.exceptions.DissectionFailure;\n import nl.basjes.parse.core.exceptions.InvalidDissectorException;\n import nl.basjes.parse.core.exceptions.MissingDissectorsException;\n@@ -73,11 +78,11 @@ public HttpdLogFormatPlugin(final String name, final DrillbitContext context, fi\n    * This class is a POJO to hold the configuration for the HttpdLogFormat Parser. This is automatically\n    * serialized/deserialized from JSON format.\n    */\n-  @JsonTypeName(PLUGIN_EXTENSION)\n+  @JsonTypeName(PLUGIN_EXTENSION) @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n   public static class HttpdLogFormatConfig implements FormatPluginConfig {\n \n-    private String logFormat;\n-    private String timestampFormat;\n+    public String logFormat;\n+    public String timestampFormat;\n \n     /**\n      * @return the logFormat\n@@ -92,6 +97,30 @@ public String getLogFormat() {\n     public String getTimestampFormat() {\n       return timestampFormat;\n     }\n+\n+    @Override\n+    public int hashCode() {\n+      int result = logFormat != null ? logFormat.hashCode() : 0;\n+      result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n+      return result;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+\n+      HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n+\n+      if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n+        return false;\n+      }\n+      return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n+    }\n   }\n \n   /**\n@@ -119,7 +148,7 @@ public HttpdLogRecordReader(final FragmentContext context, final DrillFileSystem\n      * The query fields passed in are formatted in a way that Drill requires. Those must be cleaned up to work with the\n      * parser.\n      *\n-     * @return Map<DrillFieldNames, ParserFieldNames>\n+     * @return Map with Drill field names as a key and Parser Field names as a value\n      */\n     private Map<String, String> makeParserFields() {\n       final Map<String, String> fieldMapping = Maps.newHashMap();",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "status": "modified",
                "changes": 55,
                "deletions": 13,
                "sha": "cee9a89247ce3d8ec128c735d60154949c96567c",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -164,9 +164,6 @@\n   ) throws IOException, ExecutionSetupException {\n     super(ImpersonationUtil.resolveUserName(userName));\n     this.columns = columns;\n-    if (formatConfig == null) {\n-      formatConfig = new ParquetFormatConfig();\n-    }\n     Preconditions.checkNotNull(storageConfig);\n     Preconditions.checkNotNull(formatConfig);\n     this.formatPlugin = (ParquetFormatPlugin) engineRegistry.getFormatPlugin(storageConfig, formatConfig);\n@@ -345,6 +342,7 @@ public boolean hasFiles() {\n     return true;\n   }\n \n+  @JsonIgnore\n   @Override\n   public Collection<String> getFiles() {\n     return fileSet;",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "status": "modified",
                "changes": 4,
                "deletions": 3,
                "sha": "75b18df072afcbc85b7b0f1287ccb663e8f5c2fc",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -65,9 +65,12 @@ public ParquetRowGroupScan( //\n       @JsonProperty(\"selectionRoot\") String selectionRoot, //\n       @JsonProperty(\"filter\") LogicalExpression filter\n   ) throws ExecutionSetupException {\n-    this(userName, (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig),\n-            formatConfig == null ? new ParquetFormatConfig() : formatConfig),\n-        rowGroupReadEntries, columns, selectionRoot, filter);\n+    this(userName,\n+        (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig), Preconditions.checkNotNull(formatConfig)),\n+        rowGroupReadEntries,\n+        columns,\n+        selectionRoot,\n+        filter);\n   }\n \n   public ParquetRowGroupScan( //\n@@ -79,7 +82,7 @@ public ParquetRowGroupScan( //\n       LogicalExpression filter\n   ) {\n     super(userName);\n-    this.formatPlugin = Preconditions.checkNotNull(formatPlugin);\n+    this.formatPlugin = Preconditions.checkNotNull(formatPlugin, \"Could not find format config for the given configuration\");\n     this.formatConfig = formatPlugin.getConfig();\n     this.rowGroupReadEntries = rowGroupReadEntries;\n     this.columns = columns == null ? GroupScan.ALL_COLUMNS : columns;\n@@ -97,6 +100,14 @@ public StoragePluginConfig getEngineConfig() {\n     return formatPlugin.getStorageConfig();\n   }\n \n+  /**\n+   * @return Parquet plugin format config\n+   */\n+  @JsonProperty(\"format\")\n+  public ParquetFormatConfig getFormatConfig() {\n+    return formatConfig;\n+  }\n+\n   public String getSelectionRoot() {\n     return selectionRoot;\n   }\n@@ -140,11 +151,4 @@ public int getOperatorType() {\n     return CoreOperatorType.PARQUET_ROW_GROUP_SCAN_VALUE;\n   }\n \n-  /**\n-   * @return Parquet plugin format config\n-   */\n-  public ParquetFormatConfig getFormatConfig() {\n-    return formatConfig;\n-  }\n-\n }",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "status": "modified",
                "changes": 28,
                "deletions": 12,
                "sha": "f1fb1e9207f86e23e426e214d1e8f6ec159b7007",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -21,4 +21,14 @@\n \n @JsonTypeName(\"pcap\")\n public class PcapFormatConfig implements FormatPluginConfig {\n+\n+  @Override\n+  public int hashCode() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public boolean equals(Object obj) {\n+    return obj instanceof PcapFormatConfig;\n+  }\n }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "89b56adadbab2232558a1aef3c4756dc1ee969c0",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -99,13 +98,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n                                  FileSelection selection, FileSystemPlugin fsPlugin,\n                                  String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "status": "modified",
                "changes": 9,
                "deletions": 8,
                "sha": "65ff2388ac30c8e93b4f453d0cb93d0962fb5020",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -281,6 +281,19 @@ public static void testRelLogicalPlanLevExplain(String sql, String... expectedSu\n     }\n   }\n \n+\n+  /**\n+   * Creates physical plan for the given query and then executes this plan.\n+   * This method is useful for testing serialization / deserialization issues.\n+   *\n+   * @param query query string\n+   */\n+  public static void testPhysicalPlanExecutionBasedOnQuery(String query) throws Exception {\n+    query = \"EXPLAIN PLAN for \" + QueryTestUtil.normalizeQuery(query);\n+    String plan = getPlanInString(query, JSON_FORMAT);\n+    testPhysical(plan);\n+  }\n+\n   /*\n    * This will get the plan (either logical or physical) in Optiq RelNode\n    * format, based on SqlExplainLevel and Depth.",
                "additions": 13,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "status": "modified",
                "changes": 13,
                "deletions": 0,
                "sha": "22b734b2746718f39900f3522ddcced5fdc7d675",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -19,12 +19,15 @@\n \n import static java.lang.String.format;\n import static org.apache.drill.test.TestBuilder.listOf;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertThat;\n \n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n \n import org.apache.drill.categories.SqlTest;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n import org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.test.TestBuilder;\n@@ -276,4 +279,16 @@ public void testUse() throws Exception {\n       test(\"use sys\");\n     }\n   }\n+\n+  @Test(expected = UserRemoteException.class)\n+  public void testAbsentTable() throws Exception {\n+    String schema = \"cp.default\";\n+    String tableName = \"absent_table\";\n+    try {\n+      test(\"select * from table(`%s`.`%s`(type=>'parquet'))\", schema, tableName);\n+    } catch (UserRemoteException e) {\n+      assertThat(e.getMessage(), containsString(String.format(\"Unable to find table [%s] in schema [%s]\", tableName, schema)));\n+      throw e;\n+    }\n+  }\n }",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "status": "modified",
                "changes": 17,
                "deletions": 1,
                "sha": "a6dff740bc1009de3f2c9fab65c8c245562a7ef2",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -177,7 +177,7 @@ public void run() {\n     }\n   }\n \n-  @Test\n+  //@Test\n   public void testConcurrentQueries() throws Exception {\n     QueryTestUtil.testRunAndPrint(client, UserBitShared.QueryType.SQL, alterSession);\n ",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "f096d558aef6c04c36f326e8567cecc208e62f9b",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            },
            {
                "patch": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store;\n+\n+import org.apache.drill.PlanTestBase;\n+import org.apache.drill.exec.store.avro.AvroTestUtil;\n+import org.junit.Test;\n+\n+import java.nio.file.Paths;\n+\n+public class FormatPluginSerDeTest extends PlanTestBase {\n+\n+  @Test\n+  public void testParquet() throws Exception {\n+    test(\"alter session set `planner.slice_target` = 1\");\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet'))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>false))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>true))\", \"parquet/alltypes_required.parquet\")\n+    );\n+  }\n+\n+  @Test\n+  public void testAvro() throws Exception {\n+    AvroTestUtil.AvroTestRecordWriter testSetup = AvroTestUtil.generateSimplePrimitiveSchema_NoNullValues(5);\n+    String file = testSetup.getFileName();\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", file),\n+        String.format(\"select * from table(dfs.`%s`(type=>'avro'))\", file)\n+    );\n+  }\n+\n+  @Test\n+  public void testSequenceFile() throws Exception {\n+    String path = \"sequencefiles/simple.seq\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'sequencefile'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testPcap() throws Exception {\n+    String path = \"store/pcap/tcp-1.pcap\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'pcap'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testHttpd() throws Exception {\n+    String path = \"store/httpd/dfs-bootstrap.httpd\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String logFormat = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n+    String timeStampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s'))\", path, logFormat),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s', timestampFormat=>'%s'))\", path, logFormat, timeStampFormat)\n+    );\n+  }\n+\n+  @Test\n+  public void testJson() throws Exception {\n+    testPhysicalPlanSubmission(\n+        \"select * from cp.`donuts.json`\",\n+        \"select * from table(cp.`donuts.json`(type=>'json'))\"\n+    );\n+  }\n+\n+  @Test\n+  public void testText() throws Exception {\n+    String path = \"store/text/data/regions.csv\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(dfs.`%s`(type => 'text'))\", path),\n+        String.format(\"select * from table(dfs.`%s`(type => 'text', extractHeader => false, fieldDelimiter => 'A'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testNamed() throws Exception {\n+    String path = \"store/text/WithQuote.tbl\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String query = String.format(\"select * from table(dfs.`%s`(type=>'named', name=>'psv'))\", path);\n+    testPhysicalPlanSubmission(query);\n+  }\n+\n+  private void testPhysicalPlanSubmission(String...queries) throws Exception {\n+    for (String query : queries) {\n+      PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(query);\n+    }\n+  }\n+\n+}\n+",
                "additions": 113,
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "status": "added",
                "changes": 113,
                "deletions": 0,
                "sha": "ca81eaa3e82e601d80c1d2f29892b54c68696e83",
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48"
            }
        ],
        "bug_id": "drill_47",
        "parent": "https://github.com/apache/drill/commit/d4c61cadbe5c7d88fd4393cc1b29648fbadfb9f1",
        "message": "DRILL-5771: Fix serDe errors for format plugins\n\n1. Fix various serde issues for format plugins described in DRILL-5771.\n2. Throw meaninful exception instead of NPE when table is not found when table function is used.\n3. Added unit tests for all format plugins for ensure serde is checked (physical plan is generated in json format and then submitted).\n4. Fix physical plan submission on Windows (DRILL-4640).\n\nThis closes #1014",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/366bf8e4564333dd26b1f5672d4dd0fea28afacc",
        "file": [
            {
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.cache.VectorAccessibleSerializable;\n import org.apache.drill.exec.exception.SchemaChangeException;\n+import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.exec.expr.TypeHelper;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.ops.FragmentContext;\n@@ -79,7 +80,7 @@\n   /* File descriptors needed to be able to dump to log file */\n   private OutputStream fos;\n \n-  public TraceRecordBatch(Trace pop, RecordBatch incoming, FragmentContext context) {\n+  public TraceRecordBatch(Trace pop, RecordBatch incoming, FragmentContext context) throws ExecutionSetupException {\n     super(pop, context, incoming);\n     this.traceTag = pop.traceTag;\n     logLocation = context.getConfig().getString(ExecConstants.TRACE_DUMP_DIRECTORY);\n@@ -95,7 +96,7 @@ public TraceRecordBatch(Trace pop, RecordBatch incoming, FragmentContext context\n       /* create the file */\n       fos = fs.create(new Path(fileName));\n     } catch (IOException e) {\n-      logger.error(\"Unable to create file: \" + fileName);\n+        throw new ExecutionSetupException(\"Unable to create file: \" + fileName + \" check permissions or if directory exists\", e);\n     }\n   }\n ",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/366bf8e4564333dd26b1f5672d4dd0fea28afacc/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "c138ababb655b576e61a683ab04496d8c7d8c42b",
                "blob_url": "https://github.com/apache/drill/blob/366bf8e4564333dd26b1f5672d4dd0fea28afacc/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java?ref=366bf8e4564333dd26b1f5672d4dd0fea28afacc"
            }
        ],
        "bug_id": "drill_48",
        "parent": "https://github.com/apache/drill/commit/316ce8a6f8f94c31574e7107be26addcfc92dc7f",
        "message": "DRILL-297: Trace operator throws NPE if the configured path is not writable",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/93121cbf168f63881ad93d126e2fd9306a51f64a",
        "file": [
            {
                "patch": "@@ -79,9 +79,12 @@ public void incRecordCount() {\n   \n   public void flush() throws SchemaChangeException {\n     if (recordCount == 0) {\n-      logger.warn(\"Attempted to flush an empty record batch\");\n+      // TODO:  recordCount of 0 with isLast causes recordLoader to throw an NPE.  Probably\n+      //        need to send notification rather than an actual batch.\n+      logger.warn(\"Attempted to flush an empty record batch\" + (isLast ? \" (last batch)\" : \"\"));\n+      return;\n     }\n-    logger.debug(\"Flushing record batch.  count is: \" + recordCount + \", capacity is \" + recordCapacity);\n+\n     final ExecProtos.FragmentHandle handle = context.getHandle();\n     FragmentWritableBatch writableBatch = new FragmentWritableBatch(isLast,\n                                                                     handle.getQueryId(),\n@@ -90,7 +93,7 @@ public void flush() throws SchemaChangeException {\n                                                                     operator.getOppositeMajorFragmentId(),\n                                                                     0,\n                                                                     getWritableBatch());\n-     tunnel.sendRecordBatch(statusHandler, context, writableBatch);\n+    tunnel.sendRecordBatch(statusHandler, context, writableBatch);\n \n     // reset values and reallocate the buffer for each value vector.  NOTE: the value vector is directly\n     // referenced by generated code and must not be replaced.",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/93121cbf168f63881ad93d126e2fd9306a51f64a/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/OutgoingRecordBatch.java",
                "status": "modified",
                "changes": 9,
                "deletions": 3,
                "sha": "6eff778c81b7c479a5927c318f6ab9a56157ca04",
                "blob_url": "https://github.com/apache/drill/blob/93121cbf168f63881ad93d126e2fd9306a51f64a/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/OutgoingRecordBatch.java",
                "filename": "sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/OutgoingRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/OutgoingRecordBatch.java?ref=93121cbf168f63881ad93d126e2fd9306a51f64a"
            },
            {
                "patch": "@@ -86,15 +86,20 @@ public boolean next() {\n         if (incoming.getRecordCount() > 0)\n           partitioner.partitionBatch(incoming);\n \n-        // send all pending batches\n         try {\n-          flushOutgoingBatches(true, false);\n+          // send any pending batches\n+          for (OutgoingRecordBatch batch : outgoing) {\n+            batch.setIsLast();\n+            batch.flush();\n+          }\n         } catch (SchemaChangeException e) {\n           incoming.kill();\n           logger.error(\"Error while creating partitioning sender or flushing outgoing batches\", e);\n           context.fail(e);\n           return false;\n         }\n+        context.batchesCompleted.inc(1);\n+        context.recordsCompleted.inc(incoming.getRecordCount());\n         return false;\n \n       case OK_NEW_SCHEMA:\n@@ -111,6 +116,8 @@ public boolean next() {\n         }\n       case OK:\n         partitioner.partitionBatch(incoming);\n+        context.batchesCompleted.inc(1);\n+        context.recordsCompleted.inc(incoming.getRecordCount());\n         return true;\n       case NOT_YET:\n       default:",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/93121cbf168f63881ad93d126e2fd9306a51f64a/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "status": "modified",
                "changes": 11,
                "deletions": 2,
                "sha": "293a7116f7b471a0769c09bc12adf3f110267331",
                "blob_url": "https://github.com/apache/drill/blob/93121cbf168f63881ad93d126e2fd9306a51f64a/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "filename": "sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/sandbox/prototype/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java?ref=93121cbf168f63881ad93d126e2fd9306a51f64a"
            }
        ],
        "bug_id": "drill_49",
        "parent": "https://github.com/apache/drill/commit/a136a5bf53f87ac256d311fbbe6f1a373d193b1f",
        "message": "prevent NPE in recordLoader.  still need to handle the last batch correctly.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/83d460cfebbed8d70f4363dd21c95f1636f7472e",
        "file": [
            {
                "patch": "@@ -74,14 +74,8 @@ private static void patchCloseables() throws Exception {\n     ClassPool cp = ClassPool.getDefault();\n     CtClass cc = cp.get(\"com.google.common.io.Closeables\");\n \n-    // Expose the constructor for Stopwatch for old libraries who use the pattern new Stopwatch().start().\n-    for (CtConstructor c : cc.getConstructors()) {\n-      if (!Modifier.isStatic(c.getModifiers())) {\n-        c.setModifiers(Modifier.PUBLIC);\n-      }\n-    }\n \n-    // Add back the Stopwatch.elapsedMillis() method for old consumers.\n+    // Add back the Closeables.closeQuietly() method for old consumers.\n     CtMethod newmethod = CtNewMethod.make(\n         \"public static void closeQuietly(java.io.Closeable closeable) { try{closeable.close();}catch(Exception e){} }\",\n         cc);",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/83d460cfebbed8d70f4363dd21c95f1636f7472e/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/GuavaPatcher.java",
                "status": "modified",
                "changes": 8,
                "deletions": 7,
                "sha": "8f24da8020c9cbb79b6caac9626f7bfc75ef1636",
                "blob_url": "https://github.com/apache/drill/blob/83d460cfebbed8d70f4363dd21c95f1636f7472e/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/GuavaPatcher.java",
                "filename": "contrib/storage-hbase/src/test/java/org/apache/drill/hbase/GuavaPatcher.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/GuavaPatcher.java?ref=83d460cfebbed8d70f4363dd21c95f1636f7472e"
            },
            {
                "patch": "@@ -39,11 +39,6 @@\n import org.apache.drill.exec.rpc.RpcOutcomeListener;\n import org.apache.drill.exec.rpc.control.Controller.CustomSerDe;\n \n-import com.dyuproject.protostuff.JsonIOUtil;\n-import com.dyuproject.protostuff.LinkedBuffer;\n-import com.dyuproject.protostuff.ProtobufIOUtil;\n-import com.dyuproject.protostuff.ProtostuffIOUtil;\n-import com.dyuproject.protostuff.Schema;\n import com.fasterxml.jackson.core.JsonProcessingException;\n import com.fasterxml.jackson.databind.JsonDeserializer;\n import com.fasterxml.jackson.databind.JsonSerializer;\n@@ -435,41 +430,4 @@ public MSG deserializeReceived(byte[] bytes) throws Exception {\n \n   }\n \n-  public static class ProtostuffBinarySerDe<MSG extends com.dyuproject.protostuff.Message<MSG>> implements\n-      CustomSerDe<MSG> {\n-    private Schema<MSG> schema;\n-\n-    @Override\n-    public byte[] serializeToSend(MSG send) {\n-      final LinkedBuffer buffer = LinkedBuffer.allocate(512);\n-      return ProtostuffIOUtil.toByteArray(send, schema, buffer);\n-    }\n-\n-    @Override\n-    public MSG deserializeReceived(byte[] bytes) throws Exception {\n-      MSG msg = schema.newMessage();\n-      ProtobufIOUtil.mergeFrom(bytes, msg, schema);\n-      return msg;\n-    }\n-\n-  }\n-\n-  public static class ProtostuffJsonSerDe<MSG extends com.dyuproject.protostuff.Message<MSG>> implements\n-      CustomSerDe<MSG> {\n-    private Schema<MSG> schema;\n-\n-    @Override\n-    public byte[] serializeToSend(MSG send) {\n-      final LinkedBuffer buffer = LinkedBuffer.allocate(512);\n-      return JsonIOUtil.toByteArray(send, schema, false, buffer);\n-    }\n-\n-    @Override\n-    public MSG deserializeReceived(byte[] bytes) throws Exception {\n-      MSG msg = schema.newMessage();\n-      JsonIOUtil.mergeFrom(bytes, msg, schema, false);\n-      return msg;\n-    }\n-\n-  }\n }",
                "additions": 0,
                "raw_url": "https://github.com/apache/drill/raw/83d460cfebbed8d70f4363dd21c95f1636f7472e/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/control/ControlTunnel.java",
                "status": "modified",
                "changes": 42,
                "deletions": 42,
                "sha": "9b46a7a3406376a6abbaffebf04d4a51bdfb9c85",
                "blob_url": "https://github.com/apache/drill/blob/83d460cfebbed8d70f4363dd21c95f1636f7472e/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/control/ControlTunnel.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/rpc/control/ControlTunnel.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/control/ControlTunnel.java?ref=83d460cfebbed8d70f4363dd21c95f1636f7472e"
            },
            {
                "patch": "@@ -284,7 +284,9 @@ public ProtobufLengthDecoder getDecoder(BufferAllocator allocator, OutOfMemoryHa\n   @Override\n   public void close() throws IOException {\n     try {\n-      authenticator.close();\n+      if (authenticator != null) {\n+        authenticator.close();\n+      }\n     } catch (Exception e) {\n       logger.warn(\"Failure closing authenticator.\", e);\n     }",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/83d460cfebbed8d70f4363dd21c95f1636f7472e/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/UserServer.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "8ad880a1fbc6745aaaed0f76477a2420653db801",
                "blob_url": "https://github.com/apache/drill/blob/83d460cfebbed8d70f4363dd21c95f1636f7472e/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/UserServer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/UserServer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/rpc/user/UserServer.java?ref=83d460cfebbed8d70f4363dd21c95f1636f7472e"
            }
        ],
        "bug_id": "drill_50",
        "parent": "https://github.com/apache/drill/commit/2311843ba32cca7cdb401fbf1243a972586ba3f8",
        "message": "DRILL-4358: Fix NPE in UserServer.close()\n\n- Also remove untested CustomSerDe's from CustomTunnel.\n- Fix GuavaPatcher copy-paste comment mistake.\n\ncloses #362",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/894037ab693dea425e88fb3ec3aff73ea5b15eb1",
        "file": [
            {
                "patch": "@@ -66,41 +66,44 @@ private OperatorStats(int operatorId, int operatorType, int inputCount) {\n     this.schemaCountByInput = new long[inputCount];\n   }\n \n+  private String assertionError(String msg){\n+    return String.format(\"Failure while %s for operator id %d. Currently have states of processing:%s, setup:%s, waiting:%s.\", msg, operatorId, inProcessing, inSetup, inWait);\n+  }\n   public void startSetup() {\n-    assert !inSetup  : \"Failure while starting setup.  Currently in setup.\";\n+    assert !inSetup  : assertionError(\"starting setup\");\n     stopProcessing();\n     inSetup = true;\n     setupMark = System.nanoTime();\n   }\n \n   public void stopSetup() {\n-    assert inSetup :  \"Failure while stopping setup.  Not currently in setup.\";\n+    assert inSetup :  assertionError(\"stopping setup\");\n     startProcessing();\n     setupNanos += System.nanoTime() - setupMark;\n     inSetup = false;\n   }\n \n   public void startProcessing() {\n-    assert !inProcessing : \"Failure while starting processing.  Currently in processing.\";\n+    assert !inProcessing : assertionError(\"starting processing\");\n     processingMark = System.nanoTime();\n     inProcessing = true;\n   }\n \n   public void stopProcessing() {\n-    assert inProcessing : \"Failure while stopping processing.  Not currently in processing.\";\n+    assert inProcessing : assertionError(\"stopping processing\");\n     processingNanos += System.nanoTime() - processingMark;\n     inProcessing = false;\n   }\n \n   public void startWait() {\n-    assert !inWait : \"Failure while starting waiting.  Currently in waiting.\";\n+    assert !inWait : assertionError(\"starting waiting\");\n     stopProcessing();\n     inWait = true;\n     waitMark = System.nanoTime();\n   }\n \n   public void stopWait() {\n-    assert inWait : \"Failure while stopping waiting.  Currently not in waiting.\";\n+    assert inWait : assertionError(\"stopping waiting\");\n     startProcessing();\n     waitNanos += System.nanoTime() - waitMark;\n     inWait = false;",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "status": "modified",
                "changes": 15,
                "deletions": 6,
                "sha": "dcb73c82353c57f5a7e7e71d903e2b0a2a54c3f6",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -156,9 +156,10 @@ protected void setupNewSchema() throws Exception {\n       // update the schema in RecordWriter\n       stats.startSetup();\n       recordWriter.updateSchema(incoming.getSchema());\n-      stats.stopSetup();\n     } catch(IOException ex) {\n       throw new RuntimeException(\"Failed to update schema in RecordWriter\", ex);\n+    } finally{\n+      stats.stopSetup();\n     }\n \n     eventBasedRecordWriter = new EventBasedRecordWriter(incoming.getSchema(),",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "43e0dd4c78ef3c0e5e8ea0d51f6a6952f7ef5202",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -154,14 +154,14 @@ private boolean createAggregator() {\n     try{\n       stats.startSetup();\n       this.aggregator = createAggregatorInternal();\n-      stats.stopSetup();\n       return true;\n     }catch(SchemaChangeException | ClassTransformationException | IOException ex){\n-      stats.stopSetup();\n       context.fail(ex);\n       container.clear();\n       incoming.kill();\n       return false;\n+    }finally{\n+      stats.stopSetup();\n     }\n   }\n ",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "dd58562df63fe5e374f963b298ce0e5b0f40235d",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -136,13 +136,14 @@ private boolean createAggregator() {\n     try{\n       stats.startSetup();\n       this.aggregator = createAggregatorInternal();\n-      stats.stopSetup();\n       return true;\n     }catch(SchemaChangeException | ClassTransformationException | IOException ex){\n       context.fail(ex);\n       container.clear();\n       incoming.kill();\n       return false;\n+    }finally{\n+      stats.stopSetup();\n     }\n   }\n ",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "ec12de9f2714b6b3e1e7e341d7062051cb0c542f",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -134,8 +134,8 @@\n     boolean firstOutputBatch = true;\n \n     IterOutcome leftUpstream = IterOutcome.NONE;\n-    \n-    private HashTableStats htStats = new HashTableStats();\n+\n+    private final HashTableStats htStats = new HashTableStats();\n \n     @Override\n     public int getRecordCount() {\n@@ -171,9 +171,9 @@ public IterOutcome innerNext() {\n                 // Create the run time generated code needed to probe and project\n                 hashJoinProbe = setupHashJoinProbe();\n             }\n-                        \n+\n             // Store the number of records projected\n-            if (hashTable != null \n+            if (hashTable != null\n                 || joinType != JoinRelType.INNER) {\n \n                 // Allocate the memory for the vectors in the output container\n@@ -440,12 +440,13 @@ public HashJoinBatch(HashJoinPOP popConfig, FragmentContext context, RecordBatch\n     }\n \n     private void updateStats(HashTable htable) {\n+      if(htable == null) return;\n       htable.getStats(htStats);\n       this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_BUCKETS, htStats.numBuckets);\n       this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_ENTRIES, htStats.numEntries);\n-      this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_RESIZING, htStats.numResizing);  \n+      this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_RESIZING, htStats.numResizing);\n     }\n-    \n+\n     @Override\n     public void killIncoming() {\n         this.left.kill();",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "status": "modified",
                "changes": 13,
                "deletions": 6,
                "sha": "c43b99a784e8b97ea956a7c436e683cc8db3788d",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -166,12 +166,12 @@ public IterOutcome innerNext() {\n           stats.startSetup();\n           this.worker = generateNewWorker();\n           first = true;\n-          stats.stopSetup();\n         } catch (ClassTransformationException | IOException | SchemaChangeException e) {\n-          stats.stopSetup();\n           context.fail(new SchemaChangeException(e));\n           kill();\n           return IterOutcome.STOP;\n+        } finally {\n+          stats.stopSetup();\n         }\n       }\n ",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "e32b653a07eafc1259d4997f5a994bf8d60ea17a",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            },
            {
                "patch": "@@ -66,9 +66,13 @@ public final IterOutcome next(RecordBatch b) {\n   }\n \n   public final IterOutcome next(int inputIndex, RecordBatch b){\n+    IterOutcome next = null;\n     stats.stopProcessing();\n-    IterOutcome next = b.next();\n-    stats.startProcessing();\n+    try{\n+      next = b.next();\n+    }finally{\n+      stats.startProcessing();\n+    }\n \n     switch(next){\n     case OK_NEW_SCHEMA:\n@@ -138,7 +142,7 @@ public WritableBatch getWritableBatch() {\n     return batch;\n \n   }\n-  \n+\n   @Override\n   public VectorContainer getOutgoingContainer() {\n     throw new UnsupportedOperationException(String.format(\" You should not call getOutgoingContainer() for class %s\", this.getClass().getCanonicalName()));",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "status": "modified",
                "changes": 10,
                "deletions": 3,
                "sha": "4c1f82d037d7e50c3f3050b9a21e18811147d245",
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1"
            }
        ],
        "bug_id": "drill_51",
        "parent": "https://github.com/apache/drill/commit/28992889090c000104bd549230f8a7fe4ee0558c",
        "message": "Improve OperatorStats to avoid leaking state.  Fix issue where HashJoinBatch throws NPE in stats tracking if we don't have HashTable.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/2f13c08f35152639e4619d2898b2ca8fe7115259",
        "file": [
            {
                "patch": "@@ -144,6 +144,7 @@\n   String UDF_DIRECTORY_STAGING = \"drill.exec.udf.directory.staging\";\n   String UDF_DIRECTORY_REGISTRY = \"drill.exec.udf.directory.registry\";\n   String UDF_DIRECTORY_TMP = \"drill.exec.udf.directory.tmp\";\n+  String UDF_DISABLE_DYNAMIC = \"drill.exec.udf.disable_dynamic\";\n \n   /**\n    * Local temporary directory is used as base for temporary storage of Dynamic UDF jars.\n@@ -264,7 +265,7 @@\n       SLICE_TARGET_DEFAULT);\n \n   String CAST_TO_NULLABLE_NUMERIC = \"drill.exec.functions.cast_empty_string_to_null\";\n-  OptionValidator CAST_TO_NULLABLE_NUMERIC_OPTION = new BooleanValidator(CAST_TO_NULLABLE_NUMERIC, false);\n+  BooleanValidator CAST_TO_NULLABLE_NUMERIC_OPTION = new BooleanValidator(CAST_TO_NULLABLE_NUMERIC, false);\n \n   /**\n    * HashTable runtime settings",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "91498fced7e9a00a9451f57fcde915726fc1b0a6",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            },
            {
                "patch": "@@ -27,6 +27,6 @@\n    *\n    * @param event  event details\n    */\n-  void onChange(TransientStoreEvent event);\n+  void onChange(TransientStoreEvent<?> event);\n \n }",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "3cd86f9fc46f188ef03afd7eb288689a2c23bcaf",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            },
            {
                "patch": "@@ -83,17 +83,30 @@\n   private boolean deleteTmpDir = false;\n   private File tmpDir;\n   private List<PluggableFunctionRegistry> pluggableFuncRegistries = Lists.newArrayList();\n-  private OptionManager optionManager = null;\n+  private final OptionManager optionManager;\n+  private final boolean useDynamicUdfs;\n \n-  @Deprecated @VisibleForTesting\n-  public FunctionImplementationRegistry(DrillConfig config){\n-    this(config, ClassPathScanner.fromPrescan(config));\n+  @VisibleForTesting\n+  public FunctionImplementationRegistry(DrillConfig config) {\n+    this(config, ClassPathScanner.fromPrescan(config), null);\n   }\n \n-  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan){\n+  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan) {\n+    this(config, classpathScan, null);\n+  }\n+\n+  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan, OptionManager optionManager) {\n     Stopwatch w = Stopwatch.createStarted();\n \n     logger.debug(\"Generating function registry.\");\n+    this.optionManager = optionManager;\n+\n+    // Unit tests fail if dynamic UDFs are turned on AND the test happens\n+    // to access an undefined function. Since we want a reasonable failure\n+    // rather than a crash, we provide a boot-time option, set only by\n+    // tests, to disable DUDF lookup.\n+\n+    useDynamicUdfs = ! config.getBoolean(ExecConstants.UDF_DISABLE_DYNAMIC);\n     localFunctionRegistry = new LocalFunctionRegistry(classpathScan);\n \n     Set<Class<? extends PluggableFunctionRegistry>> registryClasses =\n@@ -123,11 +136,6 @@ public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathSc\n     this.localUdfDir = getLocalUdfDir(config);\n   }\n \n-  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan, OptionManager optionManager) {\n-    this(config, classpathScan);\n-    this.optionManager = optionManager;\n-  }\n-\n   /**\n    * Register functions in given operator table.\n    * @param operatorTable operator table\n@@ -142,7 +150,7 @@ public void register(DrillOperatorTable operatorTable) {\n   }\n \n   /**\n-   * First attempts to finds the Drill function implementation that matches the name, arg types and return type.\n+   * First attempts to find the Drill function implementation that matches the name, arg types and return type.\n    * If exact function implementation was not found,\n    * syncs local function registry with remote function registry if needed\n    * and tries to find function implementation one more time\n@@ -156,17 +164,25 @@ public void register(DrillOperatorTable operatorTable) {\n   public DrillFuncHolder findDrillFunction(FunctionResolver functionResolver, FunctionCall functionCall) {\n     AtomicLong version = new AtomicLong();\n     String newFunctionName = functionReplacement(functionCall);\n-    List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n-    FunctionResolver exactResolver = FunctionResolverFactory.getExactResolver(functionCall);\n-    DrillFuncHolder holder = exactResolver.getBestMatch(functions, functionCall);\n \n-    if (holder == null) {\n+    // Dynamic UDFS: First try with exact match. If not found, we may need to\n+    // update the registry, so sync with remote.\n+\n+    if (useDynamicUdfs) {\n+      List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n+      FunctionResolver exactResolver = FunctionResolverFactory.getExactResolver(functionCall);\n+      DrillFuncHolder holder = exactResolver.getBestMatch(functions, functionCall);\n+      if (holder != null) {\n+        return holder;\n+      }\n       syncWithRemoteRegistry(version.get());\n-      List<DrillFuncHolder> updatedFunctions = localFunctionRegistry.getMethods(newFunctionName, version);\n-      holder = functionResolver.getBestMatch(updatedFunctions, functionCall);\n     }\n \n-    return holder;\n+    // Whether Dynamic UDFs or not: look in the registry for\n+    // an inexact match.\n+\n+    List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n+    return functionResolver.getBestMatch(functions, functionCall);\n   }\n \n   /**\n@@ -177,16 +193,20 @@ public DrillFuncHolder findDrillFunction(FunctionResolver functionResolver, Func\n    */\n   private String functionReplacement(FunctionCall functionCall) {\n     String funcName = functionCall.getName();\n-      if (functionCall.args.size() > 0) {\n-          MajorType majorType =  functionCall.args.get(0).getMajorType();\n-          DataMode dataMode = majorType.getMode();\n-          MinorType minorType = majorType.getMinorType();\n-          if (optionManager != null\n-              && optionManager.getOption(ExecConstants.CAST_TO_NULLABLE_NUMERIC).bool_val\n-              && CastFunctions.isReplacementNeeded(funcName, minorType)) {\n-              funcName = CastFunctions.getReplacingCastFunction(funcName, dataMode, minorType);\n-          }\n-      }\n+    if (functionCall.args.size() == 0) {\n+      return funcName;\n+    }\n+    boolean castToNullableNumeric = optionManager != null &&\n+                  optionManager.getOption(ExecConstants.CAST_TO_NULLABLE_NUMERIC_OPTION);\n+    if (! castToNullableNumeric) {\n+      return funcName;\n+    }\n+    MajorType majorType =  functionCall.args.get(0).getMajorType();\n+    DataMode dataMode = majorType.getMode();\n+    MinorType minorType = majorType.getMinorType();\n+    if (CastFunctions.isReplacementNeeded(funcName, minorType)) {\n+        funcName = CastFunctions.getReplacingCastFunction(funcName, dataMode, minorType);\n+    }\n \n     return funcName;\n   }\n@@ -200,7 +220,7 @@ private String functionReplacement(FunctionCall functionCall) {\n    * @return exactly matching function holder\n    */\n   public DrillFuncHolder findExactMatchingDrillFunction(String name, List<MajorType> argTypes, MajorType returnType) {\n-    return findExactMatchingDrillFunction(name, argTypes, returnType, true);\n+    return findExactMatchingDrillFunction(name, argTypes, returnType, useDynamicUdfs);\n   }\n \n   /**\n@@ -315,6 +335,7 @@ public RemoteFunctionRegistry getRemoteFunctionRegistry() {\n    * @param version remote function registry local function registry was based on\n    * @return true if remote and local function registries were synchronized after given version\n    */\n+  @SuppressWarnings(\"resource\")\n   public boolean syncWithRemoteRegistry(long version) {\n     if (isRegistrySyncNeeded(remoteFunctionRegistry.getRegistryVersion(), localFunctionRegistry.getVersion())) {\n       synchronized (this) {\n@@ -495,6 +516,7 @@ private File getTmpDir(DrillConfig config) {\n    * @return local path to jar that was copied\n    * @throws IOException in case of problems during jar coping process\n    */\n+  @SuppressWarnings(\"resource\")\n   private Path copyJarToLocal(String jarName, RemoteFunctionRegistry remoteFunctionRegistry) throws IOException {\n     Path registryArea = remoteFunctionRegistry.getRegistryArea();\n     FileSystem fs = remoteFunctionRegistry.getFs();\n@@ -549,7 +571,7 @@ public void close() {\n   private class UnregistrationListener implements TransientStoreListener {\n \n     @Override\n-    public void onChange(TransientStoreEvent event) {\n+    public void onChange(TransientStoreEvent<?> event) {\n       String jarName = (String) event.getValue();\n       localFunctionRegistry.unregister(jarName);\n       String localDir = localUdfDir.toUri().getPath();",
                "additions": 52,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "status": "modified",
                "changes": 82,
                "deletions": 30,
                "sha": "c1ba2d8e96e6bbf9a95601ee936baa7cbf8871f3",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            },
            {
                "patch": "@@ -245,6 +245,9 @@ drill.exec: {\n   },\n   udf: {\n     retry-attempts: 5,\n+    // Disables (parts of) the dynamic UDF functionality.\n+    // Primarily for testing.\n+    disable_dynamic: false,\n     directory: {\n       // Base directory for remote and local udf directories, unique among clusters.\n       base: ${drill.exec.zk.root}\"/udf\",",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/resources/drill-module.conf",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "3d66d19f2029edfa72714691bcdcf8004cb6edf6",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/resources/drill-module.conf",
                "filename": "exec/java-exec/src/main/resources/drill-module.conf",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/drill-module.conf?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            },
            {
                "patch": "@@ -52,7 +52,7 @@\n     GuavaPatcher.patch();\n   }\n \n-  private static final DrillConfig c = DrillConfig.create();\n+  protected static final DrillConfig c = DrillConfig.create();\n \n   @After\n   public void clear(){",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "dead858fd56fd0c8768ae95e811299cda6499497",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            },
            {
                "patch": "@@ -61,11 +61,10 @@\n import mockit.Injectable;\n \n public class TestSimpleFunctions extends ExecTest {\n-  //private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TestSimpleFunctions.class);\n-  private final DrillConfig c = DrillConfig.create();\n \n   @Test\n   public void testHashFunctionResolution() throws JClassAlreadyExistsException, IOException {\n+    @SuppressWarnings(\"resource\")\n     final FunctionImplementationRegistry registry = new FunctionImplementationRegistry(c);\n     // test required vs nullable Int input\n     resolveHash(c,\n@@ -133,7 +132,6 @@ public void resolveHash(DrillConfig config, LogicalExpression arg, TypeProtos.Ma\n                                     FunctionImplementationRegistry registry) throws JClassAlreadyExistsException, IOException {\n     final List<LogicalExpression> args = new ArrayList<>();\n     args.add(arg);\n-    final String[] registeredNames = { \"hash\" };\n     FunctionCall call = new FunctionCall(\n         \"hash\",\n         args,",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "status": "modified",
                "changes": 4,
                "deletions": 3,
                "sha": "6c48651a42d08c9e152cc4a5ab04db6d0f640e9b",
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259"
            }
        ],
        "bug_id": "drill_52",
        "parent": "https://github.com/apache/drill/commit/90bc80052f66874cf9eaf8b8bc93f28f703b5506",
        "message": "DRILL-5330: NPE in FunctionImplementationRegistry\n\nFixes:\n\n* DRILL-5330: NPE in\nFunctionImplementationRegistry.functionReplacement()\n* DRILL-5331:\nNPE in FunctionImplementationRegistry.findDrillFunction() if dynamic\nUDFs disabled\n\nWhen running in a unit test, the dynamic UDF (DUDF) mechanism is not\navailable. When running in production, the DUDF mechanism is available,\nbut may be disabled.\n\nOne confusing aspect of this code is that the function registry\nis given the option manager, but the option manager is not yet valid\n(not yet initialized) in the function registry constructor. So, we\ncannot access the option manager in the function registry constructor.\n\nIn any event, the existing system options cannot be used to disable DUDF\nsupport. For obscure reasons, DUDF support is always enabled, even when\ndisabled by the user.\n\nInstead, for DRILL-5331, we added a config option to \"really\" disable DUDFS.\nThe property is set only for tests, disables DUDF support.\nNote that, in the future, this option could be generalized to\n\"off, read-only, on\" to capture the full set of DUDF modes.\nBut, for now, just turning this off is sufficient.\n\nFor DRILL-5330, we use an existing option validator rather than\naccessing the raw option directly.\n\nAlso includes a bit of code cleanup in the class in question.\n\nThe result is that the code now works when used in a sub-operator unit\ntest.\n\nclose apache/drill#777",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/813903a34ea1c9c3fec28f2472312c8785f780c5",
        "file": [
            {
                "patch": "@@ -100,6 +100,11 @@\n   private SingleBatchSorter sorter;\n   private SortRecordBatchBuilder builder;\n   private MSorter mSorter;\n+  /**\n+   * A single PriorityQueueCopier instance is used for 2 purposes:\n+   * 1. Merge sorted batches before spilling\n+   * 2. Merge sorted batches when all incoming data fits in memory\n+   */\n   private PriorityQueueCopier copier;\n   private LinkedList<BatchGroup> batchGroups = Lists.newLinkedList();\n   private LinkedList<BatchGroup> spilledBatchGroups = Lists.newLinkedList();\n@@ -114,6 +119,12 @@\n   private final String fileName;\n   private int firstSpillBatchCount = 0;\n \n+  /**\n+   * The copier uses the COPIER_BATCH_MEM_LIMIT to estimate the target\n+   * number of records to return in each batch.\n+   */\n+  private static final int COPIER_BATCH_MEM_LIMIT = 256 * 1024;\n+\n   public static final String INTERRUPTION_AFTER_SORT = \"after-sort\";\n   public static final String INTERRUPTION_AFTER_SETUP = \"after-setup\";\n \n@@ -205,7 +216,7 @@ public void buildSchema() throws SchemaChangeException {\n     switch (outcome) {\n       case OK:\n       case OK_NEW_SCHEMA:\n-        for (VectorWrapper w : incoming) {\n+        for (VectorWrapper<?> w : incoming) {\n           ValueVector v = container.addOrGet(w.getField());\n           if (v instanceof AbstractContainerVector) {\n             w.getValueVector().makeTransferPair(v); // Can we remove this hack?\n@@ -225,6 +236,8 @@ public void buildSchema() throws SchemaChangeException {\n       case NONE:\n         state = BatchState.DONE;\n         break;\n+      default:\n+        break;\n     }\n   }\n \n@@ -291,7 +304,7 @@ public IterOutcome innerNext() {\n             first = false;\n           }\n           if (incoming.getRecordCount() == 0) {\n-            for (VectorWrapper w : incoming) {\n+            for (VectorWrapper<?> w : incoming) {\n               w.clear();\n             }\n             break;\n@@ -345,7 +358,10 @@ public IterOutcome innerNext() {\n \n               if (spilledBatchGroups.size() > firstSpillBatchCount / 2) {\n                 logger.info(\"Merging spills\");\n-                spilledBatchGroups.addFirst(mergeAndSpill(spilledBatchGroups));\n+                final BatchGroup merged = mergeAndSpill(spilledBatchGroups);\n+                if (merged != null) {\n+                  spilledBatchGroups.addFirst(merged);\n+                }\n               }\n               final BatchGroup merged = mergeAndSpill(batchGroups);\n               if (merged != null) { // make sure we don't add null to spilledBatchGroups\n@@ -366,8 +382,11 @@ public IterOutcome innerNext() {\n           logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n           highWaterMark = totalSizeInMemory;\n           if (batchesSinceLastSpill > 2) {\n-            spilledBatchGroups.add(mergeAndSpill(batchGroups));\n-            batchesSinceLastSpill = 0;\n+            final BatchGroup merged = mergeAndSpill(batchGroups);\n+            if (merged != null) {\n+              spilledBatchGroups.add(merged);\n+              batchesSinceLastSpill = 0;\n+            }\n           } else {\n             logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n             return IterOutcome.OUT_OF_MEMORY;\n@@ -418,7 +437,7 @@ public IterOutcome innerNext() {\n         long t = watch.elapsed(TimeUnit.MICROSECONDS);\n //        logger.debug(\"Took {} us to sort {} records\", t, sv4.getTotalCount());\n         container.buildSchema(SelectionVectorMode.FOUR_BYTE);\n-      } else {\n+      } else { // some batches were spilled\n         final BatchGroup merged = mergeAndSpill(batchGroups);\n         if (merged != null) {\n           spilledBatchGroups.add(merged);\n@@ -430,14 +449,14 @@ public IterOutcome innerNext() {\n         createCopier(hyperBatch, batchGroups, container, false);\n \n         int estimatedRecordSize = 0;\n-        for (VectorWrapper w : batchGroups.get(0)) {\n+        for (VectorWrapper<?> w : batchGroups.get(0)) {\n           try {\n             estimatedRecordSize += TypeHelper.getSize(w.getField().getType());\n           } catch (UnsupportedOperationException e) {\n             estimatedRecordSize += 50;\n           }\n         }\n-        targetRecordCount = Math.min(MAX_BATCH_SIZE, Math.max(1, 250 * 1000 / estimatedRecordSize));\n+        targetRecordCount = Math.min(MAX_BATCH_SIZE, Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize));\n         int count = copier.next(targetRecordCount);\n         container.buildSchema(SelectionVectorMode.NONE);\n         container.setRecordCount(count);\n@@ -470,6 +489,7 @@ private boolean hasMemoryForInMemorySort(int currentRecordCount) {\n \n   public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws SchemaChangeException {\n     logger.debug(\"Copier allocator current allocation {}\", copierAllocator.getAllocatedMemory());\n+    logger.debug(\"mergeAndSpill: starting totalSizeInMemory = {}\", totalSizeInMemory);\n     VectorContainer outputContainer = new VectorContainer();\n     List<BatchGroup> batchGroupList = Lists.newArrayList();\n     int batchCount = batchGroups.size();\n@@ -478,28 +498,36 @@ public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws Schem\n         break;\n       }\n       BatchGroup batch = batchGroups.pollLast();\n+      assert batch != null : \"Encountered a null batch during merge and spill operation\";\n       batchGroupList.add(batch);\n       long bufferSize = getBufferSize(batch);\n+      logger.debug(\"mergeAndSpill: buffer size for batch {} = {}\", i, bufferSize);\n       totalSizeInMemory -= bufferSize;\n     }\n+    logger.debug(\"mergeAndSpill: intermediate estimated total size in memory = {}\", totalSizeInMemory);\n+\n     if (batchGroupList.size() == 0) {\n       return null;\n     }\n     int estimatedRecordSize = 0;\n-    for (VectorWrapper w : batchGroupList.get(0)) {\n+    for (VectorWrapper<?> w : batchGroupList.get(0)) {\n       try {\n         estimatedRecordSize += TypeHelper.getSize(w.getField().getType());\n       } catch (UnsupportedOperationException e) {\n         estimatedRecordSize += 50;\n       }\n     }\n-    int targetRecordCount = Math.max(1, 250 * 1000 / estimatedRecordSize);\n+    int targetRecordCount = Math.max(1, COPIER_BATCH_MEM_LIMIT / estimatedRecordSize);\n     VectorContainer hyperBatch = constructHyperBatch(batchGroupList);\n     createCopier(hyperBatch, batchGroupList, outputContainer, true);\n \n     int count = copier.next(targetRecordCount);\n     assert count > 0;\n \n+    logger.debug(\"mergeAndSpill: estimated record size = {}, target record count = {}\", estimatedRecordSize, targetRecordCount);\n+\n+    // 1 output container is kept in memory, so we want to hold on to it and transferClone\n+    // allows keeping ownership\n     VectorContainer c1 = VectorContainer.getTransferClone(outputContainer);\n     c1.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n     c1.setRecordCount(count);\n@@ -512,6 +540,7 @@ public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws Schem\n       while ((count = copier.next(targetRecordCount)) > 0) {\n         outputContainer.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n         outputContainer.setRecordCount(count);\n+        // note that addBatch also clears the outputContainer\n         newGroup.addBatch(outputContainer);\n       }\n       newGroup.closeOutputStream();\n@@ -522,14 +551,16 @@ public BatchGroup mergeAndSpill(LinkedList<BatchGroup> batchGroups) throws Schem\n     } catch (IOException e) {\n       throw new RuntimeException(e);\n     }\n-    takeOwnership(c1);\n-    totalSizeInMemory += getBufferSize(c1);\n+    takeOwnership(c1); // transfer ownership from copier allocator to external sort allocator\n+    long bufSize = getBufferSize(c1);\n+    totalSizeInMemory += bufSize;\n+    logger.debug(\"mergeAndSpill: final total size in memory = {}\", totalSizeInMemory);\n     logger.info(\"Completed spilling to {}\", outputFile);\n     return newGroup;\n   }\n \n   private void takeOwnership(VectorAccessible batch) {\n-    for (VectorWrapper w : batch) {\n+    for (VectorWrapper<?> w : batch) {\n       DrillBuf[] bufs = w.getValueVector().getBuffers(false);\n       for (DrillBuf buf : bufs) {\n         if (buf.isRootBuffer()) {\n@@ -541,7 +572,7 @@ private void takeOwnership(VectorAccessible batch) {\n \n   private long getBufferSize(VectorAccessible batch) {\n     long size = 0;\n-    for (VectorWrapper w : batch) {\n+    for (VectorWrapper<?> w : batch) {\n       DrillBuf[] bufs = w.getValueVector().getBuffers(false);\n       for (DrillBuf buf : bufs) {\n         if (buf.isRootBuffer()) {\n@@ -558,7 +589,7 @@ private SelectionVector2 newSV2() throws OutOfMemoryException, InterruptedExcept\n       try {\n         spilledBatchGroups.addFirst(mergeAndSpill(batchGroups));\n       } catch (SchemaChangeException e) {\n-        throw new RuntimeException();\n+        throw new RuntimeException(e);\n       }\n       batchesSinceLastSpill = 0;\n       int waitTime = 1;\n@@ -659,7 +690,7 @@ public SingleBatchSorter createNewSorter(FragmentContext context, VectorAccessib\n     return context.getImplementationClass(cg);\n   }\n \n-  private void generateComparisons(ClassGenerator g, VectorAccessible batch) throws SchemaChangeException {\n+  private void generateComparisons(ClassGenerator<?> g, VectorAccessible batch) throws SchemaChangeException {\n     g.setMappingSet(MAIN_MAPPING);\n \n     for (Ordering od : popConfig.getOrderings()) {",
                "additions": 47,
                "raw_url": "https://github.com/apache/drill/raw/813903a34ea1c9c3fec28f2472312c8785f780c5/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "status": "modified",
                "changes": 63,
                "deletions": 16,
                "sha": "f1e22b2bd859e3e08c8414d82a6ca1f6938bec95",
                "blob_url": "https://github.com/apache/drill/blob/813903a34ea1c9c3fec28f2472312c8785f780c5/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java?ref=813903a34ea1c9c3fec28f2472312c8785f780c5"
            }
        ],
        "bug_id": "drill_53",
        "parent": "https://github.com/apache/drill/commit/e52d473eb465699bf145cac70662809f1feae78e",
        "message": "DRILL-3779: Fix NPE in mergeAndSpill().  Add more debug logging messages.  Make copier per-batch memory limit power of 2.\nGet rid of some warnings.  Add a few comments.\n\nAddress review comments.\n\nClose apache/drill#160",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/163219c2a802481cbd90171912540250d4059ea8",
        "file": [
            {
                "patch": "@@ -110,6 +110,7 @@\n   public boolean next() throws IOException {\n \n     currentPage = null;\n+    valuesRead = 0;\n \n     // TODO - the metatdata for total size appears to be incorrect for impala generated files, need to find cause\n     // and submit a bug report\n@@ -162,7 +163,6 @@ public boolean next() throws IOException {\n     pageDataByteArray = currentPage.getBytes().toByteArray();\n \n     readPosInBytes = 0;\n-    valuesRead = 0;\n     if (parentColumnReader.columnDescriptor.getMaxDefinitionLevel() != 0){\n       parentColumnReader.currDefLevel = -1;\n       if (!currentPage.getValueEncoding().usesDictionary()) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "3ad1d6c793378cb98a90e0d7f2eb937d931d29a4",
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java?ref=163219c2a802481cbd90171912540250d4059ea8"
            },
            {
                "patch": "@@ -228,7 +228,7 @@ public void setup(OutputMutator output) throws ExecutionSetupException {\n \n     // none of the columns in the parquet file matched the request columns from the query\n     if (columnsToScan == 0){\n-      return;\n+      throw new ExecutionSetupException(\"Error reading from parquet file. No columns requested were found in the file.\");\n     }\n     if (allFieldsFixedLength) {\n       recordsPerBatch = (int) Math.min(Math.min(batchSize / bitWidthAllFixedFields,",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "4c5f4bba70b1aed62d4f8085fa7fe271f2274731",
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java?ref=163219c2a802481cbd90171912540250d4059ea8"
            },
            {
                "patch": "@@ -352,6 +352,15 @@ public void testMultipleRowGroupsAndReads() throws Exception {\n         \"/tmp/test.parquet\", i, props);\n   }\n \n+  @Test\n+  public void testReadError_Drill_901() throws Exception {\n+    // select cast( L_COMMENT as varchar) from  dfs.`/tmp/drilltest/employee_parquet`\n+    HashMap<String, FieldInfo> fields = new HashMap<>();\n+    ParquetTestProperties props = new ParquetTestProperties(1, 120350, DEFAULT_BYTES_PER_PAGE, fields);\n+    testParquetFullEngineEventBased(false, false, \"/parquet/par_writer_test.json\", null,\n+        \"unused, no file is generated\", 1, props, false);\n+  }\n+\n \n   @Ignore\n   @Test",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "status": "modified",
                "changes": 9,
                "deletions": 0,
                "sha": "ad63dc96bf4f19d6df502d9807b4094fe71cf8ec",
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java?ref=163219c2a802481cbd90171912540250d4059ea8"
            },
            {
                "patch": "@@ -0,0 +1,26 @@\n+  {\n+    head : {\n+      version : 1,\n+          generator : {\n+        type : \"manual\",\n+            info : \"na\"\n+      },\n+      type : \"APACHE_DRILL_PHYSICAL\"\n+    },\n+    graph : [ {\n+    pop : \"parquet-scan\",\n+    @id : 1,\n+        entries : [ {\n+      path : \"/tpch/lineitem.parquet\"\n+    } ],\n+    storage : {\n+      type : \"file\",\n+      connection : \"classpath:///\"\n+    },\n+    columns: [ \"L_COMMENT\"]\n+  }, {\n+    pop : \"screen\",\n+    @id : 2,\n+        child : 1\n+  } ]\n+  }",
                "additions": 26,
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "status": "added",
                "changes": 26,
                "deletions": 0,
                "sha": "34f2ba6e7b147b578b3e7742ffd8f90588c6d58b",
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "filename": "exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/par_writer_test.json?ref=163219c2a802481cbd90171912540250d4059ea8"
            }
        ],
        "bug_id": "drill_54",
        "parent": "https://github.com/apache/drill/commit/393adee7e441cb5b03b4489e1497282c68ffbf52",
        "message": "DRILL-901: Fix Parquet read bug with VarBinary.\n\nAlso now throw an exception if parquet reader is not passed any columns found in the file. Previously a NPE was thrown as the setup method exited early, skipping an object initialization that manifested in the first call to the next method.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
        "file": [
            {
                "patch": "@@ -66,7 +66,6 @@\n \n   // External Sort Boot configuration\n \n-  String EXTERNAL_SORT_TARGET_BATCH_SIZE = \"drill.exec.sort.external.batch.size\";\n   String EXTERNAL_SORT_TARGET_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.batch.size\";\n   String EXTERNAL_SORT_SPILL_GROUP_SIZE = \"drill.exec.sort.external.spill.group.size\";\n   String EXTERNAL_SORT_SPILL_THRESHOLD = \"drill.exec.sort.external.spill.threshold\";\n@@ -79,6 +78,8 @@\n   String EXTERNAL_SORT_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.spill_batch_size\";\n   String EXTERNAL_SORT_MERGE_BATCH_SIZE = \"drill.exec.sort.external.spill.merge_batch_size\";\n   String EXTERNAL_SORT_MAX_MEMORY = \"drill.exec.sort.external.mem_limit\";\n+\n+  // Used only by the \"unmanaged\" sort.\n   String EXTERNAL_SORT_BATCH_LIMIT = \"drill.exec.sort.external.batch_limit\";\n \n   // External Sort Runtime options",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "60d6265275f4f052c763d85d6002eaf94871cc30",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -238,14 +238,15 @@ public void close() {\n   }\n \n   /**\n-   * For given recordcount how muchmemory does SortRecordBatchBuilder needs for its own purpose. This is used in\n+   * For given record count how much memory does SortRecordBatchBuilder needs for its own purpose. This is used in\n    * ExternalSortBatch to make decisions about whether to spill or not.\n    *\n    * @param recordCount\n    * @return\n    */\n   public static long memoryNeeded(int recordCount) {\n-    // We need 4 bytes (SV4) for each record.\n-    return recordCount * 4;\n+    // We need 4 bytes (SV4) for each record. Due to power-of-two allocations, the\n+    // backing buffer might be twice this size.\n+    return recordCount * 2 * 4;\n   }\n }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "status": "modified",
                "changes": 7,
                "deletions": 3,
                "sha": "d46990f8d0878ef44e789e3bb767b2bab6476868",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -27,22 +27,15 @@\n import org.apache.drill.exec.record.VectorAccessible;\n import org.apache.drill.exec.record.VectorWrapper;\n import org.apache.drill.exec.record.selection.SelectionVector2;\n-import org.apache.drill.exec.vector.BaseDataValueVector;\n-import org.apache.drill.exec.vector.FixedWidthVector;\n-import org.apache.drill.exec.vector.NullableVarCharVector;\n-import org.apache.drill.exec.vector.NullableVector;\n import org.apache.drill.exec.vector.ValueVector;\n-import org.apache.drill.exec.vector.VarCharVector;\n-\n-import io.netty.buffer.DrillBuf;\n \n /**\n  * Given a record batch or vector container, determines the actual memory\n  * consumed by each column, the average row, and the entire record batch.\n  */\n \n public class RecordBatchSizer {\n-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n+//  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n \n   /**\n    * Column size information.\n@@ -53,23 +46,22 @@\n     /**\n      * Assumed size from Drill metadata.\n      */\n+\n     public int stdSize;\n+\n     /**\n      * Actual memory consumed by all the vectors associated with this column.\n      */\n+\n     public int totalSize;\n+\n     /**\n      * Actual average column width as determined from actual memory use. This\n      * size is larger than the actual data size since this size includes per-\n      * column overhead such as any unused vector space, etc.\n      */\n-    public int estSize;\n \n-    /**\n-     * The size of the data vector backing the column. Useful for detecting\n-     * cases of possible direct memory fragmentation.\n-     */\n-    public int dataVectorSize;\n+    public int estSize;\n     public int capacity;\n     public int density;\n     public int dataSize;\n@@ -86,53 +78,31 @@ public ColumnSize(VectorWrapper<?> vw) {\n       if (rowCount == 0) {\n         return;\n       }\n-      DrillBuf[] bufs = v.getBuffers(false);\n-      for (DrillBuf buf : bufs) {\n-        totalSize += buf.capacity();\n-      }\n+\n+      // Total size taken by all vectors (and underlying buffers)\n+      // associated with this vector.\n+\n+      totalSize = v.getAllocatedByteCount();\n \n       // Capacity is the number of values that the vector could\n       // contain. This is useful only for fixed-length vectors.\n \n       capacity = v.getValueCapacity();\n \n-      // Crude way to get the size of the buffer underlying simple (scalar) values.\n-      // Ignores maps, lists and other esoterica. Uses a crude way to subtract out\n-      // the null \"bit\" (really byte) buffer size for nullable vectors.\n+      // The amount of memory consumed by the payload: the actual\n+      // data stored in the vectors.\n \n-      if (v instanceof BaseDataValueVector) {\n-        dataVectorSize = totalSize;\n-        if (v instanceof NullableVector) {\n-          dataVectorSize -= bufs[0].getActualMemoryConsumed();\n-        }\n-      }\n+      dataSize = v.getPayloadByteCount();\n \n       // Determine \"density\" the number of rows compared to potential\n       // capacity. Low-density batches occur at block boundaries, ends\n       // of files and so on. Low-density batches throw off our estimates\n       // for Varchar columns because we don't know the actual number of\n       // bytes consumed (that information is hidden behind the Varchar\n       // implementation where we can't get at it.)\n-      //\n-      // A better solution is to have each vector do this calc rather\n-      // than trying to do it generically, but that increases the code\n-      // change footprint and slows the commit process.\n-\n-      if (v instanceof FixedWidthVector) {\n-        dataSize = stdSize * rowCount;\n-      } else if ( v instanceof VarCharVector ) {\n-        VarCharVector vv = (VarCharVector) v;\n-        dataSize = vv.getOffsetVector().getAccessor().get(rowCount);\n-      } else if ( v instanceof NullableVarCharVector ) {\n-        NullableVarCharVector vv = (NullableVarCharVector) v;\n-        dataSize = vv.getValuesVector().getOffsetVector().getAccessor().get(rowCount);\n-      } else {\n-        dataSize = 0;\n-      }\n-      if (dataSize > 0) {\n-        density = roundUp(dataSize * 100, dataVectorSize);\n-        estSize = roundUp(dataSize, rowCount);\n-      }\n+\n+      density = roundUp(dataSize * 100, totalSize);\n+      estSize = roundUp(dataSize, rowCount);\n     }\n \n     @Override\n@@ -145,8 +115,6 @@ public String toString() {\n       buf.append(estSize);\n       buf.append(\", total size: \");\n       buf.append(totalSize);\n-      buf.append(\", vector size: \");\n-      buf.append(dataVectorSize);\n       buf.append(\", data size: \");\n       buf.append(dataSize);\n       buf.append(\", row capacity: \");\n@@ -187,10 +155,12 @@ public String toString() {\n   private int sv2Size;\n   private int avgDensity;\n \n+  private int netBatchSize;\n+\n   public RecordBatchSizer(VectorAccessible va) {\n     rowCount = va.getRecordCount();\n     for (VectorWrapper<?> vw : va) {\n-      measureField(vw);\n+      measureColumn(vw);\n     }\n \n     if (rowCount > 0) {\n@@ -201,8 +171,8 @@ public RecordBatchSizer(VectorAccessible va) {\n     if (hasSv2) {\n       @SuppressWarnings(\"resource\")\n       SelectionVector2 sv2 = va.getSelectionVector2();\n-      sv2Size = sv2.getBuffer().capacity();\n-      grossRowWidth += sv2Size;\n+      sv2Size = sv2.getBuffer(false).capacity();\n+      grossRowWidth += sv2Size / rowCount;\n       netRowWidth += 2;\n     }\n \n@@ -227,12 +197,13 @@ public void applySv2() {\n     totalBatchSize += sv2Size;\n   }\n \n-  private void measureField(VectorWrapper<?> vw) {\n+  private void measureColumn(VectorWrapper<?> vw) {\n     ColumnSize colSize = new ColumnSize(vw);\n     columnSizes.add(colSize);\n \n     stdRowWidth += colSize.stdSize;\n     totalBatchSize += colSize.totalSize;\n+    netBatchSize += colSize.dataSize;\n     netRowWidth += colSize.estSize;\n   }\n \n@@ -249,27 +220,11 @@ public static int roundUp(int num, int denom) {\n   public int netRowWidth() { return netRowWidth; }\n   public int actualSize() { return totalBatchSize; }\n   public boolean hasSv2() { return hasSv2; }\n-  public int getAvgDensity() { return avgDensity; }\n+  public int avgDensity() { return avgDensity; }\n+  public int netSize() { return netBatchSize; }\n \n   public static final int MAX_VECTOR_SIZE = 16 * 1024 * 1024; // 16 MiB\n \n-  /**\n-   * Look for columns backed by vectors larger than the 16 MiB size\n-   * employed by the Netty allocator. Such large blocks can lead to\n-   * memory fragmentation and unexpected OOM errors.\n-   * @return true if any column is oversized\n-   */\n-  public boolean checkOversizeCols() {\n-    boolean hasOversize = false;\n-    for (ColumnSize colSize : columnSizes) {\n-      if ( colSize.dataVectorSize > MAX_VECTOR_SIZE) {\n-        logger.warn( \"Column is wider than 256 characters: OOM due to memory fragmentation is possible - \" + colSize.metadata.getPath() );\n-        hasOversize = true;\n-      }\n-    }\n-    return hasOversize;\n-  }\n-\n   @Override\n   public String toString() {\n     StringBuilder buf = new StringBuilder();",
                "additions": 26,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "status": "modified",
                "changes": 97,
                "deletions": 71,
                "sha": "22b1b0eed03115d4bf17cb6e1e00409d4b470625",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -26,6 +26,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.drill.common.config.DrillConfig;\n@@ -105,7 +106,7 @@\n \n     protected HadoopFileManager(String fsName) {\n       Configuration conf = new Configuration();\n-      conf.set(\"fs.default.name\", fsName);\n+      conf.set(FileSystem.FS_DEFAULT_NAME_KEY, fsName);\n       try {\n         fs = FileSystem.get(conf);\n       } catch (IOException e) {\n@@ -169,6 +170,12 @@ public long getReadBytes(InputStream inputStream) {\n     }\n   }\n \n+  /**\n+   * Wrapper around an input stream to collect the total bytes\n+   * read through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingInputStream extends InputStream\n   {\n     private InputStream in;\n@@ -218,6 +225,12 @@ public void close() throws IOException {\n     public long getCount() { return count; }\n   }\n \n+  /**\n+   * Wrapper around an output stream to collect the total bytes\n+   * written through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingOutputStream extends OutputStream {\n \n     private OutputStream out;\n@@ -333,6 +346,7 @@ public long getReadBytes(InputStream inputStream) {\n    */\n \n   private final String spillDirName;\n+  private final String spillFileName;\n \n   private int fileCount = 0;\n \n@@ -343,8 +357,30 @@ public long getReadBytes(InputStream inputStream) {\n   private long writeBytes;\n \n   public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n+    this(context, popConfig, null, \"spill\");\n+  }\n+\n+  public SpillSet(FragmentContext context, PhysicalOperator popConfig,\n+                  String opName, String fileName) {\n+    FragmentHandle handle = context.getHandle();\n     DrillConfig config = context.getConfig();\n-    dirs = Iterators.cycle(config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS));\n+    spillFileName = fileName;\n+    List<String> dirList = config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS);\n+    dirs = Iterators.cycle(dirList);\n+\n+    // If more than one directory, semi-randomly choose an offset into\n+    // the list to avoid overloading the first directory in the list.\n+\n+    if (dirList.size() > 1) {\n+      int hash = handle.getQueryId().hashCode() +\n+                 handle.getMajorFragmentId() +\n+                 handle.getMinorFragmentId() +\n+                 popConfig.getOperatorId();\n+      int offset = hash % dirList.size();\n+      for (int i = 0; i < offset; i++) {\n+        dirs.next();\n+      }\n+    }\n \n     // Use the high-performance local file system if the local file\n     // system is selected and impersonation is off. (We use that\n@@ -357,9 +393,13 @@ public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n     } else {\n       fileManager = new HadoopFileManager(spillFs);\n     }\n-    FragmentHandle handle = context.getHandle();\n-    spillDirName = String.format(\"%s_major%s_minor%s_op%s\", QueryIdHelper.getQueryId(handle.getQueryId()),\n-        handle.getMajorFragmentId(), handle.getMinorFragmentId(), popConfig.getOperatorId());\n+    spillDirName = String.format(\n+        \"%s_major%d_minor%d_op%d%s\",\n+        QueryIdHelper.getQueryId(handle.getQueryId()),\n+        handle.getMajorFragmentId(),\n+        handle.getMinorFragmentId(),\n+        popConfig.getOperatorId(),\n+        (opName == null) ? \"\" : \"_\" + opName);\n   }\n \n   public String getNextSpillFile() {\n@@ -371,7 +411,7 @@ public String getNextSpillFile() {\n     String spillDir = dirs.next();\n     String currSpillPath = Joiner.on(\"/\").join(spillDir, spillDirName);\n     currSpillDirs.add(currSpillPath);\n-    String outputFile = Joiner.on(\"/\").join(currSpillPath, \"spill\" + ++fileCount);\n+    String outputFile = Joiner.on(\"/\").join(currSpillPath, spillFileName + ++fileCount);\n     try {\n         fileManager.deleteOnExit(currSpillPath);\n     } catch (IOException e) {",
                "additions": 46,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "status": "modified",
                "changes": 52,
                "deletions": 6,
                "sha": "74e1fb5674eced7fec3fca51915eee8c2afc6cbf",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -113,7 +113,7 @@ private VectorContainer getBatch() throws IOException {\n     if (schema != null) {\n       c = SchemaUtil.coerceContainer(c, schema, context);\n     }\n-//    logger.debug(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n+    logger.trace(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n     spilledBatches--;\n     currentContainer.zeroVectors();\n     Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "13f0dbeb59a33d43b006cba58c438167db2e88a8",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -75,17 +75,21 @@\n    */\n \n   public static class InputBatch extends BatchGroup {\n-    private SelectionVector2 sv2;\n+    private final SelectionVector2 sv2;\n+    private final int dataSize;\n \n-    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, long batchSize) {\n-      super(container, context, batchSize);\n+    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, int dataSize) {\n+      super(container, context);\n       this.sv2 = sv2;\n+      this.dataSize = dataSize;\n     }\n \n     public SelectionVector2 getSv2() {\n       return sv2;\n     }\n \n+    public int getDataSize() { return dataSize; }\n+\n     @Override\n     public int getRecordCount() {\n       if (sv2 != null) {\n@@ -148,8 +152,8 @@ public void close() throws IOException {\n     private BufferAllocator allocator;\n     private int spilledBatches = 0;\n \n-    public SpilledRun(SpillSet spillSet, String path, OperatorContext context, long batchSize) throws IOException {\n-      super(null, context, batchSize);\n+    public SpilledRun(SpillSet spillSet, String path, OperatorContext context) throws IOException {\n+      super(null, context);\n       this.spillSet = spillSet;\n       this.path = path;\n       this.allocator = context.getAllocator();\n@@ -275,25 +279,23 @@ public long closeOutputStream() throws IOException {\n       if (outputStream == null) {\n         return 0;\n       }\n-      long posn = spillSet.getPosition(outputStream);\n-      spillSet.tallyWriteBytes(posn);\n+      long writeSize = spillSet.getPosition(outputStream);\n+      spillSet.tallyWriteBytes(writeSize);\n       outputStream.close();\n       outputStream = null;\n-      logger.trace(\"Summary: Wrote {} bytes to {}\", posn, path);\n-      return posn;\n+      logger.trace(\"Summary: Wrote {} bytes to {}\", writeSize, path);\n+      return writeSize;\n     }\n   }\n \n   protected VectorContainer currentContainer;\n   protected int pointer = 0;\n-  protected OperatorContext context;\n+  protected final OperatorContext context;\n   protected BatchSchema schema;\n-  protected long dataSize;\n \n-  public BatchGroup(VectorContainer container, OperatorContext context, long dataSize) {\n+  public BatchGroup(VectorContainer container, OperatorContext context) {\n     this.currentContainer = container;\n     this.context = context;\n-    this.dataSize = dataSize;\n   }\n \n   /**\n@@ -348,8 +350,6 @@ public int getUnfilteredRecordCount() {\n     return currentContainer.getRecordCount();\n   }\n \n-  public long getDataSize() { return dataSize; }\n-\n   @Override\n   public Iterator<VectorWrapper<?>> iterator() {\n     return currentContainer.iterator();",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "status": "modified",
                "changes": 30,
                "deletions": 15,
                "sha": "7ea599c39c0e3378226c6b1f0df003b60ad161fe",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -200,6 +200,7 @@\n   public static final String INTERRUPTION_AFTER_SORT = \"after-sort\";\n   public static final String INTERRUPTION_AFTER_SETUP = \"after-setup\";\n   public static final String INTERRUPTION_WHILE_SPILLING = \"spilling\";\n+  public static final String INTERRUPTION_WHILE_MERGING = \"merging\";\n   public static final long DEFAULT_SPILL_BATCH_SIZE = 8L * 1024 * 1024;\n   public static final long MIN_SPILL_BATCH_SIZE = 256 * 1024;\n \n@@ -219,6 +220,11 @@\n \n   private BatchSchema schema;\n \n+  /**\n+   * Incoming batches buffered in memory prior to spilling\n+   * or an in-memory merge.\n+   */\n+\n   private LinkedList<BatchGroup.InputBatch> bufferedBatches = Lists.newLinkedList();\n   private LinkedList<BatchGroup.SpilledRun> spilledRuns = Lists.newLinkedList();\n   private SelectionVector4 sv4;\n@@ -231,6 +237,12 @@\n   private int mergeBatchRowCount;\n   private int peakNumBatches = -1;\n \n+  /**\n+   * Maximum memory this operator may use. Usually comes from the\n+   * operator definition, but may be overridden by a configuration\n+   * parameter for unit testing.\n+   */\n+\n   private long memoryLimit;\n \n   /**\n@@ -280,35 +292,79 @@\n   private long estimatedInputBatchSize;\n \n   /**\n-   * Maximum number of batches to hold in memory.\n-   * (Primarily for testing.)\n+   * Maximum number of spilled runs that can be merged in a single pass.\n    */\n \n-  private int bufferedBatchLimit;\n   private int mergeLimit;\n+\n+  /**\n+   * Target size of the first-generation spill files.\n+   */\n   private long spillFileSize;\n+\n+  /**\n+   * Tracks the minimum amount of remaining memory for use\n+   * in populating an operator metric.\n+   */\n+\n   private long minimumBufferSpace;\n \n   /**\n-   * Minimum memory level before spilling occurs. That is, we can buffer input\n-   * batches in memory until we are down to the level given by the spill point.\n+   * Maximum memory level before spilling occurs. That is, we can buffer input\n+   * batches in memory until we reach the level given by the buffer memory pool.\n+   */\n+\n+  private long bufferMemoryPool;\n+\n+  /**\n+   * Maximum memory that can hold batches during the merge\n+   * phase.\n    */\n \n-  private long spillPoint;\n   private long mergeMemoryPool;\n+\n+  /**\n+   * The target size for merge batches sent downstream.\n+   */\n+\n   private long preferredMergeBatchSize;\n-  private long bufferMemoryPool;\n-  private boolean hasOversizeCols;\n+\n+  /**\n+   * Sum of the total number of bytes read from upstream.\n+   * This is the raw memory bytes, not actual data bytes.\n+   */\n+\n   private long totalInputBytes;\n-  private Long spillBatchSize;\n+\n+  /**\n+   * The configured size for each spill batch.\n+   */\n+  private Long preferredSpillBatchSize;\n+\n+  /**\n+   * Tracks the maximum density of input batches. Density is\n+   * the amount of actual data / amount of memory consumed.\n+   * Low density batches indicate an EOF or something wrong in\n+   * an upstream operator because a low-density batch wastes\n+   * memory.\n+   */\n+\n   private int maxDensity;\n+  private int lastDensity = -1;\n \n   /**\n    * Estimated number of rows that fit into a single spill batch.\n    */\n \n   private int spillBatchRowCount;\n \n+  /**\n+   * The estimated actual spill batch size which depends on the\n+   * details of the data rows for any particular query.\n+   */\n+\n+  private int targetSpillBatchSize;\n+\n   // WARNING: The enum here is used within this class. But, the members of\n   // this enum MUST match those in the (unmanaged) ExternalSortBatch since\n   // that is the enum used in the UI to display metrics for the query profile.\n@@ -349,7 +405,7 @@ public ExternalSortBatch(ExternalSort popConfig, FragmentContext context, Record\n     allocator = oContext.getAllocator();\n     opCodeGen = new OperatorCodeGenerator(context, popConfig);\n \n-    spillSet = new SpillSet(context, popConfig);\n+    spillSet = new SpillSet(context, popConfig, \"sort\", \"run\");\n     copierHolder = new CopierHolder(context, allocator, opCodeGen);\n     configure(context.getConfig());\n   }\n@@ -368,12 +424,6 @@ private void configure(DrillConfig config) {\n       memoryLimit = Math.min(memoryLimit, configLimit);\n     }\n \n-    // Optional limit on the number of buffered in-memory batches.\n-    // 0 means no limit. Used primarily for testing. Must allow at least two\n-    // batches or no merging can occur.\n-\n-    bufferedBatchLimit = getConfigLimit(config, ExecConstants.EXTERNAL_SORT_BATCH_LIMIT, Integer.MAX_VALUE, 2);\n-\n     // Optional limit on the number of spilled runs to merge in a single\n     // pass. Limits the number of open file handles. Must allow at least\n     // two batches to merge to make progress.\n@@ -392,22 +442,31 @@ private void configure(DrillConfig config) {\n     // Set too large and the ratio between memory and input data sizes becomes\n     // small. Set too small and disk seek times dominate performance.\n \n-    spillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n-    spillBatchSize = Math.max(spillBatchSize, MIN_SPILL_BATCH_SIZE);\n+    preferredSpillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n+\n+    // In low memory, use no more than 1/4 of memory for each spill batch. Ensures we\n+    // can merge.\n+\n+    preferredSpillBatchSize = Math.min(preferredSpillBatchSize, memoryLimit / 4);\n+\n+    // But, the spill batch should be above some minimum size to prevent complete\n+    // thrashing.\n+\n+    preferredSpillBatchSize = Math.max(preferredSpillBatchSize, MIN_SPILL_BATCH_SIZE);\n \n     // Set the target output batch size. Use the maximum size, but only if\n     // this represents less than 10% of available memory. Otherwise, use 10%\n     // of memory, but no smaller than the minimum size. In any event, an\n     // output batch can contain no fewer than a single record.\n \n     preferredMergeBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_MERGE_BATCH_SIZE);\n-    long maxAllowance = (long) (memoryLimit * MERGE_BATCH_ALLOWANCE);\n+    long maxAllowance = (long) (memoryLimit - 2 * preferredSpillBatchSize);\n     preferredMergeBatchSize = Math.min(maxAllowance, preferredMergeBatchSize);\n     preferredMergeBatchSize = Math.max(preferredMergeBatchSize, MIN_MERGED_BATCH_SIZE);\n \n-    logger.debug(\"Config: memory limit = {}, batch limit = {}, \" +\n-                 \"spill file size = {}, batch size = {}, merge limit = {}, merge batch size = {}\",\n-                  memoryLimit, bufferedBatchLimit, spillFileSize, spillBatchSize, mergeLimit,\n+    logger.debug(\"Config: memory limit = {}, \" +\n+                 \"spill file size = {}, spill batch size = {}, merge limit = {}, merge batch size = {}\",\n+                  memoryLimit, spillFileSize, preferredSpillBatchSize, mergeLimit,\n                   preferredMergeBatchSize);\n   }\n \n@@ -513,11 +572,21 @@ public IterOutcome innerNext() {\n \n   private IterOutcome nextOutputBatch() {\n     if (resultsIterator.next()) {\n+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);\n       return IterOutcome.OK;\n     } else {\n       logger.trace(\"Deliver phase complete: Returned {} batches, {} records\",\n                     resultsIterator.getBatchCount(), resultsIterator.getRecordCount());\n       sortState = SortState.DONE;\n+\n+      // Close the iterator here to release any remaining resources such\n+      // as spill files. This is important when a query has a join: the\n+      // first branch sort may complete before the second branch starts;\n+      // it may be quite a while after returning the last row before the\n+      // fragment executor calls this opeator's close method.\n+\n+      resultsIterator.close();\n+      resultsIterator = null;\n       return IterOutcome.NONE;\n     }\n   }\n@@ -561,11 +630,11 @@ private IterOutcome loadBatch() {\n       // out of memory and that no work as in-flight and thus abandoned.\n       // Consider removing this case once resource management is in place.\n \n-      logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n+      logger.error(\"received OUT_OF_MEMORY, trying to spill\");\n       if (bufferedBatches.size() > 2) {\n         spillFromMemory();\n       } else {\n-        logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n+        logger.error(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n         return IterOutcome.OUT_OF_MEMORY;\n       }\n       break;\n@@ -693,9 +762,7 @@ private void setupSchema(IterOutcome upstream)  {\n     // Coerce all existing batches to the new schema.\n \n     for (BatchGroup b : bufferedBatches) {\n-//      System.out.println(\"Before: \" + allocator.getAllocatedMemory()); // Debug only\n       b.setSchema(schema);\n-//      System.out.println(\"After: \" + allocator.getAllocatedMemory()); // Debug only\n     }\n     for (BatchGroup b : spilledRuns) {\n       b.setSchema(schema);\n@@ -765,12 +832,12 @@ private void processBatch() {\n       spillFromMemory();\n     }\n \n-    // Sanity check. We should now be above the spill point.\n+    // Sanity check. We should now be below the buffer memory maximum.\n \n     long startMem = allocator.getAllocatedMemory();\n-    if (memoryLimit - startMem < spillPoint) {\n-      logger.error( \"ERROR: Failed to spill below the spill point. Spill point = {}, free memory = {}\",\n-                    spillPoint, memoryLimit - startMem);\n+    if (startMem > bufferMemoryPool) {\n+      logger.error( \"ERROR: Failed to spill above buffer limit. Buffer pool = {}, memory = {}\",\n+          bufferMemoryPool, startMem);\n     }\n \n     // Convert the incoming batch to the agreed-upon schema.\n@@ -835,7 +902,7 @@ private void processBatch() {\n     RecordBatchData rbd = new RecordBatchData(convertedBatch, allocator);\n     try {\n       rbd.setSv2(sv2);\n-      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, batchSize));\n+      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, sizer.netSize()));\n       if (peakNumBatches < bufferedBatches.size()) {\n         peakNumBatches = bufferedBatches.size();\n         stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);\n@@ -857,9 +924,6 @@ private void processBatch() {\n   private RecordBatchSizer analyzeIncomingBatch() {\n     RecordBatchSizer sizer = new RecordBatchSizer(incoming);\n     sizer.applySv2();\n-    if (! hasOversizeCols) {\n-      hasOversizeCols = sizer.checkOversizeCols();\n-    }\n     if (inputBatchCount == 0) {\n       logger.debug(\"{}\", sizer.toString());\n     }\n@@ -887,7 +951,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long actualBatchSize = sizer.actualSize();\n     int actualRecordCount = sizer.rowCount();\n \n-    if (actualBatchSize < memoryDelta) {\n+    if (actualBatchSize != memoryDelta) {\n       logger.debug(\"Memory delta: {}, actual batch size: {}, Diff: {}\",\n                    memoryDelta, actualBatchSize, memoryDelta - actualBatchSize);\n     }\n@@ -905,11 +969,12 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // We actually track the max density seen, and compare to 75% of that since\n     // Parquet produces very low density record batches.\n \n-    if (sizer.getAvgDensity() < maxDensity * 0.75) {\n-      logger.debug(\"Saw low density batch. Density: {}\", sizer.getAvgDensity());\n+    if (sizer.avgDensity() < maxDensity * 3 / 4 && sizer.avgDensity() != lastDensity) {\n+      logger.trace(\"Saw low density batch. Density: {}\", sizer.avgDensity());\n+      lastDensity = sizer.avgDensity();\n       return;\n     }\n-    maxDensity = Math.max(maxDensity, sizer.getAvgDensity());\n+    maxDensity = Math.max(maxDensity, sizer.avgDensity());\n \n     // We know the batch size and number of records. Use that to estimate\n     // the average record size. Since a typical batch has many records,\n@@ -934,6 +999,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long origInputBatchSize = estimatedInputBatchSize;\n     estimatedInputBatchSize = Math.max(estimatedInputBatchSize, actualBatchSize);\n \n+    // The row width may end up as zero if all fields are nulls or some\n+    // other unusual situation. In this case, assume a width of 10 just\n+    // to avoid lots of special case code.\n+\n+    if (estimatedRowWidth == 0) {\n+      estimatedRowWidth = 10;\n+    }\n+\n     // Go no further if nothing changed.\n \n     if (estimatedRowWidth == origRowEstimate && estimatedInputBatchSize == origInputBatchSize) {\n@@ -948,50 +1021,51 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // spill batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each spill batch, whichever is less.\n \n-    spillBatchRowCount = (int) Math.max(1, spillBatchSize / estimatedRowWidth);\n+    spillBatchRowCount = (int) Math.max(1, preferredSpillBatchSize / estimatedRowWidth / 2);\n     spillBatchRowCount = Math.min(spillBatchRowCount, Character.MAX_VALUE);\n \n+    // Compute the actual spill batch size which may be larger or smaller\n+    // than the preferred size depending on the row width. Double the estimated\n+    // memory needs to allow for power-of-two rounding.\n+\n+    targetSpillBatchSize = spillBatchRowCount * estimatedRowWidth * 2;\n+\n     // Determine the number of records per batch per merge step. The goal is to\n     // merge batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each merge batch, whichever is less.\n \n-    targetMergeBatchSize = preferredMergeBatchSize;\n-    mergeBatchRowCount = (int) Math.max(1, targetMergeBatchSize / estimatedRowWidth);\n+    mergeBatchRowCount = (int) Math.max(1, preferredMergeBatchSize / estimatedRowWidth / 2);\n     mergeBatchRowCount = Math.min(mergeBatchRowCount, Character.MAX_VALUE);\n+    mergeBatchRowCount = Math.max(1,  mergeBatchRowCount);\n+    targetMergeBatchSize = mergeBatchRowCount * estimatedRowWidth * 2;\n \n     // Determine the minimum memory needed for spilling. Spilling is done just\n     // before accepting a batch, so we must spill if we don't have room for a\n     // (worst case) input batch. To spill, we need room for the output batch created\n     // by merging the batches already in memory. Double this to allow for power-of-two\n     // memory allocations.\n \n-    spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n+    long spillPoint = estimatedInputBatchSize + 2 * targetSpillBatchSize;\n \n     // The merge memory pool assumes we can spill all input batches. To make\n     // progress, we must have at least two merge batches (same size as an output\n     // batch) and one output batch. Again, double to allow for power-of-two\n     // allocation and add one for a margin of error.\n \n-    int minMergeBatches = 2 * 3 + 1;\n-    long minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+    long minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n \n     // If we are in a low-memory condition, then we might not have room for the\n     // default output batch size. In that case, pick a smaller size.\n \n-    long minMemory = Math.max(spillPoint, minMergeMemory);\n-    if (minMemory > memoryLimit) {\n+    if (minMergeMemory > memoryLimit) {\n \n-      // Figure out the minimum output batch size based on memory, but can't be\n-      // any smaller than the defined minimum.\n+      // Figure out the minimum output batch size based on memory,\n+      // must hold at least one complete row.\n \n-      targetMergeBatchSize = Math.max(MIN_MERGED_BATCH_SIZE, memoryLimit / minMergeBatches);\n-\n-      // Regardless of anything else, the batch must hold at least one\n-      // complete row.\n-\n-      targetMergeBatchSize = Math.max(estimatedRowWidth, targetMergeBatchSize);\n-      spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n-      minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+      long mergeAllowance = memoryLimit - 2 * targetSpillBatchSize;\n+      targetMergeBatchSize = Math.max(estimatedRowWidth, mergeAllowance / 2);\n+      mergeBatchRowCount = (int) (targetMergeBatchSize / estimatedRowWidth / 2);\n+      minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n     }\n \n     // Determine the minimum total memory we would need to receive two input\n@@ -1004,7 +1078,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // runs when reading from disk.\n \n     bufferMemoryPool = memoryLimit - spillPoint;\n-    mergeMemoryPool = Math.max(minMergeMemory,\n+    mergeMemoryPool = Math.max(memoryLimit - minMergeMemory,\n                                (long) ((memoryLimit - 3 * targetMergeBatchSize) * 0.95));\n \n     // Sanity check: if we've been given too little memory to make progress,\n@@ -1021,14 +1095,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // Log the calculated values. Turn this on if things seem amiss.\n     // Message will appear only when the values change.\n \n-    logger.debug(\"Memory Estimates: record size = {} bytes; input batch = {} bytes, {} records; \" +\n-                  \"merge batch size = {} bytes, {} records; \" +\n-                  \"output batch size = {} bytes, {} records; \" +\n-                  \"Available memory: {}, spill point = {}, min. merge memory = {}\",\n-                estimatedRowWidth, estimatedInputBatchSize, actualRecordCount,\n-                spillBatchSize, spillBatchRowCount,\n-                targetMergeBatchSize, mergeBatchRowCount,\n-                memoryLimit, spillPoint, minMergeMemory);\n+    logger.debug(\"Input Batch Estimates: record size = {} bytes; input batch = {} bytes, {} records\",\n+                 estimatedRowWidth, estimatedInputBatchSize, actualRecordCount);\n+    logger.debug(\"Merge batch size = {} bytes, {} records; spill file size: {} bytes\",\n+                 targetSpillBatchSize, spillBatchRowCount, spillFileSize);\n+    logger.debug(\"Output batch size = {} bytes, {} records\",\n+                 targetMergeBatchSize, mergeBatchRowCount);\n+    logger.debug(\"Available memory: {}, buffer memory = {}, merge memory = {}\",\n+                 memoryLimit, bufferMemoryPool, mergeMemoryPool);\n   }\n \n   /**\n@@ -1050,14 +1124,7 @@ private boolean isSpillNeeded(int incomingSize) {\n     // Must spill if we are below the spill point (the amount of memory\n     // needed to do the minimal spill.)\n \n-    if (allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool) {\n-      return true; }\n-\n-    // For test purposes, configuration may have set a limit on the number of\n-    // batches in memory. Spill if we exceed this limit. (By default the number\n-    // of in-memory batches is unlimited.)\n-\n-    return bufferedBatches.size() > bufferedBatchLimit;\n+    return allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool;\n   }\n \n   /**\n@@ -1068,8 +1135,8 @@ private boolean isSpillNeeded(int incomingSize) {\n    */\n \n   private IterOutcome sortInMemory() {\n-    logger.info(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n-                bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n+    logger.debug(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n+                 bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n \n     // Note the difference between how we handle batches here and in the spill/merge\n     // case. In the spill/merge case, this class decides on the batch size to send\n@@ -1088,8 +1155,8 @@ private IterOutcome sortInMemory() {\n         sortState = SortState.DONE;\n         return IterOutcome.STOP;\n       } else {\n-        logger.info(\"Completed in-memory sort. Memory = {}\",\n-                allocator.getAllocatedMemory());\n+        logger.debug(\"Completed in-memory sort. Memory = {}\",\n+                     allocator.getAllocatedMemory());\n         resultsIterator = memoryMerge;\n         memoryMerge = null;\n         sortState = SortState.DELIVER;\n@@ -1111,9 +1178,9 @@ private IterOutcome sortInMemory() {\n    */\n \n   private IterOutcome mergeSpilledRuns() {\n-    logger.info(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n-                inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n-                bufferedBatches.size(), spilledRuns.size());\n+    logger.debug(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n+                 inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n+                 bufferedBatches.size(), spilledRuns.size());\n \n     // Consolidate batches to a number that can be merged in\n     // a single last pass.\n@@ -1132,7 +1199,8 @@ private IterOutcome mergeSpilledRuns() {\n     allBatches.addAll(spilledRuns);\n     spilledRuns.clear();\n \n-    logger.info(\"Starting merge phase. Runs = {}, Alloc. memory = {}\", allBatches.size(), allocator.getAllocatedMemory());\n+    logger.debug(\"Starting merge phase. Runs = {}, Alloc. memory = {}\",\n+                 allBatches.size(), allocator.getAllocatedMemory());\n \n     // Do the final merge as a results iterator.\n \n@@ -1153,9 +1221,13 @@ private boolean consolidateBatches() {\n \n     // Can't merge more than will fit into memory at one time.\n \n-    int maxMergeWidth = (int) (mergeMemoryPool / targetMergeBatchSize);\n+    int maxMergeWidth = (int) (mergeMemoryPool / targetSpillBatchSize);\n     maxMergeWidth = Math.min(mergeLimit, maxMergeWidth);\n \n+    // But, must merge at least two batches.\n+\n+    maxMergeWidth = Math.max(maxMergeWidth, 2);\n+\n     // If we can't fit all batches in memory, must spill any in-memory\n     // batches to make room for multiple spill-merge-spill cycles.\n \n@@ -1177,7 +1249,7 @@ private boolean consolidateBatches() {\n       // is available, spill some in-memory batches.\n \n       long allocated = allocator.getAllocatedMemory();\n-      long totalNeeds = spilledRunsCount * targetMergeBatchSize + allocated;\n+      long totalNeeds = spilledRunsCount * targetSpillBatchSize + allocated;\n       if (totalNeeds > mergeMemoryPool) {\n         spillFromMemory();\n         return true;\n@@ -1231,52 +1303,46 @@ private boolean consolidateBatches() {\n    * This method spills only half the accumulated batches\n    * minimizing unnecessary disk writes. The exact count must lie between\n    * the minimum and maximum spill counts.\n-    */\n+   */\n \n   private void spillFromMemory() {\n \n     // Determine the number of batches to spill to create a spill file\n     // of the desired size. The actual file size might be a bit larger\n     // or smaller than the target, which is expected.\n \n-    long estSize = 0;\n     int spillCount = 0;\n+    long spillSize = 0;\n     for (InputBatch batch : bufferedBatches) {\n-      estSize += batch.getDataSize();\n-      if (estSize > spillFileSize) {\n-        break; }\n+      long batchSize = batch.getDataSize();\n+      spillSize += batchSize;\n       spillCount++;\n+      if (spillSize + batchSize / 2 > spillFileSize) {\n+        break; }\n     }\n \n-    // Should not happen, but just to be sure...\n+    // Must always spill at least 2, even if this creates an over-size\n+    // spill file. But, if this is a final consolidation, we may have only\n+    // a single batch.\n \n-    if (spillCount == 0) {\n-      return; }\n+    spillCount = Math.max(spillCount, 2);\n+    spillCount = Math.min(spillCount, bufferedBatches.size());\n \n     // Do the actual spill.\n \n-    logger.trace(\"Starting spill from memory. Memory = {}, Buffered batch count = {}, Spill batch count = {}\",\n-                 allocator.getAllocatedMemory(), bufferedBatches.size(), spillCount);\n     mergeAndSpill(bufferedBatches, spillCount);\n   }\n \n   private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n-    if (count == 0) {\n-      return; }\n     spilledRuns.add(doMergeAndSpill(source, count));\n   }\n \n   private BatchGroup.SpilledRun doMergeAndSpill(LinkedList<? extends BatchGroup> batchGroups, int spillCount) {\n     List<BatchGroup> batchesToSpill = Lists.newArrayList();\n     spillCount = Math.min(batchGroups.size(), spillCount);\n     assert spillCount > 0 : \"Spill count to mergeAndSpill must not be zero\";\n-    long spillSize = 0;\n     for (int i = 0; i < spillCount; i++) {\n-      @SuppressWarnings(\"resource\")\n-      BatchGroup batch = batchGroups.pollFirst();\n-      assert batch != null : \"Encountered a null batch during merge and spill operation\";\n-      batchesToSpill.add(batch);\n-      spillSize += batch.getDataSize();\n+      batchesToSpill.add(batchGroups.pollFirst());\n     }\n \n     // Merge the selected set of matches and write them to the\n@@ -1288,8 +1354,11 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n     BatchGroup.SpilledRun newGroup = null;\n     try (AutoCloseable ignored = AutoCloseables.all(batchesToSpill);\n          CopierHolder.BatchMerger merger = copierHolder.startMerge(schema, batchesToSpill, spillBatchRowCount)) {\n-      logger.trace(\"Merging and spilling to {}\", outputFile);\n-      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext, spillSize);\n+      logger.trace(\"Spilling {} of {} batches, {} rows, memory = {}, write to {}\",\n+                   batchesToSpill.size(), bufferedBatches.size() + batchesToSpill.size(),\n+                   spillBatchRowCount,\n+                   allocator.getAllocatedMemory(), outputFile);\n+      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext);\n \n       // The copier will merge records from the buffered batches into\n       // the outputContainer up to targetRecordCount number of rows.\n@@ -1298,8 +1367,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       while (merger.next()) {\n \n         // Add a new batch of records (given by merger.getOutput()) to the spill\n-        // file, opening the file if not yet open, and creating the target\n-        // directory if it does not yet exist.\n+        // file.\n         //\n         // note that addBatch also clears the merger's output container\n \n@@ -1322,7 +1390,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       // It will release the memory in the close() call.\n \n       try {\n-        // Rethrow so we can organize how to handle the error.\n+        // Rethrow so we can decide how to handle the error.\n \n         throw e;\n       }\n@@ -1444,11 +1512,12 @@ public void close() {\n     } catch (RuntimeException e) {\n       ex = (ex == null) ? e : ex;\n     }\n-    try {\n-      allocator.close();\n-    } catch (RuntimeException e) {\n-      ex = (ex == null) ? e : ex;\n-    }\n+    // Note: allocator is closed by the FragmentManager\n+//    try {\n+//      allocator.close();\n+//    } catch (RuntimeException e) {\n+//      ex = (ex == null) ? e : ex;\n+//    }\n     if (ex != null) {\n       throw ex;\n     }",
                "additions": 180,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "status": "modified",
                "changes": 291,
                "deletions": 111,
                "sha": "a1162a02ecade691ba3b1054e0e1951b60d42517",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -78,6 +78,7 @@ public void clear() {\n   }\n \n \n+  @SuppressWarnings(\"resource\")\n   @Override\n   public VectorWrapper<?> getChildWrapper(int[] ids) {\n     if (ids.length == 1) {\n@@ -108,4 +109,13 @@ public void transfer(VectorWrapper<?> destination) {\n     vector.makeTransferPair(((SimpleVectorWrapper<?>)destination).vector).transfer();\n   }\n \n+  @Override\n+  public String toString() {\n+    if (vector == null) {\n+      return \"null\";\n+    } else {\n+      return vector.toString();\n+    }\n+  }\n+\n }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "0a9f3d6e129b5cfcef7274b8cd3c8de06b1523bf",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -69,7 +69,7 @@ public int getBufferSizeFor(final int valueCount) {\n \n   @Override\n   public int getValueCapacity(){\n-    return (int) (data.capacity() *1.0 / ${type.width});\n+    return data.capacity() / ${type.width};\n   }\n \n   @Override\n@@ -196,7 +196,7 @@ public void load(SerializedField metadata, DrillBuf buffer) {\n     data = buffer.slice(0, actualLength);\n     data.retain(1);\n     data.writerIndex(actualLength);\n-    }\n+  }\n \n   public TransferPair getTransferPair(BufferAllocator allocator){\n     return new TransferImpl(getField(), allocator);\n@@ -227,6 +227,11 @@ public void splitAndTransferTo(int startIndex, int length, ${minor.class}Vector\n     target.data.writerIndex(sliceLength);\n   }\n \n+  @Override\n+  public int getPayloadByteCount() {\n+    return getAccessor().getValueCount() * ${type.width};\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     private ${minor.class}Vector to;\n \n@@ -390,7 +395,6 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       return p.plusDays(days).plusMillis(millis);\n     }\n \n-\n     public StringBuilder getAsStringBuilder(int index) {\n       final int offsetIndex = index * ${type.width};\n \n@@ -539,6 +543,7 @@ public DateTime getObject(int index) {\n     public ${friendlyType} getObject(int index) {\n       return get(index);\n     }\n+\n     public ${minor.javaType!type.javaType} getPrimitiveObject(int index) {\n       return get(index);\n     }\n@@ -557,9 +562,7 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       holder.isSet = 1;\n       holder.value = data.get${(minor.javaType!type.javaType)?cap_first}(index * ${type.width});\n     }\n-\n-\n-   </#if> <#-- type.width -->\n+    </#if> <#-- type.width -->\n  }\n \n  /**\n@@ -728,84 +731,84 @@ public void generateTestData(int count) {\n    }\n \n    <#else> <#-- type.width <= 8 -->\n-   public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n-   }\n+    public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n+    }\n \n    public void setSafe(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n      while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, value);\n-   }\n-\n-   protected void set(int index, ${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+        reAlloc();\n+      }\n+      set(index, value);\n+    }\n \n-   public void setSafe(int index, ${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, ${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   protected void set(int index, Nullable${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+    public void setSafe(int index, ${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void setSafe(int index, Nullable${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, Nullable${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   @Override\n-   public void generateTestData(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n-       }else{\n-         set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n-       }\n-     }\n-   }\n+    public void setSafe(int index, Nullable${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void generateTestDataAlt(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, (${(minor.javaType!type.javaType)}) 1);\n-       }else{\n-         set(i, (${(minor.javaType!type.javaType)}) 0);\n-       }\n-     }\n-   }\n+    @Override\n+    public void generateTestData(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n+        } else {\n+          set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n+        }\n+      }\n+    }\n+\n+    public void generateTestDataAlt(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, (${(minor.javaType!type.javaType)}) 1);\n+        } else {\n+          set(i, (${(minor.javaType!type.javaType)}) 0);\n+        }\n+      }\n+    }\n \n   </#if> <#-- type.width -->\n \n-   @Override\n-   public void setValueCount(int valueCount) {\n-     final int currentValueCapacity = getValueCapacity();\n-     final int idx = (${type.width} * valueCount);\n-     while(valueCount > getValueCapacity()) {\n-       reAlloc();\n-     }\n-     if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n-       incrementAllocationMonitor();\n-     } else if (allocationMonitor > 0) {\n-       allocationMonitor = 0;\n-     }\n-     VectorTrimmer.trim(data, idx);\n-     data.writerIndex(valueCount * ${type.width});\n-   }\n- }\n+    @Override\n+    public void setValueCount(int valueCount) {\n+      final int currentValueCapacity = getValueCapacity();\n+      final int idx = (${type.width} * valueCount);\n+      while(valueCount > getValueCapacity()) {\n+        reAlloc();\n+      }\n+      if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n+        incrementAllocationMonitor();\n+      } else if (allocationMonitor > 0) {\n+        allocationMonitor = 0;\n+      }\n+      VectorTrimmer.trim(data, idx);\n+      data.writerIndex(valueCount * ${type.width});\n+    }\n+  }\n }\n \n </#if> <#-- type.major -->",
                "additions": 76,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "status": "modified",
                "changes": 149,
                "deletions": 73,
                "sha": "b2a5dc389601dabde82592b3fd07621f352274e2",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "filename": "exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/FixedValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -45,12 +45,24 @@\n  * NB: this class is automatically generated from ${.template_name} and ValueVectorTypes.tdd using FreeMarker.\n  */\n @SuppressWarnings(\"unused\")\n-public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector{\n+public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(${className}.class);\n \n   private final FieldReader reader = new Nullable${minor.class}ReaderImpl(Nullable${minor.class}Vector.this);\n \n   private final MaterializedField bitsField = MaterializedField.create(\"$bits$\", Types.required(MinorType.UINT1));\n+\n+  /**\n+   * Set value flag. Meaning:\n+   * <ul>\n+   * <li>0: value is not set (value is null).</li>\n+   * <li>1: value is set (value is not null).</li>\n+   * </ul>\n+   * That is, a 1 means that the values vector has a value. 0\n+   * means that the vector is null. Thus, all values start as\n+   * not set (null) and must be explicitly set (made not null).\n+   */\n+\n   private final UInt1Vector bits = new UInt1Vector(bitsField, allocator);\n   private final ${valuesName} values = new ${minor.class}Vector(field, allocator);\n \n@@ -108,8 +120,8 @@ public int getBufferSizeFor(final int valueCount) {\n       return 0;\n     }\n \n-    return values.getBufferSizeFor(valueCount)\n-        + bits.getBufferSizeFor(valueCount);\n+    return values.getBufferSizeFor(valueCount) +\n+           bits.getBufferSizeFor(valueCount);\n   }\n \n   @Override\n@@ -163,6 +175,18 @@ public boolean allocateNewSafe() {\n     return success;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return bits.getAllocatedByteCount() + values.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // For nullable, we include all values, null or not, in computing\n+    // the value length.\n+    return bits.getPayloadByteCount() + values.getPayloadByteCount();\n+  }\n+\n   <#if type.major == \"VarLen\">\n   @Override\n   public void allocateNew(int totalBytes, int valueCount) {",
                "additions": 27,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "status": "modified",
                "changes": 30,
                "deletions": 3,
                "sha": "b242728078ebfa734cd4a8a868edbeba4e665949",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "filename": "exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/NullableValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -201,6 +201,22 @@ public MaterializedField getField() {\n     return field;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Most vectors are held inside the internal map.\n+\n+    int count = internalMap.getAllocatedByteCount();\n+    if (bit != null) {\n+      count += bit.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return internalMap.getPayloadByteCount();\n+  }\n+\n   @Override\n   public TransferPair getTransferPair(BufferAllocator allocator) {\n     return new TransferImpl(field, allocator);",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "status": "modified",
                "changes": 16,
                "deletions": 0,
                "sha": "93854e782a59a445b2e854161248a61d83920164",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "filename": "exec/vector/src/main/codegen/templates/UnionVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/UnionVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -238,6 +238,25 @@ public boolean copyFromSafe(int fromIndex, int thisIndex, ${minor.class}Vector f\n     return true;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsetVector.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    UInt${type.width}Vector.Accessor a = offsetVector.getAccessor();\n+    int count = a.getValueCount();\n+    if (count == 0) {\n+      return 0;\n+    } else {\n+      // If 1 or more values, then the last value is set to\n+      // the offset of the next value, which is the same as\n+      // the length of existing values.\n+      return a.get(count-1);\n+    }\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     ${minor.class}Vector to;\n ",
                "additions": 19,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "status": "modified",
                "changes": 19,
                "deletions": 0,
                "sha": "ea3c9de7c90ab37adfe7e24f8dcac2bc7f42c2f9",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "filename": "exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/VariableLengthVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -87,4 +87,9 @@ public DrillBuf getBuffer() {\n    * the value vector. The purpose is to move the value vector to a \"mutate\" state\n    */\n   public void reset() {}\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return data.capacity();\n+  }\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "4def5b83761f8a90b872e2df2315c1035d687658",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -449,4 +449,10 @@ public void clear() {\n     this.valueCount = 0;\n     super.clear();\n   }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // One byte per value\n+    return valueCount;\n+  }\n }",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "a6c0ceafdaefc5c6245beed98b31a75d6605acfb",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -218,4 +218,16 @@ public void get(int index, ObjectHolder holder){\n       holder.obj = getObject(index);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n }",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "status": "modified",
                "changes": 12,
                "deletions": 0,
                "sha": "f69dc9807165c46d6205296ce542131149122d1b",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -175,6 +175,18 @@\n    */\n   void load(SerializedField metadata, DrillBuf buffer);\n \n+  /**\n+   * Return the total memory consumed by all buffers within this vector.\n+   */\n+\n+  int getAllocatedByteCount();\n+\n+  /**\n+   * Return the number of value bytes consumed by actual data.\n+   */\n+\n+  int getPayloadByteCount();\n+\n   /**\n    * An abstraction that is used to read from this vector instance.\n    */",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "status": "modified",
                "changes": 12,
                "deletions": 0,
                "sha": "f4c793556f603e945eee08b2aebeb141d6a495b1",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -17,9 +17,7 @@\n  */\n package org.apache.drill.exec.vector;\n \n-import io.netty.buffer.DrillBuf;\n-\n-public interface VariableWidthVector extends ValueVector{\n+public interface VariableWidthVector extends ValueVector {\n \n   /**\n    * Allocate a new memory space for this vector.  Must be called prior to using the ValueVector.",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "status": "modified",
                "changes": 4,
                "deletions": 3,
                "sha": "d04234c9f762c3683a78f8a7b7fe336e6afc9c86",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -176,4 +176,14 @@ public FieldReader getReader() {\n \n   @Override\n   public void load(UserBitShared.SerializedField metadata, DrillBuf buffer) { }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return 0;\n+  }\n }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "9181f2042fa6a3b292327fd9129faeb415ca1b38",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -266,7 +266,7 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n \n   @Override\n   public int getBufferSize() {\n-    int actualBufSize = 0 ;\n+    int actualBufSize = 0;\n \n     for (final ValueVector v : vectors.values()) {\n       for (final DrillBuf buf : v.getBuffers(false)) {\n@@ -275,4 +275,24 @@ public int getBufferSize() {\n     }\n     return actualBufSize;\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getPayloadByteCount();\n+    }\n+    return count;\n+  }\n }",
                "additions": 21,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "status": "modified",
                "changes": 22,
                "deletions": 1,
                "sha": "baba0865d8956fd835fa5e61600e69d71771afd3",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -209,6 +209,17 @@ protected void replaceDataVector(ValueVector v) {\n     vector = v;\n   }\n \n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + vector.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + vector.getPayloadByteCount();\n+  }\n+\n   public abstract class BaseRepeatedAccessor extends BaseValueVector.BaseAccessor implements RepeatedAccessor {\n \n     @Override\n@@ -256,5 +267,4 @@ public void setValueCount(int valueCount) {\n       vector.getMutator().setValueCount(childValueCount);\n     }\n   }\n-\n }",
                "additions": 11,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "status": "modified",
                "changes": 12,
                "deletions": 1,
                "sha": "1664b0a2930bc39c6643b93842658c6e09e84982",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -317,4 +317,14 @@ public void setValueCount(int valueCount) {\n       bits.getMutator().setValueCount(valueCount);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + bits.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + bits.getPayloadByteCount() + super.getPayloadByteCount();\n+  }\n }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "f71baa7e3c81a992f30cc24b49cedc77bf5ee603",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -426,4 +426,14 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n   public void copyFromSafe(int fromIndex, int thisIndex, RepeatedListVector from) {\n     delegate.copyFromSafe(fromIndex, thisIndex, from.delegate);\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return delegate.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return delegate.getPayloadByteCount();\n+  }\n }",
                "additions": 10,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "b5c97bf31bdba186be36560f546275adb3afda6f",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            },
            {
                "patch": "@@ -584,4 +584,9 @@ public void clear() {\n       vector.clear();\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return super.getAllocatedByteCount( ) + offsets.getAllocatedByteCount();\n+  }\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "3707ff0d3d6060c31fa3732e59ecc3acb268566d",
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd"
            }
        ],
        "bug_id": "drill_55",
        "parent": "https://github.com/apache/drill/commit/69de3a1e409bb1fb9a25e679ce1750d9f9daf238",
        "message": "DRILL-5284: Roll-up of final fixes for managed sort\n\nSee subtasks for details.\n\n* Provide detailed, accurate estimate of size consumed by a record batch\n* Managed external sort spills too often with Parquet data\n* Managed External Sort fails with OOM\n* External sort refers to the deprecated HDFS fs.default.name param\n* Config param drill.exec.sort.external.batch.size is not used\n* NPE in managed external sort while spilling to disk\n* External Sort BatchGroup leaks memory if an OOM occurs during read\n* DRILL-5294: Under certain low-memory conditions, need to force the sort to merge\ntwo batches to make progress, even though this is a bit more than\ncomfortably fits into memory.\n\nclose #761",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/3896a58243f310c5d9466a98edc205b61f9dd2e7",
        "file": [
            {
                "patch": "@@ -95,7 +95,7 @@ public void testGetJsonObject() throws Exception {\n   @Test // DRILL-3272\n   public void testIf() throws Exception {\n     testBuilder()\n-        .sqlQuery(\"select `if`(1999 > 2000, 'latest', 'old') Period from hive.kv limit 1\")\n+        .sqlQuery(\"select `if`(1999 > 2000, 'latest', 'old') `Period` from hive.kv limit 1\")\n         .ordered()\n         .baselineColumns(\"Period\")\n         .baselineValues(\"old\")",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "d4e0b5cb9c29d3ddcfb4a034067c97e1bfdf95eb",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -469,7 +469,7 @@ public void selectUser0_db_u1g1_only() throws Exception {\n \n     errorMsgTestHelper(\n         String.format(\"SELECT * FROM hive.%s.%s ORDER BY gpa DESC LIMIT 2\", db_u1g1_only, u1g1_student_all_755),\n-        String.format(\"Table 'hive.%s.%s' not found\", db_u1g1_only, u1g1_student_all_755));\n+        String.format(\"Object '%s' not found within 'hive.%s'\", u1g1_student_all_755, db_u1g1_only));\n   }\n \n   // Try to read the tables \"user1\" has access to read in db_general.\n@@ -489,7 +489,7 @@ public void selectUser1_db_u0_only() throws Exception {\n \n     errorMsgTestHelper(\n         String.format(\"SELECT * FROM hive.%s.%s ORDER BY gpa DESC LIMIT 2\", db_u0_only, u0_student_all_755),\n-        String.format(\"Table 'hive.%s.%s' not found\", db_u0_only, u0_student_all_755));\n+        String.format(\"Object '%s' not found within 'hive.%s'\", u0_student_all_755, db_u0_only));\n   }\n \n   private static void queryViewHelper(final String queryUser, final String query) throws Exception {",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "685d3bf6320bc06efd76528521d11bd9c91abccd",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -88,7 +88,7 @@ private SqlNode expandAvg(\n       final SqlNode arg) {\n     final SqlParserPos pos = SqlParserPos.ZERO;\n     final SqlNode sum =\n-        SqlStdOperatorTable.SUM.createCall(pos, arg);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, arg);\n     final SqlNode count =\n         SqlStdOperatorTable.COUNT.createCall(pos, arg);\n     final SqlNode sumAsDouble =\n@@ -128,9 +128,9 @@ private SqlNode expandVariance(\n     final SqlNode argSquared =\n         SqlStdOperatorTable.MULTIPLY.createCall(pos, castHighArg, castHighArg);\n     final SqlNode sumArgSquared =\n-        SqlStdOperatorTable.SUM.createCall(pos, argSquared);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, argSquared);\n     final SqlNode sum =\n-        SqlStdOperatorTable.SUM.createCall(pos, castHighArg);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, castHighArg);\n     final SqlNode sumSquared =\n         SqlStdOperatorTable.MULTIPLY.createCall(pos, sum, sum);\n     final SqlNode count =",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "status": "modified",
                "changes": 8,
                "deletions": 4,
                "sha": "bfb4c05c6c73e87c2fecdb9db1939135cebaf2f7",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.calcite.sql.SqlOperatorBinding;\n import org.apache.calcite.sql.SqlSyntax;\n import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.type.SqlReturnTypeInference;\n import org.apache.calcite.sql.validate.SqlMonotonicity;\n import org.apache.calcite.sql.validate.SqlValidator;\n@@ -42,6 +43,11 @@\n  * simply forwards the method calls to the wrapped SqlAggFunction.\n  */\n public class DrillCalciteSqlAggFunctionWrapper extends SqlAggFunction implements DrillCalciteSqlWrapper {\n+\n+  public final static DrillCalciteSqlAggFunctionWrapper SUM =\n+      new DrillCalciteSqlAggFunctionWrapper(SqlStdOperatorTable.SUM,\n+          SqlStdOperatorTable.SUM.getReturnTypeInference());\n+\n   private final SqlAggFunction operator;\n \n   @Override",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "bd46d2d45a8594c55be5cb897575f8548252b65e",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -22,6 +22,7 @@\n import java.util.Set;\n \n import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Sets;\n import org.apache.calcite.adapter.java.JavaTypeFactory;\n@@ -43,7 +44,10 @@\n import org.apache.calcite.rel.type.RelDataTypeFactory;\n import org.apache.calcite.rel.type.RelDataTypeSystemImpl;\n import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexLiteral;\n import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.runtime.Hook;\n import org.apache.calcite.schema.SchemaPlus;\n import org.apache.calcite.sql.SqlNode;\n import org.apache.calcite.sql.SqlOperatorTable;\n@@ -73,7 +77,6 @@\n import org.apache.drill.exec.planner.logical.DrillConstExecutor;\n import org.apache.drill.exec.planner.physical.DrillDistributionTraitDef;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n-import org.apache.drill.exec.planner.sql.parser.impl.DrillParserWithCompoundIdConverter;\n import org.apache.drill.exec.rpc.user.UserSession;\n \n import com.google.common.base.Joiner;\n@@ -106,6 +109,7 @@\n \n   private String sql;\n   private VolcanoPlanner planner;\n+  private boolean useRootSchema = false;\n \n \n   public SqlConverter(QueryContext context) {\n@@ -217,6 +221,15 @@ public void disallowTemporaryTables() {\n     catalog.disallowTemporaryTables();\n   }\n \n+  /**\n+   * Is root schema path should be used as default schema path.\n+   *\n+   * @param useRoot flag\n+   */\n+  public void useRootSchemaAsDefault(boolean useRoot) {\n+    useRootSchema = useRoot;\n+  }\n+\n   private class DrillValidator extends SqlValidatorImpl {\n     private final Set<SqlValidatorScope> identitySet = Sets.newIdentityHashSet();\n \n@@ -273,6 +286,14 @@ public RelRoot toRel(final SqlNode validatedNode) {\n     final SqlToRelConverter sqlToRelConverter =\n         new SqlToRelConverter(new Expander(), validator, catalog, cluster, DrillConvertletTable.INSTANCE,\n             sqlToRelConverterConfig);\n+\n+    /*\n+     * Sets value to false to avoid simplifying project expressions\n+     * during creating new projects since it may cause changing data mode\n+     * which causes to assertion errors during type validation\n+     */\n+    Hook.REL_BUILDER_SIMPLIFY.add(Hook.property(false));\n+\n     //To avoid unexpected column errors set a value of top to false\n     final RelRoot rel = sqlToRelConverter.convertQuery(validatedNode, false, false);\n     final RelRoot rel2 = rel.withRel(sqlToRelConverter.flattenTypes(rel.rel, true));\n@@ -430,6 +451,28 @@ public RexNode ensureType(\n         boolean matchNullability) {\n       return node;\n     }\n+\n+    /**\n+     * Creates a call to the CAST operator, expanding if possible, and optionally\n+     * also preserving nullability.\n+     *\n+     * <p>Tries to expand the cast, and therefore the result may be something\n+     * other than a {@link RexCall} to the CAST operator, such as a\n+     * {@link RexLiteral} if {@code matchNullability} is false.\n+     *\n+     * @param type             Type to cast to\n+     * @param exp              Expression being cast\n+     * @param matchNullability Whether to ensure the result has the same\n+     *                         nullability as {@code type}\n+     * @return Call to CAST operator\n+     */\n+    @Override\n+    public RexNode makeCast(RelDataType type, RexNode exp, boolean matchNullability) {\n+      if (matchNullability) {\n+        return makeAbstractCast(type, exp);\n+      }\n+      return super.makeCast(type, exp, false);\n+    }\n   }\n \n   /**\n@@ -506,6 +549,14 @@ public void disallowTemporaryTables() {\n       return table;\n     }\n \n+    @Override\n+    public List<List<String>> getSchemaPaths() {\n+      if (useRootSchema) {\n+        return ImmutableList.<List<String>>of(ImmutableList.<String>of());\n+      }\n+      return super.getSchemaPaths();\n+    }\n+\n     /**\n      * check if the schema provided is a valid schema:\n      * <li>schema is not indicated (only one element in the names list)<li/>",
                "additions": 52,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "status": "modified",
                "changes": 53,
                "deletions": 1,
                "sha": "f9005878d2bda142d9aa91b740ef7c80cc4ade88",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -41,8 +41,6 @@\n import org.apache.calcite.rel.core.TableFunctionScan;\n import org.apache.calcite.rel.core.TableScan;\n import org.apache.calcite.rel.logical.LogicalValues;\n-import org.apache.calcite.rel.metadata.CachingRelMetadataProvider;\n-import org.apache.calcite.rel.metadata.ChainedRelMetadataProvider;\n import org.apache.calcite.rel.metadata.JaninoRelMetadataProvider;\n import org.apache.calcite.rel.metadata.RelMetadataProvider;\n import org.apache.calcite.rel.metadata.RelMetadataQuery;\n@@ -622,7 +620,7 @@ public Void visitOp(PhysicalOperator op, Collection<PhysicalOperator> collection\n \n   }\n \n-  private Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException, RelConversionException, ForemanSetupException {\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException, RelConversionException, ForemanSetupException {\n     final SqlNode sqlNodeValidated = config.getConverter().validate(sqlNode);\n     final Pair<SqlNode, RelDataType> typedSqlNode = new Pair<>(sqlNodeValidated, config.getConverter().getOutputType(\n         sqlNodeValidated));",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "status": "modified",
                "changes": 4,
                "deletions": 3,
                "sha": "5c3432309f4ee87de20a390d43fd8bc0c719e191",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -28,8 +28,8 @@\n \n import java.util.List;\n \n+import org.apache.calcite.rel.type.RelDataType;\n import org.apache.calcite.schema.SchemaPlus;\n-import org.apache.calcite.sql.SqlDescribeTable;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n import org.apache.calcite.sql.SqlNode;\n@@ -38,9 +38,12 @@\n import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.parser.SqlParserPos;\n import org.apache.calcite.tools.RelConversionException;\n+import org.apache.calcite.tools.ValidationException;\n+import org.apache.calcite.util.Pair;\n import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.planner.sql.SqlConverter;\n import org.apache.drill.exec.planner.sql.parser.DrillParserUtil;\n import org.apache.drill.exec.planner.sql.parser.DrillSqlDescribeTable;\n import org.apache.drill.exec.work.foreman.ForemanSetupException;\n@@ -134,4 +137,15 @@ public SqlNode rewrite(SqlNode sqlNode) throws RelConversionException, ForemanSe\n           .build(logger);\n     }\n   }\n+\n+  @Override\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException,\n+      RelConversionException, ForemanSetupException {\n+    SqlConverter converter = config.getConverter();\n+    // set this to true since INFORMATION_SCHEMA in the root schema, not in the default\n+    converter.useRootSchemaAsDefault(true);\n+    Pair<SqlNode, RelDataType> sqlNodeRelDataTypePair = super.validateNode(sqlNode);\n+    converter.useRootSchemaAsDefault(false);\n+    return sqlNodeRelDataTypePair;\n+  }\n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "status": "modified",
                "changes": 16,
                "deletions": 1,
                "sha": "4d01424153afd39ee21311a104a2aefc3424b370",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -25,6 +25,7 @@\n \n import java.util.List;\n \n+import org.apache.calcite.rel.type.RelDataType;\n import org.apache.calcite.schema.SchemaPlus;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n@@ -34,9 +35,12 @@\n import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.parser.SqlParserPos;\n import org.apache.calcite.tools.RelConversionException;\n+import org.apache.calcite.tools.ValidationException;\n+import org.apache.calcite.util.Pair;\n import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.planner.sql.SqlConverter;\n import org.apache.drill.exec.planner.sql.parser.DrillParserUtil;\n import org.apache.drill.exec.planner.sql.parser.SqlShowTables;\n import org.apache.drill.exec.store.AbstractSchema;\n@@ -105,4 +109,15 @@ public SqlNode rewrite(SqlNode sqlNode) throws RelConversionException, ForemanSe\n     return new SqlSelect(SqlParserPos.ZERO, null, new SqlNodeList(selectList, SqlParserPos.ZERO),\n         fromClause, where, null, null, null, null, null, null);\n   }\n+\n+  @Override\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException,\n+      RelConversionException, ForemanSetupException {\n+    SqlConverter converter = config.getConverter();\n+    // set this to true since INFORMATION_SCHEMA in the root schema, not in the default\n+    converter.useRootSchemaAsDefault(true);\n+    Pair<SqlNode, RelDataType> sqlNodeRelDataTypePair = super.validateNode(sqlNode);\n+    converter.useRootSchemaAsDefault(false);\n+    return sqlNodeRelDataTypePair;\n+  }\n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "status": "modified",
                "changes": 15,
                "deletions": 0,
                "sha": "7084877b426271fd8b7e4de9d7d1faaf2ec9b008",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -86,9 +86,9 @@ public static RelNode resolveNewTableRel(boolean isNewTableView, List<String> ta\n             .build(logger);\n       }\n \n-      // CTAS's query field list shouldn't have \"*\" when table's field list is specified.\n+      // CTAS's query field list shouldn't have \"**\" when table's field list is specified.\n       for (String field : validatedRowtype.getFieldNames()) {\n-        if (field.equals(\"*\")) {\n+        if (field.equals(\"**\")) {\n           final String tblType = isNewTableView ? \"view\" : \"table\";\n           throw UserException.validationError()\n               .message(\"%s's query field list has a '*', which is invalid when %s's field list is specified.\",",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "72d269999b41f6ecf3d012c092a6eee448b0f8af",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -20,6 +20,7 @@\n import java.util.Collections;\n import java.util.List;\n \n+import com.google.common.base.Function;\n import org.apache.calcite.sql.SqlBasicCall;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n@@ -30,13 +31,19 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n \n-public class DrillCompoundIdentifier extends SqlIdentifier{\n+public class DrillCompoundIdentifier extends SqlIdentifier {\n \n-  List<IdentifierHolder> ids;\n+  private static final Function<String, String> STAR_TO_EMPTY = new Function<String, String>() {\n+    public String apply(String s) {\n+      return s.equals(\"*\") ? \"\" : s;\n+    }\n+  };\n+\n+  private final List<IdentifierHolder> ids;\n \n-  private static List<String> getNames(List<IdentifierHolder> identifiers){\n+  private static List<String> getNames(List<IdentifierHolder> identifiers) {\n     List<String> names = Lists.newArrayListWithCapacity(identifiers.size());\n-    for(IdentifierHolder h : identifiers){\n+    for (IdentifierHolder h : identifiers) {\n       names.add(h.value);\n     }\n     return names;\n@@ -47,74 +54,69 @@ public DrillCompoundIdentifier(List<IdentifierHolder> identifiers) {\n     this.ids = identifiers;\n   }\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DrillCompoundIdentifier.class);\n-\n-  public static Builder newBuilder(){\n+  public static Builder newBuilder() {\n     return new Builder();\n   }\n \n   public static class Builder {\n     private List<IdentifierHolder> identifiers = Lists.newArrayList();\n \n-    public DrillCompoundIdentifier build(){\n+    public DrillCompoundIdentifier build() {\n       return new DrillCompoundIdentifier(identifiers);\n     }\n \n-    public void addString(String name, SqlParserPos pos){\n+    public void addString(String name, SqlParserPos pos) {\n       identifiers.add(new IdentifierHolder(name, pos, false));\n     }\n \n-    public void addIndex(int index, SqlParserPos pos){\n+    public void addIndex(int index, SqlParserPos pos) {\n       identifiers.add(new IdentifierHolder(Integer.toString(index), pos, true));\n     }\n   }\n \n-  public SqlNode getAsSqlNode(){\n-    if(ids.size() == 1){\n+  public SqlNode getAsSqlNode() {\n+    if (ids.size() == 1) {\n       return new SqlIdentifier(Collections.singletonList(ids.get(0).value), ids.get(0).parserPos);\n     }\n \n     int startIndex;\n     SqlNode node;\n \n-    if(ids.get(1).isArray()){\n+    if (ids.get(1).isArray()) {\n       // handle everything post zero index as item operator.\n       startIndex = 1;\n-      node = new SqlIdentifier( //\n-          ImmutableList.of(ids.get(0).value), //\n-          null, //\n-          ids.get(0).parserPos, //\n+      node = new SqlIdentifier(\n+          ImmutableList.of(ids.get(0).value),\n+          null,\n+          ids.get(0).parserPos,\n           ImmutableList.of(ids.get(0).parserPos));\n-    }else{\n+    } else {\n       // handle everything post two index as item operator.\n       startIndex = 2;\n-      node = new SqlIdentifier( //\n-          ImmutableList.of(ids.get(0).value, ids.get(1).value), //\n-          null, //\n-          ids.get(0).parserPos, //\n+      node = new SqlIdentifier(\n+          // Replaces star by empty string. See SqlIdentifier#isStar()\n+          ImmutableList.of(ids.get(0).value, STAR_TO_EMPTY.apply(ids.get(1).value)), null,\n+          ids.get(0).parserPos,\n           ImmutableList.of(ids.get(0).parserPos, ids.get(1).parserPos));\n-\n     }\n-    for(int i = startIndex ; i < ids.size(); i++){\n+    for (int i = startIndex; i < ids.size(); i++) {\n       node = ids.get(i).getNode(node);\n     }\n \n     return node;\n   }\n \n-\n-  public SqlNode getAsCompoundIdentifier(){\n+  public SqlNode getAsCompoundIdentifier() {\n     List<String> names = Lists.newArrayListWithCapacity(ids.size());\n     List<SqlParserPos> pos = Lists.newArrayListWithCapacity(ids.size());\n-    for(int i =0; i < ids.size(); i++){\n-      IdentifierHolder holder = ids.get(i);\n+    for (IdentifierHolder holder : ids) {\n       names.add(holder.value);\n       pos.add(holder.parserPos);\n     }\n     return new SqlIdentifier(names, null, pos.get(0), pos);\n   }\n \n-  private static class IdentifierHolder{\n+  private static class IdentifierHolder {\n     String value;\n     SqlParserPos parserPos;\n     boolean isArray;\n@@ -126,18 +128,18 @@ public IdentifierHolder(String value, SqlParserPos parserPos, boolean isArray) {\n       this.parserPos = parserPos;\n     }\n \n-    public boolean isArray(){\n+    public boolean isArray() {\n       return isArray;\n     }\n \n-    public SqlNode getNode(SqlNode node){\n+    public SqlNode getNode(SqlNode node) {\n       SqlLiteral literal;\n-      if(isArray){\n+      if (isArray) {\n         literal = SqlLiteral.createExactNumeric(value, parserPos);\n-      }else{\n+      } else {\n         literal = SqlLiteral.createCharString(value, parserPos);\n       }\n-      return new SqlBasicCall(SqlStdOperatorTable.ITEM, new SqlNode[]{ node, literal }, parserPos);\n+      return new SqlBasicCall(SqlStdOperatorTable.ITEM, new SqlNode[]{node, literal}, parserPos);\n     }\n \n   }",
                "additions": 37,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "status": "modified",
                "changes": 72,
                "deletions": 35,
                "sha": "a6c75c13fec37c8a64ee3250aa6f1c86302c80b8",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.drill.exec.store.ischema;\n \n+import static org.apache.drill.exec.planner.types.DrillRelDataTypeSystem.DRILL_REL_DATATYPE_SYSTEM;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.CATS_COL_CATALOG_NAME;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.COLS_COL_COLUMN_NAME;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.IS_CATALOG_CONNECT;\n@@ -231,7 +232,7 @@ public void visitTables(String schemaPath, SchemaPlus schema) {\n       // Visit the table, and if requested ...\n       if(shouldVisitTable(schemaPath, tableName, tableType) && visitTable(schemaPath, tableName, table)) {\n         // ... do for each of the table's fields.\n-        final RelDataType tableRow = table.getRowType(new JavaTypeFactoryImpl());\n+        final RelDataType tableRow = table.getRowType(new JavaTypeFactoryImpl(DRILL_REL_DATATYPE_SYSTEM));\n         for (RelDataTypeField field: tableRow.getFieldList()) {\n           if (shouldVisitColumn(schemaPath, tableName, field.getName())) {\n             visitField(schemaPath, tableName, field);",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "d2c8c6f32f3dfb5c99e0f0d881442e104c6190e3",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -102,14 +102,14 @@ public void testFromInfoSchema() throws Exception {\n \n   @Test\n   public void testTPCH1() throws Exception {\n-    String expectedColNames = \" \\\"columns\\\" : [ \\\"`l_returnflag`\\\", \\\"`l_linestatus`\\\", \\\"`l_shipdate`\\\", \\\"`l_quantity`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\", \\\"`l_tax`\\\" ]\";\n+    String expectedColNames = \" \\\"columns\\\" : [ \\\"`l_shipdate`\\\", \\\"`l_returnflag`\\\", \\\"`l_linestatus`\\\", \\\"`l_quantity`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\", \\\"`l_tax`\\\" ]\";\n     testPhysicalPlanFromFile(\"queries/tpch/01.sql\", expectedColNames);\n   }\n \n   @Test\n   public void testTPCH3() throws Exception {\n     String expectedColNames1 = \"\\\"columns\\\" : [ \\\"`c_mktsegment`\\\", \\\"`c_custkey`\\\" ]\";\n-    String expectedColNames2 = \" \\\"columns\\\" : [ \\\"`o_orderdate`\\\", \\\"`o_shippriority`\\\", \\\"`o_custkey`\\\", \\\"`o_orderkey`\\\" \";\n+    String expectedColNames2 = \" \\\"columns\\\" : [ \\\"`o_custkey`\\\", \\\"`o_orderkey`\\\", \\\"`o_orderdate`\\\", \\\"`o_shippriority`\\\" ]\";\n     String expectedColNames3 = \"\\\"columns\\\" : [ \\\"`l_orderkey`\\\", \\\"`l_shipdate`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\" ]\";\n     testPhysicalPlanFromFile(\"queries/tpch/03.sql\", expectedColNames1, expectedColNames2, expectedColNames3);\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "ad55a0dd19d07d4d3f18b7379c58f4310ce961b5",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -16,17 +16,16 @@\n  */\n package org.apache.drill;\n \n-import mockit.Deencapsulation;\n import mockit.Mock;\n import mockit.MockUp;\n import mockit.integration.junit4.JMockit;\n-import org.apache.calcite.util.SaffronProperties;\n+import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserRemoteException;\n import org.apache.drill.test.BaseTestQuery;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n \n-import java.util.Properties;\n+import java.nio.charset.Charset;\n \n import static org.hamcrest.CoreMatchers.containsString;\n import static org.junit.Assert.assertThat;\n@@ -47,19 +46,16 @@ public void testUtf8SupportInQueryStringByDefault() throws Exception {\n \n   @Test(expected = UserRemoteException.class)\n   public void testDisableUtf8SupportInQueryString() throws Exception {\n-    Deencapsulation.setField(SaffronProperties.class, \"properties\", null);\n-    final Properties properties = System.getProperties();\n     final String charset = \"ISO-8859-1\";\n-    new MockUp<System>()\n+\n+    // Mocked Util.getDefaultCharset() since it uses static field Util.DEFAULT_CHARSET\n+    // which is initialized when declared using SaffronProperties.INSTANCE field which also is initialized\n+    // when declared.\n+    new MockUp<Util>()\n     {\n       @Mock\n-      Properties getProperties() {\n-        Properties newProperties = new Properties();\n-        newProperties.putAll(properties);\n-        newProperties.put(\"saffron.default.charset\", charset);\n-        newProperties.put(\"saffron.default.nationalcharset\", charset);\n-        newProperties.put(\"saffron.default.collation.name\", charset + \"$en_US\");\n-        return newProperties;\n+      Charset getDefaultCharset() {\n+        return Charset.forName(charset);\n       }\n     };\n \n@@ -70,8 +66,6 @@ Properties getProperties() {\n       assertThat(e.getMessage(), containsString(\n           String.format(\"Failed to encode '%s' in character set '%s'\", hello, charset)));\n       throw e;\n-    } finally {\n-      Deencapsulation.setField(SaffronProperties.class, \"properties\", null);\n     }\n   }\n ",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "status": "modified",
                "changes": 24,
                "deletions": 15,
                "sha": "a515763d89df201d8b1a66f22c70fb03371a9345",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -120,6 +120,9 @@ public void invalidQueryParserError() throws Exception {\n    */\n   @Test\n   public void invalidQueryValidationError() throws Exception {\n-    createPrepareStmt(\"SELECT * sdflkgdh\", true, ErrorType.PARSE /** Drill returns incorrect error for parse error*/);\n+    // CALCITE-1120 allows SELECT without from syntax.\n+    // So with this change the query fails with VALIDATION error.\n+    createPrepareStmt(\"SELECT * sdflkgdh\", true,\n+        ErrorType.VALIDATION /* Drill returns incorrect error for parse error*/);\n   }\n }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "97452970a9129a18b0450fb0006c8ec4d0b321c5",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1113,7 +1113,7 @@ public ExecuteResult prepareAndExecute(StatementHandle h, String sql, long maxRo\n \n   @Override\n   public ExecuteResult prepareAndExecute(final StatementHandle handle, final String sql, final long maxRowCount,\n-                                         int maxRowsInFirstFrame, final PrepareCallback callback) throws NoSuchStatementException {\n+        int maxRowsInFirstFrame, final PrepareCallback callback) throws NoSuchStatementException {\n     return prepareAndExecute(handle, sql, maxRowCount, callback);\n   }\n \n@@ -1133,13 +1133,17 @@ public Frame fetch(StatementHandle statementHandle, long l, int i) throws NoSuch\n   }\n \n   @Override\n-  public ExecuteResult execute(StatementHandle statementHandle, List<TypedValue> list, long l) throws NoSuchStatementException {\n-    throw new UnsupportedOperationException(this.getClass().getSimpleName());\n+  public ExecuteResult execute(StatementHandle statementHandle,\n+        List<TypedValue> list, long l) throws NoSuchStatementException {\n+    return new ExecuteResult(Collections.singletonList(\n+        MetaResultSet.create(statementHandle.connectionId, statementHandle.id,\n+            true, statementHandle.signature, null)));\n   }\n \n   @Override\n-  public ExecuteResult execute(StatementHandle statementHandle, List<TypedValue> list, int i) throws NoSuchStatementException {\n-    return null;\n+  public ExecuteResult execute(StatementHandle statementHandle,\n+      List<TypedValue> list, int i) throws NoSuchStatementException {\n+    return execute(statementHandle, list, (long) i);\n   }\n \n   @Override",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "status": "modified",
                "changes": 14,
                "deletions": 5,
                "sha": "0b33167fb180c3b886bb5c4ae888bb70bfcb3f39",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -38,7 +38,7 @@\n  * <p>\n  * This class has sub-classes which implement JDBC 3.0 and JDBC 4.0 APIs; it is\n  * instantiated using\n- * {@link net.hydromatic.avatica.AvaticaFactory#newPreparedStatement}.\n+ * {@link org.apache.calcite.avatica.AvaticaFactory#newPreparedStatement}.\n  * </p>\n  */\n abstract class DrillPreparedStatementImpl extends AvaticaPreparedStatement\n@@ -58,7 +58,9 @@ protected DrillPreparedStatementImpl(DrillConnectionImpl connection,\n           resultSetType, resultSetConcurrency, resultSetHoldability);\n     connection.openStatementsRegistry.addStatement(this);\n     this.preparedStatementHandle = preparedStatementHandle;\n-    ((DrillColumnMetaDataList) signature.columns).updateColumnMetaData(preparedStatementHandle.getColumnsList());\n+    if (preparedStatementHandle != null) {\n+      ((DrillColumnMetaDataList) signature.columns).updateColumnMetaData(preparedStatementHandle.getColumnsList());\n+    }\n   }\n \n   /**\n@@ -329,7 +331,7 @@ public void addBatch(String sql) throws SQLException {\n   }\n \n   @Override\n-  public void clearBatch() throws RuntimeException {\n+  public void clearBatch() {\n     try {\n       throwIfClosed();\n     } catch (AlreadyClosedSqlException e) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "a13f936670d69b45cadb54423b70cbba62af0776",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -421,19 +421,15 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n       }\n       else if (SQLClientInfoException.class == cause.getClass()\n                 && normalClosedExceptionText.equals(cause.getMessage())\n-                && (false\n-                    || method.getName().equals(\"setClientInfo\")\n-                    || method.getName().equals(\"getClientInfo\")\n-                    )) {\n+                && (method.getName().equals(\"setClientInfo\")\n+                    || method.getName().equals(\"getClientInfo\"))) {\n         // Special good case--we had to use SQLClientInfoException from those.\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n                && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getCatalog\")\n-                   || method.getName().equals(\"getSchema\")\n-                   )) {\n+               && (method.getName().equals(\"getCatalog\")\n+                  || method.getName().equals(\"getSchema\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n       }\n@@ -481,20 +477,18 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n       if (super.isOkaySpecialCaseException(method, cause)) {\n         result = true;\n       }\n-      else if (   method.getName().equals(\"executeLargeBatch\")\n+      else if (method.getName().equals(\"executeLargeBatch\")\n                || method.getName().equals(\"executeLargeUpdate\")) {\n         // TODO: New Java 8 methods not implemented in Avatica.\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n                && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getConnection\")\n+               && (method.getName().equals(\"getConnection\")\n                    || method.getName().equals(\"getFetchDirection\")\n                    || method.getName().equals(\"getFetchSize\")\n                    || method.getName().equals(\"getMaxRows\")\n-                   || method.getName().equals(\"getLargeMaxRows\") // TODO: Java 8\n-                   )) {\n+                   || method.getName().equals(\"getLargeMaxRows\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n       }\n@@ -544,27 +538,20 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n-               && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getConnection\")\n+               && cause.getMessage().contains(normalClosedExceptionText)\n+               && (method.getName().equals(\"getConnection\")\n                    || method.getName().equals(\"getFetchDirection\")\n                    || method.getName().equals(\"getFetchSize\")\n                    || method.getName().equals(\"getMaxRows\")\n                    || method.getName().equals(\"getMetaData\")\n-                   )) {\n+                   || method.getName().equals(\"clearBatch\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n-      }\n-      else if (  method.getName().equals(\"setObject\")\n-              || method.getName().equals(\"executeLargeUpdate\")\n-              || method.getName().equals(\"executeLargeBatch\")\n-              || method.getName().equals(\"getLargeMaxRows\")\n-              ) {\n-        // TODO: Java 8 methods not yet supported by Avatica.\n-        result = true;\n-      }\n-      else {\n-        result = false;\n+      } else {\n+        result = method.getName().equals(\"setObject\")\n+          || method.getName().equals(\"executeLargeUpdate\")\n+          || method.getName().equals(\"executeLargeBatch\")\n+          || method.getName().equals(\"getLargeMaxRows\");\n       }\n       return result;\n     }",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "status": "modified",
                "changes": 45,
                "deletions": 29,
                "sha": "8521586eed7024c56710e97e64613eabaef983da",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            },
            {
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -96,13 +96,8 @@ public static void setUpObjects() throws Exception {\n     catch (SQLException | UnsupportedOperationException e) {\n       // Expected.\n     }\n-    try {\n-      connection.createArrayOf(\"INTEGER\", new Object[0]);\n-      fail(\"Test seems to be out of date.  Were arrays implemented?\");\n-    }\n-    catch (SQLException | UnsupportedOperationException e) {\n-      // Expected.\n-    }\n+\n+    connection.createArrayOf(\"INTEGER\", new Object[0]);\n \n     resultSet = plainStatement.executeQuery(\"VALUES 'plain Statement query'\");\n     resultSet.next();\n@@ -161,7 +156,9 @@ protected INTF getJdbcObject() throws SQLException {\n      */\n     private static Object getDummyValueForType(Class<?> type) {\n       final Object result;\n-      if (! type.isPrimitive()) {\n+      if (type.equals(String.class)) {\n+        result = \"\";\n+      } else if (! type.isPrimitive()) {\n         result = null;\n       }\n       else {",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "status": "modified",
                "changes": 15,
                "deletions": 9,
                "sha": "9e2399bd3732dd6848baea66ce2ba1d1746061cf",
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7"
            }
        ],
        "bug_id": "drill_56",
        "parent": "https://github.com/apache/drill/commit/9fabb612f16f6f541b3bde68ad7d734cad26df33",
        "message": "DRILL-3993: Fix unit test failures connected with support Calcite 1.13\n\n- Use root schema as default for describe table statement.\nFix TestOpenTSDBPlugin.testDescribe() and TestInfoSchemaOnHiveStorage.varCharMaxLengthAndDecimalPrecisionInInfoSchema() unit tests.\n- Modify expected results for tests:\nTestPreparedStatementProvider.invalidQueryValidationError();\nTestProjectPushDown.testTPCH1();\nTestProjectPushDown.testTPCH3();\nTestStorageBasedHiveAuthorization.selectUser1_db_u0_only();\nTestStorageBasedHiveAuthorization.selectUser0_db_u1g1_only()\n- Fix TestCTAS.whenTableQueryColumnHasStarAndTableFiledListIsSpecified(), TestViewSupport.createViewWhenViewQueryColumnHasStarAndViewFiledListIsSpecified(), TestInbuiltHiveUDFs.testIf(), testDisableUtf8SupportInQueryString unit tests.\n- Fix UnsupportedOperationException and NPE for jdbc tests.\n- Fix AssertionError: Conversion to relational algebra failed to preserve datatypes\n\n*DrillCompoundIdentifier:\nAccording to the changes, made in [CALCITE-546], star Identifier is replaced by empty string during parsing the query. Since Drill uses its own DrillCompoundIdentifier, it should also replace star by empty string before creating SqlIdentifier instance to avoid further errors connected with star column. see SqlIdentifier.isStar() method.\n\n*SqlConverter:\nIn [CALCITE-1417] added simplification of expressions which should be projected every time when a new project rel node is created using RelBuilder. It causes assertion errors connected with types nullability. This hook was set to false to avoid project expressions simplification. See usage of this hook and RelBuilder.project() method.\n\nIn Drill the type nullability of the function depends on only the nullability of its arguments. In some cases, a function may return null value even if it had non-nullable arguments. When Calice simplifies expressions, it checks that the type of the result is the same as the type of the expression. Otherwise, makeCast() method is called. But when a function returns null literal, this cast does nothing, even when the function has a non-nullable type. So to avoid this issue, method makeCast() was overridden.\n\n*DrillAvgVarianceConvertlet:\nProblem with sum0 and specific changes in old Calcite (it is CALCITE-777). (see HistogramShuttle.visitCall method) Changes were made to avoid changes in Calcite.\n\n*SqlConverter, DescribeTableHandler, ShowTablesHandler:\nNew Calcite tries to combine both default and specified workspaces during the query validation. In some cases, for example, when describe table statement is used, Calcite tries to find INFORMATION_SCHEMA in the schema used as default. When it does not find the schema, it tries to find a table with such name. For some storage plugins, such as opentsdb and hbase, when a table was not found, the error is thrown, and the query fails. To avoid this issue, default schema was changed to root schema for validation stage for describe table and show tables queries.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/c8a08c3e793d53ae4c445f07caa639269c8b51b7",
        "file": [
            {
                "patch": "@@ -23,7 +23,11 @@\n \n import org.apache.drill.common.expression.IfExpression.IfCondition;\n import org.apache.drill.common.expression.visitors.ExprVisitor;\n+import org.apache.drill.common.types.TypeProtos;\n+import org.apache.drill.common.types.TypeProtos.DataMode;\n import org.apache.drill.common.types.TypeProtos.MajorType;\n+import org.apache.drill.common.types.TypeProtos.MinorType;\n+import org.apache.drill.common.types.Types;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -102,7 +106,22 @@ public IfExpression build(){\n \n   @Override\n   public MajorType getMajorType() {\n-    return this.elseExpression.getMajorType();\n+    // If the return type of one of the \"then\" expression or \"else\" expression is nullable, return \"if\" expression\n+    // type as nullable\n+    MajorType majorType = elseExpression.getMajorType();\n+    if (majorType.getMode() == DataMode.OPTIONAL) {\n+      return majorType;\n+    }\n+\n+    for(IfCondition condition : conditions) {\n+      if (condition.expression.getMajorType().getMode() == DataMode.OPTIONAL) {\n+        assert condition.expression.getMajorType().getMinorType() == majorType.getMinorType();\n+\n+        return condition.expression.getMajorType();\n+      }\n+    }\n+\n+    return majorType;\n   }\n \n   public static Builder newBuilder(){",
                "additions": 20,
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "status": "modified",
                "changes": 21,
                "deletions": 1,
                "sha": "d1df7f7eb68ab3cf58705dc811e889058e6dc603",
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "filename": "common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/expression/IfExpression.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7"
            },
            {
                "patch": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+<@pp.dropOutputFile />\n+<#list vv.types as type>\n+<#list type.minor as minor>\n+\n+<#assign className=\"GConvertToNullable${minor.class}Holder\" />\n+\n+<@pp.changeOutputFile name=\"/org/apache/drill/exec/expr/fn/impl/${className}.java\" />\n+\n+<#include \"/@includes/license.ftl\" />\n+\n+package org.apache.drill.exec.expr.fn.impl;\n+\n+import org.apache.drill.exec.expr.DrillSimpleFunc;\n+import org.apache.drill.exec.expr.annotations.*;\n+import org.apache.drill.exec.expr.holders.*;\n+import org.apache.drill.exec.record.RecordBatch;\n+\n+@FunctionTemplate(name = \"convertToNullable${minor.class?upper_case}\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.INTERNAL)\n+public class ${className} implements DrillSimpleFunc {\n+\n+  @Param ${minor.class}Holder input;\n+  @Output Nullable${minor.class}Holder output;\n+\n+  public void setup(RecordBatch incoming) { }\n+\n+  public void eval() {\n+    output.isSet = 1;\n+<#if type.major != \"VarLen\">\n+  <#if (minor.class == \"TimeStampTZ\")>\n+    output.value = input.value;\n+    output.index = input.index;\n+  <#elseif (minor.class == \"Interval\")>\n+    output.months = input.months;\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif (minor.class == \"IntervalDay\")>\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif minor.class.startsWith(\"Decimal\")>\n+    output.scale = input.scale;\n+    output.precision = input.precision;\n+    <#if minor.class.startsWith(\"Decimal28\") || minor.class.startsWith(\"Decimal38\")>\n+    output.sign = input.sign;\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+    <#else>\n+    output.value = input.value;\n+    </#if>\n+  <#elseif (type.width > 8)>\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+  <#else>\n+    output.value = input.value;\n+  </#if>\n+<#else>\n+    output.start = input.start;\n+    output.end = input.end;\n+    output.buffer = input.buffer;\n+</#if>\n+  }\n+}\n+</#list>\n+</#list>\n\\ No newline at end of file",
                "additions": 80,
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "status": "added",
                "changes": 80,
                "deletions": 0,
                "sha": "548d64550c0a5f8b7d2c7d26f3cdfce07d59e54b",
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "filename": "exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7"
            },
            {
                "patch": "@@ -284,9 +284,45 @@ public boolean apply(LogicalExpression input) {\n         }\n       }\n \n+      // If the type of the IF expression is nullable, apply a convertToNullable*Holder function for \"THEN\"/\"ELSE\"\n+      // expressions whose type is not nullable.\n+      if (IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build().getMajorType().getMode()\n+          == DataMode.OPTIONAL) {\n+        for (int i = 0; i < conditions.size(); ++i) {\n+          IfExpression.IfCondition condition = conditions.get(i);\n+          if (condition.expression.getMajorType().getMode() != DataMode.OPTIONAL) {\n+            conditions.set(i, new IfExpression.IfCondition(condition.condition,\n+                getConvertToNullableExpr(ImmutableList.of(condition.expression),\n+                    condition.expression.getMajorType().getMinorType(), registry)));\n+          }\n+        }\n+\n+        if (newElseExpr.getMajorType().getMode() != DataMode.OPTIONAL) {\n+          newElseExpr = getConvertToNullableExpr(ImmutableList.of(newElseExpr),\n+              newElseExpr.getMajorType().getMinorType(), registry);\n+        }\n+      }\n+\n       return validateNewExpr(IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build());\n     }\n \n+    private LogicalExpression getConvertToNullableExpr(List<LogicalExpression> args, MinorType minorType,\n+        FunctionImplementationRegistry registry) {\n+      String funcName = \"convertToNullable\" + minorType.toString();\n+      FunctionCall funcCall = new FunctionCall(funcName, args, ExpressionPosition.UNKNOWN);\n+      FunctionResolver resolver = FunctionResolverFactory.getResolver(funcCall);\n+\n+      DrillFuncHolder matchedConvertToNullableFuncHolder =\n+          resolver.getBestMatch(registry.getDrillRegistry().getMethods().get(funcName), funcCall);\n+\n+      if (matchedConvertToNullableFuncHolder == null) {\n+        logFunctionResolutionError(errorCollector, funcCall);\n+        return NullExpression.INSTANCE;\n+      }\n+\n+      return new DrillFuncHolderExpr(funcName, matchedConvertToNullableFuncHolder, args, ExpressionPosition.UNKNOWN);\n+    }\n+\n     private LogicalExpression rewriteNullExpression(LogicalExpression expr, MajorType type) {\n       if(expr instanceof NullExpression) {\n         return new TypedNullConstant(type);",
                "additions": 36,
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "status": "modified",
                "changes": 36,
                "deletions": 0,
                "sha": "fca743bb2d308aaef6af763dc9ae34b44a8a58bc",
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7"
            },
            {
                "patch": "@@ -338,21 +338,40 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {\n     public LogicalExpression visitLiteral(RexLiteral literal) {\n       switch(literal.getType().getSqlTypeName()){\n       case BIGINT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIGINT);\n+        }\n         long l = ((BigDecimal) literal.getValue()).longValue();\n-        return checkNullLiteral(literal, MinorType.BIGINT, ValueExpressions.getBigInt(l));\n+        return ValueExpressions.getBigInt(l);\n       case BOOLEAN:\n-        return checkNullLiteral(literal, MinorType.BIT, ValueExpressions.getBit(((Boolean) literal.getValue())));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIT);\n+        }\n+        return ValueExpressions.getBit(((Boolean) literal.getValue()));\n       case CHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case DOUBLE:\n+        if (isLiteralNull(literal)){\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double d = ((BigDecimal) literal.getValue()).doubleValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(d));\n+        return ValueExpressions.getFloat8(d);\n       case FLOAT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT4);\n+        }\n         float f = ((BigDecimal) literal.getValue()).floatValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT4, ValueExpressions.getFloat4(f));\n+        return ValueExpressions.getFloat4(f);\n       case INTEGER:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INT);\n+        }\n         int a = ((BigDecimal) literal.getValue()).intValue();\n-        return checkNullLiteral(literal, MinorType.INT, ValueExpressions.getInt(a));\n+        return ValueExpressions.getInt(a);\n+\n       case DECIMAL:\n         /* TODO: Enable using Decimal literals once we have more functions implemented for Decimal\n          * For now continue using Double instead of decimals\n@@ -367,24 +386,49 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n         } else if (precision <= 38) {\n             return ValueExpressions.getDecimal38((BigDecimal)literal.getValue());\n         } */\n-\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double dbl = ((BigDecimal) literal.getValue()).doubleValue();\n         logger.warn(\"Converting exact decimal into approximate decimal.  Should be fixed once decimal is implemented.\");\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(dbl));\n+        return ValueExpressions.getFloat8(dbl);\n       case VARCHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case SYMBOL:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(literal.getValue().toString()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(literal.getValue().toString());\n       case DATE:\n-        return checkNullLiteral(literal, MinorType.DATE, ValueExpressions.getDate((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.DATE);\n+        }\n+        return (ValueExpressions.getDate((GregorianCalendar)literal.getValue()));\n       case TIME:\n-        return checkNullLiteral(literal, MinorType.TIME, ValueExpressions.getTime((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIME);\n+        }\n+        return (ValueExpressions.getTime((GregorianCalendar)literal.getValue()));\n       case TIMESTAMP:\n-        return checkNullLiteral(literal, MinorType.TIMESTAMP, ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIMESTAMP);\n+        }\n+        return (ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n       case INTERVAL_YEAR_MONTH:\n-        return checkNullLiteral(literal, MinorType.INTERVALYEAR, ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALYEAR);\n+        }\n+        return (ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n       case INTERVAL_DAY_TIME:\n-        return checkNullLiteral(literal, MinorType.INTERVALDAY, ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALDAY);\n+        }\n+        return (ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+      case NULL:\n+        return NullExpression.INSTANCE;\n       case ANY:\n         if (isLiteralNull(literal)) {\n           return NullExpression.INSTANCE;\n@@ -395,12 +439,8 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n     }\n   }\n \n-  private static LogicalExpression checkNullLiteral(RexLiteral literal, MinorType type, LogicalExpression orExpr) {\n-    if(isLiteralNull(literal)) {\n-      return new TypedNullConstant(Types.optional(type));\n-    } else {\n-      return orExpr;\n-    }\n+  private static final TypedNullConstant createNullExpr(MinorType type) {\n+    return new TypedNullConstant(Types.optional(type));\n   }\n \n   private static boolean isLiteralNull(RexLiteral literal) {",
                "additions": 61,
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "status": "modified",
                "changes": 82,
                "deletions": 21,
                "sha": "8966f18aa2cf1977909966026e7ae5e83427f544",
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7"
            },
            {
                "patch": "@@ -947,4 +947,52 @@ public Void apply(Connection connection) {\n       }\n     });\n   }\n+\n+  @Test\n+  public void testCaseWithNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=null\\n\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWithElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name ELSE 'Test' END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Test\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=null\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name ELSE 'Test' END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=Test\\n\"\n+        );\n+  }\n }",
                "additions": 48,
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "status": "modified",
                "changes": 48,
                "deletions": 0,
                "sha": "e6842286b89f7106cf32f598decc0cc511676b59",
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7"
            }
        ],
        "bug_id": "drill_57",
        "parent": "https://github.com/apache/drill/commit/2cdbd6000abc33965f1f7b960b954fd6a4f7b58f",
        "message": "DRILL-665: Handle null values in case expressions (contd).\n\n1. Added functions for converting REQUIRED holder into NULLABLE holder where the minorType is same.\n2. Update in Optiq->Drill literal conversion. First check if it null type, before parsing the literal value. Parsing literal value will cause NPE if the type is NULL.\n3. Changed getReturnType of IfExpression to consider the nullable types of THEN and ELSE expressions.\n4. Added testcases.",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/7a4fee42406516e729b9bdae31945cec4858a332",
        "file": [
            {
                "patch": "@@ -88,6 +88,11 @@ public FileSystemPlugin(FileSystemConfig config, DrillbitContext context, String\n         for(Map.Entry<String, String> space : config.workspaces.entrySet()){\n           factories.add(new WorkspaceSchemaFactory(this, space.getKey(), name, fs, space.getValue(), matchers));\n         }\n+\n+        // if the \"default\" workspace is not given add one.\n+        if (!config.workspaces.containsKey(\"default\")) {\n+          factories.add(new WorkspaceSchemaFactory(this, \"default\", name, fs, \"/\", matchers));\n+        }\n       }\n       this.schemaFactory = new FileSystemSchemaFactory(name, factories);\n     }catch(IOException e){",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/7a4fee42406516e729b9bdae31945cec4858a332/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "97427f6fd9be2696449cac2bad1e6da2524e70f2",
                "blob_url": "https://github.com/apache/drill/blob/7a4fee42406516e729b9bdae31945cec4858a332/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java?ref=7a4fee42406516e729b9bdae31945cec4858a332"
            },
            {
                "patch": "@@ -4,7 +4,6 @@\n       type: \"file\",\n       connection: \"file:///\",\n       workspaces: {\n-        default: \"/\",\n         home: \"/\"\n       },\n       formats: {\n@@ -44,4 +43,4 @@\n           }\n       }\n   }\n-}\n\\ No newline at end of file\n+}",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/7a4fee42406516e729b9bdae31945cec4858a332/sqlparser/src/test/resources/storage-plugins.json",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "60efa50fa46875ba3f55b4d3ddd641d68730d4cb",
                "blob_url": "https://github.com/apache/drill/blob/7a4fee42406516e729b9bdae31945cec4858a332/sqlparser/src/test/resources/storage-plugins.json",
                "filename": "sqlparser/src/test/resources/storage-plugins.json",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/sqlparser/src/test/resources/storage-plugins.json?ref=7a4fee42406516e729b9bdae31945cec4858a332"
            }
        ],
        "bug_id": "drill_58",
        "parent": "https://github.com/apache/drill/commit/69c571ccd841b7bcda1c38979716862690cba696",
        "message": "DRILL-533: Queries fail with NPE if dfs/cp schema has no default workspace but has non-default workspace\n\nWe currently add the \"default\" workspace only if there are no workspaces defined, but we always choose the workspace with \"default\" name as the default workspace. As in the repro case there is no default workspace, we end with a null.\n\nFix: Add default workspace if there is no \"default\" given in storage-engines.json",
        "repo": "drill"
    },
    {
        "commit": "https://github.com/apache/drill/commit/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
        "file": [
            {
                "patch": "@@ -29,6 +29,32 @@\n \n   private final static Pattern IGNORE= Pattern.compile(\"^(sun|com\\\\.sun|java).*\");\n \n+  /**\n+   * Constructs the root error message in the form [root exception class name]: [root exception message]\n+   *\n+   * @param cause exception we want the root message for\n+   * @return root error message or empty string if none found\n+   */\n+  static String getRootMessage(final Throwable cause) {\n+    String message = \"\";\n+\n+    Throwable ex = cause;\n+    while (ex != null) {\n+      if (ex.getMessage() != null) {\n+        message = ex.getClass().getName() + \": \" + ex.getMessage();\n+      }\n+\n+      if (ex.getCause() != null && ex.getCause() != ex) {\n+        ex = ex.getCause();\n+      } else {\n+        break;\n+      }\n+    }\n+\n+    return message;\n+  }\n+\n+\n   static String buildCausesMessage(final Throwable t) {\n \n     StringBuilder sb = new StringBuilder();",
                "additions": 26,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "status": "modified",
                "changes": 26,
                "deletions": 0,
                "sha": "5dd9b67a863860d44fc6ff88858df2903591cc5e",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "filename": "common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -21,14 +21,14 @@\n import org.apache.drill.exec.proto.UserBitShared.DrillPBError;\n \n /**\n- * Base class for all user exception. The goal is to separate out common error condititions where we can give users\n+ * Base class for all user exception. The goal is to separate out common error conditions where we can give users\n  * useful feedback.\n  * <p>Throwing a user exception will guarantee it's message will be displayed to the user, along with any context\n  * information added to the exception at various levels while being sent to the client.\n  * <p>A specific class of user exceptions are system exception. They represent system level errors that don't display\n  * any specific error message to the user apart from \"A system error has occurend\" along with informations to retrieve\n  * the details of the exception from the logs.\n- * <p>although system exception should only display a generic message to the user, for now they will display the root\n+ * <p>Although system exception should only display a generic message to the user, for now they will display the root\n  * error message, until all user errors are properly sent from the server side.\n  * <p>Any thrown exception that is not wrapped inside a user exception will automatically be converted to a system\n  * exception before being sent to the client.\n@@ -37,10 +37,32 @@\n  */\n public class UserException extends DrillRuntimeException {\n \n+  public static final String MEMORY_ERROR_MSG = \"One or more nodes ran out of memory while executing the query.\";\n+\n+  /**\n+   * Creates a RESOURCE error with a prebuilt message for out of memory exceptions\n+   *\n+   * @param cause exception that will be wrapped inside a memory error\n+   * @return resource error builder\n+   */\n+  public static Builder memoryError(final Throwable cause) {\n+    return UserException.resourceError(cause)\n+      .message(MEMORY_ERROR_MSG);\n+  }\n+\n+  /**\n+   * Creates a RESOURCE error with a prebuilt message for out of memory exceptions\n+   *\n+   * @return resource error builder\n+   */\n+  public static Builder memoryError() {\n+    return memoryError(null);\n+  }\n+\n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#SYSTEM\n@@ -49,7 +71,7 @@\n    *              returned by the builder instead of creating a new user exception\n    * @return user exception builder\n    *\n-   * @deprecated this method should never need to be used explicitely, unless you are passing the exception to the\n+   * @deprecated This method should never need to be used explicitly, unless you are passing the exception to the\n    *             Rpc layer or UserResultListener.submitFailed()\n    */\n   @Deprecated\n@@ -58,7 +80,7 @@ public static Builder systemError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#CONNECTION\n    * @return user exception builder\n@@ -68,9 +90,9 @@ public static Builder connectionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a connection error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a connection error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#CONNECTION\n@@ -84,7 +106,7 @@ public static Builder connectionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_READ\n    * @return user exception builder\n@@ -94,9 +116,9 @@ public static Builder dataReadError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a data read error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a data read error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_READ\n@@ -110,7 +132,7 @@ public static Builder dataReadError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_WRITE\n    * @return user exception builder\n@@ -120,9 +142,9 @@ public static Builder dataWriteError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a data write error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a data write error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_WRITE\n@@ -136,7 +158,7 @@ public static Builder dataWriteError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#FUNCTION\n    * @return user exception builder\n@@ -146,9 +168,9 @@ public static Builder functionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a function error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a function error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#FUNCTION\n@@ -162,7 +184,7 @@ public static Builder functionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PARSE\n    * @return user exception builder\n@@ -172,9 +194,9 @@ public static Builder parseError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PARSE\n@@ -188,7 +210,7 @@ public static Builder parseError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PERMISSION\n    * @return user exception builder\n@@ -198,9 +220,9 @@ public static Builder permissionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PERMISSION\n@@ -214,7 +236,7 @@ public static Builder permissionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PLAN\n    * @return user exception builder\n@@ -224,9 +246,9 @@ public static Builder planError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PLAN\n@@ -240,7 +262,7 @@ public static Builder planError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#RESOURCE\n    * @return user exception builder\n@@ -250,9 +272,9 @@ public static Builder resourceError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#RESOURCE\n@@ -266,7 +288,7 @@ public static Builder resourceError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#UNSUPPORTED_OPERATION\n    * @return user exception builder\n@@ -276,9 +298,9 @@ public static Builder unsupportedError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#UNSUPPORTED_OPERATION\n@@ -307,7 +329,7 @@ public static Builder unsupportedError(final Throwable cause) {\n     private String message;\n \n     /**\n-     * wraps an existing exception inside a user exception.\n+     * Wraps an existing exception inside a user exception.\n      *\n      * @param errorType user exception type that should be created if the passed exception isn't,\n      *                  or doesn't wrap a user exception\n@@ -462,13 +484,20 @@ public UserException build() {\n         return uex;\n       }\n \n+      boolean isSystemError = errorType == DrillPBError.ErrorType.SYSTEM;\n+\n+      // make sure system errors use the root error message and display the root cause class name\n+      if (isSystemError) {\n+        message = ErrorHelper.getRootMessage(cause);\n+      }\n+\n       final UserException newException = new UserException(this);\n \n       // since we just created a new exception, we should log it for later reference. If this is a system error, this is\n       // an issue that the Drill admin should pay attention to and we should log as ERROR. However, if this is a user\n       // mistake or data read issue, the system admin should not be concerned about these and thus we'll log this\n       // as an INFO message.\n-      if (errorType == DrillPBError.ErrorType.SYSTEM) {\n+      if (isSystemError) {\n         logger.error(newException.getMessage(), newException);\n       } else {\n         logger.info(\"User Error Occurred\", newException);",
                "additions": 73,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "status": "modified",
                "changes": 117,
                "deletions": 44,
                "sha": "d90ace14662d5ef6039dc46aafb783a696b4d197",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "filename": "common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/exceptions/UserException.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -40,9 +40,10 @@ private Exception wrap(UserException uex, int numWraps) {\n   @Test\n   public void testBuildSystemException() {\n     String message = \"This is an exception\";\n-    UserException uex = UserException.systemError(new RuntimeException(message)).build();\n+    UserException uex = UserException.systemError(new Exception(new RuntimeException(message))).build();\n \n-    Assert.assertEquals(message, uex.getOriginalMessage());\n+    Assert.assertTrue(uex.getOriginalMessage().contains(message));\n+    Assert.assertTrue(uex.getOriginalMessage().contains(\"RuntimeException\"));\n \n     DrillPBError error = uex.getOrCreatePBError(true);\n ",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "151b762b6ac9d96422947421a0325e693ffd12e9",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "filename": "common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -38,7 +38,7 @@\n  *   The width of each element is ${type.width} byte(s)\n  *   The equivalent Java primitive is '${minor.javaType!type.javaType}'\n  *\n- * NB: this class is automatically generated from ValueVectorTypes.tdd using FreeMarker.\n+ * Source code generated using FreeMarker template ${.template_name}\n  */\n @SuppressWarnings(\"unused\")\n public final class ${minor.class}Vector extends BaseDataValueVector implements FixedWidthVector{\n@@ -92,30 +92,50 @@ public boolean allocateNewSafe() {\n       allocationValueCount = (int) (allocationValueCount * 2);\n       allocationMonitor = 0;\n     }\n-    this.data = allocator.buffer(allocationValueCount * ${type.width});\n-    if(data == null) return false;\n+\n+    DrillBuf newBuf = allocator.buffer(allocationValueCount * ${type.width});\n+    if(newBuf == null) {\n+      return false;\n+    }\n+\n+    this.data = newBuf;\n     this.data.readerIndex(0);\n     return true;\n   }\n \n   /**\n    * Allocate a new buffer that supports setting at least the provided number of values.  May actually be sized bigger depending on underlying buffer rounding size. Must be called prior to using the ValueVector.\n    * @param valueCount\n+   * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the new buffer\n    */\n   public void allocateNew(int valueCount) {\n     clear();\n-    this.data = allocator.buffer(valueCount * ${type.width});\n+\n+    DrillBuf newBuf = allocator.buffer(valueCount * ${type.width});\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(\n+        String.format(\"Failure while allocating buffer of %d bytes\",valueCount * ${type.width}));\n+    }\n+\n+    this.data = newBuf;\n     this.data.readerIndex(0);\n     this.allocationValueCount = valueCount;\n   }\n \n /**\n  * Allocate new buffer with double capacity, and copy data into the new buffer. Replace vector's buffer with new buffer, and release old one\n+ *\n+ * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the new buffer\n  */\n   public void reAlloc() {\n     logger.info(\"Realloc vector {}. [{}] -> [{}]\", field, allocationValueCount * ${type.width}, allocationValueCount * 2 * ${type.width});\n     allocationValueCount *= 2;\n     DrillBuf newBuf = allocator.buffer(allocationValueCount * ${type.width});\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(\n+      String.format(\"Failure while reallocating buffer to %d bytes\",allocationValueCount * ${type.width}));\n+    }\n+\n     newBuf.setBytes(0, data, 0, data.capacity());\n     newBuf.setZero(newBuf.capacity() / 2, newBuf.capacity() / 2);\n     newBuf.writerIndex(data.writerIndex());",
                "additions": 24,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "status": "modified",
                "changes": 28,
                "deletions": 4,
                "sha": "a805b8e239d9db2f2d203f5a653919aaaa4a1149",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "filename": "exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -46,7 +46,7 @@\n  *   The width of each element is ${type.width} byte(s)\n  *   The equivalent Java primitive is '${minor.javaType!type.javaType}'\n  *\n- * NB: this class is automatically generated from ValueVectorTypes.tdd using FreeMarker.\n+ * Source code generated using FreeMarker template ${.template_name}\n  */\n @SuppressWarnings(\"unused\")\n public final class ${minor.class}Vector extends BaseDataValueVector implements VariableWidthVector{\n@@ -281,12 +281,14 @@ public boolean allocateNewSafe() {\n       allocationMonitor = 0;\n     }\n \n-    data = allocator.buffer(allocationTotalByteCount);\n-    if(data == null){\n+    DrillBuf newBuf = allocator.buffer(allocationTotalByteCount);\n+    if(newBuf == null){\n       return false;\n     }\n-    \n+\n+    this.data = newBuf;\n     data.readerIndex(0);\n+\n     if(!offsetVector.allocateNewSafe()){\n       return false;\n     }\n@@ -297,7 +299,12 @@ public boolean allocateNewSafe() {\n   public void allocateNew(int totalBytes, int valueCount) {\n     clear();\n     assert totalBytes >= 0;\n-    data = allocator.buffer(totalBytes);\n+    DrillBuf newBuf = allocator.buffer(totalBytes);\n+    if(newBuf == null){\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of %d bytes\", totalBytes));\n+    }\n+\n+    this.data = newBuf;\n     data.readerIndex(0);\n     allocationTotalByteCount = totalBytes;\n     offsetVector.allocateNew(valueCount+1);\n@@ -307,6 +314,11 @@ public void allocateNew(int totalBytes, int valueCount) {\n     public void reAlloc() {\n       allocationTotalByteCount *= 2;\n       DrillBuf newBuf = allocator.buffer(allocationTotalByteCount);\n+      if(newBuf == null){\n+        throw new OutOfMemoryRuntimeException(\n+          String.format(\"Failure while reallocating buffer of %d bytes\", allocationTotalByteCount));\n+      }\n+\n       newBuf.setBytes(0, data, 0, data.capacity());\n       data.release();\n       data = newBuf;",
                "additions": 17,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "status": "modified",
                "changes": 22,
                "deletions": 5,
                "sha": "659d99be35d6e1226400a4e1b35fb460bb21a6dd",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "filename": "exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -27,6 +27,7 @@\n \n import org.apache.drill.common.config.DrillConfig;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.exception.ClassTransformationException;\n import org.apache.drill.exec.expr.ClassGenerator;\n@@ -35,6 +36,7 @@\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n import org.apache.drill.exec.proto.BitControl.PlanFragment;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n@@ -149,6 +151,10 @@ public FragmentContext(final DrillbitContext dbContext, final PlanFragment fragm\n     try {\n       allocator = context.getAllocator().getChildAllocator(this, fragment.getMemInitial(), fragment.getMemMax(), true);\n       Preconditions.checkNotNull(allocator, \"Unable to acuqire allocator\");\n+    } catch(final OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+      throw UserException.memoryError(e)\n+        .addContext(\"Fragment\", getHandle().getMajorFragmentId() + \":\" + getHandle().getMinorFragmentId())\n+        .build();\n     } catch(final Throwable e) {\n       throw new ExecutionSetupException(\"Failure while getting memory allocator for fragment.\", e);\n     }",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "cf4e9bbc517e880d18e831dd8b77eec6ef2108c9",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -25,13 +25,15 @@\n import java.util.Map;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.types.TypeProtos.MinorType;\n import org.apache.drill.common.types.Types;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.exception.SchemaChangeException;\n import org.apache.drill.exec.expr.TypeHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.OperatorContext;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n@@ -47,6 +49,7 @@\n import org.apache.drill.exec.record.selection.SelectionVector4;\n import org.apache.drill.exec.server.options.OptionValue;\n import org.apache.drill.exec.store.RecordReader;\n+import org.apache.drill.exec.testing.ExecutionControlsInjector;\n import org.apache.drill.exec.vector.AllocationHelper;\n import org.apache.drill.exec.vector.NullableVarCharVector;\n import org.apache.drill.exec.vector.SchemaChangeCallBack;\n@@ -60,13 +63,11 @@\n  */\n public class ScanBatch implements CloseableRecordBatch {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ScanBatch.class);\n-\n-  private static final int MAX_RECORD_CNT = Character.MAX_VALUE;\n+  private final static ExecutionControlsInjector injector = ExecutionControlsInjector.getInjector(ScanBatch.class);\n \n   private final Map<MaterializedField.Key, ValueVector> fieldVectorMap = Maps.newHashMap();\n \n   private final VectorContainer container = new VectorContainer();\n-  private VectorContainer tempContainer;\n   private int recordCount;\n   private final FragmentContext context;\n   private final OperatorContext oContext;\n@@ -79,7 +80,6 @@\n   private List<ValueVector> partitionVectors;\n   private List<Integer> selectedPartitionColumns;\n   private String partitionColumnDesignator;\n-  private boolean first = true;\n   private boolean done = false;\n   private SchemaChangeCallBack callBack = new SchemaChangeCallBack();\n \n@@ -159,13 +159,14 @@ public IterOutcome next() {\n     if (done) {\n       return IterOutcome.NONE;\n     }\n-    long t1 = System.nanoTime();\n     oContext.getStats().startProcessing();\n     try {\n       try {\n+        injector.injectChecked(context.getExecutionControls(), \"next-allocate\", OutOfMemoryException.class);\n+\n         currentReader.allocate(fieldVectorMap);\n-      } catch (OutOfMemoryException e) {\n-        logger.debug(\"Caught OutOfMemoryException\");\n+      } catch (OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+        logger.debug(\"Caught Out of Memory Exception\", e);\n         for (ValueVector v : fieldVectorMap.values()) {\n           v.clear();\n         }\n@@ -219,6 +220,9 @@ public IterOutcome next() {\n       } else {\n         return IterOutcome.OK;\n       }\n+    } catch (OutOfMemoryRuntimeException ex) {\n+      context.fail(UserException.memoryError(ex).build());\n+      return IterOutcome.STOP;\n     } catch (Exception ex) {\n       logger.debug(\"Failed to read the batch. Stopping...\", ex);\n       context.fail(ex);\n@@ -328,7 +332,7 @@ public void allocate(int recordCount) {\n     @Override\n     public boolean isNewSchema() {\n       // Check if top level schema has changed, second condition checks if one of the deeper map schema has changed\n-      if (schemaChange == true || callBack.getSchemaChange()) {\n+      if (schemaChange || callBack.getSchemaChange()) {\n         schemaChange = false;\n         return true;\n       }\n@@ -353,9 +357,6 @@ public WritableBatch getWritableBatch() {\n \n   public void close() {\n     container.clear();\n-    if (tempContainer != null) {\n-      tempContainer.clear();\n-    }\n     for (ValueVector v : partitionVectors) {\n       v.clear();\n     }",
                "additions": 12,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "status": "modified",
                "changes": 23,
                "deletions": 11,
                "sha": "6176f779bb69a4fdeafcd71d844cf2f50c2c05e1",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -21,6 +21,7 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingUserConnection;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -80,6 +81,8 @@ public boolean innerNext() {\n       IterOutcome outcome = next(incoming);\n       logger.trace(\"Screen Outcome {}\", outcome);\n       switch (outcome) {\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n         return false;\n       case NONE:",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "c31de6616f4f0876bc1903a2652062b79353fe60",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -21,6 +21,7 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -94,6 +95,8 @@ public boolean innerNext() {\n       }\n //      logger.debug(\"Outcome of sender next {}\", out);\n       switch (out) {\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n       case NONE:\n         // if we didn't do anything yet, send an empty schema.",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "fe6239e9fb04e701212dc45c35727d2c367d6a20",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -48,9 +48,7 @@\n import org.apache.drill.exec.record.BatchSchema;\n import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;\n import org.apache.drill.exec.record.ExpandableHyperContainer;\n-import org.apache.drill.exec.record.MaterializedField;\n import org.apache.drill.exec.record.RecordBatch;\n-import org.apache.drill.exec.record.TransferPair;\n import org.apache.drill.exec.record.TypedFieldId;\n import org.apache.drill.exec.record.VectorAccessible;\n import org.apache.drill.exec.record.VectorContainer;\n@@ -61,19 +59,15 @@\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.drill.exec.vector.complex.AbstractContainerVector;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import com.google.common.base.Stopwatch;\n-import com.google.common.collect.Lists;\n import com.sun.codemodel.JConditional;\n import com.sun.codemodel.JExpr;\n \n public class TopNBatch extends AbstractRecordBatch<TopN> {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TopNBatch.class);\n \n   private static final long MAX_SORT_BYTES = 1L * 1024 * 1024 * 1024;\n-  public static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n-  public static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n   private  final int batchPurgeThreshold;\n \n   public final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n@@ -89,10 +83,8 @@\n   private long countSincePurge;\n   private int batchCount;\n   private Copier copier;\n-  private boolean schemaBuilt = false;\n   private boolean first = true;\n   private int recordCount = 0;\n-  private boolean stop;\n \n   public TopNBatch(TopN popConfig, FragmentContext context, RecordBatch incoming) throws OutOfMemoryException {\n     super(popConfig, context);\n@@ -153,7 +145,11 @@ public void buildSchema() throws SchemaChangeException {\n         container.setRecordCount(0);\n         return;\n       case STOP:\n-        stop = true;\n+        state = BatchState.STOP;\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n       case NONE:\n         state = BatchState.DONE;\n       default:\n@@ -198,6 +194,7 @@ public IterOutcome innerNext() {\n           break outer;\n         case NOT_YET:\n           throw new UnsupportedOperationException();\n+        case OUT_OF_MEMORY:\n         case STOP:\n           return upstream;\n         case OK_NEW_SCHEMA:",
                "additions": 6,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "status": "modified",
                "changes": 15,
                "deletions": 9,
                "sha": "c3e70f5edfa650bc7f0ac28013b73f2facabd258",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -42,7 +42,7 @@\n \n /* Write the RecordBatch to the given RecordWriter. */\n public class WriterRecordBatch extends AbstractRecordBatch<Writer> {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WriterRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WriterRecordBatch.class);\n \n   private EventBasedRecordWriter eventBasedRecordWriter;\n   private RecordWriter recordWriter;\n@@ -78,11 +78,6 @@ public BatchSchema getSchema() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-//    try {\n-//      setupNewSchema();\n-//    } catch (Exception e) {\n-//      throw new SchemaChangeException(e);\n-//    }\n   }\n \n   @Override\n@@ -101,8 +96,9 @@ public IterOutcome innerNext() {\n         upstream = next(incoming);\n \n         switch(upstream) {\n+          case OUT_OF_MEMORY:\n           case STOP:\n-            return IterOutcome.STOP;\n+            return upstream;\n \n           case NOT_YET:\n           case NONE:\n@@ -124,7 +120,7 @@ public IterOutcome innerNext() {\n             throw new UnsupportedOperationException();\n         }\n       } while(upstream != IterOutcome.NONE);\n-    }catch(Exception ex){\n+    } catch(IOException ex) {\n       logger.error(\"Failure during query\", ex);\n       kill(false);\n       context.fail(ex);\n@@ -154,7 +150,7 @@ private void addOutputContainerData(){\n     container.setRecordCount(1);\n   }\n \n-  protected void setupNewSchema() throws Exception {\n+  protected void setupNewSchema() throws IOException {\n     try {\n       // update the schema in RecordWriter\n       stats.startSetup();",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "status": "modified",
                "changes": 14,
                "deletions": 9,
                "sha": "28a99d9fd9bb74361aeb5dd9f8d45874855a2861",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -93,11 +93,20 @@ public int getRecordCount() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(SelectionVectorMode.NONE);\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n     }\n+\n     if (!createAggregator()) {\n       state = BatchState.DONE;\n     }\n@@ -115,33 +124,29 @@ public IterOutcome innerNext() {\n \n     if (aggregator.buildComplete() && !aggregator.allFlushed()) {\n       // aggregation is complete and not all records have been output yet\n-      IterOutcome outcome = aggregator.outputCurrentBatch();\n-      return outcome;\n+      return aggregator.outputCurrentBatch();\n     }\n \n     logger.debug(\"Starting aggregator doWork; incoming record count = {} \", incoming.getRecordCount());\n \n-    while (true) {\n-      AggOutcome out = aggregator.doWork();\n-      logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n-      switch (out) {\n-      case CLEANUP_AND_RETURN:\n-        container.zeroVectors();\n-        aggregator.cleanup();\n-        state = BatchState.DONE;\n-        // fall through\n-      case RETURN_OUTCOME:\n-        IterOutcome outcome = aggregator.getOutcome();\n-        return aggregator.getOutcome();\n-      case UPDATE_AGGREGATOR:\n-        context.fail(UserException.unsupportedError()\n-          .message(\"Hash aggregate does not support schema changes\").build());\n-        close();\n-        killIncoming(false);\n-        return IterOutcome.STOP;\n-      default:\n-        throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n-      }\n+    AggOutcome out = aggregator.doWork();\n+    logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n+    switch (out) {\n+    case CLEANUP_AND_RETURN:\n+      container.zeroVectors();\n+      aggregator.cleanup();\n+      state = BatchState.DONE;\n+      // fall through\n+    case RETURN_OUTCOME:\n+      return aggregator.getOutcome();\n+    case UPDATE_AGGREGATOR:\n+      context.fail(UserException.unsupportedError()\n+        .message(\"Hash aggregate does not support schema changes\").build());\n+      close();\n+      killIncoming(false);\n+      return IterOutcome.STOP;\n+    default:\n+      throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n     }\n   }\n ",
                "additions": 32,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "status": "modified",
                "changes": 59,
                "deletions": 27,
                "sha": "e1b59096a25d72c2d1175b9146553ebae7c09337",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -309,6 +309,7 @@ public AggOutcome doWork() {\n               logger.debug(\"Received IterOutcome of {}\", out);\n             }\n             switch (out) {\n+              case OUT_OF_MEMORY:\n               case NOT_YET:\n                 this.outcome = out;\n                 return AggOutcome.RETURN_OUTCOME;",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "e92de4021c12b7be39ec2c44ccd9eac5619eb0ed",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -96,11 +96,20 @@ public int getRecordCount() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(SelectionVectorMode.NONE);\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n     }\n+\n     if (!createAggregator()) {\n       state = BatchState.DONE;\n     }\n@@ -137,6 +146,7 @@ public IterOutcome innerNext() {\n           specialBatchSent = true;\n           return IterOutcome.OK;\n         }\n+      case OUT_OF_MEMORY:\n       case NOT_YET:\n       case STOP:\n         return outcome;\n@@ -153,38 +163,37 @@ public IterOutcome innerNext() {\n       }\n     }\n \n-    while (true) {\n-      AggOutcome out = aggregator.doWork();\n-      recordCount = aggregator.getOutputCount();\n-      logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n-      switch (out) {\n-      case CLEANUP_AND_RETURN:\n-        if (!first) {\n-          container.zeroVectors();\n-        }\n+    AggOutcome out = aggregator.doWork();\n+    recordCount = aggregator.getOutputCount();\n+    logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n+    switch (out) {\n+    case CLEANUP_AND_RETURN:\n+      if (!first) {\n+        container.zeroVectors();\n+      }\n+      done = true;\n+      // fall through\n+    case RETURN_OUTCOME:\n+      IterOutcome outcome = aggregator.getOutcome();\n+      if (outcome == IterOutcome.NONE && first) {\n+        first = false;\n         done = true;\n-        // fall through\n-      case RETURN_OUTCOME:\n-        IterOutcome outcome = aggregator.getOutcome();\n-        if (outcome == IterOutcome.NONE && first) {\n-          first = false;\n-          done = true;\n-          return IterOutcome.OK_NEW_SCHEMA;\n-        } else if (outcome == IterOutcome.OK && first) {\n-          outcome = IterOutcome.OK_NEW_SCHEMA;\n-        }\n+        return IterOutcome.OK_NEW_SCHEMA;\n+      } else if (outcome == IterOutcome.OK && first) {\n+        outcome = IterOutcome.OK_NEW_SCHEMA;\n+      } else if (outcome != IterOutcome.OUT_OF_MEMORY) {\n         first = false;\n-        return outcome;\n-      case UPDATE_AGGREGATOR:\n-        context.fail(UserException.unsupportedError()\n-          .message(\"Streaming aggregate does not support schema changes\")\n-          .build());\n-        close();\n-        killIncoming(false);\n-        return IterOutcome.STOP;\n-      default:\n-        throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n       }\n+      return outcome;\n+    case UPDATE_AGGREGATOR:\n+      context.fail(UserException.unsupportedError()\n+        .message(\"Streaming aggregate does not support schema changes\")\n+        .build());\n+      close();\n+      killIncoming(false);\n+      return IterOutcome.STOP;\n+    default:\n+      throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n     }\n   }\n ",
                "additions": 42,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "status": "modified",
                "changes": 75,
                "deletions": 33,
                "sha": "b252971111c00548bac2b9f796ab986f92995412",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -97,6 +97,9 @@ public AggOutcome doWork() {\n               } else {\n                 break outer;\n               }\n+            case OUT_OF_MEMORY:\n+              outcome = out;\n+              return AggOutcome.RETURN_OUTCOME;\n             case NONE:\n               out = IterOutcome.OK_NEW_SCHEMA;\n             case STOP:",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "0bbfd18ca484c671ffea30e46c72309d32e58112",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -20,6 +20,7 @@\n import java.util.List;\n \n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -97,6 +98,8 @@ public boolean innerNext() {\n     RecordBatch.IterOutcome out = next(incoming);\n     logger.debug(\"Outcome of sender next {}\", out);\n     switch(out){\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n       case NONE:\n         for (int i = 0; i < tunnels.length; ++i) {",
                "additions": 3,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "c6a07f8aa37547d4ad32d244704ab0465db8d33c",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -20,6 +20,7 @@\n import javax.inject.Named;\n \n import org.apache.drill.exec.exception.SchemaChangeException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;\n import org.apache.drill.exec.record.RecordBatch;\n@@ -64,7 +65,7 @@ public void filterBatch(int recordCount){\n       return;\n     }\n     if (! outgoingSelectionVector.allocateNew(recordCount)) {\n-      throw new UnsupportedOperationException(\"Unable to allocate filter batch\");\n+      throw new OutOfMemoryRuntimeException(\"Unable to allocate filter batch\");\n     }\n     switch(svMode){\n     case NONE:",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "cd2fbe93ec25315e044020943c4e845af16484eb",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -171,6 +171,12 @@ public int getRecordCount() {\n   protected void buildSchema() throws SchemaChangeException {\n     leftUpstream = next(left);\n     rightUpstream = next(right);\n+\n+    if (leftUpstream == IterOutcome.OUT_OF_MEMORY || rightUpstream == IterOutcome.OUT_OF_MEMORY) {\n+      state = BatchState.OUT_OF_MEMORY;\n+      return;\n+    }\n+\n     // Initialize the hash join helper context\n     hjHelper = new HashJoinHelper(context, oContext.getAllocator());\n     try {\n@@ -328,6 +334,7 @@ public void executeBuildPhase() throws SchemaChangeException, ClassTransformatio\n \n       switch (rightUpstream) {\n \n+      case OUT_OF_MEMORY:\n       case NONE:\n       case NOT_YET:\n       case STOP:",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "status": "modified",
                "changes": 7,
                "deletions": 0,
                "sha": "56ce0ee9f43e3e627f60bac26b7999a25df020ff",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -69,7 +69,7 @@\n  */\n public class MergeJoinBatch extends AbstractRecordBatch<MergeJoinPOP> {\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergeJoinBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergeJoinBatch.class);\n \n   public static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n   public static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n@@ -148,6 +148,12 @@ public int getRecordCount() {\n \n   public void buildSchema() throws SchemaChangeException {\n     status.ensureInitial();\n+\n+    if (status.getLastLeft() == IterOutcome.OUT_OF_MEMORY || status.getLastRight() == IterOutcome.OUT_OF_MEMORY) {\n+      state = BatchState.OUT_OF_MEMORY;\n+      return;\n+    }\n+\n     allocateBatch(true);\n   }\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "status": "modified",
                "changes": 8,
                "deletions": 1,
                "sha": "0430f1bde54de7672551ed013726f861e42269f4",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -147,7 +147,7 @@ public IterOutcome innerNext() {\n       }\n \n       boolean drainRight = true;\n-      while (drainRight == true) {\n+      while (drainRight) {\n         rightUpstream = next(RIGHT_INPUT, right);\n         switch (rightUpstream) {\n           case OK_NEW_SCHEMA:\n@@ -159,8 +159,11 @@ public IterOutcome innerNext() {\n           case OK:\n             addBatchToHyperContainer(right);\n             break;\n+          case OUT_OF_MEMORY:\n+            return IterOutcome.OUT_OF_MEMORY;\n           case NONE:\n           case STOP:\n+            //TODO we got a STOP, shouldn't we stop immediately ?\n           case NOT_YET:\n             drainRight = false;\n             break;\n@@ -274,6 +277,11 @@ protected void buildSchema() throws SchemaChangeException {\n       leftUpstream = next(LEFT_INPUT, left);\n       rightUpstream = next(RIGHT_INPUT, right);\n \n+      if (leftUpstream == IterOutcome.OUT_OF_MEMORY || rightUpstream == IterOutcome.OUT_OF_MEMORY) {\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      }\n+\n       if (leftUpstream != IterOutcome.NONE) {\n         leftSchema = left.getSchema();\n         for (VectorWrapper vw : left) {",
                "additions": 9,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "status": "modified",
                "changes": 10,
                "deletions": 1,
                "sha": "4c86f5ca99f6487376348e950a8407db2fae74da",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -34,15 +34,14 @@\n \n public class LimitRecordBatch extends AbstractSingleRecordBatch<Limit> {\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LimitRecordBatch.class);\n+//  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LimitRecordBatch.class);\n \n   private SelectionVector2 outgoingSv;\n   private SelectionVector2 incomingSv;\n   private int recordsToSkip;\n   private int recordsLeft;\n   private boolean noEndLimit;\n   private boolean skipBatch;\n-  private boolean done = false;\n   private boolean first = true;\n   List<TransferPair> transfers = Lists.newArrayList();\n \n@@ -90,14 +89,13 @@ protected boolean setupNewSchema() throws SchemaChangeException {\n \n   @Override\n   public IterOutcome innerNext() {\n-    if (done) {\n-      return IterOutcome.NONE;\n-    }\n-\n     if(!first && !noEndLimit && recordsLeft <= 0) {\n       incoming.kill(true);\n \n       IterOutcome upStream = next(incoming);\n+      if (upStream == IterOutcome.OUT_OF_MEMORY) {\n+        return upStream;\n+      }\n \n       while (upStream == IterOutcome.OK || upStream == IterOutcome.OK_NEW_SCHEMA) {\n \n@@ -106,10 +104,14 @@ public IterOutcome innerNext() {\n           wrapper.getValueVector().clear();\n         }\n         upStream = next(incoming);\n+        if (upStream == IterOutcome.OUT_OF_MEMORY) {\n+          return upStream;\n+        }\n       }\n \n       return IterOutcome.NONE;\n     }\n+\n     return super.innerNext();\n   }\n ",
                "additions": 8,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "status": "modified",
                "changes": 14,
                "deletions": 6,
                "sha": "d9330ea753afcfee49abec37cbee835cba437389",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -75,7 +75,6 @@\n import org.apache.drill.exec.vector.FixedWidthVector;\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import parquet.Preconditions;\n \n@@ -88,10 +87,8 @@\n  * The MergingRecordBatch merges pre-sorted record batches from remote senders.\n  */\n public class MergingRecordBatch extends AbstractRecordBatch<MergingReceiverPOP> implements RecordBatch {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergingRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergingRecordBatch.class);\n \n-  private static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n-  private static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n   private static final int OUTGOING_BATCH_SIZE = 32 * 1024;\n \n   private RecordBatchLoader[] batchLoaders;\n@@ -170,7 +167,7 @@ public IterOutcome innerNext() {\n       prevBatchWasFull = false;\n     }\n \n-    if (hasMoreIncoming == false) {\n+    if (!hasMoreIncoming) {\n       logger.debug(\"next() was called after all values have been processed\");\n       outgoingPosition = 0;\n       return IterOutcome.NONE;",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "status": "modified",
                "changes": 7,
                "deletions": 5,
                "sha": "f19f3718f8c8ca2f90f1b030fe9ab14f7ec72e5f",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.drill.exec.expr.CodeGenerator;\n import org.apache.drill.exec.expr.ExpressionTreeMaterializer;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -169,6 +170,9 @@ public boolean innerNext() {\n         }\n         return false;\n \n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n+\n       case STOP:\n         if (partitioner != null) {\n           partitioner.clear();",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "1872a515878fe3918fa1d8ec7f65360afc6fcff9",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.drill.common.types.TypeProtos.MajorType;\n import org.apache.drill.exec.expr.TypeHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -38,7 +39,7 @@\n import org.apache.drill.exec.vector.ValueVector;\n \n public class ProducerConsumerBatch extends AbstractRecordBatch {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProducerConsumerBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProducerConsumerBatch.class);\n \n   private final RecordBatch incoming;\n   private final Thread producer = new Thread(new Producer(), Thread.currentThread().getName() + \" - Producer Thread\");\n@@ -67,7 +68,7 @@ public IterOutcome innerNext() {\n       wrapper = queue.take();\n       logger.debug(\"Got batch from queue\");\n     } catch (final InterruptedException e) {\n-      if (!context.shouldContinue()) {\n+      if (context.shouldContinue()) {\n         context.fail(e);\n       }\n       return IterOutcome.STOP;\n@@ -79,6 +80,8 @@ public IterOutcome innerNext() {\n       return IterOutcome.NONE;\n     } else if (wrapper.failed) {\n       return IterOutcome.STOP;\n+    } else if (wrapper.outOfMemory) {\n+      throw new OutOfMemoryRuntimeException();\n     }\n \n     recordCount = wrapper.batch.getRecordCount();\n@@ -131,27 +134,37 @@ public void run() {\n             case NONE:\n               stop = true;\n               break outer;\n+            case OUT_OF_MEMORY:\n+              queue.putFirst(RecordBatchDataWrapper.outOfMemory());\n+              return;\n             case STOP:\n-              queue.putFirst(new RecordBatchDataWrapper(null, false, true));\n+              queue.putFirst(RecordBatchDataWrapper.failed());\n               return;\n             case OK_NEW_SCHEMA:\n             case OK:\n-              wrapper = new RecordBatchDataWrapper(new RecordBatchData(incoming), false, false);\n+              wrapper = RecordBatchDataWrapper.batch(new RecordBatchData(incoming));\n               queue.put(wrapper);\n               wrapper = null;\n               break;\n             default:\n               throw new UnsupportedOperationException();\n           }\n         }\n+      } catch (final OutOfMemoryRuntimeException e) {\n+        try {\n+          queue.putFirst(RecordBatchDataWrapper.outOfMemory());\n+        } catch (final InterruptedException ex) {\n+          logger.error(\"Unable to enqueue the last batch indicator. Something is broken.\", ex);\n+          // TODO InterruptedException\n+        }\n       } catch (final InterruptedException e) {\n         logger.warn(\"Producer thread is interrupted.\", e);\n         // TODO InterruptedException\n       } finally {\n         if (stop) {\n           try {\n             clearQueue();\n-            queue.put(new RecordBatchDataWrapper(null, true, false));\n+            queue.put(RecordBatchDataWrapper.finished());\n           } catch (final InterruptedException e) {\n             logger.error(\"Unable to enqueue the last batch indicator. Something is broken.\", e);\n             // TODO InterruptedException\n@@ -206,14 +219,32 @@ public int getRecordCount() {\n   }\n \n   private static class RecordBatchDataWrapper {\n-    RecordBatchData batch;\n-    boolean finished;\n-    boolean failed;\n+    final RecordBatchData batch;\n+    final boolean finished;\n+    final boolean failed;\n+    final boolean outOfMemory;\n \n-    RecordBatchDataWrapper(final RecordBatchData batch, final boolean finished, final boolean failed) {\n+    RecordBatchDataWrapper(final RecordBatchData batch, final boolean finished, final boolean failed, final boolean outOfMemory) {\n       this.batch = batch;\n       this.finished = finished;\n       this.failed = failed;\n+      this.outOfMemory = outOfMemory;\n+    }\n+\n+    public static RecordBatchDataWrapper batch(final RecordBatchData batch) {\n+      return new RecordBatchDataWrapper(batch, false, false, false);\n+    }\n+\n+    public static RecordBatchDataWrapper finished() {\n+      return new RecordBatchDataWrapper(null, true, false, false);\n+    }\n+\n+    public static RecordBatchDataWrapper failed() {\n+      return new RecordBatchDataWrapper(null, false, true, false);\n+    }\n+\n+    public static RecordBatchDataWrapper outOfMemory() {\n+      return new RecordBatchDataWrapper(null, false, false, true);\n     }\n   }\n ",
                "additions": 40,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "status": "modified",
                "changes": 49,
                "deletions": 9,
                "sha": "bca9622b2239fa763aeb844b6503b00990f0fcf6",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.drill.common.expression.FunctionCall;\n import org.apache.drill.common.expression.FunctionCallFactory;\n import org.apache.drill.common.expression.LogicalExpression;\n-import org.apache.drill.common.expression.PathSegment;\n import org.apache.drill.common.expression.PathSegment.NameSegment;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.expression.ValueExpressions;\n@@ -80,7 +79,6 @@\n   private boolean hasRemainder = false;\n   private int remainderIndex = 0;\n   private int recordCount;\n-  private final boolean buildingSchema = true;\n \n   private static final String EMPTY_STRING = \"\";\n   private boolean first = true;\n@@ -144,7 +142,10 @@ protected IterOutcome doWork() {\n         IterOutcome next = null;\n         while (incomingRecordCount == 0) {\n           next = next(incoming);\n-          if (next != IterOutcome.OK && next != IterOutcome.OK_NEW_SCHEMA) {\n+          if (next == IterOutcome.OUT_OF_MEMORY) {\n+            outOfMemory = true;\n+            return next;\n+          } else if (next != IterOutcome.OK && next != IterOutcome.OK_NEW_SCHEMA) {\n             return next;\n           }\n           incomingRecordCount = incoming.getRecordCount();\n@@ -255,13 +256,7 @@ private void setValueCount(final int count) {\n \n   /** hack to make ref and full work together... need to figure out if this is still necessary. **/\n   private FieldReference getRef(final NamedExpression e) {\n-    final FieldReference ref = e.getRef();\n-    final PathSegment seg = ref.getRootSegment();\n-\n-//    if (seg.isNamed() && \"output\".contentEquals(seg.getNameSegment().getPath())) {\n-//      return new FieldReference(ref.getPath().toString().subSequence(7, ref.getPath().length()), ref.getPosition());\n-//    }\n-    return ref;\n+    return e.getRef();\n   }\n \n   private boolean isAnyWildcard(final List<NamedExpression> exprs) {\n@@ -321,7 +316,6 @@ protected boolean setupNewSchema() throws SchemaChangeException {\n             int k = 0;\n             for (final VectorWrapper<?> wrapper : incoming) {\n               final ValueVector vvIn = wrapper.getValueVector();\n-              final SchemaPath originalPath = vvIn.getField().getPath();\n               if (k > result.outputNames.size()-1) {\n                 assert false;\n               }",
                "additions": 5,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "status": "modified",
                "changes": 16,
                "deletions": 11,
                "sha": "32ffb6f0b6e29ea3eb095906cdf5e705158924dc",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -43,13 +43,12 @@\n import org.apache.drill.exec.record.selection.SelectionVector2;\n import org.apache.drill.exec.record.selection.SelectionVector4;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import com.sun.codemodel.JConditional;\n import com.sun.codemodel.JExpr;\n \n public class SortBatch extends AbstractRecordBatch<Sort> {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SortBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SortBatch.class);\n \n   public final MappingSet mainMapping = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_CONSTANT_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n   public final MappingSet leftMapping = new MappingSet(\"leftIndex\", null, ClassGenerator.DEFAULT_CONSTANT_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n@@ -107,6 +106,7 @@ public IterOutcome innerNext() {\n           break outer;\n         case NOT_YET:\n           throw new UnsupportedOperationException();\n+        case OUT_OF_MEMORY:\n         case STOP:\n           return upstream;\n         case OK_NEW_SCHEMA:",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "8748aaf77ba9fc4c3d06058b0637214f215b55d0",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -17,9 +17,7 @@\n  */\n package org.apache.drill.exec.physical.impl.svremover;\n \n-import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n-import java.io.PrintStream;\n import java.util.List;\n \n import org.apache.drill.exec.exception.ClassTransformationException;\n@@ -36,21 +34,19 @@\n import org.apache.drill.exec.record.VectorContainer;\n import org.apache.drill.exec.record.VectorWrapper;\n import org.apache.drill.exec.record.WritableBatch;\n-import org.apache.drill.exec.util.BatchPrinter;\n import org.apache.drill.exec.vector.CopyUtil;\n import org.apache.drill.exec.vector.ValueVector;\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.Lists;\n \n public class RemovingRecordBatch extends AbstractSingleRecordBatch<SelectionVectorRemover>{\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RemovingRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RemovingRecordBatch.class);\n \n   private Copier copier;\n   private int recordCount;\n   private boolean hasRemainder;\n   private int remainderIndex;\n-  private boolean first;\n \n   public RemovingRecordBatch(SelectionVectorRemover popConfig, FragmentContext context, RecordBatch incoming) throws OutOfMemoryException {\n     super(popConfig, context, incoming);\n@@ -248,7 +244,7 @@ public static Copier getGenerated4Copier(RecordBatch batch, FragmentContext cont\n \n     for(VectorWrapper<?> vv : batch){\n       ValueVector v = vv.getValueVectors()[0];\n-      TransferPair tp = v.makeTransferPair(container.addOrGet(v.getField()));\n+      v.makeTransferPair(container.addOrGet(v.getField()));\n     }\n \n     try {",
                "additions": 2,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "status": "modified",
                "changes": 8,
                "deletions": 6,
                "sha": "57e7b55d8026e82f6b5ab99f5628b71e0437ca39",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -108,8 +108,7 @@ protected IterOutcome doWork() {\n     } else {\n       sv = null;\n     }\n-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(incoming.getRecordCount(), incoming, incomingHasSv2 ? true\n-        : false);\n+    WritableBatch batch = WritableBatch.getBatchNoHVWrap(incoming.getRecordCount(), incoming, incomingHasSv2);\n     VectorAccessibleSerializable wrap = new VectorAccessibleSerializable(batch, sv, oContext.getAllocator());\n \n     try {",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "78e83d63e17f0ba4967cb136f66e1d01d747257b",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.drill.exec.expr.ValueVectorReadExpression;\n import org.apache.drill.exec.expr.ValueVectorWriteExpression;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.UnionAll;\n import org.apache.drill.exec.record.AbstractRecordBatch;\n@@ -143,7 +144,9 @@ private void setValueCount(int count) {\n \n   private boolean doAlloc() {\n     for (ValueVector v : allocationVectors) {\n-      if(!AllocationHelper.allocateNew(v, current.getRecordCount())) {\n+      try {\n+        AllocationHelper.allocateNew(v, current.getRecordCount());\n+      } catch (OutOfMemoryRuntimeException ex) {\n         return false;\n       }\n     }",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "445568b8620b21d548e5d31455ebdb4f326677e3",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -37,7 +37,6 @@\n import org.apache.drill.exec.expr.ValueVectorWriteExpression;\n import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n-import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.WindowPOP;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -128,6 +127,7 @@ public IterOutcome innerNext() {\n         case NONE:\n           noMoreBatches = true;\n           break;\n+        case OUT_OF_MEMORY:\n         case NOT_YET:\n         case STOP:\n           return upstream;\n@@ -160,7 +160,7 @@ public IterOutcome innerNext() {\n     // process a saved batch\n     try {\n       framer.doWork();\n-    } catch (DrillException | OutOfMemoryRuntimeException e) {\n+    } catch (DrillException e) {\n       context.fail(e);\n       if (framer != null) {\n         framer.cleanup();\n@@ -179,10 +179,18 @@ public IterOutcome innerNext() {\n   @Override\n   protected void buildSchema() throws SchemaChangeException {\n     logger.trace(\"buildSchema()\");\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n     }\n \n     try {",
                "additions": 14,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "status": "modified",
                "changes": 20,
                "deletions": 6,
                "sha": "428632fe43270482c2e42461d4038fd18f2d32d5",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.ExternalSort;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -185,12 +186,16 @@ public void buildSchema() throws SchemaChangeException {\n         }\n         container.buildSchema(SelectionVectorMode.NONE);\n         container.setRecordCount(0);\n-        return;\n+        break;\n       case STOP:\n+        state = BatchState.STOP;\n+        break;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        break;\n       case NONE:\n         state = BatchState.DONE;\n-      default:\n-        return;\n+        break;\n     }\n   }\n \n@@ -273,7 +278,7 @@ public IterOutcome innerNext() {\n             try {\n               sv2 = newSV2();\n             } catch (OutOfMemoryException e) {\n-              throw new RuntimeException(e);\n+              throw new OutOfMemoryRuntimeException(e);\n             }\n           }\n           int count = sv2.getCount();\n@@ -316,11 +321,15 @@ public IterOutcome innerNext() {\n //          logger.debug(\"Took {} us to sort {} records\", t, count);\n           break;\n         case OUT_OF_MEMORY:\n+          logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n           highWaterMark = totalSizeInMemory;\n           if (batchesSinceLastSpill > 2) {\n             spilledBatchGroups.add(mergeAndSpill(batchGroups));\n+            batchesSinceLastSpill = 0;\n+          } else {\n+            logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n+            return IterOutcome.OUT_OF_MEMORY;\n           }\n-          batchesSinceLastSpill = 0;\n           break;\n         default:\n           throw new UnsupportedOperationException();",
                "additions": 14,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "status": "modified",
                "changes": 19,
                "deletions": 5,
                "sha": "aab339167eae1627895a9045651ad31b54d7de8f",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Iterator;\n \n import org.apache.drill.common.exceptions.DrillRuntimeException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.exec.exception.SchemaChangeException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n@@ -67,7 +68,9 @@ protected AbstractRecordBatch(final T popConfig, final FragmentContext context,\n   protected static enum BatchState {\n     BUILD_SCHEMA, // Need to build schema and return\n     FIRST, // This is still the first data batch\n-    NOT_FIRST, // The first data batch has alread been returned\n+    NOT_FIRST, // The first data batch has already been returned\n+    STOP, // The query most likely failed, we need to propagate STOP to the root\n+    OUT_OF_MEMORY, // Out of Memory while building the Schema...Ouch!\n     DONE // All work is done, no more data to be sent\n   }\n \n@@ -119,23 +122,21 @@ public final IterOutcome next(final int inputIndex, final RecordBatch b){\n   public final IterOutcome next() {\n     try {\n       stats.startProcessing();\n-//      if (state == BatchState.BUILD_SCHEMA) {\n-//        buildSchema();\n-//        if (state == BatchState.BUILD_SCHEMA.DONE) {\n-//          return IterOutcome.NONE;\n-//        } else {\n-//          state = BatchState.FIRST;\n-//          return IterOutcome.OK_NEW_SCHEMA;\n-//        }\n-//      }\n       switch (state) {\n         case BUILD_SCHEMA: {\n           buildSchema();\n-          if (state == BatchState.DONE) {\n-            return IterOutcome.NONE;\n-          } else {\n-            state = BatchState.FIRST;\n-            return IterOutcome.OK_NEW_SCHEMA;\n+          switch (state) {\n+            case DONE:\n+              return IterOutcome.NONE;\n+            case OUT_OF_MEMORY:\n+              // because we don't support schema changes, it is safe to fail the query right away\n+              context.fail(UserException.memoryError().build());\n+              // FALL-THROUGH\n+            case STOP:\n+              return IterOutcome.STOP;\n+            default:\n+              state = BatchState.FIRST;\n+              return IterOutcome.OK_NEW_SCHEMA;\n           }\n         }\n         case DONE: {",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "status": "modified",
                "changes": 31,
                "deletions": 15,
                "sha": "330ec79c3d2e082e88417533c60ed79bd6308d23",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.drill.exec.vector;\n \n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n+\n public class AllocationHelper {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AllocationHelper.class);\n \n@@ -44,16 +46,15 @@ public static void allocate(ValueVector v, int valueCount, int bytesPerValue, in\n \n   /**\n    * Allocates the exact amount if v is fixed width, otherwise falls back to dynamic allocation\n-   * @param v\n-   * @param valueCount\n-   * @return\n+   * @param v value vector we are trying to allocate\n+   * @param valueCount  size we are trying to allocate\n+   * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the memory\n    */\n-  public static boolean allocateNew(ValueVector v, int valueCount){\n+  public static void allocateNew(ValueVector v, int valueCount) {\n     if (v instanceof  FixedWidthVector) {\n       ((FixedWidthVector) v).allocateNew(valueCount);\n-      return true;\n     } else {\n-      return v.allocateNewSafe();\n+      v.allocateNew();\n     }\n   }\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "status": "modified",
                "changes": 13,
                "deletions": 6,
                "sha": "bf465c7994bba8e225ed5ed1fb4402c636e8cf46",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -96,10 +96,12 @@ public boolean allocateNewSafe() {\n \n     clear();\n     int valueSize = getSizeFromCount(allocationValueCount);\n-    data = allocator.buffer(valueSize);\n-    if (data == null) {\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n       return false;\n     }\n+\n+    data = newBuf;\n     zeroVector();\n     return true;\n   }\n@@ -113,7 +115,12 @@ public boolean allocateNewSafe() {\n   public void allocateNew(int valueCount) {\n     clear();\n     int valueSize = getSizeFromCount(valueCount);\n-    data = allocator.buffer(valueSize);\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of d% bytes.\", valueSize));\n+    }\n+\n+    data = newBuf;\n     zeroVector();\n   }\n \n@@ -122,7 +129,12 @@ public void allocateNew(int valueCount) {\n    */\n   public void reAlloc() {\n     allocationValueCount *= 2;\n-    DrillBuf newBuf = allocator.buffer(getSizeFromCount(allocationValueCount));\n+    int valueSize = getSizeFromCount(allocationValueCount);\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of %d bytes.\", valueSize));\n+    }\n+\n     newBuf.setZero(0, newBuf.capacity());\n     newBuf.setBytes(0, data, 0, data.capacity());\n     data.release();",
                "additions": 16,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "status": "modified",
                "changes": 20,
                "deletions": 4,
                "sha": "ae5fad535a103a0ea41207e97181f7289f6e2880",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -42,6 +42,8 @@\n import org.apache.drill.exec.coord.DistributedSemaphore;\n import org.apache.drill.exec.coord.DistributedSemaphore.DistributedLease;\n import org.apache.drill.exec.exception.OptimizerException;\n+import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.QueryContext;\n import org.apache.drill.exec.opt.BasicOptimizer;\n@@ -222,6 +224,8 @@ public void run() {\n         throw new IllegalStateException();\n       }\n       injector.injectChecked(queryContext.getExecutionControls(), \"run-try-end\", ForemanException.class);\n+    } catch (final OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+      moveToState(QueryState.FAILED, UserException.memoryError(e).build());\n     } catch (final ForemanException e) {\n       moveToState(QueryState.FAILED, e);\n     } catch (AssertionError | Exception ex) {",
                "additions": 4,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "7b36e21d4f14ecb0d5070802e668d4880b28d4ab",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -207,9 +207,7 @@ public Void run() throws Exception {\n       updateState(FragmentState.FINISHED);\n     } catch (OutOfMemoryError | OutOfMemoryRuntimeException e) {\n       if (!(e instanceof OutOfMemoryError) || \"Direct buffer memory\".equals(e.getMessage())) {\n-        fail(UserException.resourceError(e)\n-            .message(\"One or more nodes ran out of memory while executing the query.\")\n-            .build());\n+        fail(UserException.memoryError(e).build());\n       } else {\n         // we have a heap out of memory error. The JVM in unstable, exit.\n         System.err.println(\"Node ran out of Heap memory, exiting.\");",
                "additions": 1,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "status": "modified",
                "changes": 4,
                "deletions": 3,
                "sha": "dc83cc644b639798548d748c4137269b8f32ef1a",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            },
            {
                "patch": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill;\n+\n+import org.apache.drill.common.exceptions.UserException;\n+import org.apache.drill.exec.proto.CoordinationProtos;\n+import org.apache.drill.exec.proto.UserBitShared.DrillPBError;\n+import org.apache.drill.exec.testing.ControlsInjectionUtil;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Run several tpch queries and inject an OutOfMemoryException in ScanBatch that will cause an OUT_OF_MEMORY outcome to\n+ * be propagated downstream. Make sure the proper \"memory error\" message is sent to the client.\n+ */\n+public class TestOutOfMemoryOutcome extends BaseTestQuery{\n+\n+  private static final String SINGLE_MODE = \"ALTER SESSION SET `planner.disable_exchanges` = true\";\n+\n+  private void testSingleMode(String fileName) throws Exception{\n+    test(SINGLE_MODE);\n+\n+    CoordinationProtos.DrillbitEndpoint endpoint = bits[0].getContext().getEndpoint();\n+    String controlsString = \"{\\\"injections\\\":[{\"\n+      + \"\\\"address\\\":\\\"\" + endpoint.getAddress() + \"\\\",\"\n+      + \"\\\"port\\\":\\\"\" + endpoint.getUserPort() + \"\\\",\"\n+      + \"\\\"type\\\":\\\"exception\\\",\"\n+      + \"\\\"siteClass\\\":\\\"\" + \"org.apache.drill.exec.physical.impl.ScanBatch\" + \"\\\",\"\n+      + \"\\\"desc\\\":\\\"\" + \"next-allocate\" + \"\\\",\"\n+      + \"\\\"nSkip\\\":0,\"\n+      + \"\\\"nFire\\\":1,\"\n+      + \"\\\"exceptionClass\\\":\\\"\" + \"org.apache.drill.exec.memory.OutOfMemoryException\" + \"\\\"\"\n+      + \"}]}\";\n+    ControlsInjectionUtil.setControls(client, controlsString);\n+\n+    String query = getFile(fileName);\n+\n+    try {\n+      test(query);\n+    } catch(UserException uex) {\n+      DrillPBError error = uex.getOrCreatePBError(false);\n+      Assert.assertEquals(DrillPBError.ErrorType.RESOURCE, error.getErrorType());\n+      Assert.assertTrue(\"Error message isn't related to memory error\",\n+        uex.getMessage().contains(UserException.MEMORY_ERROR_MSG));\n+    }\n+  }\n+\n+  @Test\n+  public void tpch01() throws Exception{\n+    testSingleMode(\"queries/tpch/01.sql\");\n+  }\n+\n+  @Test\n+  public void tpch03() throws Exception{\n+    testSingleMode(\"queries/tpch/03.sql\");\n+  }\n+\n+  @Test\n+  public void tpch04() throws Exception{\n+    testSingleMode(\"queries/tpch/04.sql\");\n+  }\n+\n+  @Test\n+  public void tpch05() throws Exception{\n+    testSingleMode(\"queries/tpch/05.sql\");\n+  }\n+\n+  @Test\n+  public void tpch06() throws Exception{\n+    testSingleMode(\"queries/tpch/06.sql\");\n+  }\n+\n+  @Test\n+  public void tpch07() throws Exception{\n+    testSingleMode(\"queries/tpch/07.sql\");\n+  }\n+\n+  @Test\n+  public void tpch08() throws Exception{\n+    testSingleMode(\"queries/tpch/08.sql\");\n+  }\n+\n+  @Test\n+  public void tpch09() throws Exception{\n+    testSingleMode(\"queries/tpch/09.sql\");\n+  }\n+\n+  @Test\n+  public void tpch10() throws Exception{\n+    testSingleMode(\"queries/tpch/10.sql\");\n+  }\n+\n+  @Test\n+  public void tpch12() throws Exception{\n+    testSingleMode(\"queries/tpch/12.sql\");\n+  }\n+\n+  @Test\n+  public void tpch13() throws Exception{\n+    testSingleMode(\"queries/tpch/13.sql\");\n+  }\n+\n+  @Test\n+  public void tpch14() throws Exception{\n+    testSingleMode(\"queries/tpch/14.sql\");\n+  }\n+\n+  @Test\n+  public void tpch18() throws Exception{\n+    testSingleMode(\"queries/tpch/18.sql\");\n+  }\n+\n+  @Test\n+  public void tpch20() throws Exception{\n+    testSingleMode(\"queries/tpch/20.sql\");\n+  }\n+}",
                "additions": 132,
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "status": "added",
                "changes": 132,
                "deletions": 0,
                "sha": "b270a8bfa08a0b3237be77da47a582e07af8198e",
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3"
            }
        ],
        "bug_id": "drill_59",
        "parent": "https://github.com/apache/drill/commit/a296383632946a1f45a9b66d4638dab00a026d30",
        "message": "DRILL-2757: Verify operators correctly handle low memory conditions and cancellations\n\nincludes:\nDRILL-2816: system error does not display the original Exception message\nDRILL-2893: ScanBatch throws a NullPointerException instead of returning OUT_OF_MEMORY\nDRILL-2894: FixedValueVectors shouldn't set it's data buffer to null when it fails to allocate it\nDRILL-2895: AbstractRecordBatch.buildSchema() should properly handle OUT_OF_MEMORY outcome\nDRILL-2905: RootExec implementations should properly handle IterOutcome.OUT_OF_MEMORY\nDRILL-2920: properly handle OutOfMemoryException\nDRILL-2947: AllocationHelper.allocateNew() doesn't have a consistent behavior when it can't allocate\n\nalso:\n- added UserException.memoryError() with a pre assigned error message\n- injection site in ScanBatch and unit test that runs various tpch queries and injects\n  an exception in the ScanBatch that will cause an OUT_OF_MEMORY outcome to be sent",
        "repo": "drill"
    }
]