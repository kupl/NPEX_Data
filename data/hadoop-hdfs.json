{
    "hadoop-hdfs_2426d84": {
        "bug_id": "hadoop-hdfs_2426d84",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/2426d84a3a13421abb9f48c81e479e935733c3f8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -390,6 +390,8 @@ Release 0.21.0 - Unreleased\n \n     HDFS-665. TestFileAppend2 sometimes hangs. (hairong)\n \n+    HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery() (shv)\n+\n Release 0.20.1 - 2009-09-01\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "sha": "5abba3a61bbc02969fd3476b07cd9bfd7ae376a4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 19,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -1982,19 +1982,21 @@ static ReplicaRecoveryInfo initReplicaRecovery(\n     return rur.createInfo();\n   }\n \n-  /** Update a replica of a block. */\n-  synchronized void updateReplica(final Block block, final long recoveryId,\n-      final long newlength) throws IOException {\n+  @Override // FSDatasetInterface\n+  public synchronized ReplicaInfo updateReplicaUnderRecovery(\n+                                    final Block oldBlock,\n+                                    final long recoveryId,\n+                                    final long newlength) throws IOException {\n     //get replica\n-    final ReplicaInfo replica = volumeMap.get(block.getBlockId());\n-    DataNode.LOG.info(\"updateReplica: block=\" + block\n+    final ReplicaInfo replica = volumeMap.get(oldBlock.getBlockId());\n+    DataNode.LOG.info(\"updateReplica: block=\" + oldBlock\n         + \", recoveryId=\" + recoveryId\n         + \", length=\" + newlength\n         + \", replica=\" + replica);\n \n     //check replica\n     if (replica == null) {\n-      throw new ReplicaNotFoundException(block);\n+      throw new ReplicaNotFoundException(oldBlock);\n     }\n \n     //check replica state\n@@ -2007,26 +2009,18 @@ synchronized void updateReplica(final Block block, final long recoveryId,\n     checkReplicaFiles(replica);\n \n     //update replica\n-    final ReplicaInfo finalized = (ReplicaInfo)updateReplicaUnderRecovery(\n-                                    replica, recoveryId, newlength);\n+    final FinalizedReplica finalized = updateReplicaUnderRecovery(\n+        (ReplicaUnderRecovery)replica, recoveryId, newlength);\n \n     //check replica files after update\n     checkReplicaFiles(finalized);\n+    return finalized;\n   }\n \n-  @Override // FSDatasetInterface\n-  public synchronized FinalizedReplica updateReplicaUnderRecovery(\n-                                          Block oldBlock,\n+  private FinalizedReplica updateReplicaUnderRecovery(\n+                                          ReplicaUnderRecovery rur,\n                                           long recoveryId,\n                                           long newlength) throws IOException {\n-    Replica r = getReplica(oldBlock.getBlockId());\n-    if(r.getState() != ReplicaState.RUR)\n-      throw new IOException(\"Replica \" + r + \" must be under recovery.\");\n-    ReplicaUnderRecovery rur = (ReplicaUnderRecovery)r;\n-    DataNode.LOG.info(\"updateReplicaUnderRecovery: recoveryId=\" + recoveryId\n-        + \", newlength=\" + newlength\n-        + \", rur=\" + rur);\n-\n     //check recovery id\n     if (rur.getRecoveryID() != recoveryId) {\n       throw new IOException(\"rur.getRecoveryID() != recoveryId = \" + recoveryId",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "12d08168eac8d3840933a776ea336865423b2838",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "patch": "@@ -350,7 +350,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   /**\n    * Update replica's generation stamp and length and finalize it.\n    */\n-  public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n+  public ReplicaInfo updateReplicaUnderRecovery(\n+                                          Block oldBlock,\n                                           long recoveryId,\n                                           long newLength) throws IOException;\n }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "sha": "462e1fffc573d6230dc402b6e34066fd7dd325f1",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -804,10 +804,10 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n     return new ReplicaRecoveryInfo(rBlock.getBlock(), ReplicaState.FINALIZED);\n   }\n \n-  @Override\n+  @Override // FSDatasetInterface\n   public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n-                                          long recoveryId,\n-                                          long newlength) throws IOException {\n+                                        long recoveryId,\n+                                        long newlength) throws IOException {\n     return new FinalizedReplica(\n         oldBlock.getBlockId(), newlength, recoveryId, null, null);\n   }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "1d9fe16525aab45d39a39a2ff7358ab4650c3c0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "patch": "@@ -234,9 +234,8 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       FSDataset.checkReplicaFiles(rur);\n \n       //update\n-      final ReplicaInfo finalized = \n-        (ReplicaInfo)fsdataset.updateReplicaUnderRecovery(\n-            rur, recoveryid, newlength);\n+      final ReplicaInfo finalized = fsdataset.updateReplicaUnderRecovery(\n+                                                rur, recoveryid, newlength);\n \n       //check meta data after update\n       FSDataset.checkReplicaFiles(finalized);",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "sha": "40d15a6f02c0cd211df29bfad0ebed066a4c5b4c",
                "status": "modified"
            }
        ],
        "message": "HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery(). Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@823732 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/f2809529a9d80906c53ae6931e84255f89ff1f8b",
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestSimulatedFSDataset.java"
        ]
    },
    "hadoop-hdfs_be31fd0": {
        "bug_id": "hadoop-hdfs_be31fd0",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/be31fd05dae345e62772ecc235f14f3f59c37e04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -471,6 +471,8 @@ Release 0.22.0 - Unreleased\n     HDFS-1560. dfs.data.dir permissions should default to 700. \n     (Todd Lipcon via eli)\n \n+    HDFS-1550. NPE when listing a file with no location. (hairong)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "sha": "71ebb27b84bef74df28bb21ef9fa2bd398da28cc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -204,6 +204,9 @@ public static String byteArray2String(byte[][] pathComponents) {\n     }\n     int nrBlocks = blocks.locatedBlockCount();\n     BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\n+    if (nrBlocks == 0) {\n+      return blkLocations;\n+    }\n     int idx = 0;\n     for (LocatedBlock blk : blocks.getLocatedBlocks()) {\n       assert idx < nrBlocks : \"Incorrect index\";",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "6975c53a25cd37604d11b0327e0db7a473aa136c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.hadoop.hdfs;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n@@ -65,5 +67,9 @@ public void testLocatedBlocks2Locations() {\n \n     assertTrue(\"expected 1 corrupt files but got \" + corruptCount, \n                corruptCount == 1);\n+    \n+    // test an empty location\n+    bs = DFSUtil.locatedBlocks2Locations(new LocatedBlocks());\n+    assertEquals(0, bs.length);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "sha": "03e6b39c735c4110059773a65fe0ee61de79ec4d",
                "status": "modified"
            }
        ],
        "message": "HDFS-1550. NPE when listing a file with no location. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1054807 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/d1d9265d1758813b62c20157297e8cfd824628ee",
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    }
}