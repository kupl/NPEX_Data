{
    "flink_101552b": {
        "bug_id": "flink_101552b",
        "commit": "https://github.com/apache/flink/commit/101552bf503cf0ca59493397ec4cd01bcc4c45a7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java?ref=101552bf503cf0ca59493397ec4cd01bcc4c45a7",
                "deletions": 1,
                "filename": "flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "patch": "@@ -123,7 +123,7 @@ public PojoSerializer(\n \t\t\tthis.fields[i].setAccessible(true);\n \t\t}\n \n-\t\tcl = Thread.currentThread().getContextClassLoader();\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \n \t\t// We only want those classes that are not our own class and are actually sub-classes.\n \t\tLinkedHashSet<Class<?>> registeredSubclasses =\n@@ -156,6 +156,7 @@ public PojoSerializer(\n \t\tthis.registeredSerializers = checkNotNull(registeredSerializers);\n \t\tthis.subclassSerializerCache = checkNotNull(subclassSerializerCache);\n \t\tthis.executionConfig = checkNotNull(executionConfig);\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \t}\n \t\n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "sha": "5c43d1e172eba312bfc6efdb385bc1add3d852fa",
                "status": "modified"
            }
        ],
        "message": "[FLINK-13159] Fix the NPE when PojoSerializer restored",
        "parent": "https://github.com/apache/flink/commit/886419f12f60df803c9d757e381f201920a8061a",
        "repo": "flink",
        "unit_tests": [
            "PojoSerializerTest.java"
        ]
    },
    "flink_13150a4": {
        "bug_id": "flink_13150a4",
        "commit": "https://github.com/apache/flink/commit/13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java?ref=13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "patch": "@@ -91,7 +91,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "sha": "233e9f10db0c809213cafdedac435b7c84af65ef",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4631] Prevent NPE in TwoInputStreamTask\n\nCheck that the input processor has been created before cleaning it up.",
        "parent": "https://github.com/apache/flink/commit/4410c04a68c7b247bb3d7113e5f40f2a9c2165af",
        "repo": "flink",
        "unit_tests": [
            "TwoInputStreamTaskTest.java"
        ]
    },
    "flink_191b9df": {
        "bug_id": "flink_191b9df",
        "commit": "https://github.com/apache/flink/commit/191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "patch": "@@ -100,13 +100,15 @@ protected SubtaskExecutionAttemptAccumulatorsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null){\n+\t\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "sha": "97da25a1230e65b32d6d207cbd94f2f347160b53",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "patch": "@@ -114,13 +114,15 @@ protected SubtaskExecutionAttemptDetailsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null) {\n+\t\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "sha": "75fd100062e22e175fc4a748aa4271f044991737",
                "status": "modified"
            }
        ],
        "message": "[FLINK-12247][rest] Fix NPE when writing the archive json file to FileSystem\n\nThis closes #8250.",
        "parent": "https://github.com/apache/flink/commit/8a174833bee081f4f4a24caa5ddc5fe45996de13",
        "repo": "flink",
        "unit_tests": [
            "SubtaskExecutionAttemptAccumulatorsHandlerTest.java",
            "SubtaskExecutionAttemptDetailsHandlerTest.java"
        ]
    },
    "flink_26bac51": {
        "bug_id": "flink_26bac51",
        "commit": "https://github.com/apache/flink/commit/26bac51cae1d298078902a02e196fffc16ea5704",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java?ref=26bac51cae1d298078902a02e196fffc16ea5704",
                "deletions": 1,
                "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "patch": "@@ -373,7 +373,10 @@ private GetRecordsResult getRecords(String shardItr, int maxNumberOfRecords) thr\n \t\t\t\tgetRecordsResult = kinesis.getRecords(shardItr, maxNumberOfRecords);\n \n \t\t\t\t// Update millis behind latest so it gets reported by the millisBehindLatest gauge\n-\t\t\t\tshardMetricsReporter.setMillisBehindLatest(getRecordsResult.getMillisBehindLatest());\n+\t\t\t\tLong millisBehindLatest = getRecordsResult.getMillisBehindLatest();\n+\t\t\t\tif (millisBehindLatest != null) {\n+\t\t\t\t\tshardMetricsReporter.setMillisBehindLatest(millisBehindLatest);\n+\t\t\t\t}\n \t\t\t} catch (ExpiredIteratorException eiEx) {\n \t\t\t\tLOG.warn(\"Encountered an unexpected expired iterator {} for shard {};\" +\n \t\t\t\t\t\" refreshing the iterator ...\", shardItr, subscribedShard);",
                "raw_url": "https://github.com/apache/flink/raw/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "sha": "36a4e92c179135fcb9f459fcff746be4069c184a",
                "status": "modified"
            }
        ],
        "message": "[FLINK-10358] fix NPE when running flink-kinesis connector against dynamodb streams\n\nThis closes #6708.",
        "parent": "https://github.com/apache/flink/commit/e58cc14db007123c6325c7e51291650da69a4ca2",
        "repo": "flink",
        "unit_tests": [
            "ShardConsumerTest.java"
        ]
    },
    "flink_26bc3c8": {
        "bug_id": "flink_26bc3c8",
        "commit": "https://github.com/apache/flink/commit/26bc3c8c65c757285c58b2cfcb0ba81111395ea4",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java?ref=26bc3c8c65c757285c58b2cfcb0ba81111395ea4",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "patch": "@@ -109,15 +109,20 @@ public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJso\n \t\t\tByteArrayOutputStream output = new ByteArrayOutputStream()) {\n \t\t\tIOUtils.copyBytes(input, output);\n \n-\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n+\t\t\ttry {\n+\t\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n \n-\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n-\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n-\t\t\t\tString path = archivePart.get(PATH).asText();\n-\t\t\t\tString json = archivePart.get(JSON).asText();\n-\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n+\t\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n+\t\t\t\t\tString path = archivePart.get(PATH).asText();\n+\t\t\t\t\tString json = archivePart.get(JSON).asText();\n+\t\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\t}\n+\t\t\t\treturn archives;\n+\t\t\t} catch (NullPointerException npe) {\n+\t\t\t\t// occurs if the archive is empty or any of the expected fields are not present\n+\t\t\t\tthrow new IOException(\"Job archive (\" + file.getPath() + \") did not conform to expected format.\");\n \t\t\t}\n-\t\t\treturn archives;\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "sha": "ab1e34d74078a89fda138bc8d4fcc9c656e58700",
                "status": "modified"
            }
        ],
        "message": "[FLINK-14337][hs] Prevent NPE on corrupt archives",
        "parent": "https://github.com/apache/flink/commit/f22f1eba8f7695857a2015ed178365191849dac4",
        "repo": "flink",
        "unit_tests": [
            "FsJobArchivistTest.java"
        ]
    },
    "flink_2b0baea": {
        "bug_id": "flink_2b0baea",
        "commit": "https://github.com/apache/flink/commit/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "patch": "@@ -130,6 +130,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> MapOperator<T, R> map(MapFunction<T, R> mapper) {\n+\t\tif (mapper == null) {\n+\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n+\t\t}\n \t\treturn new MapOperator<T, R>(this, mapper);\n \t}\n \t\n@@ -146,6 +149,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> FlatMapOperator<T, R> flatMap(FlatMapFunction<T, R> flatMapper) {\n+\t\tif (flatMapper == null) {\n+\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n+\t\t}\n \t\treturn new FlatMapOperator<T, R>(this, flatMapper);\n \t}\n \t\n@@ -163,6 +169,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic FilterOperator<T> filter(FilterFunction<T> filter) {\n+\t\tif (filter == null) {\n+\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n+\t\t}\n \t\treturn new FilterOperator<T>(this, filter);\n \t}\n \t\n@@ -229,6 +238,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -246,6 +258,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "sha": "758cbf2133192b330a4719ef71ad3d718a6ebc23",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "patch": "@@ -439,6 +439,9 @@ private CoGroupOperatorWithoutFunction(Keys<I2> keys2) {\n \t\t\t\t * @see DataSet\n \t\t\t\t */\n \t\t\t\tpublic <R> CoGroupOperator<I1, I2, R> with(CoGroupFunction<I1, I2, R> function) {\n+\t\t\t\t\tif (function == null) {\n+\t\t\t\t\t\tthrow new NullPointerException(\"CoGroup function must not be null.\");\n+\t\t\t\t\t}\n \t\t\t\t\tTypeInformation<R> returnType = TypeExtractor.getCoGroupReturnTypes(function, input1.getType(), input2.getType());\n \t\t\t\t\treturn new CoGroupOperator<I1, I2, R>(input1, input2, keys1, keys2, function, returnType);\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "sha": "ca4b1db6cb9f978ab5ab60580490a4a7087e28c3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "patch": "@@ -113,6 +113,9 @@ public DefaultCross(DataSet<I1> input1, DataSet<I2> input2) {\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> CrossOperator<I1, I2, R> with(CrossFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Cross function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getCrossReturnTypes(function, input1.getType(), input2.getType());\n \t\t\treturn new CrossOperator<I1, I2, R>(input1, input2, function, returnType);\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "sha": "3566224eac5ee4b6a5da772318d054b63277a594",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "patch": "@@ -34,10 +34,6 @@\n \tpublic FilterOperator(DataSet<T> input, FilterFunction<T> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "sha": "adbe77f3782e234af68cfbf53c1f9476eda43c84",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "patch": "@@ -37,10 +37,6 @@\n \tpublic FlatMapOperator(DataSet<IN> input, FlatMapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getFlatMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "sha": "32f2343fa82fff567cf03d3a4f66224dbcc3c0b8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "patch": "@@ -421,6 +421,9 @@ protected DefaultJoin(DataSet<I1> input1, DataSet<I2> input2,\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> EquiJoin<I1, I2, R> with(JoinFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Join function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getJoinReturnTypes(function, getInput1Type(), getInput2Type());\n \t\t\treturn new EquiJoin<I1, I2, R>(getInput1(), getInput2(), getKeys1(), getKeys2(), function, returnType, getJoinHint());\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "sha": "992cc0a74f5434945e25fed90d03006e42ffa9ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "patch": "@@ -39,10 +39,6 @@\n \tpublic MapOperator(DataSet<IN> input, MapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "sha": "00bbb27f89f3c85fb801d56ec32a8f518d69e1a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 8,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "patch": "@@ -58,10 +58,6 @@\n \tpublic ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getGroupReduceReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\tcheckCombinability();\n@@ -76,10 +72,6 @@ public ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> funct\n \tpublic ReduceGroupOperator(Grouping<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input != null ? input.getDataSet() : null, TypeExtractor.getGroupReduceReturnTypes(function, input.getDataSet().getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\tcheckCombinability();",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "sha": "e001d910b6bb292d9f885b2f4e4af19e5fab5422",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 8,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "patch": "@@ -53,10 +53,6 @@\n \tpublic ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\t\n@@ -67,10 +63,6 @@ public ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \tpublic ReduceOperator(Grouping<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input.getDataSet(), input.getDataSet().getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "sha": "6056e8b545ee7d21a2482c990c3c4b4c524f5786",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "patch": "@@ -72,6 +72,9 @@ public SortedGrouping(DataSet<T> set, Keys<T> keys, int field, Order order) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "sha": "dc26a2b697345e49b2f134f416831033cd7bf38a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "patch": "@@ -64,6 +64,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -81,6 +84,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "sha": "95e40bc44e3abbad5f6a307be680381cb8b84f94",
                "status": "modified"
            }
        ],
        "message": "Fixes bugs where the TypeExtractor throws an NPE instead of the operators",
        "parent": "https://github.com/apache/flink/commit/b746f452e7187dad08340b9cfdc2fa18a516a6c7",
        "repo": "flink",
        "unit_tests": [
            "CoGroupOperatorTest.java",
            "CrossOperatorTest.java",
            "JoinOperatorTest.java",
            "MapOperatorTest.java",
            "ReduceOperatorTest.java"
        ]
    },
    "flink_3ce8596": {
        "bug_id": "flink_3ce8596",
        "commit": "https://github.com/apache/flink/commit/3ce8596b43f88b2b6d51dab687ab224a43b825fb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java?ref=3ce8596b43f88b2b6d51dab687ab224a43b825fb",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "patch": "@@ -69,7 +69,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "sha": "0f8f4a4cf43914ca7a894f6793bbea1abe81737b",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4631] Prevent NPE in OneInputStreamTask\n\nThis closes #2709.",
        "parent": "https://github.com/apache/flink/commit/211f5db9d764efe5318867993f1b33f4eee48117",
        "repo": "flink",
        "unit_tests": [
            "OneInputStreamTaskTest.java"
        ]
    },
    "flink_4064b5b": {
        "bug_id": "flink_4064b5b",
        "commit": "https://github.com/apache/flink/commit/4064b5b67d6d220e1d5518bca96688f51cbbb891",
        "file": [
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.heartbeat;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+\n+/**\n+ * {@link HeartbeatManager} implementation which does nothing.\n+ *\n+ * @param <I> ignored\n+ * @param <O> ignored\n+ */\n+public class NoOpHeartbeatManager<I, O> implements HeartbeatManager<I, O> {\n+\tprivate static final NoOpHeartbeatManager<Object, Object> INSTANCE = new NoOpHeartbeatManager<>();\n+\n+\tprivate NoOpHeartbeatManager() {}\n+\n+\t@Override\n+\tpublic void monitorTarget(ResourceID resourceID, HeartbeatTarget<O> heartbeatTarget) {}\n+\n+\t@Override\n+\tpublic void unmonitorTarget(ResourceID resourceID) {}\n+\n+\t@Override\n+\tpublic void stop() {}\n+\n+\t@Override\n+\tpublic long getLastHeartbeatFrom(ResourceID resourceId) {\n+\t\treturn 0;\n+\t}\n+\n+\t@Override\n+\tpublic void receiveHeartbeat(ResourceID heartbeatOrigin, I heartbeatPayload) {}\n+\n+\t@Override\n+\tpublic void requestHeartbeat(ResourceID requestOrigin, I heartbeatPayload) {}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic static <A, B> NoOpHeartbeatManager<A, B> getInstance() {\n+\t\treturn (NoOpHeartbeatManager<A, B>) INSTANCE;\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "sha": "965a50b3f3539236a7e706dd51137b6b66ffcc57",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 9,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.io.network.partition.PartitionTracker;\n import org.apache.flink.runtime.io.network.partition.PartitionTrackerFactory;\n@@ -269,6 +270,8 @@ public JobMaster(\n \t\tthis.establishedResourceManagerConnection = null;\n \n \t\tthis.accumulators = new HashMap<>();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.resourceManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \tprivate SchedulerNG createScheduler(final JobManagerJobMetricGroup jobManagerJobMetricGroup) throws Exception {\n@@ -785,15 +788,8 @@ private Acknowledge suspendExecution(final Exception cause) {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n-\t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n+\t\ttaskManagerHeartbeatManager.stop();\n+\t\tresourceManagerHeartbeatManager.stop();\n \t}\n \n \tprivate void startHeartbeatServices() {",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "sha": "665c4aa479d08a98efeed8e1c6b3fee49a99cb38",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.InstanceID;\n@@ -178,6 +179,9 @@ public ResourceManager(\n \t\tthis.jmResourceIdRegistrations = new HashMap<>(4);\n \t\tthis.taskExecutors = new HashMap<>(8);\n \t\tthis.taskExecutorGatewayFutures = new HashMap<>(8);\n+\n+\t\tthis.jobManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \n@@ -972,15 +976,8 @@ private void startHeartbeatServices() {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n \t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (jobManagerHeartbeatManager != null) {\n \t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "sha": "8698e842ac548596330b545a4d74601cfd473046",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 36,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "patch": "@@ -157,8 +157,6 @@\n \t/** The task manager configuration. */\n \tprivate final TaskManagerConfiguration taskManagerConfiguration;\n \n-\tprivate final HeartbeatServices heartbeatServices;\n-\n \t/** The fatal error handler to use in case of a fatal error. */\n \tprivate final FatalErrorHandler fatalErrorHandler;\n \n@@ -207,10 +205,10 @@\n \tprivate FileCache fileCache;\n \n \t/** The heartbeat manager for job manager in the task manager. */\n-\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n \n \t/** The heartbeat manager for resource manager in the task manager. */\n-\tprivate HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n \n \tprivate final PartitionTable<JobID> partitionTable;\n \n@@ -249,7 +247,6 @@ public TaskExecutor(\n \t\tcheckArgument(taskManagerConfiguration.getNumberSlots() > 0, \"The number of slots has to be larger than 0.\");\n \n \t\tthis.taskManagerConfiguration = checkNotNull(taskManagerConfiguration);\n-\t\tthis.heartbeatServices = checkNotNull(heartbeatServices);\n \t\tthis.taskExecutorServices = checkNotNull(taskExecutorServices);\n \t\tthis.haServices = checkNotNull(haServices);\n \t\tthis.fatalErrorHandler = checkNotNull(fatalErrorHandler);\n@@ -278,6 +275,26 @@ public TaskExecutor(\n \n \t\tthis.stackTraceSampleService = new StackTraceSampleService(rpcService.getScheduledExecutor());\n \t\tthis.taskCompletionTracker = new TaskCompletionTracker();\n+\n+\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n+\t\tthis.jobManagerHeartbeatManager = createJobManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t\tthis.resourceManagerHeartbeatManager = createResourceManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t}\n+\n+\tprivate HeartbeatManager<Void, SlotReport> createResourceManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew ResourceManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n+\t}\n+\n+\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> createJobManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew JobManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n \t}\n \n \t@Override\n@@ -304,8 +321,6 @@ public void onStart() throws Exception {\n \n \tprivate void startTaskExecutorServices() throws Exception {\n \t\ttry {\n-\t\t\tstartHeartbeatServices();\n-\n \t\t\t// start by connecting to the ResourceManager\n \t\t\tresourceManagerLeaderRetriever.start(new ResourceManagerLeaderListener());\n \n@@ -412,38 +427,9 @@ private void stopTaskExecutorServices() throws Exception {\n \t\t// it will call close() recursively from the parent to children\n \t\ttaskManagerMetricGroup.close();\n \n-\t\tstopHeartbeatServices();\n-\n \t\tExceptionUtils.tryRethrowException(exception);\n \t}\n \n-\tprivate void startHeartbeatServices() {\n-\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n-\t\tjobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew JobManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\n-\t\tresourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew ResourceManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\t}\n-\n-\tprivate void stopHeartbeatServices() {\n-\t\tif (jobManagerHeartbeatManager != null) {\n-\t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n-\t}\n-\n \t// ======================================================================\n \t//  RPC methods\n \t// ======================================================================",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "sha": "f0db4cd82b2d371a72c1a12cbf6f650cfbc876e3",
                "status": "modified"
            }
        ],
        "message": "[FLINK-14315] Make heartbeat manager fields non-nullable\n\nThis commit introduces the NoOpHeartbeatManager which can be used to initialize\nan unset heartbeat manager field. This allows to make the heartbeat manager fields\nnon-nullable which in turn avoid NPE.\n\nMoreover, this commit makes the heartbeat manager fields of the TaskExecutor\nfinal.\n\nThis closes #9837.",
        "parent": "https://github.com/apache/flink/commit/c5c59feec0ed8a0ac5213802d79956c815a2b812",
        "repo": "flink",
        "unit_tests": [
            "JobMasterTest.java",
            "ResourceManagerTest.java",
            "TaskExecutorTest.java"
        ]
    },
    "flink_47db9cb": {
        "bug_id": "flink_47db9cb",
        "commit": "https://github.com/apache/flink/commit/47db9cb1a867870a8da0b403e0ec217ac461ba04",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/flink/blob/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java?ref=47db9cb1a867870a8da0b403e0ec217ac461ba04",
                "deletions": 2,
                "filename": "flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "patch": "@@ -184,8 +184,13 @@ public boolean delete(final Path f, final boolean recursive) throws IOException\n \t\tfinal File file = pathToFile(f);\n \t\tif (file.isFile()) {\n \t\t\treturn file.delete();\n-\t\t} else if ((!recursive) && file.isDirectory() && (file.listFiles().length != 0)) {\n-\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t} else if ((!recursive) && file.isDirectory()) {\n+\t\t\tFile[] containedFiles = file.listFiles();\n+\t\t\tif (containedFiles == null) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" does not exist or an I/O error occurred\");\n+\t\t\t} else if (containedFiles.length != 0) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t\t}\n \t\t}\n \n \t\treturn delete(file);",
                "raw_url": "https://github.com/apache/flink/raw/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "sha": "7ad68b35d7a5dedc5db6a8dcf212146c233fc481",
                "status": "modified"
            }
        ],
        "message": "[FLINK-5147] Prevent NPE in LocalFS#delete()\n\nThis closes #2859.",
        "parent": "https://github.com/apache/flink/commit/dc5dd5106738e393761a62a56d9e684c722c516f",
        "repo": "flink",
        "unit_tests": [
            "LocalFileSystemTest.java"
        ]
    },
    "flink_4b19e27": {
        "bug_id": "flink_4b19e27",
        "commit": "https://github.com/apache/flink/commit/4b19e272043907b70791bff8a85bd493e212947c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java?ref=4b19e272043907b70791bff8a85bd493e212947c",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "patch": "@@ -58,6 +58,8 @@ protected void run() throws Exception {\n \t\n \t@Override\n \tprotected void cancelTask() throws Exception {\n-\t\theadOperator.cancel();\n+\t\tif (headOperator != null) {\n+\t\t\theadOperator.cancel();\n+\t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "sha": "18291408d0e35e73db8cdceb731870759b5f7c70",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6182] Fix possible NPE in SourceStreamTask\n\nThis closes #3606.",
        "parent": "https://github.com/apache/flink/commit/11fe3dc89f6b6b24fa21cc51d5e935e91634dbe5",
        "repo": "flink",
        "unit_tests": [
            "SourceStreamTaskTest.java"
        ]
    },
    "flink_56017a9": {
        "bug_id": "flink_56017a9",
        "commit": "https://github.com/apache/flink/commit/56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/monitoring/metrics.md?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 3,
                "filename": "docs/monitoring/metrics.md",
                "patch": "@@ -424,7 +424,7 @@ of your Flink distribution.\n \n Parameters:\n \n-- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations).\n+- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations). In order to be able to run several instances of the reporter on one host (e.g. when one TaskManager is colocated with the JobManager) it is advisable to use a port range like `9250-9260`.\n \n Example configuration:\n \n@@ -440,11 +440,11 @@ Flink metric types are mapped to Prometheus metric types as follows:\n | Flink     | Prometheus | Note                                     |\n | --------- |------------|------------------------------------------|\n | Counter   | Gauge      |Prometheus counters cannot be decremented.|\n-| Gauge     | Gauge      |                                          |\n+| Gauge     | Gauge      |Only numbers and booleans are supported.  |\n | Histogram | Summary    |Quantiles .5, .75, .95, .98, .99 and .999 |\n | Meter     | Gauge      |The gauge exports the meter's rate.       |\n \n-All Flink metrics variables, such as `<host>`, `<job_name>`, `<tm_id>`, `<subtask_index>`, `<task_name>` and `<operator_name>`, are exported to Prometheus as labels. \n+All Flink metrics variables (see [List of all Variables](#list-of-all-variables)) are exported to Prometheus as labels. \n \n ### StatsD (org.apache.flink.metrics.statsd.StatsDReporter)\n ",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md",
                "sha": "64d7318dab3d625343015d8550be1e93686cfea4",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 11,
                "filename": "flink-metrics/flink-metrics-prometheus/pom.xml",
                "patch": "@@ -40,6 +40,13 @@ under the License.\n \t\t\t<scope>provided</scope>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-core</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n \t\t\t<artifactId>flink-runtime_${scala.binary.version}</artifactId>\n@@ -62,16 +69,10 @@ under the License.\n \n \t\t<dependency>\n \t\t\t<groupId>io.prometheus</groupId>\n-\t\t\t<artifactId>simpleclient_servlet</artifactId>\n+\t\t\t<artifactId>simpleclient_httpserver</artifactId>\n \t\t\t<version>${prometheus.version}</version>\n \t\t</dependency>\n \n-\t\t<dependency>\n-\t\t\t<groupId>org.nanohttpd</groupId>\n-\t\t\t<artifactId>nanohttpd</artifactId>\n-\t\t\t<version>2.2.0</version>\n-\t\t</dependency>\n-\n \t\t<!-- test dependencies -->\n \n \t\t<dependency>\n@@ -114,10 +115,6 @@ under the License.\n \t\t\t\t\t\t\t\t\t<pattern>io.prometheus.client</pattern>\n \t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.io.prometheus.client</shadedPattern>\n \t\t\t\t\t\t\t\t</relocation>\n-\t\t\t\t\t\t\t\t<relocation>\n-\t\t\t\t\t\t\t\t\t<pattern>fi.iki.elonen</pattern>\n-\t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.fi.iki.elonen</shadedPattern>\n-\t\t\t\t\t\t\t\t</relocation>\n \t\t\t\t\t\t\t</relocations>\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml",
                "sha": "0e9b2611db6dec8eb47aae4441a3df09c43b5424",
                "status": "modified"
            },
            {
                "additions": 134,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "changes": 229,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 95,
                "filename": "flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "patch": "@@ -24,28 +24,28 @@\n import org.apache.flink.metrics.Counter;\n import org.apache.flink.metrics.Gauge;\n import org.apache.flink.metrics.Histogram;\n-import org.apache.flink.metrics.HistogramStatistics;\n import org.apache.flink.metrics.Meter;\n import org.apache.flink.metrics.Metric;\n import org.apache.flink.metrics.MetricConfig;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.flink.metrics.reporter.MetricReporter;\n import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;\n import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;\n+import org.apache.flink.util.NetUtils;\n \n-import fi.iki.elonen.NanoHTTPD;\n import io.prometheus.client.Collector;\n import io.prometheus.client.CollectorRegistry;\n-import io.prometheus.client.exporter.common.TextFormat;\n+import io.prometheus.client.exporter.HTTPServer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.io.IOException;\n-import java.io.StringWriter;\n+import java.util.AbstractMap;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -59,7 +59,7 @@\n \tprivate static final Logger LOG = LoggerFactory.getLogger(PrometheusReporter.class);\n \n \tstatic final String ARG_PORT = \"port\";\n-\tprivate static final int DEFAULT_PORT = 9249;\n+\tprivate static final String DEFAULT_PORT = \"9249\";\n \n \tprivate static final Pattern UNALLOWED_CHAR_PATTERN = Pattern.compile(\"[^a-zA-Z0-9:_]\");\n \tprivate static final CharacterFilter CHARACTER_FILTER = new CharacterFilter() {\n@@ -72,8 +72,8 @@ public String filterCharacters(String input) {\n \tprivate static final char SCOPE_SEPARATOR = '_';\n \tprivate static final String SCOPE_PREFIX = \"flink\" + SCOPE_SEPARATOR;\n \n-\tprivate PrometheusEndpoint prometheusEndpoint;\n-\tprivate final Map<String, Collector> collectorsByMetricName = new HashMap<>();\n+\tprivate HTTPServer httpServer;\n+\tprivate final Map<String, AbstractMap.SimpleImmutableEntry<Collector, Integer>> collectorsWithCountByMetricName = new HashMap<>();\n \n \t@VisibleForTesting\n \tstatic String replaceInvalidChars(final String input) {\n@@ -84,27 +84,34 @@ static String replaceInvalidChars(final String input) {\n \n \t@Override\n \tpublic void open(MetricConfig config) {\n-\t\tint port = config.getInteger(ARG_PORT, DEFAULT_PORT);\n-\t\tLOG.info(\"Using port {}.\", port);\n-\t\tprometheusEndpoint = new PrometheusEndpoint(port);\n-\t\ttry {\n-\t\t\tprometheusEndpoint.start(NanoHTTPD.SOCKET_READ_TIMEOUT, true);\n-\t\t} catch (IOException e) {\n-\t\t\tfinal String msg = \"Could not start PrometheusEndpoint on port \" + port;\n-\t\t\tLOG.warn(msg, e);\n-\t\t\tthrow new RuntimeException(msg, e);\n+\t\tString portsConfig = config.getString(ARG_PORT, DEFAULT_PORT);\n+\t\tIterator<Integer> ports = NetUtils.getPortRangeFromString(portsConfig);\n+\n+\t\twhile (ports.hasNext()) {\n+\t\t\tint port = ports.next();\n+\t\t\ttry {\n+\t\t\t\thttpServer = new HTTPServer(port);\n+\t\t\t\tLOG.info(\"Started PrometheusReporter HTTP server on port {}.\", port);\n+\t\t\t\tbreak;\n+\t\t\t} catch (IOException ioe) { //assume port conflict\n+\t\t\t\tLOG.debug(\"Could not start PrometheusReporter HTTP server on port {}.\", port, ioe);\n+\t\t\t}\n+\t\t}\n+\t\tif (httpServer == null) {\n+\t\t\tthrow new RuntimeException(\"Could not start PrometheusReporter HTTP server on any configured port. Ports: \" + portsConfig);\n \t\t}\n \t}\n \n \t@Override\n \tpublic void close() {\n-\t\tprometheusEndpoint.stop();\n+\t\tif (httpServer != null) {\n+\t\t\thttpServer.stop();\n+\t\t}\n \t\tCollectorRegistry.defaultRegistry.clear();\n \t}\n \n \t@Override\n \tpublic void notifyOfAddedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tfinal String scope = SCOPE_PREFIX + getLogicalScope(group);\n \n \t\tList<String> dimensionKeys = new LinkedList<>();\n \t\tList<String> dimensionValues = new LinkedList<>();\n@@ -114,146 +121,178 @@ public void notifyOfAddedMetric(final Metric metric, final String metricName, fi\n \t\t\tdimensionValues.add(CHARACTER_FILTER.filterCharacters(dimension.getValue()));\n \t\t}\n \n-\t\tfinal String validMetricName = scope + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n-\t\tfinal String metricIdentifier = group.getMetricIdentifier(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tfinal String helpString = metricName + \" (scope: \" + getLogicalScope(group) + \")\";\n+\n \t\tfinal Collector collector;\n+\t\tInteger count = 0;\n+\n+\t\tsynchronized (this) {\n+\t\t\tif (collectorsWithCountByMetricName.containsKey(scopedMetricName)) {\n+\t\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\t\tcollector = collectorWithCount.getKey();\n+\t\t\t\tcount = collectorWithCount.getValue();\n+\t\t\t} else {\n+\t\t\t\tcollector = createCollector(metric, dimensionKeys, dimensionValues, scopedMetricName, helpString);\n+\t\t\t\ttry {\n+\t\t\t\t\tcollector.register();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem registering metric {}.\", metricName, e);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\taddMetric(metric, dimensionValues, collector);\n+\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count + 1));\n+\t\t}\n+\t}\n+\n+\tprivate static String getScopedName(String metricName, MetricGroup group) {\n+\t\treturn SCOPE_PREFIX + getLogicalScope(group) + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n+\t}\n+\n+\tprivate static Collector createCollector(Metric metric, List<String> dimensionKeys, List<String> dimensionValues, String scopedMetricName, String helpString) {\n+\t\tCollector collector;\n+\t\tif (metric instanceof Gauge || metric instanceof Counter || metric instanceof Meter) {\n+\t\t\tcollector = io.prometheus.client.Gauge\n+\t\t\t\t.build()\n+\t\t\t\t.name(scopedMetricName)\n+\t\t\t\t.help(helpString)\n+\t\t\t\t.labelNames(toArray(dimensionKeys))\n+\t\t\t\t.create();\n+\t\t} else if (metric instanceof Histogram) {\n+\t\t\tcollector = new HistogramSummaryProxy((Histogram) metric, scopedMetricName, helpString, dimensionKeys, dimensionValues);\n+\t\t} else {\n+\t\t\tLOG.warn(\"Cannot create collector for unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n+\t\t\t\tmetric.getClass().getName());\n+\t\t\tcollector = null;\n+\t\t}\n+\t\treturn collector;\n+\t}\n+\n+\tprivate static void addMetric(Metric metric, List<String> dimensionValues, Collector collector) {\n \t\tif (metric instanceof Gauge) {\n-\t\t\tcollector = createGauge((Gauge) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Gauge) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Counter) {\n-\t\t\tcollector = createGauge((Counter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Counter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Meter) {\n-\t\t\tcollector = createGauge((Meter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Meter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Histogram) {\n-\t\t\tcollector = createSummary((Histogram) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((HistogramSummaryProxy) collector).addChild((Histogram) metric, dimensionValues);\n \t\t} else {\n \t\t\tLOG.warn(\"Cannot add unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n \t\t\t\tmetric.getClass().getName());\n-\t\t\treturn;\n \t\t}\n-\t\tcollector.register();\n-\t\tcollectorsByMetricName.put(metricName, collector);\n \t}\n \n \t@Override\n \tpublic void notifyOfRemovedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tCollectorRegistry.defaultRegistry.unregister(collectorsByMetricName.get(metricName));\n-\t\tcollectorsByMetricName.remove(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tsynchronized (this) {\n+\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\tfinal Integer count = collectorWithCount.getValue();\n+\t\t\tfinal Collector collector = collectorWithCount.getKey();\n+\t\t\tif (count == 1) {\n+\t\t\t\ttry {\n+\t\t\t\t\tCollectorRegistry.defaultRegistry.unregister(collector);\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem unregistering metric {}.\", scopedMetricName, e);\n+\t\t\t\t}\n+\t\t\t\tcollectorsWithCountByMetricName.remove(scopedMetricName);\n+\t\t\t} else {\n+\t\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count - 1));\n+\t\t\t}\n+\t\t}\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n \tprivate static String getLogicalScope(MetricGroup group) {\n \t\treturn ((FrontMetricGroup<AbstractMetricGroup<?>>) group).getLogicalScope(CHARACTER_FILTER, SCOPE_SEPARATOR);\n \t}\n \n-\tprivate Collector createGauge(final Gauge gauge, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\t@VisibleForTesting\n+\tstatic io.prometheus.client.Gauge.Child gaugeFrom(Gauge gauge) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\tfinal Object value = gauge.getValue();\n+\t\t\t\tif (value == null) {\n+\t\t\t\t\tLOG.debug(\"Gauge {} is null-valued, defaulting to 0.\", gauge);\n+\t\t\t\t\treturn 0;\n+\t\t\t\t}\n \t\t\t\tif (value instanceof Double) {\n \t\t\t\t\treturn (double) value;\n \t\t\t\t}\n \t\t\t\tif (value instanceof Number) {\n \t\t\t\t\treturn ((Number) value).doubleValue();\n-\t\t\t\t} else if (value instanceof Boolean) {\n+\t\t\t\t}\n+\t\t\t\tif (value instanceof Boolean) {\n \t\t\t\t\treturn ((Boolean) value) ? 1 : 0;\n-\t\t\t\t} else {\n-\t\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n-\t\t\t\t\t\tgauge, value.getClass().getName());\n-\t\t\t\t\treturn 0;\n \t\t\t\t}\n+\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n+\t\t\t\t\tgauge, value.getClass().getName());\n+\t\t\t\treturn 0;\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate static Collector createGauge(final Counter counter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Counter counter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn (double) counter.getCount();\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate Collector createGauge(final Meter meter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Meter meter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn meter.getRate();\n \t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate static Collector newGauge(String name, String identifier, List<String> labelNames, List<String> labelValues, io.prometheus.client.Gauge.Child child) {\n-\t\treturn io.prometheus.client.Gauge\n-\t\t\t.build()\n-\t\t\t.name(name)\n-\t\t\t.help(identifier)\n-\t\t\t.labelNames(toArray(labelNames))\n-\t\t\t.create()\n-\t\t\t.setChild(child, toArray(labelValues));\n-\t}\n-\n-\tprivate static HistogramSummaryProxy createSummary(final Histogram histogram, final String name, final String identifier, final List<String> dimensionKeys, final List<String> dimensionValues) {\n-\t\treturn new HistogramSummaryProxy(histogram, name, identifier, dimensionKeys, dimensionValues);\n-\t}\n-\n-\tstatic class PrometheusEndpoint extends NanoHTTPD {\n-\t\tstatic final String MIME_TYPE = \"plain/text\";\n-\n-\t\tPrometheusEndpoint(int port) {\n-\t\t\tsuper(port);\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Response serve(IHTTPSession session) {\n-\t\t\tif (session.getUri().equals(\"/metrics\")) {\n-\t\t\t\tStringWriter writer = new StringWriter();\n-\t\t\t\ttry {\n-\t\t\t\t\tTextFormat.write004(writer, CollectorRegistry.defaultRegistry.metricFamilySamples());\n-\t\t\t\t} catch (IOException e) {\n-\t\t\t\t\treturn newFixedLengthResponse(Response.Status.INTERNAL_ERROR, MIME_TYPE, \"Unable to output metrics\");\n-\t\t\t\t}\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.OK, TextFormat.CONTENT_TYPE_004, writer.toString());\n-\t\t\t} else {\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.NOT_FOUND, MIME_TYPE, \"Not found\");\n-\t\t\t}\n-\t\t}\n+\t\t};\n \t}\n \n-\tprivate static class HistogramSummaryProxy extends Collector {\n-\t\tprivate static final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n+\t@VisibleForTesting\n+\tstatic class HistogramSummaryProxy extends Collector {\n+\t\tstatic final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n \n-\t\tprivate final Histogram histogram;\n \t\tprivate final String metricName;\n-\t\tprivate final String metricIdentifier;\n+\t\tprivate final String helpString;\n \t\tprivate final List<String> labelNamesWithQuantile;\n-\t\tprivate final List<String> labelValues;\n \n-\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String metricIdentifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\t\tthis.histogram = histogram;\n+\t\tprivate final Map<List<String>, Histogram> histogramsByLabelValues = new HashMap<>();\n+\n+\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String helpString, final List<String> labelNames, final List<String> labelValues) {\n \t\t\tthis.metricName = metricName;\n-\t\t\tthis.metricIdentifier = metricIdentifier;\n+\t\t\tthis.helpString = helpString;\n \t\t\tthis.labelNamesWithQuantile = addToList(labelNames, \"quantile\");\n-\t\t\tthis.labelValues = labelValues;\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n \t\t}\n \n \t\t@Override\n \t\tpublic List<MetricFamilySamples> collect() {\n \t\t\t// We cannot use SummaryMetricFamily because it is impossible to get a sum of all values (at least for Dropwizard histograms,\n \t\t\t// whose snapshot's values array only holds a sample of recent values).\n \n-\t\t\tfinal HistogramStatistics statistics = histogram.getStatistics();\n-\n \t\t\tList<MetricFamilySamples.Sample> samples = new LinkedList<>();\n+\t\t\tfor (Map.Entry<List<String>, Histogram> labelValuesToHistogram : histogramsByLabelValues.entrySet()) {\n+\t\t\t\taddSamples(labelValuesToHistogram.getKey(), labelValuesToHistogram.getValue(), samples);\n+\t\t\t}\n+\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, helpString, samples));\n+\t\t}\n+\n+\t\tvoid addChild(final Histogram histogram, final List<String> labelValues) {\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n+\t\t}\n+\n+\t\tprivate void addSamples(final List<String> labelValues, final Histogram histogram, final List<MetricFamilySamples.Sample> samples) {\n \t\t\tsamples.add(new MetricFamilySamples.Sample(metricName + \"_count\",\n \t\t\t\tlabelNamesWithQuantile.subList(0, labelNamesWithQuantile.size() - 1), labelValues, histogram.getCount()));\n \t\t\tfor (final Double quantile : QUANTILES) {\n \t\t\t\tsamples.add(new MetricFamilySamples.Sample(metricName, labelNamesWithQuantile,\n \t\t\t\t\taddToList(labelValues, quantile.toString()),\n-\t\t\t\t\tstatistics.getQuantile(quantile)));\n+\t\t\t\t\thistogram.getStatistics().getQuantile(quantile)));\n \t\t\t}\n-\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, metricIdentifier, samples));\n \t\t}\n \t}\n \n@@ -263,7 +302,7 @@ public Response serve(IHTTPSession session) {\n \t\treturn result;\n \t}\n \n-\tprivate static String[] toArray(List<String> labelNames) {\n-\t\treturn labelNames.toArray(new String[labelNames.size()]);\n+\tprivate static String[] toArray(List<String> list) {\n+\t\treturn list.toArray(new String[list.size()]);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "sha": "1e44ab966cf8478eb36dc2eb303532730986fbc1",
                "status": "modified"
            },
            {
                "additions": 188,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "changes": 188,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 0,
                "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.metrics.prometheus;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.metrics.Counter;\n+import org.apache.flink.metrics.Gauge;\n+import org.apache.flink.metrics.Histogram;\n+import org.apache.flink.metrics.Meter;\n+import org.apache.flink.metrics.SimpleCounter;\n+import org.apache.flink.metrics.util.TestMeter;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.metrics.MetricRegistry;\n+import org.apache.flink.runtime.metrics.MetricRegistryConfiguration;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskMetricGroup;\n+import org.apache.flink.runtime.metrics.util.TestingHistogram;\n+import org.apache.flink.util.AbstractID;\n+\n+import com.mashape.unirest.http.exceptions.UnirestException;\n+import io.prometheus.client.CollectorRegistry;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.createConfigWithOneReporter;\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.pollMetrics;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Test for {@link PrometheusReporter} that registers several instances of the same metric for different subtasks.\n+ */\n+public class PrometheusReporterTaskScopeTest {\n+\tprivate static final String[] LABEL_NAMES = {\"job_id\", \"task_id\", \"task_attempt_id\", \"host\", \"task_name\", \"task_attempt_num\", \"job_name\", \"tm_id\", \"subtask_index\"};\n+\n+\tprivate static final String TASK_MANAGER_HOST = \"taskManagerHostName\";\n+\tprivate static final String TASK_MANAGER_ID = \"taskManagerId\";\n+\tprivate static final String JOB_NAME = \"jobName\";\n+\tprivate static final String TASK_NAME = \"taskName\";\n+\tprivate static final int ATTEMPT_NUMBER = 0;\n+\tprivate static final int SUBTASK_INDEX_1 = 0;\n+\tprivate static final int SUBTASK_INDEX_2 = 1;\n+\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9429\")));\n+\n+\tprivate final JobID jobId = new JobID();\n+\tprivate final JobVertexID taskId1 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId1 = new AbstractID();\n+\tprivate final String[] labelValues1 = {jobId.toString(), taskId1.toString(), taskAttemptId1.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_1};\n+\tprivate final JobVertexID taskId2 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId2 = new AbstractID();\n+\tprivate final String[] labelValues2 = {jobId.toString(), taskId2.toString(), taskAttemptId2.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_2};\n+\n+\tprivate final TaskManagerMetricGroup tmMetricGroup = new TaskManagerMetricGroup(registry, TASK_MANAGER_HOST, TASK_MANAGER_ID);\n+\tprivate final TaskManagerJobMetricGroup tmJobMetricGroup = new TaskManagerJobMetricGroup(registry, tmMetricGroup, jobId, JOB_NAME);\n+\tprivate final TaskMetricGroup taskMetricGroup1 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId1, taskAttemptId1, TASK_NAME, SUBTASK_INDEX_1, ATTEMPT_NUMBER);\n+\tprivate final TaskMetricGroup taskMetricGroup2 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId2, taskAttemptId2, TASK_NAME, SUBTASK_INDEX_2, ATTEMPT_NUMBER);\n+\n+\t@Test\n+\tpublic void countersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\t}\n+\n+\t@Test\n+\tpublic void gaugesCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tGauge<Integer> gauge1 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 3;\n+\t\t\t}\n+\t\t};\n+\t\tGauge<Integer> gauge2 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 4;\n+\t\t\t}\n+\t\t};\n+\n+\t\ttaskMetricGroup1.gauge(\"my_gauge\", gauge1);\n+\t\ttaskMetricGroup2.gauge(\"my_gauge\", gauge2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(3.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(4.));\n+\t}\n+\n+\t@Test\n+\tpublic void metersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tMeter meter = new TestMeter();\n+\n+\t\ttaskMetricGroup1.meter(\"my_meter\", meter);\n+\t\ttaskMetricGroup2.meter(\"my_meter\", meter);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(5.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(5.));\n+\t}\n+\n+\t@Test\n+\tpublic void histogramsCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tHistogram histogram = new TestingHistogram();\n+\n+\t\ttaskMetricGroup1.histogram(\"my_histogram\", histogram);\n+\t\ttaskMetricGroup2.histogram(\"my_histogram\", histogram);\n+\n+\t\tfinal String exportedMetrics = pollMetrics().getBody();\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"0\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"1\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\n+\t\tfinal String[] labelNamesWithQuantile = addToArray(LABEL_NAMES, \"quantile\");\n+\t\tfor (Double quantile : PrometheusReporter.HistogramSummaryProxy.QUANTILES) {\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues1, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues2, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void removingSingleInstanceOfMetricDoesNotBreakOtherInstances() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\n+\t\ttaskMetricGroup2.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\n+\t\ttaskMetricGroup1.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tnullValue());\n+\t}\n+\n+\tprivate String[] addToArray(String[] array, String element) {\n+\t\tfinal String[] labelNames = Arrays.copyOf(array, LABEL_NAMES.length + 1);\n+\t\tlabelNames[LABEL_NAMES.length] = element;\n+\t\treturn labelNames;\n+\t}\n+\n+\t@After\n+\tpublic void shutdownRegistry() {\n+\t\tregistry.shutdown();\n+\t}\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "sha": "c7d4040734ec7451e19a17649c2372d18bff3bfb",
                "status": "added"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "changes": 185,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 42,
                "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "patch": "@@ -47,13 +47,13 @@\n import java.util.Arrays;\n \n import static org.apache.flink.metrics.prometheus.PrometheusReporter.ARG_PORT;\n-import static org.apache.flink.runtime.metrics.scope.ScopeFormat.SCOPE_SEPARATOR;\n import static org.hamcrest.Matchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n import static org.junit.Assert.assertThat;\n \n /**\n- * Test for {@link PrometheusReporter}.\n+ * Basic test for {@link PrometheusReporter}.\n  */\n public class PrometheusReporterTest extends TestLogger {\n \tprivate static final int NON_DEFAULT_PORT = 9429;\n@@ -70,22 +70,21 @@\n \t@Rule\n \tpublic ExpectedException thrown = ExpectedException.none();\n \n-\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter()));\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"\" + NON_DEFAULT_PORT)));\n+\tprivate final FrontMetricGroup<TaskManagerMetricGroup> metricGroup = new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER));\n \tprivate final MetricReporter reporter = registry.getReporters().get(0);\n \n+\t/**\n+\t * {@link io.prometheus.client.Counter} may not decrease, so report {@link Counter} as {@link io.prometheus.client.Gauge}.\n+\t *\n+\t * @throws UnirestException Might be thrown on HTTP problems.\n+\t */\n \t@Test\n \tpublic void counterIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\t//Prometheus counters may not decrease\n \t\tCounter testCounter = new SimpleCounter();\n \t\ttestCounter.inc(7);\n \n-\t\tString counterName = \"testCounter\";\n-\t\tString gaugeName = SCOPE_PREFIX + counterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testCounter, counterName),\n-\t\t\tequalTo(HELP_PREFIX + gaugeName + \" \" + getFullMetricName(counterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + gaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tgaugeName + DEFAULT_LABELS + \" 7.0\" + \"\\n\"));\n+\t\tassertThatGaugeIsExported(testCounter, \"testCounter\", \"7.0\");\n \t}\n \n \t@Test\n@@ -97,13 +96,34 @@ public Integer getValue() {\n \t\t\t}\n \t\t};\n \n-\t\tString gaugeName = \"testGauge\";\n-\t\tString prometheusGaugeName = SCOPE_PREFIX + gaugeName;\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"1.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void nullGaugeDoesNotBreakReporter() throws UnirestException {\n+\t\tGauge<Integer> testGauge = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"0.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n+\t\tMeter testMeter = new TestMeter();\n+\n+\t\tassertThatGaugeIsExported(testMeter, \"testMeter\", \"5.0\");\n+\t}\n \n-\t\tassertThat(addMetricAndPollResponse(testGauge, gaugeName),\n-\t\t\tequalTo(HELP_PREFIX + prometheusGaugeName + \" \" + getFullMetricName(gaugeName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + prometheusGaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tprometheusGaugeName + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n+\tprivate void assertThatGaugeIsExported(Metric metric, String name, String expectedValue) throws UnirestException {\n+\t\tfinal String prometheusName = SCOPE_PREFIX + name;\n+\t\tassertThat(addMetricAndPollResponse(metric, name),\n+\t\t\tcontainsString(HELP_PREFIX + prometheusName + \" \" + name + \" (scope: taskmanager)\\n\" +\n+\t\t\t\tTYPE_PREFIX + prometheusName + \" gauge\" + \"\\n\" +\n+\t\t\t\tprometheusName + DEFAULT_LABELS + \" \" + expectedValue + \"\\n\"));\n \t}\n \n \t@Test\n@@ -114,7 +134,7 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\tString summaryName = SCOPE_PREFIX + histogramName;\n \n \t\tString response = addMetricAndPollResponse(testHistogram, histogramName);\n-\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + getFullMetricName(histogramName) + \"\\n\" +\n+\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + histogramName + \" (scope: taskmanager)\\n\" +\n \t\t\tTYPE_PREFIX + summaryName + \" summary\" + \"\\n\" +\n \t\t\tsummaryName + \"_count\" + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n \t\tfor (String quantile : Arrays.asList(\"0.5\", \"0.75\", \"0.95\", \"0.98\", \"0.99\", \"0.999\")) {\n@@ -123,19 +143,6 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\t}\n \t}\n \n-\t@Test\n-\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\tMeter testMeter = new TestMeter();\n-\n-\t\tString meterName = \"testMeter\";\n-\t\tString counterName = SCOPE_PREFIX + meterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testMeter, meterName),\n-\t\t\tequalTo(HELP_PREFIX + counterName + \" \" + getFullMetricName(meterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + counterName + \" gauge\" + \"\\n\" +\n-\t\t\t\tcounterName + DEFAULT_LABELS + \" 5.0\" + \"\\n\"));\n-\t}\n-\n \t@Test\n \tpublic void endpointIsUnavailableAfterReporterIsClosed() throws UnirestException {\n \t\treporter.close();\n@@ -160,25 +167,119 @@ public void invalidCharactersAreReplacedWithUnderscore() {\n \t\tassertThat(PrometheusReporter.replaceInvalidChars(\"a,=;:?'b,=;:?'c\"), equalTo(\"a___:__b___:__c\"));\n \t}\n \n+\t@Test\n+\tpublic void doubleGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Double>() {\n+\t\t\t@Override\n+\t\t\tpublic Double getValue() {\n+\t\t\t\treturn 3.14;\n+\t\t\t}\n+\t\t}).get(), equalTo(3.14));\n+\t}\n+\n+\t@Test\n+\tpublic void shortGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Short>() {\n+\t\t\t@Override\n+\t\t\tpublic Short getValue() {\n+\t\t\t\treturn 13;\n+\t\t\t}\n+\t\t}).get(), equalTo(13.));\n+\t}\n+\n+\t@Test\n+\tpublic void booleanGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Boolean>() {\n+\t\t\t@Override\n+\t\t\tpublic Boolean getValue() {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\t\t}).get(), equalTo(1.));\n+\t}\n+\n+\t/**\n+\t * Prometheus only supports numbers, so report non-numeric gauges as 0.\n+\t */\n+\t@Test\n+\tpublic void stringGaugeCannotBeConverted() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<String>() {\n+\t\t\t@Override\n+\t\t\tpublic String getValue() {\n+\t\t\t\treturn \"I am not a number\";\n+\t\t\t}\n+\t\t}).get(), equalTo(0.));\n+\t}\n+\n+\t@Test\n+\tpublic void registeringSameMetricTwiceDoesNotThrowException() {\n+\t\tCounter counter = new SimpleCounter();\n+\t\tcounter.inc();\n+\t\tString counterName = \"testCounter\";\n+\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void addingUnknownMetricTypeDoesNotThrowException(){\n+\t\tclass SomeMetricType implements Metric{}\n+\n+\t\treporter.notifyOfAddedMetric(new SomeMetricType(), \"name\", metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartTwoReportersOnSamePort() {\n+\t\tfinal MetricRegistry fixedPort1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"12345\")));\n+\t\tfinal MetricRegistry fixedPort2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"12345\")));\n+\n+\t\tassertThat(fixedPort1.getReporters(), hasSize(1));\n+\t\tassertThat(fixedPort2.getReporters(), hasSize(0));\n+\n+\t\tfixedPort1.shutdown();\n+\t\tfixedPort2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void canStartTwoReportersWhenUsingPortRange() {\n+\t\tfinal MetricRegistry portRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9249-9252\")));\n+\t\tfinal MetricRegistry portRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9249-9252\")));\n+\n+\t\tassertThat(portRange1.getReporters(), hasSize(1));\n+\t\tassertThat(portRange2.getReporters(), hasSize(1));\n+\n+\t\tportRange1.shutdown();\n+\t\tportRange2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartThreeReportersWhenPortRangeIsTooSmall() {\n+\t\tfinal MetricRegistry smallPortRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange3 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test3\", \"9253-9254\")));\n+\n+\t\tassertThat(smallPortRange1.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange2.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange3.getReporters(), hasSize(0));\n+\n+\t\tsmallPortRange1.shutdown();\n+\t\tsmallPortRange2.shutdown();\n+\t\tsmallPortRange3.shutdown();\n+\t}\n+\n \tprivate String addMetricAndPollResponse(Metric metric, String metricName) throws UnirestException {\n-\t\treporter.notifyOfAddedMetric(metric, metricName, new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER)));\n+\t\treporter.notifyOfAddedMetric(metric, metricName, metricGroup);\n \t\treturn pollMetrics().getBody();\n \t}\n \n-\tprivate static HttpResponse<String> pollMetrics() throws UnirestException {\n+\tstatic HttpResponse<String> pollMetrics() throws UnirestException {\n \t\treturn Unirest.get(\"http://localhost:\" + NON_DEFAULT_PORT + \"/metrics\").asString();\n \t}\n \n-\tprivate static String getFullMetricName(String metricName) {\n-\t\treturn HOST_NAME + SCOPE_SEPARATOR + \"taskmanager\" + SCOPE_SEPARATOR + TASK_MANAGER + SCOPE_SEPARATOR + metricName;\n-\t}\n-\n-\tprivate static Configuration createConfigWithOneReporter() {\n+\tstatic Configuration createConfigWithOneReporter(String reporterName, String portString) {\n \t\tConfiguration cfg = new Configuration();\n-\t\tcfg.setString(MetricOptions.REPORTERS_LIST, \"test1\");\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" +\n-\t\t\tConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" + ARG_PORT, \"\" + NON_DEFAULT_PORT);\n+\t\tcfg.setString(MetricOptions.REPORTERS_LIST, reporterName);\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ARG_PORT, portString);\n \t\treturn cfg;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "sha": "956339b818930d9334d877f1cbc80c7e12e508a1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -116,7 +116,7 @@ under the License.\n \t\t<curator.version>2.12.0</curator.version>\n \t\t<jackson.version>2.7.4</jackson.version>\n \t\t<metrics.version>3.1.5</metrics.version>\n-\t\t<prometheus.version>0.0.21</prometheus.version>\n+\t\t<prometheus.version>0.0.26</prometheus.version>\n \t\t<junit.version>4.12</junit.version>\n \t\t<mockito.version>1.10.19</mockito.version>\n \t\t<powermock.version>1.6.5</powermock.version>",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml",
                "sha": "9384a48fe8472c0708f01a7cd5061d6e12d3b03e",
                "status": "modified"
            }
        ],
        "message": "[FLINK-7502][metrics] Improve PrometheusReporter\n\n* Do not throw exception when same metric is added twice\n* Add possibility to configure port range\n* Bump prometheus.version 0.0.21 -> 0.0.26\n* Use simpleclient_httpserver instead of nanohttpd\n* guard gauge report against null\n* guard close() vs NPE\n\nThis closes #4586.",
        "parent": "https://github.com/apache/flink/commit/c0199f5d181a8d249201004f6ec6f897f9b799c4",
        "repo": "flink",
        "unit_tests": [
            "PrometheusReporterTest.java"
        ]
    },
    "flink_5dfc897": {
        "bug_id": "flink_5dfc897",
        "commit": "https://github.com/apache/flink/commit/5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
                "deletions": 15,
                "filename": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "patch": "@@ -17,20 +17,20 @@\n \n package org.apache.flink.streaming.api.functions.source;\n \n-import java.io.IOException;\n-import java.net.URI;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.core.fs.FileStatus;\n import org.apache.flink.core.fs.FileSystem;\n import org.apache.flink.core.fs.Path;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n public class FileMonitoringFunction implements SourceFunction<Tuple3<String, Long, Long>> {\n \tprivate static final long serialVersionUID = 1L;\n \n@@ -95,16 +95,21 @@ public void run(SourceContext<Tuple3<String, Long, Long>> ctx) throws Exception\n \n \t\tFileStatus[] statuses = fileSystem.listStatus(new Path(path));\n \n-\t\tfor (FileStatus status : statuses) {\n-\t\t\tPath filePath = status.getPath();\n-\t\t\tString fileName = filePath.getName();\n-\t\t\tlong modificationTime = status.getModificationTime();\n-\n-\t\t\tif (!isFiltered(fileName, modificationTime)) {\n-\t\t\t\tfiles.add(filePath.toString());\n-\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\tif (statuses == null) {\n+\t\t\tLOG.warn(\"Path does not exist: {}\", path);\n+\t\t} else {\n+\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\tPath filePath = status.getPath();\n+\t\t\t\tString fileName = filePath.getName();\n+\t\t\t\tlong modificationTime = status.getModificationTime();\n+\n+\t\t\t\tif (!isFiltered(fileName, modificationTime)) {\n+\t\t\t\t\tfiles.add(filePath.toString());\n+\t\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n+\n \t\treturn files;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "sha": "a2179238c396e621c441403f829ffe5ada392ec8",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
                "deletions": 0,
                "filename": "flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *\t http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.source;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for the {@link org.apache.flink.streaming.api.functions.source.FileMonitoringFunction}.\n+ */\n+public class FileMonitoringFunctionTest {\n+\n+\t@Test\n+\tpublic void testForEmptyLocation() throws Exception {\n+\t\tfinal FileMonitoringFunction fileMonitoringFunction\n+\t\t\t= new FileMonitoringFunction(\"?non-existing-path\", 1L, FileMonitoringFunction.WatchType.ONLY_NEW_FILES);\n+\n+        new Thread() {\n+            @Override\n+            public void run() {\n+                try {\n+                    Thread.sleep(1000L);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+                fileMonitoringFunction.cancel();\n+            }\n+        }.start();\n+\n+\t\tfileMonitoringFunction.run(\n+            new StreamSource.NonWatermarkContext<Tuple3<String, Long, Long>>(\n+                new Object(),\n+                new Output<StreamRecord<Tuple3<String, Long, Long>>>() {\n+                    @Override\n+                    public void emitWatermark(Watermark mark) { }\n+                    @Override\n+                    public void collect(StreamRecord<Tuple3<String, Long, Long>> record) { }\n+                    @Override\n+                    public void close() { }\n+                })\n+        );\n+\t}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "sha": "2d9921ae943e149880bacc889ab87dfa456619e0",
                "status": "added"
            }
        ],
        "message": "[FLINK-2817] [streaming] FileMonitoring function logs on empty location\n\nInstead of throwing NPE when location is empty\n\nCloses #1251",
        "parent": "https://github.com/apache/flink/commit/2475c82d425b4a6a564f0d38f352a0f3b8753d72",
        "repo": "flink",
        "unit_tests": [
            "FileMonitoringFunctionTest.java"
        ]
    },
    "flink_5f11df6": {
        "bug_id": "flink_5f11df6",
        "commit": "https://github.com/apache/flink/commit/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
                "deletions": 0,
                "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "patch": "@@ -35,6 +35,7 @@ private CliStrings() {\n \n \tpublic static final String CLI_NAME = \"Flink SQL CLI Client\";\n \tpublic static final String DEFAULT_MARGIN = \" \";\n+\tpublic static final String NULL_COLUMN = \"(NULL)\";\n \n \t// --------------------------------------------------------------------------------------------\n ",
                "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "sha": "1e8f696bdf26cb4826f9cbe6c96f8c15d279a209",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
                "deletions": 1,
                "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "patch": "@@ -93,7 +93,12 @@ public static void normalizeColumn(AttributedStringBuilder sb, String col, int m\n \tpublic static String[] rowToString(Row row) {\n \t\tfinal String[] fields = new String[row.getArity()];\n \t\tfor (int i = 0; i < row.getArity(); i++) {\n-\t\t\tfields[i] = row.getField(i).toString();\n+\t\t\tfinal Object field = row.getField(i);\n+\t\t\tif (field == null) {\n+\t\t\t\tfields[i] = CliStrings.NULL_COLUMN;\n+\t\t\t} else {\n+\t\t\t\tfields[i] = field.toString();\n+\t\t\t}\n \t\t}\n \t\treturn fields;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "sha": "77894e8b7e192dc37b305e43f2530c91bae7c034",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [sql-client] Fix NPE when column is null",
        "parent": "https://github.com/apache/flink/commit/e8e74a648a134fe054081a49a36b8c45f30c21bc",
        "repo": "flink",
        "unit_tests": [
            "CliUtilsTest.java"
        ]
    },
    "flink_6a54544": {
        "bug_id": "flink_6a54544",
        "commit": "https://github.com/apache/flink/commit/6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "patch": "@@ -82,4 +82,12 @@ public Ordering createNewOrderingUpToIndex(int exclusiveIndex) {\n \t\t}\n \t\treturn newOrdering;\n \t}\n+\t\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic Ordering clone() {\n+\t\tOrdering newOrdering = new Ordering();\n+\t\tnewOrdering.indexes = (ArrayList<Integer>) this.indexes.clone();\n+\t\tnewOrdering.orders = (ArrayList<Order>) this.orders.clone();\n+\t\treturn this;\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "sha": "a357491a6c09f53b61a16b7d11040c4d85c6399e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 7,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "patch": "@@ -236,15 +236,13 @@ public boolean isMetBy(GlobalProperties other) {\n \t\t\tif (other.partitionedFields == null) {\n \t\t\t\treturn false;\n \t\t\t}\n-\t\t\tif (this.partitionedFields.size() > otherPartitionedFields.size()) {\n+\t\t\tif (this.partitionedFields.size() < otherPartitionedFields.size()) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\t\n-\t\t\tfor (Integer fieldIndex : this.partitionedFields) {\n-\t\t\t\tif (otherPartitionedFields.contains(fieldIndex) == false) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\t\n+\t\t\tif (this.partitionedFields.containsAll(otherPartitionedFields) == false) {\n+\t\t\t\treturn false;\n+\t\t\t}\n \t\t}\n \t\t\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -322,7 +320,14 @@ public String toString() {\n \t * @see java.lang.Object#clone()\n \t */\n \tpublic GlobalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (GlobalProperties) super.clone();\n+\t\tGlobalProperties newProps = (GlobalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.partitionedFields != null) {\n+\t\t\tnewProps.partitionedFields = (FieldSet) this.partitionedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "sha": "d18269622a12284567778195fd65a098c26de900",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 5,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "patch": "@@ -223,11 +223,12 @@ public boolean isMetBy(LocalProperties other) {\n \t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\tgroupingFulfilled = true;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif (groupingFulfilled == false) {\n+\t\t\t\treturn false;\n \t\t\t}\n-\t\t}\n-\n-\t\tif (groupingFulfilled == false) {\n-\t\t\treturn false;\n \t\t}\n \t\t// check the order\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -296,7 +297,14 @@ public String toString() {\n \t */\n \t@Override\n \tpublic LocalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (LocalProperties) super.clone();\n+\t\tLocalProperties newProps = (LocalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.groupedFields != null) {\n+\t\t\tnewProps.groupedFields = (FieldSet) this.groupedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "sha": "8f8b5b2cd2bc6137d804e1c90537b1fe86e99a24",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 5,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "patch": "@@ -48,8 +48,8 @@\n \t * properties and a maximal cost of infinite.\n \t */\n \tpublic InterestingProperties() {\n-\t\t// instantiate the maximal costs to the possible maximum\n-\t\tthis.maximalCosts = new Costs(Long.MAX_VALUE, Long.MAX_VALUE);\n+\t\t// instantiate the maximal costs to 0\n+\t\tthis.maximalCosts = new Costs(0, 0);\n \n \t\tthis.globalProps = new GlobalProperties();\n \t\tthis.localProps = new LocalProperties();\n@@ -314,11 +314,18 @@ public static void mergeUnionOfInterestingProperties(List<InterestingProperties>\n \t\tList<InterestingProperties> preserved = new ArrayList<InterestingProperties>();\n \t\t\n \t\tfor (InterestingProperties p : props) {\n-\t\t\tboolean nonTrivial = p.getGlobalProperties().filterByNodesConstantSet(node, input);\n-\t\t\tnonTrivial |= p.getLocalProperties().filterByNodesConstantSet(node, input);\n+\t\t\tGlobalProperties preservedGp = p.getGlobalProperties().createCopy();\n+\t\t\tLocalProperties preservedLp = p.getLocalProperties().createCopy();\n+\t\t\tboolean nonTrivial = preservedGp.filterByNodesConstantSet(node, input);\n+\t\t\tnonTrivial |= preservedLp.filterByNodesConstantSet(node, input);\n \n \t\t\tif (nonTrivial) {\n-\t\t\t\tpreserved.add(p);\n+\t\t\t\ttry {\n+\t\t\t\t\tpreserved.add(new InterestingProperties(p.getMaximalCosts().clone(), preservedGp, preservedLp));\n+\t\t\t\t} catch (CloneNotSupportedException cnse) {\n+\t\t\t\t\t// should never happen, but propagate just in case\n+\t\t\t\t\tthrow new RuntimeException(cnse);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "sha": "062d470cd8365a2c3edb434b7cd976e86bbbe42d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "patch": "@@ -476,6 +476,10 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:\n \t\t\treturn (constantSet != null && Arrays.binarySearch(constantSet, fieldNumber) >= 0);",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "sha": "ecff08f1e0759b3e391a25689d02aa46442d4146",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "patch": "@@ -858,6 +858,9 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n \t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "sha": "db8216a0d9bc92c12877d5a40b5d8ffbdfc7d75a",
                "status": "modified"
            }
        ],
        "message": "Fixed small bugs in Compiler (NPE, Maximal Costs of IP, Cloning of IPs)",
        "parent": "https://github.com/apache/flink/commit/dc1954428c82c85bb6a731edc7c68c248ce1fc98",
        "repo": "flink",
        "unit_tests": [
            "OrderingTest.java"
        ]
    },
    "flink_705938e": {
        "bug_id": "flink_705938e",
        "commit": "https://github.com/apache/flink/commit/705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
                "deletions": 21,
                "filename": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "patch": "@@ -17,48 +17,58 @@\n  */\n package org.apache.flink.api.java.io.jdbc.split;\n \n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n import java.io.Serializable;\n \n /** \n  * \n- * This query generator assumes that the query to parameterize contains a BETWEEN constraint on a numeric column.\n- * The generated query set will be of size equal to the configured fetchSize (apart the last one range),\n- * ranging from the min value up to the max.\n+ * This query parameters generator is an helper class to parameterize from/to queries on a numeric column.\n+ * The generated array of from/to values will be equally sized to fetchSize (apart from the last one),\n+ * ranging from minVal up to maxVal.\n  * \n  * For example, if there's a table <CODE>BOOKS</CODE> with a numeric PK <CODE>id</CODE>, using a query like:\n  * <PRE>\n  *   SELECT * FROM BOOKS WHERE id BETWEEN ? AND ?\n  * </PRE>\n  *\n- * you can use this class to automatically generate the parameters of the BETWEEN clause,\n+ * you can take advantage of this class to automatically generate the parameters of the BETWEEN clause,\n  * based on the passed constructor parameters.\n  * \n  * */\n public class NumericBetweenParametersProvider implements ParameterValuesProvider {\n \n-\tprivate long fetchSize;\n-\tprivate final long min;\n-\tprivate final long max;\n+\tprivate final long fetchSize;\n+\tprivate final long minVal;\n+\tprivate final long maxVal;\n \t\n-\tpublic NumericBetweenParametersProvider(long fetchSize, long min, long max) {\n+\t/**\n+\t * NumericBetweenParametersProvider constructor.\n+\t * \n+\t * @param fetchSize the max distance between the produced from/to pairs\n+\t * @param minVal the lower bound of the produced \"from\" values\n+\t * @param maxVal the upper bound of the produced \"to\" values\n+\t */\n+\tpublic NumericBetweenParametersProvider(long fetchSize, long minVal, long maxVal) {\n+\t\tcheckArgument(fetchSize > 0, \"Fetch size must be greater than 0.\");\n+\t\tcheckArgument(minVal <= maxVal, \"Min value cannot be greater than max value.\");\n \t\tthis.fetchSize = fetchSize;\n-\t\tthis.min = min;\n-\t\tthis.max = max;\n+\t\tthis.minVal = minVal;\n+\t\tthis.maxVal = maxVal;\n \t}\n \n \t@Override\n-\tpublic Serializable[][] getParameterValues(){\n-\t\tdouble maxElemCount = (max - min) + 1;\n-\t\tint size = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n-\t\tSerializable[][] parameters = new Serializable[size][2];\n-\t\tint count = 0;\n-\t\tfor (long i = min; i < max; i += fetchSize, count++) {\n-\t\t\tlong currentLimit = i + fetchSize - 1;\n-\t\t\tparameters[count] = new Long[]{i,currentLimit};\n-\t\t\tif (currentLimit + 1 + fetchSize > max) {\n-\t\t\t\tparameters[count + 1] = new Long[]{currentLimit + 1, max};\n-\t\t\t\tbreak;\n+\tpublic Serializable[][] getParameterValues() {\n+\t\tdouble maxElemCount = (maxVal - minVal) + 1;\n+\t\tint numBatches = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n+\t\tSerializable[][] parameters = new Serializable[numBatches][2];\n+\t\tint batchIndex = 0;\n+\t\tfor (long start = minVal; start <= maxVal; start += fetchSize, batchIndex++) {\n+\t\t\tlong end = start + fetchSize - 1;\n+\t\t\tif (end > maxVal) {\n+\t\t\t\tend = maxVal;\n \t\t\t}\n+\t\t\tparameters[batchIndex] = new Long[]{start, end};\n \t\t}\n \t\treturn parameters;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "sha": "44201723a44b114452a7ee1dc5f6ad2bef50e12f",
                "status": "modified"
            },
            {
                "additions": 107,
                "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "changes": 127,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
                "deletions": 20,
                "filename": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "patch": "@@ -18,19 +18,19 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import java.io.IOException;\n-import java.io.Serializable;\n-import java.sql.ResultSet;\n-\n import org.apache.flink.api.java.io.jdbc.split.GenericParameterValuesProvider;\n import org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider;\n import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;\n-import org.apache.flink.types.Row;\n import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.types.Row;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.sql.ResultSet;\n+\n public class JDBCInputFormatTest extends JDBCTestBase {\n \n \tprivate JDBCInputFormat jdbcInputFormat;\n@@ -116,11 +116,21 @@ public void testJDBCInputFormatWithoutParallelism() throws IOException, Instanti\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\t\n-\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\tif (next.getField(0) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(1) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(2) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(3) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(4) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t}\n \n \t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -162,11 +172,78 @@ public void testJDBCInputFormatWithParallelismAndNumericColumnSplitting() throws\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n+\n+\t\t\t\tfor (int x = 0; x < 5; x++) {\n+\t\t\t\t\tif(testData[recordCount][x]!=null) {\n+\t\t\t\t\t\tAssert.assertEquals(testData[recordCount][x], next.getField(x));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\trecordCount++;\n+\t\t\t}\n+\t\t\tjdbcInputFormat.close();\n+\t\t}\n+\t\tjdbcInputFormat.closeInputFormat();\n+\t\tAssert.assertEquals(testData.length, recordCount);\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCInputFormatWithoutParallelismAndNumericColumnSplitting() throws IOException, InstantiationException, IllegalAccessException {\n+\t\tfinal Long min = new Long(JDBCTestBase.testData[0][0] + \"\");\n+\t\tfinal Long max = new Long(JDBCTestBase.testData[JDBCTestBase.testData.length - 1][0] + \"\");\n+\t\tfinal long fetchSize = max + 1;//generate a single split\n+\t\tParameterValuesProvider pramProvider = new NumericBetweenParametersProvider(fetchSize, min, max);\n+\t\tjdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n+\t\t\t\t.setDrivername(DRIVER_CLASS)\n+\t\t\t\t.setDBUrl(DB_URL)\n+\t\t\t\t.setQuery(JDBCTestBase.SELECT_ALL_BOOKS_SPLIT_BY_ID)\n+\t\t\t\t.setRowTypeInfo(rowTypeInfo)\n+\t\t\t\t.setParametersProvider(pramProvider)\n+\t\t\t\t.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)\n+\t\t\t\t.finish();\n+\n+\t\tjdbcInputFormat.openInputFormat();\n+\t\tInputSplit[] splits = jdbcInputFormat.createInputSplits(1);\n+\t\t//assert that a single split was generated\n+\t\tAssert.assertEquals(1, splits.length);\n+\t\tint recordCount = 0;\n+\t\tRow row =  new Row(5);\n+\t\tfor (int i = 0; i < splits.length; i++) {\n+\t\t\tjdbcInputFormat.open(splits[i]);\n+\t\t\twhile (!jdbcInputFormat.reachedEnd()) {\n+\t\t\t\tRow next = jdbcInputFormat.nextRecord(row);\n+\t\t\t\tif (next == null) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -208,11 +285,21 @@ public void testJDBCInputFormatWithParallelismAndGenericSplitting() throws IOExc\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\trecordCount++;\n \t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "sha": "bee3d251a986f81ffa2f0ba582f8ca08677daf65",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6271] [jdbc] Fix NPE when there's a single split\n\nThis closes #3686.",
        "parent": "https://github.com/apache/flink/commit/c96002cef5f1867573a473746241f86b59aeddd2",
        "repo": "flink",
        "unit_tests": [
            "NumericBetweenParametersProviderTest.java"
        ]
    },
    "flink_74b09ce": {
        "bug_id": "flink_74b09ce",
        "commit": "https://github.com/apache/flink/commit/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 12,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink;\n-import org.apache.flink.types.IntValue;\n \n import java.util.UUID;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -97,7 +96,7 @@ public void close() throws Exception {\n \n \t@Override\n \tprotected boolean sendValues(Iterable<IN> values, long timestamp) throws Exception {\n-\t\tfinal IntValue updatesCount = new IntValue(0);\n+\t\tfinal AtomicInteger updatesCount = new AtomicInteger(0);\n \t\tfinal AtomicInteger updatesConfirmed = new AtomicInteger(0);\n \n \t\tfinal AtomicReference<Throwable> exception = new AtomicReference<>();\n@@ -106,8 +105,8 @@ protected boolean sendValues(Iterable<IN> values, long timestamp) throws Excepti\n \t\t\t@Override\n \t\t\tpublic void onSuccess(ResultSet resultSet) {\n \t\t\t\tupdatesConfirmed.incrementAndGet();\n-\t\t\t\tif (updatesCount.getValue() > 0) { // only set if all updates have been sent\n-\t\t\t\t\tif (updatesCount.getValue() == updatesConfirmed.get()) {\n+\t\t\t\tif (updatesCount.get() > 0) { // only set if all updates have been sent\n+\t\t\t\t\tif (updatesCount.get() == updatesConfirmed.get()) {\n \t\t\t\t\t\tsynchronized (updatesConfirmed) {\n \t\t\t\t\t\t\tupdatesConfirmed.notifyAll();\n \t\t\t\t\t\t}\n@@ -142,18 +141,19 @@ public void onFailure(Throwable throwable) {\n \t\t\t\tFutures.addCallback(result, callback);\n \t\t\t}\n \t\t}\n-\t\tupdatesCount.setValue(updatesSent);\n+\t\tupdatesCount.set(updatesSent);\n \n \t\tsynchronized (updatesConfirmed) {\n-\t\t\twhile (updatesSent != updatesConfirmed.get()) {\n-\t\t\t\tif (exception.get() != null) { // verify that no query failed until now\n-\t\t\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\twhile (exception.get() == null && updatesSent != updatesConfirmed.get()) {\n \t\t\t\tupdatesConfirmed.wait();\n \t\t\t}\n \t\t}\n-\t\tboolean success = updatesSent == updatesConfirmed.get();\n-\t\treturn success;\n+\n+\t\tif (exception.get() != null) {\n+\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\treturn true;\n+\t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "sha": "192843107ef3546b6a5896b1dd1a2564bd8529a8",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 70,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "patch": "@@ -25,39 +25,33 @@\n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.java.tuple.Tuple0;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n-import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n-import org.apache.flink.util.IterableIterator;\n-import org.junit.Assert;\n import org.junit.Test;\n-import org.junit.runner.RunWith;\n import org.mockito.Matchers;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n-import org.powermock.core.classloader.annotations.PowerMockIgnore;\n-import org.powermock.core.classloader.annotations.PrepareForTest;\n-import org.powermock.modules.junit4.PowerMockRunner;\n \n-import java.util.Iterator;\n+import java.util.Collections;\n import java.util.concurrent.Executor;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.junit.Assert.assertFalse;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyString;\n import static org.powermock.api.mockito.PowerMockito.doAnswer;\n import static org.powermock.api.mockito.PowerMockito.mock;\n import static org.powermock.api.mockito.PowerMockito.when;\n \n-@RunWith(PowerMockRunner.class)\n-@PrepareForTest({ResultPartitionWriter.class, CassandraTupleWriteAheadSink.class})\n-@PowerMockIgnore({\"javax.management.*\", \"com.sun.jndi.*\"})\n-public class CassandraConnectorUnitTest {\n-\t@Test\n+public class CassandraTupleWriteAheadSinkTest {\n+\n+\t@Test(timeout=20000)\n \tpublic void testAckLoopExitOnException() throws Exception {\n-\t\tfinal AtomicReference<Runnable> callback = new AtomicReference<>();\n+\t\tfinal AtomicReference<Runnable> runnableFuture = new AtomicReference<>();\n \n \t\tfinal ClusterBuilder clusterBuilder = new ClusterBuilder() {\n+\t\t\tprivate static final long serialVersionUID = 4624400760492936756L;\n+\n \t\t\t@Override\n \t\t\tprotected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\ttry {\n@@ -73,7 +67,10 @@ protected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\t\tdoAnswer(new Answer<Void>() {\n \t\t\t\t\t\t@Override\n \t\t\t\t\t\tpublic Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\t\t\tcallback.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\t\t\t\trunnableFuture.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\t\trunnableFuture.notifyAll();\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t}\n \t\t\t\t\t}).when(future).addListener(any(Runnable.class), any(Executor.class));\n@@ -91,68 +88,40 @@ public Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n \t\t\t}\n \t\t};\n \n-\t\tfinal IterableIterator<Tuple0> iter = new IterableIterator<Tuple0>() {\n-\t\t\tprivate boolean exhausted = false;\n-\n-\t\t\t@Override\n-\t\t\tpublic boolean hasNext() {\n-\t\t\t\treturn !exhausted;\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic Tuple0 next() {\n-\t\t\t\texhausted = true;\n-\t\t\t\treturn new Tuple0();\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic void remove() {\n-\t\t\t}\n-\n+\t\t// Our asynchronous executor thread\n+\t\tnew Thread(new Runnable() {\n \t\t\t@Override\n-\t\t\tpublic Iterator<Tuple0> iterator() {\n-\t\t\t\treturn this;\n-\t\t\t}\n-\t\t};\n-\n-\t\tfinal AtomicReference<Boolean> exceptionCaught = new AtomicReference<>();\n-\n-\t\tThread t = new Thread() {\n \t\t\tpublic void run() {\n-\t\t\t\ttry {\n-\t\t\t\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n-\t\t\t\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n-\t\t\t\t\t\t\"abc\",\n-\t\t\t\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n-\t\t\t\t\t\tclusterBuilder,\n-\t\t\t\t\t\tcc\n-\t\t\t\t\t);\n-\n-\t\t\t\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n-\t\t\t\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n-\n-\t\t\t\t\tharness.setup();\n-\t\t\t\t\tsink.open();\n-\t\t\t\t\tboolean result = sink.sendValues(iter, 0L);\n-\t\t\t\t\tsink.close();\n-\t\t\t\t\texceptionCaught.set(result == false);\n-\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tthrow new RuntimeException(e);\n+\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\twhile (runnableFuture.get() == null) {\n+\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\trunnableFuture.wait();\n+\t\t\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\t\t\t// ignore interrupts\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\trunnableFuture.get().run();\n \t\t\t}\n-\t\t};\n-\t\tt.start();\n+\t\t}).start();\n+\n+\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n+\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n+\t\t\t\"abc\",\n+\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n+\t\t\tclusterBuilder,\n+\t\t\tcc\n+\t\t);\n \n-\t\tint count = 0;\n-\t\twhile (t.getState() != Thread.State.WAITING && count < 100) { // 10 second timeout 10 * 10 * 100ms\n-\t\t\tThread.sleep(100);\n-\t\t\tcount++;\n-\t\t}\n+\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n+\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n \n-\t\tcallback.get().run();\n+\t\tharness.setup();\n+\t\tsink.open();\n \n-\t\tt.join();\n+\t\t// we should leave the loop and return false since we've seen an exception\n+\t\tassertFalse(sink.sendValues(Collections.singleton(new Tuple0()), 0L));\n \n-\t\tAssert.assertTrue(exceptionCaught.get());\n+\t\tsink.close();\n \t}\n }",
                "previous_filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraConnectorUnitTest.java",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "sha": "847d1a049e576a6f650fe84192e9719e55d1c966",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 0,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "patch": "@@ -190,6 +190,9 @@ public void processWatermark(Watermark mark) throws Exception {\n \t * used since the last completed checkpoint.\n \t **/\n \tpublic static class ExactlyOnceState implements StateHandle<Serializable> {\n+\n+\t\tprivate static final long serialVersionUID = -3571063495273460743L;\n+\n \t\tprotected TreeMap<Long, Tuple2<Long, StateHandle<DataInputView>>> pendingHandles;\n \n \t\tpublic ExactlyOnceState() {",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "sha": "5545717b24466fc5e1c6ae209c7323c9e551a8eb",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4123] [cassandra] Fix concurrency issue in CassandraTupleWriteAheadSink\n\nThe updatesCount variable in the CassandraTupleWriteAheadSink.sendValues did not have\nguaranteed visibility. Thus, it was possible that the callback thread would read an\noutdated value for updatesCount, resulting in a deadlock. Replacing IntValue updatesCount\nwith AtomicInteger updatesCount fixes this issue.\n\nFurthermore, the PR hardens the CassandraTupleWriteAheadSinkTest which could have failed\nwith a NPE if the callback runnable was not set in time.",
        "parent": "https://github.com/apache/flink/commit/5c2da21f25741502dd8ca64ce9d314a1ebea1441",
        "repo": "flink",
        "unit_tests": [
            "CassandraTupleWriteAheadSinkTest.java",
            "GenericWriteAheadSinkTest.java"
        ]
    },
    "flink_9e139a7": {
        "bug_id": "flink_9e139a7",
        "commit": "https://github.com/apache/flink/commit/9e139a72ba45f2dd820bd3b9ecdf8428588666fd",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java?ref=9e139a72ba45f2dd820bd3b9ecdf8428588666fd",
                "deletions": 11,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "patch": "@@ -591,17 +591,20 @@ public void collect(StreamRecord<T> record) {\n \t\t\t\toperator.setKeyContextElement1(copy);\n \t\t\t\toperator.processElement(copy);\n \t\t\t} catch (ClassCastException e) {\n-\t\t\t\t// Enrich error message\n-\t\t\t\tClassCastException replace = new ClassCastException(\n-\t\t\t\t\tString.format(\n-\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n-\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n-\t\t\t\t\t\t\"but identical names are being used.\",\n-\t\t\t\t\t\te.getMessage(),\n-\t\t\t\t\t\toutputTag.getId()));\n-\n-\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n-\n+\t\t\t\tif (outputTag != null) {\n+\t\t\t\t\t// Enrich error message\n+\t\t\t\t\tClassCastException replace = new ClassCastException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n+\t\t\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n+\t\t\t\t\t\t\t\t\"but identical names are being used.\",\n+\t\t\t\t\t\t\te.getMessage(),\n+\t\t\t\t\t\t\toutputTag.getId()));\n+\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n+\t\t\t\t}\n \t\t\t} catch (Exception e) {\n \t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n \t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "sha": "f3c7293fe1b17e04a8057c56aa4c2af732ae68e5",
                "status": "modified"
            }
        ],
        "message": "[FLINK-8423] OperatorChain#pushToOperator catch block may fail with NPE\n\nThis closes #5447.",
        "parent": "https://github.com/apache/flink/commit/24c30878ed6f6ed1599a5ec23362055e0e88916f",
        "repo": "flink",
        "unit_tests": [
            "OperatorChainTest.java"
        ]
    },
    "flink_a0249d9": {
        "bug_id": "flink_a0249d9",
        "commit": "https://github.com/apache/flink/commit/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java?ref=a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3",
                "deletions": 2,
                "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "patch": "@@ -149,7 +149,7 @@\n \tprivate final KinesisProxyInterface kinesis;\n \n \t/** Thread that executed runFetcher() */\n-\tprivate Thread mainThread;\n+\tprivate volatile Thread mainThread;\n \n \t/**\n \t * The current number of shards that are actively read by this fetcher.\n@@ -408,7 +408,10 @@ public void runFetcher() throws Exception {\n \t */\n \tpublic void shutdownFetcher() {\n \t\trunning = false;\n-\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\n+\t\tif (mainThread != null) {\n+\t\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\t\t}\n \n \t\tif (LOG.isInfoEnabled()) {\n \t\t\tLOG.info(\"Shutting down the shard consumer threads of subtask {} ...\", indexOfThisConsumerSubtask);",
                "raw_url": "https://github.com/apache/flink/raw/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "sha": "8f7ca6c40f1e4871054374ae6b49797a665fdb15",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6311] [kinesis] NPE in FlinkKinesisConsumer if source was closed before run\n\nThis closes #3738.",
        "parent": "https://github.com/apache/flink/commit/42328bd9b7f216e4c3aae2086b822b4a3a564970",
        "repo": "flink",
        "unit_tests": [
            "KinesisDataFetcherTest.java"
        ]
    },
    "flink_a442eb6": {
        "bug_id": "flink_a442eb6",
        "commit": "https://github.com/apache/flink/commit/a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/high_availability_zookeeper_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 1,
                "filename": "docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "patch": "@@ -60,7 +60,7 @@\n         <tr>\n             <td><h5>high-availability.zookeeper.path.mesos-workers</h5></td>\n             <td style=\"word-wrap: break-word;\">\"/mesos-workers\"</td>\n-            <td>ZooKeeper root path (ZNode) for Mesos workers.</td>\n+            <td>The ZooKeeper root path for persisting the Mesos worker information.</td>\n         </tr>\n         <tr>\n             <td><h5>high-availability.zookeeper.path.root</h5></td>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "sha": "6577878674b3de994b64bd5f540ce14ba1e4d249",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "docs/_includes/generated/mesos_configuration.html",
                "patch": "@@ -15,17 +15,17 @@\n         <tr>\n             <td><h5>mesos.initial-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td>The initial workers to bring up when the master starts</td>\n+            <td>The initial workers to bring up when the master starts. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.master</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>The Mesos master URL. The value should be in one of the following forms: \"host:port\", \"zk://host1:port1,host2:port2,.../path\", \"zk://username:password@host1:port1,host2:port2,.../path\" or \"file:///path/to/file\"</td>\n+            <td>The Mesos master URL. The value should be in one of the following forms: <ul><li>host:port</li><li>zk://host1:port1,host2:port2,.../path</li><li>zk://username:password@host1:port1,host2:port2,.../path</li><li>file:///path/to/file</li></ul></td>\n         </tr>\n         <tr>\n             <td><h5>mesos.maximum-failed-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">-1</td>\n-            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature</td>\n+            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.artifactserver.port</h5></td>\n@@ -65,7 +65,7 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.port-assignments</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Comma-separated list of configuration keys which represent a configurable port.All port keys will dynamically get a port assigned through Mesos.</td>\n+            <td>Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.</td>\n         </tr>\n     </tbody>\n </table>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html",
                "sha": "54e92e5680c51e766cc07c1a976eb52aca573280",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_task_manager_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "docs/_includes/generated/mesos_task_manager_configuration.html",
                "patch": "@@ -10,12 +10,12 @@\n         <tr>\n             <td><h5>mesos.constraints.hard.hostattribute</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Constraints for task placement on mesos.</td>\n+            <td>Constraints for task placement on Mesos based on agent attributes. Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. Example: az:eu-west-1a,series:t2</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.bootstrap-cmd</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>A command which is executed before the TaskManager is started.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.container.docker.force-pull-image</h5></td>\n@@ -50,12 +50,12 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.gpus</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td></td>\n+            <td>GPUs to assign to the Mesos workers.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.hostname</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>Optional value to define the TaskManager\u2019s hostname. The pattern _TASK_ is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.mem</h5></td>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html",
                "sha": "1e67f8429d74665ac0033d38fd26a330f2c4a892",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/ops/deployment/mesos.md?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 69,
                "filename": "docs/ops/deployment/mesos.md",
                "patch": "@@ -59,13 +59,11 @@ or configuration files. For instance, in non-containerized environments, the\n artifact server will provide the Flink binaries. What files will be served\n depends on the configuration overlay used.\n \n-### Flink's JobManager and Web Interface\n+### Flink's Dispatcher and Web Interface\n \n-The Mesos scheduler currently resides with the JobManager but will be started\n-independently of the JobManager in future versions (see\n-[FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)). The\n-proposed changes will also add a Dispatcher component which will be the central\n-point for job submission and monitoring.\n+The Dispatcher and the web interface provide a central point for monitoring,\n+job submission, and other client interaction with the cluster\n+(see [FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)).\n \n ### Startup script and configuration overlays\n \n@@ -139,7 +137,7 @@ More information about the deployment scripts can be found [here](http://mesos.a\n \n ### Installing Marathon\n \n-Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which will be necessary to run Flink in high availability (HA) mode.\n+Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which enables you to run Flink in [high availability (HA) mode](#high-availability).\n \n ### Pre-installing Flink vs Docker/Mesos containers\n \n@@ -171,8 +169,6 @@ which manage the Flink processes in a Mesos cluster:\n    It is automatically launched by the Mesos worker node to bring up a new TaskManager.\n \n In order to run the `mesos-appmaster.sh` script you have to define `mesos.master` in the `flink-conf.yaml` or pass it via `-Dmesos.master=...` to the Java process.\n-Additionally, you should define the number of task managers which are started by Mesos via `mesos.initial-tasks`.\n-This value can also be defined in the `flink-conf.yaml` or passed as a Java property.\n \n When executing `mesos-appmaster.sh`, it will create a job manager on the machine where you executed the script.\n In contrast to that, the task managers will be run as Mesos tasks in the Mesos cluster.\n@@ -188,19 +184,21 @@ For example:\n         -Djobmanager.heap.mb=1024 \\\n         -Djobmanager.rpc.port=6123 \\\n         -Drest.port=8081 \\\n-        -Dmesos.initial-tasks=10 \\\n         -Dmesos.resourcemanager.tasks.mem=4096 \\\n         -Dtaskmanager.heap.mb=3500 \\\n         -Dtaskmanager.numberOfTaskSlots=2 \\\n         -Dparallelism.default=10\n \n+<div class=\"alert alert-info\">\n+  <strong>Note:</strong> If Flink is in <a href=\"{{ site.baseurl }}/ops/config.html#legacy\">legacy mode</a>,\n+  you should additionally define the number of task managers that are started by Mesos via\n+  <a href=\"{{ site.baseurl }}/ops/config.html#mesos-initial-tasks\"><code>mesos.initial-tasks</code></a>.\n+</div>\n \n ### High Availability\n \n You will need to run a service like Marathon or Apache Aurora which takes care of restarting the Flink master process in case of node or process failures.\n-In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html)\n-\n-For the reconciliation of tasks to work correctly, please also set `high-availability.zookeeper.path.mesos-workers` to a valid Zookeeper path.\n+In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html).\n \n #### Marathon\n \n@@ -211,7 +209,7 @@ Here is an example configuration for Marathon:\n \n     {\n         \"id\": \"flink\",\n-        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.initial-tasks=1 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n+        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n         \"cpus\": 1.0,\n         \"mem\": 1024\n     }\n@@ -220,60 +218,7 @@ When running Flink with Marathon, the whole Flink cluster including the job mana\n \n ### Configuration parameters\n \n-`mesos.initial-tasks`: The initial workers to bring up when the master starts (**DEFAULT**: The number of workers specified at cluster startup).\n-\n-`mesos.constraints.hard.hostattribute`: Constraints for task placement on Mesos based on agent attributes (**DEFAULT**: None).\n-Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target\n-mesos agents.  Example: `az:eu-west-1a,series:t2`\n-\n-`mesos.maximum-failed-tasks`: The maximum number of failed workers before the cluster fails (**DEFAULT**: Number of initial workers).\n-May be set to -1 to disable this feature.\n-\n-`mesos.master`: The Mesos master URL. The value should be in one of the following forms:\n-\n-* `host:port`\n-* `zk://host1:port1,host2:port2,.../path`\n-* `zk://username:password@host1:port1,host2:port2,.../path`\n-* `file:///path/to/file`\n-\n-`mesos.failover-timeout`: The failover timeout in seconds for the Mesos scheduler, after which running tasks are automatically shut down (**DEFAULT:** 600).\n-\n-`mesos.resourcemanager.artifactserver.port`:The config parameter defining the Mesos artifact server port to use. Setting the port to 0 will let the OS choose an available port.\n-\n-`mesos.resourcemanager.framework.name`: Mesos framework name (**DEFAULT:** Flink)\n-\n-`mesos.resourcemanager.framework.role`: Mesos framework role definition (**DEFAULT:** *)\n-\n-`high-availability.zookeeper.path.mesos-workers`: The ZooKeeper root path for persisting the Mesos worker information.\n-\n-`mesos.resourcemanager.framework.principal`: Mesos framework principal (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.secret`: Mesos framework secret (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.user`: Mesos framework user (**DEFAULT:**\"\")\n-\n-`mesos.resourcemanager.artifactserver.ssl.enabled`: Enables SSL for the Flink artifact server (**DEFAULT**: true). Note that `security.ssl.enabled` also needs to be set to `true` encryption to enable encryption.\n-\n-`mesos.resourcemanager.tasks.mem`: Memory to assign to the Mesos workers in MB (**DEFAULT**: 1024)\n-\n-`mesos.resourcemanager.tasks.cpus`: CPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.gpus`: GPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.container.type`: Type of the containerization used: \"mesos\" or \"docker\" (DEFAULT: mesos);\n-\n-`mesos.resourcemanager.tasks.container.image.name`: Image name to use for the container (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.volumes`: A comma separated list of `[host_path:]`container_path`[:RO|RW]`. This allows for mounting additional volumes into your container. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.parameters`: Custom parameters to be passed into docker run command when using the docker containerizer. Comma separated list of `key=value` pairs. `value` may contain '=' (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.uris`: A comma separated list of URIs of custom artifacts to be downloaded into the sandbox of Mesos workers. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.force-pull-image`: Instruct the docker containerizer to forcefully pull the image rather than reuse a cached version. (**DEFAULT**: false)\n-\n-`mesos.resourcemanager.tasks.hostname`: Optional value to define the TaskManager's hostname. The pattern `_TASK_` is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. `_TASK_.flink-service.mesos`) for name lookups. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.bootstrap-cmd`: A command which is executed before the TaskManager is started (**NO DEFAULT**).\n+For a list of Mesos specific configuration, refer to the [Mesos section]({{ site.baseurl }}/ops/config.html#mesos)\n+of the configuration documentation.\n \n {% top %}",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md",
                "sha": "1ff8afad74ebb5edd81836abe3e4e1d53b78e21c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 1,
                "filename": "flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.annotation.docs.ConfigGroup;\n import org.apache.flink.annotation.docs.ConfigGroups;\n import org.apache.flink.annotation.docs.Documentation;\n+import org.apache.flink.configuration.description.Description;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -157,7 +158,9 @@\n \t\t\tkey(\"high-availability.zookeeper.path.mesos-workers\")\n \t\t\t.defaultValue(\"/mesos-workers\")\n \t\t\t.withDeprecatedKeys(\"recovery.zookeeper.path.mesos-workers\")\n-\t\t\t.withDescription(\"ZooKeeper root path (ZNode) for Mesos workers.\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The ZooKeeper root path for persisting the Mesos worker information.\")\n+\t\t\t\t.build());\n \n \t// ------------------------------------------------------------------------\n \t//  ZooKeeper Client Settings",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "sha": "787efffa3ede1004219cfd4d5192cb75c4e7b692",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 8,
                "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "patch": "@@ -19,6 +19,9 @@\n package org.apache.flink.mesos.configuration;\n \n import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.description.Description;\n+import org.apache.flink.configuration.description.LinkElement;\n+import org.apache.flink.configuration.description.TextElement;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -33,7 +36,10 @@\n \tpublic static final ConfigOption<Integer> INITIAL_TASKS =\n \t\tkey(\"mesos.initial-tasks\")\n \t\t\t.defaultValue(0)\n-\t\t\t.withDescription(\"The initial workers to bring up when the master starts\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The initial workers to bring up when the master starts. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The maximum number of failed Mesos tasks before entirely stopping\n@@ -44,8 +50,10 @@\n \tpublic static final ConfigOption<Integer> MAX_FAILED_TASKS =\n \t\tkey(\"mesos.maximum-failed-tasks\")\n \t\t\t.defaultValue(-1)\n-\t\t\t.withDescription(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable\" +\n-\t\t\t\t\" this feature\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The Mesos master URL.\n@@ -63,9 +71,14 @@\n \tpublic static final ConfigOption<String> MASTER_URL =\n \t\tkey(\"mesos.master\")\n \t\t\t.noDefaultValue()\n-\t\t\t.withDescription(\"The Mesos master URL. The value should be in one of the following forms:\" +\n-\t\t\t\t\" \\\"host:port\\\", \\\"zk://host1:port1,host2:port2,.../path\\\",\" +\n-\t\t\t\t\" \\\"zk://username:password@host1:port1,host2:port2,.../path\\\" or \\\"file:///path/to/file\\\"\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The Mesos master URL. The value should be in one of the following forms: \")\n+\t\t\t\t.list(\n+\t\t\t\t\tTextElement.text(\"host:port\"),\n+\t\t\t\t\tTextElement.text(\"zk://host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"zk://username:password@host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"file:///path/to/file\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The failover timeout for the Mesos scheduler, after which running tasks are automatically shut down.\n@@ -125,7 +138,9 @@\n \t */\n \tpublic static final ConfigOption<String> PORT_ASSIGNMENTS = key(\"mesos.resourcemanager.tasks.port-assignments\")\n \t\t.defaultValue(\"\")\n-\t\t.withDescription(\"Comma-separated list of configuration keys which represent a configurable port.\" +\n-\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Comma-separated list of configuration keys which represent a configurable port. \" +\n+\t\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\")\n+\t\t\t.build());\n \n }",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "sha": "426a891e814237e410169e04c8210e2b4da9667b",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.IllegalConfigurationException;\n import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.configuration.description.Description;\n import org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters;\n import org.apache.flink.util.Preconditions;\n \n@@ -65,7 +66,8 @@\n \n \tpublic static final ConfigOption<Integer> MESOS_RM_TASKS_GPUS =\n \t\tkey(\"mesos.resourcemanager.tasks.gpus\")\n-\t\t.defaultValue(0);\n+\t\t.defaultValue(0)\n+\t\t.withDescription(Description.builder().text(\"GPUs to assign to the Mesos workers.\").build());\n \n \tpublic static final ConfigOption<String> MESOS_RM_CONTAINER_TYPE =\n \t\tkey(\"mesos.resourcemanager.tasks.container.type\")\n@@ -79,15 +81,23 @@\n \n \tpublic static final ConfigOption<String> MESOS_TM_HOSTNAME =\n \t\tkey(\"mesos.resourcemanager.tasks.hostname\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Optional value to define the TaskManager\u2019s hostname. \" +\n+\t\t\t\t\"The pattern _TASK_ is replaced by the actual id of the Mesos task. \" +\n+\t\t\t\t\"This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.taskmanager-cmd\")\n \t\t.defaultValue(\"$FLINK_HOME/bin/mesos-taskmanager.sh\"); // internal\n \n \tpublic static final ConfigOption<String> MESOS_TM_BOOTSTRAP_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.bootstrap-cmd\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"A command which is executed before the TaskManager is started.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_URIS =\n \t\tkey(\"mesos.resourcemanager.tasks.uris\")\n@@ -116,7 +126,11 @@\n \tpublic static final ConfigOption<String> MESOS_CONSTRAINTS_HARD_HOSTATTR =\n \t\tkey(\"mesos.constraints.hard.hostattribute\")\n \t\t.noDefaultValue()\n-\t\t.withDescription(\"Constraints for task placement on mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Constraints for task placement on Mesos based on agent attributes. \" +\n+\t\t\t\t\"Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. \" +\n+\t\t\t\t\"Example: az:eu-west-1a,series:t2\")\n+\t\t\t.build());\n \n \t/**\n \t * Value for {@code MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE} setting. Tells to use the Mesos containerizer.",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "sha": "03156297188617e6670e72900264658fe6ec78e4",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9795][mesos, docs] Update Mesos documentation\n\n[FLINK-9795][mesos, docs] Remove unnecessary remark about task reconciliation.\n\nThe config key high-availability.zookeeper.path.mesos-workers already has a\ndefault value. Even without explicitly setting the key, the task reconciliation\nwill work. Moreover, if there would not be a default key, the code would throw an NPE. So\neither way, the remark is only confusing the reader.\n\n[FLINK-9795][mesos, docs] Remove configuration keys from Mesos Setup page.\n\n- Remove the Mesos specific configuration keys from the Mesos Setup page because\nthey duplicate what is already on the configuration page.\n- Add missing descriptions for some of the keys that are under the Mesos section of the configuration\npage.\n- Improve formatting of the descriptions.\n\n[FLINK-9795][mesos, docs] Document which config options are only used in legacy mode.\n\n[FLINK-9795][mesos, docs] Document that mesos.initial-tasks is only needed in legacy mode.\n\n[FLINK-9795][mesos, docs] Clarify necessity of Marathon in documentation.\n\n[FLINK-9795][mesos, docs] Rewrite \"Flink's JobManager and Web Interface\" section.\n\n[FLINK-9795][mesos, docs] Add missing period at the end of sentence.\n\nThis closes #6533.",
        "parent": "https://github.com/apache/flink/commit/04ba9a85920bcab6c7b6c001a58c7570e987aabb",
        "repo": "flink",
        "unit_tests": [
            "MesosTaskManagerParametersTest.java"
        ]
    },
    "flink_a95ec5a": {
        "bug_id": "flink_a95ec5a",
        "commit": "https://github.com/apache/flink/commit/a95ec5acf259884347ae539913bcffcad5bfc340",
        "file": [
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.jobmaster;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerId;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * Class which contains the connection details of an established\n+ * connection with the ResourceManager.\n+ */\n+class EstablishedResourceManagerConnection {\n+\n+\tprivate final ResourceManagerGateway resourceManagerGateway;\n+\n+\tprivate final ResourceManagerId resourceManagerId;\n+\n+\tprivate final ResourceID resourceManagerResourceID;\n+\n+\tEstablishedResourceManagerConnection(\n+\t\t\t@Nonnull ResourceManagerGateway resourceManagerGateway,\n+\t\t\t@Nonnull ResourceManagerId resourceManagerId,\n+\t\t\t@Nonnull ResourceID resourceManagerResourceID) {\n+\t\tthis.resourceManagerGateway = resourceManagerGateway;\n+\t\tthis.resourceManagerId = resourceManagerId;\n+\t\tthis.resourceManagerResourceID = resourceManagerResourceID;\n+\t}\n+\n+\tpublic ResourceManagerGateway getResourceManagerGateway() {\n+\t\treturn resourceManagerGateway;\n+\t}\n+\n+\tpublic ResourceManagerId getResourceManagerId() {\n+\t\treturn resourceManagerId;\n+\t}\n+\n+\tpublic ResourceID getResourceManagerResourceID() {\n+\t\treturn resourceManagerResourceID;\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "sha": "46c1b4bde7a3c0458bf49b046648fb352a5d156e",
                "status": "added"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 35,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "patch": "@@ -191,9 +191,6 @@\n \n \tprivate LeaderRetrievalService resourceManagerLeaderRetriever;\n \n-\t@Nullable\n-\tprivate ResourceManagerConnection resourceManagerConnection;\n-\n \t// --------- TaskManagers --------\n \n \tprivate final Map<ResourceID, Tuple2<TaskManagerLocation, TaskExecutorGateway>> registeredTaskManagers;\n@@ -211,6 +208,12 @@\n \t@Nullable\n \tprivate String lastInternalSavepoint;\n \n+\t@Nullable\n+\tprivate ResourceManagerConnection resourceManagerConnection;\n+\n+\t@Nullable\n+\tprivate EstablishedResourceManagerConnection establishedResourceManagerConnection;\n+\n \t// ------------------------------------------------------------------------\n \n \tpublic JobMaster(\n@@ -290,6 +293,9 @@ public JobMaster(\n \t\tthis.jobManagerJobMetricGroup = jobMetricGroupFactory.create(jobGraph);\n \t\tthis.executionGraph = createAndRestoreExecutionGraph(jobManagerJobMetricGroup);\n \t\tthis.jobStatusListener = null;\n+\n+\t\tthis.resourceManagerConnection = null;\n+\t\tthis.establishedResourceManagerConnection = null;\n \t}\n \n \t//----------------------------------------------------------------------------------------------\n@@ -881,12 +887,16 @@ public void disconnectResourceManager(\n \t\t\tfinal ResourceManagerId resourceManagerId,\n \t\t\tfinal Exception cause) {\n \n-\t\tif (resourceManagerConnection != null\n-\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId)) {\n+\t\tif (isConnectingToResourceManager(resourceManagerId)) {\n \t\t\tcloseResourceManagerConnection(cause);\n \t\t}\n \t}\n \n+\tprivate boolean isConnectingToResourceManager(ResourceManagerId resourceManagerId) {\n+\t\treturn resourceManagerConnection != null\n+\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId);\n+\t}\n+\n \t@Override\n \tpublic void heartbeatFromTaskManager(final ResourceID resourceID, AccumulatorReport accumulatorReport) {\n \t\ttaskManagerHeartbeatManager.receiveHeartbeat(resourceID, accumulatorReport);\n@@ -1238,11 +1248,11 @@ private void notifyOfNewResourceManagerLeader(final String resourceManagerAddres\n \t\t\t\t\treturn;\n \t\t\t\t}\n \n-\t\t\t\tcloseResourceManagerConnection(new Exception(\n-\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n-\n \t\t\t\tlog.info(\"ResourceManager leader changed from {} to {}. Registering at new leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress(), resourceManagerAddress);\n+\n+\t\t\t\tcloseResourceManagerConnection(new Exception(\n+\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n \t\t\t} else {\n \t\t\t\tlog.info(\"Current ResourceManager {} lost leader status. Waiting for new ResourceManager leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress());\n@@ -1277,9 +1287,16 @@ private void establishResourceManagerConnection(final JobMasterRegistrationSucce\n \n \t\t\tfinal ResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n \n+\t\t\tfinal ResourceID resourceManagerResourceId = success.getResourceManagerResourceId();\n+\n+\t\t\testablishedResourceManagerConnection = new EstablishedResourceManagerConnection(\n+\t\t\t\tresourceManagerGateway,\n+\t\t\t\tsuccess.getResourceManagerId(),\n+\t\t\t\tresourceManagerResourceId);\n+\n \t\t\tslotPoolGateway.connectToResourceManager(resourceManagerGateway);\n \n-\t\t\tresourceManagerHeartbeatManager.monitorTarget(success.getResourceManagerResourceId(), new HeartbeatTarget<Void>() {\n+\t\t\tresourceManagerHeartbeatManager.monitorTarget(resourceManagerResourceId, new HeartbeatTarget<Void>() {\n \t\t\t\t@Override\n \t\t\t\tpublic void receiveHeartbeat(ResourceID resourceID, Void payload) {\n \t\t\t\t\tresourceManagerGateway.heartbeatFromJobManager(resourceID);\n@@ -1297,22 +1314,31 @@ public void requestHeartbeat(ResourceID resourceID, Void payload) {\n \t}\n \n \tprivate void closeResourceManagerConnection(Exception cause) {\n-\t\tif (resourceManagerConnection != null) {\n-\t\t\tif (log.isDebugEnabled()) {\n-\t\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause);\n-\t\t\t} else {\n-\t\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause.getMessage());\n-\t\t\t}\n-\n-\t\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerConnection.getResourceManagerResourceID());\n-\n-\t\t\tResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n-\t\t\tresourceManagerGateway.disconnectJobManager(resourceManagerConnection.getJobID(), cause);\n+\t\tif (establishedResourceManagerConnection != null) {\n+\t\t\tdissolveResourceManagerConnection(establishedResourceManagerConnection, cause);\n+\t\t\testablishedResourceManagerConnection = null;\n+\t\t}\n \n+\t\tif (resourceManagerConnection != null) {\n+\t\t\t// stop a potentially ongoing registration process\n \t\t\tresourceManagerConnection.close();\n \t\t\tresourceManagerConnection = null;\n \t\t}\n+\t}\n+\n+\tprivate void dissolveResourceManagerConnection(EstablishedResourceManagerConnection establishedResourceManagerConnection, Exception cause) {\n+\t\tfinal ResourceID resourceManagerResourceID = establishedResourceManagerConnection.getResourceManagerResourceID();\n \n+\t\tif (log.isDebugEnabled()) {\n+\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerResourceID, cause);\n+\t\t} else {\n+\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerResourceID, cause.getMessage());\n+\t\t}\n+\n+\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerResourceID);\n+\n+\t\tResourceManagerGateway resourceManagerGateway = establishedResourceManagerConnection.getResourceManagerGateway();\n+\t\tresourceManagerGateway.disconnectJobManager(jobGraph.getJobID(), cause);\n \t\tslotPoolGateway.disconnectResourceManager();\n \t}\n \n@@ -1473,8 +1499,6 @@ public void handleError(final Exception exception) {\n \n \t\tprivate final JobMasterId jobMasterId;\n \n-\t\tprivate ResourceID resourceManagerResourceID;\n-\n \t\tResourceManagerConnection(\n \t\t\t\tfinal Logger log,\n \t\t\t\tfinal JobID jobID,\n@@ -1498,7 +1522,7 @@ public void handleError(final Exception exception) {\n \t\t\t\t\tgetTargetAddress(), getTargetLeaderId()) {\n \t\t\t\t@Override\n \t\t\t\tprotected CompletableFuture<RegistrationResponse> invokeRegistration(\n-\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) throws Exception {\n+\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) {\n \t\t\t\t\tTime timeout = Time.milliseconds(timeoutMillis);\n \n \t\t\t\t\treturn gateway.registerJobManager(\n@@ -1513,24 +1537,13 @@ public void handleError(final Exception exception) {\n \n \t\t@Override\n \t\tprotected void onRegistrationSuccess(final JobMasterRegistrationSuccess success) {\n-\t\t\trunAsync(() -> {\n-\t\t\t\tresourceManagerResourceID = success.getResourceManagerResourceId();\n-\t\t\t\testablishResourceManagerConnection(success);\n-\t\t\t});\n+\t\t\trunAsync(() -> establishResourceManagerConnection(success));\n \t\t}\n \n \t\t@Override\n \t\tprotected void onRegistrationFailure(final Throwable failure) {\n \t\t\thandleJobMasterError(failure);\n \t\t}\n-\n-\t\tpublic ResourceID getResourceManagerResourceID() {\n-\t\t\treturn resourceManagerResourceID;\n-\t\t}\n-\n-\t\tpublic JobID getJobID() {\n-\t\t\treturn jobID;\n-\t\t}\n \t}\n \n \t//----------------------------------------------------------------------------------------------",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "sha": "aff32808dd99070afac0e475efd0381734f40a6a",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.configuration.BlobServerOptions;\n import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.testutils.OneShotLatch;\n import org.apache.flink.runtime.blob.BlobServer;\n import org.apache.flink.runtime.blob.VoidBlobStore;\n import org.apache.flink.runtime.checkpoint.CheckpointProperties;\n@@ -361,6 +362,53 @@ public void testCheckpointPrecedesSavepointRecovery() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Tests that we can close an unestablished ResourceManager connection.\n+\t */\n+\t@Test\n+\tpublic void testCloseUnestablishedResourceManagerConnection() throws Exception {\n+\t\tfinal JobMaster jobMaster = createJobMaster(\n+\t\t\tJobMasterConfiguration.fromConfiguration(configuration),\n+\t\t\tjobGraph,\n+\t\t\thaServices,\n+\t\t\tnew TestingJobManagerSharedServicesBuilder().build());\n+\n+\t\ttry {\n+\t\t\tjobMaster.start(JobMasterId.generate(), testingTimeout).get();\n+\t\t\tfinal ResourceManagerId resourceManagerId = ResourceManagerId.generate();\n+\t\t\tfinal String firstResourceManagerAddress = \"address1\";\n+\t\t\tfinal String secondResourceManagerAddress = \"address2\";\n+\n+\t\t\tfinal TestingResourceManagerGateway firstResourceManagerGateway = new TestingResourceManagerGateway();\n+\t\t\tfinal TestingResourceManagerGateway secondResourceManagerGateway = new TestingResourceManagerGateway();\n+\n+\t\t\trpcService.registerGateway(firstResourceManagerAddress, firstResourceManagerGateway);\n+\t\t\trpcService.registerGateway(secondResourceManagerAddress, secondResourceManagerGateway);\n+\n+\t\t\tfinal OneShotLatch firstJobManagerRegistration = new OneShotLatch();\n+\t\t\tfinal OneShotLatch secondJobManagerRegistration = new OneShotLatch();\n+\n+\t\t\tfirstResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> firstJobManagerRegistration.trigger());\n+\n+\t\t\tsecondResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> secondJobManagerRegistration.trigger());\n+\n+\t\t\trmLeaderRetrievalService.notifyListener(firstResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// wait until we have seen the first registration attempt\n+\t\t\tfirstJobManagerRegistration.await();\n+\n+\t\t\t// this should stop the connection attempts towards the first RM\n+\t\t\trmLeaderRetrievalService.notifyListener(secondResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// check that we start registering at the second RM\n+\t\t\tsecondJobManagerRegistration.await();\n+\t\t} finally {\n+\t\t\tRpcUtils.terminateRpcEndpoint(jobMaster, testingTimeout);\n+\t\t}\n+\t}\n+\n \tprivate File createSavepoint(long savepointId) throws IOException {\n \t\tfinal File savepointFile = temporaryFolder.newFile();\n \t\tfinal SavepointV2 savepoint = new SavepointV2(savepointId, Collections.emptyList(), Collections.emptyList());",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "sha": "c0c916246637c173a961ee808831cec32640bbdb",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9358] Avoid NPE when closing an unestablished ResourceManager connection\n\nA NPE occurred when trying to disconnect an unestablished ResourceManager connection.\nIn order to fix this problem, we now check whether the connection has been established\nor not.\n\nThis closes #6011.",
        "parent": "https://github.com/apache/flink/commit/f4e03689dd5fef8eafeb0996a31ea021c5ea2203",
        "repo": "flink",
        "unit_tests": [
            "JobMasterTest.java"
        ]
    },
    "flink_ad34540": {
        "bug_id": "flink_ad34540",
        "commit": "https://github.com/apache/flink/commit/ad3454069fa091cd453f6cefb0e56a8021a3c269",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269",
                "deletions": 3,
                "filename": "flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.flink.core.memory.DataInputView;\n import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.util.Preconditions;\n \n import java.io.IOException;\n import java.util.HashSet;\n@@ -56,7 +57,7 @@ public DelegatingConfiguration() {\n \t */\n \tpublic DelegatingConfiguration(Configuration backingConfig, String prefix)\n \t{\n-\t\tthis.backingConfig = backingConfig;\n+\t\tthis.backingConfig = Preconditions.checkNotNull(backingConfig);\n \t\tthis.prefix = prefix;\n \t}\n \n@@ -178,14 +179,19 @@ public String toString() {\n \n \t@Override\n \tpublic Set<String> keySet() {\n+\t\tif (this.prefix == null) {\n+\t\t\treturn this.backingConfig.keySet();\n+\t\t}\n+\n \t\tfinal HashSet<String> set = new HashSet<String>();\n-\t\tfinal int prefixLen = this.prefix == null ? 0 : this.prefix.length();\n+\t\tint prefixLen = this.prefix.length();\n \n \t\tfor (String key : this.backingConfig.keySet()) {\n-\t\t\tif (key.startsWith(this.prefix)) {\n+\t\t\tif (key.startsWith(prefix)) {\n \t\t\t\tset.add(key.substring(prefixLen));\n \t\t\t}\n \t\t}\n+\n \t\treturn set;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "sha": "dba77f372326db7477c6c656e9aa2645d7fe0929",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269",
                "deletions": 0,
                "filename": "flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "patch": "@@ -26,8 +26,10 @@\n import java.lang.reflect.Modifier;\n import java.util.Arrays;\n import java.util.Comparator;\n+import java.util.Set;\n \n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n \n \n public class DelegatingConfigurationTest {\n@@ -88,4 +90,47 @@ private String typeParamToString(Class<?>[] classes) {\n \t\t\tassertTrue(\"Foo method '\" + configurationMethod.getName() + \"' has not been wrapped correctly in DelegatingConfiguration wrapper\", hasMethod);\n \t\t}\n \t}\n+\t\n+\t@Test\n+\tpublic void testDelegationConfigurationWithNullPrefix() {\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(\n+\t\t\t\tbackingConf, null);\n+\t\tSet<String> keySet = configuration.keySet();\n+\n+\t\tassertEquals(keySet, backingConf.keySet());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void testDelegationConfigurationWithPrefix() {\n+\t\tString prefix = \"pref-\";\n+\t\tString expectedKey = \"key\";\n+\n+\t\t/*\n+\t\t * Key matches the prefix\n+\t\t */\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(prefix + expectedKey, \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tSet<String> keySet = configuration.keySet();\n+\t\t\n+\n+\t\tassertEquals(keySet.size(), 1);\n+\t\tassertEquals(keySet.iterator().next(), expectedKey);\n+\n+\t\t/*\n+\t\t * Key does not match the prefix\n+\t\t */\n+\t\tbackingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tconfiguration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tkeySet = configuration.keySet();\n+\n+\t\tassertTrue(keySet.isEmpty());\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "sha": "d8b782d4dbaf1c0d2d523deba0f1b73025bf0e27",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4309] Fix potential NPE in DelegatingConfiguration\n\nThis closes #2371.",
        "parent": "https://github.com/apache/flink/commit/5ccd9071580e196d150905b2d05eef71e399a24c",
        "repo": "flink",
        "unit_tests": [
            "DelegatingConfigurationTest.java"
        ]
    },
    "flink_b2c592a": {
        "bug_id": "flink_b2c592a",
        "commit": "https://github.com/apache/flink/commit/b2c592a87139c587777f9897613943639fac1d61",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java?ref=b2c592a87139c587777f9897613943639fac1d61",
                "deletions": 1,
                "filename": "flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "patch": "@@ -427,7 +427,7 @@ public void afterBulk(long executionId, BulkRequest request, BulkResponse respon\n \n \t\t@Override\n \t\tpublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n-\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure.getCause());\n+\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure);\n \n \t\t\ttry {\n \t\t\t\tfor (ActionRequest action : request.requests()) {",
                "raw_url": "https://github.com/apache/flink/raw/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "sha": "96f4431493c9f6348f8675439391558c242360d7",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [connectors] Fix shadowed NPE in elasticsearch sink connector\n\nThis closes #8849.",
        "parent": "https://github.com/apache/flink/commit/861bf73ada01e4d2d8c671e507974e5bfacd9218",
        "repo": "flink",
        "unit_tests": [
            "ElasticsearchSinkBaseTest.java"
        ]
    },
    "flink_bcead3b": {
        "bug_id": "flink_bcead3b",
        "commit": "https://github.com/apache/flink/commit/bcead3be32c624008730555d828fd8e9447fbeff",
        "file": [
            {
                "additions": 1402,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "changes": 1402,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "patch": "@@ -0,0 +1,1402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.MapTypeInfo;\n+import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.apache.orc.TypeDescription;\n+\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TimeZone;\n+import java.util.function.DoubleFunction;\n+import java.util.function.Function;\n+import java.util.function.LongFunction;\n+\n+/**\n+ * A class that provides utility methods for orc file reading.\n+ */\n+class OrcBatchReader {\n+\n+\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n+\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n+\n+\t/**\n+\t * Converts an ORC schema to a Flink TypeInformation.\n+\t *\n+\t * @param schema The ORC schema.\n+\t * @return The TypeInformation that corresponds to the ORC schema.\n+\t */\n+\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n+\t\t\tcase INT:\n+\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n+\t\t\tcase LONG:\n+\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n+\t\t\tcase STRING:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n+\t\t\tcase DATE:\n+\t\t\t\treturn SqlTimeTypeInfo.DATE;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n+\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n+\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n+\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n+\t\t\t\t}\n+\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n+\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n+\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n+\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n+\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n+\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n+\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n+\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Fills an ORC batch into an array of Row.\n+\t *\n+\t * @param rows The batch of rows need to be filled.\n+\t * @param schema The schema of the ORC data.\n+\t * @param batch The ORC data.\n+\t * @param selectedFields The list of selected ORC fields.\n+\t * @return The number of rows that were filled.\n+\t */\n+\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n+\n+\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n+\n+\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t// read each selected field\n+\t\tfor (int fieldIdx = 0; fieldIdx < selectedFields.length; fieldIdx++) {\n+\t\t\tint orcIdx = selectedFields[fieldIdx];\n+\t\t\treadField(rows, fieldIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], rowsToRead);\n+\t\t}\n+\t\treturn rowsToRead;\n+\t}\n+\n+\t/**\n+\t * Reads a vector of data into an array of objects.\n+\t *\n+\t * @param vals The array that needs to be filled.\n+\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param schema The schema of the vector to read.\n+\t * @param vector The vector to read.\n+\t * @param childCount The number of vector entries to read.\n+\t */\n+\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, int childCount) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BYTE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase SHORT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase INT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LONG:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase FLOAT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DOUBLE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DATE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BINARY:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DECIMAL:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase STRUCT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LIST:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase MAP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tString repeatingValue = readString(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\t// we don't have to handle isRepeating because ORC assumes that it is propagated into the children.\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\n+\t\t\tint offset = (int) list.offsets[0];\n+\t\t\tint length = (int) list.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t// get function to copy list\n+\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t// create first list that will be copied\n+\t\t\tObject[] first;\n+\t\t\tif (offset == 0) {\n+\t\t\t\tfirst = children;\n+\t\t\t} else {\n+\t\t\t\tfirst = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, first, 0, length);\n+\t\t\t}\n+\n+\t\t\t// create copies of first list and set copies as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tObject[] copy = (Object[]) copyList.apply(first);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = mapsVector.keys;\n+\t\tColumnVector values = mapsVector.values;\n+\n+\t\tif (mapsVector.isRepeating) {\n+\t\t\t// first map is repeated\n+\n+\t\t\t// get map copy function\n+\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t// set all key and value entries except those of the first map to null\n+\t\t\tint offset = (int) mapsVector.offsets[0];\n+\t\t\tint length = (int) mapsVector.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t// create first map that will be copied\n+\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t// copy first map and set copy as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copyMap.apply(map);\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(map));\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\n+\t\t\tObject[] keyRows = new Object[mapsVector.childCount];\n+\t\t\tObject[] valueRows = new Object[mapsVector.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n+\t\t\tint offset = 0;\n+\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n+\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n+\t\t\t\toffset += numMapEntries;\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = map;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t}\n+\n+\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call.\n+\t\t\t\treadNonNullLongColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\tvals[i] = ts;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\n+\t\t// Early out if struct column is repeating and always null.\n+\t\t// This is the only repeating case we need to handle.\n+\t\t// ORC assumes that repeating values have been pushed to the children.\n+\t\tif (structVector.isRepeating && structVector.isNull[0]) {\n+\t\t\tif (fieldIdx < 0) {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn;\n+\t\t}\n+\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\tColumnVector fieldVector = structVector.fields[i];\n+\t\t\tif (!fieldVector.isRepeating) {\n+\t\t\t\t// Reduce fieldVector reads by setting all entries null where struct is null.\n+\t\t\t\tif (fieldVector.noNulls) {\n+\t\t\t\t\t// fieldVector had no nulls. Just use struct null information.\n+\t\t\t\t\tSystem.arraycopy(structVector.isNull, 0, fieldVector.isNull, 0, structVector.isNull.length);\n+\t\t\t\t\tstructVector.fields[i].noNulls = false;\n+\t\t\t\t} else {\n+\t\t\t\t\t// fieldVector had nulls. Merge field nulls with struct nulls.\n+\t\t\t\t\tfor (int j = 0; j < structVector.isNull.length; j++) {\n+\t\t\t\t\t\tstructVector.fields[i].isNull[j] = structVector.isNull[j] || structVector.fields[i].isNull[j];\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tboolean[] isNullVector = structVector.isNull;\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tvals[i] = structs[i];\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\t\t\t// list values are repeating. we only need to read the first list and copy it.\n+\n+\t\t\tif (list.isNull[0]) {\n+\t\t\t\t// Even better. The first list is null and so are all lists are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy list\n+\t\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) list.offsets[0];\n+\t\t\t\tint length = (int) list.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\t// read entries\n+\t\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t\t// create first list which will be copied\n+\t\t\t\tObject[] temp;\n+\t\t\t\tif (offset == 0) {\n+\t\t\t\t\ttemp = children;\n+\t\t\t\t} else {\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\t// copy repeated list and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tObject[] copy = (Object[]) copyList.apply(temp);\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\t\t\tif (!list.child.isRepeating) {\n+\t\t\t\tboolean[] childIsNull = new boolean[list.childCount];\n+\t\t\t\tArrays.fill(childIsNull, true);\n+\t\t\t\t// forward info of null lists into child vector\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info of entries of non-null lists\n+\t\t\t\t\tif (!list.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\t\tint length = (int) list.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(list.child.isNull, offset, childIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of children vector\n+\t\t\t\tlist.child.isNull = childIsNull;\n+\t\t\t\tlist.child.noNulls = false;\n+\t\t\t}\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\tObject[] temp;\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\n+\t\t\t\tif (list.isNull[i]) {\n+\t\t\t\t\ttemp = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = map.keys;\n+\t\tColumnVector values = map.values;\n+\n+\t\tif (map.isRepeating) {\n+\t\t\t// map values are repeating. we only need to read the first map and copy it.\n+\n+\t\t\tif (map.isNull[0]) {\n+\t\t\t\t// Even better. The first map is null and so are all maps are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy map\n+\t\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) map.offsets[0];\n+\t\t\t\tint length = (int) map.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t\t// read map keys and values\n+\t\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t\t// create first map which will be copied\n+\t\t\t\tHashMap temp = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t\t// copy repeated map and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copyMap.apply(temp);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(temp));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// ensure only keys and values that are referenced by non-null maps are set to non-null\n+\n+\t\t\tif (!keys.isRepeating) {\n+\t\t\t\t// propagate is null info of map into keys vector\n+\t\t\t\tboolean[] keyIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(keyIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for keys of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(keys.isNull, offset, keyIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of keys vector\n+\t\t\t\tkeys.isNull = keyIsNull;\n+\t\t\t\tkeys.noNulls = false;\n+\t\t\t}\n+\t\t\tif (!values.isRepeating) {\n+\t\t\t\t// propagate is null info of map into values vector\n+\t\t\t\tboolean[] valIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(valIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for vals of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(values.isNull, offset, valIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of values vector\n+\t\t\t\tvalues.isNull = valIsNull;\n+\t\t\t\tvalues.noNulls = false;\n+\t\t\t}\n+\n+\t\t\tObject[] keyRows = new Object[map.childCount];\n+\t\t\tObject[] valueRows = new Object[map.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tboolean[] isNullVector = map.isNull;\n+\t\t\tlong[] lengths = map.lengths;\n+\t\t\tlong[] offsets = map.offsets;\n+\n+\t\t\tif (fieldIdx == -1) { // set map as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set map as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Sets a repeating value to all objects or row fields of the passed vals array.\n+\t *\n+\t * @param vals The array of objects or Rows.\n+\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param repeatingValue The value that is set.\n+\t * @param childCount The number of times the value is set.\n+\t */\n+\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n+\n+\t\tif (fieldIdx == -1) {\n+\t\t\t// set value as an object\n+\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n+\t\t} else {\n+\t\t\t// set value as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static Class<?> getClassForType(TypeDescription schema) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.class;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn Byte.class;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn Short.class;\n+\t\t\tcase INT:\n+\t\t\t\treturn Integer.class;\n+\t\t\tcase LONG:\n+\t\t\t\treturn Long.class;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.class;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn Double.class;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\treturn String.class;\n+\t\t\tcase DATE:\n+\t\t\t\treturn Date.class;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn Timestamp.class;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn byte[].class;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BigDecimal.class;\n+\t\t\tcase STRUCT:\n+\t\t\t\treturn Row.class;\n+\t\t\tcase LIST:\n+\t\t\t\tClass<?> childClass = getClassForType(schema.getChildren().get(0));\n+\t\t\t\treturn Array.newInstance(childClass, 0).getClass();\n+\t\t\tcase MAP:\n+\t\t\t\treturn HashMap.class;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Boolean readBoolean(long l) {\n+\t\treturn l != 0;\n+\t}\n+\n+\tprivate static Byte readByte(long l) {\n+\t\treturn (byte) l;\n+\t}\n+\n+\tprivate static Short readShort(long l) {\n+\t\treturn (short) l;\n+\t}\n+\n+\tprivate static Integer readInt(long l) {\n+\t\treturn (int) l;\n+\t}\n+\n+\tprivate static Long readLong(long l) {\n+\t\treturn l;\n+\t}\n+\n+\tprivate static Float readFloat(double d) {\n+\t\treturn (float) d;\n+\t}\n+\n+\tprivate static Double readDouble(double d) {\n+\t\treturn d;\n+\t}\n+\n+\tprivate static Date readDate(long l) {\n+\t\t// day to milliseconds\n+\t\tfinal long t = l * MILLIS_PER_DAY;\n+\t\t// adjust by local timezone\n+\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n+\t}\n+\n+\tprivate static String readString(byte[] bytes, int start, int length) {\n+\t\treturn new String(bytes, start, length, StandardCharsets.UTF_8);\n+\t}\n+\n+\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n+\t\tbyte[] result = new byte[length];\n+\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n+\t\treturn result;\n+\t}\n+\n+\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n+\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n+\t\treturn hiveDecimal.bigDecimalValue();\n+\t}\n+\n+\tprivate static Timestamp readTimestamp(long time, int nanos) {\n+\t\tTimestamp ts = new Timestamp(time);\n+\t\tts.setNanos(nanos);\n+\t\treturn ts;\n+\t}\n+\n+\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n+\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n+\t\tfor (int j = 0; j < length; j++) {\n+\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n+\t\t\toffset++;\n+\t\t}\n+\t\treturn resultMap;\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static Function<Object, Object> getCopyFunction(TypeDescription schema) {\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BYTE:\n+\t\t\tcase SHORT:\n+\t\t\tcase INT:\n+\t\t\tcase LONG:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn OrcBatchReader::returnImmutable;\n+\t\t\tcase DATE:\n+\t\t\t\treturn OrcBatchReader::copyDate;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn OrcBatchReader::copyTimestamp;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn OrcBatchReader::copyBinary;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t\t\tFunction<Object, Object>[] copyFields = new Function[fieldTypes.size()];\n+\t\t\t\tfor (int i = 0; i < fieldTypes.size(); i++) {\n+\t\t\t\t\tcopyFields[i] = getCopyFunction(fieldTypes.get(i));\n+\t\t\t\t}\n+\t\t\t\treturn new CopyStruct(copyFields);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription entryType = schema.getChildren().get(0);\n+\t\t\t\tFunction<Object, Object> copyEntry = getCopyFunction(entryType);\n+\t\t\t\tClass entryClass = getClassForType(entryType);\n+\t\t\t\treturn new CopyList(copyEntry, entryClass);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keyType = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valueType = schema.getChildren().get(1);\n+\t\t\t\tFunction<Object, Object> copyKey = getCopyFunction(keyType);\n+\t\t\t\tFunction<Object, Object> copyValue = getCopyFunction(valueType);\n+\t\t\t\treturn new CopyMap(copyKey, copyValue);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Object returnImmutable(Object o) {\n+\t\treturn o;\n+\t}\n+\n+\tprivate static Date copyDate(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong date = ((Date) o).getTime();\n+\t\t\treturn new Date(date);\n+\t\t}\n+\t}\n+\n+\tprivate static Timestamp copyTimestamp(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong millis = ((Timestamp) o).getTime();\n+\t\t\tint nanos = ((Timestamp) o).getNanos();\n+\t\t\tTimestamp copy = new Timestamp(millis);\n+\t\t\tcopy.setNanos(nanos);\n+\t\t\treturn copy;\n+\t\t}\n+\t}\n+\n+\tprivate static byte[] copyBinary(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tint length = ((byte[]) o).length;\n+\t\t\treturn Arrays.copyOf((byte[]) o, length);\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyStruct implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object>[] copyFields;\n+\n+\t\tCopyStruct(Function<Object, Object>[] copyFields) {\n+\t\t\tthis.copyFields = copyFields;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tRow r = (Row) o;\n+\t\t\t\tRow copy = new Row(copyFields.length);\n+\t\t\t\tfor (int i = 0; i < copyFields.length; i++) {\n+\t\t\t\t\tcopy.setField(i, copyFields[i].apply(r.getField(i)));\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyList implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyEntry;\n+\t\tprivate final Class entryClass;\n+\n+\t\tCopyList(Function<Object, Object> copyEntry, Class entryClass) {\n+\t\t\tthis.copyEntry = copyEntry;\n+\t\t\tthis.entryClass = entryClass;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tObject[] l = (Object[]) o;\n+\t\t\t\tObject[] copy = (Object[]) Array.newInstance(entryClass, l.length);\n+\t\t\t\tfor (int i = 0; i < l.length; i++) {\n+\t\t\t\t\tcopy[i] = copyEntry.apply(l[i]);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static class CopyMap implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyKey;\n+\t\tprivate final Function<Object, Object> copyValue;\n+\n+\t\tCopyMap(Function<Object, Object> copyKey, Function<Object, Object> copyValue) {\n+\t\t\tthis.copyKey = copyKey;\n+\t\t\tthis.copyValue = copyValue;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tMap<Object, Object> m = (Map<Object, Object>) o;\n+\t\t\t\tHashMap<Object, Object> copy = new HashMap<>(m.size());\n+\n+\t\t\t\tfor (Map.Entry<Object, Object> e : m.entrySet()) {\n+\t\t\t\t\tObject keyCopy = copyKey.apply(e.getKey());\n+\t\t\t\t\tObject valueCopy = copyValue.apply(e.getValue());\n+\t\t\t\t\tcopy.put(keyCopy, valueCopy);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "sha": "3ecdeb3c956dbeaacca29ebbfed1218d6a465a63",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 2,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "patch": "@@ -55,7 +55,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n-import static org.apache.flink.orc.OrcUtils.fillRows;\n+import static org.apache.flink.orc.OrcBatchReader.fillRows;\n \n /**\n  * InputFormat to read ORC files.\n@@ -128,7 +128,7 @@ public OrcRowInputFormat(String path, TypeDescription orcSchema, Configuration o\n \n \t\t// configure OrcRowInputFormat\n \t\tthis.schema = orcSchema;\n-\t\tthis.rowType = (RowTypeInfo) OrcUtils.schemaToTypeInfo(schema);\n+\t\tthis.rowType = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(schema);\n \t\tthis.conf = orcConfig;\n \t\tthis.batchSize = batchSize;\n ",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "sha": "61575ad727cb1338d029fcc87501156d79453925",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 1,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "patch": "@@ -127,7 +127,7 @@ private OrcTableSource(String path, TypeDescription orcSchema, Configuration orc\n \t\tthis.predicates = predicates;\n \n \t\t// determine the type information from the ORC schema\n-\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcUtils.schemaToTypeInfo(this.orcSchema);\n+\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(this.orcSchema);\n \n \t\t// set return type info\n \t\tif (selectedFields == null) {",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "sha": "0eab4a043da3cd9c3d0429e90b62014621ef1f76",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "changes": 1508,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java?ref=3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb",
                "deletions": 1508,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "patch": "@@ -1,1508 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.orc;\n-\n-import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n-import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n-import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.MapTypeInfo;\n-import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n-import org.apache.flink.types.Row;\n-\n-import org.apache.hadoop.hive.common.type.HiveDecimal;\n-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n-import org.apache.orc.TypeDescription;\n-\n-import java.lang.reflect.Array;\n-import java.math.BigDecimal;\n-import java.nio.charset.StandardCharsets;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.TimeZone;\n-import java.util.function.DoubleFunction;\n-import java.util.function.IntFunction;\n-import java.util.function.LongFunction;\n-\n-/**\n- * A class that provides utility methods for orc file reading.\n- */\n-class OrcUtils {\n-\n-\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n-\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n-\n-\t/**\n-\t * Converts an ORC schema to a Flink TypeInformation.\n-\t *\n-\t * @param schema The ORC schema.\n-\t * @return The TypeInformation that corresponds to the ORC schema.\n-\t */\n-\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n-\t\t\tcase BYTE:\n-\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n-\t\t\tcase SHORT:\n-\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n-\t\t\tcase INT:\n-\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n-\t\t\tcase LONG:\n-\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n-\t\t\tcase FLOAT:\n-\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n-\t\t\tcase DOUBLE:\n-\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n-\t\t\tcase DECIMAL:\n-\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n-\t\t\tcase STRING:\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n-\t\t\tcase DATE:\n-\t\t\t\treturn SqlTimeTypeInfo.DATE;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n-\t\t\tcase BINARY:\n-\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n-\t\t\tcase STRUCT:\n-\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n-\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n-\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n-\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n-\t\t\t\t}\n-\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n-\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n-\t\t\tcase LIST:\n-\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n-\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n-\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n-\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n-\t\t\tcase MAP:\n-\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n-\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n-\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n-\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n-\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Fills an ORC batch into an array of Row.\n-\t *\n-\t * @param rows The batch of rows need to be filled.\n-\t * @param schema The schema of the ORC data.\n-\t * @param batch The ORC data.\n-\t * @param selectedFields The list of selected ORC fields.\n-\t * @return The number of rows that were filled.\n-\t */\n-\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n-\n-\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n-\n-\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n-\t\t// read each selected field\n-\t\tfor (int rowIdx = 0; rowIdx < selectedFields.length; rowIdx++) {\n-\t\t\tint orcIdx = selectedFields[rowIdx];\n-\t\t\treadField(rows, rowIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], null, rowsToRead);\n-\t\t}\n-\t\treturn rowsToRead;\n-\t}\n-\n-\t/**\n-\t * Reads a vector of data into an array of objects.\n-\t *\n-\t * @param vals The array that needs to be filled.\n-\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param schema The schema of the vector to read.\n-\t * @param vector The vector to read.\n-\t * @param lengthVector If the vector is of type List or Map, the number of sub-elements to read for each field. Otherwise, it must be null.\n-\t * @param childCount The number of vector entries to read.\n-\t */\n-\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check the type of the vector to decide how to read it.\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BYTE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase SHORT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase INT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LONG:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase FLOAT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DOUBLE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\tcase STRING:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DATE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BINARY:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DECIMAL:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase STRUCT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LIST:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase MAP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tString[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0], StandardCharsets.UTF_8);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset], StandardCharsets.UTF_8);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tbyte[][] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tDate[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tTimestamp[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tBigDecimal[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tBigDecimal repeatingValue = readBigDecimal(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\n-\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tSystem.arraycopy(structs, offset, temp, 0, temp.length);\n-\t\t\t\toffset = offset + temp.length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the list need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedLists = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedLists, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedLists[0].getClass();\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedLists, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = mapsVector.keys;\n-\t\tColumnVector values = mapsVector.values;\n-\t\tObject[] keyRows = new Object[mapsVector.childCount];\n-\t\tObject[] valueRows = new Object[mapsVector.childCount];\n-\n-\t\t// read map keys and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint offset = 0;\n-\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n-\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n-\t\t\t\toffset += numMapEntries;\n-\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = map;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tlong numMapEntries = lengthVectorMap[offset];\n-\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, numMapEntries);\n-\t\t\t\t\tmapOffset += numMapEntries;\n-\t\t\t\t\toffset++;\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == 1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (bytes.isRepeating) { // fill list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::stringArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tString[] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the binary need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::binaryArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tbyte[][] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::dateArray);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tDate[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\tvals[i] = ts;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::timestampArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tTimestamp[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::decimalArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tBigDecimal[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\tboolean[] isNullVector = structVector.isNull;\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = structs[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = structs[offset++];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) { // set list of structs as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of structs as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the lists need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedList = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedList, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedList[0].getClass();\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedList, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) { // set list of list as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of list as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = map.keys;\n-\t\tColumnVector values = map.values;\n-\t\tObject[] keyRows = new Object[map.childCount];\n-\t\tObject[] valueRows = new Object[map.childCount];\n-\n-\t\t// read map kes and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\tboolean[] isNullVector = map.isNull;\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint offset = 0;\n-\t\t\tif (fieldIdx == -1) { // set map as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]);\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set map as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]));\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, lengthVectorMap[offset]);\n-\t\t\t\t\t\tmapOffset += lengthVectorMap[offset];\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets a repeating value to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param repeatingValue The value that is set.\n-\t * @param childCount The number of times the value is set.\n-\t */\n-\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set value as an object\n-\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n-\t\t} else {\n-\t\t\t// set value as a field of Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets arrays containing only null values to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows to which the empty arrays are set.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param lengthVector The vector containing the lengths of the individual empty arrays.\n-\t * @param childCount The number of objects or Rows to fill.\n-\t * @param array A method to create arrays of the appropriate type.\n-\t * @param <T> The type of the arrays to create.\n-\t */\n-\tprivate static <T> void fillListWithRepeatingNull(Object[] vals, int fieldIdx, long[] lengthVector, int childCount, IntFunction<T[]> array) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set empty array as object\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tvals[i] = array.apply((int) lengthVector[i]);\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// set empty array as field in Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, array.apply((int) lengthVector[i]));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static Boolean readBoolean(long l) {\n-\t\treturn l != 0;\n-\t}\n-\n-\tprivate static Byte readByte(long l) {\n-\t\treturn (byte) l;\n-\t}\n-\n-\tprivate static Short readShort(long l) {\n-\t\treturn (short) l;\n-\t}\n-\n-\tprivate static Integer readInt(long l) {\n-\t\treturn (int) l;\n-\t}\n-\n-\tprivate static Long readLong(long l) {\n-\t\treturn l;\n-\t}\n-\n-\tprivate static Float readFloat(double d) {\n-\t\treturn (float) d;\n-\t}\n-\n-\tprivate static Double readDouble(double d) {\n-\t\treturn d;\n-\t}\n-\n-\tprivate static Date readDate(long l) {\n-\t\t// day to milliseconds\n-\t\tfinal long t = l * MILLIS_PER_DAY;\n-\t\t// adjust by local timezone\n-\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n-\t}\n-\n-\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n-\t\tbyte[] result = new byte[length];\n-\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n-\t\treturn result;\n-\t}\n-\n-\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n-\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n-\t\treturn hiveDecimal.bigDecimalValue();\n-\t}\n-\n-\tprivate static Timestamp readTimestamp(long time, int nanos) {\n-\t\tTimestamp ts = new Timestamp(time);\n-\t\tts.setNanos(nanos);\n-\t\treturn ts;\n-\t}\n-\n-\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n-\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n-\t\tfor (int j = 0; j < length; j++) {\n-\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n-\t\t\toffset++;\n-\t\t}\n-\t\treturn resultMap;\n-\t}\n-\n-\tprivate static Boolean[] boolArray(int len) {\n-\t\treturn new Boolean[len];\n-\t}\n-\n-\tprivate static Byte[] byteArray(int len) {\n-\t\treturn new Byte[len];\n-\t}\n-\n-\tprivate static Short[] shortArray(int len) {\n-\t\treturn new Short[len];\n-\t}\n-\n-\tprivate static Integer[] intArray(int len) {\n-\t\treturn new Integer[len];\n-\t}\n-\n-\tprivate static Long[] longArray(int len) {\n-\t\treturn new Long[len];\n-\t}\n-\n-\tprivate static Float[] floatArray(int len) {\n-\t\treturn new Float[len];\n-\t}\n-\n-\tprivate static Double[] doubleArray(int len) {\n-\t\treturn new Double[len];\n-\t}\n-\n-\tprivate static Date[] dateArray(int len) {\n-\t\treturn new Date[len];\n-\t}\n-\n-\tprivate static byte[][] binaryArray(int len) {\n-\t\treturn new byte[len][];\n-\t}\n-\n-\tprivate static String[] stringArray(int len) {\n-\t\treturn new String[len];\n-\t}\n-\n-\tprivate static BigDecimal[] decimalArray(int len) {\n-\t\treturn new BigDecimal[len];\n-\t}\n-\n-\tprivate static Timestamp[] timestampArray(int len) {\n-\t\treturn new Timestamp[len];\n-\t}\n-\n-}",
                "raw_url": "https://github.com/apache/flink/raw/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "sha": "cfb4e0e66a8183da7b44fcf4672b376b01e53766",
                "status": "removed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 4,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "patch": "@@ -31,10 +31,10 @@\n import org.junit.Test;\n \n /**\n- * Unit tests for {@link OrcUtils}.\n+ * Unit tests for {@link OrcBatchReader}.\n  *\n  */\n-public class OrcUtilsTest {\n+public class OrcBatchReaderTest {\n \n \t@Test\n \tpublic void testFlatSchemaToTypeInfo1() {\n@@ -54,7 +54,7 @@ public void testFlatSchemaToTypeInfo1() {\n \t\t\t\t\"timestamp1:timestamp,\" +\n \t\t\t\t\"decimal1:decimal(5,2)\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);\n@@ -106,7 +106,7 @@ public void testNestedSchemaToTypeInfo1() {\n \t\t\t\t\t\">\" +\n \t\t\t\t\">\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);",
                "previous_filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcUtilsTest.java",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "sha": "b90313ea2aaa1f7217989182618d94cf90e129cf",
                "status": "renamed"
            },
            {
                "additions": 211,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "changes": 218,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 7,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.core.fs.FileInputSplit;\n import org.apache.flink.core.fs.Path;\n+import org.apache.flink.orc.util.OrcTestFileGenerator;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.InstantiationUtil;\n \n@@ -50,6 +51,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.any;\n@@ -124,6 +126,32 @@ public void tearDown() throws IOException {\n \tprivate static final String TEST_FILE_NESTEDLIST = \"test-data-nestedlist.orc\";\n \tprivate static final String TEST_SCHEMA_NESTEDLIST = \"struct<mylist1:array<array<struct<mylong1:bigint>>>>\";\n \n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithNullsFile(String)}. */\n+\tprivate static final String TEST_FILE_COMPOSITES_NULLS = \"test-data-composites-with-nulls.orc\";\n+\tprivate static final String TEST_SCHEMA_COMPOSITES_NULLS =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\">\";\n+\n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithRepeatingFile(String)}. */\n+\tprivate static final String TEST_FILE_REPEATING = \"test-data-repeating.orc\";\n+\tprivate static final String TEST_SCHEMA_REPEATING =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"int2:int,\" +\n+\t\t\t\"int3:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<int>,\" +\n+\t\t\t\"list2:array<int>,\" +\n+\t\t\t\"list3:array<int>,\" +\n+\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\"map2:map<int,string>\" +\n+\t\t\">\";\n+\n \t@Test(expected = FileNotFoundException.class)\n \tpublic void testInvalidPath() throws IOException{\n \t\trowOrcInputFormat =\n@@ -477,7 +505,7 @@ public void testPredicateWithInvalidColumn() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadNestedFile() throws IOException{\n+\tpublic void testReadNestedFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -563,7 +591,7 @@ public void testReadNestedFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadTimeTypeFile() throws IOException{\n+\tpublic void testReadTimeTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_TIMETYPES), TEST_SCHEMA_TIMETYPES, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -590,7 +618,7 @@ public void testReadTimeTypeFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadDecimalTypeFile() throws IOException{\n+\tpublic void testReadDecimalTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_DECIMAL), TEST_SCHEMA_DECIMAL, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -653,7 +681,183 @@ public void testReadNestedListFile() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadWithProjection() throws IOException{\n+\tpublic void testReadCompositesNullsFile() throws Exception {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_COMPOSITES_NULLS),\n+\t\t\tTEST_SCHEMA_COMPOSITES_NULLS,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tint structNullCnt = 0;\n+\t\tint nestedListNullCnt = 0;\n+\t\tint mapListNullCnt = 0;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(4, row.getArity());\n+\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\n+\t\t\tif (row.getField(1) == null) {\n+\t\t\t\tstructNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(1);\n+\t\t\t\tassertTrue(f instanceof Row);\n+\t\t\t\tassertEquals(2, ((Row) f).getArity());\n+\t\t\t}\n+\n+\t\t\tif (row.getField(2) == null) {\n+\t\t\t\tnestedListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(2);\n+\t\t\t\tassertTrue(f instanceof Row[][][]);\n+\t\t\t\tassertEquals(4, ((Row[][][]) f).length);\n+\t\t\t}\n+\n+\t\t\tif (row.getField(3) == null) {\n+\t\t\t\tmapListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(3);\n+\t\t\t\tassertTrue(f instanceof HashMap[]);\n+\t\t\t\tassertEquals(3, ((HashMap[]) f).length);\n+\t\t\t}\n+\t\t\tcnt++;\n+\t\t}\n+\t\t// number of rows in file\n+\t\tassertEquals(2500, cnt);\n+\t\t// check number of null fields\n+\t\tassertEquals(1250, structNullCnt);\n+\t\tassertEquals(835, nestedListNullCnt);\n+\t\tassertEquals(835, mapListNullCnt);\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\t@Test\n+\tpublic void testReadRepeatingValuesFile() throws IOException {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_REPEATING),\n+\t\t\tTEST_SCHEMA_REPEATING,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tRow firstRow1 = null;\n+\t\tInteger[] firstList1 = null;\n+\t\tHashMap firstMap1 = null;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\tcnt++;\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(10, row.getArity());\n+\n+\t\t\t// check first int field (always 42)\n+\t\t\tassertNotNull(row.getField(0));\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\t\t\tassertEquals(42, ((Integer) row.getField(0)).intValue());\n+\n+\t\t\t// check second int field (always null)\n+\t\t\tassertNull(row.getField(1));\n+\n+\t\t\t// check first int field (always 99)\n+\t\t\tassertNotNull(row.getField(2));\n+\t\t\tassertTrue(row.getField(2) instanceof Integer);\n+\t\t\tassertEquals(99, ((Integer) row.getField(2)).intValue());\n+\n+\t\t\t// check first row field (always (23, null))\n+\t\t\tassertNotNull(row.getField(3));\n+\t\t\tassertTrue(row.getField(3) instanceof Row);\n+\t\t\tRow nestedRow = (Row) row.getField(3);\n+\t\t\t// check first field of nested row\n+\t\t\tassertNotNull(nestedRow.getField(0));\n+\t\t\tassertTrue(nestedRow.getField(0) instanceof Integer);\n+\t\t\tassertEquals(23, ((Integer) nestedRow.getField(0)).intValue());\n+\t\t\t// check second field of nested row\n+\t\t\tassertNull(nestedRow.getField(1));\n+\t\t\t// validate reference\n+\t\t\tif (firstRow1 == null) {\n+\t\t\t\tfirstRow1 = nestedRow;\n+\t\t\t} else {\n+\t\t\t\t// repeated rows must be different instances\n+\t\t\t\tassertTrue(firstRow1 != nestedRow);\n+\t\t\t}\n+\n+\t\t\t// check second row field (always null)\n+\t\t\tassertNull(row.getField(4));\n+\n+\t\t\t// check first list field (always [1, 2, 3])\n+\t\t\tassertNotNull(row.getField(5));\n+\t\t\tassertTrue(row.getField(5) instanceof Integer[]);\n+\t\t\tInteger[] list1 = ((Integer[]) row.getField(5));\n+\t\t\tassertEquals(1, list1[0].intValue());\n+\t\t\tassertEquals(2, list1[1].intValue());\n+\t\t\tassertEquals(3, list1[2].intValue());\n+\t\t\t// validate reference\n+\t\t\tif (firstList1 == null) {\n+\t\t\t\tfirstList1 = list1;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstList1 != list1);\n+\t\t\t}\n+\n+\t\t\t// check second list field (always [7, 7, 7])\n+\t\t\tassertNotNull(row.getField(6));\n+\t\t\tassertTrue(row.getField(6) instanceof Integer[]);\n+\t\t\tInteger[] list2 = ((Integer[]) row.getField(6));\n+\t\t\tassertEquals(7, list2[0].intValue());\n+\t\t\tassertEquals(7, list2[1].intValue());\n+\t\t\tassertEquals(7, list2[2].intValue());\n+\n+\t\t\t// check third list field (always null)\n+\t\t\tassertNull(row.getField(7));\n+\n+\t\t\t// check first map field (always {2->\"Hello\", 4->\"Hello})\n+\t\t\tassertNotNull(row.getField(8));\n+\t\t\tassertTrue(row.getField(8) instanceof HashMap);\n+\t\t\tHashMap<Integer, String> map = (HashMap<Integer, String>) row.getField(8);\n+\t\t\tassertEquals(2, map.size());\n+\t\t\tassertEquals(\"Hello\", map.get(2));\n+\t\t\tassertEquals(\"Hello\", map.get(4));\n+\t\t\t// validate reference\n+\t\t\tif (firstMap1 == null) {\n+\t\t\t\tfirstMap1 = map;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstMap1 != map);\n+\t\t\t}\n+\n+\t\t\t// check second map field (always null)\n+\t\t\tassertNull(row.getField(9));\n+\t\t}\n+\n+\t\trowOrcInputFormat.close();\n+\t\trowOrcInputFormat.closeInputFormat();\n+\n+\t\tassertEquals(256, cnt);\n+\t}\n+\n+\t@Test\n+\tpublic void testReadWithProjection() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\trowOrcInputFormat.selectFields(7, 0, 10, 8);\n@@ -691,7 +895,7 @@ public void testReadWithProjection() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileInSplits() throws IOException{\n+\tpublic void testReadFileInSplits() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -717,7 +921,7 @@ public void testReadFileInSplits() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithFilter() throws IOException{\n+\tpublic void testReadFileWithFilter() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -751,7 +955,7 @@ public void testReadFileWithFilter() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithEvolvedSchema() throws IOException{\n+\tpublic void testReadFileWithEvolvedSchema() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(\n \t\t\tgetPath(TEST_FILE_FLAT),",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "sha": "2eb3231eedabcfb5bf2e949b3390768803a52f7b",
                "status": "modified"
            },
            {
                "additions": 373,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "changes": 373,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "patch": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc.util;\n+\n+import org.apache.flink.orc.OrcRowInputFormatTest;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.Writer;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * A generator for ORC test files.\n+ */\n+public class OrcTestFileGenerator {\n+\n+\tpublic static void main(String[] args) throws IOException {\n+\t\twriteCompositeTypesWithNullsFile(args[0]);\n+//\t\twriteCompositeTypesWithRepeatingFile(args[0]);\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and null values on different levels.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_COMPOSITES_NULLS}.\n+\t */\n+\tprivate static void writeCompositeTypesWithNullsFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[1];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[2];\n+\t\tListColumnVector nestedList = (ListColumnVector) list1.child;\n+\t\tListColumnVector nestedList2 = (ListColumnVector) nestedList.child;\n+\t\tStructColumnVector listEntries = (StructColumnVector) nestedList2.child;\n+\t\tBytesColumnVector entryField1 = (BytesColumnVector) listEntries.fields[0];\n+\t\tBytesColumnVector entryField2 = (BytesColumnVector) listEntries.fields[1];\n+\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[3];\n+\t\tMapColumnVector map1 = (MapColumnVector) list2.child;\n+\t\tBytesColumnVector keys = (BytesColumnVector) map1.keys;\n+\t\tLongColumnVector vals = (LongColumnVector) map1.values;\n+\n+\t\tfinal int list1Size = 4;\n+\t\tfinal int nestedListSize = 3;\n+\t\tfinal int nestedList2Size = 2;\n+\t\tfinal int list2Size = 3;\n+\t\tfinal int mapSize = 3;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tnestedList.ensureSize(batchSize * list1Size, false);\n+\t\tnestedList2.ensureSize(batchSize * list1Size * nestedListSize, false);\n+\t\tlistEntries.ensureSize(batchSize * list1Size * nestedListSize * nestedList2Size, false);\n+\t\tmap1.ensureSize(batchSize * list2Size, false);\n+\t\tkeys.ensureSize(batchSize * list2Size * mapSize, false);\n+\t\tvals.ensureSize(batchSize * list2Size * mapSize, false);\n+\n+\t\t// add 2500 rows to file\n+\t\tfor (int r = 0; r < 2500; ++r) {\n+\t\t\tint row = batch.size++;\n+\n+\t\t\t// mark nullable fields\n+\t\t\tlist1.noNulls = false;\n+\t\t\tnestedList.noNulls = false;\n+\t\t\tlistEntries.noNulls = false;\n+\t\t\tentryField1.noNulls = false;\n+\t\t\trecord1.noNulls = false;\n+\t\t\trecord1F2.noNulls = false;\n+\t\t\tlist2.noNulls = false;\n+\t\t\tmap1.noNulls = false;\n+\t\t\tkeys.noNulls = false;\n+\t\t\tvals.noNulls = false;\n+\n+\t\t\t// first field: int1\n+\t\t\tint1.vector[row] = r;\n+\n+\t\t\t// second field: struct\n+\t\t\tif (row % 2 != 0) {\n+\t\t\t\t// in every second row, the struct is null\n+\t\t\t\trecord1F1.vector[row] = row;\n+\t\t\t\tif (row % 5 != 0) {\n+\t\t\t\t\t// in every fifth row, the second field of the struct is null\n+\t\t\t\t\trecord1F2.setVal(row, (\"f2-\" + row).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t} else {\n+\t\t\t\t\trecord1F2.isNull[row] = true;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\trecord1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// third field: deeply nested list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the nested list is null\n+\t\t\t\tlist1.offsets[row] = list1.childCount;\n+\t\t\t\tlist1.lengths[row] = list1Size;\n+\t\t\t\tlist1.childCount += list1Size;\n+\n+\t\t\t\tfor (int i = 0; i < list1Size; i++) {\n+\n+\t\t\t\t\tint listOffset = (int) list1.offsets[row] + i;\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second nested list is always null\n+\t\t\t\t\t\tnestedList.offsets[listOffset] = nestedList.childCount;\n+\t\t\t\t\t\tnestedList.lengths[listOffset] = nestedListSize;\n+\t\t\t\t\t\tnestedList.childCount += nestedListSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < nestedListSize; j++) {\n+\t\t\t\t\t\t\tint nestedOffset = (int) nestedList.offsets[listOffset] + j;\n+\t\t\t\t\t\t\tnestedList2.offsets[nestedOffset] = nestedList2.childCount;\n+\t\t\t\t\t\t\tnestedList2.lengths[nestedOffset] = nestedList2Size;\n+\t\t\t\t\t\t\tnestedList2.childCount += nestedList2Size;\n+\n+\t\t\t\t\t\t\tfor (int k = 0; k < nestedList2Size; k++) {\n+\t\t\t\t\t\t\t\tint nestedOffset2 = (int) nestedList2.offsets[nestedOffset] + k;\n+\t\t\t\t\t\t\t\t// list entries\n+\t\t\t\t\t\t\t\tif (k != 1) {\n+\t\t\t\t\t\t\t\t\t// second struct is always null\n+\t\t\t\t\t\t\t\t\tif (k != 0) {\n+\t\t\t\t\t\t\t\t\t\t// first struct field in first struct is always null\n+\t\t\t\t\t\t\t\t\t\tentryField1.setVal(nestedOffset2, (\"f1-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\t\tentryField1.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\tentryField2.setVal(nestedOffset2, (\"f2-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\tlistEntries.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tnestedList.isNull[listOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// forth field: map in list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the map list is null\n+\t\t\t\tlist2.offsets[row] = list2.childCount;\n+\t\t\t\tlist2.lengths[row] = list2Size;\n+\t\t\t\tlist2.childCount += list2Size;\n+\n+\t\t\t\tfor (int i = 0; i < list2Size; i++) {\n+\t\t\t\t\tint mapOffset = (int) list2.offsets[row] + i;\n+\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second map list entry is always null\n+\t\t\t\t\t\tmap1.offsets[mapOffset] = map1.childCount;\n+\t\t\t\t\t\tmap1.lengths[mapOffset] = mapSize;\n+\t\t\t\t\t\tmap1.childCount += mapSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < mapSize; j++) {\n+\t\t\t\t\t\t\tint mapEntryOffset = (int) map1.offsets[mapOffset] + j;\n+\n+\t\t\t\t\t\t\tif (j != 1) {\n+\t\t\t\t\t\t\t\t// key in second map entry is always null\n+\t\t\t\t\t\t\t\tkeys.setVal(mapEntryOffset, (\"key-\" + row + \"-\" + j).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tkeys.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tif (j != 2) {\n+\t\t\t\t\t\t\t\t// value in third map entry is always null\n+\t\t\t\t\t\t\t\tvals.vector[mapEntryOffset] = row + i + j;\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tvals.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tmap1.isNull[mapOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist2.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\tif (row == batchSize - 1) {\n+\t\t\t\twriter.addRowBatch(batch);\n+\t\t\t\tbatch.reset();\n+\t\t\t}\n+\t\t}\n+\t\tif (batch.size != 0) {\n+\t\t\twriter.addRowBatch(batch);\n+\t\t\tbatch.reset();\n+\t\t}\n+\t\twriter.close();\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and repeated values.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_REPEATING}.\n+\t */\n+\tprivate static void writeCompositeTypesWithRepeatingFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"int2:int,\" +\n+\t\t\t\t\t\"int3:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<int>,\" +\n+\t\t\t\t\t\"list2:array<int>,\" +\n+\t\t\t\t\t\"list3:array<int>,\" +\n+\t\t\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\t\t\"map2:map<int,string>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\t\tLongColumnVector int2 = (LongColumnVector) batch.cols[1];\n+\t\tLongColumnVector int3 = (LongColumnVector) batch.cols[2];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[3];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\t\tStructColumnVector record2 = (StructColumnVector) batch.cols[4];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[5];\n+\t\tLongColumnVector list1int = (LongColumnVector) list1.child;\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[6];\n+\t\tLongColumnVector list2int = (LongColumnVector) list2.child;\n+\t\tListColumnVector list3 = (ListColumnVector) batch.cols[7];\n+\n+\t\tMapColumnVector map1 = (MapColumnVector) batch.cols[8];\n+\t\tLongColumnVector map1keys = (LongColumnVector) map1.keys;\n+\t\tBytesColumnVector map1vals = (BytesColumnVector) map1.values;\n+\t\tMapColumnVector map2 = (MapColumnVector) batch.cols[9];\n+\n+\t\tfinal int listSize = 3;\n+\t\tfinal int mapSize = 2;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tlist1int.ensureSize(batchSize * listSize, false);\n+\t\tlist2int.ensureSize(batchSize * listSize, false);\n+\t\tmap1keys.ensureSize(batchSize * mapSize, false);\n+\t\tmap1vals.ensureSize(batchSize * mapSize, false);\n+\n+\t\t// int1: all values are 42\n+\t\tint1.noNulls = true;\n+\t\tint1.setRepeating(true);\n+\t\tint1.vector[0] = 42;\n+\n+\t\t// int2: all values are null\n+\t\tint2.noNulls = false;\n+\t\tint2.setRepeating(true);\n+\t\tint2.isNull[0] = true;\n+\n+\t\t// int3: all values are 99\n+\t\tint3.noNulls = false;\n+\t\tint3.setRepeating(true);\n+\t\tint3.isNull[0] = false;\n+\t\tint3.vector[0] = 99;\n+\n+\t\t// record1: all records are [23, \"Hello\"]\n+\t\trecord1.noNulls = true;\n+\t\trecord1.setRepeating(true);\n+\t\tfor (int i = 0; i < batchSize; i++) {\n+\t\t\trecord1F1.vector[i] = i + 23;\n+\t\t}\n+\t\trecord1F2.noNulls = false;\n+\t\trecord1F2.isNull[0] = true;\n+\n+\t\t// record2: all records are null\n+\t\trecord2.noNulls = false;\n+\t\trecord2.setRepeating(true);\n+\t\trecord2.isNull[0] = true;\n+\n+\t\t// list1: all lists are [1, 2, 3]\n+\t\tlist1.noNulls = true;\n+\t\tlist1.setRepeating(true);\n+\t\tlist1.lengths[0] = listSize;\n+\t\tlist1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * listSize; i++) {\n+\t\t\tlist1int.vector[i] = i;\n+\t\t}\n+\n+\t\t// list2: all lists are [7, 7, 7]\n+\t\tlist2.noNulls = true;\n+\t\tlist2.setRepeating(true);\n+\t\tlist2.lengths[0] = listSize;\n+\t\tlist2.offsets[0] = 0;\n+\t\tlist2int.setRepeating(true);\n+\t\tlist2int.vector[0] = 7;\n+\n+\t\t// list3: all lists are null\n+\t\tlist3.noNulls = false;\n+\t\tlist3.setRepeating(true);\n+\t\tlist3.isNull[0] = true;\n+\n+\t\t// map1: all maps are [2 -> \"HELLO\", 4 -> \"HELLO\"]\n+\t\tmap1.noNulls = true;\n+\t\tmap1.setRepeating(true);\n+\t\tmap1.lengths[0] = mapSize;\n+\t\tmap1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * mapSize; i++) {\n+\t\t\tmap1keys.vector[i] = i * 2;\n+\t\t}\n+\t\tmap1vals.setRepeating(true);\n+\t\tmap1vals.setVal(0, \"Hello\".getBytes(StandardCharsets.UTF_8));\n+\n+\t\t// map2: all maps are null\n+\t\tmap2.noNulls = false;\n+\t\tmap2.setRepeating(true);\n+\t\tmap2.isNull[0] = true;\n+\n+\t\tbatch.size = 256;\n+\n+\t\twriter.addRowBatch(batch);\n+\t\tbatch.reset();\n+\t\twriter.close();\n+\t}\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "sha": "9d3be63b294c0c22e35f44abe450e22abc6062d3",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "sha": "eed1c554cc019a6181e1e65e0f1a049e22c00dd3",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "sha": "ff2c917c6d47587813dee98a33192430550ef979",
                "status": "added"
            }
        ],
        "message": "[FLINK-8230] [orc] Fix NPEs when reading nested columns.\n\n- fixes NPEs for null-valued structs, lists, and maps\n- fixes NPEs for repeating structs, lists, and maps\n- adds test for deeply nested data with nulls\n- adds test for columns with repeating values\n\nThis closes #5373.",
        "parent": "https://github.com/apache/flink/commit/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb",
        "repo": "flink",
        "unit_tests": [
            "OrcBatchReaderTest.java",
            "OrcRowInputFormatTest.java",
            "OrcTableSourceTest.java"
        ]
    },
    "flink_c5dd1f1": {
        "bug_id": "flink_c5dd1f1",
        "commit": "https://github.com/apache/flink/commit/c5dd1f11f71471ba42e3a075651649e2ca258551",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=c5dd1f11f71471ba42e3a075651649e2ca258551",
                "deletions": 2,
                "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "patch": "@@ -180,8 +180,10 @@ public String toString()\n \t\t\t\tbuf.append(\",\");\n \t\t\t}\n \t\t\tbuf.append(this.indexes.get(i));\n-\t\t\tbuf.append(\":\");\n-\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\tif (this.types.get(i) != null) {\n+\t\t\t\tbuf.append(\":\");\n+\t\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\t}\n \t\t\tbuf.append(\":\");\n \t\t\tbuf.append(this.orders.get(i).name());\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "sha": "ca0e4f8e38e1825fea35a9a382e5642ab5ce8615",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in Ordering toString method: Keytypes may be null",
        "parent": "https://github.com/apache/flink/commit/692318593be6e5f7aa5fc62bf624ef34cb5a357a",
        "repo": "flink",
        "unit_tests": [
            "OrderingTest.java"
        ]
    },
    "flink_cc41285": {
        "bug_id": "flink_cc41285",
        "commit": "https://github.com/apache/flink/commit/cc412859001a9437e5176596dc284f05bd740a40",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "patch": "@@ -99,9 +99,11 @@ public boolean isAssigned() {\n \t *\n \t * @return True if the location has been assigned and the shared slot is alive,\n \t *         false otherwise.\n+\t * @deprecated Should only be called by legacy code (if using {@link Scheduler})\n \t */\n+\t@Deprecated\n \tpublic boolean isAssignedAndAlive() {\n-\t\treturn lockedLocation != null && sharedSlot.isAlive();\n+\t\treturn lockedLocation != null && sharedSlot != null && sharedSlot.isAlive();\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "sha": "23c9c214ccc31ba6a8437c1948a16d37004edd8e",
                "status": "modified"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "changes": 143,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "patch": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.executiongraph;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.runtime.concurrent.ScheduledExecutor;\n+import org.apache.flink.runtime.executiongraph.restart.RestartCallback;\n+import org.apache.flink.runtime.executiongraph.restart.RestartStrategy;\n+import org.apache.flink.runtime.jobgraph.JobStatus;\n+import org.apache.flink.runtime.jobgraph.JobVertex;\n+import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n+import org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase;\n+import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.util.FlinkException;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.runtime.jobgraph.JobStatus.FINISHED;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Additional {@link ExecutionGraph} restart tests {@link ExecutionGraphRestartTest} which\n+ * require the usage of a {@link SlotProvider}.\n+ */\n+@RunWith(Parameterized.class)\n+public class ExecutionGraphCoLocationRestartTest extends SchedulerTestBase {\n+\n+\tprivate static final int NUM_TASKS = 31;\n+\n+\tpublic ExecutionGraphCoLocationRestartTest(SchedulerType schedulerType) {\n+\t\tsuper(schedulerType);\n+\t}\n+\n+\t@Test\n+\tpublic void testConstraintsAfterRestart() throws Exception {\n+\t\tfinal long timeout = 5000L;\n+\n+\t\t//setting up\n+\t\ttestingSlotProvider.addTaskManager(NUM_TASKS);\n+\n+\t\tJobVertex groupVertex = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\t\tJobVertex groupVertex2 = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\n+\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n+\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n+\n+\t\t//initiate and schedule job\n+\t\tfinal ExecutionGraph eg = ExecutionGraphTestUtils.createSimpleTestGraph(\n+\t\t\tnew JobID(),\n+\t\t\ttestingSlotProvider,\n+\t\t\tnew OneTimeDirectRestartStrategy(),\n+\t\t\tgroupVertex,\n+\t\t\tgroupVertex2);\n+\n+\t\tif (schedulerType == SchedulerType.SLOT_POOL) {\n+\t\t\t// enable the queued scheduling for the slot pool\n+\t\t\teg.setQueuedSchedulingAllowed(true);\n+\t\t}\n+\n+\t\tassertEquals(JobStatus.CREATED, eg.getState());\n+\n+\t\teg.scheduleForExecution();\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n+\n+\t\t//sanity checks\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new FlinkException(\"Test exception\"));\n+\n+\t\t// wait until we have restarted\n+\t\tExecutionGraphTestUtils.waitUntilJobStatus(eg, JobStatus.RUNNING, timeout);\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\t//checking execution vertex properties\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.finishAllVertices(eg);\n+\n+\t\tassertThat(eg.getState(), is(FINISHED));\n+\t}\n+\n+\tprivate void validateConstraints(ExecutionGraph eg) {\n+\n+\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n+\n+\t\tfor(int i = 0; i < NUM_TASKS; i++){\n+\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tassertThat(constr1.isAssigned(), is(true));\n+\t\t\tassertThat(constr1.getLocation(), equalTo(constr2.getLocation()));\n+\t\t}\n+\n+\t}\n+\n+\tprivate static final class OneTimeDirectRestartStrategy implements RestartStrategy {\n+\t\tprivate boolean hasRestarted = false;\n+\n+\t\t@Override\n+\t\tpublic boolean canRestart() {\n+\t\t\treturn !hasRestarted;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void restart(RestartCallback restarter, ScheduledExecutor executor) {\n+\t\t\thasRestarted = true;\n+\t\t\trestarter.triggerFullRecovery();\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "sha": "aba6bfad5ec22a10e07b43d98ca532939c280e9d",
                "status": "added"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 101,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "patch": "@@ -41,19 +41,18 @@\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionType;\n import org.apache.flink.runtime.jobgraph.DistributionPattern;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.ScheduleMode;\n-import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n import org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n import org.apache.flink.runtime.testingUtils.TestingUtils;\n import org.apache.flink.runtime.testtasks.NoOpInvokable;\n@@ -73,6 +72,7 @@\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Consumer;\n \n@@ -125,62 +125,6 @@ public void testNoManualRestart() throws Exception {\n \t\tassertEquals(JobStatus.FAILED, eg.getState());\n \t}\n \n-\t@Test\n-\tpublic void testConstraintsAfterRestart() throws Exception {\n-\t\t\n-\t\t//setting up\n-\t\tInstance instance = ExecutionGraphTestUtils.getInstance(\n-\t\t\tnew ActorTaskManagerGateway(\n-\t\t\t\tnew SimpleActorGateway(TestingUtils.directExecutionContext())),\n-\t\t\tNUM_TASKS);\n-\t\t\n-\t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n-\t\tscheduler.newInstanceAvailable(instance);\n-\n-\t\tJobVertex groupVertex = newJobVertex(\"Task1\", NUM_TASKS, NoOpInvokable.class);\n-\t\tJobVertex groupVertex2 = newJobVertex(\"Task2\", NUM_TASKS, NoOpInvokable.class);\n-\n-\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n-\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n-\t\t\n-\t\t//initiate and schedule job\n-\t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", groupVertex, groupVertex2);\n-\t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 0L), scheduler);\n-\t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n-\n-\t\tassertEquals(JobStatus.CREATED, eg.getState());\n-\t\t\n-\t\teg.scheduleForExecution();\n-\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n-\t\t\n-\t\t//sanity checks\n-\t\tvalidateConstraints(eg);\n-\n-\t\t//restart automatically\n-\t\trestartAfterFailure(eg, new FiniteDuration(2, TimeUnit.MINUTES), false);\n-\t\t\n-\t\t//checking execution vertex properties\n-\t\tvalidateConstraints(eg);\n-\n-\t\thaltExecution(eg);\n-\t}\n-\n-\tprivate void validateConstraints(ExecutionGraph eg) {\n-\t\t\n-\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n-\t\t\n-\t\tfor(int i=0; i<NUM_TASKS; i++){\n-\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tassertNotNull(constr1.getSharedSlot());\n-\t\t\tassertTrue(constr1.isAssigned());\n-\t\t\tassertEquals(constr1, constr2);\n-\t\t}\n-\t\t\n-\t}\n-\n \t@Test\n \tpublic void testRestartAutomatically() throws Exception {\n \t\tRestartStrategy restartStrategy = new FixedDelayRestartStrategy(1, 1000);\n@@ -383,8 +327,8 @@ public void testFailingExecutionAfterRestart() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task1\", 1, NoOpInvokable.class);\n-\t\tJobVertex receiver = newJobVertex(\"Task2\", 1, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task1\", 1, NoOpInvokable.class);\n+\t\tJobVertex receiver = ExecutionGraphTestUtils.createJobVertex(\"Task2\", 1, NoOpInvokable.class);\n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender, receiver);\n \t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 1000), scheduler);\n \t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n@@ -447,7 +391,7 @@ public void testFailExecutionAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -493,7 +437,7 @@ public void testFailExecutionGraphAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -918,7 +862,7 @@ public void run() {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n \n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender);\n \n@@ -935,13 +879,6 @@ public void run() {\n \t\treturn new Tuple2<>(eg, instance);\n \t}\n \n-\tprivate static JobVertex newJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n-\t\tJobVertex groupVertex = new JobVertex(task1);\n-\t\tgroupVertex.setInvokableClass(invokable);\n-\t\tgroupVertex.setParallelism(numTasks);\n-\t\treturn groupVertex;\n-\t}\n-\n \tprivate static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy, Scheduler scheduler) throws IOException {\n \t\treturn new ExecutionGraph(\n \t\t\tTestingUtils.defaultExecutor(),\n@@ -955,8 +892,11 @@ private static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy,\n \t\t\tscheduler);\n \t}\n \n-\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException {\n-\t\tmakeAFailureAndWait(eg, timeout);\n+\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException, TimeoutException {\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new Exception(\"Test Exception\"));\n+\n+\t\t// Wait for async restart\n+\t\twaitForAsyncRestart(eg, timeout);\n \n \t\tassertEquals(JobStatus.RUNNING, eg.getState());\n \n@@ -973,32 +913,11 @@ private static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeou\n \t\t}\n \t}\n \n-\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws InterruptedException {\n-\t\tboolean success = false;\n-\n-\t\twhile (deadline.hasTimeLeft() && !success) {\n-\t\t\tsuccess = true;\n-\n-\t\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\t\tif (vertex.getCurrentExecutionAttempt().getAssignedResource() == null) {\n-\t\t\t\t\tsuccess = false;\n-\t\t\t\t\tThread.sleep(100);\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void makeAFailureAndWait(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n-\t\teg.getAllExecutionVertices().iterator().next().fail(new Exception(\"Test Exception\"));\n-\t\tassertEquals(JobStatus.FAILING, eg.getState());\n-\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n-\t\t}\n-\n-\t\t// Wait for async restart\n-\t\twaitForAsyncRestart(eg, timeout);\n+\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws TimeoutException {\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\tdeadline.timeLeft().toMillis());\n \t}\n \n \tprivate static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n@@ -1009,9 +928,7 @@ private static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeou\n \t}\n \n \tprivate static void haltExecution(ExecutionGraph eg) {\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().markFinished();\n-\t\t}\n+\t\tfinishAllVertices(eg);\n \n \t\tassertEquals(JobStatus.FINISHED, eg.getState());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "sha": "9b98de7814377a76c3dfbab96da76b3ee3d4b2c7",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.time.Deadline;\n import org.apache.flink.api.common.time.Time;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;\n@@ -41,16 +42,16 @@\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.instance.SimpleSlot;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.runtime.instance.SimpleSlotContext;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n-import org.apache.flink.runtime.instance.SimpleSlotContext;\n-import org.apache.flink.runtime.jobmaster.SlotOwner;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.SlotOwner;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.messages.Acknowledge;\n import org.apache.flink.runtime.messages.TaskMessages.CancelTask;\n import org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;\n@@ -65,11 +66,14 @@\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n+\n import java.lang.reflect.Field;\n import java.net.InetAddress;\n+import java.time.Duration;\n import java.util.List;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n \n import scala.concurrent.ExecutionContext;\n import scala.concurrent.ExecutionContext$;\n@@ -144,6 +148,53 @@ public static void waitUntilExecutionState(Execution execution, ExecutionState s\n \t\t}\n \t}\n \n+\t/**\n+\t * Waits until all executions fulfill the given predicate.\n+\t *\n+\t * @param executionGraph for which to check the executions\n+\t * @param executionPredicate predicate which is to be fulfilled\n+\t * @param maxWaitMillis timeout for the wait operation\n+\t * @throws TimeoutException if the executions did not reach the target state in time\n+\t */\n+\tpublic static void waitForAllExecutionsPredicate(\n+\t\t\tExecutionGraph executionGraph,\n+\t\t\tPredicate<Execution> executionPredicate,\n+\t\t\tlong maxWaitMillis) throws TimeoutException {\n+\t\tfinal Iterable<ExecutionVertex> allExecutionVertices = executionGraph.getAllExecutionVertices();\n+\n+\t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofMillis(maxWaitMillis));\n+\t\tboolean predicateResult;\n+\n+\t\tdo {\n+\t\t\tpredicateResult = true;\n+\t\t\tfor (ExecutionVertex executionVertex : allExecutionVertices) {\n+\t\t\t\tfinal Execution currentExecution = executionVertex.getCurrentExecutionAttempt();\n+\n+\t\t\t\tif (currentExecution == null || !executionPredicate.test(currentExecution)) {\n+\t\t\t\t\tpredicateResult = false;\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (!predicateResult) {\n+\t\t\t\ttry {\n+\t\t\t\t\tThread.sleep(2L);\n+\t\t\t\t} catch (InterruptedException ignored) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} while (!predicateResult && deadline.hasTimeLeft());\n+\n+\t\tif (!predicateResult) {\n+\t\t\tthrow new TimeoutException(\"Not all executions fulfilled the predicate in time.\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Predicate which is true if the given {@link Execution} has a resource assigned.\n+\t */\n+\tpublic static final Predicate<Execution> hasResourceAssigned = (Execution execution) -> execution.getAssignedResource() != null;\n+\n \tpublic static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus status, long maxWaitMillis)\n \t\t\tthrows TimeoutException {\n \n@@ -165,6 +216,15 @@ public static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus\n \t\t}\n \t}\n \n+\tpublic static void failExecutionGraph(ExecutionGraph executionGraph, Exception cause) {\n+\t\texecutionGraph.getAllExecutionVertices().iterator().next().fail(cause);\n+\t\tassertEquals(JobStatus.FAILING, executionGraph.getState());\n+\n+\t\tfor (ExecutionVertex vertex : executionGraph.getAllExecutionVertices()) {\n+\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n+\t\t}\n+\t}\n+\n \t/**\n \t * Takes all vertices in the given ExecutionGraph and switches their current\n \t * execution to RUNNING.\n@@ -382,6 +442,13 @@ public static Instance getInstance(final TaskManagerGateway gateway, final int n\n \t\treturn new Instance(gateway, connection, new InstanceID(), hardwareDescription, numberOfSlots);\n \t}\n \n+\tpublic static JobVertex createJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n+\t\tJobVertex groupVertex = new JobVertex(task1);\n+\t\tgroupVertex.setInvokableClass(invokable);\n+\t\tgroupVertex.setParallelism(numTasks);\n+\t\treturn groupVertex;\n+\t}\n+\n \t@SuppressWarnings(\"serial\")\n \tpublic static class SimpleActorGateway extends BaseTestingActorGateway {\n ",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "sha": "9cfe90ec070763028624cfc4c5a9b184db00304c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "patch": "@@ -72,11 +72,11 @@\n \n \tprotected TestingSlotProvider testingSlotProvider;\n \n-\tprivate SchedulerType schedulerType;\n+\tprotected SchedulerType schedulerType;\n \n \tprivate RpcService rpcService;\n \n-\tenum SchedulerType {\n+\tpublic enum SchedulerType {\n \t\tSCHEDULER,\n \t\tSLOT_POOL\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "sha": "3d5441293bd5ae25470df4724a762b46be51c1cd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "patch": "@@ -23,12 +23,11 @@\n import org.apache.flink.runtime.executiongraph.Execution;\n import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;\n import org.apache.flink.runtime.executiongraph.ExecutionVertex;\n-import org.apache.flink.runtime.instance.DummyActorGateway;\n+import org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n-import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n \n import java.net.InetAddress;\n@@ -75,7 +74,7 @@ public static Instance getRandomInstance(int numSlots) {\n \t\tHardwareDescription resources = new HardwareDescription(4, 4*GB, 3*GB, 2*GB);\n \t\t\n \t\treturn new Instance(\n-\t\t\tnew ActorTaskManagerGateway(DummyActorGateway.INSTANCE),\n+\t\t\tnew SimpleAckingTaskManagerGateway(),\n \t\t\tci,\n \t\t\tnew InstanceID(),\n \t\t\tresources,",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "sha": "3c074d1b6e0f4cef175d06c0baa8b9a457ffef3b",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9458] Ignore SharedSlot in CoLocationConstraint when not using legacy mode\n\nThe SharedSlot in CoLocationConstraint is only set when using the legacy mode. Thus,\nCoLocationConstraint#isAssignedAlive should only check the SharedSlot if it was\npreviously set. This fixes the NPE when restarting a job with a co-location constraint\nwhen using the new mode.\n\nThis closes #6119.",
        "parent": "https://github.com/apache/flink/commit/fdabce0339a5ba89787f8719e463cd01f35b4601",
        "repo": "flink",
        "unit_tests": [
            "CoLocationConstraintTest.java"
        ]
    },
    "flink_ccd574a": {
        "bug_id": "flink_ccd574a",
        "commit": "https://github.com/apache/flink/commit/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "patch": "@@ -328,7 +328,7 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t// register this execution at the execution graph, to receive call backs\n \t\t\tvertex.getExecutionGraph().registerExecution(this);\n \n-\t\t\tInstance instance = slot.getInstance();\n+\t\t\tfinal Instance instance = slot.getInstance();\n \t\t\tFuture<Object> deployAction = Patterns.ask(instance.getTaskManager(),\n \t\t\t\t\tnew SubmitTask(deployment), new Timeout(timeout));\n \n@@ -338,7 +338,9 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t\tpublic void onComplete(Throwable failure, Object success) throws Throwable {\n \t\t\t\t\tif (failure != null) {\n \t\t\t\t\t\tif (failure instanceof TimeoutException) {\n-\t\t\t\t\t\t\tmarkFailed(new Exception(\"Cannot deploy task - TaskManager not responding.\", failure));\n+\t\t\t\t\t\t\tmarkFailed(new Exception(\n+\t\t\t\t\t\t\t\t\t\"Cannot deploy task - TaskManager \" + instance + \" not responding.\",\n+\t\t\t\t\t\t\t\t\tfailure));\n \t\t\t\t\t\t}\n \t\t\t\t\t\telse {\n \t\t\t\t\t\t\tmarkFailed(failure);",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "sha": "baed9474dcefaff1d5bfcf97dde0a280334ce4d6",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "patch": "@@ -68,10 +68,6 @@ public InputSplit getNextInputSplit() {\n \n \t\t\tfinal Object result = Await.result(response, timeout.duration());\n \n-\t\t\tif (result == null) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\n \t\t\tif(!(result instanceof JobManagerMessages.NextInputSplit)){\n \t\t\t\tthrow new RuntimeException(\"RequestNextInputSplit requires a response of type \" +\n \t\t\t\t\t\t\"NextInputSplit. Instead response is of type \" + result.getClass() + \".\");\n@@ -80,9 +76,14 @@ public InputSplit getNextInputSplit() {\n \t\t\t\t\t\t(JobManagerMessages.NextInputSplit) result;\n \n \t\t\t\tbyte[] serializedData = nextInputSplit.splitData();\n-\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n-\t\t\t\t\t\tusercodeClassLoader);\n-\t\t\t\treturn (InputSplit) deserialized;\n+\n+\t\t\t\tif(serializedData == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t} else {\n+\t\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n+\t\t\t\t\t\t\tusercodeClassLoader);\n+\t\t\t\t\treturn (InputSplit) deserialized;\n+\t\t\t\t}\n \t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Requesting the next InputSplit failed.\", e);",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "sha": "5a698509c9ed068135a2c26cc2881620882cccc6",
                "status": "modified"
            },
            {
                "additions": 93,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "patch": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.taskmanager;\n+\n+import akka.actor.ActorRef;\n+import akka.actor.ActorSystem;\n+import akka.actor.Props;\n+import akka.actor.Status;\n+import akka.actor.UntypedActor;\n+import akka.testkit.JavaTestKit;\n+import akka.util.Timeout;\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.messages.JobManagerMessages;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+public class TaskInputSplitProviderTest {\n+\n+\tprivate static ActorSystem system;\n+\n+\t@BeforeClass\n+\tpublic static void setup() throws Exception {\n+\t\tsystem = ActorSystem.create(\"TestActorSystem\", TestingUtils.testConfig());\n+\t}\n+\n+\t@AfterClass\n+\tpublic static void teardown() throws Exception {\n+\t\tJavaTestKit.shutdownActorSystem(system);\n+\t\tsystem = null;\n+\t}\n+\n+\t@Test\n+\tpublic void testRequestNextInputSplitWithInvalidExecutionID() {\n+\n+\t\tfinal JobID jobID = new JobID();\n+\t\tfinal JobVertexID vertexID = new JobVertexID();\n+\t\tfinal ExecutionAttemptID executionID = new ExecutionAttemptID();\n+\t\tfinal Timeout timeout = new Timeout(10, TimeUnit.SECONDS);\n+\n+\t\tfinal ActorRef jobManagerRef = system.actorOf(Props.create(NullInputSplitJobManager.class));\n+\n+\t\tfinal TaskInputSplitProvider provider = new TaskInputSplitProvider(\n+\t\t\t\tjobManagerRef,\n+\t\t\t\tjobID,\n+\t\t\t\tvertexID,\n+\t\t\t\texecutionID,\n+\t\t\t\tgetClass().getClassLoader(),\n+\t\t\t\ttimeout\n+\t\t);\n+\n+\t\t// The jobManager will return a\n+\t\tInputSplit nextInputSplit = provider.getNextInputSplit();\n+\n+\t\tassertTrue(nextInputSplit == null);\n+\t}\n+\n+\tpublic static class NullInputSplitJobManager extends UntypedActor {\n+\n+\t\t@Override\n+\t\tpublic void onReceive(Object message) throws Exception {\n+\t\t\tif(message instanceof JobManagerMessages.RequestNextInputSplit) {\n+\t\t\t\tsender().tell(new JobManagerMessages.NextInputSplit(null), getSelf());\n+\t\t\t} else {\n+\t\t\t\tsender().tell(new Status.Failure(new Exception(\"Invalid message type\")), getSelf());\n+\t\t\t}\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "sha": "f0689789a2d223e6ac814bbf774b38da27786595",
                "status": "added"
            }
        ],
        "message": "[FLINK-1922] [runtime] Fixes NPE when TM receives a null input split\n\nThis closes #631",
        "parent": "https://github.com/apache/flink/commit/9a18e579021304f6ee0687cd1c9579740b11b98d",
        "repo": "flink",
        "unit_tests": [
            "ExecutionTest.java"
        ]
    },
    "flink_d6435e8": {
        "bug_id": "flink_d6435e8",
        "commit": "https://github.com/apache/flink/commit/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "patch": "@@ -127,8 +127,9 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> where(IterativeCondition<F> condition) {\n-\t\tClosureCleaner.clean(condition, true);\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n \n+\t\tClosureCleaner.clean(condition, true);\n \t\tif (this.condition == null) {\n \t\t\tthis.condition = condition;\n \t\t} else {\n@@ -148,6 +149,8 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> or(IterativeCondition<F> condition) {\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n+\n \t\tClosureCleaner.clean(condition, true);\n \n \t\tif (this.condition == null) {\n@@ -167,6 +170,8 @@ public Quantifier getQuantifier() {\n \t * @return The same pattern with the new subtype constraint\n \t */\n \tpublic <S extends F> Pattern<T, S> subtype(final Class<S> subtypeClass) {\n+\t\tPreconditions.checkNotNull(subtypeClass, \"The class cannot be null.\");\n+\n \t\tif (condition == null) {\n \t\t\tthis.condition = new SubtypeCondition<F>(subtypeClass);\n \t\t} else {",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "sha": "b100bc5f80135981aa94027bb068a0f75a2ceb51",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 2,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code AND} and returns {@code true} if both are {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic AndCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "sha": "ac34c41301b9e24c2c5818f94b4a12dfad5f930f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "patch": "@@ -35,6 +35,6 @@ public NotCondition(final IterativeCondition<T> original) {\n \n \t@Override\n \tpublic boolean filter(T value, Context<T> ctx) throws Exception {\n-\t\treturn !original.filter(value, ctx);\n+\t\treturn original != null && !original.filter(value, ctx);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "sha": "9318c2f67726fb2cb6d30c1bad6824c302e2fc39",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 2,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code OR} and returns {@code true} if at least one is {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic OrCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "sha": "d3690ab4da065734b933c95dd954dc559798501d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which filters elements of the given type.\n  * An element is filtered out iff it is not assignable to the given subtype of {@code T}.\n@@ -31,7 +33,7 @@\n \tprivate final Class<? extends T> subtype;\n \n \tpublic SubtypeCondition(final Class<? extends T> subtype) {\n-\t\tthis.subtype = subtype;\n+\t\tthis.subtype = Preconditions.checkNotNull(subtype, \"The subtype cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "sha": "cff8693c588aacb7f1ed0d702b59e50fb768d4a3",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 0,
                "filename": "flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "patch": "@@ -50,6 +50,72 @@\n @SuppressWarnings(\"unchecked\")\n public class NFAITCase extends TestLogger {\n \n+\t@Test\n+\tpublic void testNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedBy(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n+\t@Test\n+\tpublic void testAnyWithNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedByAny(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(a, c),\n+\t\t\t\tLists.newArrayList(a, d),\n+\t\t\t\tLists.newArrayList(a, e),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(b, d),\n+\t\t\t\tLists.newArrayList(b, e),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(c, e),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n \t@Test\n \tpublic void testSimplePatternNFA() {\n \t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "sha": "fe31564af30af948fa8149b5a62215f800ab3bb2",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6445] [cep] Fix NPE in no-condition patterns.",
        "parent": "https://github.com/apache/flink/commit/a2ec3ee664b540c1213991d7fcf56d8873e60d40",
        "repo": "flink",
        "unit_tests": [
            "PatternTest.java"
        ]
    },
    "flink_d7911c5": {
        "bug_id": "flink_d7911c5",
        "commit": "https://github.com/apache/flink/commit/d7911c5a8a6896261c55b61ea4633e706270baa1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java?ref=d7911c5a8a6896261c55b61ea4633e706270baa1",
                "deletions": 2,
                "filename": "flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "patch": "@@ -414,8 +414,10 @@ private void initFileSystem() throws IOException {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n-\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\tif (state != null) {\n+\t\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n+\t\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\t\t}\n \t\t}\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "sha": "db0a5d859bed4592627860748d687762a35964a7",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6294] Fix potential NPE in BucketingSink.close()",
        "parent": "https://github.com/apache/flink/commit/d7c2c417213502130b1aeab1868313df178555cc",
        "repo": "flink",
        "unit_tests": [
            "BucketingSinkTest.java"
        ]
    },
    "flink_ecde6c3": {
        "bug_id": "flink_ecde6c3",
        "commit": "https://github.com/apache/flink/commit/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "patch": "@@ -157,6 +157,10 @@ public int getPartitionRequestMaxBackoff() {\n \t\treturn partitionRequestMaxBackoff;\n \t}\n \n+\tpublic boolean isCreditBased() {\n+\t\treturn enableCreditBased;\n+\t}\n+\n \tpublic KvStateRegistry getKvStateRegistry() {\n \t\treturn kvStateRegistry;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "sha": "f25475638723d8d364fe5e04c3b6b05bd2eed56d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 5,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "patch": "@@ -157,9 +157,11 @@\n \t */\n \tprivate BufferPool bufferPool;\n \n-\t/** Global network buffer pool to request and recycle exclusive buffers. */\n+\t/** Global network buffer pool to request and recycle exclusive buffers (only for credit-based). */\n \tprivate NetworkBufferPool networkBufferPool;\n \n+\tprivate final boolean isCreditBased;\n+\n \tprivate boolean hasReceivedAllEndOfPartitionEvents;\n \n \t/** Flag indicating whether partitions have been requested. */\n@@ -189,7 +191,8 @@ public SingleInputGate(\n \t\tint consumedSubpartitionIndex,\n \t\tint numberOfInputChannels,\n \t\tTaskActions taskActions,\n-\t\tTaskIOMetricGroup metrics) {\n+\t\tTaskIOMetricGroup metrics,\n+\t\tboolean isCreditBased) {\n \n \t\tthis.owningTaskName = checkNotNull(owningTaskName);\n \t\tthis.jobId = checkNotNull(jobId);\n@@ -208,6 +211,7 @@ public SingleInputGate(\n \t\tthis.enqueuedInputChannelsWithData = new BitSet(numberOfInputChannels);\n \n \t\tthis.taskActions = checkNotNull(taskActions);\n+\t\tthis.isCreditBased = isCreditBased;\n \t}\n \n \t// ------------------------------------------------------------------------\n@@ -288,6 +292,7 @@ public void setBufferPool(BufferPool bufferPool) {\n \t * @param networkBuffersPerChannel The number of exclusive buffers for each channel\n \t */\n \tpublic void assignExclusiveSegments(NetworkBufferPool networkBufferPool, int networkBuffersPerChannel) throws IOException {\n+\t\tcheckState(this.isCreditBased, \"Bug in input gate setup logic: exclusive buffers only exist with credit-based flow control.\");\n \t\tcheckState(this.networkBufferPool == null, \"Bug in input gate setup logic: global buffer pool has\" +\n \t\t\t\"already been set for this input gate.\");\n \n@@ -347,8 +352,13 @@ public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOE\n \t\t\t\t}\n \t\t\t\telse if (partitionLocation.isRemote()) {\n \t\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n-\t\t\t\t\t((RemoteInputChannel)newChannel).assignExclusiveSegments(\n-\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\n+\t\t\t\t\tif (this.isCreditBased) {\n+\t\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n+\t\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n+\t\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n+\t\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n@@ -661,7 +671,7 @@ public static SingleInputGate create(\n \n \t\tfinal SingleInputGate inputGate = new SingleInputGate(\n \t\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n-\t\t\ticdd.length, taskActions, metrics);\n+\t\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n \n \t\t// Create the input channels. There is one input channel for each consumed partition.\n \t\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "sha": "06e80ff531aaff98c8de7fd0ecc221e800578bf0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "patch": "@@ -329,7 +329,7 @@ private static ResultPartition createResultPartition(\n \t *\n \t * @return input gate with some fake settings\n \t */\n-\tprivate static SingleInputGate createSingleInputGate(\n+\tprivate SingleInputGate createSingleInputGate(\n \t\t\tfinal ResultPartitionType partitionType, final int channels) {\n \t\treturn spy(new SingleInputGate(\n \t\t\t\"Test Task Name\",\n@@ -339,7 +339,8 @@ private static SingleInputGate createSingleInputGate(\n \t\t\t0,\n \t\t\tchannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()));\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl));\n \t}\n \n \tprivate static void createRemoteInputChannel(",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "sha": "f790b5f02b960c85f954b7778be1b4d99f1366d4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "patch": "@@ -221,7 +221,8 @@ static SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "sha": "842aed8186d2ee53f34901ed0c5ef110d2a1626c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "patch": "@@ -66,7 +66,8 @@ public void testConsumptionWithLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -102,7 +103,8 @@ public void testConsumptionWithRemoteChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tRemoteInputChannel channel = new RemoteInputChannel(\n@@ -151,7 +153,8 @@ public void testConsumptionWithMixedChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0, local = 0; i < numChannels; i++) {\n \t\t\tif (localOrRemote.get(i)) {",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "sha": "73f3cfbc49a392925f56a09cab1dd3ccda650c18",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 6,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "patch": "@@ -93,7 +93,8 @@ public void testFairConsumptionLocalChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -146,7 +147,8 @@ public void testFairConsumptionLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -196,7 +198,8 @@ public void testFairConsumptionRemoteChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -251,7 +254,8 @@ public void testFairConsumptionRemoteChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -349,11 +353,12 @@ public FairnessVerifyingInputGate(\n \t\t\t\tint consumedSubpartitionIndex,\n \t\t\t\tint numberOfInputChannels,\n \t\t\t\tTaskActions taskActions,\n-\t\t\t\tTaskIOMetricGroup metrics) {\n+\t\t\t\tTaskIOMetricGroup metrics,\n+\t\t\t\tboolean isCreditBased) {\n \n \t\t\tsuper(owningTaskName, jobId, consumedResultId, ResultPartitionType.PIPELINED,\n \t\t\t\tconsumedSubpartitionIndex,\n-\t\t\t\t\tnumberOfInputChannels, taskActions, metrics);\n+\t\t\t\t\tnumberOfInputChannels, taskActions, metrics, isCreditBased);\n \n \t\t\ttry {\n \t\t\t\tField f = SingleInputGate.class.getDeclaredField(\"inputChannelsWithData\");",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "sha": "82a27cc92c0255b97102f4d95910534b73252041",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "patch": "@@ -293,7 +293,8 @@ public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue\n \t\t);\n \n \t\tResultPartitionManager partitionManager = mock(ResultPartitionManager.class);\n@@ -490,7 +491,8 @@ public TestLocalInputChannelConsumer(\n \t\t\t\t\tsubpartitionIndex,\n \t\t\t\t\tnumberOfInputChannels,\n \t\t\t\t\tmock(TaskActions.class),\n-\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\t\ttrue);\n \n \t\t\t// Set buffer pool\n \t\t\tinputGate.setBufferPool(bufferPool);",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "sha": "1ecb67ff82c62f50a753a7c519081d7e37e647fb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "patch": "@@ -889,7 +889,8 @@ private SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \tprivate RemoteInputChannel createRemoteInputChannel(SingleInputGate inputGate)",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "sha": "802cb936c09493db28adb8820bce809fc37a77cf",
                "status": "modified"
            },
            {
                "additions": 248,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "changes": 318,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 70,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "patch": "@@ -19,13 +19,13 @@\n package org.apache.flink.runtime.io.network.partition.consumer;\n \n import org.apache.flink.api.common.JobID;\n-import org.apache.flink.core.memory.MemorySegment;\n import org.apache.flink.core.memory.MemorySegmentFactory;\n import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.ResultPartitionLocation;\n import org.apache.flink.runtime.event.TaskEvent;\n import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n import org.apache.flink.runtime.io.network.ConnectionID;\n import org.apache.flink.runtime.io.network.ConnectionManager;\n import org.apache.flink.runtime.io.network.LocalConnectionManager;\n@@ -45,31 +45,51 @@\n import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;\n import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.query.KvStateRegistry;\n import org.apache.flink.runtime.taskmanager.TaskActions;\n \n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyInt;\n-import static org.mockito.Matchers.anyListOf;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+/**\n+ * Tests for {@link SingleInputGate}.\n+ */\n+@RunWith(Parameterized.class)\n public class SingleInputGateTest {\n \n+\t@Parameterized.Parameter\n+\tpublic boolean enableCreditBasedFlowControl;\n+\n+\t@Parameterized.Parameters(name = \"Credit-based = {0}\")\n+\tpublic static List<Boolean> parameters() {\n+\t\treturn Arrays.asList(Boolean.TRUE, Boolean.FALSE);\n+\t}\n+\n \t/**\n \t * Tests basic correctness of buffer-or-event interleaving and correct <code>null</code> return\n \t * value after receiving all end-of-partition events.\n@@ -324,12 +344,7 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\tint initialBackoff = 137;\n \t\tint maxBackoff = 1001;\n \n-\t\tNetworkEnvironment netEnv = mock(NetworkEnvironment.class);\n-\t\twhen(netEnv.getResultPartitionManager()).thenReturn(new ResultPartitionManager());\n-\t\twhen(netEnv.getTaskEventDispatcher()).thenReturn(new TaskEventDispatcher());\n-\t\twhen(netEnv.getPartitionRequestInitialBackoff()).thenReturn(initialBackoff);\n-\t\twhen(netEnv.getPartitionRequestMaxBackoff()).thenReturn(maxBackoff);\n-\t\twhen(netEnv.getConnectionManager()).thenReturn(new LocalConnectionManager());\n+\t\tfinal NetworkEnvironment netEnv = createNetworkEnvironment(2, 8, initialBackoff, maxBackoff);\n \n \t\tSingleInputGate gate = SingleInputGate.create(\n \t\t\t\"TestTask\",\n@@ -340,37 +355,43 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\t\tmock(TaskActions.class),\n \t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n \n-\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n+\t\ttry {\n+\t\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n \n-\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n+\t\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n \n-\t\tassertEquals(3, channelMap.size());\n-\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n-\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n+\t\t\tassertEquals(3, channelMap.size());\n+\t\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n+\t\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n \n-\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n-\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n+\t\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n+\t\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n \n-\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n-\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n+\t\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n+\t\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n \n-\t\tInputChannel[] channels = new InputChannel[]{localChannel, remoteChannel, unknownChannel};\n-\t\tfor (InputChannel ch : channels) {\n-\t\t\tassertEquals(0, ch.getCurrentBackoff());\n+\t\t\tInputChannel[] channels =\n+\t\t\t\tnew InputChannel[] {localChannel, remoteChannel, unknownChannel};\n+\t\t\tfor (InputChannel ch : channels) {\n+\t\t\t\tassertEquals(0, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tgate.releaseAllResources();\n+\t\t\tnetEnv.shutdown();\n \t\t}\n \t}\n \n@@ -379,26 +400,39 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithRemoteInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = new SingleInputGate(\n-\t\t\t\"t1\",\n-\t\t\tnew JobID(),\n-\t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED_BOUNDED,\n-\t\t\t0,\n-\t\t\t1,\n-\t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n-\n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tinputGate.setInputChannel(new IntermediateResultPartitionID(), remote);\n-\n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\t// Trigger requests of segments from global pool and assign buffers to remote input channel\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n-\n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\taddRemoteInputChannel(network, inputGate, connectionId, resultPartitionId, 0);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t/**\n@@ -407,51 +441,195 @@ public void testRequestBuffersWithRemoteInputChannel() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithUnknownInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = createInputGate(1);\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, extraNetworkBuffersPerGate, 0, 0);\n \n-\t\tUnknownInputChannel unknown = mock(UnknownInputChannel.class);\n-\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n-\t\tinputGate.setInputChannel(resultPartitionId.getPartitionId(), unknown);\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, resultPartitionId, 0);\n \n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n-\t\twhen(unknown.toRemoteInputChannel(connectionId)).thenReturn(remote);\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n \n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool, times(0)).requestMemorySegments(buffersPerChannel);\n \n-\t\t// Trigger updates to remote input channel from unknown input channel\n-\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n-\t\t\tresultPartitionId,\n-\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments(),\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tresultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n+\t}\n \n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t/**\n+\t * Tests that input gate can successfully convert unknown input channels into local and remote\n+\t * channels.\n+\t */\n+\t@Test\n+\tpublic void testUpdateUnknownInputChannel() throws Exception {\n+\t\tfinal SingleInputGate inputGate = createInputGate(2);\n+\t\tint buffersPerChannel = 2;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, 8, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID localResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, localResultPartitionId, 0);\n+\n+\t\t\tfinal ResultPartitionID remoteResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, remoteResultPartitionId, 1);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID remoteConnectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tremoteResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(remoteConnectionId)));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to local input channel from unknown input channel\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tlocalResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createLocal()));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((LocalInputChannel.class))));\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t// ---------------------------------------------------------------------------------------------\n \n-\tprivate static SingleInputGate createInputGate() {\n+\tprivate NetworkEnvironment createNetworkEnvironment(\n+\t\t\tint buffersPerChannel,\n+\t\t\tint extraNetworkBuffersPerGate,\n+\t\t\tint initialBackoff,\n+\t\t\tint maxBackoff) {\n+\t\treturn new NetworkEnvironment(\n+\t\t\tspy(new NetworkBufferPool(100, 32)),\n+\t\t\tnew LocalConnectionManager(),\n+\t\t\tnew ResultPartitionManager(),\n+\t\t\tnew TaskEventDispatcher(),\n+\t\t\tnew KvStateRegistry(),\n+\t\t\tnull,\n+\t\t\tnull,\n+\t\t\tIOManager.IOMode.SYNC,\n+\t\t\tinitialBackoff,\n+\t\t\tmaxBackoff,\n+\t\t\tbuffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate,\n+\t\t\tenableCreditBasedFlowControl);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate() {\n \t\treturn createInputGate(2);\n \t}\n \n-\tprivate static SingleInputGate createInputGate(int numberOfInputChannels) {\n+\tprivate SingleInputGate createInputGate(int numberOfInputChannels) {\n+\t\treturn createInputGate(numberOfInputChannels, ResultPartitionType.PIPELINED);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate(\n+\t\t\tint numberOfInputChannels, ResultPartitionType partitionType) {\n \t\tSingleInputGate inputGate = new SingleInputGate(\n \t\t\t\"Test Task Name\",\n \t\t\tnew JobID(),\n \t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED,\n+\t\t\tpartitionType,\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl);\n \n-\t\tassertEquals(ResultPartitionType.PIPELINED, inputGate.getConsumedPartitionType());\n+\t\tassertEquals(partitionType, inputGate.getConsumedPartitionType());\n \n \t\treturn inputGate;\n \t}\n \n+\tprivate void addUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tUnknownInputChannel unknown =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), unknown);\n+\t}\n+\n+\tprivate UnknownInputChannel createUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\treturn new UnknownInputChannel(\n+\t\t\tinputGate,\n+\t\t\tchannelIndex,\n+\t\t\tpartitionId,\n+\t\t\tnetwork.getResultPartitionManager(),\n+\t\t\tnetwork.getTaskEventDispatcher(),\n+\t\t\tnetwork.getConnectionManager(),\n+\t\t\tnetwork.getPartitionRequestInitialBackoff(),\n+\t\t\tnetwork.getPartitionRequestMaxBackoff(),\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t);\n+\t}\n+\n+\tprivate void addRemoteInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tConnectionID connectionId,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tRemoteInputChannel remote =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex)\n+\t\t\t\t.toRemoteInputChannel(connectionId);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), remote);\n+\t}\n+\n \tstatic void verifyBufferOrEvent(\n \t\t\tInputGate inputGate,\n \t\t\tboolean expectedIsBuffer,",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "sha": "c24466880369b5ba00ddaf0d8efc626145e55bdb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "patch": "@@ -60,7 +60,8 @@ public TestSingleInputGate(int numberOfInputChannels, boolean initialize) {\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tthis.inputGate = spy(realGate);\n ",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "sha": "33dc1ca205dc84fe92134c8db734151ddf61274f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "patch": "@@ -50,13 +50,15 @@ public void testBasicGetNextLogic() throws Exception {\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 3,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t\tfinal SingleInputGate ig2 = new SingleInputGate(\n \t\t\ttestTaskName, new JobID(),\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 5,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tfinal UnionInputGate union = new UnionInputGate(new SingleInputGate[]{ig1, ig2});\n ",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "sha": "081d97d5cbe409e05d350c0c17ce0d68ab90b339",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9256] [network] Fix NPE in SingleInputGate#updateInputChannel() for non-credit based flow control\n\nThis closes #5914",
        "parent": "https://github.com/apache/flink/commit/b957bf26fb588cab072e51240e9026456b862ce7",
        "repo": "flink",
        "unit_tests": [
            "SingleInputGateTest.java",
            "TestSingleInputGate.java"
        ]
    },
    "flink_eece0dd": {
        "bug_id": "flink_eece0dd",
        "commit": "https://github.com/apache/flink/commit/eece0dd05bc38b88fcb6cbcef15add7f98eab456",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java?ref=eece0dd05bc38b88fcb6cbcef15add7f98eab456",
                "deletions": 2,
                "filename": "flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "patch": "@@ -291,9 +291,9 @@ public void commitSpecificOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets)\n \t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n \t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n \t\t\t// This does not affect Flink's checkpoints/saved state.\n-\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition()) + 1;\n+\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition());\n \t\t\tif (offsetToCommit != null) {\n-\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit + 1));\n \t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "sha": "ad7efa28014ffe6ac21c5d26f0b98b8fd2cb1ef5",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [kafka] Fix NPE in Kafka09Fetcher",
        "parent": "https://github.com/apache/flink/commit/72e6b760fd951764c3ecc6fc191dc99a42d55e0b",
        "repo": "flink",
        "unit_tests": [
            "Kafka09FetcherTest.java"
        ]
    },
    "flink_f486a3f": {
        "bug_id": "flink_f486a3f",
        "commit": "https://github.com/apache/flink/commit/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java?ref=f486a3fd6ed80b67e8eeed9245ad37b6b0be740b",
                "deletions": 3,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "patch": "@@ -20,9 +20,9 @@\n \n import org.apache.flink.runtime.io.network.NetworkSequenceViewReader;\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n+import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CancelPartitionRequest;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CloseRequest;\n-import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionProvider;\n import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;\n@@ -99,12 +99,12 @@ protected void channelRead0(ChannelHandlerContext ctx, NettyMessage msg) throws\n \t\t\t\t\t\t\toutboundQueue);\n \t\t\t\t\t}\n \n-\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n-\n \t\t\t\t\treader.requestSubpartitionView(\n \t\t\t\t\t\tpartitionProvider,\n \t\t\t\t\t\trequest.partitionId,\n \t\t\t\t\t\trequest.queueIndex);\n+\n+\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n \t\t\t\t} catch (PartitionNotFoundException notFound) {\n \t\t\t\t\trespondWithError(ctx, notFound, request.receiverId);\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "sha": "e9ee10cbc4d953248955811899711ba3bfd12208",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9057][network] fix an NPE when cleaning up before requesting a subpartition view\n\nIn PartitionRequestServerHandler, the view reader was created and immediately\nafterwards added to the PartitionRequestQueue which would attempt a cleanup of\nthe view reader's subpartition view. This view, however, was currently only\ncreated after adding the reader to the PartitionRequestQueue and may thus result\nin a NullPointerException if the cleanup happens very early in the\ninitialization phase, e.g. due to failures.\n\nThis closes #5747.",
        "parent": "https://github.com/apache/flink/commit/41ae13122bd1ca16f1c6779983dc0d17e3633e97",
        "repo": "flink",
        "unit_tests": [
            "PartitionRequestServerHandlerTest.java"
        ]
    }
}