{
    "hbase_00c1b87": {
        "bug_id": "hbase_00c1b87",
        "commit": "https://github.com/apache/hbase/commit/00c1b877e8bb104516139dbad4d70e27c67c688b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -11,6 +11,7 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n     HADOOP-1729 Recent renaming or META tables breaks hbase shell\n+    HADOOP-1730 unexpected null value causes META scanner to exit (silently)\n \n   IMPROVEMENTS\n ",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/CHANGES.txt",
                "sha": "5dcf4dea33cd7e7038a525923cfbc37daada696a",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 25,
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "patch": "@@ -46,7 +46,12 @@\n  * multiple HBase instances\n  */\n public class HConnectionManager implements HConstants {\n-  private HConnectionManager() {}                        // Not instantiable\n+  /*\n+   * Private. Not instantiable.\n+   */\n+  private HConnectionManager() {\n+    super();\n+  }\n   \n   // A Map of master HServerAddress -> connection information for that instance\n   // Note that although the Map is synchronized, the objects it contains\n@@ -298,28 +303,37 @@ public boolean tableExists(final Text tableName) {\n       }\n       SortedMap<Text, HRegionLocation> servers =\n         new TreeMap<Text, HRegionLocation>();\n-\n       servers.putAll(tableServers);\n       return servers;\n     }\n \n     /** {@inheritDoc} */\n     public SortedMap<Text, HRegionLocation>\n     reloadTableServers(final Text tableName) throws IOException {\n-      \n       closedTables.remove(tableName);\n-\n-      SortedMap<Text, HRegionLocation> servers =\n+      SortedMap<Text, HRegionLocation> tableServers =\n         new TreeMap<Text, HRegionLocation>();\n-      \n       // Reload information for the whole table\n-\n-      servers.putAll(findServersForTable(tableName));\n+      tableServers.putAll(findServersForTable(tableName));\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Result of findTable: \" + servers.toString());\n+        StringBuilder sb = new StringBuilder();\n+        int count = 0;\n+        for (HRegionLocation location: tableServers.values()) {\n+          if (sb.length() > 0) {\n+            sb.append(\" \");\n+          }\n+          sb.append(count++);\n+          sb.append(\". \");\n+          sb.append(\"address=\");\n+          sb.append(location.getServerAddress());\n+          sb.append(\", \");\n+          sb.append(location.getRegionInfo().getRegionName());\n+        }\n+        LOG.debug(\"Result of findTable on \" + tableName.toString() +\n+          \": \" + sb.toString());\n       }\n       \n-      return servers;\n+      return tableServers;\n     }\n \n     /** {@inheritDoc} */\n@@ -413,7 +427,7 @@ public void close(Text tableName) {\n         }\n       }\n       \n-      SortedMap<Text, HRegionLocation> servers =\n+      SortedMap<Text, HRegionLocation> srvrs =\n         new TreeMap<Text, HRegionLocation>();\n       \n       if (tableName.equals(ROOT_TABLE_NAME)) {\n@@ -428,7 +442,7 @@ public void close(Text tableName) {\n           if (tableServers == null) {\n             tableServers = locateRootRegion();\n           }\n-          servers.putAll(tableServers);\n+          srvrs.putAll(tableServers);\n         }\n         \n       } else if (tableName.equals(META_TABLE_NAME)) {\n@@ -459,7 +473,7 @@ public void close(Text tableName) {\n               }\n             }\n           }\n-          servers.putAll(tableServers);\n+          srvrs.putAll(tableServers);\n         }\n       } else {\n         boolean waited = false;\n@@ -486,7 +500,7 @@ public void close(Text tableName) {\n             if (tableServers == null) {\n               throw new TableNotFoundException(\"table not found: \" + tableName);\n             }\n-            servers.putAll(tableServers);\n+            srvrs.putAll(tableServers);\n           }\n         }\n         if (!waited) {\n@@ -504,7 +518,7 @@ public void close(Text tableName) {\n \n               for (HRegionLocation t: metaServers.values()) {\n                 try {\n-                  servers.putAll(scanOneMetaRegion(t, tableName));\n+                  srvrs.putAll(scanOneMetaRegion(t, tableName));\n \n                 } catch (IOException e) {\n                   if (tries < numRetries - 1) {\n@@ -528,15 +542,8 @@ public void close(Text tableName) {\n           }\n         }\n       }\n-      this.tablesToServers.put(tableName, servers);\n-      if (LOG.isDebugEnabled()) {\n-        int count = 0;\n-        for (Map.Entry<Text, HRegionLocation> e: servers.entrySet()) {\n-          LOG.debug(\"Region \" + (1 + count++) + \" of \" + servers.size() +\n-            \": \" + e.getValue());\n-        }\n-      }\n-      return servers;\n+      this.tablesToServers.put(tableName, srvrs);\n+      return srvrs;\n     }\n \n     /*\n@@ -598,7 +605,6 @@ public void close(Text tableName) {\n         try {\n           rootRegion.getRegionInfo(HGlobals.rootRegionInfo.regionName);\n           break;\n-          \n         } catch (IOException e) {\n           if (tries == numRetries - 1) {\n             // Don't bother sleeping. We've run out of retries.",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "sha": "089f5395c57662562960d296be0ce7e487c7e29b",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HMaster.java",
                "changes": 141,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 66,
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "patch": "@@ -262,109 +262,114 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n         }\n       }\n \n-      // Scan is finished.  Take a look at split parents to see if any we can clean up.\n-\n+      // Scan is finished.  Take a look at split parents to see if any we can\n+      // clean up.\n       if (splitParents.size() > 0) {\n         for (Map.Entry<HRegionInfo, SortedMap<Text, byte[]>> e:\n-          splitParents.entrySet()) {\n-          \n-          SortedMap<Text, byte[]> results = e.getValue();\n-          cleanupSplits(region.regionName, regionServer, e.getKey(), \n-              (HRegionInfo) Writables.getWritable(results.get(COL_SPLITA),\n-                  new HRegionInfo()),\n-              (HRegionInfo) Writables.getWritable(results.get(COL_SPLITB),\n-                  new HRegionInfo()));\n+            splitParents.entrySet()) {\n+          HRegionInfo hri = e.getKey();\n+          cleanupSplits(region.regionName, regionServer, hri, e.getValue());\n         }\n       }\n       LOG.info(Thread.currentThread().getName() + \" scan of meta region \" +\n           region.regionName + \" complete\");\n     }\n \n+    /*\n+     * @param info Region to check.\n+     * @return True if this is a split parent.\n+     */\n     private boolean isSplitParent(final HRegionInfo info) {\n-      boolean result = false;\n-\n-      // Skip if not a split region.\n-      \n       if (!info.isSplit()) {\n-        return result;\n+        return false;\n       }\n       if (!info.isOffline()) {\n         LOG.warn(\"Region is split but not offline: \" + info.regionName);\n       }\n       return true;\n     }\n \n-    /**\n-     * @param metaRegionName\n+    /*\n+     * If daughters no longer hold reference to the parents, delete the parent.\n+     * @param metaRegionName Meta region name.\n      * @param server HRegionInterface of meta server to talk to \n-     * @param info HRegionInfo of split parent\n-     * @param splitA low key range child region \n-     * @param splitB upper key range child region\n-     * @return True if we removed <code>info</code> and this region has\n-     * been cleaned up.\n+     * @param parent HRegionInfo of split parent\n+     * @param rowContent Content of <code>parent</code> row in\n+     * <code>metaRegionName</code>\n+     * @return True if we removed <code>parent</code> from meta table and from\n+     * the filesystem.\n      * @throws IOException\n      */\n     private boolean cleanupSplits(final Text metaRegionName, \n-        final HRegionInterface server, final HRegionInfo info,\n-        final HRegionInfo splitA, final HRegionInfo splitB) throws IOException {\n-    \n+        final HRegionInterface srvr, final HRegionInfo parent,\n+        SortedMap<Text, byte[]> rowContent)\n+    throws IOException {\n       boolean result = false;\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Checking \" + info.getRegionName() + \" to see if daughter \" +\n-        \"splits still hold references\");\n+        LOG.debug(\"Checking \" + parent.getRegionName() +\n+          \" to see if daughter splits still hold references\");\n       }\n-      boolean noReferencesA = splitA == null;\n-      boolean noReferencesB = splitB == null;\n+\n+      boolean hasReferencesA = hasReferences(metaRegionName, srvr,\n+          parent.getRegionName(), rowContent, COL_SPLITA);\n+      boolean hasReferencesB = hasReferences(metaRegionName, srvr,\n+          parent.getRegionName(), rowContent, COL_SPLITB);\n       \n-      if (!noReferencesA) {\n-        noReferencesA = hasReferences(metaRegionName, server,\n-          info.getRegionName(), splitA, COL_SPLITA);\n-      }\n-      if (!noReferencesB) {\n-        noReferencesB = hasReferences(metaRegionName, server,\n-          info.getRegionName(), splitB, COL_SPLITB);\n-      }\n-      if (!noReferencesA && !noReferencesB) {\n-        // No references.  Remove this item from table and deleted region on\n-        // disk.\n-        LOG.info(\"Deleting region \" + info.getRegionName() +\n+      if (!hasReferencesA && !hasReferencesB) {\n+        LOG.info(\"Deleting region \" + parent.getRegionName() +\n         \" because daughter splits no longer hold references\");\n-        \n-        if (!HRegion.deleteRegion(fs, dir, info.getRegionName())) {\n-          LOG.warn(\"Deletion of \" + info.getRegionName() + \" failed\");\n+\n+        if (!HRegion.deleteRegion(fs, dir, parent.getRegionName())) {\n+          LOG.warn(\"Deletion of \" + parent.getRegionName() + \" failed\");\n         }\n         \n         BatchUpdate b = new BatchUpdate();\n-        long lockid = b.startUpdate(info.getRegionName());\n+        long lockid = b.startUpdate(parent.getRegionName());\n         b.delete(lockid, COL_REGIONINFO);\n         b.delete(lockid, COL_SERVER);\n         b.delete(lockid, COL_STARTCODE);\n-        server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n+        srvr.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n         result = true;\n       }\n       \n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Done checking \" + info.getRegionName() + \": splitA: \" +\n-            noReferencesA + \", splitB: \"+ noReferencesB);\n+        LOG.debug(\"Done checking \" + parent.getRegionName() + \": splitA: \" +\n+            hasReferencesA + \", splitB: \"+ hasReferencesB);\n       }\n       return result;\n     }\n-\n+    \n+    /* \n+     * Checks if a daughter region -- either splitA or splitB -- still holds\n+     * references to parent.  If not, removes reference to the split from\n+     * the parent meta region row.\n+     * @param metaRegionName Name of meta region to look in.\n+     * @param srvr Where region resides.\n+     * @param parent Parent region name. \n+     * @param rowContent Keyed content of the parent row in meta region.\n+     * @param splitColumn Column name of daughter split to examine\n+     * @return True if still has references to parent.\n+     * @throws IOException\n+     */\n     protected boolean hasReferences(final Text metaRegionName, \n-        final HRegionInterface server, final Text regionName,\n-        final HRegionInfo split, final Text column) throws IOException {\n-      \n+      final HRegionInterface srvr, final Text parent,\n+      SortedMap<Text, byte[]> rowContent, final Text splitColumn)\n+    throws IOException {\n       boolean result = false;\n+      HRegionInfo split =\n+        Writables.getHRegionInfoOrNull(rowContent.get(splitColumn));\n+      if (split == null) {\n+        return result;\n+      }\n       for (Text family: split.getTableDesc().families().keySet()) {\n         Path p = HStoreFile.getMapDir(fs.makeQualified(dir),\n             split.getRegionName(), HStoreKey.extractFamily(family));\n-        \n-        // Look for reference files.\n-        \n+        // Look for reference files.  Call listPaths with an anonymous\n+        // instance of PathFilter.\n         Path [] ps = fs.listPaths(p,\n             new PathFilter () {\n-              public boolean accept(Path p) {\n-                return HStoreFile.isReference(p);\n+              public boolean accept(Path path) {\n+                return HStoreFile.isReference(path);\n               }\n             }\n         );\n@@ -381,13 +386,13 @@ public boolean accept(Path p) {\n       \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(split.getRegionName().toString()\n-            +\" no longer has references to \" + regionName.toString());\n+            +\" no longer has references to \" + parent.toString());\n       }\n       \n       BatchUpdate b = new BatchUpdate();\n-      long lockid = b.startUpdate(regionName);\n-      b.delete(lockid, column);\n-      server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n+      long lockid = b.startUpdate(parent);\n+      b.delete(lockid, splitColumn);\n+      srvr.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n         \n       return result;\n     }\n@@ -468,7 +473,6 @@ private void scanRoot() {\n                 HGlobals.rootRegionInfo.regionName, null));\n           }\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             try {\n@@ -485,6 +489,10 @@ private void scanRoot() {\n           } else {\n             LOG.error(\"Scan ROOT region\", e);\n           }\n+        } catch (Exception e) {\n+          // If for some reason we get some other kind of exception, \n+          // at least log it rather than go out silently.\n+          LOG.error(\"Unexpected exception\", e);\n         }\n         if (!closed) {\n           // sleep before retry\n@@ -597,19 +605,16 @@ private void scanOneMetaRegion(MetaRegion region) {\n \n         try {\n           // Don't interrupt us while we're working\n-\n           synchronized (metaScannerLock) {\n             scanRegion(region);\n             onlineMetaRegions.put(region.startKey, region);\n           }\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             try {\n               e = RemoteExceptionHandler.decodeRemoteException(\n                   (RemoteException) e);\n-\n             } catch (IOException ex) {\n               e = ex;\n             }\n@@ -620,10 +625,14 @@ private void scanOneMetaRegion(MetaRegion region) {\n           } else {\n             LOG.error(\"Scan one META region\", e);\n           }\n+        } catch (Exception e) {\n+          // If for some reason we get some other kind of exception, \n+          // at least log it rather than go out silently.\n+          LOG.error(\"Unexpected exception\", e);\n         }\n+        \n         if (!closed) {\n           // sleep before retry\n-\n           try {\n             Thread.sleep(threadWakeFrequency);                  \n           } catch (InterruptedException e) {",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HMaster.java",
                "sha": "deac39ed8b8b8c09080ec516d3c7c7c68a558d11",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -20,7 +20,9 @@\n package org.apache.hadoop.hbase;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import java.util.TreeMap;\n@@ -971,20 +973,25 @@ void internalFlushcache() throws IOException {\n    * @throws IOException\n    */\n   public HInternalScannerInterface getScanner(Text[] cols, Text firstRow,\n-      long timestamp, RowFilterInterface filter) throws IOException {\n+      long timestamp, RowFilterInterface filter)\n+  throws IOException {\n     lock.obtainReadLock();\n     try {\n       TreeSet<Text> families = new TreeSet<Text>();\n       for(int i = 0; i < cols.length; i++) {\n         families.add(HStoreKey.extractFamily(cols[i]));\n       }\n \n-      HStore[] storelist = new HStore[families.size()];\n-      int i = 0;\n+      List<HStore> storelist = new ArrayList<HStore>();\n       for (Text family: families) {\n-        storelist[i++] = stores.get(family);\n+        HStore s = stores.get(family);\n+        if (s == null) {\n+          continue;\n+        }\n+        storelist.add(stores.get(family));\n       }\n-      return new HScanner(cols, firstRow, timestamp, memcache, storelist, filter);\n+      return new HScanner(cols, firstRow, timestamp, memcache,\n+        storelist.toArray(new HStore [] {}), filter);\n     } finally {\n       lock.releaseReadLock();\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "fc08e62d42befadb6b42b21c29e05eb8872d534d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HTable.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTable.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/HTable.java",
                "patch": "@@ -586,11 +586,10 @@ public synchronized void commit(long lockid, long timestamp)\n         HRegionLocation r = getRegionLocation(batch.getRow());\n         HRegionInterface server =\n           connection.getHRegionConnection(r.getServerAddress());\n-\n         try {\n-          server.batchUpdate(r.getRegionInfo().getRegionName(), timestamp, batch);\n+          server.batchUpdate(r.getRegionInfo().getRegionName(), timestamp,\n+            batch);\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             e = RemoteExceptionHandler.decodeRemoteException(\n@@ -601,7 +600,6 @@ public synchronized void commit(long lockid, long timestamp)\n               LOG.debug(\"reloading table servers because: \" + e.getMessage());\n             }\n             tableServers = connection.reloadTableServers(tableName);\n-\n           } else {\n             throw e;\n           }\n@@ -628,6 +626,7 @@ public synchronized void commit(long lockid, long timestamp)\n    */\n   @Deprecated\n   public synchronized void renewLease(@SuppressWarnings(\"unused\") long lockid) {\n+    // noop\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HTable.java",
                "sha": "e95ecf1d3d73420666208775b673efe3c7666d7a",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Writables.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/util/Writables.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.io.Writable;\n \n import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n \n /**\n  * Utility class with methods for manipulating Writable objects\n@@ -89,6 +90,28 @@ public static Writable getWritable(final byte [] bytes, final Writable w)\n       in.close();\n     }\n   }\n+  \n+  /**\n+   * @param bytes\n+   * @return A HRegionInfo instance built out of passed <code>bytes</code>.\n+   * @throws IOException\n+   */\n+  public static HRegionInfo getHRegionInfo(final byte [] bytes)\n+  throws IOException {\n+    return (HRegionInfo)getWritable(bytes, new HRegionInfo());\n+  }\n+ \n+  /**\n+   * @param bytes\n+   * @return A HRegionInfo instance built out of passed <code>bytes</code>\n+   * or <code>null</code> if passed bytes are null or an empty array.\n+   * @throws IOException\n+   */\n+  public static HRegionInfo getHRegionInfoOrNull(final byte [] bytes)\n+  throws IOException {\n+    return (bytes == null || bytes.length <= 0)?\n+      (HRegionInfo)null: getHRegionInfo(bytes);\n+  }\n \n   /**\n    * Copy one Writable to another.  Copies bytes using data streams.",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "sha": "90d3148e2d8394208f1f4586814d8d7c91013aa4",
                "status": "modified"
            },
            {
                "additions": 112,
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "changes": 183,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestSplit.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "deletions": 71,
                "filename": "src/test/org/apache/hadoop/hbase/TestSplit.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.ConcurrentModificationException;\n+import java.util.Map;\n import java.util.SortedMap;\n import java.util.TreeMap;\n \n@@ -168,86 +169,63 @@ public void testSplitRegionIsDeleted() throws Exception {\n       int count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       t = new HTable(this.conf, new Text(getName()));\n       addContent(new HTableLoader(t), COLFAMILY_NAME3);\n-      // All is running in the one JVM so I should be able to get the\n+      // All is running in the one JVM so I should be able to get the single\n       // region instance and bring on a split.\n       HRegionInfo hri =\n         t.getRegionLocation(HConstants.EMPTY_START_ROW).getRegionInfo();\n-      HRegion r =\n-        cluster.regionThreads.get(0).getRegionServer().onlineRegions.get(\n-            hri.getRegionName());\n+      HRegion r = cluster.regionThreads.get(0).getRegionServer().\n+        onlineRegions.get(hri.getRegionName());\n       // Flush will provoke a split next time the split-checker thread runs.\n       r.flushcache(false);\n       // Now, wait until split makes it into the meta table.\n       for (int i = 0; i < retries &&\n-      (count(meta, HConstants.COLUMN_FAMILY_STR) <= count); i++) {\n+          (count(meta, HConstants.COLUMN_FAMILY_STR) <= count); i++) {\n         Thread.sleep(5000);\n       }\n       int oldCount = count;\n       count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       if (count <= oldCount) {\n         throw new IOException(\"Failed waiting on splits to show up\");\n       }\n-      HRegionInfo parent = getSplitParent(meta);\n+      // Get info on the parent from the meta table.  Pass in 'hri'. Its the\n+      // region we have been dealing with up to this. Its the parent of the\n+      // region split.\n+      Map<Text, byte []> data = getSplitParentInfo(meta, hri);\n+      HRegionInfo parent =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_REGIONINFO));\n       assertTrue(parent.isOffline());\n+      assertTrue(parent.isSplit());\n+      HRegionInfo splitA =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_SPLITA));\n+      HRegionInfo splitB =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_SPLITB));\n       Path parentDir = HRegion.getRegionDir(d, parent.getRegionName());\n       assertTrue(fs.exists(parentDir));\n-      LOG.info(\"Split happened and parent \" + parent.getRegionName() + \" is \" +\n-      \"offline\");\n-      for (int i = 0; i < retries; i++) {\n-        // Now open a scanner on the table. This will force HTable to recalibrate\n-        // and in doing so, will force us to wait until the new child regions\n-        // come on-line (since they are no longer automatically served by the \n-        // HRegionServer that was serving the parent. In this test they will\n-        // end up on the same server (since there is only one), but we have to\n-        // wait until the master assigns them.\n-        try {\n-          HScannerInterface s =\n-            t.obtainScanner(new Text[] {new Text(COLFAMILY_NAME3)},\n-                HConstants.EMPTY_START_ROW);\n-          try {\n-            HStoreKey key = new HStoreKey();\n-            TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n-            s.next(key, results);\n-            break;\n-\n-          } finally {\n-            s.close();\n-          }\n-        } catch (NotServingRegionException x) {\n-          Thread.sleep(5000);\n-        }\n-      }\n-      // Now, force a compaction.  This will rewrite references and make it\n-      // so the parent region becomes deletable.\n-      LOG.info(\"Starting compaction\");\n-      for (MiniHBaseCluster.RegionServerThread thread: cluster.regionThreads) {\n-        SortedMap<Text, HRegion> regions =\n-          thread.getRegionServer().onlineRegions;\n-        // Retry if ConcurrentModification... alternative of sync'ing is not\n-        // worth it for sake of unit test.\n-        for (int i = 0; i < 10; i++) {\n-          try {\n-            for (HRegion online: regions.values()) {\n-              if (online.getTableDesc().getName().toString().equals(getName())) {\n-                online.compactStores();\n-              }\n-            }\n-            break;\n-          } catch (ConcurrentModificationException e) {\n-            LOG.warn(\"Retrying because ...\" + e.toString() + \" -- one or \" +\n-            \"two should be fine\");\n-            continue;\n-          }\n-        }\n+      LOG.info(\"Split happened. Parent is \" + parent.getRegionName() +\n+        \" and daughters are \" + splitA.getRegionName() + \", \" +\n+        splitB.getRegionName());\n+      // Recalibrate will cause us to wait on new regions' deployment\n+      recalibrate(t, new Text(COLFAMILY_NAME3), retries);\n+      // Compact a region at a time so we can test case where one region has\n+      // no references but the other still has some\n+      compact(cluster, splitA);\n+      // Wait till the parent only has reference to remaining split, one that\n+      // still has references.\n+      while (getSplitParentInfo(meta, parent).size() == 3) {\n+        Thread.sleep(5000);\n       }\n-\n+      LOG.info(\"Parent split returned \" +\n+          getSplitParentInfo(meta, parent).keySet().toString());\n+      // Call second split.\n+      compact(cluster, splitB);\n       // Now wait until parent disappears.\n       LOG.info(\"Waiting on parent \" + parent.getRegionName() +\n       \" to disappear\");\n-      for (int i = 0; i < retries && getSplitParent(meta) != null; i++) {\n+      for (int i = 0; i < retries &&\n+          getSplitParentInfo(meta, parent) != null; i++) {\n         Thread.sleep(5000);\n       }\n-      assertTrue(getSplitParent(meta) == null);\n+      assertTrue(getSplitParentInfo(meta, parent) == null);\n       // Assert cleaned up.\n       for (int i = 0; i < retries && fs.exists(parentDir); i++) {\n         Thread.sleep(5000);\n@@ -258,6 +236,70 @@ public void testSplitRegionIsDeleted() throws Exception {\n     }\n   }\n   \n+  /*\n+   * Compact the passed in region <code>r</code>. \n+   * @param cluster\n+   * @param r\n+   * @throws IOException\n+   */\n+  private void compact(final MiniHBaseCluster cluster, final HRegionInfo r)\n+  throws IOException {\n+    LOG.info(\"Starting compaction\");\n+    for (MiniHBaseCluster.RegionServerThread thread: cluster.regionThreads) {\n+      SortedMap<Text, HRegion> regions =\n+        thread.getRegionServer().onlineRegions;\n+      // Retry if ConcurrentModification... alternative of sync'ing is not\n+      // worth it for sake of unit test.\n+      for (int i = 0; i < 10; i++) {\n+        try {\n+          for (HRegion online: regions.values()) {\n+            if (online.getRegionName().toString().\n+                equals(r.getRegionName().toString())) {\n+              online.compactStores();\n+            }\n+          }\n+          break;\n+        } catch (ConcurrentModificationException e) {\n+          LOG.warn(\"Retrying because ...\" + e.toString() + \" -- one or \" +\n+          \"two should be fine\");\n+          continue;\n+        }\n+      }\n+    }\n+  }\n+  \n+  /*\n+   * Recalibrate passed in HTable.  Run after change in region geography.\n+   * Open a scanner on the table. This will force HTable to recalibrate\n+   * and in doing so, will force us to wait until the new child regions\n+   * come on-line (since they are no longer automatically served by the \n+   * HRegionServer that was serving the parent. In this test they will\n+   * end up on the same server (since there is only one), but we have to\n+   * wait until the master assigns them. \n+   * @param t\n+   * @param retries\n+   */\n+  private void recalibrate(final HTable t, final Text column,\n+      final int retries)\n+  throws IOException, InterruptedException {\n+    for (int i = 0; i < retries; i++) {\n+      try {\n+        HScannerInterface s =\n+          t.obtainScanner(new Text[] {column}, HConstants.EMPTY_START_ROW);\n+        try {\n+          HStoreKey key = new HStoreKey();\n+          TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n+          s.next(key, results);\n+          break;\n+        } finally {\n+          s.close();\n+        }\n+      } catch (NotServingRegionException x) {\n+        Thread.sleep(5000);\n+      }\n+    }\n+  }\n+  \n   private void assertGet(final HRegion r, final String family, final Text k)\n   throws IOException {\n     // Now I have k, get values out and assert they are as expected.\n@@ -270,30 +312,29 @@ private void assertGet(final HRegion r, final String family, final Text k)\n     }\n   }\n   \n-  private HRegionInfo getSplitParent(final HTable t)\n+  /*\n+   * @return Return row info for passed in region or null if not found in scan.\n+   */\n+  private Map<Text, byte []> getSplitParentInfo(final HTable t,\n+    final HRegionInfo parent)\n   throws IOException {\n-    HRegionInfo result = null;\n-    HScannerInterface s = t.obtainScanner(HConstants.COL_REGIONINFO_ARRAY,\n+    HScannerInterface s = t.obtainScanner(HConstants.COLUMN_FAMILY_ARRAY,\n       HConstants.EMPTY_START_ROW, System.currentTimeMillis(), null);\n     try {\n       HStoreKey curKey = new HStoreKey();\n       TreeMap<Text, byte []> curVals = new TreeMap<Text, byte []>();\n       while(s.next(curKey, curVals)) {\n-        byte[] bytes = curVals.get(HConstants.COL_REGIONINFO);\n-        if (bytes == null || bytes.length == 0) {\n+        HRegionInfo hri = Writables.\n+          getHRegionInfoOrNull(curVals.get(HConstants.COL_REGIONINFO));\n+        if (hri == null) {\n           continue;\n         }\n-        HRegionInfo hri =\n-          (HRegionInfo) Writables.getWritable(bytes, new HRegionInfo());\n-        \n-        // Assert that if region is a split region, that it is also offline.\n-        // Otherwise, if not a split region, assert that it is online.\n-        if (hri.isSplit() && hri.isOffline()) {\n-          result = hri;\n-          break;\n+        if (hri.getRegionName().toString().\n+            equals(parent.getRegionName().toString())) {\n+          return curVals;\n         }\n       }\n-      return result;\n+      return null;\n     } finally {\n       s.close();\n     }   ",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "sha": "4344d9c6105a1a344a398e1fa9e3bc1b2cf9d7b9",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1730 unexpected null value causes META scanner to exit (silently)\n\nAdded handling for legal null value scanning META table and added\nlogging of unexpected exceptions that arise scanning.\n\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestSplit.java\n    Refactored to do a staged removal of daughter references.\n    (compact, recalibrate): Added.\n    (getSplitParent): Refactored as getSplitParentInfo.\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HConnectionManager.java\n    Added formatting of the find table result string so shorter\n    (when 30-odd regions fills page with its output).\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HTable.java\n    Formatting to clean eclipse warnings.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMaster.java\n    The split column in a parent meta table entry can be null (Happens\n    if a daughter split no longer has references -- it removes its\n    entry from parent).  Add handling and clean up around split\n    management code.  Added logging of unexpected exceptions\n    scanning a region.\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegion.java\n    Added fix for NPE when client asks for scanner but passes\n    non-existent columns.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/util/Writables.java\n    (getHRegionInfo, getHRegionInfoOrNull): Added.:\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@567308 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/17cc1759fceb3ee260e18c5e11ce7598cb8c31aa",
        "patched_files": [
            "HRegion.java",
            "HConnectionManager.java",
            "HTable.java",
            "CHANGES.java",
            "HMaster.java",
            "Writables.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplit.java",
            "TestHRegion.java"
        ]
    },
    "hbase_01db60d": {
        "bug_id": "hbase_01db60d",
        "commit": "https://github.com/apache/hbase/commit/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "patch": "@@ -520,6 +520,9 @@ private void checkQueuesDeleted(String peerId) throws ReplicationException {\n     if (queuesClient == null) return;\n     try {\n       List<String> replicators = queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "sha": "751e45441f8006bb8dd1f65ef81bf976147b429c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "patch": "@@ -98,7 +98,7 @@ public void init() throws ReplicationException {\n     for (int retry = 0; ; retry++) {\n       int v0 = getQueuesZNodeCversion();\n       List<String> rss = getListOfReplicators();\n-      if (rss == null) {\n+      if (rss == null || rss.isEmpty()) {\n         LOG.debug(\"Didn't find any region server that replicates, won't prevent any deletions.\");\n         return ImmutableSet.of();\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "sha": "0115b6f812796382dad1af9fb7f299a150dfd95f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "patch": "@@ -77,6 +77,9 @@ public ReplicationZKNodeCleaner(Configuration conf, ZooKeeperWatcher zkw, Aborta\n     Set<String> peerIds = new HashSet<>(this.replicationPeers.getAllPeerIds());\n     try {\n       List<String> replicators = this.queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return undeletedQueues;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = this.queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "sha": "6d8962e9234b6fd1866319240cac3ba0caedd90d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "patch": "@@ -323,6 +323,9 @@ public String dumpQueues(ClusterConnection connection, ZooKeeperWatcher zkw, Set\n     // Loops each peer on each RS and dumps the queues\n     try {\n       List<String> regionservers = queuesClient.getListOfReplicators();\n+      if (regionservers == null || regionservers.isEmpty()) {\n+        return sb.toString();\n+      }\n       for (String regionserver : regionservers) {\n         List<String> queueIds = queuesClient.getAllQueues(regionserver);\n         replicationQueues.init(regionserver);",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "sha": "4bda75be366d7f77854a9262521f9ab20dafbea1",
                "status": "modified"
            }
        ],
        "message": "HBASE-18330 NPE in ReplicationZKLockCleanerChore",
        "parent": "https://github.com/apache/hbase/commit/5f54e28510fdbdc1a08688168f8df19904bcd975",
        "patched_files": [
            "DumpReplicationQueues.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestDumpReplicationQueues.java"
        ]
    },
    "hbase_022f70b": {
        "bug_id": "hbase_022f70b",
        "commit": "https://github.com/apache/hbase/commit/022f70b7220fdd791fcb5eef23ac69852156201a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -1056,6 +1056,8 @@ Release 0.21.0 - Unreleased\n                (Kannan Muthukkaruppan via Stack)\n    HBASE-3102  Enhance HBase rMetrics for Long-running Stats\n                (Nicolas Spiegelberg via Stack)\n+   HBASE-3169  NPE when master joins running cluster if a RIT references\n+               a RS no longer present\n \n   NEW FEATURES\n    HBASE-1961  HBase EC2 scripts",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "sha": "91446ab4b4ac2b53ee1292a817de93c02428bf64",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -292,8 +292,17 @@ void processRegionsInTransition(final RegionTransitionData data,\n         // Region is opened, insert into RIT and handle it\n         regionsInTransition.put(encodedRegionName, new RegionState(\n             regionInfo, RegionState.State.OPENING, data.getStamp()));\n-        new OpenedRegionHandler(master, this, data, regionInfo,\n-            serverManager.getServerInfo(data.getServerName())).process();\n+        HServerInfo hsi = serverManager.getServerInfo(data.getServerName());\n+        // hsi could be null if this server is no longer online.  If\n+        // that the case, just let this RIT timeout; it'll be assigned\n+        // to new server then.\n+        if (hsi == null) {\n+          LOG.warn(\"Region in transition \" + regionInfo.getEncodedName() +\n+            \" references a server no longer up \" + data.getServerName() +\n+            \"; letting RIT timeout so will be assigned elsewhere\");\n+          break;\n+        }\n+        new OpenedRegionHandler(master, this, data, regionInfo, hsi).process();\n         break;\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "78fc7c3f4061d6bda3e15be7ae7f317a8fae1785",
                "status": "modified"
            }
        ],
        "message": "HBASE-3169  NPE when master joins running cluster if a RIT references a RS no longer present\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1028872 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/73727e534a8590d233255db780b6eb8784ce8beb",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_02f6104": {
        "bug_id": "hbase_02f6104",
        "commit": "https://github.com/apache/hbase/commit/02f6104dc289a3b9d691fdf5abcd4bf226600610",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.92.0 - Unreleased\n    HBASE-4745  LRU statistics thread should be a daemon\n    HBASE-4749  TestMasterFailover#testMasterFailoverWithMockedRITOnDeadRS\n                occasionally fails\n+   HBASE-4753  org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD\n+               throws NPE on trunk (nkeywal)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "sha": "803c7eb6c04b8c94159126507eafb0816c205fa0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "patch": "@@ -430,7 +430,9 @@ public static void deleteTableDescriptorIfExists(String tableName,\n     FileSystem fs = FSUtils.getCurrentFileSystem(conf);\n     FileStatus status = getTableInfoPath(fs, FSUtils.getRootDir(conf), tableName);\n     // The below deleteDirectory works for either file or directory.\n-    if (fs.exists(status.getPath())) FSUtils.deleteDirectory(fs, status.getPath());\n+    if (status != null && fs.exists(status.getPath()))  {\n+      FSUtils.deleteDirectory(fs, status.getPath());\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "sha": "b2783aedbb08c62dc8a5ab1d1d4ed8ff09a88754",
                "status": "modified"
            }
        ],
        "message": "HBASE-4753 org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD throws NPE on trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1198581 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7d8d42d01bdfca946425cabf84dedecbf0d8602b",
        "patched_files": [
            "FSTableDescriptors.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFSTableDescriptors.java"
        ]
    },
    "hbase_03e4712": {
        "bug_id": "hbase_03e4712",
        "commit": "https://github.com/apache/hbase/commit/03e4712f0ca08d57586b3fc4d93cf02c999515d8",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=03e4712f0ca08d57586b3fc4d93cf02c999515d8",
                "deletions": 15,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -345,19 +344,13 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive the mob data if there is a mob-enabled column\n-    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n-    boolean hasMob = MobUtils.hasMobColumns(htd);\n-    Path mobTableDir = null;\n-    if (hasMob) {\n-      // Archive mob data\n-      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-              tableName);\n-      Path regionDir =\n-              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-      if (fs.exists(regionDir)) {\n-        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n-      }\n+    // Archive mob data\n+    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+            tableName);\n+    Path regionDir =\n+            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+    if (fs.exists(regionDir)) {\n+      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -366,7 +359,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "sha": "1e86254a061de41e730c62f0b5bf28a69db51462",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hbase/blob/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=03e4712f0ca08d57586b3fc4d93cf02c999515d8",
                "deletions": 24,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "patch": "@@ -195,32 +195,15 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n+    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n+  }\n \n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n+  @Test(timeout=90000)\n+  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n+    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n+    testRollbackAndDoubleExecution(htd);\n   }\n \n   @Test(timeout=90000)\n@@ -282,4 +265,31 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n+\n+  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+    TableName tableName = htd.getTableName();\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "sha": "676a3f4dec947a85be7f243cf086f23b976c233e",
                "status": "modified"
            }
        ],
        "message": "HBASE-4907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)",
        "parent": "https://github.com/apache/hbase/commit/8b3d1f144408e4a7a014c5ac46418c9e91b9b0db",
        "patched_files": [
            "CreateTableProcedure.java",
            "DeleteTableProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCreateTableProcedure.java",
            "TestDeleteTableProcedure.java"
        ]
    },
    "hbase_08693f6": {
        "bug_id": "hbase_08693f6",
        "commit": "https://github.com/apache/hbase/commit/08693f66a877ef782ac706e8425aba6fcb72d267",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -26,6 +26,9 @@ Release 0.20.0 - Unreleased\n    HBASE-1198  OOME in IPC server does not trigger abort behavior\n    HBASE-1209  Make port displayed the same as is used in URL for RegionServer\n                table in UI (Lars George via Stack)\n+   HBASE-1217  add new compression and hfile blocksize to HColumnDescriptor\n+   HBASE-859   HStoreKey needs a reworking\n+   HBASE-1211  NPE in retries exhausted exception\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/CHANGES.txt",
                "sha": "607fc9fa34f54287d5e2a46c3e01921426ae50e5",
                "status": "modified"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 48,
                "filename": "src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "patch": "@@ -27,6 +27,8 @@\n import java.util.Map;\n \n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.rest.exception.HBaseRestException;\n import org.apache.hadoop.hbase.rest.serializer.IRestSerializer;\n import org.apache.hadoop.hbase.rest.serializer.ISerializable;\n@@ -52,11 +54,12 @@\n   // Time-to-live feature.  Version 4 was when we moved to byte arrays, HBASE-82.\n   // Version 5 was when bloom filter descriptors were removed.\n   // Version 6 adds metadata as a map where keys and values are byte[].\n-  private static final byte COLUMN_DESCRIPTOR_VERSION = (byte)6;\n+  private static final byte COLUMN_DESCRIPTOR_VERSION = (byte)7;\n \n   /** \n    * The type of compression.\n    * @see org.apache.hadoop.io.SequenceFile.Writer\n+   * @deprecated Replaced by {@link Compression.Algorithm}.\n    */\n   public static enum CompressionType {\n     /** Do not compress records. */\n@@ -67,20 +70,21 @@\n     BLOCK\n   }\n \n-  public static final String COMPRESSION = \"COMPRESSION\";       //TODO: change to protected\n-  public static final String BLOCKCACHE = \"BLOCKCACHE\";         //TODO: change to protected\n-  public static final String LENGTH = \"LENGTH\";                 //TODO: change to protected\n-  public static final String TTL = \"TTL\";                       //TODO: change to protected\n-  public static final String BLOOMFILTER = \"BLOOMFILTER\";       //TODO: change to protected\n-  public static final String FOREVER = \"FOREVER\";               //TODO: change to protected\n-  public static final String MAPFILE_INDEX_INTERVAL =           //TODO: change to protected\n+  public static final String COMPRESSION = \"COMPRESSION\";\n+  public static final String BLOCKCACHE = \"BLOCKCACHE\";\n+  public static final String BLOCKSIZE = \"BLOCKSIZE\";\n+  public static final String LENGTH = \"LENGTH\";\n+  public static final String TTL = \"TTL\";\n+  public static final String BLOOMFILTER = \"BLOOMFILTER\";\n+  public static final String FOREVER = \"FOREVER\";\n+  public static final String MAPFILE_INDEX_INTERVAL =\n       \"MAPFILE_INDEX_INTERVAL\";\n \n   /**\n    * Default compression type.\n    */\n-  public static final CompressionType DEFAULT_COMPRESSION =\n-    CompressionType.NONE;\n+  public static final String DEFAULT_COMPRESSION =\n+    Compression.Algorithm.NONE.getName();\n \n   /**\n    * Default number of versions of a record to keep.\n@@ -100,6 +104,12 @@\n    */\n   private volatile Integer maxValueLength = null;\n \n+  /*\n+   * Cache here the HCD value.\n+   * Question: its OK to cache since when we're reenable, we create a new HCD?\n+   */\n+  private volatile Integer blocksize = null;\n+\n   /**\n    * Default setting for whether to serve from memory or not.\n    */\n@@ -110,6 +120,12 @@\n    */\n   public static final boolean DEFAULT_BLOCKCACHE = false;\n \n+  /**\n+   * Default size of blocks in files store to the filesytem.  Use smaller for\n+   * faster random-access at expense of larger indices (more memory consumption).\n+   */\n+  public static final int DEFAULT_BLOCKSIZE = HFile.DEFAULT_BLOCKSIZE;\n+\n   /**\n    * Default setting for whether or not to use bloomfilters.\n    */\n@@ -123,16 +139,10 @@\n   // Column family name\n   private byte [] name;\n \n-  /**\n-   * Default mapfile index interval.\n-   */\n-  public static final int DEFAULT_MAPFILE_INDEX_INTERVAL = 128;\n-\n   // Column metadata\n   protected Map<ImmutableBytesWritable,ImmutableBytesWritable> values =\n     new HashMap<ImmutableBytesWritable,ImmutableBytesWritable>();\n \n-\n   /**\n    * Default constructor. Must be present for Writable.\n    */\n@@ -200,9 +210,37 @@ public HColumnDescriptor(HColumnDescriptor desc) {\n    * @throws IllegalArgumentException if the number of versions is &lt;= 0\n    */\n   public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n-      final CompressionType compression, final boolean inMemory,\n+      final String compression, final boolean inMemory,\n       final boolean blockCacheEnabled, final int maxValueLength,\n       final int timeToLive, final boolean bloomFilter) {\n+    this(familyName, maxVersions, compression, inMemory, blockCacheEnabled,\n+      DEFAULT_BLOCKSIZE, maxValueLength, timeToLive, bloomFilter);\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param familyName Column family name. Must be 'printable' -- digit or\n+   * letter -- and end in a <code>:<code>\n+   * @param maxVersions Maximum number of versions to keep\n+   * @param compression Compression type\n+   * @param inMemory If true, column data should be kept in an HRegionServer's\n+   * cache\n+   * @param blockCacheEnabled If true, MapFile blocks should be cached\n+   * @param maxValueLength Restrict values to &lt;= this value\n+   * @param timeToLive Time-to-live of cell contents, in seconds\n+   * (use HConstants.FOREVER for unlimited TTL)\n+   * @param bloomFilter Enable the specified bloom filter for this column\n+   * \n+   * @throws IllegalArgumentException if passed a family name that is made of \n+   * other than 'word' characters: i.e. <code>[a-zA-Z_0-9]</code> and does not\n+   * end in a <code>:</code>\n+   * @throws IllegalArgumentException if the number of versions is &lt;= 0\n+   */\n+  public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n+      final String compression, final boolean inMemory,\n+      final boolean blockCacheEnabled, final int blocksize,\n+      final int maxValueLength,\n+      final int timeToLive, final boolean bloomFilter) {\n     isLegalFamilyName(familyName);\n     this.name = stripColon(familyName);\n     if (maxVersions <= 0) {\n@@ -215,10 +253,12 @@ public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n     setBlockCacheEnabled(blockCacheEnabled);\n     setMaxValueLength(maxValueLength);\n     setTimeToLive(timeToLive);\n-    setCompressionType(compression);\n+    setCompressionType(Compression.Algorithm.\n+      valueOf(compression.toUpperCase()));\n     setBloomfilter(bloomFilter);\n+    setBlocksize(blocksize);\n   }\n-  \n+\n   private static byte [] stripColon(final byte [] n) {\n     byte [] result = new byte [n.length - 1];\n     // Have the stored family name be absent the colon delimiter\n@@ -321,15 +361,8 @@ public void setValue(String key, String value) {\n \n   /** @return compression type being used for the column family */\n   @TOJSON\n-  public CompressionType getCompression() {\n-    String value = getValue(COMPRESSION);\n-    if (value != null) {\n-      if (value.equalsIgnoreCase(\"BLOCK\"))\n-        return CompressionType.BLOCK;\n-      else if (value.equalsIgnoreCase(\"RECORD\"))\n-        return CompressionType.RECORD;\n-    }\n-    return CompressionType.NONE;\n+  public Compression.Algorithm getCompression() {\n+    return Compression.Algorithm.valueOf(getValue(COMPRESSION));\n   }\n   \n   /** @return maximum number of versions */\n@@ -347,24 +380,44 @@ public int getMaxVersions() {\n   public void setMaxVersions(int maxVersions) {\n     setValue(HConstants.VERSIONS, Integer.toString(maxVersions));\n   }\n-  \n+\n+  /**\n+   * @return Blocksize.\n+   */\n+  @TOJSON\n+  public synchronized int getBlocksize() {\n+    if (this.blocksize == null) {\n+      String value = getValue(BLOCKSIZE);\n+      this.blocksize = (value != null)?\n+        Integer.decode(value): Integer.valueOf(DEFAULT_BLOCKSIZE);\n+    }\n+    return this.blocksize.intValue();\n+  }\n+\n+  /**\n+   * @param s\n+   */\n+  public void setBlocksize(int s) {\n+    setValue(BLOCKSIZE, Integer.toString(s));\n+    this.blocksize = null;\n+  }\n+\n   /**\n    * @return Compression type setting.\n    */\n   @TOJSON\n-  public CompressionType getCompressionType() {\n+  public Compression.Algorithm getCompressionType() {\n     return getCompression();\n   }\n \n   /**\n    * @param type Compression type setting.\n    */\n-  public void setCompressionType(CompressionType type) {\n+  public void setCompressionType(Compression.Algorithm type) {\n     String compressionType;\n     switch (type) {\n-      case BLOCK:  compressionType = \"BLOCK\";   break;\n-      case RECORD: compressionType = \"RECORD\";  break;\n-      default:     compressionType = \"NONE\";    break;\n+      case GZ: compressionType = \"GZ\"; break;\n+      default: compressionType = \"NONE\"; break;\n     }\n     setValue(COMPRESSION, compressionType);\n   }\n@@ -461,17 +514,6 @@ public void setBloomfilter(final boolean onOff) {\n     setValue(BLOOMFILTER, Boolean.toString(onOff));\n   }\n \n-  /**\n-   * @return The number of entries that are added to the store MapFile before\n-   * an index entry is added.\n-   */\n-  public int getMapFileIndexInterval() {\n-    String value = getValue(MAPFILE_INDEX_INTERVAL);\n-    if (value != null)\n-      return Integer.valueOf(value).intValue();\n-    return DEFAULT_MAPFILE_INDEX_INTERVAL;\n-  }\n-\n   /**\n    * @param interval The number of entries that are added to the store MapFile before\n    * an index entry is added.\n@@ -531,7 +573,7 @@ public void readFields(DataInput in) throws IOException {\n       this.values.clear();\n       setMaxVersions(in.readInt());\n       int ordinal = in.readInt();\n-      setCompressionType(CompressionType.values()[ordinal]);\n+      setCompressionType(Compression.Algorithm.values()[ordinal]);\n       setInMemory(in.readBoolean());\n       setMaxValueLength(in.readInt());\n       setBloomfilter(in.readBoolean());\n@@ -551,7 +593,7 @@ public void readFields(DataInput in) throws IOException {\n        setTimeToLive(in.readInt());\n       }\n     } else {\n-      // version 6+\n+      // version 7+\n       this.name = Bytes.readByteArray(in);\n       this.values.clear();\n       int numValues = in.readInt();\n@@ -562,6 +604,10 @@ public void readFields(DataInput in) throws IOException {\n         value.readFields(in);\n         values.put(key, value);\n       }\n+      if (version == 6) {\n+        // Convert old values.\n+        setValue(COMPRESSION, Compression.Algorithm.NONE.getName());\n+      }\n     }\n   }\n \n@@ -597,4 +643,4 @@ else if (result > 0)\n   public void restSerialize(IRestSerializer serializer) throws HBaseRestException {\n     serializer.serializeColumnDescriptor(this);    \n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "sha": "ef7909e4eb2b4720d6fc0985cac19a1f6e83197b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionInfo.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "patch": "@@ -427,13 +427,13 @@ public int compareTo(HRegionInfo o) {\n     }\n \n     // Compare start keys.\n-    result = HStoreKey.compareTwoRowKeys(this.startKey, o.startKey);\n+    result = Bytes.compareTo(this.startKey, o.startKey);\n     if (result != 0) {\n       return result;\n     }\n     \n     // Compare end keys.\n-    return HStoreKey.compareTwoRowKeys(this.endKey, o.endKey);\n+    return Bytes.compareTo(this.endKey, o.endKey);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "sha": "be0fc3386511dabf3fec3ca33941d48ddc9368de",
                "status": "modified"
            },
            {
                "additions": 442,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "changes": 710,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 268,
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "patch": "@@ -20,10 +20,8 @@\n package org.apache.hadoop.hbase;\n \n \n-import java.io.ByteArrayOutputStream;\n import java.io.DataInput;\n import java.io.DataOutput;\n-import java.io.DataOutputStream;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n \n@@ -44,10 +42,6 @@\n    * Colon character in UTF-8\n    */\n   public static final char COLUMN_FAMILY_DELIMITER = ':';\n-  \n-  private byte [] row = HConstants.EMPTY_BYTE_ARRAY;\n-  private byte [] column = HConstants.EMPTY_BYTE_ARRAY;\n-  private long timestamp = Long.MAX_VALUE;\n \n   /**\n    * Estimated size tax paid for each instance of HSK.  Estimate based on\n@@ -56,9 +50,17 @@\n   // In jprofiler, says shallow size is 48 bytes.  Add to it cost of two\n   // byte arrays and then something for the HRI hosting.\n   public static final int ESTIMATED_HEAP_TAX = 48;\n-  \n-  public static final StoreKeyByteComparator BYTECOMPARATOR =\n-    new StoreKeyByteComparator();\n+\n+  private byte [] row = HConstants.EMPTY_BYTE_ARRAY;\n+  private byte [] column = HConstants.EMPTY_BYTE_ARRAY;\n+  private long timestamp = Long.MAX_VALUE;\n+\n+  private static final HStoreKey.StoreKeyComparator PLAIN_COMPARATOR =\n+    new HStoreKey.StoreKeyComparator();\n+  private static final HStoreKey.StoreKeyComparator META_COMPARATOR =\n+    new HStoreKey.MetaStoreKeyComparator();\n+  private static final HStoreKey.StoreKeyComparator ROOT_COMPARATOR =\n+    new HStoreKey.RootStoreKeyComparator();\n \n   /** Default constructor used in conjunction with Writable interface */\n   public HStoreKey() {\n@@ -93,7 +95,6 @@ public HStoreKey(final String row) {\n    * \n    * @param row row key\n    * @param timestamp timestamp value\n-   * @param hri HRegionInfo\n    */\n   public HStoreKey(final byte [] row, final long timestamp) {\n     this(row, HConstants.EMPTY_BYTE_ARRAY, timestamp);\n@@ -130,7 +131,6 @@ public HStoreKey(final byte [] row, final byte [] column) {\n    * @param row row key\n    * @param column column key\n    * @param timestamp timestamp value\n-   * @param regionInfo region info\n    */\n   public HStoreKey(final String row, final String column, final long timestamp) {\n     this (Bytes.toBytes(row), Bytes.toBytes(column), timestamp);\n@@ -143,7 +143,6 @@ public HStoreKey(final String row, final String column, final long timestamp) {\n    * @param row row key\n    * @param column column key\n    * @param timestamp timestamp value\n-   * @param regionInfo region info\n    */\n   public HStoreKey(final byte [] row, final byte [] column, final long timestamp) {\n     // Make copies\n@@ -256,7 +255,7 @@ public boolean matchesWithoutColumn(final HStoreKey other) {\n   public boolean matchesRowFamily(final HStoreKey that) {\n     final int delimiterIndex = getFamilyDelimiterIndex(getColumn());\n     return equalsTwoRowKeys(getRow(), that.getRow()) &&\n-      Bytes.compareTo(getColumn(), 0, delimiterIndex, that.getColumn(), 0,\n+    Bytes.compareTo(getColumn(), 0, delimiterIndex, that.getColumn(), 0,\n         delimiterIndex) == 0;\n   }\n   \n@@ -277,34 +276,44 @@ public boolean equals(final Object obj) {\n     }\n     return compareTo(other) == 0;\n   }\n-  \n-  @Override\n+\n   public int hashCode() {\n-    int result = Bytes.hashCode(getRow());\n-    result ^= Bytes.hashCode(getColumn());\n-    result ^= getTimestamp();\n-    return result;\n+    int c = Bytes.hashCode(getRow());\n+    c ^= Bytes.hashCode(getColumn());\n+    c ^= getTimestamp();\n+    return c;\n   }\n \n   // Comparable\n \n+  /**\n+   * @deprecated Use Comparators instead.  This can give wrong results.\n+   */\n   public int compareTo(final HStoreKey o) {\n     return compareTo(this, o);\n   }\n+\n+  /**\n+   * @param left\n+   * @param right\n+   * @return\n+   * @deprecated Use Comparators instead.  This can give wrong results because\n+   * does not take into account special handling needed for meta and root rows.\n+   */\n   static int compareTo(final HStoreKey left, final HStoreKey right) {\n     // We can be passed null\n     if (left == null && right == null) return 0;\n     if (left == null) return -1;\n     if (right == null) return 1;\n     \n-    int result = compareTwoRowKeys(left.getRow(), right.getRow());\n+    int result = Bytes.compareTo(left.getRow(), right.getRow());\n     if (result != 0) {\n       return result;\n     }\n     result = left.getColumn() == null && right.getColumn() == null? 0:\n       left.getColumn() == null && right.getColumn() != null? -1:\n         left.getColumn() != null && right.getColumn() == null? 1:\n-      Bytes.compareTo(left.getColumn(), right.getColumn());\n+          Bytes.compareTo(left.getColumn(), right.getColumn());\n     if (result != 0) {\n       return result;\n     }\n@@ -365,7 +374,7 @@ public static boolean matchingFamily(final byte [] family,\n     }\n     return Bytes.compareTo(family, 0, index, column, 0, index) == 0;\n   }\n-  \n+\n   /**\n    * @param family\n    * @return Return <code>family</code> plus the family delimiter.\n@@ -413,39 +422,66 @@ public static boolean matchingFamily(final byte [] family,\n       len);\n     return result;\n   }\n-  \n+\n   /**\n    * @param b\n    * @return Index of the family-qualifier colon delimiter character in passed\n    * buffer.\n    */\n   public static int getFamilyDelimiterIndex(final byte [] b) {\n+    return getDelimiter(b, 0, b.length, (int)COLUMN_FAMILY_DELIMITER);\n+  }\n+\n+  private static int getRequiredDelimiterInReverse(final byte [] b,\n+      final int offset, final int length, final int delimiter) {\n+    int index = getDelimiterInReverse(b, offset, length, delimiter);\n+    if (index < 0) {\n+      throw new IllegalArgumentException(\"No \" + delimiter + \" in <\" +\n+        Bytes.toString(b) + \">\" + \", length=\" + length + \", offset=\" + offset);\n+    }\n+    return index;\n+  }\n+  /*\n+   * @param b\n+   * @param delimiter\n+   * @return Index of delimiter having started from end of <code>b</code> moving\n+   * leftward.\n+   */\n+  private static int getDelimiter(final byte [] b, int offset, final int length,\n+      final int delimiter) {\n     if (b == null) {\n       throw new NullPointerException();\n     }\n     int result = -1;\n-    for (int i = 0; i < b.length; i++) {\n-      if (b[i] == COLUMN_FAMILY_DELIMITER) {\n+    for (int i = offset; i < length + offset; i++) {\n+      if (b[i] == delimiter) {\n         result = i;\n         break;\n       }\n     }\n     return result;\n   }\n \n-  /**\n-   * Utility method to compare two row keys.\n-   * This is required because of the meta delimiters.\n-   * This is a hack.\n-   * @param regionInfo\n-   * @param rowA\n-   * @param rowB\n-   * @return value of the comparison\n+  /*\n+   * @param b\n+   * @param delimiter\n+   * @return Index of delimiter\n    */\n-  public static int compareTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n-    return Bytes.compareTo(rowA, rowB);\n+  private static int getDelimiterInReverse(final byte [] b, final int offset,\n+      final int length, final int delimiter) {\n+    if (b == null) {\n+      throw new NullPointerException();\n+    }\n+    int result = -1;\n+    for (int i = (offset + length) - 1; i >= offset; i--) {\n+      if (b[i] == delimiter) {\n+        result = i;\n+        break;\n+      }\n+    }\n+    return result;\n   }\n-  \n+\n   /**\n    * Utility method to check if two row keys are equal.\n    * This is required because of the meta delimiters\n@@ -457,7 +493,7 @@ public static int compareTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n   public static boolean equalsTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n     return ((rowA == null) && (rowB == null)) ? true:\n       (rowA == null) || (rowB == null) || (rowA.length != rowB.length) ? false:\n-        compareTwoRowKeys(rowA,rowB) == 0;\n+        Bytes.compareTo(rowA, rowB) == 0;\n   }\n \n   // Writable\n@@ -518,153 +554,38 @@ public long heapSize() {\n    * @throws IOException\n    */\n   public static byte [] getBytes(final HStoreKey hsk) throws IOException {\n-    // TODO: Redo with system.arraycopy instead of DOS.\n-    if (hsk == null) {\n-      throw new IllegalArgumentException(\"Writable cannot be null\");\n-    }\n-    final int serializedSize = getSerializedSize(hsk);\n-    final ByteArrayOutputStream byteStream = new ByteArrayOutputStream(serializedSize);\n-    DataOutputStream out = new DataOutputStream(byteStream);\n-    try {\n-      hsk.write(out);\n-      out.close();\n-      out = null;\n-      final byte [] serializedKey = byteStream.toByteArray();\n-      if (serializedKey.length != serializedSize) {\n-        // REMOVE THIS AFTER CONFIDENCE THAT OUR SIZING IS BEING DONE PROPERLY\n-        throw new AssertionError(\"Sizes do not agree \" + serializedKey.length +\n-          \", \" + serializedSize);\n-      }\n-      return serializedKey;\n-    } finally {\n-      if (out != null) {\n-        out.close();\n-      }\n-    }\n+    return getBytes(hsk.getRow(), hsk.getColumn(), hsk.getTimestamp());\n   }\n-  \n+\n   /**\n-   * Pass this class into {@link org.apache.hadoop.io.MapFile}.getClosest when\n-   * searching for the key that comes BEFORE this one but NOT this one.  This\n-   * class will return > 0 when asked to compare against itself rather than 0.\n-   * This is a hack for case where getClosest returns a deleted key and we want\n-   * to get the previous.  Can't unless use use this class; it'll just keep\n-   * returning us the deleted key (getClosest gets exact or nearest before when\n-   * you pass true argument).  TODO: Throw this class away when MapFile has\n-   * a real 'previous' method.  See HBASE-751.\n-   * @deprecated\n+   * @param row Can't be null\n+   * @return Passed arguments as a serialized HSK.\n+   * @throws IOException\n    */\n-  public static class BeforeThisStoreKey extends HStoreKey {\n-    private final HStoreKey beforeThisKey;\n-\n-    /**\n-     * @param beforeThisKey \n-     */\n-    public BeforeThisStoreKey(final HStoreKey beforeThisKey) {\n-      super();\n-      this.beforeThisKey = beforeThisKey;\n-    }\n-    \n-    @Override\n-    public int compareTo(final HStoreKey o) {\n-      final int result = this.beforeThisKey.compareTo(o);\n-      return result == 0? -1: result;\n-    }\n-    \n-    @Override\n-    public boolean equals(final Object obj) {\n-      return false;\n-    }\n-\n-    @Override\n-    public byte[] getColumn() {\n-      return this.beforeThisKey.getColumn();\n-    }\n-\n-    @Override\n-    public byte[] getRow() {\n-      return this.beforeThisKey.getRow();\n-    }\n-\n-    @Override\n-    public long heapSize() {\n-      return this.beforeThisKey.heapSize();\n-    }\n-\n-    @Override\n-    public long getTimestamp() {\n-      return this.beforeThisKey.getTimestamp();\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      return this.beforeThisKey.hashCode();\n-    }\n-\n-    @Override\n-    public boolean matchesRowCol(final HStoreKey other) {\n-      return this.beforeThisKey.matchesRowCol(other);\n-    }\n-\n-    @Override\n-    public boolean matchesRowFamily(final HStoreKey that) {\n-      return this.beforeThisKey.matchesRowFamily(that);\n-    }\n-\n-    @Override\n-    public boolean matchesWithoutColumn(final HStoreKey other) {\n-      return this.beforeThisKey.matchesWithoutColumn(other);\n-    }\n-\n-    @Override\n-    public void readFields(final DataInput in) throws IOException {\n-      this.beforeThisKey.readFields(in);\n-    }\n-\n-    @Override\n-    public void set(final HStoreKey k) {\n-      this.beforeThisKey.set(k);\n-    }\n-\n-    @Override\n-    public void setColumn(final byte[] c) {\n-      this.beforeThisKey.setColumn(c);\n-    }\n-\n-    @Override\n-    public void setRow(final byte[] newrow) {\n-      this.beforeThisKey.setRow(newrow);\n-    }\n-\n-    @Override\n-    public void setVersion(final long timestamp) {\n-      this.beforeThisKey.setVersion(timestamp);\n-    }\n-\n-    @Override\n-    public String toString() {\n-      return this.beforeThisKey.toString();\n-    }\n-\n-    @Override\n-    public void write(final DataOutput out) throws IOException {\n-      this.beforeThisKey.write(out);\n-    }\n+  public static byte [] getBytes(final byte [] row)\n+  throws IOException {\n+    return getBytes(row, null, HConstants.LATEST_TIMESTAMP);\n   }\n \n   /**\n-   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * @param row Can't be null\n+   * @param column Can be null\n+   * @param ts\n+   * @return Passed arguments as a serialized HSK.\n+   * @throws IOException\n    */\n-  public static class HStoreKeyWritableComparator extends WritableComparator {\n-    public HStoreKeyWritableComparator() {\n-      super(HStoreKey.class);\n-    }\n-    \n-    @SuppressWarnings(\"unchecked\")\n-    public int compare(final WritableComparable left,\n-        final WritableComparable right) {\n-      return compareTo((HStoreKey)left, (HStoreKey)right);\n-    }\n+  public static byte [] getBytes(final byte [] row, final byte [] column,\n+    final long ts)\n+  throws IOException {\n+    // TODO: Get vint sizes as I calculate serialized size of hsk.\n+    byte [] b = new byte [getSerializedSize(row) +\n+      getSerializedSize(column) + Bytes.SIZEOF_LONG];\n+    int offset = Bytes.writeByteArray(b, 0, row, 0, row.length);\n+    byte [] c = column == null? HConstants.EMPTY_BYTE_ARRAY: column;\n+    offset = Bytes.writeByteArray(b, offset, c, 0, c.length);\n+    byte [] timestamp = Bytes.toBytes(ts);\n+    System.arraycopy(timestamp, 0, b, offset, timestamp.length);\n+    return b;\n   }\n \n   /**\n@@ -688,17 +609,11 @@ public int compare(final WritableComparable left,\n    * @return Column\n    */\n   public static byte [] getColumn(final ByteBuffer bb) {\n-    byte firstByte = bb.get(0);\n+    // Skip over row.\n+    int offset = skipVintdByteArray(bb, 0);\n+    byte firstByte = bb.get(offset);\n     int vint = firstByte;\n     int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n-    }\n-    // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n-    vint = firstByte;\n-    vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n       vint = getBigVint(vintWidth, firstByte, bb.array(),\n         bb.arrayOffset() + offset);\n@@ -714,32 +629,318 @@ public int compare(final WritableComparable left,\n    * @return Timestamp\n    */\n   public static long getTimestamp(final ByteBuffer bb) {\n-    byte firstByte = bb.get(0);\n+    return bb.getLong(bb.limit() - Bytes.SIZEOF_LONG);\n+  }\n+\n+  /*\n+   * @param bb\n+   * @param offset\n+   * @return Amount to skip to get paste a byte array that is preceded by a\n+   * vint of how long it is.\n+   */\n+  private static int skipVintdByteArray(final ByteBuffer bb, final int offset) {\n+    byte firstByte = bb.get(offset);\n     int vint = firstByte;\n     int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n+      vint = getBigVint(vintWidth, firstByte, bb.array(),\n+        bb.arrayOffset() + offset);\n     }\n+    return vint + vintWidth + offset;\n+  }\n+\n+  /*\n+   * Vint is wider than one byte.  Find out how much bigger it is.\n+   * @param vintWidth\n+   * @param firstByte\n+   * @param buffer\n+   * @param offset\n+   * @return\n+   */\n+  static int getBigVint(final int vintWidth, final byte firstByte,\n+      final byte [] buffer, final int offset) {\n+    long i = 0;\n+    for (int idx = 0; idx < vintWidth - 1; idx++) {\n+      final byte b = buffer[offset + 1 + idx];\n+      i = i << 8;\n+      i = i | (b & 0xFF);\n+    }\n+    i = (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n+    if (i > Integer.MAX_VALUE) {\n+      throw new IllegalArgumentException(\"Calculated vint too large\");\n+    }\n+    return (int)i;\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param bb\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final ByteBuffer bb)\n+  throws IOException {\n+    return HStoreKey.create(bb.array(), bb.arrayOffset(), bb.limit());\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param b Serialized HStoreKey; a byte array with a row only in it won't do.\n+   * It must have all the vints denoting r/c/ts lengths.\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final byte [] b) throws IOException {\n+    return create(b, 0, b.length);\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param b Serialized HStoreKey\n+   * @param offset\n+   * @param length\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final byte [] b, final int offset,\n+    final int length)\n+  throws IOException {\n+    byte firstByte = b[offset];\n+    int vint = firstByte;\n+    int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n+    if (vintWidth != 1) {\n+      vint = getBigVint(vintWidth, firstByte, b, offset);\n+    }\n+    byte [] row = new byte [vint];\n+    System.arraycopy(b, offset + vintWidth,\n+      row, 0, row.length);\n     // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n+    int extraOffset = vint + vintWidth;\n+    firstByte = b[offset + extraOffset];\n     vint = firstByte;\n     vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(),\n-        bb.arrayOffset() + offset);\n+      vint = getBigVint(vintWidth, firstByte, b, offset + extraOffset);\n     }\n+    byte [] column = new byte [vint];\n+    System.arraycopy(b, offset + extraOffset + vintWidth,\n+      column, 0, column.length);\n     // Skip over column\n-    offset += (vint + vintWidth);\n-    return bb.getLong(offset);\n+    extraOffset += (vint + vintWidth);\n+    return new HStoreKey(row, column, Bytes.toLong(b, offset + extraOffset));\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * Use this comparing keys in the -ROOT_ table.\n+   */\n+  public static class HStoreKeyRootComparator extends HStoreKeyMetaComparator {\n+    protected int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareRootRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * Use this comprator for keys in the .META. table.\n+   */\n+  public static class HStoreKeyMetaComparator extends HStoreKeyComparator {\n+    protected int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareMetaRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   */\n+  public static class HStoreKeyComparator extends WritableComparator {\n+    public HStoreKeyComparator() {\n+      super(HStoreKey.class);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public int compare(final WritableComparable l,\n+        final WritableComparable r) {\n+      HStoreKey left = (HStoreKey)l;\n+      HStoreKey right = (HStoreKey)r;\n+      // We can be passed null\n+      if (left == null && right == null) return 0;\n+      if (left == null) return -1;\n+      if (right == null) return 1;\n+      \n+      byte [] lrow = left.getRow();\n+      byte [] rrow = right.getRow();\n+      int result = compareRows(lrow, 0, lrow.length, rrow, 0, rrow.length);\n+      if (result != 0) {\n+        return result;\n+      }\n+      result = left.getColumn() == null && right.getColumn() == null? 0:\n+        left.getColumn() == null && right.getColumn() != null? -1:\n+          left.getColumn() != null && right.getColumn() == null? 1:\n+            Bytes.compareTo(left.getColumn(), right.getColumn());\n+      if (result != 0) {\n+        return result;\n+      }\n+      // The below older timestamps sorting ahead of newer timestamps looks\n+      // wrong but it is intentional. This way, newer timestamps are first\n+      // found when we iterate over a memcache and newer versions are the\n+      // first we trip over when reading from a store file.\n+      if (left.getTimestamp() < right.getTimestamp()) {\n+        result = 1;\n+      } else if (left.getTimestamp() > right.getTimestamp()) {\n+        result = -1;\n+      }\n+      return result;\n+    }\n+\n+    protected int compareRows(final byte [] left, final int loffset,\n+        final int llength, final byte [] right, final int roffset,\n+        final int rlength) {\n+      return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * StoreKeyComparator for the -ROOT- table.\n+   */\n+  public static class RootStoreKeyComparator\n+  extends MetaStoreKeyComparator {\n+    public int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareRootRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * StoreKeyComparator for the .META. table.\n+   */\n+  public static class MetaStoreKeyComparator extends StoreKeyComparator {\n+    public int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareMetaRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /*\n+   * @param left\n+   * @param loffset\n+   * @param llength\n+   * @param right\n+   * @param roffset\n+   * @param rlength\n+   * @return Result of comparing two rows from the -ROOT- table both of which\n+   * are of the form .META.,(TABLE,REGIONNAME,REGIONID),REGIONID.\n+   */\n+  protected static int compareRootRows(byte [] left, int loffset, int llength,\n+      byte [] right, int roffset, int rlength) {\n+    // Rows look like this: .META.,ROW_FROM_META,RID\n+    // System.out.println(\"ROOT \" + Bytes.toString(left, loffset, llength) +\n+    //  \"---\" + Bytes.toString(right, roffset, rlength));\n+    int lmetaOffsetPlusDelimiter = loffset + 7; // '.META.,'\n+    int leftFarDelimiter = getDelimiterInReverse(left, lmetaOffsetPlusDelimiter,\n+      llength - lmetaOffsetPlusDelimiter, HRegionInfo.DELIMITER);\n+    int rmetaOffsetPlusDelimiter = roffset + 7; // '.META.,'\n+    int rightFarDelimiter = getDelimiterInReverse(right,\n+      rmetaOffsetPlusDelimiter, rlength - rmetaOffsetPlusDelimiter,\n+      HRegionInfo.DELIMITER);\n+    if (leftFarDelimiter < 0 && rightFarDelimiter >= 0) {\n+      // Nothing between .META. and regionid.  Its first key.\n+      return -1;\n+    } else if (rightFarDelimiter < 0 && leftFarDelimiter >= 0) {\n+       return 1;\n+    } else if (leftFarDelimiter < 0 && rightFarDelimiter < 0) {\n+      return 0;\n+    }\n+    int result = compareMetaRows(left, lmetaOffsetPlusDelimiter,\n+      leftFarDelimiter - lmetaOffsetPlusDelimiter,\n+      right, rmetaOffsetPlusDelimiter,\n+      rightFarDelimiter - rmetaOffsetPlusDelimiter);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare last part of row, the rowid.\n+    leftFarDelimiter++;\n+    rightFarDelimiter++;\n+    result = compareRowid(left, leftFarDelimiter, llength - leftFarDelimiter,\n+      right, rightFarDelimiter, rlength - rightFarDelimiter);\n+    return result;\n+  }\n+\n+  /*\n+   * @param left\n+   * @param loffset\n+   * @param llength\n+   * @param right\n+   * @param roffset\n+   * @param rlength\n+   * @return Result of comparing two rows from the .META. table both of which\n+   * are of the form TABLE,REGIONNAME,REGIONID.\n+   */\n+  protected static int compareMetaRows(final byte[] left, final int loffset,\n+      final int llength, final byte[] right, final int roffset,\n+      final int rlength) {\n+//    System.out.println(\"META \" + Bytes.toString(left, loffset, llength) +\n+//      \"---\" + Bytes.toString(right, roffset, rlength));\n+    int leftDelimiter = getDelimiter(left, loffset, llength,\n+      HRegionInfo.DELIMITER);\n+    int rightDelimiter = getDelimiter(right, roffset, rlength,\n+      HRegionInfo.DELIMITER);\n+    if (leftDelimiter < 0 && rightDelimiter >= 0) {\n+      // Nothing between .META. and regionid.  Its first key.\n+      return -1;\n+    } else if (rightDelimiter < 0 && leftDelimiter >= 0) {\n+      return 1;\n+    } else if (leftDelimiter < 0 && rightDelimiter < 0) {\n+      return 0;\n+    }\n+    // Compare up to the delimiter\n+    int result = Bytes.compareTo(left, loffset, leftDelimiter - loffset,\n+      right, roffset, rightDelimiter - roffset);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare middle bit of the row.\n+    // Move past delimiter\n+    leftDelimiter++;\n+    rightDelimiter++;\n+    int leftFarDelimiter = getRequiredDelimiterInReverse(left, leftDelimiter,\n+        llength - (leftDelimiter - loffset), HRegionInfo.DELIMITER);\n+    int rightFarDelimiter = getRequiredDelimiterInReverse(right,\n+        rightDelimiter, rlength - (rightDelimiter - roffset),\n+        HRegionInfo.DELIMITER);\n+    // Now compare middlesection of row.\n+    result = Bytes.compareTo(left, leftDelimiter,\n+      leftFarDelimiter - leftDelimiter, right, rightDelimiter,\n+      rightFarDelimiter - rightDelimiter);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare last part of row, the rowid.\n+    leftFarDelimiter++;\n+    rightFarDelimiter++;\n+    result = compareRowid(left, leftFarDelimiter,\n+      llength - (leftFarDelimiter - loffset),\n+      right, rightFarDelimiter, rlength - (rightFarDelimiter - roffset));\n+    return result;\n+  }\n+\n+  private static int compareRowid(byte[] left, int loffset, int llength,\n+      byte[] right, int roffset, int rlength) {\n+    return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n   }\n \n   /**\n    * RawComparator for plain -- i.e. non-catalog table keys such as \n-   * -ROOT- and .META. -- HStoreKeys.  Compares at byte level.\n+   * -ROOT- and .META. -- HStoreKeys.  Compares at byte level.  Knows how to\n+   * handle the vints that introduce row and columns in the HSK byte array\n+   * representation. Adds\n+   * {@link #compareRows(byte[], int, int, byte[], int, int)} to\n+   * {@link RawComparator}\n    */\n-  public static class StoreKeyByteComparator implements RawComparator<byte []> {\n-    public StoreKeyByteComparator() {\n+  public static class StoreKeyComparator implements RawComparator<byte []> {\n+    public StoreKeyComparator() {\n       super();\n     }\n \n@@ -770,7 +971,7 @@ public int compare(final byte [] b1, int o1, int l1,\n         vint2 = getBigVint(vintWidth2, firstByte2, b2, o2);\n       }\n       // Compare the rows.\n-      int result = WritableComparator.compareBytes(b1, o1 + vintWidth1, vint1,\n+      int result = compareRows(b1, o1 + vintWidth1, vint1,\n           b2, o2 + vintWidth2, vint2);\n       if (result != 0) {\n         return result;\n@@ -797,8 +998,10 @@ public int compare(final byte [] b1, int o1, int l1,\n         vint2 = getBigVint(vintWidth2, firstByte2, b2, o2);\n       }\n       // Compare columns.\n-      result = WritableComparator.compareBytes(b1, o1 + vintWidth1, vint1,\n-          b2, o2 + vintWidth2, vint2);\n+      // System.out.println(\"COL <\" + Bytes.toString(b1, o1 + vintWidth1, vint1) +\n+      //  \"> <\" + Bytes.toString(b2, o2 + vintWidth2, vint2) + \">\");\n+      result = Bytes.compareTo(b1, o1 + vintWidth1, vint1,\n+        b2, o2 + vintWidth2, vint2);\n       if (result != 0) {\n         return result;\n       }\n@@ -825,88 +1028,59 @@ public int compare(final byte [] b1, int o1, int l1,\n       }\n       return 0;\n     }\n-  }\n \n-  /*\n-   * Vint is wider than one byte.  Find out how much bigger it is.\n-   * @param vintWidth\n-   * @param firstByte\n-   * @param buffer\n-   * @param offset\n-   * @return\n-   */\n-  static int getBigVint(final int vintWidth, final byte firstByte,\n-      final byte [] buffer, final int offset) {\n-    long i = 0;\n-    for (int idx = 0; idx < vintWidth - 1; idx++) {\n-      final byte b = buffer[offset + 1 + idx];\n-      i = i << 8;\n-      i = i | (b & 0xFF);\n+    /**\n+     * @param left\n+     * @param right\n+     * @return Result comparing rows.\n+     */\n+    public int compareRows(final byte [] left, final byte [] right) {\n+      return compareRows(left, 0, left.length, right, 0, right.length);\n     }\n-    i = (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n-    if (i > Integer.MAX_VALUE) {\n-      throw new IllegalArgumentException(\"Calculated vint too large\");\n+\n+    /**\n+     * @param left\n+     * @param loffset\n+     * @param llength\n+     * @param right\n+     * @param roffset\n+     * @param rlength\n+     * @return Result comparing rows.\n+     */\n+    public int compareRows(final byte [] left, final int loffset,\n+        final int llength, final byte [] right, final int roffset, final int rlength) {\n+      return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n     }\n-    return (int)i;\n   }\n \n   /**\n-   * Create a store key.\n-   * @param bb\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param hri\n+   * @return Compatible comparator\n    */\n-  public static HStoreKey create(final ByteBuffer bb)\n-  throws IOException {\n-    byte firstByte = bb.get(0);\n-    int vint = firstByte;\n-    int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n-    }\n-    byte [] row = new byte [vint];\n-    System.arraycopy(bb.array(), bb.arrayOffset() + vintWidth,\n-      row, 0, row.length);\n-    // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n-    vint = firstByte;\n-    vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(),\n-        bb.arrayOffset() + offset);\n-    }\n-    byte [] column = new byte [vint];\n-    System.arraycopy(bb.array(), bb.arrayOffset() + offset + vintWidth,\n-      column, 0, column.length);\n-    // Skip over column\n-    offset += (vint + vintWidth);\n-    long ts = bb.getLong(offset);\n-    return new HStoreKey(row, column, ts);\n+  public static WritableComparator getWritableComparator(final HRegionInfo hri) {\n+    return hri.isRootRegion()?\n+        new HStoreKey.HStoreKeyRootComparator(): hri.isMetaRegion()?\n+          new HStoreKey.HStoreKeyMetaComparator():\n+            new HStoreKey.HStoreKeyComparator();\n   }\n \n   /**\n-   * Create a store key.\n-   * @param b Serialized HStoreKey; a byte array with a row only in it won't do.\n-   * It must have all the vints denoting r/c/ts lengths.\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param hri\n+   * @return Compatible raw comparator\n    */\n-  public static HStoreKey create(final byte [] b) throws IOException {\n-    return create(b, 0, b.length);\n+  public static StoreKeyComparator getRawComparator(final HRegionInfo hri) {\n+    return hri.isRootRegion()? ROOT_COMPARATOR:\n+      hri.isMetaRegion()? META_COMPARATOR: META_COMPARATOR;\n   }\n \n   /**\n-   * Create a store key.\n-   * @param b Serialized HStoreKey\n-   * @param offset\n-   * @param length\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param tablename\n+   * @return Compatible raw comparator\n    */\n-  public static HStoreKey create(final byte [] b, final int offset,\n-    final int length)\n-  throws IOException {\n-    return (HStoreKey)Writables.getWritable(b, offset, length, new HStoreKey());\n+  public static HStoreKey.StoreKeyComparator getComparator(final byte [] tablename) {\n+    return Bytes.equals(HTableDescriptor.ROOT_TABLEDESC.getName(), tablename)?\n+      ROOT_COMPARATOR:\n+      (Bytes.equals(HTableDescriptor.META_TABLEDESC.getName(),tablename))?\n+      META_COMPARATOR: PLAIN_COMPARATOR;\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "sha": "c2f204ed04b8599a38d2e3259e99c6e242a1f137",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.exception.HBaseRestException;\n import org.apache.hadoop.hbase.rest.serializer.IRestSerializer;\n import org.apache.hadoop.hbase.rest.serializer.ISerializable;\n@@ -667,19 +668,20 @@ public static Path getTableDir(Path rootdir, final byte [] tableName) {\n       HConstants.ROOT_TABLE_NAME,\n       new HColumnDescriptor[] { new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n           10,  // Ten is arbitrary number.  Keep versions to help debuggging.\n-          HColumnDescriptor.CompressionType.NONE, false, true,\n+          Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n           Integer.MAX_VALUE, HConstants.FOREVER, false) });\n   \n   /** Table descriptor for <code>.META.</code> catalog table */\n   public static final HTableDescriptor META_TABLEDESC = new HTableDescriptor(\n       HConstants.META_TABLE_NAME, new HColumnDescriptor[] {\n           new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n             10, // Ten is arbitrary number.  Keep versions to help debuggging.\n-            HColumnDescriptor.CompressionType.NONE, false, true,\n+            Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n             Integer.MAX_VALUE, HConstants.FOREVER, false),\n           new HColumnDescriptor(HConstants.COLUMN_FAMILY_HISTORIAN,\n-            HConstants.ALL_VERSIONS, HColumnDescriptor.CompressionType.NONE,\n-            false, false, Integer.MAX_VALUE, HConstants.WEEK_IN_SECONDS, false)});\n+            HConstants.ALL_VERSIONS, Compression.Algorithm.NONE.getName(),\n+            false, false,  8 * 1024,\n+            Integer.MAX_VALUE, HConstants.WEEK_IN_SECONDS, false)});\n \n   /* (non-Javadoc)\n    * @see org.apache.hadoop.hbase.rest.xml.IOutputXML#toXML()",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "sha": "a2e49d3c9dd564b6fb0d22a15162b7158a2abd1b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "patch": "@@ -65,6 +65,7 @@\n  * Used by {@link HTable} and {@link HBaseAdmin}\n  */\n public class HConnectionManager implements HConstants {\n+\n   /*\n    * Not instantiable.\n    */\n@@ -646,7 +647,7 @@ private HRegionLocation getCachedLocation(final byte [] tableName,\n           // signifying that the region we're checking is actually the last\n           // region in the table.\n           if (HStoreKey.equalsTwoRowKeys(endKey, HConstants.EMPTY_END_ROW) ||\n-              HStoreKey.compareTwoRowKeys(endKey, row) > 0) {\n+              HStoreKey.getComparator(tableName).compareRows(endKey, row) > 0) {\n             return possibleRegion;\n           }\n         }\n@@ -683,7 +684,7 @@ private void deleteCachedLocation(final byte [] tableName,\n \n           // by nature of the map, we know that the start key has to be < \n           // otherwise it wouldn't be in the headMap. \n-          if (HStoreKey.compareTwoRowKeys(endKey, row) <= 0) {\n+          if (HStoreKey.getComparator(tableName).compareRows(endKey, row) <= 0) {\n             // delete any matching entry\n             HRegionLocation rl =\n               tableLocations.remove(matchingRegions.lastKey());\n@@ -1023,4 +1024,4 @@ void close(boolean stopProxy) {\n       }\n     }\n   } \n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "sha": "710f7f3b9ea3a76c35def692b39ea0e481aa752c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HTable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/client/HTable.java",
                "patch": "@@ -303,7 +303,7 @@ public Cell get(final String row, final String column)\n    * @return value for specified row/column\n    * @throws IOException\n    */\n-  public Cell[] get(final String row, final String column, int numVersions)\n+  public Cell [] get(final String row, final String column, int numVersions)\n   throws IOException {\n     return get(Bytes.toBytes(row), Bytes.toBytes(column), numVersions);\n   }\n@@ -337,7 +337,7 @@ public Cell call() throws IOException {\n    * @return Array of Cells.\n    * @throws IOException\n    */\n-  public Cell[] get(final byte [] row, final byte [] column,\n+  public Cell [] get(final byte [] row, final byte [] column,\n     final int numVersions) \n   throws IOException {\n     return connection.getRegionServerWithRetries(\n@@ -1276,11 +1276,12 @@ public Boolean call() throws IOException {\n           if (rl != null) {\n             lockId = rl.getLockId();\n           }\n-          return server.exists(location.getRegionInfo().getRegionName(), row,\n-            column, timestamp, lockId);\n+          return Boolean.valueOf(server.\n+            exists(location.getRegionInfo().getRegionName(), row,\n+            column, timestamp, lockId));\n         }\n       }\n-    );\n+    ).booleanValue();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "sha": "d0001454ebb62aa194b419723a3d412aeb48e570",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "patch": "@@ -7,6 +7,7 @@\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.RowResult;\n+import org.apache.hadoop.hbase.util.Bytes;\n \n /**\n  * Scanner class that contains the <code>.META.</code> table scanning logic \n@@ -69,7 +70,7 @@ public static void metaScan(HBaseConfiguration configuration,\n         callable.setClose();\n         connection.getRegionServerWithRetries(callable);\n       }\n-    } while (HStoreKey.compareTwoRowKeys(startRow, LAST_ROW) != 0);\n+    } while (Bytes.compareTo(startRow, LAST_ROW) != 0);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "sha": "540aff1f1f8047d1e0de9d62b34479114018e7ff",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "patch": "@@ -47,9 +47,9 @@ private static String getMessage(String serverName, final byte [] regionName,\n     StringBuilder buffer = new StringBuilder(\"Trying to contact region server \");\n     buffer.append(serverName);\n     buffer.append(\" for region \");\n-    buffer.append(Bytes.toString(regionName));\n+    buffer.append(regionName == null? \"\": Bytes.toString(regionName));\n     buffer.append(\", row '\");\n-    buffer.append(Bytes.toString(row));\n+    buffer.append(row == null? \"\": Bytes.toString(row));\n     buffer.append(\"', but failed after \");\n     buffer.append(numTries + 1);\n     buffer.append(\" attempts.\\nExceptions:\\n\");",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "sha": "bdc768cde14894a065546a34e57558113335a0d8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "patch": "@@ -76,7 +76,7 @@ public void instantiateServer(boolean reload) throws IOException {\n       // open the scanner\n       scannerId = openScanner();\n     } else {\n-      RowResult[] rrs = server.next(scannerId, caching);\n+      RowResult [] rrs = server.next(scannerId, caching);\n       return rrs.length == 0 ? null : rrs;\n     }\n     return null;",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "sha": "ad48401ab7b4124cfd2f1741830b41a697ce8d32",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "patch": "@@ -1,6 +1,7 @@\n package org.apache.hadoop.hbase.client;\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n \n /**\n  * Immutable HColumnDescriptor\n@@ -50,12 +51,12 @@ public void setTimeToLive(int timeToLive) {\n   }\n \n   @Override\n-  public void setCompressionType(CompressionType type) {\n+  public void setCompressionType(Compression.Algorithm type) {\n     throw new UnsupportedOperationException(\"HColumnDescriptor is read-only\");\n   }\n \n   @Override\n   public void setMapFileIndexInterval(int interval) {\n     throw new UnsupportedOperationException(\"HTableDescriptor is read-only\");\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "sha": "ac4bfa3e7509acfc6bc9eec414c68565930dc09c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/Cell.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/Cell.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/io/Cell.java",
                "patch": "@@ -55,6 +55,7 @@ public int compare(Long l1, Long l2) {\n \n   /** For Writable compatibility */\n   public Cell() {\n+    super();\n   }\n \n   /**\n@@ -93,7 +94,7 @@ public Cell(final ByteBuffer bb, long timestamp) {\n    * @param ts\n    *          array of timestamps\n    */\n-  public Cell(String[] vals, long[] ts) {\n+  public Cell(String [] vals, long[] ts) {\n     this(Bytes.toByteArrays(vals), ts);\n   }\n \n@@ -235,4 +236,4 @@ public void restSerialize(IRestSerializer serializer)\n       throws HBaseRestException {\n     serializer.serializeCell(this);\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/Cell.java",
                "sha": "dd0fa8578b16621255c87cb15ebfde35250890b7",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "patch": "@@ -72,10 +72,10 @@ public HBaseReader(FileSystem fs, String dirName, Configuration conf,\n     public HBaseReader(FileSystem fs, String dirName, Configuration conf,\n         boolean blockCacheEnabled, HRegionInfo hri)\n     throws IOException {\n-      super(fs, dirName, new HStoreKey.HStoreKeyWritableComparator(), \n+      super(fs, dirName, new HStoreKey.HStoreKeyComparator(), \n           conf, false); // defer opening streams\n       this.blockCacheEnabled = blockCacheEnabled;\n-      open(fs, dirName, new HStoreKey.HStoreKeyWritableComparator(), conf);\n+      open(fs, dirName, new HStoreKey.HStoreKeyComparator(), conf);\n       \n       // Force reading of the mapfile index by calling midKey. Reading the\n       // index will bring the index into memory over here on the client and\n@@ -121,7 +121,7 @@ protected FSDataInputStream openFile(FileSystem fs, Path file,\n     public HBaseWriter(Configuration conf, FileSystem fs, String dirName,\n         SequenceFile.CompressionType compression, final HRegionInfo hri)\n     throws IOException {\n-      super(conf, fs, dirName, new HStoreKey.HStoreKeyWritableComparator(),\n+      super(conf, fs, dirName, new HStoreKey.HStoreKeyComparator(),\n          VALUE_CLASS, compression);\n       // Default for mapfiles is 128.  Makes random reads faster if we\n       // have more keys indexed and we're not 'next'-ing around in the",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "sha": "73ad5359b651b211bf15698cb463e5d3c7dcd6d3",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "patch": "@@ -73,6 +73,10 @@ public HalfHFileReader(final FileSystem fs, final Path p, final BlockCache c,\n     this.top = Reference.isTopFileRegion(r.getFileRegion());\n   }\n \n+  protected boolean isTop() {\n+    return this.top;\n+  }\n+\n   public HFileScanner getScanner() {\n     final HFileScanner s = super.getScanner();\n     return new HFileScanner() {",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "sha": "0b29f245560b7ebd84b88dd0f87fcc41a51f4710",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 23,
                "filename": "src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "patch": "@@ -46,25 +46,9 @@\n  * @param <V> value Expects a Writable or byte [].\n  */\n public class HbaseMapWritable <K,V>\n-implements SortedMap<byte[],V>, Configurable, Writable,\n-  CodeToClassAndBack{\n-  \n+implements SortedMap<byte[],V>, Configurable, Writable, CodeToClassAndBack{\n   private AtomicReference<Configuration> conf = null;\n   protected SortedMap<byte [], V> instance = null;\n-  \n-  // Static maps of code to class and vice versa.  Includes types used in hbase\n-  // only. These maps are now initialized in a static loader interface instead\n-  // of in a static contructor for this class, this is done so that it is\n-  // possible to have a regular contructor here, so that different params can\n-  // be used.\n-  \n-  // Removed the old types like Text from the maps, if needed to add more types\n-  // this can be done in the StaticHBaseMapWritableLoader interface. Only\n-  // byte[] and Cell are supported now.\n-  //   static final Map<Byte, Class<?>> CODE_TO_CLASS =\n-  //     new HashMap<Byte, Class<?>>();\n-  //   static final Map<Class<?>, Byte> CLASS_TO_CODE =\n-  //     new HashMap<Class<?>, Byte>();\n \n   /**\n    * The default contructor where a TreeMap is used\n@@ -73,11 +57,11 @@ public HbaseMapWritable(){\n      this (new TreeMap<byte [], V>(Bytes.BYTES_COMPARATOR));\n    }\n \n-   /**\n-  * Contructor where another SortedMap can be used\n-  * \n-  * @param map the SortedMap to be used \n-  **/\n+  /**\n+   * Contructor where another SortedMap can be used\n+   * \n+   * @param map the SortedMap to be used \n+   */\n   public HbaseMapWritable(SortedMap<byte[], V> map){\n     conf = new AtomicReference<Configuration>();\n     instance = map;\n@@ -233,4 +217,4 @@ public void readFields(DataInput in) throws IOException {\n       this.instance.put(key, value);\n     }\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "sha": "60ba2d4a8eaa1666221243ecf99d006778af7e62",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/RowResult.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "patch": "@@ -78,21 +78,21 @@ public RowResult (final byte [] row,\n   // Map interface\n   // \n   \n-  public Cell put(byte [] key,\n-    Cell value) {\n+  public Cell put(@SuppressWarnings(\"unused\") byte [] key,\n+    @SuppressWarnings(\"unused\") Cell value) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  public void putAll(Map map) {\n+  public void putAll(@SuppressWarnings(\"unused\") Map map) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n   public Cell get(Object key) {\n     return this.cells.get(key);\n   }\n \n-  public Cell remove(Object key) {\n+  public Cell remove(@SuppressWarnings(\"unused\") Object key) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n@@ -104,7 +104,7 @@ public boolean containsKey(String key) {\n     return cells.containsKey(Bytes.toBytes(key));\n   }\n \n-  public boolean containsValue(Object value) {\n+  public boolean containsValue(@SuppressWarnings(\"unused\") Object value) {\n     throw new UnsupportedOperationException(\"Don't support containsValue!\");\n   }\n \n@@ -135,7 +135,7 @@ public void clear() {\n   /**\n    * This method used solely for the REST serialization\n    * \n-   * @return\n+   * @return Cells\n    */\n   @TOJSON\n   public RestCell[] getCells() {\n@@ -211,7 +211,7 @@ public Cell get(String key) {\n       this.cell = cell;\n     }\n     \n-    public Cell setValue(Cell c) {\n+    public Cell setValue(@SuppressWarnings(\"unused\") Cell c) {\n       throw new UnsupportedOperationException(\"RowResult is read-only!\");\n     }\n     ",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "sha": "13e84028640f47b40c3723052ebdf6fe208b109f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "patch": "@@ -297,7 +297,7 @@ public String getName() {\n     }\n   }\n \n-  static Algorithm getCompressionAlgorithmByName(String compressName) {\n+  public static Algorithm getCompressionAlgorithmByName(String compressName) {\n     Algorithm[] algos = Algorithm.class.getEnumConstants();\n \n     for (Algorithm a : algos) {",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "sha": "9f9efb0dc5206f6b3400651390430f2b78ec494d",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "changes": 104,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 25,
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "patch": "@@ -141,8 +141,10 @@\n   /**\n    * Default compression: none.\n    */\n+  public final static Compression.Algorithm DEFAULT_COMPRESSION_ALGORITHM =\n+    Compression.Algorithm.NONE;\n   public final static String DEFAULT_COMPRESSION =\n-    Compression.Algorithm.NONE.getName();\n+    DEFAULT_COMPRESSION_ALGORITHM.getName();\n \n   /**\n    * HFile Writer.\n@@ -216,7 +218,7 @@\n      */\n     public Writer(FileSystem fs, Path path)\n     throws IOException {\n-      this(fs, path, DEFAULT_BLOCKSIZE, null, null);\n+      this(fs, path, DEFAULT_BLOCKSIZE, null, null, false);\n     }\n \n     /**\n@@ -226,12 +228,35 @@ public Writer(FileSystem fs, Path path)\n      * @param blocksize\n      * @param compress\n      * @param comparator\n+     * @param bloomfilter\n+     * @throws IOException \n+     * @throws IOException\n+     */\n+    public Writer(FileSystem fs, Path path, int blocksize,\n+      String compress, final RawComparator<byte []> comparator)\n+    throws IOException {\n+      this(fs, path, blocksize,\n+        compress == null? DEFAULT_COMPRESSION_ALGORITHM:\n+          Compression.getCompressionAlgorithmByName(compress),\n+        comparator, false);\n+    }\n+\n+    /**\n+     * Constructor that takes a Path.\n+     * @param fs\n+     * @param path\n+     * @param blocksize\n+     * @param compress\n+     * @param comparator\n+     * @param bloomfilter\n      * @throws IOException\n      */\n-    public Writer(FileSystem fs, Path path, int blocksize, String compress,\n-      final RawComparator<byte []> comparator)\n+    public Writer(FileSystem fs, Path path, int blocksize,\n+      Compression.Algorithm compress,\n+      final RawComparator<byte []> comparator,\n+      final boolean bloomfilter)\n     throws IOException {\n-      this(fs.create(path), blocksize, compress, comparator);\n+      this(fs.create(path), blocksize, compress, comparator, bloomfilter);\n       this.closeOutputStream = true;\n       this.name = path.toString();\n       this.path = path;\n@@ -243,19 +268,38 @@ public Writer(FileSystem fs, Path path, int blocksize, String compress,\n      * @param blocksize\n      * @param compress\n      * @param c\n+     * @param bloomfilter\n      * @throws IOException\n      */\n     public Writer(final FSDataOutputStream ostream, final int blocksize,\n-        final String compress, final RawComparator<byte []> c)\n+        final String  compress, final RawComparator<byte []> c)\n+    throws IOException {\n+      this(ostream, blocksize,\n+        compress == null? DEFAULT_COMPRESSION_ALGORITHM:\n+          Compression.getCompressionAlgorithmByName(compress), c, false);\n+    }\n+\n+    /**\n+     * Constructor that takes a stream.\n+     * @param ostream Stream to use.\n+     * @param blocksize\n+     * @param compress\n+     * @param c\n+     * @param bloomfilter\n+     * @throws IOException\n+     */\n+    public Writer(final FSDataOutputStream ostream, final int blocksize,\n+        final Compression.Algorithm  compress,\n+        final RawComparator<byte []> c,\n+        final boolean bloomfilter)\n     throws IOException {\n       this.outputStream = ostream;\n       this.closeOutputStream = false;\n       this.blocksize = blocksize;\n       this.comparator = c == null? Bytes.BYTES_RAWCOMPARATOR: c;\n       this.name = this.outputStream.toString();\n-      this.compressAlgo =\n-        Compression.getCompressionAlgorithmByName(compress == null?\n-          Compression.Algorithm.NONE.getName(): compress);\n+      this.compressAlgo = compress == null?\n+        DEFAULT_COMPRESSION_ALGORITHM: compress;\n     }\n \n     /*\n@@ -391,7 +435,7 @@ public String toString() {\n \n     /**\n      * Add key/value to file.\n-     * Keys must be added in an order that agrees with the RawComparator passed\n+     * Keys must be added in an order that agrees with the Comparator passed\n      * on construction.\n      * @param key Key to add.  Cannot be empty nor null.\n      * @param value Value to add.  Cannot be empty nor null.\n@@ -430,7 +474,7 @@ private void checkKey(final byte [] key) throws IOException {\n       if (this.lastKey != null) {\n         if (this.comparator.compare(this.lastKey, key) > 0) {\n           throw new IOException(\"Added a key not lexically larger than\" +\n-            \" previous: key=\" + Bytes.toString(key) + \", lastkey=\" +\n+            \" previous key=\" + Bytes.toString(key) + \", lastkey=\" +\n             Bytes.toString(lastKey));\n         }\n       }\n@@ -622,14 +666,22 @@ public Reader(final FSDataInputStream fsdis, final long size,\n \n     public String toString() {\n       return \"reader=\" + this.name +\n-        (!isFileInfoLoaded()? \"\":\n-          \", compression=\" + this.compressAlgo.getName() +\n-          \", firstKey=\" + Bytes.toString(getFirstKey()) +\n-          \", lastKey=\" + Bytes.toString(getLastKey()) +\n-          \", avgKeyLen=\" + this.avgKeyLen +\n-          \", avgValueLen=\" + this.avgValueLen +\n-          \", entries=\" + this.trailer.entryCount +\n-          \", length=\" + this.fileSize);\n+          (!isFileInfoLoaded()? \"\":\n+            \", compression=\" + this.compressAlgo.getName() +\n+            \", firstKey=\" + toStringFirstKey() +\n+            \", lastKey=\" + toStringLastKey()) +\n+            \", avgKeyLen=\" + this.avgKeyLen +\n+            \", avgValueLen=\" + this.avgValueLen +\n+            \", entries=\" + this.trailer.entryCount +\n+            \", length=\" + this.fileSize;\n+    }\n+\n+    protected String toStringFirstKey() {\n+      return Bytes.toString(getFirstKey());\n+    }\n+\n+    protected String toStringLastKey() {\n+      return Bytes.toString(getFirstKey());\n     }\n \n     public long length() {\n@@ -661,7 +713,7 @@ public long length() {\n \n       // Read in the metadata index.\n       if (trailer.metaIndexCount > 0) {\n-        this.metaIndex = BlockIndex.readIndex(Bytes.BYTES_RAWCOMPARATOR,\n+        this.metaIndex = BlockIndex.readIndex(this.comparator,\n           this.istream, this.trailer.metaIndexOffset, trailer.metaIndexCount);\n       }\n       this.fileInfoLoaded = true;\n@@ -679,7 +731,7 @@ boolean isFileInfoLoaded() {\n         return null;\n       }\n       try {\n-        return (RawComparator)Class.forName(clazzName).newInstance();\n+        return (RawComparator<byte[]>) Class.forName(clazzName).newInstance();\n       } catch (InstantiationException e) {\n         throw new IOException(e);\n       } catch (IllegalAccessException e) {\n@@ -1014,7 +1066,7 @@ private int blockSeek(byte[] key, boolean seekBefore) {\n           klen = block.getInt();\n           vlen = block.getInt();\n           int comp = this.reader.comparator.compare(key, 0, key.length,\n-              block.array(), block.arrayOffset() + block.position(), klen);\n+            block.array(), block.arrayOffset() + block.position(), klen);\n           if (comp == 0) {\n             if (seekBefore) {\n               block.position(block.position() - lastLen - 16);\n@@ -1035,8 +1087,10 @@ private int blockSeek(byte[] key, boolean seekBefore) {\n           }\n           block.position(block.position() + klen + vlen);\n           lastLen = klen + vlen ;\n-        } while( block.remaining() > 0 );\n+        } while(block.remaining() > 0);\n         // ok we are at the end, so go back a littleeeeee....\n+        // The 8 in the below is intentionally different to the 16s in the above\n+        // Do the math you you'll figure it.\n         block.position(block.position() - lastLen - 8);\n         currKeyLen = block.getInt();\n         currValueLen = block.getInt();\n@@ -1213,7 +1267,7 @@ public String toString() {\n   \n     /* Needed doing lookup on blocks.\n      */\n-    RawComparator<byte []> comparator;\n+    final RawComparator<byte []> comparator;\n   \n     /*\n      * Shutdown default constructor\n@@ -1227,7 +1281,7 @@ private BlockIndex() {\n      * Constructor\n      * @param trailer File tail structure with index stats.\n      */\n-    BlockIndex(final RawComparator<byte []> c) {\n+    BlockIndex(final RawComparator<byte []>c) {\n       this.comparator = c;\n       // Guess that cost of three arrays + this object is 4 * 8 bytes.\n       this.size += (4 * 8);",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "sha": "609f8cccaf75166d2605f7b0416d4b7dce0b78e5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/MetaRegion.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "patch": "@@ -21,7 +21,6 @@\n \n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HServerAddress;\n-import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.util.Bytes;\n \n \n@@ -87,7 +86,7 @@ public int hashCode() {\n   public int compareTo(MetaRegion other) {\n     int result = Bytes.compareTo(this.regionName, other.getRegionName());\n     if(result == 0) {\n-      result = HStoreKey.compareTwoRowKeys(this.startKey, other.getStartKey());\n+      result = Bytes.compareTo(this.startKey, other.getStartKey());\n       if (result == 0) {\n         // Might be on different host?\n         result = this.server.compareTo(other.server);",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "sha": "3f1b0bbcad790863bfa4d295dd466619a52276b3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "patch": "@@ -125,7 +125,8 @@ boolean isWildCardMatch() {\n   private boolean multipleMatchers;\n \n   /** Constructor for abstract base class */\n-  protected HAbstractScanner(long timestamp, byte [][] targetCols) throws IOException {\n+  protected HAbstractScanner(long timestamp, byte [][] targetCols)\n+  throws IOException {\n     this.timestamp = timestamp;\n     this.wildcardMatch = false;\n     this.multipleMatchers = false;",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "sha": "6e593915ee7d668218b12b7322ec356dac53ae49",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 30,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -1440,11 +1440,10 @@ public void deleteAll(final byte [] row, final long ts, final Integer lockid)\n     long now = System.currentTimeMillis();\n     try {\n       for (Store store : stores.values()) {\n-        List<HStoreKey> keys =\n-          store.getKeys(new HStoreKey(row, ts),\n-            ALL_VERSIONS, now, null);\n+        List<HStoreKey> keys = store.getKeys(new HStoreKey(row, ts),\n+          ALL_VERSIONS, now, null);\n         TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-          new HStoreKey.HStoreKeyWritableComparator());\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n         for (HStoreKey key: keys) {\n           edits.put(key, HLogEdit.DELETED_BYTES);\n         }\n@@ -1474,15 +1473,14 @@ public void deleteAllByRegex(final byte [] row, final String columnRegex,\n     long now = System.currentTimeMillis();\n     try {\n       for (Store store : stores.values()) {\n-        List<HStoreKey> keys =\n-          store.getKeys(new HStoreKey(row, timestamp),\n+        List<HStoreKey> keys = store.getKeys(new HStoreKey(row, timestamp),\n             ALL_VERSIONS, now, columnPattern);\n-          TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-            new HStoreKey.HStoreKeyWritableComparator());\n-          for (HStoreKey key: keys) {\n-            edits.put(key, HLogEdit.DELETED_BYTES);\n-          }\n-          update(edits);\n+        TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n+        for (HStoreKey key: keys) {\n+          edits.put(key, HLogEdit.DELETED_BYTES);\n+        }\n+        update(edits);\n       }\n     } finally {\n       if(lockid == null) releaseRowLock(lid);\n@@ -1514,7 +1512,7 @@ public void deleteFamily(byte [] row, byte [] family, long timestamp,\n         ALL_VERSIONS, now, null);\n       // delete all the cells\n       TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        HStoreKey.getWritableComparator(store.getHRegionInfo()));\n       for (HStoreKey key: keys) {\n         edits.put(key, HLogEdit.DELETED_BYTES);\n       }\n@@ -1553,7 +1551,7 @@ public void deleteFamilyByRegex(byte [] row, String familyRegex, long timestamp,\n         List<HStoreKey> keys = store.getKeys(new HStoreKey(row, timestamp),\n           ALL_VERSIONS, now, null);\n         TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-          new HStoreKey.HStoreKeyWritableComparator());\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n         for (HStoreKey key: keys) {\n           edits.put(key, HLogEdit.DELETED_BYTES);\n         }\n@@ -1583,8 +1581,11 @@ private void deleteMultiple(final byte [] row, final byte [] column,\n     HStoreKey origin = new HStoreKey(row, column, ts);\n     Set<HStoreKey> keys = getKeys(origin, versions);\n     if (keys.size() > 0) {\n+      // I think the below map doesn't have to be exactly storetd.  Its deletes\n+      // they don't have to go in in exact sorted order (we don't have to worry\n+      // about the meta or root sort comparator here).\n       TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        new HStoreKey.HStoreKeyComparator());\n       for (HStoreKey key: keys) {\n         edits.put(key, HLogEdit.DELETED_BYTES);\n       }\n@@ -1650,8 +1651,10 @@ private void localput(final Integer lockid, final HStoreKey key,\n     checkReadOnly();\n     TreeMap<HStoreKey, byte []> targets = this.targetColumns.get(lockid);\n     if (targets == null) {\n-      targets = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+      // I think the below map doesn't have to be exactly storetd.  Its deletes\n+      // they don't have to go in in exact sorted order (we don't have to worry\n+      // about the meta or root sort comparator here).\n+      targets = new TreeMap<HStoreKey, byte []>(new HStoreKey.HStoreKeyComparator());\n       this.targetColumns.put(lockid, targets);\n     }\n     targets.put(key, val);\n@@ -1948,9 +1951,7 @@ public Path getBaseDir() {\n       this.scanners = new InternalScanner[stores.length];\n       try {\n         for (int i = 0; i < stores.length; i++) {\n-          \n           // Only pass relevant columns to each store\n-          \n           List<byte[]> columns = new ArrayList<byte[]>();\n           for (int j = 0; j < cols.length; j++) {\n             if (Bytes.equals(HStoreKey.getFamily(cols[j]),\n@@ -2007,8 +2008,8 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         for (int i = 0; i < this.keys.length; i++) {\n           if (scanners[i] != null &&\n              (chosenRow == null ||\n-               (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) < 0) ||\n-               ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) &&\n+               (Bytes.compareTo(this.keys[i].getRow(), chosenRow) < 0) ||\n+               ((Bytes.compareTo(this.keys[i].getRow(), chosenRow) == 0) &&\n                       (keys[i].getTimestamp() > chosenTimestamp)))) {\n             chosenRow = keys[i].getRow();\n             chosenTimestamp = keys[i].getTimestamp();\n@@ -2025,7 +2026,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n \n           for (int i = 0; i < scanners.length; i++) {\n             if (scanners[i] != null &&\n-              HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) {\n+              Bytes.compareTo(this.keys[i].getRow(), chosenRow) == 0) {\n               // NOTE: We used to do results.putAll(resultSets[i]);\n               // but this had the effect of overwriting newer\n               // values with older ones. So now we only insert\n@@ -2047,7 +2048,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n           // If the current scanner is non-null AND has a lower-or-equal\n           // row label, then its timestamp is bad. We need to advance it.\n           while ((scanners[i] != null) &&\n-              (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) <= 0)) {\n+              (Bytes.compareTo(this.keys[i].getRow(), chosenRow) <= 0)) {\n             resultSets[i].clear();\n             if (!scanners[i].next(keys[i], resultSets[i])) {\n               closeScanner(i);\n@@ -2228,7 +2229,7 @@ public static void addRegionToMETA(HRegion meta, HRegion r)\n       HStoreKey key = new HStoreKey(row, COL_REGIONINFO,\n         System.currentTimeMillis());\n       TreeMap<HStoreKey, byte[]> edits = new TreeMap<HStoreKey, byte[]>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        HStoreKey.getWritableComparator(r.getRegionInfo()));\n       edits.put(key, Writables.getBytes(r.getRegionInfo()));\n       meta.update(edits);\n     } finally {\n@@ -2351,9 +2352,9 @@ public static Path getRegionDir(final Path rootdir, final HRegionInfo info) {\n    */\n   public static boolean rowIsInRange(HRegionInfo info, final byte [] row) {\n     return ((info.getStartKey().length == 0) ||\n-        (HStoreKey.compareTwoRowKeys(info.getStartKey(), row) <= 0)) &&\n+        (Bytes.compareTo(info.getStartKey(), row) <= 0)) &&\n         ((info.getEndKey().length == 0) ||\n-            (HStoreKey.compareTwoRowKeys(info.getEndKey(), row) > 0));\n+            (Bytes.compareTo(info.getEndKey(), row) > 0));\n   }\n \n   /**\n@@ -2396,7 +2397,7 @@ public static HRegion mergeAdjacent(final HRegion srcA, final HRegion srcB)\n       }\n       // A's start key is null but B's isn't. Assume A comes before B\n     } else if ((srcB.getStartKey() == null) ||\n-      (HStoreKey.compareTwoRowKeys(srcA.getStartKey(), srcB.getStartKey()) > 0)) {\n+      (Bytes.compareTo(srcA.getStartKey(), srcB.getStartKey()) > 0)) {\n       a = srcB;\n       b = srcA;\n     }\n@@ -2448,14 +2449,14 @@ public static HRegion merge(HRegion a, HRegion b) throws IOException {\n     final byte [] startKey = HStoreKey.equalsTwoRowKeys(a.getStartKey(),\n         EMPTY_BYTE_ARRAY) ||\n       HStoreKey.equalsTwoRowKeys(b.getStartKey(), EMPTY_BYTE_ARRAY)?\n-        EMPTY_BYTE_ARRAY: HStoreKey.compareTwoRowKeys(a.getStartKey(), \n+        EMPTY_BYTE_ARRAY: Bytes.compareTo(a.getStartKey(), \n           b.getStartKey()) <= 0?\n         a.getStartKey(): b.getStartKey();\n     final byte [] endKey = HStoreKey.equalsTwoRowKeys(a.getEndKey(),\n         EMPTY_BYTE_ARRAY) ||\n       HStoreKey.equalsTwoRowKeys(b.getEndKey(), EMPTY_BYTE_ARRAY)?\n         EMPTY_BYTE_ARRAY:\n-        HStoreKey.compareTwoRowKeys(a.getEndKey(), b.getEndKey()) <= 0? b.getEndKey(): a.getEndKey();\n+        Bytes.compareTo(a.getEndKey(), b.getEndKey()) <= 0? b.getEndKey(): a.getEndKey();\n \n     HRegionInfo newRegionInfo = new HRegionInfo(tabledesc, startKey, endKey);\n     LOG.info(\"Creating new region \" + newRegionInfo.toString());\n@@ -2584,4 +2585,4 @@ private static void listPaths(FileSystem fs, Path dir) throws IOException {\n       }\n     }\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "dcb73a720d8d4a5ae705b9712df01d636a0dafd0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -1608,8 +1608,8 @@ public RowResult next(final long scannerId) throws IOException {\n     }\n   }\n \n-  public void batchUpdate(final byte [] regionName, BatchUpdate b,\n-      long lockId) throws IOException {\n+  public void batchUpdate(final byte [] regionName, BatchUpdate b, long lockId)\n+  throws IOException {\n     if (b.getRow() == null)\n       throw new IllegalArgumentException(\"update has null row\");\n     ",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "7bee630a0d0296419257ffd87414a20f43be877e",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 21,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.rmi.UnexpectedException;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n@@ -68,35 +69,38 @@\n \n   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n \n+  private final Comparator<HStoreKey> comparator;\n+  private final HStoreKey.StoreKeyComparator rawcomparator;\n+\n   /**\n    * Default constructor. Used for tests.\n    */\n+  @SuppressWarnings(\"unchecked\")\n   public Memcache() {\n-    this.ttl = HConstants.FOREVER;\n-    this.memcache = createSynchronizedSortedMap();\n-    this.snapshot = createSynchronizedSortedMap();\n+    this(HConstants.FOREVER, new HStoreKey.HStoreKeyComparator(),\n+      new HStoreKey.StoreKeyComparator());\n   }\n \n   /**\n    * Constructor.\n    * @param ttl The TTL for cache entries, in milliseconds.\n    * @param regionInfo The HRI for this cache \n    */\n-  public Memcache(final long ttl) {\n+  public Memcache(final long ttl, final Comparator<HStoreKey> c,\n+      final HStoreKey.StoreKeyComparator rc) {\n     this.ttl = ttl;\n-    this.memcache = createSynchronizedSortedMap();\n-    this.snapshot = createSynchronizedSortedMap();\n+    this.comparator = c;\n+    this.rawcomparator = rc;\n+    this.memcache = createSynchronizedSortedMap(c);\n+    this.snapshot = createSynchronizedSortedMap(c);\n   }\n \n   /*\n    * Utility method using HSKWritableComparator\n    * @return synchronized sorted map of HStoreKey to byte arrays.\n    */\n-  @SuppressWarnings(\"unchecked\")\n-  private SortedMap<HStoreKey, byte[]> createSynchronizedSortedMap() {\n-    return Collections.synchronizedSortedMap(\n-      new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator()));\n+  private SortedMap<HStoreKey, byte[]> createSynchronizedSortedMap(final Comparator<HStoreKey> c) {\n+    return Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>(c));\n   }\n \n   /**\n@@ -119,7 +123,7 @@ void snapshot() {\n         // mistake. St.Ack\n         if (this.memcache.size() != 0) {\n           this.snapshot = this.memcache;\n-          this.memcache = createSynchronizedSortedMap();\n+          this.memcache = createSynchronizedSortedMap(this.comparator);\n         }\n       }\n     } finally {\n@@ -156,7 +160,7 @@ void clearSnapshot(final SortedMap<HStoreKey, byte []> ss)\n       // OK. Passed in snapshot is same as current snapshot.  If not-empty,\n       // create a new snapshot and let the old one go.\n       if (ss.size() != 0) {\n-        this.snapshot = createSynchronizedSortedMap();\n+        this.snapshot = createSynchronizedSortedMap(this.comparator);\n       }\n     } finally {\n       this.lock.writeLock().unlock();\n@@ -261,7 +265,7 @@ long heapSize(final HStoreKey key, final byte [] value,\n     if (b == null) {\n       return a;\n     }\n-    return HStoreKey.compareTwoRowKeys(a, b) <= 0? a: b;\n+    return Bytes.compareTo(a, b) <= 0? a: b;\n   }\n \n   /**\n@@ -296,7 +300,7 @@ long heapSize(final HStoreKey key, final byte [] value,\n       // Iterate until we fall into the next row; i.e. move off current row\n       for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n         HStoreKey itKey = es.getKey();\n-        if (HStoreKey.compareTwoRowKeys(itKey.getRow(), row) <= 0)\n+        if (Bytes.compareTo(itKey.getRow(), row) <= 0)\n           continue;\n         // Note: Not suppressing deletes or expired cells.\n         result = itKey.getRow();\n@@ -344,7 +348,8 @@ private void internalGetFull(SortedMap<HStoreKey, byte[]> map, HStoreKey key,\n       HStoreKey itKey = es.getKey();\n       byte [] itCol = itKey.getColumn();\n       Cell cell = results.get(itCol);\n-      if ((cell == null || cell.getNumValues() < numVersions) && key.matchesWithoutColumn(itKey)) {\n+      if ((cell == null || cell.getNumValues() < numVersions) &&\n+          key.matchesWithoutColumn(itKey)) {\n         if (columns == null || columns.contains(itKey.getColumn())) {\n           byte [] val = tailMap.get(itKey);\n           if (HLogEdit.isDeleted(val)) {\n@@ -367,7 +372,7 @@ private void internalGetFull(SortedMap<HStoreKey, byte[]> map, HStoreKey key,\n             }\n           }\n         }\n-      } else if (HStoreKey.compareTwoRowKeys(key.getRow(), itKey.getRow()) < 0) {\n+      } else if (this.rawcomparator.compareRows(key.getRow(), itKey.getRow()) < 0) {\n         break;\n       }\n     }\n@@ -428,7 +433,7 @@ private void getRowKeyAtOrBefore(final SortedMap<HStoreKey, byte []> map,\n     // the search key, or a range of values between the first candidate key\n     // and the ultimate search key (or the end of the cache)\n     if (!tailMap.isEmpty() &&\n-        HStoreKey.compareTwoRowKeys(tailMap.firstKey().getRow(),\n+        Bytes.compareTo(tailMap.firstKey().getRow(),\n           search_key.getRow()) <= 0) {\n       Iterator<HStoreKey> key_iterator = tailMap.keySet().iterator();\n \n@@ -437,9 +442,9 @@ private void getRowKeyAtOrBefore(final SortedMap<HStoreKey, byte []> map,\n       HStoreKey deletedOrExpiredRow = null;\n       for (HStoreKey found_key = null; key_iterator.hasNext() &&\n           (found_key == null ||\n-            HStoreKey.compareTwoRowKeys(found_key.getRow(), row) <= 0);) {\n+            Bytes.compareTo(found_key.getRow(), row) <= 0);) {\n         found_key = key_iterator.next();\n-        if (HStoreKey.compareTwoRowKeys(found_key.getRow(), row) <= 0) {\n+        if (Bytes.compareTo(found_key.getRow(), row) <= 0) {\n           if (HLogEdit.isDeleted(tailMap.get(found_key))) {\n             Store.handleDeleted(found_key, candidateKeys, deletes);\n             if (deletedOrExpiredRow == null) {\n@@ -900,4 +905,4 @@ public static void main(String [] args) throws InterruptedException {\n     }\n     LOG.info(\"Exiting.\");\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "sha": "d33fb99ff1d915462e6994dafdbd0932545558e8",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 21,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n@@ -54,6 +55,7 @@\n import org.apache.hadoop.hbase.filter.RowFilterInterface;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.SequenceFile;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -126,6 +128,14 @@\n   private final Path compactionDir;\n   private final Integer compactLock = new Integer(0);\n   private final int compactionThreshold;\n+  private final int blocksize;\n+  private final boolean bloomfilter;\n+  private final Compression.Algorithm compression;\n+  \n+  // Comparing HStoreKeys in byte arrays.\n+  final HStoreKey.StoreKeyComparator rawcomparator;\n+  // Comparing HStoreKey objects.\n+  final Comparator<HStoreKey> comparator;\n \n   /**\n    * Constructor\n@@ -141,6 +151,7 @@\n    * failed.  Can be null.\n    * @throws IOException\n    */\n+  @SuppressWarnings(\"unchecked\")\n   protected Store(Path basedir, HRegionInfo info, HColumnDescriptor family,\n     FileSystem fs, Path reconstructionLog, HBaseConfiguration conf,\n     final Progressable reporter)\n@@ -151,12 +162,23 @@ protected Store(Path basedir, HRegionInfo info, HColumnDescriptor family,\n     this.family = family;\n     this.fs = fs;\n     this.conf = conf;\n+    this.bloomfilter = family.isBloomfilter();\n+    this.blocksize = family.getBlocksize();\n+    this.compression = family.getCompression();\n+    this.rawcomparator = info.isRootRegion()?\n+      new HStoreKey.RootStoreKeyComparator(): info.isMetaRegion()?\n+        new HStoreKey.MetaStoreKeyComparator():\n+          new HStoreKey.StoreKeyComparator();\n+    this.comparator = info.isRootRegion()?\n+      new HStoreKey.HStoreKeyRootComparator(): info.isMetaRegion()?\n+        new HStoreKey.HStoreKeyMetaComparator():\n+          new HStoreKey.HStoreKeyComparator();\n     // getTimeToLive returns ttl in seconds.  Convert to milliseconds.\n     this.ttl = family.getTimeToLive();\n     if (ttl != HConstants.FOREVER) {\n       this.ttl *= 1000;\n     }\n-    this.memcache = new Memcache(this.ttl);\n+    this.memcache = new Memcache(this.ttl, this.comparator, this.rawcomparator);\n     this.compactionDir = HRegion.getCompactionDir(basedir);\n     this.storeName = Bytes.toBytes(this.regioninfo.getEncodedName() + \"/\" +\n       Bytes.toString(this.family.getName()));\n@@ -254,7 +276,6 @@ private void runReconstructionLog(final Path reconstructionLog,\n    * lower than maxSeqID.  (Because we know such log messages are already \n    * reflected in the MapFiles.)\n    */\n-  @SuppressWarnings(\"unchecked\")\n   private void doReconstructionLog(final Path reconstructionLog,\n     final long maxSeqID, final Progressable reporter)\n   throws UnsupportedEncodingException, IOException {\n@@ -273,7 +294,7 @@ private void doReconstructionLog(final Path reconstructionLog,\n     // general memory usage accounting.\n     long maxSeqIdInLog = -1;\n     NavigableMap<HStoreKey, byte []> reconstructedCache =\n-      new TreeMap<HStoreKey, byte []>(new HStoreKey.HStoreKeyWritableComparator());\n+      new TreeMap<HStoreKey, byte []>(this.comparator);\n     SequenceFile.Reader logReader = new SequenceFile.Reader(this.fs,\n       reconstructionLog, this.conf);\n     try {\n@@ -463,7 +484,7 @@ private StoreFile internalFlushCache(final SortedMap<HStoreKey, byte []> cache,\n     // if we fail.\n     synchronized (flushLock) {\n       // A. Write the map out to the disk\n-      writer = StoreFile.getWriter(this.fs, this.homedir);\n+      writer = getWriter();\n       int entries = 0;\n       try {\n         for (Map.Entry<HStoreKey, byte []> es: cache.entrySet()) {\n@@ -493,7 +514,16 @@ private StoreFile internalFlushCache(final SortedMap<HStoreKey, byte []> cache,\n     }\n     return sf;\n   }\n-  \n+\n+  /**\n+   * @return Writer for this store.\n+   * @throws IOException\n+   */\n+  HFile.Writer getWriter() throws IOException {\n+    return StoreFile.getWriter(this.fs, this.homedir, this.blocksize,\n+        this.compression, this.rawcomparator, this.bloomfilter);\n+  }\n+\n   /*\n    * Change storefiles adding into place the Reader produced by this new flush.\n    * @param logCacheFlushId\n@@ -650,7 +680,7 @@ StoreSize compact(final boolean mc) throws IOException {\n       }\n  \n       // Step through them, writing to the brand-new file\n-      HFile.Writer writer = StoreFile.getWriter(this.fs, this.homedir);\n+      HFile.Writer writer = getWriter();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Started compaction of \" + filesToCompact.size() + \" file(s)\" +\n           (references? \", hasReferences=true,\": \" \") + \" into \" +\n@@ -992,6 +1022,7 @@ void getFull(HStoreKey key, final Set<byte []> columns,\n       final int numVersions, Map<byte [], Cell> results)\n   throws IOException {\n     int versions = versionsToReturn(numVersions);\n+    // This is map of columns to timestamp\n     Map<byte [], Long> deletes =\n       new TreeMap<byte [], Long>(Bytes.BYTES_COMPARATOR);\n     // if the key is null, we're not even looking for anything. return.\n@@ -1061,7 +1092,9 @@ private void getFullFromStoreFile(StoreFile f, HStoreKey key,\n             }\n           }\n         }\n-      } else if (HStoreKey.compareTwoRowKeys(key.getRow(), readkey.getRow()) < 0) {\n+      } else if (this.rawcomparator.compareRows(key.getRow(), 0,\n+            key.getRow().length,\n+          readkey.getRow(), 0, readkey.getRow().length) < 0) {\n         // if we've crossed into the next row, then we can just stop\n         // iterating\n         break;\n@@ -1280,15 +1313,14 @@ private boolean hasEnoughVersions(final int versions, final List<Cell> c) {\n    * @return Found row\n    * @throws IOException\n    */\n-  @SuppressWarnings(\"unchecked\")\n   byte [] getRowKeyAtOrBefore(final byte [] row)\n   throws IOException{\n     // Map of HStoreKeys that are candidates for holding the row key that\n     // most closely matches what we're looking for. We'll have to update it as\n     // deletes are found all over the place as we go along before finally\n     // reading the best key out of it at the end.\n-    NavigableMap<HStoreKey, Long> candidateKeys = new TreeMap<HStoreKey, Long>(\n-      new HStoreKey.HStoreKeyWritableComparator());\n+    NavigableMap<HStoreKey, Long> candidateKeys =\n+      new TreeMap<HStoreKey, Long>(this.comparator);\n     \n     // Keep a list of deleted cell keys.  We need this because as we go through\n     // the store files, the cell with the delete marker may be in one file and\n@@ -1365,11 +1397,13 @@ private void rowAtOrBeforeCandidate(final HStoreKey startKey,\n     // up to the row before and return that.\n     HStoreKey finalKey = HStoreKey.create(f.getReader().getLastKey());\n     HStoreKey searchKey = null;\n-    if (HStoreKey.compareTwoRowKeys(finalKey.getRow(), row) < 0) {\n+    if (this.rawcomparator.compareRows(finalKey.getRow(), 0,\n+          finalKey.getRow().length,\n+        row, 0, row.length) < 0) {\n       searchKey = finalKey;\n     } else {\n       searchKey = new HStoreKey(row);\n-      if (searchKey.compareTo(startKey) < 0) {\n+      if (this.comparator.compare(searchKey, startKey) < 0) {\n         searchKey = startKey;\n       }\n     }\n@@ -1435,8 +1469,9 @@ private void rowAtOrBeforeCandidate(final StoreFile f,\n           if (deletedOrExpiredRow == null) {\n             deletedOrExpiredRow = copy;\n           }\n-        } else if (HStoreKey.compareTwoRowKeys(readkey.getRow(),\n-            searchKey.getRow()) > 0) {\n+        } else if (this.rawcomparator.compareRows(readkey.getRow(), 0,\n+              readkey.getRow().length,\n+            searchKey.getRow(), 0, searchKey.getRow().length) > 0) {\n           // if the row key we just read is beyond the key we're searching for,\n           // then we're done.\n           break;\n@@ -1457,7 +1492,7 @@ private void rowAtOrBeforeCandidate(final StoreFile f,\n           }\n         }\n       } while(scanner.next() && (knownNoGoodKey == null ||\n-          readkey.compareTo(knownNoGoodKey) < 0));\n+          this.comparator.compare(readkey, knownNoGoodKey) < 0));\n \n       // If we get here and have no candidates but we did find a deleted or\n       // expired candidate, we need to look at the key before that\n@@ -1502,11 +1537,14 @@ private void rowAtOrBeforeWithCandidates(final HStoreKey startKey,\n     // of the row in case there are deletes for this candidate in this mapfile\n     // BUT do not backup before the first key in the store file.\n     // TODO: FIX THIS PROFLIGATE OBJECT MAKING!!!\n-    byte [] searchKey =\n-      new HStoreKey(candidateKeys.firstKey().getRow()).getBytes();\n-    if (f.getReader().getComparator().compare(searchKey, 0, searchKey.length,\n-        startKey.getRow(), 0, startKey.getRow().length) < 0) {\n+    HStoreKey firstCandidateKey = candidateKeys.firstKey();\n+    byte [] searchKey = null;\n+    HStoreKey.StoreKeyComparator c =\n+      (HStoreKey.StoreKeyComparator)f.getReader().getComparator();\n+    if (c.compareRows(firstCandidateKey.getRow(), startKey.getRow()) < 0) {\n       searchKey = startKey.getBytes();\n+    } else {\n+      searchKey = new HStoreKey(firstCandidateKey.getRow()).getBytes();\n     }\n \n     // Seek to the exact row, or the one that would be immediately before it\n@@ -1523,7 +1561,7 @@ private void rowAtOrBeforeWithCandidates(final HStoreKey startKey,\n       // as a candidate key\n       if (HStoreKey.equalsTwoRowKeys(k.getRow(), row)) {\n         handleKey(k, v, now, deletes, candidateKeys);\n-      } else if (HStoreKey.compareTwoRowKeys(k.getRow(), row) > 0 ) {\n+      } else if (this.rawcomparator.compareRows(k.getRow(), row) > 0 ) {\n         // if the row key we just read is beyond the key we're searching for,\n         // then we're done.\n         break;\n@@ -1804,4 +1842,4 @@ static boolean getClosest(final HFileScanner s, final byte [] b)\n     }\n     return true;\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "sha": "c1c1b0abeef3314f1b52fc3a164917b806eeef33",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "patch": "@@ -35,9 +35,10 @@\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.HalfHFileReader;\n import org.apache.hadoop.hbase.io.Reference;\n+import org.apache.hadoop.hbase.io.hfile.BlockCache;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.apache.hadoop.io.RawComparator;\n \n /**\n  * A Store data file.  Stores usually have one or more of these files.  They\n@@ -210,7 +211,7 @@ public long getMaxSequenceId() {\n       this.reader = new HalfHFileReader(this.fs, this.referencePath, null,\n         this.reference);\n     } else {\n-      this.reader = new HFile.Reader(this.fs, this.path, null);\n+      this.reader = new StoreFileReader(this.fs, this.path, null);\n     }\n     // Load up indices and fileinfo.\n     Map<byte [], byte []> map = this.reader.loadFileInfo();\n@@ -241,6 +242,71 @@ public long getMaxSequenceId() {\n     }\n     return this.reader;\n   }\n+  \n+  /**\n+   * Override to add some customization on HFile.Reader\n+   */\n+  static class StoreFileReader extends HFile.Reader {\n+    public StoreFileReader(FileSystem fs, Path path, BlockCache cache)\n+        throws IOException {\n+      super(fs, path, cache);\n+    }\n+\n+    protected String toStringFirstKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getFirstKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString first key\", e);\n+      }\n+      return result;\n+    }\n+\n+    protected String toStringLastKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getLastKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString last key\", e);\n+      }\n+      return result;\n+    }\n+  }\n+\n+  /**\n+   * Override to add some customization on HalfHFileReader\n+   */\n+  static class HalfStoreFileReader extends HalfHFileReader {\n+    public HalfStoreFileReader(FileSystem fs, Path p, BlockCache c, Reference r)\n+        throws IOException {\n+      super(fs, p, c, r);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return super.toString() + (isTop()? \", half=top\": \", half=bottom\");\n+    }\n+\n+    protected String toStringFirstKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getFirstKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString first key\", e);\n+      }\n+      return result;\n+    }\n+\n+    protected String toStringLastKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getLastKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString last key\", e);\n+      }\n+      return result;\n+    }\n+  }\n \n   /**\n    * @return Current reader.  Must call open first.\n@@ -309,7 +375,7 @@ public static Path rename(final FileSystem fs, final Path src,\n    */\n   public static HFile.Writer getWriter(final FileSystem fs, final Path dir)\n   throws IOException {\n-    return getWriter(fs, dir, DEFAULT_BLOCKSIZE_SMALL, null, null);\n+    return getWriter(fs, dir, DEFAULT_BLOCKSIZE_SMALL, null, null, false);\n   }\n \n   /**\n@@ -326,15 +392,16 @@ public static Path rename(final FileSystem fs, final Path src,\n    * @throws IOException\n    */\n   public static HFile.Writer getWriter(final FileSystem fs, final Path dir,\n-    final int blocksize, final String algorithm, final RawComparator<byte []> c)\n+    final int blocksize, final Compression.Algorithm algorithm,\n+    final HStoreKey.StoreKeyComparator c, final boolean bloomfilter)\n   throws IOException {\n     if (!fs.exists(dir)) {\n       fs.mkdirs(dir);\n     }\n     Path path = getUniqueFile(fs, dir);\n     return new HFile.Writer(fs, path, blocksize,\n-      algorithm == null? HFile.DEFAULT_COMPRESSION: algorithm,\n-      c == null? HStoreKey.BYTECOMPARATOR: c);\n+      algorithm == null? HFile.DEFAULT_COMPRESSION_ALGORITHM: algorithm,\n+      c == null? new HStoreKey.StoreKeyComparator(): c, bloomfilter);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "sha": "39b763b70a7dc2629b11d82262ab86d25b8631b0",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 16,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "patch": "@@ -141,19 +141,17 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n     try {\n       // Find the next viable row label (and timestamp).\n       ViableRow viableRow = getNextViableRow();\n-      \n+\n       // Grab all the values that match this row/timestamp\n       boolean insertedItem = false;\n       if (viableRow.getRow() != null) {\n         key.setRow(viableRow.getRow());\n         key.setVersion(viableRow.getTimestamp());\n-\n         for (int i = 0; i < keys.length; i++) {\n           // Fetch the data\n           while ((keys[i] != null) &&\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(),\n-              viableRow.getRow()) == 0)) {\n-\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow.getRow()) == 0)) {\n             // If we are doing a wild card match or there are multiple matchers\n             // per column, we need to scan all the older versions of this row\n             // to pick up the rest of the family members\n@@ -162,8 +160,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n                 && (keys[i].getTimestamp() != viableRow.getTimestamp())) {\n               break;\n             }\n-\n-            if(columnMatch(i)) {              \n+            if(columnMatch(i)) {\n               // We only want the first result for any specific family member\n               if(!results.containsKey(keys[i].getColumn())) {\n                 results.put(keys[i].getColumn(), \n@@ -179,10 +176,10 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n           // Advance the current scanner beyond the chosen row, to\n           // a valid timestamp, so we're ready next time.\n           while ((keys[i] != null) &&\n-              ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(),\n-                viableRow.getRow()) <= 0)\n-                  || (keys[i].getTimestamp() > this.timestamp)\n-                  || (! columnMatch(i)))) {\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow.getRow()) <= 0) ||\n+              (keys[i].getTimestamp() > this.timestamp) ||\n+              (! columnMatch(i)))) {\n             getNext(i);\n           }\n         }\n@@ -192,7 +189,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n       this.lock.readLock().unlock();\n     }\n   }\n-  \n+\n   // Data stucture to hold next, viable row (and timestamp).\n   class ViableRow {\n     private final byte [] row;\n@@ -239,8 +236,10 @@ private ViableRow getNextViableRow() throws IOException {\n           // column matches and the timestamp of the row is less than or equal\n           // to this.timestamp, so we do not need to test that here\n           && ((viableRow == null) ||\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), viableRow) < 0) ||\n-            ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), viableRow) == 0) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              viableRow) < 0) ||\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow) == 0) &&\n               (keys[i].getTimestamp() > viableTimestamp)))) {\n         if (ttl == HConstants.FOREVER || now < keys[i].getTimestamp() + ttl) {\n           viableRow = keys[i].getRow();\n@@ -270,8 +269,7 @@ private boolean findFirstRow(int i, final byte [] firstRow) throws IOException {\n         return true;\n       }\n     } else {\n-      if (!Store.getClosest(this.scanners[i],\n-          new HStoreKey(firstRow).getBytes())) {\n+      if (!Store.getClosest(this.scanners[i], HStoreKey.getBytes(firstRow))) {\n         closeSubScanner(i);\n         return true;\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "sha": "7bbbf048941c69570f2432911a63f01882338f15",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 8,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -153,8 +153,10 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n       for (int i = 0; i < this.keys.length; i++) {\n         if (scanners[i] != null &&\n             (chosenRow == null ||\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) < 0) ||\n-            ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) < 0) ||\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) == 0) &&\n             (keys[i].getTimestamp() > chosenTimestamp)))) {\n           chosenRow = keys[i].getRow();\n           chosenTimestamp = keys[i].getTimestamp();\n@@ -181,10 +183,9 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         // problem, could redo as bloom filter.\n         Set<HStoreKey> deletes = new HashSet<HStoreKey>();\n         for (int i = 0; i < scanners.length && !filtered; i++) {\n-          while ((scanners[i] != null\n-              && !filtered\n-              && moreToFollow)\n-              && (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0)) {\n+          while ((scanners[i] != null && !filtered && moreToFollow) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) == 0)) {\n             // If we are doing a wild card match or there are multiple\n             // matchers per column, we need to scan all the older versions of \n             // this row to pick up the rest of the family members\n@@ -206,7 +207,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n               if (HLogEdit.isDeleted(e.getValue().getValue())) {\n                 // Only first key encountered is added; deletes is a Set.\n                 deletes.add(new HStoreKey(hsk));\n-              } else if (!deletes.contains(hsk) &&\n+              } else if ((deletes.size() == 0 || !deletes.contains(hsk)) &&\n                   !filtered &&\n                   moreToFollow &&\n                   !results.containsKey(e.getKey())) {\n@@ -233,7 +234,8 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         // If the current scanner is non-null AND has a lower-or-equal\n         // row label, then its timestamp is bad. We need to advance it.\n         while ((scanners[i] != null) &&\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) <= 0)) {\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) <= 0)) {\n           resultSets[i].clear();\n           if (!scanners[i].next(keys[i], resultSets[i])) {\n             closeScanner(i);",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "084d0c5d0caf326c7973c865fec2a0662690f09b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "patch": "@@ -23,6 +23,7 @@\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.RESTConstants;\n import org.apache.hadoop.hbase.rest.descriptors.RowUpdateDescriptor;\n import org.apache.hadoop.hbase.rest.descriptors.ScannerDescriptor;\n@@ -76,7 +77,7 @@ private HColumnDescriptor getColumnDescriptor(JSONObject jsonObject)\n     byte[] name = Bytes.toBytes(strTemp);\n \n     int maxVersions;\n-    HColumnDescriptor.CompressionType cType;\n+    String cType;\n     boolean inMemory;\n     boolean blockCacheEnabled;\n     int maxValueLength;\n@@ -96,10 +97,9 @@ private HColumnDescriptor getColumnDescriptor(JSONObject jsonObject)\n     }\n \n     try {\n-      cType = HColumnDescriptor.CompressionType.valueOf(jsonObject\n-          .getString(\"compression_type\"));\n+      cType = jsonObject.getString(\"compression_type\").toUpperCase();\n     } catch (JSONException e) {\n-      cType = HColumnDescriptor.CompressionType.NONE;\n+      cType = HColumnDescriptor.DEFAULT_COMPRESSION;\n     }\n \n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "sha": "caa6e75dc71b51cbe8e6edfaa1c2a1844cc31f37",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.RESTConstants;\n import org.apache.hadoop.hbase.rest.descriptors.RowUpdateDescriptor;\n import org.apache.hadoop.hbase.rest.descriptors.ScannerDescriptor;\n@@ -95,7 +96,7 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     String colname = makeColumnName(name_node.getFirstChild().getNodeValue());\n \n     int max_versions = HColumnDescriptor.DEFAULT_VERSIONS;\n-    CompressionType compression = HColumnDescriptor.DEFAULT_COMPRESSION;\n+    String compression = HColumnDescriptor.DEFAULT_COMPRESSION;\n     boolean in_memory = HColumnDescriptor.DEFAULT_IN_MEMORY;\n     boolean block_cache = HColumnDescriptor.DEFAULT_BLOCKCACHE;\n     int max_cell_size = HColumnDescriptor.DEFAULT_LENGTH;\n@@ -126,8 +127,8 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     NodeList compression_list = columnfamily\n         .getElementsByTagName(\"compression\");\n     if (compression_list.getLength() > 0) {\n-      compression = CompressionType.valueOf(compression_list.item(0)\n-          .getFirstChild().getNodeValue());\n+      compression = compression_list.item(0)\n+          .getFirstChild().getNodeValue().toUpperCase();\n     }\n \n     NodeList in_memory_list = columnfamily.getElementsByTagName(\"in-memory\");\n@@ -163,8 +164,8 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     }\n \n     HColumnDescriptor hcd = new HColumnDescriptor(Bytes.toBytes(colname),\n-        max_versions, compression, in_memory, block_cache, max_cell_size, ttl,\n-        bloomfilter);\n+        max_versions, compression, in_memory, block_cache,\n+        max_cell_size, ttl, bloomfilter);\n \n     NodeList metadataList = columnfamily.getElementsByTagName(\"metadata\");\n     for (int i = 0; i < metadataList.getLength(); i++) {",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "sha": "c9416045080d8db4943d7b4c0293b9bd5a6a0eae",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "patch": "@@ -22,9 +22,9 @@\n import java.util.TreeMap;\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.RowResult;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;\n import org.apache.hadoop.hbase.thrift.generated.IllegalArgument;\n import org.apache.hadoop.hbase.thrift.generated.TCell;\n@@ -44,7 +44,8 @@\n    */\n   static public HColumnDescriptor colDescFromThrift(ColumnDescriptor in)\n       throws IllegalArgument {\n-    CompressionType comp = CompressionType.valueOf(in.compression);\n+    Compression.Algorithm comp =\n+      Compression.getCompressionAlgorithmByName(in.compression.toLowerCase());\n     boolean bloom = false;\n     if (in.bloomFilterType.compareTo(\"NONE\") != 0) {\n       bloom = true;\n@@ -54,7 +55,7 @@ static public HColumnDescriptor colDescFromThrift(ColumnDescriptor in)\n       throw new IllegalArgument(\"column name is empty\");\n     }\n     HColumnDescriptor col = new HColumnDescriptor(in.name,\n-        in.maxVersions, comp, in.inMemory, in.blockCacheEnabled,\n+        in.maxVersions, comp.getName(), in.inMemory, in.blockCacheEnabled,\n         in.maxValueLength, in.timeToLive, bloom);\n     return col;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "sha": "80ce5da3889c2fb327fcd99fb3a2e6337bc3fde8",
                "status": "modified"
            },
            {
                "additions": 114,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "changes": 128,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Bytes.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 14,
                "filename": "src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "patch": "@@ -116,6 +116,15 @@ public static void writeByteArray(final DataOutput out, final byte [] b)\n     out.write(b, 0, b.length);\n   }\n \n+  public static int writeByteArray(final byte [] tgt, final int tgtOffset,\n+      final byte [] src, final int srcOffset, final int srcLength) {\n+    byte [] vint = vintToBytes(srcLength);\n+    System.arraycopy(vint, 0, tgt, tgtOffset, vint.length);\n+    int offset = tgtOffset + vint.length;\n+    System.arraycopy(src, srcOffset, tgt, offset, srcLength);\n+    return offset + srcLength;\n+  }\n+\n   /**\n    * Reads a zero-compressed encoded long from input stream and returns it.\n    * @param buffer Binary array\n@@ -218,16 +227,99 @@ public static boolean toBoolean(final byte [] b) {\n     return bb.array();\n   }\n \n+  /**\n+   * @param vint Integer to make a vint of.\n+   * @return Vint as bytes array.\n+   */\n+  public static byte [] vintToBytes(final long vint) {\n+    long i = vint;\n+    int size = WritableUtils.getVIntSize(i);\n+    byte [] result = new byte[size];\n+    int offset = 0;\n+    if (i >= -112 && i <= 127) {\n+      result[offset] = ((byte)i);\n+      return result;\n+    }\n+    offset++;\n+\n+    int len = -112;\n+    if (i < 0) {\n+      i ^= -1L; // take one's complement'\n+      len = -120;\n+    }\n+\n+    long tmp = i;\n+    while (tmp != 0) {\n+      tmp = tmp >> 8;\n+    len--;\n+    }\n+\n+    result[offset++] = (byte)len;\n+\n+    len = (len < -120) ? -(len + 120) : -(len + 112);\n+\n+    for (int idx = len; idx != 0; idx--) {\n+      int shiftbits = (idx - 1) * 8;\n+      long mask = 0xFFL << shiftbits;\n+      result[offset++] = (byte)((i & mask) >> shiftbits);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * @param buffer\n+   * @return vint bytes as an integer.\n+   */\n+  public static long bytesToVint(final byte [] buffer) {\n+    int offset = 0;\n+    byte firstByte = buffer[offset++];\n+    int len = WritableUtils.decodeVIntSize(firstByte);\n+    if (len == 1) {\n+      return firstByte;\n+    }\n+    long i = 0;\n+    for (int idx = 0; idx < len-1; idx++) {\n+      byte b = buffer[offset++];\n+      i = i << 8;\n+      i = i | (b & 0xFF);\n+    }\n+    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n+  }\n+\n   /**\n    * Converts a byte array to a long value\n    * @param bytes\n    * @return the long value\n    */\n   public static long toLong(byte[] bytes) {\n-    if (bytes == null || bytes.length == 0) {\n+    return toLong(bytes, 0);\n+  }\n+\n+  /**\n+   * Converts a byte array to a long value\n+   * @param bytes\n+   * @return the long value\n+   */\n+  public static long toLong(byte[] bytes, int offset) {\n+    return toLong(bytes, offset, SIZEOF_LONG);\n+  }\n+\n+  /**\n+   * Converts a byte array to a long value\n+   * @param bytes\n+   * @return the long value\n+   */\n+  public static long toLong(byte[] bytes, int offset,final int length) {\n+    if (bytes == null || bytes.length == 0 ||\n+        (offset + length > bytes.length)) {\n       return -1L;\n     }\n-    return ByteBuffer.wrap(bytes).getLong();\n+    long l = 0;\n+    for(int i = offset; i < (offset + length); i++) {\n+      l <<= 8;\n+      l ^= (long)bytes[i] & 0xFF;\n+    }\n+    return l;\n   }\n   \n   /**\n@@ -309,21 +401,29 @@ public static int compareTo(final byte [] left, final byte [] right) {\n   }\n \n   /**\n-   * @param left\n-   * @param right\n-   * @param leftOffset Where to start comparing in the left buffer\n-   * @param rightOffset Where to start comparing in the right buffer\n-   * @param leftLength How much to compare from the left buffer\n-   * @param rightLength How much to compare from the right buffer\n+   * @param b1\n+   * @param b2\n+   * @param s1 Where to start comparing in the left buffer\n+   * @param s2 Where to start comparing in the right buffer\n+   * @param l1 How much to compare from the left buffer\n+   * @param l2 How much to compare from the right buffer\n    * @return 0 if equal, < 0 if left is less than right, etc.\n    */\n-  public static int compareTo(final byte [] left, final int leftOffset,\n-      final int leftLength, final byte [] right, final int rightOffset,\n-      final int rightLength) {\n-    return WritableComparator.compareBytes(left,leftOffset, leftLength,\n-        right, rightOffset, rightLength);\n+  public static int compareTo(byte[] b1, int s1, int l1,\n+      byte[] b2, int s2, int l2) {\n+    // Bring WritableComparator code local\n+    int end1 = s1 + l1;\n+    int end2 = s2 + l2;\n+    for (int i = s1, j = s2; i < end1 && j < end2; i++, j++) {\n+      int a = (b1[i] & 0xff);\n+      int b = (b2[j] & 0xff);\n+      if (a != b) {\n+        return a - b;\n+      }\n+    }\n+    return l1 - l2;\n   }\n-  \n+\n   /**\n    * @param left\n    * @param right",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "sha": "d2d53b3a1358cfc3f76a6d987d8ec0563a153494",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/TestBytes.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "patch": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright 2009 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.util;\n+\n+import junit.framework.TestCase;\n+\n+public class TestBytes extends TestCase {\n+  public void testToLong() throws Exception {\n+    long [] longs = {-1l, 123l, 122232323232l};\n+    for (int i = 0; i < longs.length; i++) {\n+      byte [] b = Bytes.toBytes(longs[i]);\n+      assertEquals(longs[i], Bytes.toLong(b));\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "sha": "4c0cd9999547fcd414facc5cca493c1942a38402",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 15,
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "patch": "@@ -22,25 +22,25 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.SortedMap;\n-import java.util.Iterator;\n+\n import junit.framework.TestCase;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n-import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.RowResult;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n import org.apache.hadoop.hbase.regionserver.InternalScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n \n /**\n  * Abstract base class for test cases. Performs all static initialization\n@@ -93,8 +93,7 @@ public HBaseTestCase(String name) {\n   private void init() {\n     conf = new HBaseConfiguration();\n     try {\n-      START_KEY =\n-        new String(START_KEY_BYTES, HConstants.UTF8_ENCODING) + PUNCTUATION;\n+      START_KEY = new String(START_KEY_BYTES, HConstants.UTF8_ENCODING);\n     } catch (UnsupportedEncodingException e) {\n       LOG.fatal(\"error during initialization\", e);\n       fail();\n@@ -191,14 +190,14 @@ protected HTableDescriptor createTableDescriptor(final String name,\n       final int versions) {\n     HTableDescriptor htd = new HTableDescriptor(name);\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+      HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+        Integer.MAX_VALUE, HConstants.FOREVER, false));\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE, \n-      HConstants.FOREVER, false));\n+        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+        Integer.MAX_VALUE,  HConstants.FOREVER, false));\n     return htd;\n   }\n   \n@@ -279,9 +278,7 @@ protected static long addContent(final Incommon updater, final String column,\n     EXIT: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n       for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n         for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          String s = Bytes.toString(bytes) + PUNCTUATION;\n-          byte [] t = Bytes.toBytes(s);\n+          byte [] t = new byte [] {(byte)c, (byte)d, (byte)e};\n           if (endKey != null && endKey.length > 0\n               && Bytes.compareTo(endKey, t) <= 0) {\n             break EXIT;",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "sha": "a58142e14db4c41ed546dff86d3956e60ab0b1fb",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 4,
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -30,16 +30,21 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.filter.PageRowFilter;\n+import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n+import org.apache.hadoop.hbase.io.RowResult;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.FileInputFormat;\n@@ -52,8 +57,6 @@\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.mapred.TextInputFormat;\n import org.apache.hadoop.mapred.TextOutputFormat;\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n \n \n /**\n@@ -88,6 +91,7 @@\n   }\n   \n   private static final String RANDOM_READ = \"randomRead\";\n+  private static final String RANDOM_SEEK_SCAN = \"randomSeekScan\";\n   private static final String RANDOM_READ_MEM = \"randomReadMem\";\n   private static final String RANDOM_WRITE = \"randomWrite\";\n   private static final String SEQUENTIAL_READ = \"sequentialRead\";\n@@ -96,6 +100,7 @@\n   \n   private static final List<String> COMMANDS =\n     Arrays.asList(new String [] {RANDOM_READ,\n+      RANDOM_SEEK_SCAN,\n       RANDOM_READ_MEM,\n       RANDOM_WRITE,\n       SEQUENTIAL_READ,\n@@ -406,7 +411,36 @@ long test() throws IOException {\n      */\n     abstract String getTestName();\n   }\n-  \n+\n+  class RandomSeekScanTest extends Test {\n+    RandomSeekScanTest(final HBaseConfiguration conf, final int startRow,\n+        final int perClientRunRows, final int totalRows, final Status status) {\n+      super(conf, startRow, perClientRunRows, totalRows, status);\n+    }\n+    \n+    void testRow(@SuppressWarnings(\"unused\") final int i) throws IOException {\n+      Scanner s = this.table.getScanner(new byte [][] {COLUMN_NAME},\n+        getRandomRow(this.rand, this.totalRows),\n+        new WhileMatchRowFilter(new PageRowFilter(120)));\n+      int count = 0;\n+      for (RowResult rr = null; (rr = s.next()) != null;) {\n+        // LOG.info(\"\" + count++ + \" \" + rr.toString());\n+      }\n+      s.close();\n+    }\n+ \n+    @Override\n+    protected int getReportingPeriod() {\n+      // \n+      return this.perClientRunRows / 100;\n+    }\n+\n+    @Override\n+    String getTestName() {\n+      return \"randomSeekScanTest\";\n+    }\n+  }\n+\n   class RandomReadTest extends Test {\n     RandomReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n@@ -581,6 +615,10 @@ long runOneClient(final String cmd, final int startRow,\n       Test t = new SequentialWriteTest(this.conf, startRow, perClientRunRows,\n         totalRows, status);\n       totalElapsedTime = t.test();\n+    } else if (cmd.equals(RANDOM_SEEK_SCAN)) {\n+      Test t = new RandomSeekScanTest(this.conf, startRow, perClientRunRows,\n+          totalRows, status);\n+        totalElapsedTime = t.test();\n     } else {\n       new IllegalArgumentException(\"Invalid command value: \" + cmd);\n     }\n@@ -671,6 +709,7 @@ private void printUsage(final String message) {\n     System.err.println(\" randomRead      Run random read test\");\n     System.err.println(\" randomReadMem   Run random read test where table \" +\n       \"is in memory\");\n+    System.err.println(\" randomSeekScan  Run random seek and scan 100 test\");\n     System.err.println(\" randomWrite     Run random write test\");\n     System.err.println(\" sequentialRead  Run sequential read test\");\n     System.err.println(\" sequentialWrite Run sequential write test\");",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "91001e6b64037d1e19a84ae6f4b819c663248f37",
                "status": "modified"
            },
            {
                "additions": 182,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "changes": 225,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHStoreKey.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 43,
                "filename": "src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "patch": "@@ -19,10 +19,11 @@\n \n import java.io.IOException;\n import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.TreeSet;\n \n import junit.framework.TestCase;\n \n-import org.apache.hadoop.hbase.HStoreKey.StoreKeyByteComparator;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Writables;\n \n@@ -38,6 +39,184 @@ protected void tearDown() throws Exception {\n     super.tearDown();\n   }\n \n+  public void testMoreComparisons() throws Exception {\n+    // Root compares\n+    HStoreKey a = new HStoreKey(\".META.,,99999999999999\");\n+    HStoreKey b = new HStoreKey(\".META.,,1\");\n+    HStoreKey.StoreKeyComparator c = new HStoreKey.RootStoreKeyComparator();\n+    assertTrue(c.compare(b.getBytes(), a.getBytes()) < 0);\n+    HStoreKey aa = new HStoreKey(\".META.,,1\");\n+    HStoreKey bb = new HStoreKey(\".META.,,1\", \"info:regioninfo\", 1235943454602L);\n+    assertTrue(c.compare(aa.getBytes(), bb.getBytes()) < 0);\n+    \n+    // Meta compares\n+    HStoreKey aaa = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236020145502\");\n+    HStoreKey bbb = new HStoreKey(\"TestScanMultipleVersions,,99999999999999\");\n+    c = new HStoreKey.MetaStoreKeyComparator();\n+    assertTrue(c.compare(bbb.getBytes(), aaa.getBytes()) < 0);\n+    \n+    HStoreKey aaaa = new HStoreKey(\"TestScanMultipleVersions,,1236023996656\",\n+      \"info:regioninfo\", 1236024396271L);\n+    assertTrue(c.compare(aaaa.getBytes(), bbb.getBytes()) < 0);\n+    \n+    HStoreKey x = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236034574162\",\n+      \"\", 9223372036854775807L);\n+    HStoreKey y = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236034574162\",\n+      \"info:regioninfo\", 1236034574912L);\n+    assertTrue(c.compare(x.getBytes(), y.getBytes()) < 0);\n+    \n+    comparisons(new HStoreKey.HStoreKeyRootComparator());\n+    comparisons(new HStoreKey.HStoreKeyMetaComparator());\n+    comparisons(new HStoreKey.HStoreKeyComparator());\n+    metacomparisons(new HStoreKey.HStoreKeyRootComparator());\n+    metacomparisons(new HStoreKey.HStoreKeyMetaComparator());\n+  }\n+\n+  /**\n+   * Tests cases where rows keys have characters below the ','.\n+   * See HBASE-832\n+   * @throws IOException \n+   */\n+  public void testHStoreKeyBorderCases() throws IOException {\n+    HStoreKey rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n+      \"\", Long.MAX_VALUE);\n+    byte [] rowABytes = Writables.getBytes(rowA);\n+    HStoreKey rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n+      \"\", Long.MAX_VALUE);\n+    byte [] rowBBytes = Writables.getBytes(rowB);\n+    // This is a plain compare on the row. It gives wrong answer for meta table\n+    // row entry.\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    HStoreKey.MetaStoreKeyComparator c =\n+      new HStoreKey.MetaStoreKeyComparator();\n+    assertTrue(c.compare(rowABytes, rowBBytes) < 0);\n+\n+    rowA = new HStoreKey(\"testtable,,1234\", \"\", Long.MAX_VALUE);\n+    rowB = new HStoreKey(\"testtable,$www.hbase.org/,99999\", \"\", Long.MAX_VALUE);\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    assertTrue(c.compare( Writables.getBytes(rowA),  Writables.getBytes(rowB)) < 0);\n+\n+    rowA = new HStoreKey(\".META.,testtable,www.hbase.org/,1234,4321\", \"\",\n+      Long.MAX_VALUE);\n+    rowB = new HStoreKey(\".META.,testtable,www.hbase.org/%20,99999,99999\", \"\",\n+      Long.MAX_VALUE);\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    HStoreKey.RootStoreKeyComparator rootComparator =\n+      new HStoreKey.RootStoreKeyComparator();\n+    assertTrue(rootComparator.compare( Writables.getBytes(rowA),\n+      Writables.getBytes(rowB)) < 0);\n+  }\n+\n+  private void metacomparisons(final HStoreKey.HStoreKeyComparator c) {\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,1\"),\n+      new HStoreKey(\".META.,a,,0,1\")) == 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,1\"),\n+      new HStoreKey(\".META.,a,,0,2\")) < 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,2\"),\n+      new HStoreKey(\".META.,a,,0,1\")) > 0);\n+  }\n+\n+  private void comparisons(final HStoreKey.HStoreKeyComparator c) {\n+    assertTrue(c.compare(new HStoreKey(\".META.,,1\"),\n+      new HStoreKey(\".META.,,1\")) == 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,,1\"),\n+      new HStoreKey(\".META.,,2\")) < 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,,2\"),\n+      new HStoreKey(\".META.,,1\")) > 0);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public void testBinaryKeys() throws Exception {\n+\tSet<HStoreKey> set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyComparator());\n+\tHStoreKey [] keys = {new HStoreKey(\"aaaaa,\\u0000\\u0000,2\", getName(), 2),\n+\t  new HStoreKey(\"aaaaa,\\u0001,3\", getName(), 3),\n+\t  new HStoreKey(\"aaaaa,,1\", getName(), 1),\n+\t  new HStoreKey(\"aaaaa,\\u1000,5\", getName(), 5),\n+\t  new HStoreKey(\"aaaaa,a,4\", getName(), 4),\n+    new HStoreKey(\"a,a,0\", getName(), 0),\n+\t};\n+\t// Add to set with bad comparator\n+\tfor (int i = 0; i < keys.length; i++) {\n+\t  set.add(keys[i]);\n+\t}\n+\t// This will output the keys incorrectly.\n+\tboolean assertion = false;\n+\tint count = 0;\n+\ttry {\n+      for (HStoreKey k: set) {\n+        assertTrue(count++ == k.getTimestamp());\n+      }\n+\t} catch (junit.framework.AssertionFailedError e) {\n+\t  // Expected\n+\t  assertion = true;\n+\t}\n+\tassertTrue(assertion);\n+\t// Make set with good comparator\n+\tset = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyMetaComparator());\n+\tfor (int i = 0; i < keys.length; i++) {\n+      set.add(keys[i]);\n+    }\n+    count = 0;\n+    for (HStoreKey k: set) {\n+      assertTrue(count++ == k.getTimestamp());\n+    }\n+    // Make up -ROOT- table keys.\n+    HStoreKey [] rootKeys = {\n+        new HStoreKey(\".META.,aaaaa,\\u0000\\u0000,0,2\", getName(), 2),\n+        new HStoreKey(\".META.,aaaaa,\\u0001,0,3\", getName(), 3),\n+        new HStoreKey(\".META.,aaaaa,,0,1\", getName(), 1),\n+        new HStoreKey(\".META.,aaaaa,\\u1000,0,5\", getName(), 5),\n+        new HStoreKey(\".META.,aaaaa,a,0,4\", getName(), 4),\n+        new HStoreKey(\".META.,,0\", getName(), 0),\n+      };\n+    // This will output the keys incorrectly.\n+    set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyMetaComparator());\n+    // Add to set with bad comparator\n+    for (int i = 0; i < keys.length; i++) {\n+      set.add(rootKeys[i]);\n+    }\n+    assertion = false;\n+    count = 0;\n+    try {\n+      for (HStoreKey k: set) {\n+        assertTrue(count++ == k.getTimestamp());\n+      }\n+    } catch (junit.framework.AssertionFailedError e) {\n+      // Expected\n+      assertion = true;\n+    }\n+    // Now with right comparator\n+    set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyRootComparator());\n+    // Add to set with bad comparator\n+    for (int i = 0; i < keys.length; i++) {\n+      set.add(rootKeys[i]);\n+    }\n+    count = 0;\n+    for (HStoreKey k: set) {\n+      assertTrue(count++ == k.getTimestamp());\n+    }\n+  }\n+\n+  public void testSerialization() throws IOException {\n+    HStoreKey hsk = new HStoreKey(getName(), getName(), 123);\n+    byte [] b = hsk.getBytes();\n+    HStoreKey hsk2 = HStoreKey.create(b);\n+    assertTrue(hsk.equals(hsk2));\n+    // Test getBytes with empty column\n+    hsk = new HStoreKey(getName());\n+    assertTrue(Bytes.equals(hsk.getBytes(),\n+      HStoreKey.getBytes(Bytes.toBytes(getName()), null,\n+      HConstants.LATEST_TIMESTAMP)));\n+  }\n+\n+  public void testGetBytes() throws IOException {\n+    long now = System.currentTimeMillis();\n+    HStoreKey hsk = new HStoreKey(\"one\", \"two\", now);\n+    byte [] writablesBytes = Writables.getBytes(hsk);\n+    byte [] selfSerializationBytes = hsk.getBytes();\n+    Bytes.equals(writablesBytes, selfSerializationBytes);\n+  }\n+\n   public void testByteBuffer() throws Exception {\n     final long ts = 123;\n     final byte [] row = Bytes.toBytes(\"row\");\n@@ -61,7 +240,8 @@ public void testRawComparator() throws IOException {\n     byte [] nowBytes = Writables.getBytes(now);\n     HStoreKey future = new HStoreKey(a, a, timestamp + 10);\n     byte [] futureBytes = Writables.getBytes(future);\n-    StoreKeyByteComparator comparator = new HStoreKey.StoreKeyByteComparator();\n+    HStoreKey.StoreKeyComparator comparator =\n+      new HStoreKey.StoreKeyComparator();\n     assertTrue(past.compareTo(now) > 0);\n     assertTrue(comparator.compare(pastBytes, nowBytes) > 0);\n     assertTrue(now.compareTo(now) == 0);\n@@ -84,45 +264,4 @@ public void testRawComparator() throws IOException {\n     assertTrue(nocolumn.compareTo(withcolumn) < 0);\n     assertTrue(comparator.compare(nocolumnBytes, withcolumnBytes) < 0);\n   }\n-  \n-//  /**\n-//   * Tests cases where rows keys have characters below the ','.\n-//   * See HBASE-832\n-//   * @throws IOException \n-//   */\n-//  public void testHStoreKeyBorderCases() throws IOException {\n-//    HRegionInfo info = new HRegionInfo(new HTableDescriptor(\"testtable\"),\n-//        HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY);\n-//\n-//    HStoreKey rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n-//      \"\", Long.MAX_VALUE, info);\n-//    byte [] rowABytes = Writables.getBytes(rowA);\n-//    HStoreKey rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n-//      \"\", Long.MAX_VALUE, info);\n-//    byte [] rowBBytes = Writables.getBytes(rowB);\n-//    assertTrue(rowA.compareTo(rowB) > 0);\n-//    HStoreKey.Comparator comparator = new HStoreKey.PlainStoreKeyComparator();\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) > 0);\n-//\n-//    rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) < 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) < 0);\n-//\n-//    rowA = new HStoreKey(\"testtable,,1234\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    rowB = new HStoreKey(\"testtable,$www.hbase.org/,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) < 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) < 0);\n-//\n-//    rowA = new HStoreKey(\".META.,testtable,www.hbase.org/,1234,4321\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.ROOT_REGIONINFO);\n-//    rowB = new HStoreKey(\".META.,testtable,www.hbase.org/%20,99999,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.ROOT_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) > 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) > 0);\n-//  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "sha": "1f1278283b207965cc83e733ecd752d98c2914b2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "patch": "@@ -66,7 +66,7 @@ public void setUp() throws Exception {\n     desc.addFamily(new HColumnDescriptor(CONTENTS_STR));\n     desc.addFamily(new HColumnDescriptor(SMALLFAM, \n         HColumnDescriptor.DEFAULT_VERSIONS, \n-        HColumnDescriptor.DEFAULT_COMPRESSION,  \n+        HColumnDescriptor.DEFAULT_COMPRESSION,\n         HColumnDescriptor.DEFAULT_IN_MEMORY, \n         HColumnDescriptor.DEFAULT_BLOCKCACHE, SMALL_LENGTH, \n         HColumnDescriptor.DEFAULT_TTL, HColumnDescriptor.DEFAULT_BLOOMFILTER));",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "sha": "f3559e732c891ad742c15bacb708d0e2d80ec581",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "patch": "@@ -89,4 +89,4 @@ private void stopRowTests(RowFilterInterface filter) throws Exception {\n     \n     assertFalse(\"Filter a null\", filter.filterRowKey(null));\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "sha": "a28e374b5445c1e9d73d09ca21f0d5cf4afa084b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 6,
                "filename": "src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "patch": "@@ -35,10 +35,10 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.hfile.HFile.Reader;\n import org.apache.hadoop.hbase.io.hfile.HFile.Writer;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.apache.hadoop.io.RawComparator;\n \n /**\n  * test hfile features.\n@@ -129,7 +129,8 @@ private FSDataOutputStream createFSOutput(Path name) throws IOException {\n   void basicWithSomeCodec(String codec) throws IOException {\n     Path ncTFile = new Path(ROOT_DIR, \"basic.hfile\");\n     FSDataOutputStream fout = createFSOutput(ncTFile);\n-    Writer writer = new Writer(fout, minBlockSize, codec, null);\n+    Writer writer = new Writer(fout, minBlockSize,\n+      Compression.getCompressionAlgorithmByName(codec), null, false);\n     LOG.info(writer);\n     writeRecords(writer);\n     fout.close();\n@@ -192,7 +193,8 @@ private void someReadingWithMetaBlock(Reader reader) throws IOException {\n   private void metablocks(final String compress) throws Exception {\n     Path mFile = new Path(ROOT_DIR, \"meta.tfile\");\n     FSDataOutputStream fout = createFSOutput(mFile);\n-    Writer writer = new Writer(fout, minBlockSize, compress, null);\n+    Writer writer = new Writer(fout, minBlockSize,\n+      Compression.getCompressionAlgorithmByName(compress), null, false);\n     someTestingWithMetaBlock(writer);\n     writer.close();\n     fout.close();\n@@ -227,8 +229,8 @@ public void testCompressionOrdinance() {\n   public void testComparator() throws IOException {\n     Path mFile = new Path(ROOT_DIR, \"meta.tfile\");\n     FSDataOutputStream fout = createFSOutput(mFile);\n-    Writer writer = new Writer(fout, minBlockSize, \"none\",\n-      new RawComparator<byte []>() {\n+    Writer writer = new Writer(fout, minBlockSize, null,\n+      new HStoreKey.StoreKeyComparator() {\n         @Override\n         public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,\n             int l2) {\n@@ -239,7 +241,7 @@ public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,\n         public int compare(byte[] o1, byte[] o2) {\n           return compare(o1, 0, o1.length, o2, 0, o2.length);\n         }\n-      });\n+      }, false);\n     writer.append(\"3\".getBytes(), \"0\".getBytes());\n     writer.append(\"2\".getBytes(), \"0\".getBytes());\n     writer.append(\"1\".getBytes(), \"0\".getBytes());",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "sha": "181511534736843062054e35b5f5c8d35a441a6f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "patch": "@@ -168,7 +168,7 @@ public void testComputedParameters() throws IOException {\n     desc.addFamily(\n         new HColumnDescriptor(CONTENTS,               // Column name\n             1,                                        // Max versions\n-            HColumnDescriptor.CompressionType.NONE,   // no compression\n+            HColumnDescriptor.DEFAULT_COMPRESSION,   // no compression\n             HColumnDescriptor.DEFAULT_IN_MEMORY,      // not in memory\n             HColumnDescriptor.DEFAULT_BLOCKCACHE,\n             HColumnDescriptor.DEFAULT_LENGTH,",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "sha": "359e3973399d232af5f60c3c4e0b78815b36f331",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "patch": "@@ -186,7 +186,7 @@ private void createStoreFile(final HRegion region) throws IOException {\n   private void createSmallerStoreFile(final HRegion region) throws IOException {\n     HRegionIncommon loader = new HRegionIncommon(region); \n     addContent(loader, Bytes.toString(COLUMN_FAMILY),\n-        (\"bbb\" + PUNCTUATION).getBytes(), null);\n+        (\"bbb\").getBytes(), null);\n     loader.flushcache();\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "sha": "5806d03d70ee7f18dcb96e04ac898a9ce228de7d",
                "status": "modified"
            },
            {
                "additions": 175,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "changes": 300,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 125,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "patch": "@@ -28,12 +28,17 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HBaseTestCase;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.filter.StopRowFilter;\n+import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n@@ -55,10 +60,20 @@\n     HConstants.COL_STARTCODE\n   };\n   \n-  private static final byte [] ROW_KEY =\n-    HRegionInfo.ROOT_REGIONINFO.getRegionName();\n-  private static final HRegionInfo REGION_INFO =\n-    HRegionInfo.ROOT_REGIONINFO;\n+  static final HTableDescriptor TESTTABLEDESC =\n+    new HTableDescriptor(\"testscanner\");\n+  static {\n+    TESTTABLEDESC.addFamily(new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n+      10,  // Ten is arbitrary number.  Keep versions to help debuggging.\n+      Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n+  }\n+  /** HRegionInfo for root region */\n+  public static final HRegionInfo REGION_INFO =\n+    new HRegionInfo(TESTTABLEDESC, HConstants.EMPTY_BYTE_ARRAY,\n+    HConstants.EMPTY_BYTE_ARRAY);\n+  \n+  private static final byte [] ROW_KEY = REGION_INFO.getRegionName();\n   \n   private static final long START_CODE = Long.MAX_VALUE;\n \n@@ -75,7 +90,127 @@ public void setUp() throws Exception {\n     super.setUp();\n     \n   }\n-  \n+\n+  /** The test!\n+   * @throws IOException\n+   */\n+  public void testScanner() throws IOException {\n+    try {\n+      r = createNewHRegion(TESTTABLEDESC, null, null);\n+      region = new HRegionIncommon(r);\n+      \n+      // Write information to the meta table\n+\n+      BatchUpdate batchUpdate =\n+        new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      ByteArrayOutputStream byteStream = new ByteArrayOutputStream();\n+      DataOutputStream s = new DataOutputStream(byteStream);\n+      REGION_INFO.write(s);\n+      batchUpdate.put(HConstants.COL_REGIONINFO, byteStream.toByteArray());\n+      region.commit(batchUpdate);\n+\n+      // What we just committed is in the memcache. Verify that we can get\n+      // it back both with scanning and get\n+      \n+      scan(false, null);\n+      getRegionInfo();\n+      \n+      // Close and re-open\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Verify we can get the data back now that it is on disk.\n+      \n+      scan(false, null);\n+      getRegionInfo();\n+      \n+      // Store some new information\n+ \n+      HServerAddress address = new HServerAddress(\"foo.bar.com:1234\");\n+\n+      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      batchUpdate.put(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));\n+\n+      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));\n+\n+      region.commit(batchUpdate);\n+      \n+      // Validate that we can still get the HRegionInfo, even though it is in\n+      // an older row on disk and there is a newer row in the memcache\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+      \n+      // flush cache\n+\n+      region.flushcache();\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Close and reopen\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Now update the information again\n+\n+      address = new HServerAddress(\"bar.foo.com:4321\");\n+      \n+      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      batchUpdate.put(HConstants.COL_SERVER, \n+        Bytes.toBytes(address.toString()));\n+\n+      region.commit(batchUpdate);\n+      \n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // flush cache\n+\n+      region.flushcache();\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Close and reopen\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+      \n+      // clean up\n+      \n+      r.close();\n+      r.getLog().closeAndDelete();\n+      \n+    } finally {\n+      shutdownDfs(cluster);\n+    }\n+  }\n+\n   /** Compare the HRegionInfo we read from HBase to what we stored */\n   private void validateRegionInfo(byte [] regionBytes) throws IOException {\n     HRegionInfo info =\n@@ -145,7 +280,41 @@ private void getRegionInfo() throws IOException {\n     byte [] bytes = region.get(ROW_KEY, HConstants.COL_REGIONINFO).getValue();\n     validateRegionInfo(bytes);  \n   }\n-  \n+\n+  /**\n+   * Test basic stop row filter works.\n+   */\n+  public void testStopRow() throws Exception {\n+    byte [] startrow = Bytes.toBytes(\"bbb\");\n+    byte [] stoprow = Bytes.toBytes(\"ccc\");\n+    try {\n+      this.r = createNewHRegion(REGION_INFO.getTableDesc(), null, null);\n+      addContent(this.r, HConstants.COLUMN_FAMILY);\n+      InternalScanner s = r.getScanner(HConstants.COLUMN_FAMILY_ARRAY,\n+        startrow, HConstants.LATEST_TIMESTAMP,\n+        new WhileMatchRowFilter(new StopRowFilter(stoprow)));\n+      HStoreKey key = new HStoreKey();\n+      SortedMap<byte [], Cell> results =\n+        new TreeMap<byte [], Cell>(Bytes.BYTES_COMPARATOR);\n+      int count = 0;\n+      for (boolean first = true; s.next(key, results);) {\n+        if (first) {\n+          assertTrue(Bytes.BYTES_COMPARATOR.compare(startrow, key.getRow()) == 0);\n+          first = false;\n+        }\n+        count++;\n+      }\n+      assertTrue(Bytes.BYTES_COMPARATOR.compare(stoprow, key.getRow()) > 0);\n+      // We got something back.\n+      assertTrue(count > 10);\n+      s.close();\n+    } finally {\n+      this.r.close();\n+      this.r.getLog().closeAndDelete();\n+      shutdownDfs(this.cluster);\n+    }\n+  }\n+\n   /**\n    * HBase-910.\n    * @throws Exception\n@@ -194,123 +363,4 @@ private int count(final HRegionIncommon hri, final int flushIndex)\n     LOG.info(\"Found \" + count + \" items\");\n     return count;\n   }\n-\n-  /** The test!\n-   * @throws IOException\n-   */\n-  public void testScanner() throws IOException {\n-    try {\n-      r = createNewHRegion(REGION_INFO.getTableDesc(), null, null);\n-      region = new HRegionIncommon(r);\n-      \n-      // Write information to the meta table\n-\n-      BatchUpdate batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      ByteArrayOutputStream byteStream = new ByteArrayOutputStream();\n-      DataOutputStream s = new DataOutputStream(byteStream);\n-      HRegionInfo.ROOT_REGIONINFO.write(s);\n-      batchUpdate.put(HConstants.COL_REGIONINFO, byteStream.toByteArray());\n-      region.commit(batchUpdate);\n-\n-      // What we just committed is in the memcache. Verify that we can get\n-      // it back both with scanning and get\n-      \n-      scan(false, null);\n-      getRegionInfo();\n-      \n-      // Close and re-open\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Verify we can get the data back now that it is on disk.\n-      \n-      scan(false, null);\n-      getRegionInfo();\n-      \n-      // Store some new information\n- \n-      HServerAddress address = new HServerAddress(\"foo.bar.com:1234\");\n-\n-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      batchUpdate.put(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));\n-\n-      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));\n-\n-      region.commit(batchUpdate);\n-      \n-      // Validate that we can still get the HRegionInfo, even though it is in\n-      // an older row on disk and there is a newer row in the memcache\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-      \n-      // flush cache\n-\n-      region.flushcache();\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Close and reopen\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Now update the information again\n-\n-      address = new HServerAddress(\"bar.foo.com:4321\");\n-      \n-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      batchUpdate.put(HConstants.COL_SERVER, \n-        Bytes.toBytes(address.toString()));\n-\n-      region.commit(batchUpdate);\n-      \n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // flush cache\n-\n-      region.flushcache();\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Close and reopen\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-      \n-      // clean up\n-      \n-      r.close();\n-      r.getLog().closeAndDelete();\n-      \n-    } finally {\n-      shutdownDfs(cluster);\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "sha": "71ad82b9153be985c59f604048dd9b7d0beef4ed",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 2,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "patch": "@@ -70,7 +70,7 @@ public void testBasicHalfMapFile() throws Exception {\n     // Make up a directory hierarchy that has a regiondir and familyname.\n     HFile.Writer writer = StoreFile.getWriter(this.fs,\n       new Path(new Path(this.testDir, \"regionname\"), \"familyname\"),\n-      2 * 1024, null, null);\n+      2 * 1024, null, null, false);\n     writeStoreFile(writer);\n     checkHalfHFile(new StoreFile(this.fs, writer.getPath()));\n   }\n@@ -107,7 +107,8 @@ public void testReference()\n     Path storedir = new Path(new Path(this.testDir, \"regionname\"), \"familyname\");\n     Path dir = new Path(storedir, \"1234567890\");\n     // Make a store file and write data to it.\n-    HFile.Writer writer = StoreFile.getWriter(this.fs, dir, 8 * 1024, null, null);\n+    HFile.Writer writer = StoreFile.getWriter(this.fs, dir, 8 * 1024, null,\n+      null, false);\n     writeStoreFile(writer);\n     StoreFile hsf = new StoreFile(this.fs, writer.getPath());\n     HFile.Reader reader = hsf.getReader();",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "sha": "4b0d3c7d9335b6cb01678de66fdfc958fd4fd342",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.TimestampTestBase;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.util.Bytes;\n \n /**\n@@ -79,8 +78,8 @@ public void testTimestampScanning() throws IOException {\n   private HRegion createRegion() throws IOException {\n     HTableDescriptor htd = createTableDescriptor(getName());\n     htd.addFamily(new HColumnDescriptor(COLUMN, VERSIONS,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+      HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n     return createNewHRegion(htd, null, null);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "sha": "04da57c1adad37709e05eb959708328494c0bc8d",
                "status": "modified"
            }
        ],
        "message": "HBASE-1217 Changed HColumnDescriptor so it now has blocksize, and compression\nis not lzo, gz or none (rather than block, key or none).  Made changes to\nshell so you can do this configuration therein.  Bloomfilter still not\nhooked up.\n\nHBASE-859 HStoreKey needs a reworking... binary keys work again.\n\nHBASE-1211 NPE in retries exhausted exception\n\n\nM src/test/org/apache/hadoop/hbase/TestHStoreKey.java\n    Added tests comparing binary keys, plain, meta and root keys.\n    Enabled the commented-out testHStoreKeyBorderCases.\nM  src/test/org/apache/hadoop/hbase/HBaseTestCase.java\n    Removed PUNCTUATION pollution of keys.\nM src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java\n    Use different compression type.\nM src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java\n    Use new HCD constructor.\nM src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java\n    Was using ROOT table but a plain hstorekey comparator; that won't\n    work (have to be careful about which comparator is used on which\n    table from here on out).\nM src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java\n    getWriter now takes whether or not bloom filter, etc.\nM  src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java\n    Removed PUNCTUATION pollution.\nM src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java\nM src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java\n    Whitespace.\nM src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java\n    Pass compressor and whether bloomfilter.\nM src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java\n    Added new seek+scan test.\nM src/java/org/apache/hadoop/hbase/HColumnDescriptor.java\n    Added in BLOCKSIZE, changed COMPRESSION arguments.\n    Removed DEFAULT_MAPFILE_INDEX_INTERVAL.\n    Upped the version from 6 to 7.\nM src/java/org/apache/hadoop/hbase/HStoreKey.java\n    New object and raw byte comparators, one for each context of\n    plain, meta, and root.  Use same code.  Other utility such\n    as static creates that take a ByteBuffer or a byte array of\n    a serialized HStoreKey.  Old compareTo methods are deprecated\n    since they can return wrong answer if binary keys.\n    New getBytes without creating Streams.\n    Removed the old BeforeThisStoreKey trick.\n    Custom vint math methods that work with ByteBuffers and byte []\n    instead of streams.\nM src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java\n    Fixup for new compression.\nM src/java/org/apache/hadoop/hbase/regionserver/Memcache.java\n    Pass in comparator to use.\nM src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java\nM src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java\n    Use right comparator comparing rows\nM src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\nM src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n    Formatting.\n    Might be issues here with key compares but no store in the\n    context... leave it for now.\nM src/java/org/apache/hadoop/hbase/regionserver/Store.java\n    Use new comparators and hcd changes.\nM src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java\n    Added override of HFile.Reader so could format the toString\n    (so HFile doesn't have to know about HStoreKeys though thats\n    what we put in there always -- let it just be about\n    byte []).\nM src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java\nM src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java\nM src/java/org/apache/hadoop/hbase/HTableDescriptor.java\n    Fix up for new HCD.\nM src/java/org/apache/hadoop/hbase/HRegionInfo.java\nM src/java/org/apache/hadoop/hbase/master/MetaRegion.java\n    Use Bytes.compareTo.\nM src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java\n    (isTop): Added.\nM src/java/org/apache/hadoop/hbase/io/RowResult.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/io/hfile/Compression.java\n    Make getCompressionAlgorithmByName public.\nM src/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n    Add in writer constructor that takes compression and bloom filter.\n    Fix some of the comparator use.\nM src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java\n    Remove commented out code\nM src/java/org/apache/hadoop/hbase/io/Cell.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java\n    Use new comparator names.\nA src/java/org/apache/hadoop/hbase/util/TestBytes.java\n    Small bytes test.\nM src/java/org/apache/hadoop/hbase/util/Bytes.java\n    Some overrides to help when source is ByteBuffer.\n    Added in some vint math utility.\nM src/java/org/apache/hadoop/hbase/client/HTable.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/client/MetaScanner.java\n    Use Bytes.compareTO.\nM src/java/org/apache/hadoop/hbase/client/HConnectionManager.java\n    Use right comparator.\nM src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java\n    Make it match HCD.\nM src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java\n    Fix NPE\nM src/java/org/apache/hadoop/hbase/client/ScannerCallable.java\n    Formatting.\nM bin/HBase.rb\n    Support for new HCD.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@749546 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/8206da62c0b4f0f87c97ec21572e8253664b90d0",
        "patched_files": [
            "HbaseMapWritable.java",
            "Cell.java",
            "MetaRegion.java",
            "HBaseMapFile.java",
            "ThriftUtilities.java",
            "HRegion.java",
            "HTableDescriptor.java",
            "HRegionInfo.java",
            "Store.java",
            "XMLRestParser.java",
            "HConnectionManager.java",
            "ScannerCallable.java",
            "HRegionServer.java",
            "HFile.java",
            "UnmodifyableHColumnDescriptor.java",
            "RetriesExhaustedException.java",
            "RowResult.java",
            "Compression.java",
            "Bytes.java",
            "HBaseTestCase.java",
            "HColumnDescriptor.java",
            "CHANGES.java",
            "HAbstractScanner.java",
            "Memcache.java",
            "HStoreKey.java",
            "StoreFileScanner.java",
            "PerformanceEvaluation.java",
            "HTable.java",
            "JsonRestParser.java",
            "MetaScanner.java",
            "HalfHFileReader.java",
            "StoreFile.java",
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStoreFile.java",
            "TestCompaction.java",
            "TestHColumnDescriptor.java",
            "TestHFile.java",
            "TestBloomFilters.java",
            "TestScanner.java",
            "TestHRegion.java",
            "TestHTableDescriptor.java",
            "TestPerformanceEvaluation.java",
            "TestStoreScanner.java",
            "TestBytes.java",
            "TestStopRowFilter.java",
            "TestTimestamp.java",
            "TestHStoreKey.java",
            "TestBatchUpdate.java",
            "TestHRegionInfo.java",
            "CompressionTest.java"
        ]
    },
    "hbase_087f1df": {
        "bug_id": "hbase_087f1df",
        "commit": "https://github.com/apache/hbase/commit/087f1df0e2a538f7627aaaa94e6e92215545dce4",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java?ref=087f1df0e2a538f7627aaaa94e6e92215545dce4",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "patch": "@@ -30,7 +30,9 @@\n \n \n /**\n- * Data structure to describe the distribution of HDFS blocks amount hosts\n+ * Data structure to describe the distribution of HDFS blocks amount hosts.\n+ *\n+ * Adding erroneous data will be ignored silently.\n  */\n @InterfaceAudience.Private\n public class HDFSBlocksDistribution {\n@@ -122,8 +124,10 @@ public synchronized String toString() {\n    */\n   public void addHostsAndBlockWeight(String[] hosts, long weight) {\n     if (hosts == null || hosts.length == 0) {\n-      throw new NullPointerException(\"empty hosts\");\n+      // erroneous data\n+      return;\n     }\n+\n     addUniqueWeight(weight);\n     for (String hostname : hosts) {\n       addHostAndBlockWeight(hostname, weight);\n@@ -146,7 +150,8 @@ private void addUniqueWeight(long weight) {\n    */\n   private void addHostAndBlockWeight(String host, long weight) {\n     if (host == null) {\n-      throw new NullPointerException(\"Passed hostname is null\");\n+      // erroneous data\n+      return;\n     }\n \n     HostAndWeight hostAndWeight = this.hostAndWeights.get(host);",
                "raw_url": "https://github.com/apache/hbase/raw/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "sha": "11619d3683e3a90af9cf257a2ec9434f1b8634a1",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hbase/blob/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java?ref=087f1df0e2a538f7627aaaa94e6e92215545dce4",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "patch": "@@ -0,0 +1,69 @@\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static junit.framework.Assert.assertEquals;\n+\n+@Category(SmallTests.class)\n+public class TestHDFSBlocksDistribution {\n+  @Test\n+  public void testAddHostsAndBlockWeight() throws Exception {\n+    HDFSBlocksDistribution distribution = new HDFSBlocksDistribution();\n+    distribution.addHostsAndBlockWeight(null, 100);\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[0], 100);\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[] {\"test\"}, 101);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[] {\"test\"}, 202);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    assertEquals(\"test host should have weight 303\", 303,\n+        distribution.getHostAndWeights().get(\"test\").getWeight());\n+    distribution.addHostsAndBlockWeight(new String[] {\"testTwo\"}, 222);\n+    assertEquals(\"Should be two hosts\", 2, distribution.getHostAndWeights().size());\n+    assertEquals(\"Total weight should be 525\", 525, distribution.getUniqueBlocksTotalWeight());\n+  }\n+\n+  public class MockHDFSBlocksDistribution extends HDFSBlocksDistribution {\n+    public Map<String,HostAndWeight> getHostAndWeights() {\n+      HashMap<String, HostAndWeight> map = new HashMap<String, HostAndWeight>();\n+      map.put(\"test\", new HostAndWeight(null, 100));\n+      return map;\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testAdd() throws Exception {\n+    HDFSBlocksDistribution distribution = new HDFSBlocksDistribution();\n+    distribution.add(new MockHDFSBlocksDistribution());\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[]{\"test\"}, 10);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    distribution.add(new MockHDFSBlocksDistribution());\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    assertEquals(\"Total weight should be 10\", 10, distribution.getUniqueBlocksTotalWeight());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "sha": "ea694067c7821d1bec532681e388628d70c8086c",
                "status": "added"
            }
        ],
        "message": "HBASE-7513 HDFSBlocksDistribution shouldn't send NPEs when something goes wrong\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1430560 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/f77e5b5bff0d7718890d2f2b44f65dacf26ac4d1",
        "patched_files": [
            "HDFSBlocksDistribution.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHDFSBlocksDistribution.java"
        ]
    },
    "hbase_0beea37": {
        "bug_id": "hbase_0beea37",
        "commit": "https://github.com/apache/hbase/commit/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -394,6 +394,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2738  TestTimeRangeMapRed updated now that we keep multiple cells with\n                same timestamp in MemStore\n    HBASE-2725  Shutdown hook management is gone in trunk; restore\n+   HBASE-2740  NPE in ReadWriteConsistencyControl\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/CHANGES.txt",
                "sha": "0902c1f2957c99a01cdb7453750c38cce3447f34",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -31,7 +31,7 @@\n import java.util.NavigableSet;\n \n /**\n- * Scanner scans both the memstore and the HStore. Coaleace KeyValue stream\n+ * Scanner scans both the memstore and the HStore. Coalesce KeyValue stream\n  * into List<KeyValue> for a single row.\n  */\n class StoreScanner implements KeyValueScanner, InternalScanner, ChangedReadersObserver {\n@@ -194,6 +194,8 @@ public synchronized void close() {\n       this.store.deleteChangedReaderObserver(this);\n     if (this.heap != null)\n       this.heap.close();\n+    this.heap = null; // CLOSED!\n+    this.lastTop = null; // If both are null, we are closed.\n   }\n \n   public synchronized boolean seek(KeyValue key) throws IOException {\n@@ -298,6 +300,13 @@ public synchronized boolean next(List<KeyValue> outResult) throws IOException {\n   public synchronized void updateReaders() throws IOException {\n     if (this.closing) return;\n \n+    // All public synchronized API calls will call 'checkReseek' which will cause\n+    // the scanner stack to reseek if this.heap==null && this.lastTop != null.\n+    // But if two calls to updateReaders() happen without a 'next' or 'peek' then we\n+    // will end up calling this.peek() which would cause a reseek in the middle of a updateReaders\n+    // which is NOT what we want, not to mention could cause an NPE. So we early out here.\n+    if (this.heap == null) return;\n+\n     // this could be null.\n     this.lastTop = this.peek();\n ",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "7680a222cb5a9164ade205e222582ea2cf92e67b",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "deletions": 6,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "patch": "@@ -109,7 +109,6 @@ public void testScanTimeRange() throws IOException {\n     results = new ArrayList<KeyValue>();\n     assertEquals(true, scan.next(results));\n     assertEquals(3, results.size());\n-\n   }\n \n   public void testScanSameTimestamp() throws IOException {\n@@ -285,6 +284,7 @@ public void testWildCardOneVersionScan() throws IOException {\n     assertEquals(kvs[0], results.get(0));\n     assertEquals(kvs[1], results.get(1));\n   }\n+\n   public void testWildCardScannerUnderDeletes() throws IOException {\n     KeyValue [] kvs = new KeyValue [] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 2, KeyValue.Type.Put, \"dont-care\"), // inc\n@@ -317,6 +317,7 @@ public void testWildCardScannerUnderDeletes() throws IOException {\n     assertEquals(kvs[6], results.get(3));\n     assertEquals(kvs[7], results.get(4));\n   }\n+\n   public void testDeleteFamily() throws IOException {\n     KeyValue [] kvs = new KeyValue[] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 100, KeyValue.Type.DeleteFamily, \"dont-care\"),\n@@ -363,8 +364,7 @@ public void testDeleteColumn() throws IOException {\n     assertEquals(kvs[3], results.get(0));\n   }\n \n-  public void testSkipColumn() throws IOException {\n-    KeyValue [] kvs = new KeyValue[] {\n+  private static final  KeyValue [] kvs = new KeyValue[] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"b\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"c\", 11, KeyValue.Type.Put, \"dont-care\"),\n@@ -376,7 +376,11 @@ public void testSkipColumn() throws IOException {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"i\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R2\", \"cf\", \"a\", 11, KeyValue.Type.Put, \"dont-care\"),\n     };\n-    List<KeyValueScanner> scanners = scanFixture(kvs);\n+\n+  public void testSkipColumn() throws IOException {\n+    KeyValueScanner [] scanners = new KeyValueScanner[] {\n+        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n+    };\n     StoreScanner scan =\n       new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\", \"d\"), scanners);\n@@ -395,9 +399,9 @@ public void testSkipColumn() throws IOException {\n     results.clear();\n     assertEquals(false, scan.next(results));\n   }\n-  \n+\n   /*\n-   * Test expiration of KeyValues in combination with a configured TTL for \n+   * Test expiration of KeyValues in combination with a configured TTL for\n    * a column family (as should be triggered in a major compaction).\n    */\n   public void testWildCardTtlScan() throws IOException {\n@@ -435,6 +439,24 @@ public void testWildCardTtlScan() throws IOException {\n \n     assertEquals(false, scanner.next(results));\n   }\n+\n+  public void testScannerReseekDoesntNPE() throws Exception {\n+    KeyValueScanner [] scanners = new KeyValueScanner[] {\n+        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n+    };\n+    StoreScanner scan =\n+        new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n+            getCols(\"a\", \"d\"), scanners);\n+\n+\n+    // Previously a updateReaders twice in a row would cause an NPE.  In test this would also\n+    // normally cause an NPE because scan.store is null.  So as long as we get through these\n+    // two calls we are good and the bug was quashed.\n+\n+    scan.updateReaders();\n+\n+    scan.updateReaders();\n+  }\n     \n   \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "sha": "af95a5c2b263cf092bcdd3c08d0f41594a833b17",
                "status": "modified"
            }
        ],
        "message": "HBASE-2740  NPE in ReadWriteConsistencyControl\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@955784 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7aedf3a36f8adcd50efb37bf1eb4a67cfdf6f67b",
        "patched_files": [
            "CHANGES.java",
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStoreScanner.java"
        ]
    },
    "hbase_0caad46": {
        "bug_id": "hbase_0caad46",
        "commit": "https://github.com/apache/hbase/commit/0caad4616e2c67cc21e50ec4d8efce89f3469006",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=0caad4616e2c67cc21e50ec4d8efce89f3469006",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -4112,7 +4112,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n   private List<KeyValue> get(Get get, boolean withCoprocessor)\n   throws IOException {\n     long now = EnvironmentEdgeManager.currentTimeMillis();\n-    Scan scan = new Scan(get);\n \n     List<KeyValue> results = new ArrayList<KeyValue>();\n \n@@ -4123,8 +4122,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n-    Scan scan = new Scan(get);\n-\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "raw_url": "https://github.com/apache/hbase/raw/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "54675ffb7e3fae75eeaf0b688accd91f577f79d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0 -- REVERT OVERCOMMIT TO HREGION\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245768 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_0f7285a": {
        "bug_id": "hbase_0f7285a",
        "commit": "https://github.com/apache/hbase/commit/0f7285a81a75b14be937e411f949081d465d1b92",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=0f7285a81a75b14be937e411f949081d465d1b92",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -676,7 +676,17 @@ protected void cleanupCalls(long rpcTimeout) {\n         Call c = itor.next().getValue();\n         long waitTime = System.currentTimeMillis() - c.getStartTime();\n         if (waitTime >= rpcTimeout) {\n-          c.setException(closeException); // local exception\n+          if (this.closeException == null) {\n+            // There may be no exception in the case that there are many calls\n+            // being multiplexed over this connection and these are succeeding\n+            // fine while this Call object is taking a long time to finish\n+            // over on the server; e.g. I just asked the regionserver to bulk\n+            // open 3k regions or its a big fat multiput into a heavily-loaded\n+            // server (Perhaps this only happens at the extremes?)\n+            this.closeException = new CallTimeoutException(\"Call id=\" + c.id +\n+              \", waitTime=\" + waitTime + \", rpcTimetout=\" + rpcTimeout);\n+          }\n+          c.setException(this.closeException);\n           synchronized (c) {\n             c.notifyAll();\n           }\n@@ -705,6 +715,15 @@ protected void cleanupCalls(long rpcTimeout) {\n     }\n   }\n \n+  /**\n+   * Client-side call timeout\n+   */\n+  public static class CallTimeoutException extends IOException {\n+    public CallTimeoutException(final String msg) {\n+      super(msg);\n+    }\n+  }\n+\n   /** Call implementation used for parallel calls. */\n   protected class ParallelCall extends Call {\n     private final ParallelResults results;",
                "raw_url": "https://github.com/apache/hbase/raw/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "2602461c58e6278b6c1b41abf622c685f402def9",
                "status": "modified"
            }
        ],
        "message": "HBASE-4890 fix possible NPE in HConnectionManager\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1298272 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/edc6696bc5a611a20fb1e713eb00b48d2cb7dd87",
        "patched_files": [
            "HBaseClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_10eec81": {
        "bug_id": "hbase_10eec81",
        "commit": "https://github.com/apache/hbase/commit/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "patch": "@@ -1617,4 +1617,17 @@ public int getOperationTimeout() {\n     return operationTimeout;\n   }\n \n-}\n+  /**\n+   * Run basic test.\n+   * @param args Pass table name and row and will get the content.\n+   * @throws IOException\n+   */\n+  public static void main(String[] args) throws IOException {\n+    HTable t = new HTable(HBaseConfiguration.create(), args[0]);\n+    try {\n+      System.out.println(t.get(new Get(Bytes.toBytes(args[1]))));\n+    } finally {\n+      t.close();\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "sha": "da53bec7811d939d2d6d0cfedd2c16ea6a149028",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "deletions": 2,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "patch": "@@ -150,9 +150,9 @@ public static Cell createCell(final byte[] row, final byte[] family, final byte[\n    * @param cellScannerables\n    * @return CellScanner interface over <code>cellIterables</code>\n    */\n-  public static CellScanner createCellScanner(final List<CellScannable> cellScannerables) {\n+  public static CellScanner createCellScanner(final List<? extends CellScannable> cellScannerables) {\n     return new CellScanner() {\n-      private final Iterator<CellScannable> iterator = cellScannerables.iterator();\n+      private final Iterator<? extends CellScannable> iterator = cellScannerables.iterator();\n       private CellScanner cellScanner = null;\n \n       @Override",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "sha": "7728b7d211aed1ae10e70f0b118256c276056544",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "deletions": 21,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -875,7 +875,7 @@ public void run() {\n       mxBean = null;\n     }\n     if (this.thriftServer != null) this.thriftServer.shutdown();\n-    this.leases.closeAfterLeasesExpire();\n+    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n     this.rpcServer.stop();\n     if (this.splitLogWorker != null) {\n       splitLogWorker.stop();\n@@ -2785,7 +2785,8 @@ public GetResponse get(final RpcController controller,\n       if (existence != null) {\n         builder.setExists(existence.booleanValue());\n       } else if (r != null) {\n-        builder.setResult(ProtobufUtil.toResult(r));\n+        ClientProtos.Result pbr = ProtobufUtil.toResult(r);\n+        builder.setResult(pbr);\n       }\n       return builder.build();\n     } catch (IOException ie) {\n@@ -2810,8 +2811,7 @@ public MultiGetResponse multiGet(final RpcController controller, final MultiGetR\n       requestCount.add(request.getGetCount());\n       HRegion region = getRegion(request.getRegion());\n       MultiGetResponse.Builder builder = MultiGetResponse.newBuilder();\n-      for (ClientProtos.Get get: request.getGetList())\n-      {\n+      for (ClientProtos.Get get: request.getGetList()) {\n         Boolean existence = null;\n         Result r = null;\n         if (request.getClosestRowBefore()) {\n@@ -2947,23 +2947,36 @@ public MutateResponse mutate(final RpcController rpcc,\n           throw new DoNotRetryIOException(\n             \"Unsupported mutate type: \" + type.name());\n       }\n-      CellScannable cellsToReturn = null;\n-      if (processed != null) {\n-        builder.setProcessed(processed.booleanValue());\n-      } else if (r != null) {\n-        builder.setResult(ProtobufUtil.toResultNoData(r));\n-        cellsToReturn = r;\n-      }\n-      if (controller != null && cellsToReturn != null) {\n-        controller.setCellScanner(cellsToReturn.cellScanner());\n-      }\n+      if (processed != null) builder.setProcessed(processed.booleanValue());\n+      addResult(builder, r, controller);\n       return builder.build();\n     } catch (IOException ie) {\n       checkFileSystem();\n       throw new ServiceException(ie);\n     }\n   }\n \n+\n+  /**\n+   * @return True if current call supports cellblocks\n+   */\n+  private boolean isClientCellBlockSupport() {\n+    RpcCallContext context = RpcServer.getCurrentCall();\n+    return context != null && context.isClientCellBlockSupport();\n+  }\n+\n+  private void addResult(final MutateResponse.Builder builder,\n+      final Result result, final PayloadCarryingRpcController rpcc) {\n+    if (result == null) return;\n+    if (isClientCellBlockSupport()) {\n+      builder.setResult(ProtobufUtil.toResultNoData(result));\n+      rpcc.setCellScanner(result.cellScanner());\n+    } else {\n+      ClientProtos.Result pbr = ProtobufUtil.toResult(result);\n+      builder.setResult(pbr);\n+    }\n+  }\n+\n   //\n   // remote scanner interface\n   //\n@@ -3148,7 +3161,7 @@ public ScanResponse scan(final RpcController controller, final ScanRequest reque\n               moreResults = false;\n               results = null;\n             } else {\n-              formatResults(builder, results, controller);\n+              addResults(builder, results, controller);\n             }\n           } finally {\n             // We're done. On way out re-add the above removed lease.\n@@ -3196,18 +3209,15 @@ public ScanResponse scan(final RpcController controller, final ScanRequest reque\n     }\n   }\n \n-  private void formatResults(final ScanResponse.Builder builder, final List<Result> results,\n+  private void addResults(final ScanResponse.Builder builder, final List<Result> results,\n       final RpcController controller) {\n     if (results == null || results.isEmpty()) return;\n-    RpcCallContext context = RpcServer.getCurrentCall();\n-    if (context != null && context.isClientCellBlockSupport()) {\n-      List<CellScannable> cellScannables = new ArrayList<CellScannable>(results.size());\n+    if (isClientCellBlockSupport()) {\n       for (Result res : results) {\n-        cellScannables.add(res);\n         builder.addCellsPerResult(res.size());\n       }\n       ((PayloadCarryingRpcController)controller).\n-        setCellScanner(CellUtil.createCellScanner(cellScannables));\n+        setCellScanner(CellUtil.createCellScanner(results));\n     } else {\n       for (Result res: results) {\n         ClientProtos.Result pbr = ProtobufUtil.toResult(res);",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "56c844ccedcc005d89afdae7772298525bc437c9",
                "status": "modified"
            }
        ],
        "message": "HBASE-9541 icv fails w/ npe if client does not support cellblocks\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523812 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2ee04ae2cfbce4a36662176385c1464d531971b7",
        "patched_files": [
            "CellUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCellUtil.java"
        ]
    },
    "hbase_1222383": {
        "bug_id": "hbase_1222383",
        "commit": "https://github.com/apache/hbase/commit/1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java?ref=1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "patch": "@@ -211,7 +211,12 @@ public void append(HLog.Entry entry) throws IOException {\n   @Override\n   public void close() throws IOException {\n     if (this.writer != null) {\n-      this.writer.close();\n+      try {\n+        this.writer.close();\n+      } catch (NullPointerException npe) {\n+        // Can get a NPE coming up from down in DFSClient$DFSOutputStream#close\n+        LOG.warn(npe);\n+      }\n       this.writer = null;\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "sha": "cbef70f159291edc1583aa85278f4879f162b698",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java?ref=1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
                "deletions": 1,
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "patch": "@@ -254,7 +254,6 @@ public void testRecoveredEdits() throws Exception {\n    * detects that the region server has aborted.\n    * @throws Exception\n    */\n-  @Ignore\n   @Test (timeout=300000)\n   public void testWorkerAbort() throws Exception {\n     LOG.info(\"testWorkerAbort\");",
                "raw_url": "https://github.com/apache/hbase/raw/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "sha": "a348f0c9f4edc3dd63749e9a6e7c8cb911103a6f",
                "status": "modified"
            }
        ],
        "message": "HBASE-5029 TestDistributedLogSplitting fails on occasion; Added catch of NPE and reenabled ignored test\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1220991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2062ea1d004172684a625790c85a459e1ffe3559",
        "patched_files": [
            "SequenceFileLogWriter.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestDistributedLogSplitting.java"
        ]
    },
    "hbase_13c5af3": {
        "bug_id": "hbase_13c5af3",
        "commit": "https://github.com/apache/hbase/commit/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "patch": "@@ -60,9 +60,9 @@\n   // i.e. empty column and a timestamp of LATEST_TIMESTAMP.\n   protected final byte [] splitkey;\n \n-  protected final Cell splitCell;\n+  private final Cell splitCell;\n \n-  private Optional<Cell> firstKey = null;\n+  private Optional<Cell> firstKey = Optional.empty();\n \n   private boolean firstKeySeeked = false;\n \n@@ -269,7 +269,8 @@ public int reseekTo(Cell key) throws IOException {\n       public boolean seekBefore(Cell key) throws IOException {\n         if (top) {\n           Optional<Cell> fk = getFirstKey();\n-          if (PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n+          if (fk.isPresent() &&\n+                  PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n             return false;\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hbase/raw/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "sha": "11ab068ef3d15b358faeec46102b53ada57244d2",
                "status": "modified"
            }
        ],
        "message": "HBASE-22520 Avoid possible NPE while performing seekBefore in Hal\u2026 (#281)\n\nHBASE-22520 Avoid possible NPE while performing seekBefore in HalfStoreFileReader",
        "parent": "https://github.com/apache/hbase/commit/6ea2566ac3f1d1cb76cedf87f00cda6583013b2f",
        "patched_files": [
            "HalfStoreFileReader.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_142997c": {
        "bug_id": "hbase_142997c",
        "commit": "https://github.com/apache/hbase/commit/142997c04e796784b3f0aa40818aa4b7c59664d9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java?ref=142997c04e796784b3f0aa40818aa4b7c59664d9",
                "deletions": 1,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "patch": "@@ -210,7 +210,9 @@ public static int getCompressedSize(Algorithm algo, Compressor compressor,\n     } finally {\n       nullOutputStream.close();\n       compressedStream.close();\n-      compressingStream.close();\n+      if (compressingStream != null) {\n+        compressingStream.close();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "sha": "afaf1976dc6d97878621254fb35d2a7d043de7ff",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hbase/blob/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java?ref=142997c04e796784b3f0aa40818aa4b7c59664d9",
                "deletions": 0,
                "filename": "hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "patch": "@@ -0,0 +1,65 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.io.encoding;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;\n+import org.apache.hadoop.hbase.testclassification.MiscTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.mockito.Mockito;\n+\n+/**\n+ * Test for EncodedDataBlock\n+ */\n+@Category({MiscTests.class, SmallTests.class})\n+public class TestEncodedDataBlock {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestEncodedDataBlock.class);\n+\n+  private Algorithm algo;\n+  private static final byte[] INPUT_BYTES = new byte[]{0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0,\n+    1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0};\n+\n+  @Before\n+  public void setUp() throws IOException {\n+    algo = Mockito.mock(Algorithm.class);\n+  }\n+\n+  @Test\n+  public void testGetCompressedSize() throws Exception {\n+    Mockito.when(algo.createCompressionStream(Mockito.any(), Mockito.any(), Mockito.anyInt()))\n+      .thenThrow(IOException.class);\n+    try {\n+      EncodedDataBlock.getCompressedSize(algo, null, INPUT_BYTES, 0, 0);\n+      throw new RuntimeException(\"Should not reach here\");\n+    } catch (IOException e) {\n+      Mockito.verify(algo, Mockito.times(1)).createCompressionStream(Mockito.any(),\n+        Mockito.any(), Mockito.anyInt());\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "sha": "e2cfa45d6b7d20e5e96cf0d9657de703d41d31ac",
                "status": "added"
            }
        ],
        "message": "HBASE-23342 : Handle NPE while closing compressingStream (#877)\n\nSigned-off-by Anoop Sam John <anoopsamjohn@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/d69ecf6092c08323f49d8bb4131312b8f4981aa9",
        "patched_files": [
            "EncodedDataBlock.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestEncodedDataBlock.java"
        ]
    },
    "hbase_157a60f": {
        "bug_id": "hbase_157a60f",
        "commit": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=157a60f1b396ab9adc7f934a15352f2dbc5493a9",
                "deletions": 12,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,10 +17,7 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n+import javax.annotation.Nullable;\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -47,8 +44,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n-import javax.annotation.Nullable;\n-\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -63,6 +58,7 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -88,7 +84,6 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n-import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -134,6 +129,10 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3768,11 +3767,8 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        HMaster master = getMiniHBaseCluster().getMaster();\n-        if (master == null) return false;\n-        AssignmentManager am = master.getAssignmentManager();\n-        if (am == null) return false;\n-        final RegionStates regionStates = am.getRegionStates();\n+        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n+            .getAssignmentManager().getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "5bb25dbdc0efebb03d279ffa916bd87c35b5f3b9",
                "status": "modified"
            }
        ],
        "message": "Revert \"HBASE-14909 NPE testing for RIT\"\n\nThis reverts commit da0cc598feab995eed12527d90805dd627674035.",
        "parent": "https://github.com/apache/hbase/commit/35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
        "patched_files": [
            "HBaseTestingUtility.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_1596f8a": {
        "bug_id": "hbase_1596f8a",
        "commit": "https://github.com/apache/hbase/commit/1596f8aa10e3af2549c406a90a04ab85447830ea",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -46,6 +46,7 @@ Trunk (unreleased changes)\n     HADOOP-1847 Many HBase tests do not fail well. (phase 2)\n     HADOOP-1870 Once file system failure has been detected, don't check it again\n                 and get on with shutting down the hbase cluster.\n+    HADOOP-1888 NullPointerException in HMemcacheScanner\n \n   IMPROVEMENTS\n     HADOOP-1737 Make HColumnDescriptor data publically members settable",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/CHANGES.txt",
                "sha": "e47f49259ce3a027625f62f85bcc59b3f6300e61",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "patch": "@@ -28,7 +28,7 @@\n   /** constructor */\n   public HBaseConfiguration() {\n     super();\n-    addDefaultResource(\"hbase-default.xml\");\n-    addDefaultResource(\"hbase-site.xml\");\n+    addResource(\"hbase-default.xml\");\n+    addResource(\"hbase-site.xml\");\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "sha": "71f53173e46074ff2826ca694f383fe11d10fb63",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMaster.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "patch": "@@ -897,7 +897,7 @@ public HMaster(Path dir, HServerAddress address, Configuration conf)\n       \n     } catch (IOException e) {\n       LOG.fatal(\"Not starting HMaster because:\", e);\n-      return;\n+      throw e;\n     }\n \n     this.threadWakeFrequency = conf.getLong(THREAD_WAKE_FREQUENCY, 10 * 1000);\n@@ -1145,6 +1145,11 @@ public void run() {\n    * by remote region servers have expired.\n    */\n   private void letRegionServersShutdown() {\n+    if (!fsOk) {\n+      // Forget waiting for the region servers if the file system has gone\n+      // away. Just exit as quickly as possible.\n+      return;\n+    }\n     synchronized (serversToServerInfo) {\n       while (this.serversToServerInfo.size() > 0) {\n         LOG.info(\"Waiting on following regionserver(s) to go down (or \" +",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMaster.java",
                "sha": "5d7aa06a4446d12d4ede629cdd83ff30e0d46d11",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 32,
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "patch": "@@ -26,7 +26,6 @@\n import java.util.Map;\n import java.util.SortedMap;\n import java.util.TreeMap;\n-import java.util.Vector;\n import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.commons.logging.Log;\n@@ -40,10 +39,13 @@\n  */\n public class HMemcache {\n   static final Log LOG = LogFactory.getLog(HMemcache.class);\n-  TreeMap<HStoreKey, byte []> memcache =\n-    new TreeMap<HStoreKey, byte []>();\n-  final Vector<TreeMap<HStoreKey, byte []>> history\n-    = new Vector<TreeMap<HStoreKey, byte []>>();\n+  \n+  // Note that since these structures are always accessed with a lock held,\n+  // no additional synchronization is required.\n+  \n+  TreeMap<HStoreKey, byte []> memcache = new TreeMap<HStoreKey, byte []>();\n+  final ArrayList<TreeMap<HStoreKey, byte []>> history =\n+    new ArrayList<TreeMap<HStoreKey, byte []>>();\n   TreeMap<HStoreKey, byte []> snapshot = null;\n \n   final HLocking lock = new HLocking();\n@@ -124,7 +126,8 @@ public void deleteSnapshot() throws IOException {\n         throw new IOException(\"Snapshot not present!\");\n       }\n       for (Iterator<TreeMap<HStoreKey, byte []>> it = history.iterator(); \n-          it.hasNext();) {\n+        it.hasNext(); ) {\n+        \n         TreeMap<HStoreKey, byte []> cur = it.next();\n         if (snapshot == cur) {\n           it.remove();\n@@ -183,10 +186,11 @@ public long getSize() {\n           break;\n         }\n         results.addAll(results.size(),\n-          get(history.elementAt(i), key, numVersions - results.size()));\n+            get(history.get(i), key, numVersions - results.size()));\n       }\n-      return (results.size() == 0)?\n-        null: ImmutableBytesWritable.toArray(results);\n+      return (results.size() == 0) ? null :\n+        ImmutableBytesWritable.toArray(results);\n+      \n     } finally {\n       this.lock.releaseReadLock();\n     }\n@@ -205,8 +209,8 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       internalGetFull(memcache, key, results);\n-      for (int i = history.size()-1; i >= 0; i--) {\n-        TreeMap<HStoreKey, byte []> cur = history.elementAt(i);\n+      for (int i = history.size() - 1; i >= 0; i--) {\n+        TreeMap<HStoreKey, byte []> cur = history.get(i);\n         internalGetFull(cur, key, results);\n       }\n       return results;\n@@ -285,9 +289,9 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n     try {\n       List<HStoreKey> results = getKeys(this.memcache, origin, versions);\n       for (int i = history.size() - 1; i >= 0; i--) {\n-        results.addAll(results.size(), getKeys(history.elementAt(i), origin,\n-          versions == HConstants.ALL_VERSIONS? versions:\n-           (results != null? versions - results.size(): versions)));\n+        results.addAll(results.size(), getKeys(history.get(i), origin,\n+            versions == HConstants.ALL_VERSIONS ? versions :\n+              (versions - results.size())));\n       }\n       return results;\n     } finally {\n@@ -345,9 +349,8 @@ boolean isDeleted(final byte [] value) {\n    * Return a scanner over the keys in the HMemcache\n    */\n   HInternalScannerInterface getScanner(long timestamp,\n-      Text targetCols[], Text firstRow)\n-  throws IOException {  \n-    return new HMemcacheScanner(timestamp, targetCols, firstRow);\n+      Text targetCols[], Text firstRow) throws IOException {  \n+      return new HMemcacheScanner(timestamp, targetCols, firstRow);\n   }\n \n   //////////////////////////////////////////////////////////////////////////////\n@@ -361,35 +364,38 @@ HInternalScannerInterface getScanner(long timestamp,\n \n     @SuppressWarnings(\"unchecked\")\n     HMemcacheScanner(final long timestamp, final Text targetCols[],\n-        final Text firstRow)\n-    throws IOException {\n+        final Text firstRow) throws IOException {\n+\n       super(timestamp, targetCols);\n       lock.obtainReadLock();\n       try {\n         this.backingMaps = new TreeMap[history.size() + 1];\n-        \n-        //NOTE: Since we iterate through the backing maps from 0 to n, we need\n-        // to put the memcache first, the newest history second, ..., etc.\n+\n+        // Note that since we iterate through the backing maps from 0 to n, we\n+        // need to put the memcache first, the newest history second, ..., etc.\n+\n         backingMaps[0] = memcache;\n-        for(int i = history.size() - 1; i > 0; i--) {\n-          backingMaps[i] = history.elementAt(i);\n+        for (int i = history.size() - 1; i > 0; i--) {\n+          backingMaps[i + 1] = history.get(i);\n         }\n-      \n+\n         this.keyIterators = new Iterator[backingMaps.length];\n         this.keys = new HStoreKey[backingMaps.length];\n         this.vals = new byte[backingMaps.length][];\n \n         // Generate list of iterators\n+        \n         HStoreKey firstKey = new HStoreKey(firstRow);\n-        for(int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = (/*firstRow != null &&*/ firstRow.getLength() != 0)?\n-            backingMaps[i].tailMap(firstKey).keySet().iterator():\n-            backingMaps[i].keySet().iterator();\n-          while(getNext(i)) {\n-            if(! findFirstRow(i, firstRow)) {\n+        for (int i = 0; i < backingMaps.length; i++) {\n+          keyIterators[i] = firstRow.getLength() != 0 ?\n+              backingMaps[i].tailMap(firstKey).keySet().iterator() :\n+                backingMaps[i].keySet().iterator();\n+\n+          while (getNext(i)) {\n+            if (!findFirstRow(i, firstRow)) {\n               continue;\n             }\n-            if(columnMatch(i)) {\n+            if (columnMatch(i)) {\n               break;\n             }\n           }",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "sha": "b029e987b92c48bb322b7473f71bcaec5b84eddc",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 17,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "patch": "@@ -676,8 +676,10 @@ public void run() {\n                 if (LOG.isDebugEnabled()) {\n                   LOG.debug(\"Got call server startup message\");\n                 }\n-                closeAllRegions();\n-                restart = true;\n+                if (fsOk) {\n+                  closeAllRegions();\n+                  restart = true;\n+                }\n                 break;\n \n               case HMsg.MSG_REGIONSERVER_STOP:\n@@ -689,10 +691,12 @@ public void run() {\n                 break;\n \n               default:\n-                try {\n-                  toDo.put(new ToDoEntry(msgs[i]));\n-                } catch (InterruptedException e) {\n-                  throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+                if (fsOk) {\n+                  try {\n+                    toDo.put(new ToDoEntry(msgs[i]));\n+                  } catch (InterruptedException e) {\n+                    throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+                  }\n                 }\n               }\n             }\n@@ -747,20 +751,24 @@ public void run() {\n     }\n \n     if (abortRequested) {\n-      try {\n-        log.close();\n-        LOG.info(\"On abort, closed hlog\");\n-      } catch (IOException e) {\n-        if (e instanceof RemoteException) {\n-          try {\n-            e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);\n-          } catch (IOException ex) {\n-            e = ex;\n+      if (fsOk) {\n+        // Only try to clean up if the file system is available\n+\n+        try {\n+          log.close();\n+          LOG.info(\"On abort, closed hlog\");\n+        } catch (IOException e) {\n+          if (e instanceof RemoteException) {\n+            try {\n+              e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);\n+            } catch (IOException ex) {\n+              e = ex;\n+            }\n           }\n+          LOG.error(\"Unable to close log in abort\", e);\n         }\n-        LOG.error(\"Unable to close log in abort\", e);\n+        closeAllRegions(); // Don't leave any open file handles\n       }\n-      closeAllRegions(); // Don't leave any open file handles\n       LOG.info(\"aborting server at: \" +\n         serverInfo.getServerAddress().toString());\n     } else {",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "sha": "c3fd41d059d95e35f7a9c6b1e80c5388713808ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestDFSAbort.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "patch": "@@ -32,7 +32,6 @@ public TestDFSAbort() {\n     super();\n     conf.setInt(\"ipc.client.timeout\", 5000);            // reduce ipc client timeout\n     conf.setInt(\"ipc.client.connect.max.retries\", 5);   // and number of retries\n-    conf.setInt(\"hbase.client.retries.number\", 5);      // reduce HBase retries\n     Logger.getRootLogger().setLevel(Level.WARN);\n     Logger.getLogger(this.getClass().getPackage().getName()).setLevel(Level.DEBUG);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "sha": "88779c9fbea97f17ab971e320363e9121e719089",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1888 NullPointerException in HMemcacheScanner\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@575791 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d660cfc5269085dcc5da75ab2f8fab098a31f177",
        "patched_files": [
            "CHANGES.java",
            "HBaseConfiguration.java",
            "HMaster.java",
            "HRegionServer.java",
            "HMemcache.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestDFSAbort.java",
            "TestHBaseConfiguration.java"
        ]
    },
    "hbase_1758553": {
        "bug_id": "hbase_1758553",
        "commit": "https://github.com/apache/hbase/commit/1758553f45e4a38e493f0cb5dee37e483c33905a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -803,6 +803,8 @@ Release 0.90.0 - Unreleased\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n    HBASE-3386  NPE in TableRecordReaderImpl.restart\n+   HBASE-3388  NPE processRegionInTransition(AssignmentManager.java:264)\n+               doing rolling-restart.sh\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "sha": "030ae6e4bed5625f79d3c5276e0bb2828ef839df",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -261,8 +261,13 @@ boolean processRegionInTransition(final String encodedRegionName,\n   throws KeeperException, IOException {\n     RegionTransitionData data = ZKAssign.getData(watcher, encodedRegionName);\n     if (data == null) return false;\n-    HRegionInfo hri = (regionInfo != null)? regionInfo:\n-      MetaReader.getRegion(catalogTracker, data.getRegionName()).getFirst();\n+    HRegionInfo hri = regionInfo;\n+    if (hri == null) {\n+      Pair<HRegionInfo, HServerAddress> p =\n+        MetaReader.getRegion(catalogTracker, data.getRegionName());\n+      if (p == null) return false;\n+      hri = p.getFirst();\n+    }\n     processRegionsInTransition(data, hri);\n     return true;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "2b345fb2ff5007351702ceee493a4f9442240453",
                "status": "modified"
            }
        ],
        "message": "HBASE-3388 NPE processRegionInTransition(AssignmentManager.java:264) doing rolling-restart.sh\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052058 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_1b51219": {
        "bug_id": "hbase_1b51219",
        "commit": "https://github.com/apache/hbase/commit/1b51219af609dba2144d61d17e28ab86a434105e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/1b51219af609dba2144d61d17e28ab86a434105e/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java?ref=1b51219af609dba2144d61d17e28ab86a434105e",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "patch": "@@ -33,7 +33,6 @@\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n-import org.apache.hadoop.hbase.client.replication.ReplicationAdmin;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.TableInputFormat;\n import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\n@@ -154,7 +153,10 @@ public Void connect(HConnection conn) throws IOException {\n     }\n \n     protected void cleanup(Context context) {\n-      replicatedScanner.close();\n+      if (replicatedScanner != null) {\n+        replicatedScanner.close();\n+        replicatedScanner = null;\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/1b51219af609dba2144d61d17e28ab86a434105e/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "sha": "8fb0b8e370189e91e2aceefa041e0b6c4d1d2b1e",
                "status": "modified"
            }
        ],
        "message": "HBASE-6394 verifyrep MR job map tasks throws NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1361469 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/3a687567044615e8cec144024ebc39c18f3fd530",
        "patched_files": [
            "VerifyReplication.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestVerifyReplication.java"
        ]
    },
    "hbase_201c838": {
        "bug_id": "hbase_201c838",
        "commit": "https://github.com/apache/hbase/commit/201c8382508da1266d11e04d3c7cbef42e0a256a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "patch": "@@ -388,7 +388,8 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    istream.seek(seekPoint);\n+    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n+\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "sha": "185423603628a26c1bd271dfaeddb4b81621ff86",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          istream.seek(fileOffset);\n+          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "sha": "0b140b6e301eff851c9c78f30252f64fc4eced2e",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "patch": "@@ -0,0 +1,43 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.io.hfile;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+public class HFileUtil {\n+\n+  /** guards against NullPointer\n+   * utility which tries to seek on the DFSIS and will try an alternative source\n+   * if the FSDataInputStream throws an NPE HBASE-17501\n+   * @param istream\n+   * @param offset\n+   * @throws IOException\n+   */\n+  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n+    try {\n+      // attempt to seek inside of current blockReader\n+      istream.seek(offset);\n+    } catch (NullPointerException e) {\n+      // retry the seek on an alternate copy of the data\n+      // this can occur if the blockReader on the DFSInputStream is null\n+      istream.seekToNewSource(offset);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "status": "added"
            }
        ],
        "message": "guard against NPE while reading FileTrailer and HFileBlock\n\nguard against NPE from FSInputStream#seek\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/35d7a0cd0798cabe7df5766fcc993512eca6c92e",
        "patched_files": [
            "HFileBlock.java",
            "FixedFileTrailer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFixedFileTrailer.java",
            "TestHFileBlock.java"
        ]
    },
    "hbase_209b6f7": {
        "bug_id": "hbase_209b6f7",
        "commit": "https://github.com/apache/hbase/commit/209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-resource-bundle/src/main/resources/supplemental-models.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 36,
                "filename": "hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "patch": "@@ -61,24 +61,6 @@ under the License.\n       </licenses>\n     </project>\n   </supplement>\n-  <supplement>\n-    <project>\n-      <groupId>commons-beanutils</groupId>\n-      <artifactId>commons-beanutils-core</artifactId>\n-\n-      <organization>\n-        <name>The Apache Software Foundation</name>\n-        <url>http://www.apache.org/</url>\n-      </organization>\n-      <licenses>\n-        <license>\n-          <name>Apache Software License, Version 2.0</name>\n-          <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n-          <distribution>repo</distribution>\n-        </license>\n-      </licenses>\n-    </project>\n-  </supplement>\n <!-- Artifacts with ambiguously named licenses in POM -->\n   <supplement>\n     <project>\n@@ -1213,22 +1195,4 @@ Copyright (c) 2007-2011 The JRuby project\n       </licenses>\n     </project>\n   </supplement>\n-  <supplement>\n-    <project>\n-      <groupId>xalan</groupId>\n-      <artifactId>xalan</artifactId>\n-\n-      <organization>\n-        <name>The Apache Software Foundation</name>\n-        <url>http://www.apache.org/</url>\n-      </organization>\n-      <licenses>\n-        <license>\n-          <name>The Apache Software License, Version 2.0</name>\n-          <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n-          <distribution>repo</distribution>\n-        </license>\n-      </licenses>\n-    </project>\n-  </supplement>\n </supplementalDataModels>",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "sha": "2f94226400e8f9223ee7ceb9902b6d4e95cae66e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/pom.xml",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/pom.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 11,
                "filename": "hbase-server/pom.xml",
                "patch": "@@ -561,17 +561,6 @@\n       <artifactId>bcprov-jdk16</artifactId>\n       <scope>test</scope>\n     </dependency>\n-    <dependency>\n-      <groupId>org.owasp.esapi</groupId>\n-      <artifactId>esapi</artifactId>\n-      <version>2.1.0.1</version>\n-      <exclusions>\n-        <exclusion>\n-          <artifactId>xercesImpl</artifactId>\n-          <groupId>xerces</groupId>\n-        </exclusion>\n-      </exclusions>\n-    </dependency>\n     <dependency>\n       <groupId>org.apache.kerby</groupId>\n       <artifactId>kerb-client</artifactId>",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/pom.xml",
                "sha": "ff001b75fe248b6252d106b9c7a88fc1581f2b91",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 11,
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "patch": "@@ -20,22 +20,12 @@ limitations under the License.\n java.util.*;\n org.apache.hadoop.hbase.monitoring.*;\n org.apache.hadoop.util.StringUtils;\n-org.owasp.esapi.ESAPI;\n-org.owasp.esapi.errors.EncodingException;\n </%import>\n <%args>\n TaskMonitor taskMonitor = TaskMonitor.get();\n String filter = \"general\";\n String format = \"html\";\n </%args>\n-<%class>\n-    public String encodeFilter() {\n-    try {\n-    return ESAPI.encoder().encodeForURL(filter);\n-    }catch(EncodingException e) {}\n-    return ESAPI.encoder().encodeForHTML(filter);\n-    }\n-</%class>\n <%java>\n List<? extends MonitoredTask> tasks = taskMonitor.getTasks();\n Iterator<? extends MonitoredTask> iter = tasks.iterator();\n@@ -72,7 +62,7 @@ boolean first = true;\n     <li <%if filter.equals(\"handler\")%>class=\"active\"</%if>><a href=\"?filter=handler\">Show All RPC Handler Tasks</a></li>\n     <li <%if filter.equals(\"rpc\")%>class=\"active\"</%if>><a href=\"?filter=rpc\">Show Active RPC Calls</a></li>\n     <li <%if filter.equals(\"operation\")%>class=\"active\"</%if>><a href=\"?filter=operation\">Show Client Operations</a></li>\n-    <li><a href=\"?format=json&filter=<% encodeFilter() %>\">View as JSON</a></li>\n+    <li><a href=\"?format=json&filter=<% filter %>\">View as JSON</a></li>\n   </ul>\n   <%if tasks.isEmpty()%>\n     <p>No tasks currently running on this node.</p>",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "sha": "b4a5feae4567d740c99a5d2f0e27a832f2d35860",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 7,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.http.HttpServer;\n import org.apache.hadoop.hbase.util.JSONBean;\n-import org.owasp.esapi.ESAPI;\n \n /*\n  * This servlet is based off of the JMXProxyServlet from Tomcat 7.0.14. It has\n@@ -168,7 +167,7 @@ public void doGet(HttpServletRequest request, HttpServletResponse response) {\n         jsonpcb = request.getParameter(CALLBACK_PARAM);\n         if (jsonpcb != null) {\n           response.setContentType(\"application/javascript; charset=utf8\");\n-          writer.write(encodeJS(jsonpcb) + \"(\");\n+          writer.write(jsonpcb + \"(\");\n         } else {\n           response.setContentType(\"application/json; charset=utf8\");\n         }\n@@ -221,9 +220,4 @@ public void doGet(HttpServletRequest request, HttpServletResponse response) {\n       response.setStatus(HttpServletResponse.SC_BAD_REQUEST);\n     }\n   }\n-\n-  private String encodeJS(String inputStr) {\n-    return ESAPI.encoder().encodeForJavaScript(inputStr);\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "sha": "45c2c1515610348f814dfd7c4a97d28b7504a02c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/main/resources/ESAPI.properties",
                "changes": 431,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/ESAPI.properties?ref=81b06b3bdd56c1c434406fe3e3aea7e47948f958",
                "deletions": 431,
                "filename": "hbase-server/src/main/resources/ESAPI.properties",
                "patch": "@@ -1,431 +0,0 @@\n-#\n-# OWASP Enterprise Security API (ESAPI) Properties file -- PRODUCTION Version\n-#\n-# This file is part of the Open Web Application Security Project (OWASP)\n-# Enterprise Security API (ESAPI) project. For details, please see\n-# http://www.owasp.org/index.php/ESAPI.\n-#\n-# Copyright (c) 2008,2009 - The OWASP Foundation\n-#\n-# DISCUSS: This may cause a major backwards compatibility issue, etc. but\n-#           from a name space perspective, we probably should have prefaced\n-#           all the property names with ESAPI or at least OWASP. Otherwise\n-#           there could be problems is someone loads this properties file into\n-#           the System properties.  We could also put this file into the\n-#           esapi.jar file (perhaps as a ResourceBundle) and then allow an external\n-#           ESAPI properties be defined that would overwrite these defaults.\n-#           That keeps the application's properties relatively simple as usually\n-#           they will only want to override a few properties. If looks like we\n-#           already support multiple override levels of this in the\n-#           DefaultSecurityConfiguration class, but I'm suggesting placing the\n-#           defaults in the esapi.jar itself. That way, if the jar is signed,\n-#           we could detect if those properties had been tampered with. (The\n-#           code to check the jar signatures is pretty simple... maybe 70-90 LOC,\n-#           but off course there is an execution penalty (similar to the way\n-#           that the separate sunjce.jar used to be when a class from it was\n-#           first loaded). Thoughts?\n-###############################################################################\n-#\n-# WARNING: Operating system protection should be used to lock down the .esapi\n-# resources directory and all the files inside and all the directories all the\n-# way up to the root directory of the file system.  Note that if you are using\n-# file-based implementations, that some files may need to be read-write as they\n-# get updated dynamically.\n-#\n-# Before using, be sure to update the MasterKey and MasterSalt as described below.\n-# N.B.: If you had stored data that you have previously encrypted with ESAPI 1.4,\n-#        you *must* FIRST decrypt it using ESAPI 1.4 and then (if so desired)\n-#        re-encrypt it with ESAPI 2.0. If you fail to do this, you will NOT be\n-#        able to decrypt your data with ESAPI 2.0.\n-#\n-#        YOU HAVE BEEN WARNED!!! More details are in the ESAPI 2.0 Release Notes.\n-#\n-#===========================================================================\n-# ESAPI Configuration\n-#\n-# If true, then print all the ESAPI properties set here when they are loaded.\n-# If false, they are not printed. Useful to reduce output when running JUnit tests.\n-# If you need to troubleshoot a properties related problem, turning this on may help.\n-# This is 'false' in the src/test/resources/.esapi version. It is 'true' by\n-# default for reasons of backward compatibility with earlier ESAPI versions.\n-ESAPI.printProperties=true\n-\n-# ESAPI is designed to be easily extensible. You can use the reference implementation\n-# or implement your own providers to take advantage of your enterprise's security\n-# infrastructure. The functions in ESAPI are referenced using the ESAPI locator, like:\n-#\n-#    String ciphertext =\n-#        ESAPI.encryptor().encrypt(\"Secret message\");   // Deprecated in 2.0\n-#    CipherText cipherText =\n-#        ESAPI.encryptor().encrypt(new PlainText(\"Secret message\")); // Preferred\n-#\n-# Below you can specify the classname for the provider that you wish to use in your\n-# application. The only requirement is that it implement the appropriate ESAPI interface.\n-# This allows you to switch security implementations in the future without rewriting the\n-# entire application.\n-#\n-# ExperimentalAccessController requires ESAPI-AccessControlPolicy.xml in .esapi directory\n-ESAPI.AccessControl=org.owasp.esapi.reference.DefaultAccessController\n-# FileBasedAuthenticator requires users.txt file in .esapi directory\n-ESAPI.Authenticator=org.owasp.esapi.reference.FileBasedAuthenticator\n-ESAPI.Encoder=org.owasp.esapi.reference.DefaultEncoder\n-ESAPI.Encryptor=org.owasp.esapi.reference.crypto.JavaEncryptor\n-\n-ESAPI.Executor=org.owasp.esapi.reference.DefaultExecutor\n-ESAPI.HTTPUtilities=org.owasp.esapi.reference.DefaultHTTPUtilities\n-ESAPI.IntrusionDetector=org.owasp.esapi.reference.DefaultIntrusionDetector\n-# Log4JFactory Requires log4j.xml or log4j.properties in classpath - http://www.laliluna.de/log4j-tutorial.html\n-ESAPI.Logger=org.owasp.esapi.reference.Log4JLogFactory\n-#ESAPI.Logger=org.owasp.esapi.reference.JavaLogFactory\n-ESAPI.Randomizer=org.owasp.esapi.reference.DefaultRandomizer\n-ESAPI.Validator=org.owasp.esapi.reference.DefaultValidator\n-\n-#===========================================================================\n-# ESAPI Authenticator\n-#\n-Authenticator.AllowedLoginAttempts=3\n-Authenticator.MaxOldPasswordHashes=13\n-Authenticator.UsernameParameterName=username\n-Authenticator.PasswordParameterName=password\n-# RememberTokenDuration (in days)\n-Authenticator.RememberTokenDuration=14\n-# Session Timeouts (in minutes)\n-Authenticator.IdleTimeoutDuration=20\n-Authenticator.AbsoluteTimeoutDuration=120\n-\n-#===========================================================================\n-# ESAPI Encoder\n-#\n-# ESAPI canonicalizes input before validation to prevent bypassing filters with encoded attacks.\n-# Failure to canonicalize input is a very common mistake when implementing validation schemes.\n-# Canonicalization is automatic when using the ESAPI Validator, but you can also use the\n-# following code to canonicalize data.\n-#\n-#      ESAPI.Encoder().canonicalize( \"%22hello world&#x22;\" );\n-#\n-# Multiple encoding is when a single encoding format is applied multiple times, multiple\n-# different encoding formats are applied, or when multiple formats are nested. Allowing\n-# multiple encoding is strongly discouraged.\n-Encoder.AllowMultipleEncoding=false\n-#\n-# The default list of codecs to apply when canonicalizing untrusted data. The list should include the codecs\n-# for all downstream interpreters or decoders. For example, if the data is likely to end up in a URL, HTML, or\n-# inside JavaScript, then the list of codecs below is appropriate. The order of the list is not terribly important.\n-Encoder.DefaultCodecList=HTMLEntityCodec,PercentCodec,JavaScriptCodec\n-\n-\n-#===========================================================================\n-# ESAPI Encryption\n-#\n-# The ESAPI Encryptor provides basic cryptographic functions with a simplified API.\n-# To get started, generate a new key using java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-# There is not currently any support for key rotation, so be careful when changing your key and salt as it\n-# will invalidate all signed, encrypted, and hashed data.\n-#\n-# WARNING: Not all combinations of algorithms and key lengths are supported.\n-# If you choose to use a key length greater than 128, you MUST download the\n-# unlimited strength policy files and install in the lib directory of your JRE/JDK.\n-# See http://java.sun.com/javase/downloads/index.jsp for more information.\n-#\n-# Backward compatibility with ESAPI Java 1.4 is supported by the two deprecated API\n-# methods, Encryptor.encrypt(String) and Encryptor.decrypt(String). However, whenever\n-# possible, these methods should be avoided as they use ECB cipher mode, which in almost\n-# all circumstances a poor choice because of it's weakness. CBC cipher mode is the default\n-# for the new Encryptor encrypt / decrypt methods for ESAPI Java 2.0.  In general, you\n-# should only use this compatibility setting if you have persistent data encrypted with\n-# version 1.4 and even then, you should ONLY set this compatibility mode UNTIL\n-# you have decrypted all of your old encrypted data and then re-encrypted it with\n-# ESAPI 2.0 using CBC mode. If you have some reason to mix the deprecated 1.4 mode\n-# with the new 2.0 methods, make sure that you use the same cipher algorithm for both\n-# (256-bit AES was the default for 1.4; 128-bit is the default for 2.0; see below for\n-# more details.) Otherwise, you will have to use the new 2.0 encrypt / decrypt methods\n-# where you can specify a SecretKey. (Note that if you are using the 256-bit AES,\n-# that requires downloading the special jurisdiction policy files mentioned above.)\n-#\n-#        ***** IMPORTANT: Do NOT forget to replace these with your own values! *****\n-# To calculate these values, you can run:\n-#        java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-#\n-Encryptor.MasterKey=\n-Encryptor.MasterSalt=\n-\n-# Provides the default JCE provider that ESAPI will \"prefer\" for its symmetric\n-# encryption and hashing. (That is it will look to this provider first, but it\n-# will defer to other providers if the requested algorithm is not implemented\n-# by this provider.) If left unset, ESAPI will just use your Java VM's current\n-# preferred JCE provider, which is generally set in the file\n-# \"$JAVA_HOME/jre/lib/security/java.security\".\n-#\n-# The main intent of this is to allow ESAPI symmetric encryption to be\n-# used with a FIPS 140-2 compliant crypto-module. For details, see the section\n-# \"Using ESAPI Symmetric Encryption with FIPS 140-2 Cryptographic Modules\" in\n-# the ESAPI 2.0 Symmetric Encryption User Guide, at:\n-# http://owasp-esapi-java.googlecode.com/svn/trunk/documentation/esapi4java-core-2.0-symmetric-crypto-user-guide.html\n-# However, this property also allows you to easily use an alternate JCE provider\n-# such as \"Bouncy Castle\" without having to make changes to \"java.security\".\n-# See Javadoc for SecurityProviderLoader for further details. If you wish to use\n-# a provider that is not known to SecurityProviderLoader, you may specify the\n-# fully-qualified class name of the JCE provider class that implements\n-# java.security.Provider. If the name contains a '.', this is interpreted as\n-# a fully-qualified class name that implements java.security.Provider.\n-#\n-# NOTE: Setting this property has the side-effect of changing it in your application\n-#       as well, so if you are using JCE in your application directly rather than\n-#       through ESAPI (you wouldn't do that, would you? ;-), it will change the\n-#       preferred JCE provider there as well.\n-#\n-# Default: Keeps the JCE provider set to whatever JVM sets it to.\n-Encryptor.PreferredJCEProvider=\n-\n-# AES is the most widely used and strongest encryption algorithm. This\n-# should agree with your Encryptor.CipherTransformation property.\n-# By default, ESAPI Java 1.4 uses \"PBEWithMD5AndDES\" and which is\n-# very weak. It is essentially a password-based encryption key, hashed\n-# with MD5 around 1K times and then encrypted with the weak DES algorithm\n-# (56-bits) using ECB mode and an unspecified padding (it is\n-# JCE provider specific, but most likely \"NoPadding\"). However, 2.0 uses\n-# \"AES/CBC/PKCSPadding\". If you want to change these, change them here.\n-# Warning: This property does not control the default reference implementation for\n-#           ESAPI 2.0 using JavaEncryptor. Also, this property will be dropped\n-#           in the future.\n-# @deprecated\n-Encryptor.EncryptionAlgorithm=AES\n-#        For ESAPI Java 2.0 - New encrypt / decrypt methods use this.\n-Encryptor.CipherTransformation=AES/CBC/PKCS5Padding\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Comma-separated list of cipher modes that provide *BOTH*\n-# confidentiality *AND* message authenticity. (NIST refers to such cipher\n-# modes as \"combined modes\" so that's what we shall call them.) If any of these\n-# cipher modes are used then no MAC is calculated and stored\n-# in the CipherText upon encryption. Likewise, if one of these\n-# cipher modes is used with decryption, no attempt will be made\n-# to validate the MAC contained in the CipherText object regardless\n-# of whether it contains one or not. Since the expectation is that\n-# these cipher modes support support message authenticity already,\n-# injecting a MAC in the CipherText object would be at best redundant.\n-#\n-# Note that as of JDK 1.5, the SunJCE provider does not support *any*\n-# of these cipher modes. Of these listed, only GCM and CCM are currently\n-# NIST approved. YMMV for other JCE providers. E.g., Bouncy Castle supports\n-# GCM and CCM with \"NoPadding\" mode, but not with \"PKCS5Padding\" or other\n-# padding modes.\n-Encryptor.cipher_modes.combined_modes=GCM,CCM,IAPM,EAX,OCB,CWC\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Additional cipher modes allowed for ESAPI 2.0 encryption. These\n-# cipher modes are in _addition_ to those specified by the property\n-# 'Encryptor.cipher_modes.combined_modes'.\n-# Note: We will add support for streaming modes like CFB & OFB once\n-# we add support for 'specified' to the property 'Encryptor.ChooseIVMethod'\n-# (probably in ESAPI 2.1).\n-# DISCUSS: Better name?\n-Encryptor.cipher_modes.additional_allowed=CBC\n-\n-# 128-bit is almost always sufficient and appears to be more resistant to\n-# related key attacks than is 256-bit AES. Use '_' to use default key size\n-# for cipher algorithms (where it makes sense because the algorithm supports\n-# a variable key size). Key length must agree to what's provided as the\n-# cipher transformation, otherwise this will be ignored after logging a\n-# warning.\n-#\n-# NOTE: This is what applies BOTH ESAPI 1.4 and 2.0. See warning above about mixing!\n-Encryptor.EncryptionKeyLength=128\n-\n-# Because 2.0 uses CBC mode by default, it requires an initialization vector (IV).\n-# (All cipher modes except ECB require an IV.) There are two choices: we can either\n-# use a fixed IV known to both parties or allow ESAPI to choose a random IV. While\n-# the IV does not need to be hidden from adversaries, it is important that the\n-# adversary not be allowed to choose it. Also, random IVs are generally much more\n-# secure than fixed IVs. (In fact, it is essential that feed-back cipher modes\n-# such as CFB and OFB use a different IV for each encryption with a given key so\n-# in such cases, random IVs are much preferred. By default, ESAPI 2.0 uses random\n-# IVs. If you wish to use 'fixed' IVs, set 'Encryptor.ChooseIVMethod=fixed' and\n-# uncomment the Encryptor.fixedIV.\n-#\n-# Valid values:        random|fixed|specified        'specified' not yet implemented; planned for 2.1\n-Encryptor.ChooseIVMethod=random\n-# If you choose to use a fixed IV, then you must place a fixed IV here that\n-# is known to all others who are sharing your secret key. The format should\n-# be a hex string that is the same length as the cipher block size for the\n-# cipher algorithm that you are using. The following is an example for AES\n-# from an AES test vector for AES-128/CBC as described in:\n-# NIST Special Publication 800-38A (2001 Edition)\n-# \"Recommendation for Block Cipher Modes of Operation\".\n-# (Note that the block size for AES is 16 bytes == 128 bits.)\n-#\n-Encryptor.fixedIV=0x000102030405060708090a0b0c0d0e0f\n-\n-# Whether or not CipherText should use a message authentication code (MAC) with it.\n-# This prevents an adversary from altering the IV as well as allowing a more\n-# fool-proof way of determining the decryption failed because of an incorrect\n-# key being supplied. This refers to the \"separate\" MAC calculated and stored\n-# in CipherText, not part of any MAC that is calculated as a result of a\n-# \"combined mode\" cipher mode.\n-#\n-# If you are using ESAPI with a FIPS 140-2 cryptographic module, you *must* also\n-# set this property to false.\n-Encryptor.CipherText.useMAC=true\n-\n-# Whether or not the PlainText object may be overwritten and then marked\n-# eligible for garbage collection. If not set, this is still treated as 'true'.\n-Encryptor.PlainText.overwrite=true\n-\n-# Do not use DES except in a legacy situations. 56-bit is way too small key size.\n-#Encryptor.EncryptionKeyLength=56\n-#Encryptor.EncryptionAlgorithm=DES\n-\n-# TripleDES is considered strong enough for most purposes.\n-#    Note:    There is also a 112-bit version of DESede. Using the 168-bit version\n-#            requires downloading the special jurisdiction policy from Sun.\n-#Encryptor.EncryptionKeyLength=168\n-#Encryptor.EncryptionAlgorithm=DESede\n-\n-Encryptor.HashAlgorithm=SHA-512\n-Encryptor.HashIterations=1024\n-Encryptor.DigitalSignatureAlgorithm=SHA1withDSA\n-Encryptor.DigitalSignatureKeyLength=1024\n-Encryptor.RandomAlgorithm=SHA1PRNG\n-Encryptor.CharacterEncoding=UTF-8\n-\n-\n-#===========================================================================\n-# ESAPI HttpUtilties\n-#\n-# The HttpUtilities provide basic protections to HTTP requests and responses. Primarily these methods\n-# protect against malicious data from attackers, such as unprintable characters, escaped characters,\n-# and other simple attacks. The HttpUtilities also provides utility methods for dealing with cookies,\n-# headers, and CSRF tokens.\n-#\n-# Default file upload location (remember to escape backslashes with \\\\)\n-HttpUtilities.UploadDir=C:\\\\ESAPI\\\\testUpload\n-HttpUtilities.UploadTempDir=C:\\\\temp\n-# Force flags on cookies, if you use HttpUtilities to set cookies\n-HttpUtilities.ForceHttpOnlySession=false\n-HttpUtilities.ForceSecureSession=false\n-HttpUtilities.ForceHttpOnlyCookies=true\n-HttpUtilities.ForceSecureCookies=true\n-# Maximum size of HTTP headers\n-HttpUtilities.MaxHeaderSize=4096\n-# File upload configuration\n-HttpUtilities.ApprovedUploadExtensions=.zip,.pdf,.doc,.docx,.ppt,.pptx,.tar,.gz,.tgz,.rar,.war,.jar,.ear,.xls,.rtf,.properties,.java,.class,.txt,.xml,.jsp,.jsf,.exe,.dll\n-HttpUtilities.MaxUploadFileBytes=500000000\n-# Using UTF-8 throughout your stack is highly recommended. That includes your database driver,\n-# container, and any other technologies you may be using. Failure to do this may expose you\n-# to Unicode transcoding injection attacks. Use of UTF-8 does not hinder internationalization.\n-HttpUtilities.ResponseContentType=text/html; charset=UTF-8\n-\n-\n-\n-#===========================================================================\n-# ESAPI Executor\n-# CHECKME - Not sure what this is used for, but surely it should be made OS independent.\n-Executor.WorkingDirectory=C:\\\\Windows\\\\Temp\n-Executor.ApprovedExecutables=C:\\\\Windows\\\\System32\\\\cmd.exe,C:\\\\Windows\\\\System32\\\\runas.exe\n-\n-\n-#===========================================================================\n-# ESAPI Logging\n-# Set the application name if these logs are combined with other applications\n-Logger.ApplicationName=ExampleApplication\n-# If you use an HTML log viewer that does not properly HTML escape log data, you can set LogEncodingRequired to true\n-Logger.LogEncodingRequired=false\n-# Determines whether ESAPI should log the application name. This might be clutter in some single-server/single-app environments.\n-Logger.LogApplicationName=true\n-# Determines whether ESAPI should log the server IP and port. This might be clutter in some single-server environments.\n-Logger.LogServerIP=true\n-# LogFileName, the name of the logging file. Provide a full directory path (e.g., C:\\\\ESAPI\\\\ESAPI_logging_file) if you\n-# want to place it in a specific directory.\n-Logger.LogFileName=ESAPI_logging_file\n-# MaxLogFileSize, the max size (in bytes) of a single log file before it cuts over to a new one (default is 10,000,000)\n-Logger.MaxLogFileSize=10000000\n-\n-\n-#===========================================================================\n-# ESAPI Intrusion Detection\n-#\n-# Each event has a base to which .count, .interval, and .action are added\n-# The IntrusionException will fire if we receive \"count\" events within \"interval\" seconds\n-# The IntrusionDetector is configurable to take the following actions: log, logout, and disable\n-#  (multiple actions separated by commas are allowed e.g. event.test.actions=log,disable\n-#\n-# Custom Events\n-# Names must start with \"event.\" as the base\n-# Use IntrusionDetector.addEvent( \"test\" ) in your code to trigger \"event.test\" here\n-# You can also disable intrusion detection completely by changing\n-# the following parameter to true\n-#\n-IntrusionDetector.Disable=false\n-#\n-IntrusionDetector.event.test.count=2\n-IntrusionDetector.event.test.interval=10\n-IntrusionDetector.event.test.actions=disable,log\n-\n-# Exception Events\n-# All EnterpriseSecurityExceptions are registered automatically\n-# Call IntrusionDetector.getInstance().addException(e) for Exceptions that do not extend EnterpriseSecurityException\n-# Use the fully qualified classname of the exception as the base\n-\n-# any intrusion is an attack\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.count=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.interval=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.actions=log,disable,logout\n-\n-# for test purposes\n-# CHECKME: Shouldn't there be something in the property name itself that designates\n-#           that these are for testing???\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.count=10\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.interval=5\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.actions=log,disable,logout\n-\n-# rapid validation errors indicate scans or attacks in progress\n-# org.owasp.esapi.errors.ValidationException.count=10\n-# org.owasp.esapi.errors.ValidationException.interval=10\n-# org.owasp.esapi.errors.ValidationException.actions=log,logout\n-\n-# sessions jumping between hosts indicates session hijacking\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.count=2\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.interval=10\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.actions=log,logout\n-\n-\n-#===========================================================================\n-# ESAPI Validation\n-#\n-# The ESAPI Validator works on regular expressions with defined names. You can define names\n-# either here, or you may define application specific patterns in a separate file defined below.\n-# This allows enterprises to specify both organizational standards as well as application specific\n-# validation rules.\n-#\n-Validator.ConfigurationFile=validation.properties\n-\n-# Validators used by ESAPI\n-Validator.AccountName=^[a-zA-Z0-9]{3,20}$\n-Validator.SystemCommand=^[a-zA-Z\\\\-\\\\/]{1,64}$\n-Validator.RoleName=^[a-z]{1,20}$\n-\n-#the word TEST below should be changed to your application\n-#name - only relative URL's are supported\n-Validator.Redirect=^\\\\/test.*$\n-\n-# Global HTTP Validation Rules\n-# Values with Base64 encoded data (e.g. encrypted state) will need at least [a-zA-Z0-9\\/+=]\n-Validator.HTTPScheme=^(http|https)$\n-Validator.HTTPServerName=^[a-zA-Z0-9_.\\\\-]*$\n-Validator.HTTPParameterName=^[a-zA-Z0-9_]{1,32}$\n-Validator.HTTPParameterValue=^[a-zA-Z0-9.\\\\-\\\\/+=_ ]*$\n-Validator.HTTPCookieName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPCookieValue=^[a-zA-Z0-9\\\\-\\\\/+=_ ]*$\n-Validator.HTTPHeaderName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPHeaderValue=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPContextPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPServletPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPPath=^[a-zA-Z0-9.\\\\-_]*$\n-Validator.HTTPQueryString=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ %]*$\n-Validator.HTTPURI=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPURL=^.*$\n-Validator.HTTPJSESSIONID=^[A-Z0-9]{10,30}$\n-\n-# Validation of file related input\n-Validator.FileName=^[a-zA-Z0-9!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$\n-Validator.DirectoryName=^[a-zA-Z0-9:/\\\\\\\\!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$",
                "raw_url": "https://github.com/apache/hbase/raw/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/main/resources/ESAPI.properties",
                "sha": "907400189d8dd33f34b03469af8fc1f250844cf8",
                "status": "removed"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/table.jsp?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 3,
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "patch": "@@ -29,7 +29,6 @@\n   import=\"java.util.Collection\"\n   import=\"java.util.Collections\"\n   import=\"java.util.Comparator\"\n-  import=\"org.owasp.esapi.ESAPI\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.util.StringUtils\"\n   import=\"org.apache.hadoop.hbase.HRegionInfo\"\n@@ -85,7 +84,7 @@\n     <% if ( !readOnly && action != null ) { %>\n         <title>HBase Master: <%= master.getServerName() %></title>\n     <% } else { %>\n-        <title>Table: <%= ESAPI.encoder().encodeForHTML(fqtn) %></title>\n+        <title>Table: <%= fqtn %></title>\n     <% } %>\n     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n     <meta name=\"description\" content=\"\">\n@@ -181,7 +180,7 @@ if ( fqtn != null ) {\n <div class=\"container-fluid content\">\n     <div class=\"row inner_header\">\n         <div class=\"page-header\">\n-            <h1>Table <small><%= ESAPI.encoder().encodeForHTML(fqtn) %></small></h1>\n+            <h1>Table <small><%= fqtn %></small></h1>\n         </div>\n     </div>\n     <div class=\"row\">",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "sha": "27388e78210544df7c47f0bbf0e6b595120d95bf",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 15,
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "patch": "@@ -21,7 +21,6 @@\n   import=\"java.util.Collection\"\n   import=\"java.util.Date\"\n   import=\"java.util.List\"\n-  import=\"org.owasp.esapi.ESAPI\"\n   import=\"static org.apache.commons.lang.StringEscapeUtils.escapeXml\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.hbase.HTableDescriptor\"\n@@ -36,14 +35,10 @@\n   String regionName = request.getParameter(\"name\");\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n   Configuration conf = rs.getConfiguration();\n-  String displayName = null;\n+\n   Region region = rs.getFromOnlineRegions(regionName);\n-  if(region == null) {\n-    displayName= ESAPI.encoder().encodeForHTML(regionName) + \" does not exist\";\n-  } else {\n-    displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+  String displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n     rs.getConfiguration());\n-  }\n %>\n <!--[if IE]>\n <!DOCTYPE html>\n@@ -126,14 +121,7 @@\n          <p> <%= storeFiles.size() %> StoreFile(s) in set.</p>\n          </table>\n    <%  }\n-   } else { %>\n-   <div class=\"container-fluid content\">\n-   <div class=\"row inner_header\">\n-   </div>\n-   <p><hr><p>\n-   <p>Go <a href=\"javascript:history.back()\">Back</a>\n-   </div>\n-  <% } %>\n+   }%>\n </div>\n <script src=\"/static/js/jquery.min.js\" type=\"text/javascript\"></script>\n <script src=\"/static/js/bootstrap.min.js\" type=\"text/javascript\"></script>",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "sha": "874ac4392440667aa6d23a8e314d7ca7c87ce0ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 6,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "patch": "@@ -105,11 +105,5 @@ public static void assertReFind(String re, String value) {\n     assertReFind(\"\\\"committed\\\"\\\\s*:\", result);\n     assertReFind(\"\\\\}\\\\);$\", result);\n \n-    // test to get XSS JSONP result\n-    result = readOutput(new URL(baseUrl, \"/jmx?qry=java.lang:type=Memory&callback=<script>alert('hello')</script>\"));\n-    LOG.info(\"/jmx?qry=java.lang:type=Memory&callback=<script>alert('hello')</script> RESULT: \"+result);\n-    assertTrue(!result.contains(\"<script>\"));\n-\n-\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "sha": "031ddce8bf06bc89f9fb7818845f0473ae594e3c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/test/resources/ESAPI.properties",
                "changes": 431,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/resources/ESAPI.properties?ref=81b06b3bdd56c1c434406fe3e3aea7e47948f958",
                "deletions": 431,
                "filename": "hbase-server/src/test/resources/ESAPI.properties",
                "patch": "@@ -1,431 +0,0 @@\n-#\n-# OWASP Enterprise Security API (ESAPI) Properties file -- PRODUCTION Version\n-#\n-# This file is part of the Open Web Application Security Project (OWASP)\n-# Enterprise Security API (ESAPI) project. For details, please see\n-# http://www.owasp.org/index.php/ESAPI.\n-#\n-# Copyright (c) 2008,2009 - The OWASP Foundation\n-#\n-# DISCUSS: This may cause a major backwards compatibility issue, etc. but\n-#           from a name space perspective, we probably should have prefaced\n-#           all the property names with ESAPI or at least OWASP. Otherwise\n-#           there could be problems is someone loads this properties file into\n-#           the System properties.  We could also put this file into the\n-#           esapi.jar file (perhaps as a ResourceBundle) and then allow an external\n-#           ESAPI properties be defined that would overwrite these defaults.\n-#           That keeps the application's properties relatively simple as usually\n-#           they will only want to override a few properties. If looks like we\n-#           already support multiple override levels of this in the\n-#           DefaultSecurityConfiguration class, but I'm suggesting placing the\n-#           defaults in the esapi.jar itself. That way, if the jar is signed,\n-#           we could detect if those properties had been tampered with. (The\n-#           code to check the jar signatures is pretty simple... maybe 70-90 LOC,\n-#           but off course there is an execution penalty (similar to the way\n-#           that the separate sunjce.jar used to be when a class from it was\n-#           first loaded). Thoughts?\n-###############################################################################\n-#\n-# WARNING: Operating system protection should be used to lock down the .esapi\n-# resources directory and all the files inside and all the directories all the\n-# way up to the root directory of the file system.  Note that if you are using\n-# file-based implementations, that some files may need to be read-write as they\n-# get updated dynamically.\n-#\n-# Before using, be sure to update the MasterKey and MasterSalt as described below.\n-# N.B.: If you had stored data that you have previously encrypted with ESAPI 1.4,\n-#        you *must* FIRST decrypt it using ESAPI 1.4 and then (if so desired)\n-#        re-encrypt it with ESAPI 2.0. If you fail to do this, you will NOT be\n-#        able to decrypt your data with ESAPI 2.0.\n-#\n-#        YOU HAVE BEEN WARNED!!! More details are in the ESAPI 2.0 Release Notes.\n-#\n-#===========================================================================\n-# ESAPI Configuration\n-#\n-# If true, then print all the ESAPI properties set here when they are loaded.\n-# If false, they are not printed. Useful to reduce output when running JUnit tests.\n-# If you need to troubleshoot a properties related problem, turning this on may help.\n-# This is 'false' in the src/test/resources/.esapi version. It is 'true' by\n-# default for reasons of backward compatibility with earlier ESAPI versions.\n-ESAPI.printProperties=true\n-\n-# ESAPI is designed to be easily extensible. You can use the reference implementation\n-# or implement your own providers to take advantage of your enterprise's security\n-# infrastructure. The functions in ESAPI are referenced using the ESAPI locator, like:\n-#\n-#    String ciphertext =\n-#        ESAPI.encryptor().encrypt(\"Secret message\");   // Deprecated in 2.0\n-#    CipherText cipherText =\n-#        ESAPI.encryptor().encrypt(new PlainText(\"Secret message\")); // Preferred\n-#\n-# Below you can specify the classname for the provider that you wish to use in your\n-# application. The only requirement is that it implement the appropriate ESAPI interface.\n-# This allows you to switch security implementations in the future without rewriting the\n-# entire application.\n-#\n-# ExperimentalAccessController requires ESAPI-AccessControlPolicy.xml in .esapi directory\n-ESAPI.AccessControl=org.owasp.esapi.reference.DefaultAccessController\n-# FileBasedAuthenticator requires users.txt file in .esapi directory\n-ESAPI.Authenticator=org.owasp.esapi.reference.FileBasedAuthenticator\n-ESAPI.Encoder=org.owasp.esapi.reference.DefaultEncoder\n-ESAPI.Encryptor=org.owasp.esapi.reference.crypto.JavaEncryptor\n-\n-ESAPI.Executor=org.owasp.esapi.reference.DefaultExecutor\n-ESAPI.HTTPUtilities=org.owasp.esapi.reference.DefaultHTTPUtilities\n-ESAPI.IntrusionDetector=org.owasp.esapi.reference.DefaultIntrusionDetector\n-# Log4JFactory Requires log4j.xml or log4j.properties in classpath - http://www.laliluna.de/log4j-tutorial.html\n-ESAPI.Logger=org.owasp.esapi.reference.Log4JLogFactory\n-#ESAPI.Logger=org.owasp.esapi.reference.JavaLogFactory\n-ESAPI.Randomizer=org.owasp.esapi.reference.DefaultRandomizer\n-ESAPI.Validator=org.owasp.esapi.reference.DefaultValidator\n-\n-#===========================================================================\n-# ESAPI Authenticator\n-#\n-Authenticator.AllowedLoginAttempts=3\n-Authenticator.MaxOldPasswordHashes=13\n-Authenticator.UsernameParameterName=username\n-Authenticator.PasswordParameterName=password\n-# RememberTokenDuration (in days)\n-Authenticator.RememberTokenDuration=14\n-# Session Timeouts (in minutes)\n-Authenticator.IdleTimeoutDuration=20\n-Authenticator.AbsoluteTimeoutDuration=120\n-\n-#===========================================================================\n-# ESAPI Encoder\n-#\n-# ESAPI canonicalizes input before validation to prevent bypassing filters with encoded attacks.\n-# Failure to canonicalize input is a very common mistake when implementing validation schemes.\n-# Canonicalization is automatic when using the ESAPI Validator, but you can also use the\n-# following code to canonicalize data.\n-#\n-#      ESAPI.Encoder().canonicalize( \"%22hello world&#x22;\" );\n-#\n-# Multiple encoding is when a single encoding format is applied multiple times, multiple\n-# different encoding formats are applied, or when multiple formats are nested. Allowing\n-# multiple encoding is strongly discouraged.\n-Encoder.AllowMultipleEncoding=false\n-#\n-# The default list of codecs to apply when canonicalizing untrusted data. The list should include the codecs\n-# for all downstream interpreters or decoders. For example, if the data is likely to end up in a URL, HTML, or\n-# inside JavaScript, then the list of codecs below is appropriate. The order of the list is not terribly important.\n-Encoder.DefaultCodecList=HTMLEntityCodec,PercentCodec,JavaScriptCodec\n-\n-\n-#===========================================================================\n-# ESAPI Encryption\n-#\n-# The ESAPI Encryptor provides basic cryptographic functions with a simplified API.\n-# To get started, generate a new key using java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-# There is not currently any support for key rotation, so be careful when changing your key and salt as it\n-# will invalidate all signed, encrypted, and hashed data.\n-#\n-# WARNING: Not all combinations of algorithms and key lengths are supported.\n-# If you choose to use a key length greater than 128, you MUST download the\n-# unlimited strength policy files and install in the lib directory of your JRE/JDK.\n-# See http://java.sun.com/javase/downloads/index.jsp for more information.\n-#\n-# Backward compatibility with ESAPI Java 1.4 is supported by the two deprecated API\n-# methods, Encryptor.encrypt(String) and Encryptor.decrypt(String). However, whenever\n-# possible, these methods should be avoided as they use ECB cipher mode, which in almost\n-# all circumstances a poor choice because of it's weakness. CBC cipher mode is the default\n-# for the new Encryptor encrypt / decrypt methods for ESAPI Java 2.0.  In general, you\n-# should only use this compatibility setting if you have persistent data encrypted with\n-# version 1.4 and even then, you should ONLY set this compatibility mode UNTIL\n-# you have decrypted all of your old encrypted data and then re-encrypted it with\n-# ESAPI 2.0 using CBC mode. If you have some reason to mix the deprecated 1.4 mode\n-# with the new 2.0 methods, make sure that you use the same cipher algorithm for both\n-# (256-bit AES was the default for 1.4; 128-bit is the default for 2.0; see below for\n-# more details.) Otherwise, you will have to use the new 2.0 encrypt / decrypt methods\n-# where you can specify a SecretKey. (Note that if you are using the 256-bit AES,\n-# that requires downloading the special jurisdiction policy files mentioned above.)\n-#\n-#        ***** IMPORTANT: Do NOT forget to replace these with your own values! *****\n-# To calculate these values, you can run:\n-#        java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-#\n-Encryptor.MasterKey=\n-Encryptor.MasterSalt=\n-\n-# Provides the default JCE provider that ESAPI will \"prefer\" for its symmetric\n-# encryption and hashing. (That is it will look to this provider first, but it\n-# will defer to other providers if the requested algorithm is not implemented\n-# by this provider.) If left unset, ESAPI will just use your Java VM's current\n-# preferred JCE provider, which is generally set in the file\n-# \"$JAVA_HOME/jre/lib/security/java.security\".\n-#\n-# The main intent of this is to allow ESAPI symmetric encryption to be\n-# used with a FIPS 140-2 compliant crypto-module. For details, see the section\n-# \"Using ESAPI Symmetric Encryption with FIPS 140-2 Cryptographic Modules\" in\n-# the ESAPI 2.0 Symmetric Encryption User Guide, at:\n-# http://owasp-esapi-java.googlecode.com/svn/trunk/documentation/esapi4java-core-2.0-symmetric-crypto-user-guide.html\n-# However, this property also allows you to easily use an alternate JCE provider\n-# such as \"Bouncy Castle\" without having to make changes to \"java.security\".\n-# See Javadoc for SecurityProviderLoader for further details. If you wish to use\n-# a provider that is not known to SecurityProviderLoader, you may specify the\n-# fully-qualified class name of the JCE provider class that implements\n-# java.security.Provider. If the name contains a '.', this is interpreted as\n-# a fully-qualified class name that implements java.security.Provider.\n-#\n-# NOTE: Setting this property has the side-effect of changing it in your application\n-#       as well, so if you are using JCE in your application directly rather than\n-#       through ESAPI (you wouldn't do that, would you? ;-), it will change the\n-#       preferred JCE provider there as well.\n-#\n-# Default: Keeps the JCE provider set to whatever JVM sets it to.\n-Encryptor.PreferredJCEProvider=\n-\n-# AES is the most widely used and strongest encryption algorithm. This\n-# should agree with your Encryptor.CipherTransformation property.\n-# By default, ESAPI Java 1.4 uses \"PBEWithMD5AndDES\" and which is\n-# very weak. It is essentially a password-based encryption key, hashed\n-# with MD5 around 1K times and then encrypted with the weak DES algorithm\n-# (56-bits) using ECB mode and an unspecified padding (it is\n-# JCE provider specific, but most likely \"NoPadding\"). However, 2.0 uses\n-# \"AES/CBC/PKCSPadding\". If you want to change these, change them here.\n-# Warning: This property does not control the default reference implementation for\n-#           ESAPI 2.0 using JavaEncryptor. Also, this property will be dropped\n-#           in the future.\n-# @deprecated\n-Encryptor.EncryptionAlgorithm=AES\n-#        For ESAPI Java 2.0 - New encrypt / decrypt methods use this.\n-Encryptor.CipherTransformation=AES/CBC/PKCS5Padding\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Comma-separated list of cipher modes that provide *BOTH*\n-# confidentiality *AND* message authenticity. (NIST refers to such cipher\n-# modes as \"combined modes\" so that's what we shall call them.) If any of these\n-# cipher modes are used then no MAC is calculated and stored\n-# in the CipherText upon encryption. Likewise, if one of these\n-# cipher modes is used with decryption, no attempt will be made\n-# to validate the MAC contained in the CipherText object regardless\n-# of whether it contains one or not. Since the expectation is that\n-# these cipher modes support support message authenticity already,\n-# injecting a MAC in the CipherText object would be at best redundant.\n-#\n-# Note that as of JDK 1.5, the SunJCE provider does not support *any*\n-# of these cipher modes. Of these listed, only GCM and CCM are currently\n-# NIST approved. YMMV for other JCE providers. E.g., Bouncy Castle supports\n-# GCM and CCM with \"NoPadding\" mode, but not with \"PKCS5Padding\" or other\n-# padding modes.\n-Encryptor.cipher_modes.combined_modes=GCM,CCM,IAPM,EAX,OCB,CWC\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Additional cipher modes allowed for ESAPI 2.0 encryption. These\n-# cipher modes are in _addition_ to those specified by the property\n-# 'Encryptor.cipher_modes.combined_modes'.\n-# Note: We will add support for streaming modes like CFB & OFB once\n-# we add support for 'specified' to the property 'Encryptor.ChooseIVMethod'\n-# (probably in ESAPI 2.1).\n-# DISCUSS: Better name?\n-Encryptor.cipher_modes.additional_allowed=CBC\n-\n-# 128-bit is almost always sufficient and appears to be more resistant to\n-# related key attacks than is 256-bit AES. Use '_' to use default key size\n-# for cipher algorithms (where it makes sense because the algorithm supports\n-# a variable key size). Key length must agree to what's provided as the\n-# cipher transformation, otherwise this will be ignored after logging a\n-# warning.\n-#\n-# NOTE: This is what applies BOTH ESAPI 1.4 and 2.0. See warning above about mixing!\n-Encryptor.EncryptionKeyLength=128\n-\n-# Because 2.0 uses CBC mode by default, it requires an initialization vector (IV).\n-# (All cipher modes except ECB require an IV.) There are two choices: we can either\n-# use a fixed IV known to both parties or allow ESAPI to choose a random IV. While\n-# the IV does not need to be hidden from adversaries, it is important that the\n-# adversary not be allowed to choose it. Also, random IVs are generally much more\n-# secure than fixed IVs. (In fact, it is essential that feed-back cipher modes\n-# such as CFB and OFB use a different IV for each encryption with a given key so\n-# in such cases, random IVs are much preferred. By default, ESAPI 2.0 uses random\n-# IVs. If you wish to use 'fixed' IVs, set 'Encryptor.ChooseIVMethod=fixed' and\n-# uncomment the Encryptor.fixedIV.\n-#\n-# Valid values:        random|fixed|specified        'specified' not yet implemented; planned for 2.1\n-Encryptor.ChooseIVMethod=random\n-# If you choose to use a fixed IV, then you must place a fixed IV here that\n-# is known to all others who are sharing your secret key. The format should\n-# be a hex string that is the same length as the cipher block size for the\n-# cipher algorithm that you are using. The following is an example for AES\n-# from an AES test vector for AES-128/CBC as described in:\n-# NIST Special Publication 800-38A (2001 Edition)\n-# \"Recommendation for Block Cipher Modes of Operation\".\n-# (Note that the block size for AES is 16 bytes == 128 bits.)\n-#\n-Encryptor.fixedIV=0x000102030405060708090a0b0c0d0e0f\n-\n-# Whether or not CipherText should use a message authentication code (MAC) with it.\n-# This prevents an adversary from altering the IV as well as allowing a more\n-# fool-proof way of determining the decryption failed because of an incorrect\n-# key being supplied. This refers to the \"separate\" MAC calculated and stored\n-# in CipherText, not part of any MAC that is calculated as a result of a\n-# \"combined mode\" cipher mode.\n-#\n-# If you are using ESAPI with a FIPS 140-2 cryptographic module, you *must* also\n-# set this property to false.\n-Encryptor.CipherText.useMAC=true\n-\n-# Whether or not the PlainText object may be overwritten and then marked\n-# eligible for garbage collection. If not set, this is still treated as 'true'.\n-Encryptor.PlainText.overwrite=true\n-\n-# Do not use DES except in a legacy situations. 56-bit is way too small key size.\n-#Encryptor.EncryptionKeyLength=56\n-#Encryptor.EncryptionAlgorithm=DES\n-\n-# TripleDES is considered strong enough for most purposes.\n-#    Note:    There is also a 112-bit version of DESede. Using the 168-bit version\n-#            requires downloading the special jurisdiction policy from Sun.\n-#Encryptor.EncryptionKeyLength=168\n-#Encryptor.EncryptionAlgorithm=DESede\n-\n-Encryptor.HashAlgorithm=SHA-512\n-Encryptor.HashIterations=1024\n-Encryptor.DigitalSignatureAlgorithm=SHA1withDSA\n-Encryptor.DigitalSignatureKeyLength=1024\n-Encryptor.RandomAlgorithm=SHA1PRNG\n-Encryptor.CharacterEncoding=UTF-8\n-\n-\n-#===========================================================================\n-# ESAPI HttpUtilties\n-#\n-# The HttpUtilities provide basic protections to HTTP requests and responses. Primarily these methods\n-# protect against malicious data from attackers, such as unprintable characters, escaped characters,\n-# and other simple attacks. The HttpUtilities also provides utility methods for dealing with cookies,\n-# headers, and CSRF tokens.\n-#\n-# Default file upload location (remember to escape backslashes with \\\\)\n-HttpUtilities.UploadDir=C:\\\\ESAPI\\\\testUpload\n-HttpUtilities.UploadTempDir=C:\\\\temp\n-# Force flags on cookies, if you use HttpUtilities to set cookies\n-HttpUtilities.ForceHttpOnlySession=false\n-HttpUtilities.ForceSecureSession=false\n-HttpUtilities.ForceHttpOnlyCookies=true\n-HttpUtilities.ForceSecureCookies=true\n-# Maximum size of HTTP headers\n-HttpUtilities.MaxHeaderSize=4096\n-# File upload configuration\n-HttpUtilities.ApprovedUploadExtensions=.zip,.pdf,.doc,.docx,.ppt,.pptx,.tar,.gz,.tgz,.rar,.war,.jar,.ear,.xls,.rtf,.properties,.java,.class,.txt,.xml,.jsp,.jsf,.exe,.dll\n-HttpUtilities.MaxUploadFileBytes=500000000\n-# Using UTF-8 throughout your stack is highly recommended. That includes your database driver,\n-# container, and any other technologies you may be using. Failure to do this may expose you\n-# to Unicode transcoding injection attacks. Use of UTF-8 does not hinder internationalization.\n-HttpUtilities.ResponseContentType=text/html; charset=UTF-8\n-\n-\n-\n-#===========================================================================\n-# ESAPI Executor\n-# CHECKME - Not sure what this is used for, but surely it should be made OS independent.\n-Executor.WorkingDirectory=C:\\\\Windows\\\\Temp\n-Executor.ApprovedExecutables=C:\\\\Windows\\\\System32\\\\cmd.exe,C:\\\\Windows\\\\System32\\\\runas.exe\n-\n-\n-#===========================================================================\n-# ESAPI Logging\n-# Set the application name if these logs are combined with other applications\n-Logger.ApplicationName=ExampleApplication\n-# If you use an HTML log viewer that does not properly HTML escape log data, you can set LogEncodingRequired to true\n-Logger.LogEncodingRequired=false\n-# Determines whether ESAPI should log the application name. This might be clutter in some single-server/single-app environments.\n-Logger.LogApplicationName=true\n-# Determines whether ESAPI should log the server IP and port. This might be clutter in some single-server environments.\n-Logger.LogServerIP=true\n-# LogFileName, the name of the logging file. Provide a full directory path (e.g., C:\\\\ESAPI\\\\ESAPI_logging_file) if you\n-# want to place it in a specific directory.\n-Logger.LogFileName=ESAPI_logging_file\n-# MaxLogFileSize, the max size (in bytes) of a single log file before it cuts over to a new one (default is 10,000,000)\n-Logger.MaxLogFileSize=10000000\n-\n-\n-#===========================================================================\n-# ESAPI Intrusion Detection\n-#\n-# Each event has a base to which .count, .interval, and .action are added\n-# The IntrusionException will fire if we receive \"count\" events within \"interval\" seconds\n-# The IntrusionDetector is configurable to take the following actions: log, logout, and disable\n-#  (multiple actions separated by commas are allowed e.g. event.test.actions=log,disable\n-#\n-# Custom Events\n-# Names must start with \"event.\" as the base\n-# Use IntrusionDetector.addEvent( \"test\" ) in your code to trigger \"event.test\" here\n-# You can also disable intrusion detection completely by changing\n-# the following parameter to true\n-#\n-IntrusionDetector.Disable=false\n-#\n-IntrusionDetector.event.test.count=2\n-IntrusionDetector.event.test.interval=10\n-IntrusionDetector.event.test.actions=disable,log\n-\n-# Exception Events\n-# All EnterpriseSecurityExceptions are registered automatically\n-# Call IntrusionDetector.getInstance().addException(e) for Exceptions that do not extend EnterpriseSecurityException\n-# Use the fully qualified classname of the exception as the base\n-\n-# any intrusion is an attack\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.count=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.interval=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.actions=log,disable,logout\n-\n-# for test purposes\n-# CHECKME: Shouldn't there be something in the property name itself that designates\n-#           that these are for testing???\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.count=10\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.interval=5\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.actions=log,disable,logout\n-\n-# rapid validation errors indicate scans or attacks in progress\n-# org.owasp.esapi.errors.ValidationException.count=10\n-# org.owasp.esapi.errors.ValidationException.interval=10\n-# org.owasp.esapi.errors.ValidationException.actions=log,logout\n-\n-# sessions jumping between hosts indicates session hijacking\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.count=2\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.interval=10\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.actions=log,logout\n-\n-\n-#===========================================================================\n-# ESAPI Validation\n-#\n-# The ESAPI Validator works on regular expressions with defined names. You can define names\n-# either here, or you may define application specific patterns in a separate file defined below.\n-# This allows enterprises to specify both organizational standards as well as application specific\n-# validation rules.\n-#\n-Validator.ConfigurationFile=validation.properties\n-\n-# Validators used by ESAPI\n-Validator.AccountName=^[a-zA-Z0-9]{3,20}$\n-Validator.SystemCommand=^[a-zA-Z\\\\-\\\\/]{1,64}$\n-Validator.RoleName=^[a-z]{1,20}$\n-\n-#the word TEST below should be changed to your application\n-#name - only relative URL's are supported\n-Validator.Redirect=^\\\\/test.*$\n-\n-# Global HTTP Validation Rules\n-# Values with Base64 encoded data (e.g. encrypted state) will need at least [a-zA-Z0-9\\/+=]\n-Validator.HTTPScheme=^(http|https)$\n-Validator.HTTPServerName=^[a-zA-Z0-9_.\\\\-]*$\n-Validator.HTTPParameterName=^[a-zA-Z0-9_]{1,32}$\n-Validator.HTTPParameterValue=^[a-zA-Z0-9.\\\\-\\\\/+=_ ]*$\n-Validator.HTTPCookieName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPCookieValue=^[a-zA-Z0-9\\\\-\\\\/+=_ ]*$\n-Validator.HTTPHeaderName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPHeaderValue=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPContextPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPServletPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPPath=^[a-zA-Z0-9.\\\\-_]*$\n-Validator.HTTPQueryString=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ %]*$\n-Validator.HTTPURI=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPURL=^.*$\n-Validator.HTTPJSESSIONID=^[A-Z0-9]{10,30}$\n-\n-# Validation of file related input\n-Validator.FileName=^[a-zA-Z0-9!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$\n-Validator.DirectoryName=^[a-zA-Z0-9:/\\\\\\\\!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$",
                "raw_url": "https://github.com/apache/hbase/raw/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/test/resources/ESAPI.properties",
                "sha": "907400189d8dd33f34b03469af8fc1f250844cf8",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/pom.xml",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/pom.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -874,7 +874,6 @@\n               <exclude>**/patchprocess/**</exclude>\n               <exclude>src/main/site/resources/repo/**</exclude>\n               <exclude>**/dependency-reduced-pom.xml</exclude>\n-              <exclude>**/ESAPI.properties</exclude>\n               <exclude>**/rat.txt</exclude>\n             </excludes>\n           </configuration>",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/pom.xml",
                "sha": "b116ae49c986a533bb742511fbc8d9430d46bdbb",
                "status": "modified"
            }
        ],
        "message": "HBASE-16317 revert all ESAPI changes\n\nRevert \"HBASE-15270 Use appropriate encoding for \"filter\" field in TaskMonitorTmpl.jamon.\"\n\nThis reverts commit bba4f107c19b92eb51c7772eddb408397bea3002.\n\nRevert \"HBASE-15122 Servlets generate XSS_REQUEST_PARAMETER_TO_SERVLET_WRITER findbugs warnings (Samir Ahmic)\"\n\nThis reverts commit 68b300173f82b2b3ae06da1ec303a8f6f072c414.\n\n Conflicts:\n\thbase-server/pom.xml\n\nRevert \"HBASE-15329 Cross-Site Scripting: Reflected in table.jsp (Samir Ahmic)\"\n\nThis reverts commit 4b3e38705cb24aee82615b1b9af47ed549ea1358.\n\n Conflicts:\n\thbase-server/src/main/resources/hbase-webapps/master/table.jsp\n\nRevert \"HBASE-15369 Handle NPE in region.jsp (Samir Ahmic)\"\n\nThis reverts commit 3826894f890a850270053a25b53f07a007555711.",
        "parent": "https://github.com/apache/hbase/commit/81b06b3bdd56c1c434406fe3e3aea7e47948f958",
        "patched_files": [
            "supplemental-models.java",
            "JMXJsonServlet.java",
            "TaskMonitorTmpl.java",
            "table.java",
            "pom.java",
            "ESAPI.java",
            "region.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestJMXJsonServlet.java"
        ]
    },
    "hbase_20a6c00": {
        "bug_id": "hbase_20a6c00",
        "commit": "https://github.com/apache/hbase/commit/20a6c00b5cafd152c84a5d35ab66420805d75e15",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 1,
                "filename": "CHANGES.txt",
                "patch": "@@ -47,7 +47,7 @@ Trunk (unreleased changes)\n     HADOOP-1847 Many HBase tests do not fail well. (phase 2)\n     HADOOP-1870 Once file system failure has been detected, don't check it again\n                 and get on with shutting down the hbase cluster.\n-    HADOOP-1888 NullPointerException in HMemcacheScanner\n+    HADOOP-1888 NullPointerException in HMemcacheScanner (reprise)\n     HADOOP-1903 Possible data loss if Exception happens between snapshot and flush\n                 to disk.\n ",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/CHANGES.txt",
                "sha": "cf5f737b16174b200a3066b83790b8304d5a0fb1",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 41,
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -43,10 +44,10 @@\n   // Note that since these structures are always accessed with a lock held,\n   // no additional synchronization is required.\n   \n-  TreeMap<HStoreKey, byte []> memcache = new TreeMap<HStoreKey, byte []>();\n-  final ArrayList<TreeMap<HStoreKey, byte []>> history =\n-    new ArrayList<TreeMap<HStoreKey, byte []>>();\n-  TreeMap<HStoreKey, byte []> snapshot = null;\n+  volatile SortedMap<HStoreKey, byte []> memcache;\n+  List<SortedMap<HStoreKey, byte []>> history =\n+    Collections.synchronizedList(new ArrayList<SortedMap<HStoreKey, byte []>>());\n+  volatile SortedMap<HStoreKey, byte []> snapshot = null;\n \n   final HLocking lock = new HLocking();\n   \n@@ -62,14 +63,16 @@\n    */\n   public HMemcache() {\n     super();\n+    memcache  =\n+      Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>());\n   }\n \n   /** represents the state of the memcache at a specified point in time */\n   static class Snapshot {\n-    final TreeMap<HStoreKey, byte []> memcacheSnapshot;\n+    final SortedMap<HStoreKey, byte []> memcacheSnapshot;\n     final long sequenceId;\n     \n-    Snapshot(final TreeMap<HStoreKey, byte[]> memcache, final Long i) {\n+    Snapshot(final SortedMap<HStoreKey, byte[]> memcache, final Long i) {\n       super();\n       this.memcacheSnapshot = memcache;\n       this.sequenceId = i.longValue();\n@@ -103,8 +106,11 @@ Snapshot snapshotMemcacheForLog(HLog log) throws IOException {\n         new Snapshot(memcache, Long.valueOf(log.startCacheFlush()));\n       // From here on, any failure is catastrophic requiring replay of hlog\n       this.snapshot = memcache;\n-      history.add(memcache);\n-      memcache = new TreeMap<HStoreKey, byte []>();\n+      synchronized (history) {\n+        history.add(memcache);\n+      }\n+      memcache =\n+        Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>());\n       // Reset size of this memcache.\n       this.size.set(0);\n       return retval;\n@@ -126,14 +132,8 @@ public void deleteSnapshot() throws IOException {\n       if(snapshot == null) {\n         throw new IOException(\"Snapshot not present!\");\n       }\n-      for (Iterator<TreeMap<HStoreKey, byte []>> it = history.iterator(); \n-        it.hasNext(); ) {\n-        \n-        TreeMap<HStoreKey, byte []> cur = it.next();\n-        if (snapshot == cur) {\n-          it.remove();\n-          break;\n-        }\n+      synchronized (history) {\n+        history.remove(snapshot);\n       }\n       this.snapshot = null;\n     } finally {\n@@ -182,12 +182,14 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       ArrayList<byte []> results = get(memcache, key, numVersions);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        if (numVersions > 0 && results.size() >= numVersions) {\n-          break;\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          if (numVersions > 0 && results.size() >= numVersions) {\n+            break;\n+          }\n+          results.addAll(results.size(),\n+              get(history.get(i), key, numVersions - results.size()));\n         }\n-        results.addAll(results.size(),\n-            get(history.get(i), key, numVersions - results.size()));\n       }\n       return (results.size() == 0) ? null :\n         ImmutableBytesWritable.toArray(results);\n@@ -210,9 +212,11 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       internalGetFull(memcache, key, results);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        TreeMap<HStoreKey, byte []> cur = history.get(i);\n-        internalGetFull(cur, key, results);\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          SortedMap<HStoreKey, byte []> cur = history.get(i);\n+          internalGetFull(cur, key, results);\n+        }\n       }\n       return results;\n       \n@@ -221,7 +225,7 @@ public long getSize() {\n     }\n   }\n   \n-  void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key, \n+  void internalGetFull(SortedMap<HStoreKey, byte []> map, HStoreKey key, \n       TreeMap<Text, byte []> results) {\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(key);\n     for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n@@ -252,7 +256,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    * @return Ordered list of items found in passed <code>map</code>.  If no\n    * matching values, returns an empty list (does not return null).\n    */\n-  ArrayList<byte []> get(final TreeMap<HStoreKey, byte []> map,\n+  ArrayList<byte []> get(final SortedMap<HStoreKey, byte []> map,\n       final HStoreKey key, final int numVersions) {\n     ArrayList<byte []> result = new ArrayList<byte []>();\n     // TODO: If get is of a particular version -- numVersions == 1 -- we\n@@ -289,10 +293,12 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n     this.lock.obtainReadLock();\n     try {\n       List<HStoreKey> results = getKeys(this.memcache, origin, versions);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        results.addAll(results.size(), getKeys(history.get(i), origin,\n-            versions == HConstants.ALL_VERSIONS ? versions :\n-              (versions - results.size())));\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          results.addAll(results.size(), getKeys(history.get(i), origin,\n+              versions == HConstants.ALL_VERSIONS ? versions :\n+                (versions - results.size())));\n+        }\n       }\n       return results;\n     } finally {\n@@ -308,7 +314,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    * equal or older timestamp.  If no keys, returns an empty List. Does not\n    * return null.\n    */\n-  private List<HStoreKey> getKeys(final TreeMap<HStoreKey, byte []> map,\n+  private List<HStoreKey> getKeys(final SortedMap<HStoreKey, byte []> map,\n       final HStoreKey origin, final int versions) {\n     List<HStoreKey> result = new ArrayList<HStoreKey>();\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(origin);\n@@ -360,7 +366,7 @@ HInternalScannerInterface getScanner(long timestamp,\n   //////////////////////////////////////////////////////////////////////////////\n \n   class HMemcacheScanner extends HAbstractScanner {\n-    final TreeMap<HStoreKey, byte []> backingMaps[];\n+    SortedMap<HStoreKey, byte []> backingMaps[];\n     final Iterator<HStoreKey> keyIterators[];\n \n     @SuppressWarnings(\"unchecked\")\n@@ -370,14 +376,16 @@ HInternalScannerInterface getScanner(long timestamp,\n       super(timestamp, targetCols);\n       lock.obtainReadLock();\n       try {\n-        this.backingMaps = new TreeMap[history.size() + 1];\n+        synchronized (history) {\n+          this.backingMaps = new SortedMap[history.size() + 1];\n \n-        // Note that since we iterate through the backing maps from 0 to n, we\n-        // need to put the memcache first, the newest history second, ..., etc.\n+          // Note that since we iterate through the backing maps from 0 to n, we\n+          // need to put the memcache first, the newest history second, ..., etc.\n \n-        backingMaps[0] = memcache;\n-        for (int i = history.size() - 1; i > 0; i--) {\n-          backingMaps[i + 1] = history.get(i);\n+          backingMaps[0] = memcache;\n+          for (int i = history.size() - 1; i >= 0; i--) {\n+            backingMaps[i + 1] = history.get(i);\n+          }\n         }\n \n         this.keyIterators = new Iterator[backingMaps.length];\n@@ -388,9 +396,13 @@ HInternalScannerInterface getScanner(long timestamp,\n         \n         HStoreKey firstKey = new HStoreKey(firstRow);\n         for (int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = firstRow.getLength() != 0 ?\n-              backingMaps[i].tailMap(firstKey).keySet().iterator() :\n-                backingMaps[i].keySet().iterator();\n+          if (firstRow != null && firstRow.getLength() != 0) {\n+            keyIterators[i] =\n+              backingMaps[i].tailMap(firstKey).keySet().iterator();\n+            \n+          } else {\n+            keyIterators[i] = backingMaps[i].keySet().iterator();\n+          }\n \n           while (getNext(i)) {\n             if (!findFirstRow(i, firstRow)) {",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "sha": "852582733d737031bd820e79d119adf0fc8bcddc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -1615,6 +1615,7 @@ public boolean isMultipleMatchScanner() {\n       return multipleMatchers;\n     }\n \n+    /** {@inheritDoc} */\n     public boolean next(HStoreKey key, SortedMap<Text, byte[]> results)\n     throws IOException {\n       // Filtered flag is set by filters.  If a cell has been 'filtered out'",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "69911669920ecb8added94603be0e31ff5924035",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 17,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n+import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.Vector;\n import java.util.Map.Entry;\n@@ -439,13 +440,13 @@ private void flushBloomFilter() throws IOException {\n    * @param logCacheFlushId flush sequence number\n    * @throws IOException\n    */\n-  void flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCache(final SortedMap<HStoreKey, byte []> inputCache,\n     final long logCacheFlushId)\n   throws IOException {\n     flushCacheHelper(inputCache, logCacheFlushId, true);\n   }\n   \n-  void flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCacheHelper(SortedMap<HStoreKey, byte []> inputCache,\n       long logCacheFlushId, boolean addToAvailableMaps)\n   throws IOException {\n     synchronized(flushLock) {\n@@ -1123,7 +1124,7 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n    * @param key\n    * @param numVersions Number of versions to fetch.  Must be > 0.\n    * @param memcache Checked for deletions\n-   * @return\n+   * @return values for the specified versions\n    * @throws IOException\n    */\n   byte [][] get(HStoreKey key, int numVersions, final HMemcache memcache)\n@@ -1171,10 +1172,11 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n               break;\n             }\n           }\n-          while ((readval = new ImmutableBytesWritable()) != null &&\n+          for (readval = new ImmutableBytesWritable();\n               map.next(readkey, readval) &&\n               readkey.matchesRowCol(key) &&\n-              !hasEnoughVersions(numVersions, results)) {\n+              !hasEnoughVersions(numVersions, results);\n+              readval = new ImmutableBytesWritable()) {\n             if (!isDeleted(readkey, readval.get(), memcache, deletes)) {\n               results.add(readval.get());\n             }\n@@ -1212,10 +1214,11 @@ private boolean hasEnoughVersions(final int numVersions,\n    * @throws IOException\n    */\n   List<HStoreKey> getKeys(final HStoreKey origin, List<HStoreKey> allKeys,\n-      final int versions)\n-  throws IOException {\n-    if (allKeys == null) {\n-      allKeys = new ArrayList<HStoreKey>();\n+      final int versions) throws IOException {\n+    \n+    List<HStoreKey> keys = allKeys;\n+    if (keys == null) {\n+      keys = new ArrayList<HStoreKey>();\n     }\n     // This code below is very close to the body of the get method.\n     this.lock.obtainReadLock();\n@@ -1238,23 +1241,24 @@ private boolean hasEnoughVersions(final int numVersions,\n             continue;\n           }\n           if (!isDeleted(readkey, readval.get(), null, null) &&\n-              !allKeys.contains(readkey)) {\n-            allKeys.add(new HStoreKey(readkey));\n+              !keys.contains(readkey)) {\n+            keys.add(new HStoreKey(readkey));\n           }\n-          while ((readval = new ImmutableBytesWritable()) != null &&\n+          for (readval = new ImmutableBytesWritable();\n               map.next(readkey, readval) &&\n-              readkey.matchesRowCol(origin)) {\n+              readkey.matchesRowCol(origin);\n+              readval = new ImmutableBytesWritable()) {\n             if (!isDeleted(readkey, readval.get(), null, null) &&\n-                !allKeys.contains(readkey)) {\n-              allKeys.add(new HStoreKey(readkey));\n-              if (versions != ALL_VERSIONS && allKeys.size() >= versions) {\n+                !keys.contains(readkey)) {\n+              keys.add(new HStoreKey(readkey));\n+              if (versions != ALL_VERSIONS && keys.size() >= versions) {\n                 break;\n               }\n             }\n           }\n         }\n       }\n-      return allKeys;\n+      return keys;\n     } finally {\n       this.lock.releaseReadLock();\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "cd88bf4e9d49ae6c42c2a88c2d6cdf1535e7b731",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 15,
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "patch": "@@ -50,7 +50,8 @@\n   private FileSystem fs;\n   private Path parentdir;\n   private MasterThread masterThread = null;\n-  ArrayList<RegionServerThread> regionThreads;\n+  ArrayList<RegionServerThread> regionThreads =\n+    new ArrayList<RegionServerThread>();\n   private boolean deleteOnExit = true;\n \n   /**\n@@ -125,7 +126,7 @@ private void init(final int nRegionNodes) throws IOException {\n       this.parentdir = new Path(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR));\n       fs.mkdirs(parentdir);\n       this.masterThread = startMaster(this.conf);\n-      this.regionThreads = startRegionServers(this.conf, nRegionNodes);\n+      this.regionThreads.addAll(startRegionServers(this.conf, nRegionNodes));\n     } catch(IOException e) {\n       shutdown();\n       throw e;\n@@ -357,17 +358,15 @@ public static void shutdown(final MasterThread masterThread,\n     if(masterThread != null) {\n       masterThread.getMaster().shutdown();\n     }\n-    if (regionServerThreads != null) {\n-      synchronized(regionServerThreads) {\n-        if (regionServerThreads != null) {\n-          for(Thread t: regionServerThreads) {\n-            if (t.isAlive()) {\n-              try {\n-                t.join();\n-              } catch (InterruptedException e) {\n-                // continue\n-              }\n-            }\n+    // regionServerThreads can never be null because they are initialized when\n+    // the class is constructed.\n+    synchronized(regionServerThreads) {\n+      for(Thread t: regionServerThreads) {\n+        if (t.isAlive()) {\n+          try {\n+            t.join();\n+          } catch (InterruptedException e) {\n+            // continue\n           }\n         }\n       }\n@@ -381,8 +380,7 @@ public static void shutdown(final MasterThread masterThread,\n     }\n     LOG.info(\"Shutdown \" +\n       ((masterThread != null)? masterThread.getName(): \"0 masters\") + \" \" +\n-      ((regionServerThreads == null)? 0: regionServerThreads.size()) +\n-      \" region server(s)\");\n+      regionServerThreads.size() + \" region server(s)\");\n   }\n   \n   void shutdown() {",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "sha": "093d3594eb0f76953c038cbde3ee7863298367cf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHMemcache.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n import java.util.Map;\n+import java.util.SortedMap;\n import java.util.TreeMap;\n \n import junit.framework.TestCase;\n@@ -97,7 +98,7 @@ private Snapshot runSnapshot(final HMemcache hmc, final HLog log)\n     \n     // Save off old state.\n     int oldHistorySize = hmc.history.size();\n-    TreeMap<HStoreKey, byte []> oldMemcache = hmc.memcache;\n+    SortedMap<HStoreKey, byte []> oldMemcache = hmc.memcache;\n     // Run snapshot.\n     Snapshot s = hmc.snapshotMemcacheForLog(log);\n     // Make some assertions about what just happened.",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "sha": "68c8338ef81fa13578e93c1cc3244cd445e4c27c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1888 NullPointerException in HMemcacheScanner (reprise)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@575986 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/b271048e2f86b9d186be6b71c504c0dda92c0966",
        "patched_files": [
            "HRegion.java",
            "CHANGES.java",
            "MiniHBaseCluster.java",
            "HMemcache.java",
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java",
            "TestHRegion.java",
            "TestHMemcache.java"
        ]
    },
    "hbase_21ea0b5": {
        "bug_id": "hbase_21ea0b5",
        "commit": "https://github.com/apache/hbase/commit/21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "deletions": 9,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -1159,23 +1159,27 @@ private void waitOnAllRegionsToClose(final boolean abort) {\n   }\n \n   private void closeWAL(final boolean delete) {\n-    try {\n-      if (this.hlogForMeta != null) {\n-        //All hlogs (meta and non-meta) are in the same directory. Don't call \n-        //closeAndDelete here since that would delete all hlogs not just the \n-        //meta ones. We will just 'close' the hlog for meta here, and leave\n-        //the directory cleanup to the follow-on closeAndDelete call.\n+    if (this.hlogForMeta != null) {\n+      // All hlogs (meta and non-meta) are in the same directory. Don't call\n+      // closeAndDelete here since that would delete all hlogs not just the\n+      // meta ones. We will just 'close' the hlog for meta here, and leave\n+      // the directory cleanup to the follow-on closeAndDelete call.\n+      try {\n         this.hlogForMeta.close();\n+      } catch (Throwable e) {\n+        LOG.error(\"Metalog close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n       }\n-      if (this.hlog != null) {\n+    }\n+    if (this.hlog != null) {\n+      try {\n         if (delete) {\n           hlog.closeAndDelete();\n         } else {\n           hlog.close();\n         }\n+      } catch (Throwable e) {\n+        LOG.error(\"Close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n       }\n-    } catch (Throwable e) {\n-      LOG.error(\"Close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "4a8e5bf47c24f4e15591a8928f590be9046d33ad",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "deletions": 10,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "patch": "@@ -193,9 +193,9 @@ public HLogSplitter(Configuration conf, Path rootDir, Path srcDir,\n \n     status = TaskMonitor.get().createStatus(\n         \"Splitting logs in \" + srcDir);\n-    \n+\n     long startTime = EnvironmentEdgeManager.currentTimeMillis();\n-    \n+\n     status.setStatus(\"Determining files to split...\");\n     List<Path> splits = null;\n     if (!fs.exists(srcDir)) {\n@@ -219,7 +219,7 @@ public HLogSplitter(Configuration conf, Path rootDir, Path srcDir,\n     LOG.info(msg);\n     return splits;\n   }\n-  \n+\n   private void logAndReport(String msg) {\n     status.setStatus(msg);\n     LOG.info(msg);\n@@ -321,7 +321,7 @@ void setDistributedLogSplittingHelper(DistributedLogSplittingHelper helper) {\n         }\n       }\n       status.setStatus(\"Log splits complete. Checking for orphaned logs.\");\n-      \n+\n       if (fs.listStatus(srcDir).length > processedLogs.size()\n           + corruptedLogs.size()) {\n         throw new OrphanHLogAfterSplitException(\n@@ -511,7 +511,12 @@ public static void finishSplitLogFile(Path rootdir, Path oldLogDir,\n     List<Path> corruptedLogs = new ArrayList<Path>();\n     FileSystem fs;\n     fs = rootdir.getFileSystem(conf);\n-    Path logPath = new Path(logfile);\n+    Path logPath = null;\n+    if (FSUtils.isStartingWithPath(rootdir, logfile)) {\n+      logPath = new Path(logfile);\n+    } else {\n+      logPath = new Path(rootdir, logfile);\n+    }\n     if (ZKSplitLog.isCorrupted(rootdir, logPath.getName(), fs)) {\n       corruptedLogs.add(logPath);\n     } else {\n@@ -842,7 +847,7 @@ void appendEntry(Entry entry) throws InterruptedException, IOException {\n           buffer = new RegionEntryBuffer(key.getTablename(), key.getEncodedRegionName());\n           buffers.put(key.getEncodedRegionName(), buffer);\n         }\n-        incrHeap= buffer.appendEntry(entry);        \n+        incrHeap= buffer.appendEntry(entry);\n       }\n \n       // If we crossed the chunk threshold, wait for more space to be available\n@@ -1092,7 +1097,7 @@ private boolean reportProgressIfIsDistributedLogSplitting() {\n \n   /**\n    * A class used in distributed log splitting\n-   * \n+   *\n    */\n   class DistributedLogSplittingHelper {\n     // Report progress, only used in distributed log splitting\n@@ -1143,7 +1148,7 @@ private boolean reportProgress() {\n         new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR));\n \n     private boolean closeAndCleanCompleted = false;\n-    \n+\n     private boolean logWritersClosed  = false;\n \n     private final int numThreads;\n@@ -1171,7 +1176,7 @@ synchronized void startWriterThreads() {\n     }\n \n     /**\n-     * \n+     *\n      * @return null if failed to report progress\n      * @throws IOException\n      */\n@@ -1303,7 +1308,7 @@ public Void call() throws Exception {\n       }\n       return paths;\n     }\n-    \n+\n     private List<IOException> closeLogWriters(List<IOException> thrown)\n         throws IOException {\n       if (!logWritersClosed) {",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "sha": "5ec4a65e9a19fc2fde2e44640aa09ce2f521825d",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "patch": "@@ -86,6 +86,61 @@ protected FSUtils() {\n     super();\n   }\n \n+  /**\n+   * Compare of path component. Does not consider schema; i.e. if schemas different but <code>path\n+   * <code> starts with <code>rootPath<code>, then the function returns true\n+   * @param rootPath\n+   * @param path \n+   * @return True if <code>path</code> starts with <code>rootPath</code>\n+   */\n+  public static boolean isStartingWithPath(final Path rootPath, final String path) {\n+    String uriRootPath = rootPath.toUri().getPath();\n+    String tailUriPath = (new Path(path)).toUri().getPath();\n+    return tailUriPath.startsWith(uriRootPath);\n+  }\n+\n+  /**\n+   * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the\n+   * '/a/b/c' part. Does not consider schema; i.e. if schemas different but path or subpath matches,\n+   * the two will equate.\n+   * @param pathToSearch Path we will be trying to match.\n+   * @param pathTail\n+   * @return True if <code>pathTail</code> is tail on the path of <code>pathToSearch</code>\n+   */\n+  public static boolean isMatchingTail(final Path pathToSearch, String pathTail) {\n+    return isMatchingTail(pathToSearch, new Path(pathTail));\n+  }\n+\n+  /**\n+   * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the\n+   * '/a/b/c' part. If you passed in 'hdfs://a/b/c and b/c, it would return true.  Does not consider\n+   * schema; i.e. if schemas different but path or subpath matches, the two will equate.\n+   * @param pathToSearch Path we will be trying to match.\n+   * @param pathTail\n+   * @return True if <code>pathTail</code> is tail on the path of <code>pathToSearch</code>\n+   */\n+  public static boolean isMatchingTail(final Path pathToSearch, final Path pathTail) {\n+    if (pathToSearch.depth() != pathTail.depth()) return false;\n+    Path tailPath = pathTail;\n+    String tailName;\n+    Path toSearch = pathToSearch;\n+    String toSearchName;\n+    boolean result = false;\n+    do {\n+      tailName = tailPath.getName();\n+      if (tailName == null || tailName.length() <= 0) {\n+        result = true;\n+        break;\n+      }\n+      toSearchName = toSearch.getName();\n+      if (toSearchName == null || toSearchName.length() <= 0) break;\n+      // Move up a parent on each path for next go around.  Path doesn't let us go off the end.\n+      tailPath = tailPath.getParent();\n+      toSearch = toSearch.getParent();\n+    } while(tailName.equals(toSearchName));\n+    return result;\n+  }\n+\n   public static FSUtils getInstance(FileSystem fs, Configuration conf) {\n     String scheme = fs.getUri().getScheme();\n     if (scheme == null) {",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "sha": "680def3e48fa131dd8bf2a4c789f0d70c81e061e",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "patch": "@@ -34,12 +34,12 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n-import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HDFSBlocksDistribution;\n import org.apache.hadoop.hbase.MediumTests;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n@@ -49,6 +49,33 @@\n  */\n @Category(MediumTests.class)\n public class TestFSUtils {\n+  /**\n+   * Test path compare and prefix checking.\n+   * @throws IOException \n+   */\n+  @Test\n+  public void testMatchingTail() throws IOException {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    final FileSystem fs = htu.getTestFileSystem();\n+    Path rootdir = htu.getDataTestDir();\n+    assertTrue(rootdir.depth() > 1);\n+    Path partPath = new Path(\"a\", \"b\");\n+    Path fullPath = new Path(rootdir, partPath);\n+    Path fullyQualifiedPath = fs.makeQualified(fullPath);\n+    assertFalse(FSUtils.isMatchingTail(fullPath, partPath));\n+    assertFalse(FSUtils.isMatchingTail(fullPath, partPath.toString()));\n+    assertTrue(FSUtils.isStartingWithPath(rootdir, fullPath.toString()));\n+    assertTrue(FSUtils.isStartingWithPath(fullyQualifiedPath, fullPath.toString()));\n+    assertFalse(FSUtils.isStartingWithPath(rootdir, partPath.toString()));\n+    assertFalse(FSUtils.isMatchingTail(fullyQualifiedPath, partPath));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fullPath));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fullPath.toString()));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fs.makeQualified(fullPath)));\n+    assertTrue(FSUtils.isStartingWithPath(rootdir, fullyQualifiedPath.toString()));\n+    assertFalse(FSUtils.isMatchingTail(fullPath, new Path(\"x\")));\n+    assertFalse(FSUtils.isMatchingTail(new Path(\"x\"), fullPath));\n+  }\n+\n   @Test\n   public void testVersion() throws DeserializationException, IOException {\n     HBaseTestingUtility htu = new HBaseTestingUtility();",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "sha": "e2d3c3e3da71b8d883cfc3e8f36bb247b052a890",
                "status": "modified"
            }
        ],
        "message": "HBASE-7982 TestReplicationQueueFailover* runs for a minute, spews 3/4million lines complaining 'Filesystem closed', has an NPE, and still passes?\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1453712 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/a92c7f761247de70a5ac98c4026ab9fadbec479a",
        "patched_files": [
            "FSUtils.java",
            "HLogSplitter.java",
            "HRegionServer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFSUtils.java"
        ]
    },
    "hbase_221eb95": {
        "bug_id": "hbase_221eb95",
        "commit": "https://github.com/apache/hbase/commit/221eb9576839ecc976120197a4845a9a1e371d9c",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java?ref=221eb9576839ecc976120197a4845a9a1e371d9c",
                "deletions": 6,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "patch": "@@ -311,12 +311,14 @@ private void run() {\n       if (task == CLOSE) {\n         break;\n       }\n-      if (task == null && pendingRequests == 0) {\n-        LOG.debug(\n-          \"{} to {} no activities for {} ms, close active connection. \" +\n-            \"Will reconnect next time when there are new requests\",\n-          getId(), connectString, keepAliveTimeMs);\n-        closeZk();\n+      if (task == null) {\n+        if (pendingRequests == 0) {\n+          LOG.debug(\n+            \"{} to {} no activities for {} ms, close active connection. \" +\n+              \"Will reconnect next time when there are new requests\",\n+            getId(), connectString, keepAliveTimeMs);\n+          closeZk();\n+        }\n         continue;\n       }\n       if (!task.needZk()) {",
                "raw_url": "https://github.com/apache/hbase/raw/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "sha": "275fafbebad6e11840a0a7d8b75b8512c2e46166",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hbase/blob/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java?ref=221eb9576839ecc976120197a4845a9a1e371d9c",
                "deletions": 20,
                "filename": "hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "patch": "@@ -31,11 +31,15 @@\n import static org.mockito.ArgumentMatchers.anyBoolean;\n import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.doAnswer;\n-import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Exchanger;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ThreadLocalRandom;\n import org.apache.hadoop.conf.Configuration;\n@@ -45,20 +49,17 @@\n import org.apache.hadoop.hbase.Waiter.ExplainingPredicate;\n import org.apache.hadoop.hbase.testclassification.MediumTests;\n import org.apache.hadoop.hbase.testclassification.ZKTests;\n-import org.apache.zookeeper.AsyncCallback.StatCallback;\n+import org.apache.zookeeper.AsyncCallback;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.KeeperException.Code;\n import org.apache.zookeeper.ZooDefs;\n import org.apache.zookeeper.ZooKeeper;\n-import org.apache.zookeeper.data.Stat;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n-import org.mockito.invocation.InvocationOnMock;\n-import org.mockito.stubbing.Answer;\n \n @Category({ ZKTests.class, MediumTests.class })\n public class TestReadOnlyZKClient {\n@@ -165,25 +166,26 @@ public void testSessionExpire() throws Exception {\n \n   @Test\n   public void testNotCloseZkWhenPending() throws Exception {\n-    assertArrayEquals(DATA, RO_ZK.get(PATH).get());\n-    ZooKeeper mockedZK = spy(RO_ZK.zookeeper);\n-    CountDownLatch latch = new CountDownLatch(1);\n-    doAnswer(new Answer<Object>() {\n-\n-      @Override\n-      public Object answer(InvocationOnMock invocation) throws Throwable {\n-        latch.await();\n-        return invocation.callRealMethod();\n-      }\n-    }).when(mockedZK).exists(anyString(), anyBoolean(), any(StatCallback.class), any());\n+    ZooKeeper mockedZK = mock(ZooKeeper.class);\n+    Exchanger<AsyncCallback.DataCallback> exchanger = new Exchanger<>();\n+    doAnswer(i -> {\n+      exchanger.exchange(i.getArgument(2));\n+      return null;\n+    }).when(mockedZK).getData(anyString(), anyBoolean(),\n+      any(AsyncCallback.DataCallback.class), any());\n+    doAnswer(i -> null).when(mockedZK).close();\n+    when(mockedZK.getState()).thenReturn(ZooKeeper.States.CONNECTED);\n     RO_ZK.zookeeper = mockedZK;\n-    CompletableFuture<Stat> future = RO_ZK.exists(PATH);\n+    CompletableFuture<byte[]> future = RO_ZK.get(PATH);\n+    AsyncCallback.DataCallback callback = exchanger.exchange(null);\n     // 2 * keep alive time to ensure that we will not close the zk when there are pending requests\n     Thread.sleep(6000);\n     assertNotNull(RO_ZK.zookeeper);\n-    latch.countDown();\n-    assertEquals(CHILDREN, future.get().getNumChildren());\n+    verify(mockedZK, never()).close();\n+    callback.processResult(Code.OK.intValue(), PATH, null, DATA, null);\n+    assertArrayEquals(DATA, future.get());\n     // now we will close the idle connection.\n     waitForIdleConnectionClosed();\n+    verify(mockedZK, times(1)).close();\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "sha": "a97a7c617464913b019755eccb416e1b1c49a620",
                "status": "modified"
            }
        ],
        "message": "HBASE-19870 Fix the NPE in ReadOnlyZKClient#run\n\nSigned-off-by: Chia-Ping Tsai <chia7712@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/f9480a56c74e7f9f42035f6e88c24fed08710e3c",
        "patched_files": [
            "ReadOnlyZKClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReadOnlyZKClient.java"
        ]
    },
    "hbase_231e0c0": {
        "bug_id": "hbase_231e0c0",
        "commit": "https://github.com/apache/hbase/commit/231e0c0043ddbe5297cb8f0716e6de5a7381892d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -362,6 +362,7 @@ Release 0.20.0 - Unreleased\n                hudson too\n    HBASE-1780  HTable.flushCommits clears write buffer in finally clause\n    HBASE-1784  Missing rows after medium intensity insert\n+   HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "sha": "b057b966b1a05ead1000a665159ecb2c3c7d3625",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "deletions": 12,
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "patch": "@@ -967,18 +967,26 @@ ByteBuffer readBlock(int block, boolean cacheBlock) throws IOException {\n     private ByteBuffer decompress(final long offset, final int compressedSize,\n       final int decompressedSize) \n     throws IOException {\n-      Decompressor decompressor = this.compressAlgo.getDecompressor();\n-      // My guess is that the bounded range fis is needed to stop the \n-      // decompressor reading into next block -- IIRC, it just grabs a\n-      // bunch of data w/o regard to whether decompressor is coming to end of a\n-      // decompression.\n-      InputStream is = this.compressAlgo.createDecompressionStream(\n-        new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n-        decompressor, 0);\n-      ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n-      IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n-      is.close();\n-      this.compressAlgo.returnDecompressor(decompressor);\n+      \n+      Decompressor decompressor = null;\n+      \n+      try {\n+        decompressor = this.compressAlgo.getDecompressor();\n+        // My guess is that the bounded range fis is needed to stop the \n+        // decompressor reading into next block -- IIRC, it just grabs a\n+        // bunch of data w/o regard to whether decompressor is coming to end of a\n+        // decompression.\n+        InputStream is = this.compressAlgo.createDecompressionStream(\n+          new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n+          decompressor, 0);\n+        ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n+        IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n+        is.close();        \n+      } finally {\n+        if (null != decompressor) {\n+          this.compressAlgo.returnDecompressor(decompressor);          \n+        }\n+      }\n       return buf;\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "sha": "f67fea35dc4ad04a89a1386536972f4d3303426b",
                "status": "modified"
            }
        ],
        "message": "HBASE-1809 NPE thrown in BoundedRangeFileInputStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@810301 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/dea3c1207cd0b60ddb3bbda3ae120d5efba97121",
        "patched_files": [
            "HFile.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHFile.java"
        ]
    },
    "hbase_261aa94": {
        "bug_id": "hbase_261aa94",
        "commit": "https://github.com/apache/hbase/commit/261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -43,6 +43,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n+import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -325,7 +326,8 @@ static boolean checkTable(Admin admin, TestOptions opts) throws IOException {\n     // recreate the table when user has requested presplit or when existing\n     // {RegionSplitPolicy,replica count} does not match requested.\n     if ((exists && opts.presplitRegions != DEFAULT_OPTS.presplitRegions)\n-      || (!isReadCmd && desc != null && !desc.getRegionSplitPolicyClassName().equals(opts.splitPolicy))\n+      || (!isReadCmd && desc != null &&\n+          !StringUtils.equals(desc.getRegionSplitPolicyClassName(), opts.splitPolicy))\n       || (!isReadCmd && desc != null && desc.getRegionReplication() != opts.replicas)) {\n       needsDelete = true;\n       // wait, why did it delete my table?!?",
                "raw_url": "https://github.com/apache/hbase/raw/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "40e50cfc60d91c7a21c660a0dcdd2af5ebdb254b",
                "status": "modified"
            }
        ],
        "message": "HBASE-17803 Addendum fix NPE",
        "parent": "https://github.com/apache/hbase/commit/23abc90068f0ea75f09c3eecf6ef758f1aee9219",
        "patched_files": [
            "PerformanceEvaluation.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_270eb98": {
        "bug_id": "hbase_270eb98",
        "commit": "https://github.com/apache/hbase/commit/270eb9886efd351d4346bb9de0f4fc3c17cf1910",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/270eb9886efd351d4346bb9de0f4fc3c17cf1910/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java?ref=270eb9886efd351d4346bb9de0f4fc3c17cf1910",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "patch": "@@ -237,7 +237,7 @@ private boolean isEqual(RegionLocations locs1, RegionLocations locs2) {\n   // which prevents us being added. The upper layer can use this value to complete pending requests.\n   private RegionLocations addToCache(TableCache tableCache, RegionLocations locs) {\n     LOG.trace(\"Try adding {} to cache\", locs);\n-    byte[] startKey = locs.getDefaultRegionLocation().getRegion().getStartKey();\n+    byte[] startKey = locs.getRegionLocation().getRegion().getStartKey();\n     for (;;) {\n       RegionLocations oldLocs = tableCache.cache.putIfAbsent(startKey, locs);\n       if (oldLocs == null) {",
                "raw_url": "https://github.com/apache/hbase/raw/270eb9886efd351d4346bb9de0f4fc3c17cf1910/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "sha": "d36ffea9b6e8f29348947f7c51fd47a452c87ff5",
                "status": "modified"
            }
        ],
        "message": "HBASE-23376 NPE happens while replica region is moving (#906)\n\nSigned-off-by: Duo Zhang <zhangduo@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/80ba354e2e05899d00da59c07817f73bbbb45585",
        "patched_files": [
            "AsyncNonMetaRegionLocator.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAsyncNonMetaRegionLocator.java"
        ]
    },
    "hbase_281c29f": {
        "bug_id": "hbase_281c29f",
        "commit": "https://github.com/apache/hbase/commit/281c29ff6070e08743ee96b0f7ce916bb1c3bba5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/281c29ff6070e08743ee96b0f7ce916bb1c3bba5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java?ref=281c29ff6070e08743ee96b0f7ce916bb1c3bba5",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "patch": "@@ -224,8 +224,8 @@ public void close() {\n       this.current.close();\n     }\n     if (this.heap != null) {\n-      KeyValueScanner scanner;\n-      while ((scanner = this.heap.poll()) != null) {\n+      // Order of closing the scanners shouldn't matter here, so simply iterate and close them.\n+      for (KeyValueScanner scanner : heap) {\n         scanner.close();\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/281c29ff6070e08743ee96b0f7ce916bb1c3bba5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "sha": "b0f42d7e1f31ec5dbf8b519c8abea96030fbc8f5",
                "status": "modified"
            }
        ],
        "message": "HBASE-20350 NullPointerException in Scanner during close()",
        "parent": "https://github.com/apache/hbase/commit/199b392ec8fa1137876e2251bca566b52e426a04",
        "patched_files": [
            "KeyValueHeap.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestKeyValueHeap.java"
        ]
    },
    "hbase_28a0350": {
        "bug_id": "hbase_28a0350",
        "commit": "https://github.com/apache/hbase/commit/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java?ref=28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -221,6 +222,24 @@ protected InternalScanner postCreateCoprocScanner(final CompactionRequest reques\n     return store.getCoprocessorHost().preCompact(store, scanner, scanType, request);\n   }\n \n+  /**\n+   * Used to prevent compaction name conflict when multiple compactions running parallel on the\n+   * same store.\n+   */\n+  private static final AtomicInteger NAME_COUNTER = new AtomicInteger(0);\n+\n+  private String generateCompactionName() {\n+    int counter;\n+    for (;;) {\n+      counter = NAME_COUNTER.get();\n+      int next = counter == Integer.MAX_VALUE ? 0 : counter + 1;\n+      if (NAME_COUNTER.compareAndSet(counter, next)) {\n+        break;\n+      }\n+    }\n+    return store.getRegionInfo().getRegionNameAsString() + \"#\"\n+        + store.getFamily().getNameAsString() + \"#\" + counter;\n+  }\n   /**\n    * Performs the compaction.\n    * @param scanner Where to read from.\n@@ -242,8 +261,7 @@ protected boolean performCompaction(InternalScanner scanner, CellSink writer,\n     if (LOG.isDebugEnabled()) {\n       lastMillis = EnvironmentEdgeManager.currentTime();\n     }\n-    String compactionName =\n-        store.getRegionInfo().getRegionNameAsString() + \"#\" + store.getFamily().getNameAsString();\n+    String compactionName = generateCompactionName();\n     long now = 0;\n     boolean hasMore;\n     ScannerContext scannerContext =",
                "raw_url": "https://github.com/apache/hbase/raw/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "sha": "78a5cac659d152c7a67801e6a083d4af3af7150f",
                "status": "modified"
            }
        ],
        "message": "HBASE-13970 NPE during compaction in trunk",
        "parent": "https://github.com/apache/hbase/commit/272b025b25fed979da0e59ffd41615bbb9e105ea",
        "patched_files": [
            "Compactor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCompactor.java"
        ]
    },
    "hbase_2a4f052": {
        "bug_id": "hbase_2a4f052",
        "commit": "https://github.com/apache/hbase/commit/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -93,6 +93,8 @@ Release 0.91.0 - Unreleased\n                cluster and was returning master hostname for rs to use\n    HBASE-3829  TestMasterFailover failures in jenkins\n    HBASE-3843  splitLogWorker starts too early (Prakash Khemani)\n+   HBASE-3838  RegionCoprocesorHost.preWALRestore throws npe in case there is\n+               no RegionObserver registered (Himanshu Vashishtha)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "sha": "331392dc81ed4ddcb4156c11e3af4456162839ce",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "deletions": 7,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "patch": "@@ -920,11 +920,12 @@ public boolean preWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).preWALRestore(ctx, info, logKey,\n             logEdit);\n+        bypass |= ctx.shouldBypass();\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      bypass |= ctx.shouldBypass();\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+     \n     }\n     return bypass;\n   }\n@@ -943,10 +944,11 @@ public void postWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).postWALRestore(ctx, info,\n             logKey, logEdit);\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+      \n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "sha": "ac76db421126e8a49d1aa476b368d9e4f5bcbe78",
                "status": "modified"
            }
        ],
        "message": "HBASE-3838 RegionCoprocesorHost.preWALRestore throws npe in case there is no RegionObserver registered\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1098705 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6c32f36250df2538598fcb7c8f99a615a8e4846d",
        "patched_files": [
            "RegionCoprocessorHost.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRegionCoprocessorHost.java"
        ]
    },
    "hbase_2b981ab": {
        "bug_id": "hbase_2b981ab",
        "commit": "https://github.com/apache/hbase/commit/2b981abcbbd550f432fcf368e5a703cde8d614d6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -335,6 +335,8 @@ Release 0.92.0 - Unreleased\n    HBASE-4531  hbase-4454 failsafe broke mvn site; back it out or fix\n                (Akash Ashok)\n    HBASE-4334  HRegion.get never validates row (Lars Hofhansl)\n+   HBASE-4494  AvroServer:: get fails with NPE on a non-existent row\n+               (Kay Kay)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/CHANGES.txt",
                "sha": "3e1d4d4a9a4e4a7547da9cb9aa8da1ba290d8749",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "patch": "@@ -302,7 +302,8 @@ static public Get agetToGet(AGet aget) throws IOException {\n   // TODO(hammer): Pick one: Timestamp or TimeStamp\n   static public AResult resultToAResult(Result result) {\n     AResult aresult = new AResult();\n-    aresult.row = ByteBuffer.wrap(result.getRow());\n+    byte[] row = result.getRow();\n+    aresult.row = ByteBuffer.wrap(row != null ? row : new byte[1]);\n     Schema s = Schema.createArray(AResultEntry.SCHEMA$);\n     GenericData.Array<AResultEntry> entries = null;\n     List<KeyValue> resultKeyValues = result.list();",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "sha": "abd2ae6ace7f2a789034acc6adaaebaef4fa5004",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "patch": "@@ -0,0 +1,40 @@\n+/** \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.avro;\n+\n+\n+import org.apache.hadoop.hbase.avro.generated.AResult;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+public class TestAvroUtil {\n+\n+  \n+  @Test\n+  public void testGetEmpty() {\n+    Result result = Mockito.mock(Result.class);\n+    Mockito.when(result.getRow()).thenReturn(null);\n+    //Get on a row, that does not exist, returns a result, \n+    //whose row is null.\n+    AResult aresult = AvroUtil.resultToAResult(result);\n+    Assert.assertNotNull(aresult);\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "sha": "1e4d31034b8daa2150c7ad6df400333092595809",
                "status": "added"
            }
        ],
        "message": "HBASE-4494 AvroServer:: get fails with NPE on a non-existent row\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1179014 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/fa6fb569324a622fcd421e6624d6655576acfb40",
        "patched_files": [
            "CHANGES.java",
            "AvroUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAvroUtil.java"
        ]
    },
    "hbase_2ba4c4e": {
        "bug_id": "hbase_2ba4c4e",
        "commit": "https://github.com/apache/hbase/commit/2ba4c4eb9fe568b962f9d71de829531f51c5375b",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java?ref=2ba4c4eb9fe568b962f9d71de829531f51c5375b",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "patch": "@@ -41,6 +41,7 @@\n   public TagRewriteCell(Cell cell, byte[] tags) {\n     assert cell instanceof SettableSequenceId;\n     assert cell instanceof SettableTimestamp;\n+    assert tags != null;\n     this.cell = cell;\n     this.tags = tags;\n     // tag offset will be treated as 0 and length this.tags.length\n@@ -143,6 +144,10 @@ public int getTagsOffset() {\n \n   @Override\n   public int getTagsLength() {\n+    if (null == this.tags) {\n+      // Nulled out tags array optimization in constructor\n+      return 0;\n+    }\n     return this.tags.length;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "sha": "eb6d3bef82f8511270033e96a6c6f4c3b8f24d5d",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hbase/blob/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java?ref=2ba4c4eb9fe568b962f9d71de829531f51c5375b",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "patch": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category(SmallTests.class)\n+public class TestTagRewriteCell {\n+\n+  @Test\n+  public void testHeapSize() {\n+    Cell originalCell = CellUtil.createCell(Bytes.toBytes(\"row\"), Bytes.toBytes(\"value\"));\n+    final int fakeTagArrayLength = 10;\n+    TagRewriteCell trCell = new TagRewriteCell(originalCell, new byte[fakeTagArrayLength]);\n+\n+    // Get the heapSize before the internal tags array in trCell are nuked\n+    long trCellHeapSize = trCell.heapSize();\n+\n+    // Make another TagRewriteCell with the original TagRewriteCell\n+    // This happens on systems with more than one RegionObserver/Coproc loaded (such as\n+    // VisibilityController and AccessController)\n+    TagRewriteCell trCell2 = new TagRewriteCell(trCell, new byte[fakeTagArrayLength]);\n+\n+    assertTrue(\"TagRewriteCell containing a TagRewriteCell's heapsize should be larger than a \" +\n+        \"single TagRewriteCell's heapsize\", trCellHeapSize < trCell2.heapSize());\n+    assertTrue(\"TagRewriteCell should have had nulled out tags array\", trCell.heapSize() <\n+        trCellHeapSize);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "sha": "56cecf4ca4046f082d75aea6b11b5606a8f5cedf",
                "status": "added"
            }
        ],
        "message": "HBASE-13520 NullPointerException in TagRewriteCell.(Josh Elser)",
        "parent": "https://github.com/apache/hbase/commit/eb82b8b3098d6a9ac62aa50189f9d4b289f38472",
        "patched_files": [
            "TagRewriteCell.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTagRewriteCell.java"
        ]
    },
    "hbase_2bbc958": {
        "bug_id": "hbase_2bbc958",
        "commit": "https://github.com/apache/hbase/commit/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java?ref=2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "patch": "@@ -477,8 +477,9 @@ public static void writeObject(DataOutput out, Object instance,\n       int length = list.size();\n       out.writeInt(length);\n       for (int i = 0; i < length; i++) {\n-        writeObject(out, list.get(i),\n-                  list.get(i).getClass(), conf);\n+        Object elem = list.get(i);\n+        writeObject(out, elem,\n+                  elem == null ? Writable.class : elem.getClass(), conf);\n       }\n     } else if (declClass == String.class) {   // String\n       Text.writeString(out, (String)instanceObj);",
                "raw_url": "https://github.com/apache/hbase/raw/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "sha": "e19af32ba644fab789bd598e0a15863d89b0d747",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java?ref=2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "patch": "@@ -198,6 +198,14 @@ public void testReadObjectDataInputConfiguration() throws IOException {\n     obj = doType(conf, list, List.class);\n     assertTrue(obj instanceof List);\n     Assert.assertArrayEquals(list.toArray(), ((List)obj).toArray() );\n+    //List.class with null values\n+    List<String> listWithNulls = new ArrayList<String>();\n+    listWithNulls.add(\"hello\");\n+    listWithNulls.add(\"world\");\n+    listWithNulls.add(null);\n+    obj = doType(conf, listWithNulls, List.class);\n+    assertTrue(obj instanceof List);\n+    Assert.assertArrayEquals(listWithNulls.toArray(), ((List)obj).toArray() );\n     //ArrayList.class\n     ArrayList<String> arr = new ArrayList<String>();\n     arr.add(\"hello\");",
                "raw_url": "https://github.com/apache/hbase/raw/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "sha": "2e666668d63613858fb76e6fa18ea8b16e961046",
                "status": "modified"
            }
        ],
        "message": "HBASE-6049 Serializing 'List' containing null elements will cause NullPointerException in HbaseObjectWritable.writeObject()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1344363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/20bd3f02d03743429e28a64e9d2095ad72a690d1",
        "patched_files": [
            "HbaseObjectWritable.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHbaseObjectWritable.java"
        ]
    },
    "hbase_2f2faa6": {
        "bug_id": "hbase_2f2faa6",
        "commit": "https://github.com/apache/hbase/commit/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java?ref=2f2faa68d1b158f3d44b1ea4f2ef832705933f3f",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "patch": "@@ -85,6 +85,7 @@ public void testEnableTableWithNoRegionServers() throws Exception {\n     // disable once more\n     admin.disableTable(tableName);\n \n+    TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n     // now stop region servers\n     JVMClusterUtil.RegionServerThread rs = cluster.getRegionServerThreads().get(0);\n     rs.getRegionServer().stop(\"stop\");",
                "raw_url": "https://github.com/apache/hbase/raw/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "sha": "d3d623944857e222ce81e773cfb866114bb59756",
                "status": "modified"
            }
        ],
        "message": "HBASE-12966 NPE in HMaster while recovering tables in Enabling state; ADDENDUM",
        "parent": "https://github.com/apache/hbase/commit/eddd5739a14ceb5cfc9b9c7d2e357eea96bd9703",
        "patched_files": [],
        "repo": "hbase",
        "unit_tests": [
            "TestEnableTableHandler.java"
        ]
    },
    "hbase_2ff6d0f": {
        "bug_id": "hbase_2ff6d0f",
        "commit": "https://github.com/apache/hbase/commit/2ff6d0fe4789857ab51685949711d755dedd459a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 3,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "patch": "@@ -23,6 +23,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -2400,8 +2401,7 @@ public static KeyValue cloneAndAddTags(Cell c, List<Tag> newTags) {\n    * Create a KeyValue reading from the raw InputStream.\n    * Named <code>iscreate</code> so doesn't clash with {@link #create(DataInput)}\n    * @param in\n-   * @return Created KeyValue OR if we find a length of zero, we will return null which\n-   * can be useful marking a stream as done.\n+   * @return Created KeyValue or throws an exception\n    * @throws IOException\n    * {@link Deprecated} As of 1.2. Use {@link KeyValueUtil#iscreate(InputStream, boolean)} instead.\n    */\n@@ -2412,7 +2412,9 @@ public static KeyValue iscreate(final InputStream in) throws IOException {\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0) return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "sha": "7534e9df6a2f690d57f5dd9e90feee26879e4144",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 5,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -187,7 +188,7 @@ public static void appendToByteBuffer(final ByteBuffer bb, final KeyValue kv,\n    * position to the start of the next KeyValue. Does not allocate a new array or copy data.\n    * @param bb\n    * @param includesMvccVersion\n-   * @param includesTags \n+   * @param includesTags\n    */\n   public static KeyValue nextShallowCopy(final ByteBuffer bb, final boolean includesMvccVersion,\n       boolean includesTags) {\n@@ -231,7 +232,7 @@ public static KeyValue previousKey(final KeyValue in) {\n     return createFirstOnRow(CellUtil.cloneRow(in), CellUtil.cloneFamily(in),\n       CellUtil.cloneQualifier(in), in.getTimestamp() - 1);\n   }\n-  \n+\n \n   /**\n    * Create a KeyValue for the specified row, family and qualifier that would be\n@@ -449,6 +450,7 @@ public static KeyValue ensureKeyValue(final Cell cell) {\n   @Deprecated\n   public static List<KeyValue> ensureKeyValues(List<Cell> cells) {\n     List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {\n+      @Override\n       public KeyValue apply(Cell arg0) {\n         return KeyValueUtil.ensureKeyValue(arg0);\n       }\n@@ -491,8 +493,9 @@ public static KeyValue iscreate(final InputStream in, boolean withTags) throws I\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0)\n-          return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;\n@@ -555,7 +558,7 @@ public static KeyValue create(final DataInput in) throws IOException {\n \n   /**\n    * Create a KeyValue reading <code>length</code> from <code>in</code>\n-   * \n+   *\n    * @param length\n    * @param in\n    * @return Created KeyValue OR if we find a length of zero, we will return",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "sha": "98e2205c9c75b49d91d7f4e6aa550719c16e63a3",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 9,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "patch": "@@ -20,6 +20,9 @@\n import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.PushbackInputStream;\n+\n+import javax.annotation.Nonnull;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -32,27 +35,41 @@\n @InterfaceAudience.Private\n public abstract class BaseDecoder implements Codec.Decoder {\n   protected static final Log LOG = LogFactory.getLog(BaseDecoder.class);\n-  protected final InputStream in;\n-  private boolean hasNext = true;\n+\n+  protected final PBIS in;\n   private Cell current = null;\n \n+  protected static class PBIS extends PushbackInputStream {\n+    public PBIS(InputStream in, int size) {\n+      super(in, size);\n+    }\n+\n+    public void resetBuf(int size) {\n+      this.buf = new byte[size];\n+      this.pos = size;\n+    }\n+  }\n+\n   public BaseDecoder(final InputStream in) {\n-    this.in = in;\n+    this.in = new PBIS(in, 1);\n   }\n \n   @Override\n   public boolean advance() throws IOException {\n-    if (!this.hasNext) return this.hasNext;\n-    if (this.in.available() == 0) {\n-      this.hasNext = false;\n-      return this.hasNext;\n+    int firstByte = in.read();\n+    if (firstByte == -1) {\n+      return false;\n+    } else {\n+      in.unread(firstByte);\n     }\n+\n     try {\n       this.current = parseCell();\n     } catch (IOException ioEx) {\n+      in.resetBuf(1); // reset the buffer in case the underlying stream is read from upper layers\n       rethrowEofException(ioEx);\n     }\n-    return this.hasNext;\n+    return true;\n   }\n \n   private void rethrowEofException(IOException ioEx) throws IOException {\n@@ -72,9 +89,12 @@ private void rethrowEofException(IOException ioEx) throws IOException {\n   }\n \n   /**\n-   * @return extract a Cell\n+   * Extract a Cell.\n+   * @return a parsed Cell or throws an Exception. EOFException or a generic IOException maybe\n+   * thrown if EOF is reached prematurely. Does not return null.\n    * @throws IOException\n    */\n+  @Nonnull\n   protected abstract Cell parseCell() throws IOException;\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "sha": "09dc37fd3674d64d89aaaf0470f18876cd0b2e80",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 0,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "patch": "@@ -79,6 +79,7 @@ public CellDecoder(final InputStream in) {\n       super(in);\n     }\n \n+    @Override\n     protected Cell parseCell() throws IOException {\n       byte [] row = readByteArray(this.in);\n       byte [] family = readByteArray(in);",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "sha": "666f440eea643ae3cf56f3b10352642a76a270aa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 7,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "patch": "@@ -19,7 +19,6 @@\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n-import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -84,12 +83,8 @@ protected Cell parseCell() throws IOException {\n         return super.parseCell();\n       }\n       int ivLength = 0;\n-      try {\n-        ivLength = StreamUtils.readRawVarint32(in);\n-      } catch (EOFException e) {\n-        // EOF at start is OK\n-        return null;\n-      }\n+\n+      ivLength = StreamUtils.readRawVarint32(in);\n \n       // TODO: An IV length of 0 could signify an unwrapped cell, when the\n       // encoder supports that just read the remainder in directly",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "sha": "69181e5e94d6d82b8baca91823580f2263c7858e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -855,9 +855,10 @@ private int countDistinctRowKeys(WALEdit edit) {\n       int distinctRowKeys = 1;\n       Cell lastCell = cells.get(0);\n       for (int i = 0; i < edit.size(); i++) {\n-        if (!CellUtil.matchingRow(cells.get(i), lastCell)) {\n+        if (!CellUtil.matchingRows(cells.get(i), lastCell)) {\n           distinctRowKeys++;\n         }\n+        lastCell = cells.get(i);\n       }\n       return distinctRowKeys;\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "3d9952324e5ef8bf560261746d43b8db0a7a814f",
                "status": "modified"
            }
        ],
        "message": "HBASE-14501 NPE in replication with TDE",
        "parent": "https://github.com/apache/hbase/commit/6143b7694cc02e905b931de86462c6125ca8b3b6",
        "patched_files": [
            "KeyValue.java",
            "CellCodec.java",
            "ReplicationSource.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java",
            "TestCellCodec.java",
            "TestKeyValue.java"
        ]
    },
    "hbase_30c7e95": {
        "bug_id": "hbase_30c7e95",
        "commit": "https://github.com/apache/hbase/commit/30c7e958ee20256a3d3fb88606eec7302139680f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=30c7e958ee20256a3d3fb88606eec7302139680f",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -561,8 +561,10 @@ private void resetScannerStack(KeyValue lastTopKey) throws IOException {\n \n   @Override\n   public synchronized boolean reseek(KeyValue kv) throws IOException {\n-    //Heap cannot be null, because this is only called from next() which\n-    //guarantees that heap will never be null before this call.\n+    //Heap will not be null, if this is called from next() which.\n+    //If called from RegionScanner.reseek(...) make sure the scanner\n+    //stack is reset if needed.\n+    checkReseek();\n     if (explicitColumnQuery && lazySeekEnabledGlobally) {\n       return heap.requestSeek(kv, true, useRowColBloom);\n     } else {",
                "raw_url": "https://github.com/apache/hbase/raw/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "b595c066bac18a539ef5579154934564ef1f34c4",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hbase/blob/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=30c7e958ee20256a3d3fb88606eec7302139680f",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "patch": "@@ -93,6 +93,7 @@\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.util.PairOfSameType;\n import org.apache.hadoop.hbase.util.Threads;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.mockito.Mockito;\n@@ -205,6 +206,40 @@ public void testCompactionAffectedByScanners() throws Exception {\n     System.out.println(results);\n     assertEquals(0, results.size());\n   }\n+  \n+  @Test\n+  public void testToShowNPEOnRegionScannerReseek() throws Exception{\n+    String method = \"testToShowNPEOnRegionScannerReseek\";\n+    byte[] tableName = Bytes.toBytes(method);\n+    byte[] family = Bytes.toBytes(\"family\");\n+    Configuration conf = HBaseConfiguration.create();\n+    this.region = initHRegion(tableName, method, conf, family);\n+\n+    Put put = new Put(Bytes.toBytes(\"r1\"));\n+    put.add(family, Bytes.toBytes(\"q1\"), Bytes.toBytes(\"v1\"));\n+    region.put(put);\n+    put = new Put(Bytes.toBytes(\"r2\"));\n+    put.add(family, Bytes.toBytes(\"q1\"), Bytes.toBytes(\"v1\"));\n+    region.put(put);\n+    region.flushcache();\n+\n+\n+    Scan scan = new Scan();\n+    scan.setMaxVersions(3);\n+    // open the first scanner\n+    RegionScanner scanner1 = region.getScanner(scan);\n+\n+    System.out.println(\"Smallest read point:\" + region.getSmallestReadPoint());\n+    \n+    region.compactStores(true);\n+\n+    scanner1.reseek(Bytes.toBytes(\"r2\"));\n+    List<KeyValue> results = new ArrayList<KeyValue>();\n+    scanner1.next(results);\n+    KeyValue keyValue = results.get(0);\n+    Assert.assertTrue(Bytes.compareTo(keyValue.getRow(), Bytes.toBytes(\"r2\")) == 0);\n+    scanner1.close();\n+  }\n \n   public void testSkipRecoveredEditsReplay() throws Exception {\n     String method = \"testSkipRecoveredEditsReplay\";",
                "raw_url": "https://github.com/apache/hbase/raw/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "sha": "b908763b50ddca118fcb710f7c6b81ad8fe1f40b",
                "status": "modified"
            }
        ],
        "message": "HBASE-6900 RegionScanner.reseek() creates NPE when a flush or compaction happens before the reseek.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1394377 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6f1af4dba4b2bf46722cb1e21f98b06d0476bc0f",
        "patched_files": [
            "HRegion.java",
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestStoreScanner.java"
        ]
    },
    "hbase_33bedf8": {
        "bug_id": "hbase_33bedf8",
        "commit": "https://github.com/apache/hbase/commit/33bedf8d4d7ec320c5cc01c1c031035a1523f973",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 4,
                "filename": "hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "patch": "@@ -40,6 +40,7 @@\n \n import java.io.IOException;\n import java.net.URI;\n+import java.util.Objects;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n@@ -187,10 +188,7 @@ public static void main(String[] args) throws Exception {\n \n   @Override\n   public int run(String[] args) throws IOException {\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "sha": "cfccf30e3a0d46540deea94ddf6a3d32354232e7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 4,
                "filename": "hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "patch": "@@ -35,6 +35,7 @@\n import java.io.IOException;\n import java.net.URI;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.commons.lang3.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n@@ -232,10 +233,7 @@ public static void main(String[] args) throws Exception {\n \n   @Override\n   public int run(String[] args) {\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "sha": "39cc440a4edcb036f8e39a114b56811581ba0422",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 3,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.NavigableMap;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.regex.Matcher;\n@@ -230,13 +231,13 @@ public static void fullScanTables(Connection connection, final Visitor visitor)\n    * Callers should call close on the returned {@link Table} instance.\n    * @param connection connection we're using to access Meta\n    * @return An {@link Table} for <code>hbase:meta</code>\n+   * @throws NullPointerException if {@code connection} is {@code null}\n    */\n   public static Table getMetaHTable(final Connection connection)\n   throws IOException {\n     // We used to pass whole CatalogTracker in here, now we just pass in Connection\n-    if (connection == null) {\n-      throw new NullPointerException(\"No connection\");\n-    } else if (connection.isClosed()) {\n+    Objects.requireNonNull(connection, \"Connection cannot be null\");\n+    if (connection.isClosed()) {\n       throw new IOException(\"connection is closed\");\n     }\n     return connection.getTable(TableName.META_TABLE_NAME);",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "sha": "65a3393cf8e8c765023db39becbce1e0e70cabbe",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 5,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "patch": "@@ -38,6 +38,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.function.Function;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n@@ -1594,14 +1595,13 @@ public static ScanMetrics toScanMetrics(final byte[] bytes) {\n   }\n \n   /**\n-   * Unwraps an exception from a protobuf service into the underlying (expected) IOException.\n-   * This method will <strong>always</strong> throw an exception.\n+   * Unwraps an exception from a protobuf service into the underlying (expected) IOException. This\n+   * method will <strong>always</strong> throw an exception.\n    * @param se the {@code ServiceException} instance to convert into an {@code IOException}\n+   * @throws NullPointerException if {@code se} is {@code null}\n    */\n   public static void toIOException(ServiceException se) throws IOException {\n-    if (se == null) {\n-      throw new NullPointerException(\"Null service exception passed!\");\n-    }\n+    Objects.requireNonNull(se, \"Service exception cannot be null\");\n \n     Throwable cause = se.getCause();\n     if (cause != null && cause instanceof IOException) {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "sha": "2dcfe0e7d6b3373883ca5c7b4722f7e1d1e3c5b6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 3,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n \n import org.apache.commons.lang3.StringUtils;\n import org.apache.hadoop.hbase.HConstants;\n@@ -56,6 +57,7 @@ private AccessControlUtil() {}\n    * @param qualifier optional qualifier\n    * @param actions the permissions to be granted\n    * @return A {@link AccessControlProtos} GrantRequest\n+   * @throws NullPointerException if {@code tableName} is {@code null}\n    */\n   public static AccessControlProtos.GrantRequest buildGrantRequest(\n       String username, TableName tableName, byte[] family, byte[] qualifier,\n@@ -67,9 +69,9 @@ private AccessControlUtil() {}\n     for (AccessControlProtos.Permission.Action a : actions) {\n       permissionBuilder.addAction(a);\n     }\n-    if (tableName == null) {\n-      throw new NullPointerException(\"TableName cannot be null\");\n-    }\n+\n+    Objects.requireNonNull(tableName, \"TableName cannot be null\");\n+\n     permissionBuilder.setTableName(ProtobufUtil.toProtoTableName(tableName));\n \n     if (family != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "sha": "b3152c3c0eff9135bf5459f765edaf89484edf5e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 5,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "patch": "@@ -35,6 +35,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.Callable;\n@@ -1965,14 +1966,13 @@ public static ScanMetrics toScanMetrics(final byte[] bytes) {\n   }\n \n   /**\n-   * Unwraps an exception from a protobuf service into the underlying (expected) IOException.\n-   * This method will <strong>always</strong> throw an exception.\n+   * Unwraps an exception from a protobuf service into the underlying (expected) IOException. This\n+   * method will <strong>always</strong> throw an exception.\n    * @param se the {@code ServiceException} instance to convert into an {@code IOException}\n+   * @throws NullPointerException if {@code se} is {@code null}\n    */\n   public static void toIOException(ServiceException se) throws IOException {\n-    if (se == null) {\n-      throw new NullPointerException(\"Null service exception passed!\");\n-    }\n+    Objects.requireNonNull(se, \"Service exception cannot be null\");\n \n     Throwable cause = se.getCause();\n     if (cause != null && cause instanceof IOException) {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "sha": "94a2805b61cc4f73ae720d1c711d83fbd6e2bf80",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 13,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.IOException;\n import java.io.OutputStream;\n import java.nio.ByteBuffer;\n+import java.util.Objects;\n \n import org.apache.hadoop.hbase.io.util.StreamUtils;\n import org.apache.hadoop.hbase.util.ByteBufferUtils;\n@@ -58,22 +59,21 @@ public ByteBufferWriterOutputStream(OutputStream os, int size) {\n   }\n \n   /**\n-   * Writes len bytes from the specified ByteBuffer starting at offset off to\n-   * this OutputStream. If b is null, a NullPointerException is thrown. If off\n-   * is negative or larger than the ByteBuffer then an ArrayIndexOutOfBoundsException\n-   * is thrown. If len is greater than the length of the ByteBuffer, then an\n-   * ArrayIndexOutOfBoundsException is thrown. This method does not change the\n-   * position of the ByteBuffer.\n-   *\n-   * @param b    the ByteBuffer\n-   * @param off  the start offset in the data\n-   * @param len  the number of bytes to write\n-   * @throws IOException\n-   *             if an I/O error occurs. In particular, an IOException is thrown\n-   *             if the output stream is closed.\n+   * Writes len bytes from the specified ByteBuffer starting at offset off to this OutputStream. If\n+   * off is negative or larger than the ByteBuffer then an ArrayIndexOutOfBoundsException is thrown.\n+   * If len is greater than the length of the ByteBuffer, then an ArrayIndexOutOfBoundsException is\n+   * thrown. This method does not change the position of the ByteBuffer.\n+   * @param b the ByteBuffer\n+   * @param off the start offset in the data\n+   * @param len the number of bytes to write\n+   * @throws IOException if an I/O error occurs. In particular, an IOException is thrown if the\n+   *           output stream is closed.\n+   * @throws NullPointerException if {@code b} is {@code null}\n    */\n   @Override\n   public void write(ByteBuffer b, int off, int len) throws IOException {\n+    Objects.requireNonNull(b);\n+\n     // Lazily load in the event that this version of 'write' is not invoked\n     if (this.buf == null) {\n       this.buf = new byte[this.bufSize];",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "sha": "13025569276f8696a3d7cb24d64bba401835a024",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 3,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "patch": "@@ -83,10 +83,8 @@ public void reset() {\n   }\n \n   protected void init() {\n+    Preconditions.checkState(iv != null, \"IV is null\");\n     try {\n-      if (iv == null) {\n-        throw new NullPointerException(\"IV is null\");\n-      }\n       cipher.init(javax.crypto.Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));\n     } catch (InvalidKeyException e) {\n       throw new RuntimeException(e);",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "sha": "fd512ec4cfdb4cc89b477b90d21a391a4bd9b819",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 4,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.util.Comparator;\n import java.util.HashMap;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n@@ -115,10 +116,8 @@ public void setConf(Configuration conf) {\n   @Override\n   public int run(String[] args) throws IOException {\n     cmdLineArgs = args;\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     List<String> argsList = new ArrayList<>(args.length);",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "sha": "adb69ff45fa8347fb2658e71a588f375dcac677c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 6,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.conf.Configuration;\n@@ -1018,19 +1019,18 @@ public static FSDataOutputStream createForWal(FileSystem fs, Path path, boolean\n   }\n \n   /**\n-   * If our FileSystem version includes the StreamCapabilities class, check if\n-   * the given stream has a particular capability.\n+   * If our FileSystem version includes the StreamCapabilities class, check if the given stream has\n+   * a particular capability.\n    * @param stream capabilities are per-stream instance, so check this one specifically. must not be\n-   *        null\n+   *          null\n    * @param capability what to look for, per Hadoop Common's FileSystem docs\n    * @return true if there are no StreamCapabilities. false if there are, but this stream doesn't\n    *         implement it. return result of asking the stream otherwise.\n+   * @throws NullPointerException if {@code stream} is {@code null}\n    */\n   public static boolean hasCapability(FSDataOutputStream stream, String capability) {\n     // be consistent whether or not StreamCapabilities is present\n-    if (stream == null) {\n-      throw new NullPointerException(\"stream parameter must not be null.\");\n-    }\n+    Objects.requireNonNull(stream, \"stream cannot be null\");\n     // If o.a.h.fs.StreamCapabilities doesn't exist, assume everyone does everything\n     // otherwise old versions of Hadoop will break.\n     boolean result = true;",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "sha": "6a0c1cd55c935a72c9fb27eb85a9a875b2b9db1d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 9,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import java.lang.ref.Reference;\n import java.lang.ref.ReferenceQueue;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.locks.Lock;\n@@ -75,7 +76,7 @@\n    *\n    * @param objectFactory the factory to supply new objects on demand\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    */\n   public ObjectPool(ObjectFactory<K, V> objectFactory) {\n     this(objectFactory, DEFAULT_INITIAL_CAPACITY, DEFAULT_CONCURRENCY_LEVEL);\n@@ -88,7 +89,7 @@ public ObjectPool(ObjectFactory<K, V> objectFactory) {\n    * @param objectFactory the factory to supply new objects on demand\n    * @param initialCapacity the initial capacity to keep objects in the pool\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    * @throws IllegalArgumentException if {@code initialCapacity} is negative\n    */\n   public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity) {\n@@ -103,7 +104,7 @@ public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity) {\n    * @param initialCapacity the initial capacity to keep objects in the pool\n    * @param concurrencyLevel the estimated count of concurrently accessing threads\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    * @throws IllegalArgumentException if {@code initialCapacity} is negative or\n    *    {@code concurrencyLevel} is non-positive\n    */\n@@ -112,10 +113,7 @@ public ObjectPool(\n       int initialCapacity,\n       int concurrencyLevel) {\n \n-    if (objectFactory == null) {\n-      throw new NullPointerException(\"Given object factory instance is NULL\");\n-    }\n-    this.objectFactory = objectFactory;\n+    this.objectFactory = Objects.requireNonNull(objectFactory, \"Object factory cannot be null\");\n \n     this.referenceCache =\n         new ConcurrentHashMap<K, Reference<V>>(initialCapacity, 0.75f, concurrencyLevel);\n@@ -164,10 +162,10 @@ public void purge() {\n   /**\n    * Returns a shared object associated with the given {@code key},\n    * which is identified by the {@code equals} method.\n-   * @throws NullPointerException if {@code key} is null\n+   * @throws NullPointerException if {@code key} is {@code null}\n    */\n   public V get(K key) {\n-    Reference<V> ref = referenceCache.get(key);\n+    Reference<V> ref = referenceCache.get(Objects.requireNonNull(key));\n     if (ref != null) {\n       V obj = ref.get();\n       if (obj != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "sha": "30f5a4636103774483810ae93353f168f30fbe9b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.hadoop.hbase.Cell;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -40,12 +41,13 @@\n final public class FilterWrapper extends Filter {\n   Filter filter = null;\n \n-  public FilterWrapper( Filter filter ) {\n-    if (null == filter) {\n-      // ensure the filter instance is not null\n-      throw new NullPointerException(\"Cannot create FilterWrapper with null Filter\");\n-    }\n-    this.filter = filter;\n+  /**\n+   * Constructor.\n+   * @param filter filter to wrap\n+   * @throws NullPointerException if {@code filter} is {@code null}\n+   */\n+  public FilterWrapper(Filter filter) {\n+    this.filter = Objects.requireNonNull(filter, \"Cannot create FilterWrapper with null Filter\");\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "sha": "b127493fc5c287ecf2264d4e96aa86ea36e5bf30",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.io.DataOutput;\n import java.io.IOException;\n import java.util.ArrayDeque;\n+import java.util.Objects;\n import java.util.Queue;\n \n import org.apache.hadoop.hbase.Cell;\n@@ -163,8 +164,7 @@ private void enqueueReadyChunk(boolean closing) {\n \n   @Override\n   public void append(Cell cell) throws IOException {\n-    if (cell == null)\n-      throw new NullPointerException();\n+    Objects.requireNonNull(cell);\n \n     enqueueReadyChunk(false);\n ",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "sha": "228b54c7ab0033a3c28dd3e056431aee9105471e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "patch": "@@ -29,6 +29,7 @@\n import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.TreeMap;\n@@ -122,12 +123,13 @@ public HFileInfo(ReaderContext context, Configuration conf) throws IOException {\n    *          with the reserved prefix\n    * @return this file info object\n    * @throws IOException if the key or value is invalid\n+   * @throws NullPointerException if {@code key} or {@code value} is {@code null}\n    */\n   public HFileInfo append(final byte[] k, final byte[] v,\n       final boolean checkPrefix) throws IOException {\n-    if (k == null || v == null) {\n-      throw new NullPointerException(\"Key nor value may be null\");\n-    }\n+    Objects.requireNonNull(k, \"key cannot be null\");\n+    Objects.requireNonNull(v, \"value cannot be null\");\n+\n     if (checkPrefix && isReservedFileInfoKey(k)) {\n       throw new IOException(\"Keys with a \" + HFileInfo.RESERVED_PREFIX\n           + \" are reserved\");",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "sha": "89b3d34a7750751f753f2fd3b156506ea31e9fe4",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -43,6 +43,7 @@\n import java.util.Map.Entry;\n import java.util.NavigableMap;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.RandomAccess;\n import java.util.Set;\n@@ -7322,13 +7323,14 @@ public static HRegion openHRegion(final Configuration conf, final FileSystem fs,\n    * @param rsServices An interface we can request flushes against.\n    * @param reporter An interface we can report progress against.\n    * @return new HRegion\n+   * @throws NullPointerException if {@code info} is {@code null}\n    */\n   public static HRegion openHRegion(final Configuration conf, final FileSystem fs,\n       final Path rootDir, final Path tableDir, final RegionInfo info, final TableDescriptor htd,\n       final WAL wal, final RegionServerServices rsServices,\n       final CancelableProgressable reporter)\n       throws IOException {\n-    if (info == null) throw new NullPointerException(\"Passed region info is null\");\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Opening region: \" + info);\n     }\n@@ -7403,12 +7405,11 @@ protected HRegion openHRegion(final CancelableProgressable reporter)\n    * @param info Info for region to be opened.\n    * @param htd the table descriptor\n    * @return new HRegion\n+   * @throws NullPointerException if {@code info} is {@code null}\n    */\n   public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, final FileSystem fs,\n       final Path tableDir, RegionInfo info, final TableDescriptor htd) throws IOException {\n-    if (info == null) {\n-      throw new NullPointerException(\"Passed region info is null\");\n-    }\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Opening region (readOnly filesystem): \" + info);\n     }\n@@ -7426,7 +7427,7 @@ public static void warmupHRegion(final RegionInfo info,\n       final CancelableProgressable reporter)\n       throws IOException {\n \n-    if (info == null) throw new NullPointerException(\"Passed region info is null\");\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"HRegion.Warming up region: \" + info);",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "99043e83caaf456a2c0c2f7e4efb260247714f2e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "patch": "@@ -46,7 +46,9 @@\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;\n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n \n /**\n@@ -269,9 +271,7 @@ public boolean isHFile() {\n \n   @Override\n   public boolean isMajorCompactionResult() {\n-    if (this.majorCompaction == null) {\n-      throw new NullPointerException(\"This has not been set yet\");\n-    }\n+    Preconditions.checkState(this.majorCompaction != null, \"Major compation has not been set yet\");\n     return this.majorCompaction.get();\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "sha": "fa152c5061aeca63e7b9493dcda79d2e44e4a155",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "patch": "@@ -135,7 +135,7 @@\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n+import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;\n import org.apache.hbase.thirdparty.com.google.common.collect.ArrayListMultimap;\n import org.apache.hbase.thirdparty.com.google.common.collect.ImmutableSet;\n import org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap;\n@@ -716,11 +716,9 @@ public void start(CoprocessorEnvironment env) throws IOException {\n       }\n     }\n \n-    if (zkPermissionWatcher == null) {\n-      throw new NullPointerException(\"ZKPermissionWatcher is null\");\n-    } else if (accessChecker == null) {\n-      throw new NullPointerException(\"AccessChecker is null\");\n-    }\n+    Preconditions.checkState(zkPermissionWatcher != null, \"ZKPermissionWatcher is null\");\n+    Preconditions.checkState(accessChecker != null, \"AccessChecker is null\");\n+\n     // set the user-provider.\n     this.userProvider = UserProvider.instantiate(env.getConfiguration());\n     tableAcls = new MapMaker().weakValues().makeMap();",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "sha": "7719b53230a4329245900892c6a286a85c79f4a2",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.util.Collection;\n import java.util.Comparator;\n import java.util.Iterator;\n+import java.util.Objects;\n import java.util.AbstractQueue;\n \n import org.apache.yetus.audience.InterfaceAudience;\n@@ -157,7 +158,7 @@ public BoundedPriorityBlockingQueue(int capacity,\n \n   @Override\n   public boolean offer(E e) {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n \n     lock.lock();\n     try {\n@@ -174,7 +175,7 @@ public boolean offer(E e) {\n \n   @Override\n   public void put(E e) throws InterruptedException {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n \n     lock.lock();\n     try {\n@@ -191,7 +192,7 @@ public void put(E e) throws InterruptedException {\n   @Override\n   public boolean offer(E e, long timeout, TimeUnit unit)\n       throws InterruptedException {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n     long nanos = unit.toNanos(timeout);\n \n     lock.lockInterruptibly();\n@@ -321,8 +322,7 @@ public int drainTo(Collection<? super E> c) {\n \n   @Override\n   public int drainTo(Collection<? super E> c, int maxElements) {\n-    if (c == null)\n-        throw new NullPointerException();\n+    Objects.requireNonNull(c);\n     if (c == this)\n         throw new IllegalArgumentException();\n     if (maxElements <= 0)",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "sha": "efff41e11c865cca24f4185bda27788125ca4b5c",
                "status": "modified"
            }
        ],
        "message": "HBASE-23308: Review of NullPointerExceptions (#836)\n\nSigned-off-by: stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
        "patched_files": [
            "HStoreFile.java",
            "FilterWrapper.java",
            "CommonFSUtils.java",
            "MetaTableAccessor.java",
            "HRegion.java",
            "AccessController.java",
            "AbstractHBaseTool.java",
            "BoundedPriorityBlockingQueue.java",
            "ProtobufUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCommonFSUtils.java",
            "TestBoundedPriorityBlockingQueue.java",
            "TestMetaTableAccessor.java",
            "AbstractHBaseToolTest.java",
            "TestHRegion.java",
            "TestFilterWrapper.java",
            "TestHStoreFile.java",
            "TestAccessController.java",
            "TestProtobufUtil.java"
        ]
    },
    "hbase_34db492": {
        "bug_id": "hbase_34db492",
        "commit": "https://github.com/apache/hbase/commit/34db492b4627298cde804a565f076ce1bb3fa2c6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -642,6 +642,7 @@ Release 0.21.0 - Unreleased\n    HBASE-3185  User-triggered compactions are triggering splits!\n    HBASE-1932  Encourage use of 'lzo' compression... add the wiki page to\n                getting started\n+   HBASE-3151  NPE when trying to read regioninfo from .META.\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "sha": "381471c096018a191c3088661917b9c543b6759c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "patch": "@@ -205,7 +205,7 @@ public static void deleteDaughterReferenceInParent(CatalogTracker catalogTracker\n     catalogTracker.waitForMetaServerConnectionDefault().\n       delete(CatalogTracker.META_REGION, delete);\n     LOG.info(\"Deleted daughter \" + daughter.getRegionNameAsString() +\n-      \" \" + Bytes.toString(qualifier) + \" from parent \" +\n+      \" \" + Bytes.toString(qualifier) + \" reference in parent \" +\n       parent.getRegionNameAsString());\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "sha": "53bc9bef78aacb06d814ce126e57652abcf26d41",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -136,6 +136,7 @@ private static boolean isMetaRegion(final byte [] regionName) {\n       public boolean visit(Result r) throws IOException {\n         if (r ==  null || r.isEmpty()) return true;\n         Pair<HRegionInfo,HServerAddress> region = metaRowToRegionPair(r);\n+        if (region == null) return true;\n         regions.put(region.getFirst(), region.getSecond());\n         return true;\n       }\n@@ -296,13 +297,16 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n   /**\n    * @param data A .META. table row.\n    * @return A pair of the regioninfo and the server address from <code>data</code>\n-   * (or null for server address if no address set in .META.).\n+   * or null for server address if no address set in .META. or null for a result\n+   * if no HRegionInfo found.\n    * @throws IOException\n    */\n   public static Pair<HRegionInfo, HServerAddress> metaRowToRegionPair(\n       Result data) throws IOException {\n-    HRegionInfo info = Writables.getHRegionInfo(\n-      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));\n+    byte [] bytes =\n+      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n+    if (bytes == null) return null;\n+    HRegionInfo info = Writables.getHRegionInfo(bytes);\n     final byte[] value = data.getValue(HConstants.CATALOG_FAMILY,\n       HConstants.SERVER_QUALIFIER);\n     if (value != null && value.length > 0) {\n@@ -463,6 +467,7 @@ public static boolean tableExists(CatalogTracker catalogTracker,\n       while((data = metaServer.next(scannerid)) != null) {\n         if (data != null && data.size() > 0) {\n           Pair<HRegionInfo, HServerAddress> region = metaRowToRegionPair(data);\n+          if (region == null) continue;\n           if (region.getFirst().getTableDesc().getNameAsString().equals(\n               tableName)) {\n             regions.add(region);",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "8556595745dba52ef4743547fef1c34c318602e3",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "patch": "@@ -24,6 +24,8 @@\n import java.util.ArrayList;\n import java.util.List;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -40,6 +42,7 @@\n  * minor releases.\n  */\n public class MetaScanner {\n+  private static final Log LOG = LogFactory.getLog(MetaScanner.class);\n   /**\n    * Scans the meta table and calls a visitor on each RowResult and uses a empty\n    * start row value as table name.\n@@ -93,7 +96,8 @@ public static void metaScan(Configuration configuration,\n     // if row is not null, we want to use the startKey of the row's region as\n     // the startRow for the meta scan.\n     byte[] startRow;\n-    if (row != null) {\n+    // row could be non-null but empty -- the HConstants.EMPTY_START_ROW\n+    if (row != null && row.length > 0) {\n       // Scan starting at a particular row in a particular table\n       assert tableName != null;\n       byte[] searchRow =\n@@ -118,7 +122,9 @@ public static void metaScan(Configuration configuration,\n       byte[] rowBefore = regionInfo.getStartKey();\n       startRow = HRegionInfo.createRegionName(tableName, rowBefore,\n           HConstants.ZEROES, false);\n-    } else if (tableName == null || tableName.length == 0) {\n+    } else if (row == null ||\n+        (row != null && Bytes.equals(HConstants.EMPTY_START_ROW, row)) ||\n+        (tableName == null || tableName.length == 0)) {\n       // Full META scan\n       startRow = HConstants.EMPTY_START_ROW;\n     } else {\n@@ -133,8 +139,12 @@ public static void metaScan(Configuration configuration,\n         configuration.getInt(\"hbase.meta.scanner.caching\", 100));\n     do {\n       final Scan scan = new Scan(startRow).addFamily(HConstants.CATALOG_FAMILY);\n-      callable = new ScannerCallable(connection, HConstants.META_TABLE_NAME,\n-          scan);\n+      byte [] metaTableName = Bytes.equals(tableName, HConstants.ROOT_TABLE_NAME)?\n+        HConstants.ROOT_TABLE_NAME: HConstants.META_TABLE_NAME;\n+      LOG.debug(\"Scanning \" + Bytes.toString(metaTableName) +\n+        \" starting at row=\" + Bytes.toString(startRow) + \" for max=\" +\n+        rowUpperLimit + \" rows\");\n+      callable = new ScannerCallable(connection, metaTableName, scan);\n       // Open scanner\n       connection.getRegionServerWithRetries(callable);\n \n@@ -172,10 +182,24 @@ public static void metaScan(Configuration configuration,\n \n   /**\n    * Lists all of the regions currently in META.\n-   * @return\n+   * @param conf\n+   * @return List of all user-space regions.\n    * @throws IOException\n    */\n   public static List<HRegionInfo> listAllRegions(Configuration conf)\n+  throws IOException {\n+    return listAllRegions(conf, true);\n+  }\n+\n+  /**\n+   * Lists all of the regions currently in META.\n+   * @param conf\n+   * @param offlined True if we are to include offlined regions, false and we'll\n+   * leave out offlined regions from returned list.\n+   * @return List of all user-space regions.\n+   * @throws IOException\n+   */\n+  public static List<HRegionInfo> listAllRegions(Configuration conf, final boolean offlined)\n   throws IOException {\n     final List<HRegionInfo> regions = new ArrayList<HRegionInfo>();\n     MetaScannerVisitor visitor =\n@@ -185,9 +209,15 @@ public boolean processRow(Result result) throws IOException {\n           if (result == null || result.isEmpty()) {\n             return true;\n           }\n-          HRegionInfo regionInfo = Writables.getHRegionInfo(\n-              result.getValue(HConstants.CATALOG_FAMILY,\n-                  HConstants.REGIONINFO_QUALIFIER));\n+          byte [] bytes = result.getValue(HConstants.CATALOG_FAMILY,\n+            HConstants.REGIONINFO_QUALIFIER);\n+          if (bytes == null) {\n+            LOG.warn(\"Null REGIONINFO_QUALIFIER: \" + result);\n+            return true;\n+          }\n+          HRegionInfo regionInfo = Writables.getHRegionInfo(bytes);\n+          // If region offline AND we are not to include offlined regions, return.\n+          if (regionInfo.isOffline() && !offlined) return true;\n           regions.add(regionInfo);\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "sha": "03ac47df1d277faeb43b1e90f243afafe688972a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 6,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -519,9 +519,6 @@ public void regionOnline(HRegionInfo regionInfo, HServerInfo serverInfo) {\n         this.regionsInTransition.remove(regionInfo.getEncodedName());\n       if (rs != null) {\n         this.regionsInTransition.notifyAll();\n-      } else {\n-        LOG.warn(\"Asked online a region that was not in \" +\n-          \"regionsInTransition: \" + rs);\n       }\n     }\n     synchronized (this.regions) {\n@@ -597,7 +594,10 @@ public void setOffline(HRegionInfo regionInfo) {\n       HServerInfo serverInfo = this.regions.remove(regionInfo);\n       if (serverInfo != null) {\n         List<HRegionInfo> serverRegions = this.servers.get(serverInfo);\n-        serverRegions.remove(regionInfo);\n+        if (!serverRegions.remove(regionInfo)) {\n+          LOG.warn(\"Asked offline a region that was not on expected server: \" +\n+            regionInfo + \", \" + serverInfo.getServerName());\n+        }\n       } else {\n         LOG.warn(\"Asked offline a region that was not online: \" + regionInfo);\n       }\n@@ -1104,9 +1104,9 @@ public void assignAllUserRegions() throws IOException {\n     // First experiment at synchronous assignment\n     // Simpler because just wait for no regions in transition\n \n-    // Scan META for all user regions\n+    // Scan META for all user regions; do not include offlined regions in list.\n     List<HRegionInfo> allRegions =\n-      MetaScanner.listAllRegions(master.getConfiguration());\n+      MetaScanner.listAllRegions(master.getConfiguration(), false);\n     if (allRegions == null || allRegions.isEmpty()) return;\n \n     // Get all available servers",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "d383e2bf28463d866f33b25c044a2cc0c81aaa9d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "patch": "@@ -98,6 +98,7 @@ public boolean visit(Result r) throws IOException {\n         if (r == null || r.isEmpty()) return true;\n         count.incrementAndGet();\n         HRegionInfo info = getHRegionInfo(r);\n+        if (info == null) return true; // Keep scanning\n         if (info.isSplitParent()) splitParents.put(info, r);\n         // Returning true means \"keep scanning\"\n         return true;\n@@ -157,7 +158,7 @@ boolean cleanParent(final HRegionInfo parent,\n     boolean hasReferencesB =\n       checkDaughter(parent, rowContent, HConstants.SPLITB_QUALIFIER);\n     if (!hasReferencesA && !hasReferencesB) {\n-      LOG.info(\"Deleting region \" + parent.getRegionNameAsString() +\n+      LOG.debug(\"Deleting region \" + parent.getRegionNameAsString() +\n         \" because daughter splits no longer hold references\");\n       FileSystem fs = this.services.getMasterFileSystem().getFileSystem();\n       Path rootdir = this.services.getMasterFileSystem().getRootDir();",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "sha": "9fbc376f41dd8b1d72b4c972f27402558491fb05",
                "status": "modified"
            }
        ],
        "message": "HBASE-3151 NPE when trying to read regioninfo from .META.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1030293 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/c767ca49d13d3a296cc4a1fa34e3003db87626bd",
        "patched_files": [
            "AssignmentManager.java",
            "CatalogJanitor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCatalogJanitor.java",
            "TestAssignmentManager.java"
        ]
    },
    "hbase_35a7b56": {
        "bug_id": "hbase_35a7b56",
        "commit": "https://github.com/apache/hbase/commit/35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
                "deletions": 8,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -344,13 +345,19 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive mob data\n-    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-            tableName);\n-    Path regionDir =\n-            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-    if (fs.exists(regionDir)) {\n-      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n+    // Archive the mob data if there is a mob-enabled column\n+    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n+    boolean hasMob = MobUtils.hasMobColumns(htd);\n+    Path mobTableDir = null;\n+    if (hasMob) {\n+      // Archive mob data\n+      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+              tableName);\n+      Path regionDir =\n+              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+      if (fs.exists(regionDir)) {\n+        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n+      }\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -359,7 +366,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "sha": "4b95fa89d74499704a1b845f628638ea4caa6f07",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hbase/blob/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
                "deletions": 34,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "patch": "@@ -195,15 +195,32 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n-    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n-  }\n \n-  @Test(timeout=90000)\n-  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n-    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n-    testRollbackAndDoubleExecution(htd);\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n   }\n \n   @Test(timeout=90000)\n@@ -265,31 +282,4 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n-\n-  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-    TableName tableName = htd.getTableName();\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n-  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "sha": "0aad5fa81cf04fe505ccd311662cd2f2937fbb7f",
                "status": "modified"
            }
        ],
        "message": "Revert \"HBASE-4907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)\"\nRevert because I pitched this against the wrong JIRA number!\n\nThis reverts commit 03e4712f0ca08d57586b3fc4d93cf02c999515d8.",
        "parent": "https://github.com/apache/hbase/commit/26dd0d17f81627d3688f28bba1a293513ff5d702",
        "patched_files": [
            "CreateTableProcedure.java",
            "DeleteTableProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCreateTableProcedure.java",
            "TestDeleteTableProcedure.java"
        ]
    },
    "hbase_3757da6": {
        "bug_id": "hbase_3757da6",
        "commit": "https://github.com/apache/hbase/commit/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java?ref=3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
                "deletions": 5,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "patch": "@@ -550,17 +550,20 @@ public void deleteMetaLocation(ZooKeeperWatcher zookeeper, int replicaId)\n       final long timeout, Configuration conf)\n           throws InterruptedException {\n     int numReplicasConfigured = 1;\n+\n+    List<ServerName> servers = new ArrayList<ServerName>();\n+    // Make the blocking call first so that we do the wait to know\n+    // the znodes are all in place or timeout.\n+    ServerName server = blockUntilAvailable(zkw, timeout);\n+    if (server == null) return null;\n+    servers.add(server);\n+\n     try {\n       List<String> metaReplicaNodes = zkw.getMetaReplicaNodes();\n       numReplicasConfigured = metaReplicaNodes.size();\n     } catch (KeeperException e) {\n       LOG.warn(\"Got ZK exception \" + e);\n     }\n-    List<ServerName> servers = new ArrayList<ServerName>(numReplicasConfigured);\n-    ServerName server = blockUntilAvailable(zkw, timeout);\n-    if (server == null) return null;\n-    servers.add(server);\n-\n     for (int replicaId = 1; replicaId < numReplicasConfigured; replicaId++) {\n       // return all replica locations for the meta\n       servers.add(getMetaRegionLocation(zkw, replicaId));",
                "raw_url": "https://github.com/apache/hbase/raw/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "sha": "7b64e0cb4a31a438c5c70db299938498a9359ca3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java?ref=3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
                "deletions": 3,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "patch": "@@ -481,9 +481,11 @@ public boolean isDefaultMetaReplicaZnode(String node) {\n   public List<String> getMetaReplicaNodes() throws KeeperException {\n     List<String> childrenOfBaseNode = ZKUtil.listChildrenNoWatch(this, baseZNode);\n     List<String> metaReplicaNodes = new ArrayList<String>(2);\n-    String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n-    for (String child : childrenOfBaseNode) {\n-      if (child.startsWith(pattern)) metaReplicaNodes.add(child);\n+    if (childrenOfBaseNode != null) {\n+      String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n+      for (String child : childrenOfBaseNode) {\n+        if (child.startsWith(pattern)) metaReplicaNodes.add(child);\n+      }\n     }\n     return metaReplicaNodes;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "sha": "1f3afe42436c0d8a2062e35ec0f7ad090ef143d8",
                "status": "modified"
            }
        ],
        "message": "HBASE-16732 Avoid possible NPE in MetaTableLocator",
        "parent": "https://github.com/apache/hbase/commit/bf3c928b7499797735f71974992b68c9d876b97c",
        "patched_files": [
            "MetaTableLocator.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetaTableLocator.java"
        ]
    },
    "hbase_3812294": {
        "bug_id": "hbase_3812294",
        "commit": "https://github.com/apache/hbase/commit/3812294456ff9a8672a7e2368d466a7119ff4959",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.hbase.exceptions.FailedLogCloseException;\n import org.apache.hadoop.hbase.exceptions.HBaseSnapshotException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n import org.apache.hadoop.hbase.exceptions.NotServingRegionException;\n import org.apache.hadoop.hbase.exceptions.RegionException;\n import org.apache.hadoop.hbase.exceptions.RestoreSnapshotException;\n@@ -1710,6 +1711,9 @@ public void mergeRegions(final byte[] encodedNameOfRegionA,\n       if (ioe instanceof UnknownRegionException) {\n         throw (UnknownRegionException) ioe;\n       }\n+      if (ioe instanceof MergeRegionException) {\n+        throw (MergeRegionException) ioe;\n+      }\n       LOG.error(\"Unexpected exception: \" + se\n           + \" from calling HMaster.dispatchMergingRegions\");\n     } catch (DeserializationException de) {\n@@ -2740,4 +2744,4 @@ public long sleep(long pause, int tries) {\n   public CoprocessorRpcChannel coprocessorService() {\n     return new MasterCoprocessorRpcChannel(connection);\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "sha": "fffc213491586bdd160e161d7915e80c9db4654c",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.exceptions;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Thrown when something is wrong in trying to merge two regions.\n+ */\n+@InterfaceAudience.Public\n+@InterfaceStability.Stable\n+public class MergeRegionException extends RegionException {\n+\n+  private static final long serialVersionUID = 4970899110066124122L;\n+\n+  /** default constructor */\n+  public MergeRegionException() {\n+    super();\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param s message\n+   */\n+  public MergeRegionException(String s) {\n+    super(s);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "sha": "d4fe3d06325bfd4a8dcb0f8e801070e3b0b702c9",
                "status": "added"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "deletions": 8,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -69,16 +69,17 @@\n import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n import org.apache.hadoop.hbase.exceptions.PleaseHoldException;\n import org.apache.hadoop.hbase.exceptions.TableNotDisabledException;\n import org.apache.hadoop.hbase.exceptions.TableNotFoundException;\n import org.apache.hadoop.hbase.exceptions.UnknownProtocolException;\n import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n import org.apache.hadoop.hbase.executor.ExecutorService;\n import org.apache.hadoop.hbase.executor.ExecutorType;\n-import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.RpcServer;\n import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;\n+import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerRpcController;\n import org.apache.hadoop.hbase.ipc.SimpleRpcScheduler;\n import org.apache.hadoop.hbase.master.balancer.BalancerChore;\n@@ -1525,16 +1526,28 @@ public DispatchMergingRegionsResponse dispatchMergingRegions(\n               : encodedNameOfRegionB)));\n     }\n \n-    if (!forcible && !HRegionInfo.areAdjacent(regionStateA.getRegion(),\n-            regionStateB.getRegion())) {\n-      throw new ServiceException(\"Unable to merge not adjacent regions \"\n-          + regionStateA.getRegion().getRegionNameAsString() + \", \"\n-          + regionStateB.getRegion().getRegionNameAsString()\n-          + \" where forcible = \" + forcible);\n+    if (!regionStateA.isOpened() || !regionStateB.isOpened()) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge regions not online \" + regionStateA + \", \" + regionStateB));\n+    }\n+\n+    HRegionInfo regionInfoA = regionStateA.getRegion();\n+    HRegionInfo regionInfoB = regionStateB.getRegion();\n+    if (regionInfoA.compareTo(regionInfoB) == 0) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge a region to itself \" + regionInfoA + \", \" + regionInfoB));\n+    }\n+\n+    if (!forcible && !HRegionInfo.areAdjacent(regionInfoA, regionInfoB)) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge not adjacent regions \"\n+          + regionInfoA.getRegionNameAsString() + \", \"\n+          + regionInfoB.getRegionNameAsString()\n+          + \" where forcible = \" + forcible));\n     }\n \n     try {\n-      dispatchMergingRegions(regionStateA.getRegion(), regionStateB.getRegion(), forcible);\n+      dispatchMergingRegions(regionInfoA, regionInfoB, forcible);\n     } catch (IOException ioe) {\n       throw new ServiceException(ioe);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "5c491735c9d319c23448daa1823c037a5cc7ff81",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "patch": "@@ -20,12 +20,14 @@\n \n import java.io.IOException;\n import java.io.InterruptedIOException;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.RegionLoad;\n+import org.apache.hadoop.hbase.ServerLoad;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.exceptions.RegionOpeningException;\n import org.apache.hadoop.hbase.executor.EventHandler;\n@@ -34,6 +36,7 @@\n import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.master.RegionPlan;\n import org.apache.hadoop.hbase.master.RegionStates;\n+import org.apache.hadoop.hbase.master.ServerManager;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n \n /**\n@@ -98,12 +101,8 @@ public void process() throws IOException {\n       // Move region_b to region a's location, switch region_a and region_b if\n       // region_a's load lower than region_b's, so we will always move lower\n       // load region\n-      RegionLoad loadOfRegionA = masterServices.getServerManager()\n-          .getLoad(region_a_location).getRegionsLoad()\n-          .get(region_a.getRegionName());\n-      RegionLoad loadOfRegionB = masterServices.getServerManager()\n-          .getLoad(region_b_location).getRegionsLoad()\n-          .get(region_b.getRegionName());\n+      RegionLoad loadOfRegionA = getRegionLoad(region_a_location, region_a);\n+      RegionLoad loadOfRegionB = getRegionLoad(region_b_location, region_b);\n       if (loadOfRegionA != null && loadOfRegionB != null\n           && loadOfRegionA.getRequestsCount() < loadOfRegionB\n               .getRequestsCount()) {\n@@ -174,4 +173,16 @@ public void process() throws IOException {\n           + (EnvironmentEdgeManager.currentTimeMillis() - startTime) + \"ms\");\n     }\n   }\n+\n+  private RegionLoad getRegionLoad(ServerName sn, HRegionInfo hri) {\n+    ServerManager serverManager =  masterServices.getServerManager();\n+    ServerLoad load = serverManager.getLoad(sn);\n+    if (load != null) {\n+      Map<byte[], RegionLoad> regionsLoad = load.getRegionsLoad();\n+      if (regionsLoad != null) {\n+        return regionsLoad.get(hri.getRegionName());\n+      }\n+    }\n+    return null;\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "sha": "0a8c6f84cdc6b454fdd8b563b0ca69bc872133d3",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "patch": "@@ -21,6 +21,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.IOException;\n import java.util.List;\n@@ -43,7 +44,10 @@\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n+import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Pair;\n import org.junit.AfterClass;\n@@ -190,6 +194,58 @@ public void testCleanMergeReference() throws Exception {\n     }\n   }\n \n+  /**\n+   * This test tests 1, merging region not online;\n+   * 2, merging same two regions; 3, merging unknown regions.\n+   * They are in one test case so that we don't have to create\n+   * many tables, and these tests are simple.\n+   */\n+  @Test\n+  public void testMerge() throws Exception {\n+    LOG.info(\"Starting testMerge\");\n+    final byte[] tableName = Bytes.toBytes(\"testMerge\");\n+\n+    try {\n+      // Create table and load data.\n+      HTable table = createTableAndLoadData(master, tableName);\n+      RegionStates regionStates = master.getAssignmentManager().getRegionStates();\n+      List<HRegionInfo> regions = regionStates.getRegionsOfTable(tableName);\n+      // Fake offline one region\n+      HRegionInfo a = regions.get(0);\n+      HRegionInfo b = regions.get(1);\n+      regionStates.regionOffline(a);\n+      try {\n+        // Merge offline region. Region a is offline here\n+        admin.mergeRegions(a.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), false);\n+        fail(\"Offline regions should not be able to merge\");\n+      } catch (IOException ie) {\n+        assertTrue(\"Exception should mention regions not online\",\n+          ie.getMessage().contains(\"regions not online\")\n+            && ie instanceof MergeRegionException);\n+      }\n+      try {\n+        // Merge the same region: b and b.\n+        admin.mergeRegions(b.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), true);\n+        fail(\"A region should not be able to merge with itself, even forcifully\");\n+      } catch (IOException ie) {\n+        assertTrue(\"Exception should mention regions not online\",\n+          ie.getMessage().contains(\"region to itself\")\n+            && ie instanceof MergeRegionException);\n+      }\n+      try {\n+        // Merge unknown regions\n+        admin.mergeRegions(Bytes.toBytes(\"-f1\"), Bytes.toBytes(\"-f2\"), true);\n+        fail(\"Unknown region could not be merged\");\n+      } catch (IOException ie) {\n+        assertTrue(\"UnknownRegionException should be thrown\",\n+          ie instanceof UnknownRegionException);\n+      }\n+      table.close();\n+    } finally {\n+      TEST_UTIL.deleteTable(tableName);\n+    }\n+  }\n+\n   private void mergeRegionsAndVerifyRegionNum(HMaster master, byte[] tablename,\n       int regionAnum, int regionBnum, int expectedRegionNum) throws Exception {\n     requestMergeRegion(master, tablename, regionAnum, regionBnum);",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "sha": "49d77873052ed44cce4ad5847b04ee0f9e7bdeb3",
                "status": "modified"
            }
        ],
        "message": "HBASE-9044 Merging regions throws NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1507500 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/ff5fdb671f0e0d9fc76b6ead1800188569fb2348",
        "patched_files": [
            "HMaster.java",
            "HBaseAdmin.java",
            "DispatchMergingRegionHandler.java",
            "MergeRegionException.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRegionMergeTransactionOnCluster.java"
        ]
    },
    "hbase_39330cf": {
        "bug_id": "hbase_39330cf",
        "commit": "https://github.com/apache/hbase/commit/39330cff754d9b088523703409698ec2b180df79",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -782,6 +782,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3356  Add more checks in replication if RS is stopped\n    HBASE-3358  Recovered replication queue wait on themselves when terminating\n    HBASE-3359  LogRoller not added as a WAL listener when replication is enabled\n+   HBASE-3360  ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/CHANGES.txt",
                "sha": "8407559c5deb9ec9fd5c22b3dbcde9a5818d0eea",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -361,6 +361,9 @@\n   /** HBCK special code name used as server name when manipulating ZK nodes */\n   public static final String HBCK_CODE_NAME = \"HBCKServerName\";\n \n+  public static final String HBASE_MASTER_LOGCLEANER_PLUGINS =\n+      \"hbase.master.logcleaner.plugins\";\n+\n   private HConstants() {\n     // Can't be instantiated with this ctor.\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "a44a0b958df879d971799d1822b8de455d2e5167",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -74,6 +74,7 @@\n import org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler;\n import org.apache.hadoop.hbase.master.metrics.MasterMetrics;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.replication.regionserver.Replication;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.InfoServer;\n import org.apache.hadoop.hbase.util.Pair;\n@@ -203,6 +204,8 @@ public HMaster(final Configuration conf)\n     // set the thread name now we have an address\n     setName(MASTER + \"-\" + this.address);\n \n+    Replication.decorateMasterConfiguration(this.conf);\n+\n     this.rpcServer.startThreads();\n \n     // Hack! Maps DFSClient => Master for logs.  HDFS made this",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "18f7787e1e199b4d2955bf6d0637b859e2371440",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "patch": "@@ -34,6 +34,8 @@\n import org.apache.hadoop.hbase.Stoppable;\n import org.apache.hadoop.hbase.regionserver.wal.HLog;\n \n+import static org.apache.hadoop.hbase.HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS;\n+\n /**\n  * This Chore, everytime it runs, will clear the wal logs in the old logs folder\n  * that are deletable for each log cleaner in the chain, in order to limit the\n@@ -79,7 +81,7 @@ public LogCleaner(final int p, final Stoppable s,\n    * ReplicationLogCleaner and SnapshotLogCleaner.\n    */\n   private void initLogCleanersChain() {\n-    String[] logCleaners = conf.getStrings(\"hbase.master.logcleaner.plugins\");\n+    String[] logCleaners = conf.getStrings(HBASE_MASTER_LOGCLEANER_PLUGINS);\n     if (logCleaners != null) {\n       for (String className : logCleaners) {\n         LogCleanerDelegate logCleaner = newLogCleaner(className, conf);",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "sha": "f4252a62ab4bb9a8afeeaa2fb6f9803234f2d470",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.Server;\n@@ -36,9 +35,14 @@\n import org.apache.hadoop.hbase.regionserver.wal.WALEdit;\n import org.apache.hadoop.hbase.regionserver.wal.WALObserver;\n import org.apache.hadoop.hbase.replication.ReplicationZookeeper;\n+import org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.zookeeper.KeeperException;\n \n+import static org.apache.hadoop.hbase.HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS;\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_ENABLE_KEY;\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL;\n+\n /**\n  * Gateway to Replication.  Used by {@link org.apache.hadoop.hbase.regionserver.HRegionServer}.\n  */\n@@ -82,7 +86,7 @@ public Replication(final Server server, final FileSystem fs,\n    * @return True if replication is enabled.\n    */\n   public static boolean isReplication(final Configuration c) {\n-    return c.getBoolean(HConstants.REPLICATION_ENABLE_KEY, false);\n+    return c.getBoolean(REPLICATION_ENABLE_KEY, false);\n   }\n \n   /**\n@@ -134,7 +138,7 @@ public void visitLogEntryBeforeWrite(HRegionInfo info, HLogKey logKey,\n     for (KeyValue kv : logEdit.getKeyValues()) {\n       family = kv.getFamily();\n       int scope = info.getTableDesc().getFamily(family).getScope();\n-      if (scope != HConstants.REPLICATION_SCOPE_LOCAL &&\n+      if (scope != REPLICATION_SCOPE_LOCAL &&\n           !scopes.containsKey(family)) {\n         scopes.put(family, scope);\n       }\n@@ -149,6 +153,22 @@ public void logRolled(Path p) {\n     getReplicationManager().logRolled(p);\n   }\n \n+  /**\n+   * This method modifies the master's configuration in order to inject\n+   * replication-related features\n+   * @param conf\n+   */\n+  public static void decorateMasterConfiguration(Configuration conf) {\n+    if (!isReplication(conf)) {\n+      return;\n+    }\n+    String plugins = conf.get(HBASE_MASTER_LOGCLEANER_PLUGINS);\n+    if (!plugins.contains(ReplicationLogCleaner.class.toString())) {\n+      conf.set(HBASE_MASTER_LOGCLEANER_PLUGINS,\n+          plugins + \",\" + ReplicationLogCleaner.class.getCanonicalName());\n+    }\n+  }\n+\n   @Override\n   public void logRollRequested() {\n     // Not interested",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "sha": "1a87947ba251d42fc0a8d0011dc1c413df0ba666",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/resources/hbase-default.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/resources/hbase-default.xml?ref=39330cff754d9b088523703409698ec2b180df79",
                "deletions": 1,
                "filename": "src/main/resources/hbase-default.xml",
                "patch": "@@ -290,7 +290,7 @@\n   </property>\n   <property>\n     <name>hbase.master.logcleaner.plugins</name>\n-    <value>org.apache.hadoop.hbase.master.TimeToLiveLogCleaner,org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner</value>\n+    <value>org.apache.hadoop.hbase.master.TimeToLiveLogCleaner</value>\n     <description>A comma-separated list of LogCleanerDelegate invoked by\n     the LogsCleaner service. These WAL/HLog cleaners are called in order,\n     so put the HLog cleaner that prunes the most HLog files in front. To",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/resources/hbase-default.xml",
                "sha": "f1cc4ae0f8557c552a44dd246824e0efd72b5325",
                "status": "modified"
            }
        ],
        "message": "HBASE-3360  ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1049680 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/1842a1ee8573b356e4140bf7b47a7102461aacd1",
        "patched_files": [
            "Replication.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "ReplicationTests.java"
        ]
    },
    "hbase_3a652ba": {
        "bug_id": "hbase_3a652ba",
        "commit": "https://github.com/apache/hbase/commit/3a652ba41c86aa566ea5377234611888da5a8bd8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java?ref=3a652ba41c86aa566ea5377234611888da5a8bd8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "patch": "@@ -927,7 +927,9 @@ public void stop(String why) {\n       restoreHandler.cancel(why);\n     }\n     try {\n-      coordinator.close();\n+      if (coordinator != null) {\n+        coordinator.close();\n+      }\n     } catch (IOException e) {\n       LOG.error(\"stop ProcedureCoordinator error\", e);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "sha": "b7a891d01bab67bb82d92c1e9931b386a5628ff8",
                "status": "modified"
            }
        ],
        "message": "HBASE-12692 NPE from SnapshotManager#stop (Ashish Singhi)",
        "parent": "https://github.com/apache/hbase/commit/afb753ecc3a94a5824a510121aa186948fb317df",
        "patched_files": [
            "SnapshotManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSnapshotManager.java"
        ]
    },
    "hbase_3bb8daa": {
        "bug_id": "hbase_3bb8daa",
        "commit": "https://github.com/apache/hbase/commit/3bb8daa60565ec2f7955352e52c2f6379176d8c6",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java?ref=3bb8daa60565ec2f7955352e52c2f6379176d8c6",
                "deletions": 4,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "patch": "@@ -94,7 +94,6 @@\n   private RegionInfo daughter_1_RI;\n   private RegionInfo daughter_2_RI;\n   private byte[] bestSplitRow;\n-  private TableDescriptor htd;\n   private RegionSplitPolicy splitPolicy;\n \n   public SplitTableRegionProcedure() {\n@@ -120,14 +119,14 @@ public SplitTableRegionProcedure(final MasterProcedureEnv env,\n         .setSplit(false)\n         .setRegionId(rid)\n         .build();\n-    this.htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n-    if(this.htd.getRegionSplitPolicyClassName() != null) {\n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n+    if(htd.getRegionSplitPolicyClassName() != null) {\n       // Since we don't have region reference here, creating the split policy instance without it.\n       // This can be used to invoke methods which don't require Region reference. This instantiation\n       // of a class on Master-side though it only makes sense on the RegionServer-side is\n       // for Phoenix Local Indexing. Refer HBASE-12583 for more information.\n       Class<? extends RegionSplitPolicy> clazz =\n-          RegionSplitPolicy.getSplitPolicyClass(this.htd, env.getMasterConfiguration());\n+          RegionSplitPolicy.getSplitPolicyClass(htd, env.getMasterConfiguration());\n       this.splitPolicy = ReflectionUtils.newInstance(clazz, env.getMasterConfiguration());\n     }\n   }\n@@ -611,6 +610,7 @@ public void createDaughterRegions(final MasterProcedureEnv env) throws IOExcepti\n       maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n     final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n \n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n     // Split each store file.\n     for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n       byte [] familyName = Bytes.toBytes(e.getKey());",
                "raw_url": "https://github.com/apache/hbase/raw/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "sha": "be0741d87305803f6448fd68ecfc8ff786682b83",
                "status": "modified"
            }
        ],
        "message": "HBASE-19939 Fixed NPE in tests TestSplitTableRegionProcedure#testSplitWithoutPONR() and testRecoveryAndDoubleExecution()\n\nValue of 'htd' is null as it is initialized in the constructor but when the object is deserialized its null. Got rid of member variable htd and made it local to method.",
        "parent": "https://github.com/apache/hbase/commit/f5197979aaac7e36b6af36b86ea8dc8d7774fabe",
        "patched_files": [
            "SplitTableRegionProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitTableRegionProcedure.java"
        ]
    },
    "hbase_3cb1168": {
        "bug_id": "hbase_3cb1168",
        "commit": "https://github.com/apache/hbase/commit/3cb1168601bee83e82a21de7c359d70d27c8c767",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -279,6 +279,8 @@ Release 0.21.0 - Unreleased\n                is a prefix (Todd Lipcon via Stack)\n    HBASE-2463  Various Bytes.* functions silently ignore invalid arguments\n                (Benoit Sigoure via Stack)\n+   HBASE-2443  IPC client can throw NPE if socket creation fails\n+               (Todd Lipcon via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "sha": "fa5c3828acc854ca8071a0a6356abe21368c0c35",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "deletions": 4,
                "filename": "core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -351,10 +351,12 @@ protected synchronized void setupIOstreams() throws IOException {\n     private void handleConnectionFailure(\n         int curRetries, int maxRetries, IOException ioe) throws IOException {\n       // close the current connection\n-      try {\n-        socket.close();\n-      } catch (IOException e) {\n-        LOG.warn(\"Not able to close a socket\", e);\n+      if (socket != null) { // could be null if the socket creation failed\n+        try {\n+          socket.close();\n+        } catch (IOException e) {\n+          LOG.warn(\"Not able to close a socket\", e);\n+        }\n       }\n       // set socket to null so that the next call to setupIOstreams\n       // can start the process of connect all over again.",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "359a5b1b87d942fd18915c72729d94702b058217",
                "status": "modified"
            }
        ],
        "message": "HBASE-2443 IPC client can throw NPE if socket creation fails\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@936107 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2b2f1d1670040f2e0d27921a3f002099ff6ba447",
        "patched_files": [
            "HBaseClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_3ccfd50": {
        "bug_id": "hbase_3ccfd50",
        "commit": "https://github.com/apache/hbase/commit/3ccfd50bd937571aeed3033561b7ca52c967f105",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=3ccfd50bd937571aeed3033561b7ca52c967f105",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "patch": "@@ -492,7 +492,7 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+          sink.updateReadFailures(region.getRegionNameAsString(),\n               serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "sha": "4f59cf3328463517244f41c3ee0ff38fb3c3a6d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nAddendum to fix findbugs complaint.",
        "parent": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "patched_files": [
            "CanaryTool.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCanaryTool.java"
        ]
    },
    "hbase_3d156a5": {
        "bug_id": "hbase_3d156a5",
        "commit": "https://github.com/apache/hbase/commit/3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -339,6 +339,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4494  AvroServer:: get fails with NPE on a non-existent row\n                (Kay Kay)\n    HBASE-4481  TestMergeTool failed in 0.92 build 20\n+   HBASE-4386  Fix a potential NPE in TaskMonitor (todd)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "sha": "295513df4de5e8663e2a93162d828c662e387163",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "patch": "@@ -72,7 +72,9 @@ public MonitoredTask createStatus(String description) {\n         new Class<?>[] { MonitoredTask.class },\n         new PassthroughInvocationHandler<MonitoredTask>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -84,7 +86,9 @@ public MonitoredRPCHandler createRPCStatus(String description) {\n         new Class<?>[] { MonitoredRPCHandler.class },\n         new PassthroughInvocationHandler<MonitoredRPCHandler>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -142,7 +146,7 @@ private boolean canPurge(MonitoredTask stat) {\n   public void dumpAsText(PrintWriter out) {\n     long now = System.currentTimeMillis();\n     \n-    List<MonitoredTask> tasks = TaskMonitor.get().getTasks();\n+    List<MonitoredTask> tasks = getTasks();\n     for (MonitoredTask task : tasks) {\n       out.println(\"Task: \" + task.getDescription());\n       out.println(\"Status: \" + task.getState() + \":\" + task.getStatus());",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "sha": "fc9c8301e470eda48495bcb8cd64954c32e3a56c",
                "status": "modified"
            }
        ],
        "message": "HBASE-4386  Fix a potential NPE in TaskMonitor\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1179479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/ead9159ecd6b82b27dafce0ee3891d69444c1704",
        "patched_files": [
            "TaskMonitor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTaskMonitor.java"
        ]
    },
    "hbase_3e7b90a": {
        "bug_id": "hbase_3e7b90a",
        "commit": "https://github.com/apache/hbase/commit/3e7b90ac6d808c171ea988d8d32ef998146713ac",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=3e7b90ac6d808c171ea988d8d32ef998146713ac",
                "deletions": 1,
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -406,7 +406,7 @@ protected static HTableDescriptor getTableDescriptor(TestOptions opts) {\n     if (opts.replicas != DEFAULT_OPTS.replicas) {\n       desc.setRegionReplication(opts.replicas);\n     }\n-    if (opts.splitPolicy != DEFAULT_OPTS.splitPolicy) {\n+    if (opts.splitPolicy != null && !opts.splitPolicy.equals(DEFAULT_OPTS.splitPolicy)) {\n       desc.setRegionSplitPolicyClassName(opts.splitPolicy);\n     }\n     return desc;",
                "raw_url": "https://github.com/apache/hbase/raw/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "8255573a2bc614f46904c4f3b4acf87abca08b82",
                "status": "modified"
            }
        ],
        "message": "HBASE-19445 PerformanceEvaluation NPE processing split policy option",
        "parent": "https://github.com/apache/hbase/commit/00750fe79acbb6a43daa62b9fcabbd1f5ce4cf6c",
        "patched_files": [
            "PerformanceEvaluation.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_3f5229c": {
        "bug_id": "hbase_3f5229c",
        "commit": "https://github.com/apache/hbase/commit/3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -28,3 +28,4 @@ Trunk (unreleased changes)\n  15. HADOOP-1421 Failover detection, split log files.\n      For the files modified, also clean up javadoc, class, field and method \n      visibility (HADOOP-1466)\n+ 16. HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "sha": "2c20370f465fbbfe06d2c4b7bfa76d29ea995c2e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -835,7 +835,7 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n    * If 'numVersions' is negative, the method returns all available versions.\n    */\n   BytesWritable[] get(HStoreKey key, int numVersions) throws IOException {\n-    if(numVersions <= 0) {\n+    if (numVersions <= 0) {\n       throw new IllegalArgumentException(\"Number of versions must be > 0\");\n     }\n     \n@@ -852,15 +852,19 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n           BytesWritable readval = new BytesWritable();\n           map.reset();\n           HStoreKey readkey = (HStoreKey)map.getClosest(key, readval);\n-          \n-          if(readkey.matchesRowCol(key)) {\n+          if (readkey == null) {\n+            // map.getClosest returns null if the passed key is > than the\n+            // last key in the map file.  getClosest is a bit of a misnomer\n+            // since it returns exact match or the next closest key AFTER not\n+            // BEFORE.\n+            continue;\n+          }\n+          if (readkey.matchesRowCol(key)) {\n             results.add(readval);\n             readval = new BytesWritable();\n-\n             while(map.next(readkey, readval) && readkey.matchesRowCol(key)) {\n-              if(numVersions > 0 && (results.size() >= numVersions)) {\n+              if (numVersions > 0 && (results.size() >= numVersions)) {\n                 break;\n-                \n               }\n               results.add(readval);\n               readval = new BytesWritable();",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "a627bede8d36a81ec8ef42d941fe41b294a7e783",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@546275 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/09cf0a100f19517653806511b6b64e02fe5d31f8",
        "patched_files": [
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_433f7a3": {
        "bug_id": "hbase_433f7a3",
        "commit": "https://github.com/apache/hbase/commit/433f7a3ff6d7e18b44202cc950478cfac49daae0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -677,6 +677,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3219  Split parents are reassigned on restart and on disable/enable\n    HBASE-3222  Regionserver region listing in UI is no longer ordered\n    HBASE-3221  Race between splitting and disabling\n+   HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "sha": "b0d353359652589ca164f65a58ab90f0e1320941",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/docbkx/book.xml?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "src/docbkx/book.xml",
                "patch": "@@ -508,6 +508,15 @@ index e70ebc6..96f8c27 100644\n       </section>\n \n       <section xml:id=\"recommended_configurations\"><title>Recommended Configuations</title>\n+      <section xml:id=\"big_memory\">\n+        <title>Configuration for large memory machines</title>\n+        <para>\n+          HBase ships with a reasonable configuration that will work on nearly all\n+          machine types that people might want to test with. If you have larger\n+          machines you might the following configuration options helpful.\n+        </para>\n+\n+      </section>\n       <section xml:id=\"lzo\">\n       <title>LZO compression</title>\n       <para>You should consider enabling LZO compression.  Its",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "sha": "056520122e0afbed171032d8f770dd249ccc8949",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "patch": "@@ -229,6 +229,10 @@ public int reseekTo(byte[] key, int offset, int length)\n             return 1;\n           }\n         }\n+        if (atEnd) {\n+          // skip the 'reseek' and just return 1.\n+          return 1;\n+        }\n         return delegate.reseekTo(key, offset, length);\n       }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "sha": "40be64977075fbbe5856a09d6afd68a383876859",
                "status": "modified"
            }
        ],
        "message": "HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1034229 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/b5086781068232ace42d4e9687e2eb351a52cced",
        "patched_files": [
            "HalfStoreFileReader.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_4350632": {
        "bug_id": "hbase_4350632",
        "commit": "https://github.com/apache/hbase/commit/43506320a1bb6ca2193162edfb5dee21fffc08a9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "patch": "@@ -70,6 +70,7 @@ public void loadFiles(List<StoreFile> storeFiles) {\n \n   @Override\n   public final Collection<StoreFile> getStorefiles() {\n+    // TODO: I can return a null list of StoreFiles? That'll mess up clients. St.Ack 20151111\n     return storefiles;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "sha": "9f38b9eb00dc344a71054484d8df76c182975b96",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "deletions": 20,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -960,32 +960,34 @@ private void initializeWarmup(final CancelableProgressable reporter) throws IOEx\n     initializeStores(reporter, status);\n   }\n \n-  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n+  /**\n+   * @return Map of StoreFiles by column family\n+   */\n+  private NavigableMap<byte[], List<Path>> getStoreFiles() {\n+    NavigableMap<byte[], List<Path>> allStoreFiles =\n+      new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n     for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      List<Path> storeFileNames = new ArrayList<Path>();\n+      for (StoreFile storeFile: storeFiles) {\n         storeFileNames.add(storeFile.getPath());\n       }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n+      allStoreFiles.put(store.getFamily().getName(), storeFileNames);\n     }\n+    return allStoreFiles;\n+  }\n \n+  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionOpenDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_OPEN, getRegionInfo(), openSeqId,\n       getRegionServerServices().getServerName(), storeFiles);\n     WALUtil.writeRegionEventMarker(wal, getTableDesc(), getRegionInfo(), regionOpenDesc, mvcc);\n   }\n \n   private void writeRegionCloseMarker(WAL wal) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n-    for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n-        storeFileNames.add(storeFile.getPath());\n-      }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n-    }\n-\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionEventDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_CLOSE, getRegionInfo(), mvcc.getReadPoint(),\n       getRegionServerServices().getServerName(), storeFiles);\n@@ -1016,7 +1018,9 @@ public HDFSBlocksDistribution getHDFSBlocksDistribution() {\n       new HDFSBlocksDistribution();\n     synchronized (this.stores) {\n       for (Store store : this.stores.values()) {\n-        for (StoreFile sf : store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           HDFSBlocksDistribution storeFileBlocksDistribution =\n             sf.getHDFSBlockDistribution();\n           hdfsBlocksDistribution.add(storeFileBlocksDistribution);\n@@ -1059,7 +1063,6 @@ public static HDFSBlocksDistribution computeHDFSBlocksDistribution(final Configu\n     for (HColumnDescriptor family: tableDescriptor.getFamilies()) {\n       Collection<StoreFileInfo> storeFiles = regionFs.getStoreFiles(family.getNameAsString());\n       if (storeFiles == null) continue;\n-\n       for (StoreFileInfo storeFileInfo : storeFiles) {\n         try {\n           hdfsBlocksDistribution.add(storeFileInfo.computeHDFSBlocksDistribution(fs));\n@@ -1639,10 +1642,16 @@ public long getEarliestFlushTimeForAllStores() {\n   public long getOldestHfileTs(boolean majorCompactioOnly) throws IOException {\n     long result = Long.MAX_VALUE;\n     for (Store store : getStores()) {\n-      for (StoreFile file : store.getStorefiles()) {\n-        HFile.Reader reader = file.getReader().getHFileReader();\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      for (StoreFile file : storeFiles) {\n+        StoreFile.Reader sfReader = file.getReader();\n+        if (sfReader == null) continue;\n+        HFile.Reader reader = sfReader.getHFileReader();\n+        if (reader == null) continue;\n         if (majorCompactioOnly) {\n           byte[] val = reader.loadFileInfo().get(StoreFile.MAJOR_COMPACTION_KEY);\n+          if (val == null) continue;\n           if (val == null || !Bytes.toBoolean(val)) {\n             continue;\n           }\n@@ -4898,7 +4907,9 @@ private void logRegionFiles() {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(getRegionInfo().getEncodedName() + \" : Store files for region: \");\n       for (Store s : stores.values()) {\n-        for (StoreFile sf : s.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = s.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           LOG.trace(getRegionInfo().getEncodedName() + \" : \" + sf);\n         }\n       }\n@@ -5006,7 +5017,9 @@ private Store getStore(Cell cell) {\n           throw new IllegalArgumentException(\"No column family : \" +\n               new String(column) + \" available\");\n         }\n-        for (StoreFile storeFile: store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile storeFile: storeFiles) {\n           storeFileNames.add(storeFile.getPath().toString());\n         }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "994270b447590dd17e4d8e7909a107f08de4512c",
                "status": "modified"
            }
        ],
        "message": "HBASE-14798 NPE reporting server load causes regionserver abort; causes TestAcidGuarantee to fail",
        "parent": "https://github.com/apache/hbase/commit/1fa7b71cf82cc30757ecf5d2a8e0cfba654ed469",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_44ca13f": {
        "bug_id": "hbase_44ca13f",
        "commit": "https://github.com/apache/hbase/commit/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 7,
                "filename": "hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "patch": "@@ -486,22 +486,34 @@ message TransitPeerSyncReplicationStateStateData {\n \n enum RecoverStandbyState {\n   RENAME_SYNC_REPLICATION_WALS_DIR = 1;\n-  INIT_WORKERS = 2;\n-  DISPATCH_TASKS = 3;\n-  SNAPSHOT_SYNC_REPLICATION_WALS_DIR = 4;\n+  REGISTER_PEER_TO_WORKER_STORAGE = 2;\n+  DISPATCH_WALS = 3;\n+  UNREGISTER_PEER_FROM_WORKER_STORAGE = 4;\n+  SNAPSHOT_SYNC_REPLICATION_WALS_DIR = 5;\n+}\n+\n+enum SyncReplicationReplayWALState {\n+  ASSIGN_WORKER = 1;\n+  DISPATCH_WALS_TO_WORKER = 2;\n+  RELEASE_WORKER = 3;\n }\n \n message RecoverStandbyStateData {\n+  required bool serial  = 1;\n+}\n+\n+message SyncReplicationReplayWALStateData {\n   required string peer_id = 1;\n+  repeated string wal = 2;\n }\n \n-message ReplaySyncReplicationWALStateData {\n+message SyncReplicationReplayWALRemoteStateData {\n   required string peer_id = 1;\n-  required string wal = 2;\n-  optional ServerName target_server = 3;\n+  repeated string wal = 2;\n+  required ServerName target_server = 3;\n }\n \n message ReplaySyncReplicationWALParameter {\n   required string peer_id = 1;\n-  required string wal = 2;\n+  repeated string wal = 2;\n }",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "sha": "a062e9a8193369d35bf75c240287bf1db1a9bcfd",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -138,8 +138,8 @@\n import org.apache.hadoop.hbase.master.replication.DisablePeerProcedure;\n import org.apache.hadoop.hbase.master.replication.EnablePeerProcedure;\n import org.apache.hadoop.hbase.master.replication.RemovePeerProcedure;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure;\n import org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n@@ -343,7 +343,7 @@ public void run() {\n   // manager of replication\n   private ReplicationPeerManager replicationPeerManager;\n \n-  private ReplaySyncReplicationWALManager replaySyncReplicationWALManager;\n+  private SyncReplicationReplayWALManager syncReplicationReplayWALManager;\n \n   // buffer for \"fatal error\" notices from region servers\n   // in the cluster. This is only used for assisting\n@@ -754,6 +754,7 @@ protected void initializeZKBasedSystemTrackers()\n     this.splitOrMergeTracker.start();\n \n     this.replicationPeerManager = ReplicationPeerManager.create(zooKeeper, conf);\n+    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n \n     this.drainingServerTracker = new DrainingServerTracker(zooKeeper, this, this.serverManager);\n     this.drainingServerTracker.start();\n@@ -852,7 +853,6 @@ private void finishActiveMasterInitialization(MonitoredTask status) throws IOExc\n     initializeMemStoreChunkCreator();\n     this.fileSystemManager = new MasterFileSystem(conf);\n     this.walManager = new MasterWalManager(this);\n-    this.replaySyncReplicationWALManager = new ReplaySyncReplicationWALManager(this);\n \n     // enable table descriptors cache\n     this.tableDescriptors.setCacheOn();\n@@ -3764,7 +3764,7 @@ public SnapshotQuotaObserverChore getSnapshotQuotaObserverChore() {\n   }\n \n   @Override\n-  public ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager() {\n-    return this.replaySyncReplicationWALManager;\n+  public SyncReplicationReplayWALManager getSyncReplicationReplayWALManager() {\n+    return this.syncReplicationReplayWALManager;\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "dc627523569b91f4285a18890dc45058ee33fc46",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "patch": "@@ -38,8 +38,8 @@\n import org.apache.hadoop.hbase.master.locking.LockManager;\n import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;\n import org.apache.hadoop.hbase.procedure2.LockedResource;\n@@ -462,9 +462,9 @@ ReplicationPeerConfig getReplicationPeerConfig(String peerId) throws Replication\n   ReplicationPeerManager getReplicationPeerManager();\n \n   /**\n-   * Returns the {@link ReplaySyncReplicationWALManager}.\n+   * Returns the {@link SyncReplicationReplayWALManager}.\n    */\n-  ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager();\n+  SyncReplicationReplayWALManager getSyncReplicationReplayWALManager();\n \n   /**\n    * Update the peerConfig for the specified peer",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "sha": "7b0c56a924ee6ad797a973557161d25bf0eb4486",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "patch": "@@ -207,7 +207,8 @@ protected Procedure dequeue() {\n       // check if the next procedure is still a child.\n       // if not, remove the rq from the fairq and go back to the xlock state\n       Procedure<?> nextProc = rq.peek();\n-      if (nextProc != null && !Procedure.haveSameParent(nextProc, pollResult)) {\n+      if (nextProc != null && !Procedure.haveSameParent(nextProc, pollResult)\n+          && nextProc.getRootProcId() != pollResult.getRootProcId()) {\n         removeFromRunQueue(fairq, rq);\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "sha": "8a28b84b8162f0a06992c35137631d40271011fd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "patch": "@@ -24,7 +24,7 @@\n \n   enum PeerOperationType {\n     ADD, REMOVE, ENABLE, DISABLE, UPDATE_CONFIG, REFRESH, TRANSIT_SYNC_REPLICATION_STATE,\n-    RECOVER_STANDBY, REPLAY_SYNC_REPLICATION_WAL\n+    RECOVER_STANDBY, SYNC_REPLICATION_REPLAY_WAL, SYNC_REPLICATION_REPLAY_WAL_REMOTE\n   }\n \n   String getPeerId();",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "sha": "0195ab9c66ada42c0383f9b8d916873bd8b69254",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "patch": "@@ -50,6 +50,7 @@ public boolean requireExclusiveLock(Procedure<?> proc) {\n \n   private static boolean requirePeerExclusiveLock(PeerProcedureInterface proc) {\n     return proc.getPeerOperationType() != PeerOperationType.REFRESH\n-        && proc.getPeerOperationType() != PeerOperationType.REPLAY_SYNC_REPLICATION_WAL;\n+        && proc.getPeerOperationType() != PeerOperationType.SYNC_REPLICATION_REPLAY_WAL\n+        && proc.getPeerOperationType() != PeerOperationType.SYNC_REPLICATION_REPLAY_WAL_REMOTE;\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "sha": "86d8e4341ab61328e797fb0db09630a799374f8f",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 16,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "patch": "@@ -18,60 +18,79 @@\n package org.apache.hadoop.hbase.master.replication;\n \n import java.io.IOException;\n+import java.util.Arrays;\n import java.util.List;\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;\n import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyStateData;\n \n @InterfaceAudience.Private\n public class RecoverStandbyProcedure extends AbstractPeerProcedure<RecoverStandbyState> {\n \n   private static final Logger LOG = LoggerFactory.getLogger(RecoverStandbyProcedure.class);\n \n+  private boolean serial;\n+\n   public RecoverStandbyProcedure() {\n   }\n \n-  public RecoverStandbyProcedure(String peerId) {\n+  public RecoverStandbyProcedure(String peerId, boolean serial) {\n     super(peerId);\n+    this.serial = serial;\n   }\n \n   @Override\n   protected Flow executeFromState(MasterProcedureEnv env, RecoverStandbyState state)\n       throws ProcedureSuspendedException, ProcedureYieldException, InterruptedException {\n-    ReplaySyncReplicationWALManager replaySyncReplicationWALManager =\n-        env.getMasterServices().getReplaySyncReplicationWALManager();\n+    SyncReplicationReplayWALManager syncReplicationReplayWALManager =\n+        env.getMasterServices().getSyncReplicationReplayWALManager();\n     switch (state) {\n       case RENAME_SYNC_REPLICATION_WALS_DIR:\n         try {\n-          replaySyncReplicationWALManager.renameToPeerReplayWALDir(peerId);\n+          syncReplicationReplayWALManager.renameToPeerReplayWALDir(peerId);\n         } catch (IOException e) {\n           LOG.warn(\"Failed to rename remote wal dir for peer id={}\", peerId, e);\n           setFailure(\"master-recover-standby\", e);\n           return Flow.NO_MORE_STATE;\n         }\n-        setNextState(RecoverStandbyState.INIT_WORKERS);\n+        setNextState(RecoverStandbyState.REGISTER_PEER_TO_WORKER_STORAGE);\n         return Flow.HAS_MORE_STATE;\n-      case INIT_WORKERS:\n-        replaySyncReplicationWALManager.initPeerWorkers(peerId);\n-        setNextState(RecoverStandbyState.DISPATCH_TASKS);\n+      case REGISTER_PEER_TO_WORKER_STORAGE:\n+        try {\n+          syncReplicationReplayWALManager.registerPeer(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.warn(\"Failed to register peer to worker storage for peer id={}, retry\", peerId, e);\n+          throw new ProcedureYieldException();\n+        }\n+        setNextState(RecoverStandbyState.DISPATCH_WALS);\n         return Flow.HAS_MORE_STATE;\n-      case DISPATCH_TASKS:\n-        addChildProcedure(getReplayWALs(replaySyncReplicationWALManager).stream()\n-            .map(wal -> new ReplaySyncReplicationWALProcedure(peerId,\n-                replaySyncReplicationWALManager.removeWALRootPath(wal)))\n-            .toArray(ReplaySyncReplicationWALProcedure[]::new));\n+      case DISPATCH_WALS:\n+        dispathWals(syncReplicationReplayWALManager);\n+        setNextState(RecoverStandbyState.UNREGISTER_PEER_FROM_WORKER_STORAGE);\n+        return Flow.HAS_MORE_STATE;\n+      case UNREGISTER_PEER_FROM_WORKER_STORAGE:\n+        try {\n+          syncReplicationReplayWALManager.unregisterPeer(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.warn(\"Failed to unregister peer from worker storage for peer id={}, retry\", peerId,\n+              e);\n+          throw new ProcedureYieldException();\n+        }\n         setNextState(RecoverStandbyState.SNAPSHOT_SYNC_REPLICATION_WALS_DIR);\n         return Flow.HAS_MORE_STATE;\n       case SNAPSHOT_SYNC_REPLICATION_WALS_DIR:\n         try {\n-          replaySyncReplicationWALManager.renameToPeerSnapshotWALDir(peerId);\n+          syncReplicationReplayWALManager.renameToPeerSnapshotWALDir(peerId);\n         } catch (IOException e) {\n           LOG.warn(\"Failed to cleanup replay wals dir for peer id={}, , retry\", peerId, e);\n           throw new ProcedureYieldException();\n@@ -82,10 +101,14 @@ protected Flow executeFromState(MasterProcedureEnv env, RecoverStandbyState stat\n     }\n   }\n \n-  private List<Path> getReplayWALs(ReplaySyncReplicationWALManager replaySyncReplicationWALManager)\n+  // TODO: dispatch wals by region server when serial is true and sort wals\n+  private void dispathWals(SyncReplicationReplayWALManager syncReplicationReplayWALManager)\n       throws ProcedureYieldException {\n     try {\n-      return replaySyncReplicationWALManager.getReplayWALsAndCleanUpUnusedFiles(peerId);\n+      List<Path> wals = syncReplicationReplayWALManager.getReplayWALsAndCleanUpUnusedFiles(peerId);\n+      addChildProcedure(wals.stream().map(wal -> new SyncReplicationReplayWALProcedure(peerId,\n+          Arrays.asList(syncReplicationReplayWALManager.removeWALRootPath(wal))))\n+          .toArray(SyncReplicationReplayWALProcedure[]::new));\n     } catch (IOException e) {\n       LOG.warn(\"Failed to get replay wals for peer id={}, , retry\", peerId, e);\n       throw new ProcedureYieldException();\n@@ -111,4 +134,17 @@ protected RecoverStandbyState getInitialState() {\n   public PeerOperationType getPeerOperationType() {\n     return PeerOperationType.RECOVER_STANDBY;\n   }\n+\n+  @Override\n+  protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    super.serializeStateData(serializer);\n+    serializer.serialize(RecoverStandbyStateData.newBuilder().setSerial(serial).build());\n+  }\n+\n+  @Override\n+  protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    super.deserializeStateData(serializer);\n+    RecoverStandbyStateData data = serializer.deserialize(RecoverStandbyStateData.class);\n+    serial = data.getSerial();\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "sha": "54947429d8b61f0861bfbb2719d4993ad99a4a9f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "patch": "@@ -67,11 +67,10 @@ protected void updatePeerStorage(MasterProcedureEnv env) throws ReplicationExcep\n   }\n \n   private void removeRemoteWALs(MasterProcedureEnv env) throws IOException {\n-    env.getMasterServices().getReplaySyncReplicationWALManager().removePeerRemoteWALs(peerId);\n+    env.getMasterServices().getSyncReplicationReplayWALManager().removePeerRemoteWALs(peerId);\n   }\n \n-  @Override\n-  protected void postPeerModification(MasterProcedureEnv env)\n+  @Override  protected void postPeerModification(MasterProcedureEnv env)\n       throws IOException, ReplicationException {\n     if (peerConfig.isSyncReplication()) {\n       removeRemoteWALs(env);",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "sha": "4b77c8dba46ba0620d8b52967549bdf21ebebe9c",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 28,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "patch": "@@ -17,23 +17,26 @@\n  */\n package org.apache.hadoop.hbase.master.replication;\n \n+import static org.apache.hadoop.hbase.replication.ReplicationUtils.REMOTE_WAL_REPLAY_SUFFIX;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerRemoteWALDir;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerReplayWALDir;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerSnapshotWALDir;\n \n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.LinkedBlockingQueue;\n-import java.util.concurrent.TimeUnit;\n+import java.util.Optional;\n+import java.util.Set;\n+\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n import org.apache.hadoop.hbase.replication.ReplicationUtils;\n import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -43,10 +46,10 @@\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n \n @InterfaceAudience.Private\n-public class ReplaySyncReplicationWALManager {\n+public class SyncReplicationReplayWALManager {\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(ReplaySyncReplicationWALManager.class);\n+      LoggerFactory.getLogger(SyncReplicationReplayWALManager.class);\n \n   private final MasterServices services;\n \n@@ -56,15 +59,67 @@\n \n   private final Path remoteWALDir;\n \n-  private final Map<String, BlockingQueue<ServerName>> availServers = new HashMap<>();\n+  private final ZKSyncReplicationReplayWALWorkerStorage workerStorage;\n+\n+  private final Map<String, Set<ServerName>> workers = new HashMap<>();\n \n-  public ReplaySyncReplicationWALManager(MasterServices services) {\n+  private final Object workerLock = new Object();\n+\n+  public SyncReplicationReplayWALManager(MasterServices services)\n+      throws IOException, ReplicationException {\n     this.services = services;\n     this.fs = services.getMasterFileSystem().getWALFileSystem();\n     this.walRootDir = services.getMasterFileSystem().getWALRootDir();\n     this.remoteWALDir = new Path(this.walRootDir, ReplicationUtils.REMOTE_WAL_DIR_NAME);\n+    this.workerStorage = new ZKSyncReplicationReplayWALWorkerStorage(services.getZooKeeper(),\n+        services.getConfiguration());\n+    checkReplayingWALDir();\n+  }\n+\n+  private void checkReplayingWALDir() throws IOException, ReplicationException {\n+    FileStatus[] files = fs.listStatus(remoteWALDir);\n+    for (FileStatus file : files) {\n+      String name = file.getPath().getName();\n+      if (name.endsWith(REMOTE_WAL_REPLAY_SUFFIX)) {\n+        String peerId = name.substring(0, name.length() - REMOTE_WAL_REPLAY_SUFFIX.length());\n+        workers.put(peerId, workerStorage.getPeerWorkers(peerId));\n+      }\n+    }\n+  }\n+\n+  public void registerPeer(String peerId) throws ReplicationException {\n+    workers.put(peerId, new HashSet<>());\n+    workerStorage.addPeer(peerId);\n+  }\n+\n+  public void unregisterPeer(String peerId) throws ReplicationException {\n+    workers.remove(peerId);\n+    workerStorage.removePeer(peerId);\n+  }\n+\n+  public ServerName getPeerWorker(String peerId) throws ReplicationException {\n+    Optional<ServerName> worker = Optional.empty();\n+    ServerName workerServer = null;\n+    synchronized (workerLock) {\n+      worker = services.getServerManager().getOnlineServers().keySet().stream()\n+          .filter(server -> !workers.get(peerId).contains(server)).findFirst();\n+      if (worker.isPresent()) {\n+        workerServer = worker.get();\n+        workers.get(peerId).add(workerServer);\n+      }\n+    }\n+    if (workerServer != null) {\n+      workerStorage.addPeerWorker(peerId, workerServer);\n+    }\n+    return workerServer;\n   }\n \n+  public void removePeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    synchronized (workerLock) {\n+      workers.get(peerId).remove(worker);\n+    }\n+    workerStorage.removePeerWorker(peerId, worker);\n+  }\n   public void createPeerRemoteWALDir(String peerId) throws IOException {\n     Path peerRemoteWALDir = getPeerRemoteWALDir(remoteWALDir, peerId);\n     if (!fs.exists(peerRemoteWALDir) && !fs.mkdirs(peerRemoteWALDir)) {\n@@ -77,23 +132,23 @@ private void rename(Path src, Path dst, String peerId) throws IOException {\n       deleteDir(dst, peerId);\n       if (!fs.rename(src, dst)) {\n         throw new IOException(\n-          \"Failed to rename dir from \" + src + \" to \" + dst + \" for peer id=\" + peerId);\n+            \"Failed to rename dir from \" + src + \" to \" + dst + \" for peer id=\" + peerId);\n       }\n       LOG.info(\"Renamed dir from {} to {} for peer id={}\", src, dst, peerId);\n     } else if (!fs.exists(dst)) {\n       throw new IOException(\n-        \"Want to rename from \" + src + \" to \" + dst + \", but they both do not exist\");\n+          \"Want to rename from \" + src + \" to \" + dst + \", but they both do not exist\");\n     }\n   }\n \n   public void renameToPeerReplayWALDir(String peerId) throws IOException {\n     rename(getPeerRemoteWALDir(remoteWALDir, peerId), getPeerReplayWALDir(remoteWALDir, peerId),\n-      peerId);\n+        peerId);\n   }\n \n   public void renameToPeerSnapshotWALDir(String peerId) throws IOException {\n     rename(getPeerReplayWALDir(remoteWALDir, peerId), getPeerSnapshotWALDir(remoteWALDir, peerId),\n-      peerId);\n+        peerId);\n   }\n \n   public List<Path> getReplayWALsAndCleanUpUnusedFiles(String peerId) throws IOException {\n@@ -103,7 +158,7 @@ public void renameToPeerSnapshotWALDir(String peerId) throws IOException {\n       Path src = status.getPath();\n       String srcName = src.getName();\n       String dstName =\n-        srcName.substring(0, srcName.length() - ReplicationUtils.RENAME_WAL_SUFFIX.length());\n+          srcName.substring(0, srcName.length() - ReplicationUtils.RENAME_WAL_SUFFIX.length());\n       FSUtils.renameFile(fs, src, new Path(src.getParent(), dstName));\n     }\n     List<Path> wals = new ArrayList<>();\n@@ -140,28 +195,22 @@ public void removePeerRemoteWALs(String peerId) throws IOException {\n     deleteDir(getPeerSnapshotWALDir(remoteWALDir, peerId), peerId);\n   }\n \n-  public void initPeerWorkers(String peerId) {\n-    BlockingQueue<ServerName> servers = new LinkedBlockingQueue<>();\n-    services.getServerManager().getOnlineServers().keySet()\n-        .forEach(server -> servers.offer(server));\n-    availServers.put(peerId, servers);\n-  }\n-\n-  public ServerName getAvailServer(String peerId, long timeout, TimeUnit unit)\n-      throws InterruptedException {\n-    return availServers.get(peerId).poll(timeout, unit);\n-  }\n-\n-  public void addAvailServer(String peerId, ServerName server) {\n-    availServers.get(peerId).offer(server);\n-  }\n-\n   public String removeWALRootPath(Path path) {\n     String pathStr = path.toString();\n     // remove the \"/\" too.\n     return pathStr.substring(walRootDir.toString().length() + 1);\n   }\n \n+  public void finishReplayWAL(String wal) throws IOException {\n+    Path walPath = new Path(walRootDir, wal);\n+    fs.truncate(walPath, 0);\n+  }\n+\n+  public boolean isReplayWALFinished(String wal) throws IOException {\n+    Path walPath = new Path(walRootDir, wal);\n+    return fs.getFileStatus(walPath).getLen() == 0;\n+  }\n+\n   @VisibleForTesting\n   public Path getRemoteWALDir() {\n     return remoteWALDir;",
                "previous_filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplaySyncReplicationWALManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "sha": "377c9f1e22a53a0e3af3bd4555da9032592c0c3b",
                "status": "renamed"
            },
            {
                "additions": 164,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "changes": 164,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "patch": "@@ -0,0 +1,164 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface;\n+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;\n+import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n+import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALState;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALStateData;\n+\n+@InterfaceAudience.Private\n+public class SyncReplicationReplayWALProcedure\n+    extends StateMachineProcedure<MasterProcedureEnv, SyncReplicationReplayWALState>\n+    implements PeerProcedureInterface {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SyncReplicationReplayWALProcedure.class);\n+\n+  private String peerId;\n+\n+  private ServerName worker = null;\n+\n+  private List<String> wals;\n+\n+  public SyncReplicationReplayWALProcedure() {\n+  }\n+\n+  public SyncReplicationReplayWALProcedure(String peerId, List<String> wals) {\n+    this.peerId = peerId;\n+    this.wals = wals;\n+  }\n+\n+  @Override protected Flow executeFromState(MasterProcedureEnv env,\n+      SyncReplicationReplayWALState state)\n+      throws ProcedureSuspendedException, ProcedureYieldException, InterruptedException {\n+    SyncReplicationReplayWALManager syncReplicationReplayWALManager =\n+        env.getMasterServices().getSyncReplicationReplayWALManager();\n+    switch (state) {\n+      case ASSIGN_WORKER:\n+        try {\n+          worker = syncReplicationReplayWALManager.getPeerWorker(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.info(\"Failed to get worker to replay wals {} for peer id={}, retry\", wals, peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        if (worker == null) {\n+          LOG.info(\"No worker to replay wals {} for peer id={}, retry\", wals, peerId);\n+          setNextState(SyncReplicationReplayWALState.ASSIGN_WORKER);\n+        } else {\n+          setNextState(SyncReplicationReplayWALState.DISPATCH_WALS_TO_WORKER);\n+        }\n+        return Flow.HAS_MORE_STATE;\n+      case DISPATCH_WALS_TO_WORKER:\n+        addChildProcedure(new SyncReplicationReplayWALRemoteProcedure(peerId, wals, worker));\n+        setNextState(SyncReplicationReplayWALState.RELEASE_WORKER);\n+        return Flow.HAS_MORE_STATE;\n+      case RELEASE_WORKER:\n+        boolean finished = false;\n+        try {\n+          finished = syncReplicationReplayWALManager.isReplayWALFinished(wals.get(0));\n+        } catch (IOException e) {\n+          LOG.info(\"Failed to check whether replay wals {} finished for peer id={}\", wals, peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        try {\n+          syncReplicationReplayWALManager.removePeerWorker(peerId, worker);\n+        } catch (ReplicationException e) {\n+          LOG.info(\"Failed to remove worker for peer id={}, retry\", peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        if (!finished) {\n+          LOG.info(\"Failed to replay wals {} for peer id={}, retry\", wals, peerId);\n+          setNextState(SyncReplicationReplayWALState.ASSIGN_WORKER);\n+          return Flow.HAS_MORE_STATE;\n+        }\n+        return Flow.NO_MORE_STATE;\n+      default:\n+        throw new UnsupportedOperationException(\"unhandled state=\" + state);\n+    }\n+  }\n+\n+  @Override\n+  protected void rollbackState(MasterProcedureEnv env,\n+      SyncReplicationReplayWALState state)\n+      throws IOException, InterruptedException {\n+    if (state == getInitialState()) {\n+      return;\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  protected SyncReplicationReplayWALState getState(int state) {\n+    return SyncReplicationReplayWALState.forNumber(state);\n+  }\n+\n+  @Override\n+  protected int getStateId(\n+      SyncReplicationReplayWALState state) {\n+    return state.getNumber();\n+  }\n+\n+  @Override\n+  protected SyncReplicationReplayWALState getInitialState() {\n+    return SyncReplicationReplayWALState.ASSIGN_WORKER;\n+  }\n+\n+  @Override\n+  protected void serializeStateData(ProcedureStateSerializer serializer)\n+      throws IOException {\n+    SyncReplicationReplayWALStateData.Builder builder =\n+        SyncReplicationReplayWALStateData.newBuilder();\n+    builder.setPeerId(peerId);\n+    wals.stream().forEach(builder::addWal);\n+    serializer.serialize(builder.build());\n+  }\n+\n+  @Override\n+  protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    SyncReplicationReplayWALStateData data =\n+        serializer.deserialize(SyncReplicationReplayWALStateData.class);\n+    peerId = data.getPeerId();\n+    wals = new ArrayList<>();\n+    data.getWalList().forEach(wals::add);\n+  }\n+\n+  @Override\n+  public String getPeerId() {\n+    return peerId;\n+  }\n+\n+  @Override\n+  public PeerOperationType getPeerOperationType() {\n+    return PeerOperationType.SYNC_REPLICATION_REPLAY_WAL;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "sha": "26d6a3f78b44f424ae02369b1f8a24ff5cd04c62",
                "status": "added"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 48,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "patch": "@@ -18,7 +18,8 @@\n package org.apache.hadoop.hbase.master.replication;\n \n import java.io.IOException;\n-import java.util.concurrent.TimeUnit;\n+import java.util.ArrayList;\n+import java.util.List;\n \n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n@@ -40,42 +41,45 @@\n \n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ReplaySyncReplicationWALParameter;\n-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ReplaySyncReplicationWALStateData;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALRemoteStateData;\n \n @InterfaceAudience.Private\n-public class ReplaySyncReplicationWALProcedure extends Procedure<MasterProcedureEnv>\n+public class SyncReplicationReplayWALRemoteProcedure extends Procedure<MasterProcedureEnv>\n     implements RemoteProcedure<MasterProcedureEnv, ServerName>, PeerProcedureInterface {\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(ReplaySyncReplicationWALProcedure.class);\n-\n-  private static final long DEFAULT_WAIT_AVAILABLE_SERVER_TIMEOUT = 10000;\n+      LoggerFactory.getLogger(SyncReplicationReplayWALRemoteProcedure.class);\n \n   private String peerId;\n \n-  private ServerName targetServer = null;\n+  private ServerName targetServer;\n \n-  private String wal;\n+  private List<String> wals;\n \n   private boolean dispatched;\n \n   private ProcedureEvent<?> event;\n \n   private boolean succ;\n \n-  public ReplaySyncReplicationWALProcedure() {\n+  public SyncReplicationReplayWALRemoteProcedure() {\n   }\n \n-  public ReplaySyncReplicationWALProcedure(String peerId, String wal) {\n+  public SyncReplicationReplayWALRemoteProcedure(String peerId, List<String> wals,\n+      ServerName targetServer) {\n     this.peerId = peerId;\n-    this.wal = wal;\n+    this.wals = wals;\n+    this.targetServer = targetServer;\n   }\n \n   @Override\n   public RemoteOperation remoteCallBuild(MasterProcedureEnv env, ServerName remote) {\n+    ReplaySyncReplicationWALParameter.Builder builder =\n+        ReplaySyncReplicationWALParameter.newBuilder();\n+    builder.setPeerId(peerId);\n+    wals.stream().forEach(builder::addWal);\n     return new ServerOperation(this, getProcId(), ReplaySyncReplicationWALCallable.class,\n-        ReplaySyncReplicationWALParameter.newBuilder().setPeerId(peerId).setWal(wal).build()\n-            .toByteArray());\n+        builder.build().toByteArray());\n   }\n \n   @Override\n@@ -99,22 +103,47 @@ private void complete(MasterProcedureEnv env, Throwable error) {\n         getProcId());\n       return;\n     }\n-    ReplaySyncReplicationWALManager replaySyncReplicationWALManager =\n-        env.getMasterServices().getReplaySyncReplicationWALManager();\n     if (error != null) {\n-      LOG.warn(\"Replay sync replication wal {} on {} failed for peer id={}\", wal, targetServer,\n-        peerId, error);\n+      LOG.warn(\"Replay wals {} on {} failed for peer id={}\", wals, targetServer, peerId, error);\n       this.succ = false;\n     } else {\n-      LOG.warn(\"Replay sync replication wal {} on {} suceeded for peer id={}\", wal, targetServer,\n-        peerId);\n+      truncateWALs(env);\n+      LOG.info(\"Replay wals {} on {} succeed for peer id={}\", wals, targetServer, peerId);\n       this.succ = true;\n-      replaySyncReplicationWALManager.addAvailServer(peerId, targetServer);\n     }\n     event.wake(env.getProcedureScheduler());\n     event = null;\n   }\n \n+  /**\n+   * Only truncate wals one by one when task succeed. The parent procedure will check the first\n+   * wal length to know whether this task succeed.\n+   */\n+  private void truncateWALs(MasterProcedureEnv env) {\n+    String firstWal = wals.get(0);\n+    try {\n+      env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(firstWal);\n+    } catch (IOException e) {\n+      // As it is idempotent to rerun this task. Just ignore this exception and return.\n+      LOG.warn(\"Failed to truncate wal {} for peer id={}\", firstWal, peerId, e);\n+      return;\n+    }\n+    for (int i = 1; i < wals.size(); i++) {\n+      String wal = wals.get(i);\n+      try {\n+        env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(wal);\n+      } catch (IOException e1) {\n+        try {\n+          // retry\n+          env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(wal);\n+        } catch (IOException e2) {\n+          // As the parent procedure only check the first wal length. Just ignore this exception.\n+          LOG.warn(\"Failed to truncate wal {} for peer id={}\", wal, peerId, e2);\n+        }\n+      }\n+    }\n+  }\n+\n   @Override\n   protected Procedure<MasterProcedureEnv>[] execute(MasterProcedureEnv env)\n       throws ProcedureYieldException, ProcedureSuspendedException, InterruptedException {\n@@ -126,25 +155,14 @@ private void complete(MasterProcedureEnv env, Throwable error) {\n       dispatched = false;\n     }\n \n-    // Try poll a available server\n-    if (targetServer == null) {\n-      targetServer = env.getMasterServices().getReplaySyncReplicationWALManager()\n-          .getAvailServer(peerId, DEFAULT_WAIT_AVAILABLE_SERVER_TIMEOUT, TimeUnit.MILLISECONDS);\n-      if (targetServer == null) {\n-        LOG.info(\"No available server to replay wal {} for peer id={}, retry\", wal, peerId);\n-        throw new ProcedureYieldException();\n-      }\n-    }\n-\n     // Dispatch task to target server\n     try {\n       env.getRemoteDispatcher().addOperationToNode(targetServer, this);\n     } catch (FailedRemoteDispatchException e) {\n-      LOG.info(\n-        \"Can not add remote operation for replay wal {} on {} for peer id={}, \" +\n-          \"this usually because the server is already dead, \" + \"retry\",\n-        wal, targetServer, peerId, e);\n-      targetServer = null;\n+      LOG.warn(\n+          \"Can not add remote operation for replay wals {} on {} for peer id={}, \"\n+              + \"this usually because the server is already dead, retry\",\n+          wals, targetServer, peerId);\n       throw new ProcedureYieldException();\n     }\n     dispatched = true;\n@@ -164,24 +182,23 @@ protected boolean abort(MasterProcedureEnv env) {\n   }\n \n   @Override\n-  protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {\n-    ReplaySyncReplicationWALStateData.Builder builder =\n-        ReplaySyncReplicationWALStateData.newBuilder().setPeerId(peerId).setWal(wal);\n-    if (targetServer != null) {\n-      builder.setTargetServer(ProtobufUtil.toServerName(targetServer));\n-    }\n+  protected void serializeStateData(ProcedureStateSerializer serializer)\n+      throws IOException {\n+    SyncReplicationReplayWALRemoteStateData.Builder builder =\n+        SyncReplicationReplayWALRemoteStateData.newBuilder().setPeerId(peerId)\n+            .setTargetServer(ProtobufUtil.toServerName(targetServer));\n+    wals.stream().forEach(builder::addWal);\n     serializer.serialize(builder.build());\n   }\n \n   @Override\n   protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n-    ReplaySyncReplicationWALStateData data =\n-        serializer.deserialize(ReplaySyncReplicationWALStateData.class);\n+    SyncReplicationReplayWALRemoteStateData data =\n+        serializer.deserialize(SyncReplicationReplayWALRemoteStateData.class);\n     peerId = data.getPeerId();\n-    wal = data.getWal();\n-    if (data.hasTargetServer()) {\n-      targetServer = ProtobufUtil.toServerName(data.getTargetServer());\n-    }\n+    wals = new ArrayList<>();\n+    data.getWalList().forEach(wals::add);\n+    targetServer = ProtobufUtil.toServerName(data.getTargetServer());\n   }\n \n   @Override\n@@ -191,6 +208,6 @@ public String getPeerId() {\n \n   @Override\n   public PeerOperationType getPeerOperationType() {\n-    return PeerOperationType.REPLAY_SYNC_REPLICATION_WAL;\n+    return PeerOperationType.SYNC_REPLICATION_REPLAY_WAL_REMOTE;\n   }\n }\n\\ No newline at end of file",
                "previous_filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplaySyncReplicationWALProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "sha": "9f4f33088ad328e9ae9190d047c011ed0ac0ec4a",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "patch": "@@ -186,8 +186,8 @@ private void setNextStateAfterRefreshEnd() {\n     }\n   }\n \n-  private void replayRemoteWAL() {\n-    addChildProcedure(new RecoverStandbyProcedure(peerId));\n+  private void replayRemoteWAL(boolean serial) {\n+    addChildProcedure(new RecoverStandbyProcedure(peerId, serial));\n   }\n \n   @Override\n@@ -232,7 +232,7 @@ protected Flow executeFromState(MasterProcedureEnv env,\n         setNextStateAfterRefreshBegin();\n         return Flow.HAS_MORE_STATE;\n       case REPLAY_REMOTE_WAL_IN_PEER:\n-        replayRemoteWAL();\n+        replayRemoteWAL(env.getReplicationPeerManager().getPeerConfig(peerId).get().isSerial());\n         setNextState(\n           PeerSyncReplicationStateTransitionState.TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE);\n         return Flow.HAS_MORE_STATE;",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "sha": "c650974df8f05308cbd7b0e27770a1aefda82471",
                "status": "modified"
            },
            {
                "additions": 108,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "patch": "@@ -0,0 +1,108 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n+import org.apache.hadoop.hbase.replication.ZKReplicationStorageBase;\n+import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n+import org.apache.hadoop.hbase.zookeeper.ZKWatcher;\n+import org.apache.hadoop.hbase.zookeeper.ZNodePaths;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+\n+@InterfaceAudience.Private\n+public class ZKSyncReplicationReplayWALWorkerStorage extends ZKReplicationStorageBase {\n+\n+  public static final String WORKERS_ZNODE = \"zookeeper.znode.sync.replication.replaywal.workers\";\n+\n+  public static final String WORKERS_ZNODE_DEFAULT = \"replaywal-workers\";\n+\n+  /**\n+   * The name of the znode that contains a list of workers to replay wal.\n+   */\n+  private final String workersZNode;\n+\n+  public ZKSyncReplicationReplayWALWorkerStorage(ZKWatcher zookeeper, Configuration conf) {\n+    super(zookeeper, conf);\n+    String workersZNodeName = conf.get(WORKERS_ZNODE, WORKERS_ZNODE_DEFAULT);\n+    workersZNode = ZNodePaths.joinZNode(replicationZNode, workersZNodeName);\n+  }\n+\n+  private String getPeerNode(String peerId) {\n+    return ZNodePaths.joinZNode(workersZNode, peerId);\n+  }\n+\n+  public void addPeer(String peerId) throws ReplicationException {\n+    try {\n+      ZKUtil.createWithParents(zookeeper, getPeerNode(peerId));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\n+          \"Failed to add peer id=\" + peerId + \" to replaywal-workers storage\", e);\n+    }\n+  }\n+\n+  public void removePeer(String peerId) throws ReplicationException {\n+    try {\n+      ZKUtil.deleteNodeRecursively(zookeeper, getPeerNode(peerId));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\n+          \"Failed to remove peer id=\" + peerId + \" to replaywal-workers storage\", e);\n+    }\n+  }\n+\n+  private String getPeerWorkerNode(String peerId, ServerName worker) {\n+    return ZNodePaths.joinZNode(getPeerNode(peerId), worker.getServerName());\n+  }\n+\n+  public void addPeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    try {\n+      ZKUtil.createWithParents(zookeeper, getPeerWorkerNode(peerId, worker));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to add worker=\" + worker + \" for peer id=\" + peerId,\n+          e);\n+    }\n+  }\n+\n+  public void removePeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    try {\n+      ZKUtil.deleteNode(zookeeper, getPeerWorkerNode(peerId, worker));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to remove worker=\" + worker + \" for peer id=\" + peerId,\n+          e);\n+    }\n+  }\n+\n+  public Set<ServerName> getPeerWorkers(String peerId) throws ReplicationException {\n+    try {\n+      List<String> children = ZKUtil.listChildrenNoWatch(zookeeper, getPeerNode(peerId));\n+      if (children == null) {\n+        return new HashSet<>();\n+      }\n+      return children.stream().map(ServerName::valueOf).collect(Collectors.toSet());\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to list workers for peer id=\" + peerId, e);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "sha": "5991cf048e30d479a7d261a05460c6c7430b3f97",
                "status": "added"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 15,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.concurrent.locks.Lock;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -32,6 +34,7 @@\n import org.apache.hadoop.hbase.regionserver.HRegionServer;\n import org.apache.hadoop.hbase.regionserver.wal.WALUtil;\n import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.hbase.util.KeyLocker;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.wal.WAL.Entry;\n import org.apache.hadoop.hbase.wal.WAL.Reader;\n@@ -68,31 +71,28 @@\n \n   private String peerId;\n \n-  private String wal;\n+  private List<String> wals = new ArrayList<>();\n \n   private Exception initError;\n \n   private long batchSize;\n \n+  private final KeyLocker<String> peersLock = new KeyLocker<>();\n+\n   @Override\n   public Void call() throws Exception {\n     if (initError != null) {\n       throw initError;\n     }\n-    LOG.info(\"Received a replay sync replication wal {} event, peerId={}\", wal, peerId);\n+    LOG.info(\"Received a replay sync replication wals {} event, peerId={}\", wals, peerId);\n     if (rs.getReplicationSinkService() != null) {\n-      try (Reader reader = getReader()) {\n-        List<Entry> entries = readWALEntries(reader);\n-        while (!entries.isEmpty()) {\n-          Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner> pair = ReplicationProtbufUtil\n-              .buildReplicateWALEntryRequest(entries.toArray(new Entry[entries.size()]));\n-          ReplicateWALEntryRequest request = pair.getFirst();\n-          rs.getReplicationSinkService().replicateLogEntries(request.getEntryList(),\n-            pair.getSecond(), request.getReplicationClusterId(),\n-            request.getSourceBaseNamespaceDirPath(), request.getSourceHFileArchiveDirPath());\n-          // Read next entries.\n-          entries = readWALEntries(reader);\n+      Lock peerLock = peersLock.acquireLock(wals.get(0));\n+      try {\n+        for (String wal : wals) {\n+          replayWAL(wal);\n         }\n+      } finally {\n+        peerLock.unlock();\n       }\n     }\n     return null;\n@@ -107,7 +107,7 @@ public void init(byte[] parameter, HRegionServer rs) {\n       ReplaySyncReplicationWALParameter param =\n           ReplaySyncReplicationWALParameter.parseFrom(parameter);\n       this.peerId = param.getPeerId();\n-      this.wal = param.getWal();\n+      param.getWalList().forEach(this.wals::add);\n       this.batchSize = rs.getConfiguration().getLong(REPLAY_SYNC_REPLICATION_WAL_BATCH_SIZE,\n         DEFAULT_REPLAY_SYNC_REPLICATION_WAL_BATCH_SIZE);\n     } catch (InvalidProtocolBufferException e) {\n@@ -120,7 +120,23 @@ public EventType getEventType() {\n     return EventType.RS_REPLAY_SYNC_REPLICATION_WAL;\n   }\n \n-  private Reader getReader() throws IOException {\n+  private void replayWAL(String wal) throws IOException {\n+    try (Reader reader = getReader(wal)) {\n+      List<Entry> entries = readWALEntries(reader);\n+      while (!entries.isEmpty()) {\n+        Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner> pair = ReplicationProtbufUtil\n+            .buildReplicateWALEntryRequest(entries.toArray(new Entry[entries.size()]));\n+        ReplicateWALEntryRequest request = pair.getFirst();\n+        rs.getReplicationSinkService().replicateLogEntries(request.getEntryList(),\n+            pair.getSecond(), request.getReplicationClusterId(),\n+            request.getSourceBaseNamespaceDirPath(), request.getSourceHFileArchiveDirPath());\n+        // Read next entries.\n+        entries = readWALEntries(reader);\n+      }\n+    }\n+  }\n+\n+  private Reader getReader(String wal) throws IOException {\n     Path path = new Path(rs.getWALRootDir(), wal);\n     long length = rs.getWALFileSystem().getFileStatus(path).getLen();\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "sha": "24963f1ad020198af6edc108655b4fccf6cb1aef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "patch": "@@ -41,8 +41,8 @@\n import org.apache.hadoop.hbase.master.locking.LockManager;\n import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;\n import org.apache.hadoop.hbase.procedure2.LockedResource;\n@@ -476,7 +476,7 @@ public long transitReplicationPeerSyncReplicationState(String peerId,\n   }\n \n   @Override\n-  public ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager() {\n+  public SyncReplicationReplayWALManager getSyncReplicationReplayWALManager() {\n     return null;\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "sha": "ac20dbd0e2ba2096f8ca8de66e6658f742ea8cc3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 3,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "patch": "@@ -78,7 +78,7 @@\n \n   protected static Path REMOTE_WAL_DIR2;\n \n-  private static void initTestingUtility(HBaseTestingUtility util, String zkParent) {\n+  protected static void initTestingUtility(HBaseTestingUtility util, String zkParent) {\n     util.setZkCluster(ZK_UTIL.getZkCluster());\n     Configuration conf = util.getConfiguration();\n     conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkParent);\n@@ -102,8 +102,8 @@ public static void setUp() throws Exception {\n     ZK_UTIL.startMiniZKCluster();\n     initTestingUtility(UTIL1, \"/cluster1\");\n     initTestingUtility(UTIL2, \"/cluster2\");\n-    UTIL1.startMiniCluster(3);\n-    UTIL2.startMiniCluster(3);\n+    UTIL1.startMiniCluster(2,3);\n+    UTIL2.startMiniCluster(2,3);\n     TableDescriptor td =\n       TableDescriptorBuilder.newBuilder(TABLE_NAME).setColumnFamily(ColumnFamilyDescriptorBuilder\n         .newBuilder(CF).setScope(HConstants.REPLICATION_SCOPE_GLOBAL).build()).build();",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "sha": "f765139099af3ceded1f50f0eac9690bb2faa873",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "patch": "@@ -0,0 +1,88 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.master.MasterFileSystem;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationStandbyKillMaster extends SyncReplicationTestBase {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestSyncReplicationStandbyKillMaster.class);\n+\n+  private final long SLEEP_TIME = 2000;\n+\n+  private final int COUNT = 1000;\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestSyncReplicationStandbyKillMaster.class);\n+\n+  @Test\n+  public void testStandbyKillMaster() throws Exception {\n+    MasterFileSystem mfs = UTIL2.getHBaseCluster().getMaster().getMasterFileSystem();\n+    Path remoteWALDir = getRemoteWALDir(mfs, PEER_ID);\n+    assertFalse(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.STANDBY);\n+    assertTrue(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.ACTIVE);\n+\n+    // Disable async replication and write data, then shutdown\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, COUNT);\n+    UTIL1.shutdownMiniCluster();\n+\n+    Thread t = new Thread(() -> {\n+      try {\n+        Thread.sleep(SLEEP_TIME);\n+        UTIL2.getMiniHBaseCluster().getMaster().stop(\"Stop master for test\");\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to stop master\", e);\n+      }\n+    });\n+    t.start();\n+\n+    // Transit standby to DA to replay logs\n+    try {\n+      UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to transit standby cluster to \" + SyncReplicationState.DOWNGRADE_ACTIVE);\n+    }\n+\n+    while (UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID)\n+        != SyncReplicationState.DOWNGRADE_ACTIVE) {\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    verify(UTIL2, 0, COUNT);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "sha": "6265f5cce7c54f37fed17704a1778760333c61d4",
                "status": "added"
            },
            {
                "additions": 119,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "patch": "@@ -0,0 +1,119 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.master.MasterFileSystem;\n+import org.apache.hadoop.hbase.master.ServerManager;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationStandbyKillRS extends SyncReplicationTestBase {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestSyncReplicationStandbyKillRS.class);\n+\n+  private final long SLEEP_TIME = 1000;\n+\n+  private final int COUNT = 1000;\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestSyncReplicationStandbyKillRS.class);\n+\n+  @Test\n+  public void testStandbyKillRegionServer() throws Exception {\n+    MasterFileSystem mfs = UTIL2.getHBaseCluster().getMaster().getMasterFileSystem();\n+    Path remoteWALDir = getRemoteWALDir(mfs, PEER_ID);\n+    assertFalse(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.STANDBY);\n+    assertTrue(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.ACTIVE);\n+\n+    // Disable async replication and write data, then shutdown\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, COUNT);\n+    UTIL1.shutdownMiniCluster();\n+\n+    JVMClusterUtil.MasterThread activeMaster = UTIL2.getMiniHBaseCluster().getMasterThread();\n+    Thread t = new Thread(() -> {\n+      try {\n+        List<JVMClusterUtil.RegionServerThread> regionServers =\n+            UTIL2.getMiniHBaseCluster().getLiveRegionServerThreads();\n+        for (JVMClusterUtil.RegionServerThread rst : regionServers) {\n+          ServerName serverName = rst.getRegionServer().getServerName();\n+          rst.getRegionServer().stop(\"Stop RS for test\");\n+          waitForRSShutdownToStartAndFinish(activeMaster, serverName);\n+          JVMClusterUtil.RegionServerThread restarted =\n+              UTIL2.getMiniHBaseCluster().startRegionServer();\n+          restarted.waitForServerOnline();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to kill RS\", e);\n+      }\n+    });\n+    t.start();\n+\n+    // Transit standby to DA to replay logs\n+    try {\n+      UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to transit standby cluster to \" + SyncReplicationState.DOWNGRADE_ACTIVE);\n+    }\n+\n+    while (UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID)\n+        != SyncReplicationState.DOWNGRADE_ACTIVE) {\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    verify(UTIL2, 0, COUNT);\n+  }\n+\n+  private void waitForRSShutdownToStartAndFinish(JVMClusterUtil.MasterThread activeMaster,\n+      ServerName serverName) throws InterruptedException {\n+    ServerManager sm = activeMaster.getMaster().getServerManager();\n+    // First wait for it to be in dead list\n+    while (!sm.getDeadServers().isDeadServer(serverName)) {\n+      LOG.debug(\"Waiting for [\" + serverName + \"] to be listed as dead in master\");\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    LOG.debug(\"Server [\" + serverName + \"] marked as dead, waiting for it to \" +\n+        \"finish dead processing\");\n+    while (sm.areDeadServersInProgress()) {\n+      LOG.debug(\"Server [\" + serverName + \"] still being processed, waiting\");\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    LOG.debug(\"Server [\" + serverName + \"] done with server shutdown processing\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "sha": "3c9724f260ee164e49e8ab0262429c2316c103e6",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "deletions": 5,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "patch": "@@ -40,7 +40,7 @@\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n import org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;\n import org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter;\n@@ -92,7 +92,7 @@\n \n   private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n \n-  private static ReplaySyncReplicationWALManager replaySyncReplicationWALManager;\n+  private static SyncReplicationReplayWALManager syncReplicationReplayWALManager;\n \n   private static ProcedureExecutor<MasterProcedureEnv> procExec;\n \n@@ -107,7 +107,7 @@ public static void setupCluster() throws Exception {\n     conf = UTIL.getConfiguration();\n     HMaster master = UTIL.getHBaseCluster().getMaster();\n     fs = master.getMasterFileSystem().getWALFileSystem();\n-    replaySyncReplicationWALManager = master.getReplaySyncReplicationWALManager();\n+    syncReplicationReplayWALManager = master.getSyncReplicationReplayWALManager();\n     procExec = master.getMasterProcedureExecutor();\n   }\n \n@@ -138,7 +138,7 @@ public void tearDownAfterTest() throws IOException {\n   @Test\n   public void testRecoverStandby() throws IOException, StreamLacksCapabilityException {\n     setupSyncReplicationWALs();\n-    long procId = procExec.submitProcedure(new RecoverStandbyProcedure(PEER_ID));\n+    long procId = procExec.submitProcedure(new RecoverStandbyProcedure(PEER_ID, false));\n     ProcedureTestingUtility.waitProcedure(procExec, procId);\n     ProcedureTestingUtility.assertProcNotFailed(procExec, procId);\n \n@@ -153,7 +153,7 @@ public void testRecoverStandby() throws IOException, StreamLacksCapabilityExcept\n \n   private void setupSyncReplicationWALs() throws IOException, StreamLacksCapabilityException {\n     Path peerRemoteWALDir = ReplicationUtils\n-      .getPeerRemoteWALDir(replaySyncReplicationWALManager.getRemoteWALDir(), PEER_ID);\n+      .getPeerRemoteWALDir(syncReplicationReplayWALManager.getRemoteWALDir(), PEER_ID);\n     if (!fs.exists(peerRemoteWALDir)) {\n       fs.mkdirs(peerRemoteWALDir);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "sha": "d01a0ac61ad260c8c926eafdbb012e0d5235eb6a",
                "status": "modified"
            }
        ],
        "message": "HBASE-20569 NPE in RecoverStandbyProcedure.execute",
        "parent": "https://github.com/apache/hbase/commit/7448b045ccc96cea2876ebd39cb6b5ef7b73a207",
        "patched_files": [
            "TransitPeerSyncReplicationStateProcedure.java",
            "RecoverStandbyProcedure.java",
            "HMaster.java",
            "SyncReplicationReplayWALManager.java",
            "SyncReplicationReplayWALRemoteProcedure.java",
            "ReplaySyncReplicationWALCallable.java",
            "MasterProcedure.java",
            "PeerQueue.java",
            "PeerProcedureInterface.java",
            "RemovePeerProcedure.java",
            "MockNoopMasterServices.java",
            "SyncReplicationReplayWALProcedure.java",
            "MasterProcedureScheduler.java",
            "SyncReplicationTestBase.java",
            "ZKSyncReplicationReplayWALWorkerStorage.java",
            "MasterServices.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSyncReplicationStandbyKillRS.java",
            "TestSyncReplicationReplayWALManager.java",
            "TestRecoverStandbyProcedure.java",
            "TestSyncReplicationStandbyKillMaster.java",
            "TestMasterProcedureScheduler.java"
        ]
    },
    "hbase_45968ad": {
        "bug_id": "hbase_45968ad",
        "commit": "https://github.com/apache/hbase/commit/45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "patch": "@@ -222,11 +222,25 @@ public void visitLogEntryBeforeWrite(HRegionInfo info, HLogKey logKey,\n   @Override\n   public void visitLogEntryBeforeWrite(HTableDescriptor htd, HLogKey logKey,\n                                        WALEdit logEdit) {\n+    scopeWALEdits(htd, logKey, logEdit);\n+  }\n+\n+  /**\n+   * Utility method used to set the correct scopes on each log key. Doesn't set a scope on keys\n+   * from compaction WAL edits and if the scope is local.\n+   * @param htd Descriptor used to find the scope to use\n+   * @param logKey Key that may get scoped according to its edits\n+   * @param logEdit Edits used to lookup the scopes\n+   */\n+  public static void scopeWALEdits(HTableDescriptor htd, HLogKey logKey,\n+                                   WALEdit logEdit) {\n     NavigableMap<byte[], Integer> scopes =\n         new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);\n     byte[] family;\n     for (KeyValue kv : logEdit.getKeyValues()) {\n       family = kv.getFamily();\n+      if (kv.matchingFamily(WALEdit.METAFAMILY)) continue;\n+\n       int scope = htd.getFamily(family).getScope();\n       if (scope != REPLICATION_SCOPE_LOCAL &&\n           !scopes.containsKey(family)) {",
                "raw_url": "https://github.com/apache/hbase/raw/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "sha": "8692390abab9202de3e54de7064393cdc1327791",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hbase/blob/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java?ref=45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "patch": "@@ -24,6 +24,10 @@\n import org.apache.hadoop.hbase.LargeTests;\n import org.apache.hadoop.hbase.client.*;\n import org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication;\n+import org.apache.hadoop.hbase.protobuf.generated.WALProtos;\n+import org.apache.hadoop.hbase.regionserver.wal.HLogKey;\n+import org.apache.hadoop.hbase.regionserver.wal.WALEdit;\n+import org.apache.hadoop.hbase.replication.regionserver.Replication;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n import org.apache.hadoop.hbase.util.JVMClusterUtil;\n@@ -461,4 +465,17 @@ public void testVerifyRepJob() throws Exception {\n         findCounter(VerifyReplication.Verifier.Counters.BADROWS).getValue());\n   }\n \n+  /**\n+   * Test for HBASE-9038, Replication.scopeWALEdits would NPE if it wasn't filtering out\n+   * the compaction WALEdit\n+   * @throws Exception\n+   */\n+  @Test(timeout=300000)\n+  public void testCompactionWALEdits() throws Exception {\n+    WALProtos.CompactionDescriptor compactionDescriptor =\n+        WALProtos.CompactionDescriptor.getDefaultInstance();\n+    WALEdit edit = WALEdit.createCompaction(compactionDescriptor);\n+    Replication.scopeWALEdits(htable1.getTableDescriptor(), new HLogKey(), edit);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hbase/raw/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "sha": "922102240e3f96e342c8c17b53729ae5288cf920",
                "status": "modified"
            }
        ],
        "message": "HBASE-9038  Compaction WALEdit gives NPEs with Replication enabled\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1508255 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/fd991fe9089afe84a45fa2720d3cd09801cd059f",
        "patched_files": [
            "Replication.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "ReplicationTests.java",
            "TestReplicationSmallTests.java"
        ]
    },
    "hbase_4754202": {
        "bug_id": "hbase_4754202",
        "commit": "https://github.com/apache/hbase/commit/475420205c05adf821896d6e4c436a428ee8981a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java?ref=475420205c05adf821896d6e4c436a428ee8981a",
                "deletions": 0,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "patch": "@@ -183,6 +183,9 @@ public int compareKey(KVComparator comparator, byte[] key, int offset, int lengt\n \n     @Override\n     public void setCurrentBuffer(ByteBuffer buffer) {\n+      if (this.tagCompressionContext != null) {\n+        this.tagCompressionContext.clear();\n+      }\n       currentBuffer = buffer;\n       decodeFirst();\n       previous.invalidate();",
                "raw_url": "https://github.com/apache/hbase/raw/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "sha": "513e2e22e9b5615bfe14bfe7d4861dae1b7e70b1",
                "status": "modified"
            }
        ],
        "message": "HBASE-10438 NPE from LRUDictionary when size reaches the max init value (Ramkrishna S. Vasudevan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1562578 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5ecb63b539bc99e019a317e47c328d2a4d4d9b18",
        "patched_files": [
            "BufferedDataBlockEncoder.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestBufferedDataBlockEncoder.java"
        ]
    },
    "hbase_4969879": {
        "bug_id": "hbase_4969879",
        "commit": "https://github.com/apache/hbase/commit/4969879df5f6d1d9a2774236cdee1710694ddf6a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "patch": "@@ -58,4 +58,7 @@\n   public static final String OPEN_PARAN = \"(\";\n   public static final String CLOSED_PARAN = \")\";\n \n+  /** Label ordinal value for invalid labels */\n+  public static final int NON_EXIST_LABEL_ORDINAL = 0;\n+\n }",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "sha": "782d1637d915296b052a24d4f517c07b28463401",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.client.Table;\n import org.apache.hadoop.hbase.security.visibility.Authorizations;\n+import org.apache.hadoop.hbase.security.visibility.VisibilityConstants;\n import org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider;\n import org.apache.hadoop.hbase.security.visibility.VisibilityUtils;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -124,7 +125,12 @@ public void init() {\n     VisibilityLabelOrdinalProvider provider = new VisibilityLabelOrdinalProvider() {\n       @Override\n       public int getLabelOrdinal(String label) {\n-        return labels.get(label);\n+        Integer ordinal = null;\n+        ordinal = labels.get(label);\n+        if (ordinal != null) {\n+          return ordinal.intValue();\n+        }\n+        return VisibilityConstants.NON_EXIST_LABEL_ORDINAL;\n       }\n \n       @Override",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "sha": "597a7641e822b940b18630425ca96485a86b4b10",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 11,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "patch": "@@ -24,16 +24,17 @@\n import java.util.Set;\n import java.util.TreeSet;\n \n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.CellComparator;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.KeyValueUtil;\n import org.apache.hadoop.hbase.Tag;\n import org.apache.hadoop.hbase.TagType;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n+import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.security.visibility.InvalidLabelException;\n import org.apache.hadoop.hbase.util.Base64;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.Text;\n@@ -186,21 +187,15 @@ protected void reduce(\n             kvs.add(kv);\n             curSize += kv.heapSize();\n           }\n-        } catch (ImportTsv.TsvParser.BadTsvLineException badLine) {\n+        } catch (ImportTsv.TsvParser.BadTsvLineException | IllegalArgumentException\n+            | InvalidLabelException badLine) {\n           if (skipBadLines) {\n             System.err.println(\"Bad line.\" + badLine.getMessage());\n             incrementBadLineCount(1);\n             continue;\n           }\n           throw new IOException(badLine);\n-        } catch (IllegalArgumentException e) {\n-          if (skipBadLines) {\n-            System.err.println(\"Bad line.\" + e.getMessage());\n-            incrementBadLineCount(1);\n-            continue;\n-          } \n-          throw new IOException(e);\n-        } \n+        }\n       }\n       context.setStatus(\"Read \" + kvs.size() + \" entries of \" + kvs.getClass()\n           + \"(\" + StringUtils.humanReadableInt(curSize) + \")\");",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "sha": "8fe5cc7407683b136d7ccb05793fdfaf705b94dc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "patch": "@@ -21,17 +21,18 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.Tag;\n import org.apache.hadoop.hbase.TagType;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n+import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Put;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser.BadTsvLineException;\n import org.apache.hadoop.hbase.security.visibility.CellVisibility;\n+import org.apache.hadoop.hbase.security.visibility.InvalidLabelException;\n import org.apache.hadoop.hbase.util.Base64;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.LongWritable;\n@@ -182,7 +183,8 @@ public void map(LongWritable offset, Text value,\n         populatePut(lineBytes, parsed, put, i);\n       }\n       context.write(rowKey, put);\n-    } catch (ImportTsv.TsvParser.BadTsvLineException|IllegalArgumentException badLine) {\n+    } catch (ImportTsv.TsvParser.BadTsvLineException | IllegalArgumentException\n+        | InvalidLabelException badLine) {\n       if (logBadLines) {\n         System.err.println(value);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "sha": "98dc25eaaf08a8d680f068bf10060a47cfa1a81d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "patch": "@@ -49,7 +49,6 @@\n public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {\n \n   private static final Log LOG = LogFactory.getLog(VisibilityLabelsCache.class);\n-  private static final int NON_EXIST_LABEL_ORDINAL = 0;\n   private static final List<String> EMPTY_LIST = Collections.emptyList();\n   private static final Set<Integer> EMPTY_SET = Collections.emptySet();\n   private static VisibilityLabelsCache instance;\n@@ -175,7 +174,7 @@ public int getLabelOrdinal(String label) {\n       return ordinal.intValue();\n     }\n     // 0 denotes not available\n-    return NON_EXIST_LABEL_ORDINAL;\n+    return VisibilityConstants.NON_EXIST_LABEL_ORDINAL;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "sha": "0948520b0c5d7ba4ab0bee22e9ab663273e0ed0d",
                "status": "modified"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "changes": 95,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "deletions": 9,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "patch": "@@ -42,18 +42,16 @@\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.client.Admin;\n-import org.apache.hadoop.hbase.testclassification.LargeTests;\n-import org.apache.hadoop.hbase.testclassification.MapReduceTests;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Delete;\n-import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.VisibilityLabelsResponse;\n import org.apache.hadoop.hbase.security.User;\n import org.apache.hadoop.hbase.security.visibility.Authorizations;\n@@ -64,6 +62,8 @@\n import org.apache.hadoop.hbase.security.visibility.VisibilityConstants;\n import org.apache.hadoop.hbase.security.visibility.VisibilityController;\n import org.apache.hadoop.hbase.security.visibility.VisibilityUtils;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.MapReduceTests;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.mapred.Utils.OutputFileUtils.OutputFilesFilter;\n import org.apache.hadoop.util.Tool;\n@@ -274,6 +274,50 @@ public void testMRWithOutputFormat() throws Exception {\n     util.deleteTable(tableName);\n   }\n \n+  @Test\n+  public void testBulkOutputWithInvalidLabels() throws Exception {\n+    TableName tableName = TableName.valueOf(\"test-\" + UUID.randomUUID());\n+    Path hfiles = new Path(util.getDataTestDirOnTestFS(tableName.getNameAsString()), \"hfiles\");\n+    // Prepare the arguments required for the test.\n+    String[] args =\n+        new String[] { \"-D\" + ImportTsv.BULK_OUTPUT_CONF_KEY + \"=\" + hfiles.toString(),\n+            \"-D\" + ImportTsv.COLUMNS_CONF_KEY + \"=HBASE_ROW_KEY,FAM:A,FAM:B,HBASE_CELL_VISIBILITY\",\n+            \"-D\" + ImportTsv.SEPARATOR_CONF_KEY + \"=\\u001b\", tableName.getNameAsString() };\n+\n+    // 2 Data rows, one with valid label and one with invalid label\n+    String data =\n+        \"KEY\\u001bVALUE1\\u001bVALUE2\\u001bprivate\\nKEY1\\u001bVALUE1\\u001bVALUE2\\u001binvalid\\n\";\n+    util.createTable(tableName, FAMILY);\n+    doMROnTableTest(util, FAMILY, data, args, 1, 2);\n+    util.deleteTable(tableName);\n+  }\n+\n+  @Test\n+  public void testBulkOutputWithTsvImporterTextMapperWithInvalidLabels() throws Exception {\n+    TableName tableName = TableName.valueOf(\"test-\" + UUID.randomUUID());\n+    Path hfiles = new Path(util.getDataTestDirOnTestFS(tableName.getNameAsString()), \"hfiles\");\n+    // Prepare the arguments required for the test.\n+    String[] args =\n+        new String[] {\n+            \"-D\" + ImportTsv.MAPPER_CONF_KEY\n+                + \"=org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper\",\n+            \"-D\" + ImportTsv.BULK_OUTPUT_CONF_KEY + \"=\" + hfiles.toString(),\n+            \"-D\" + ImportTsv.COLUMNS_CONF_KEY + \"=HBASE_ROW_KEY,FAM:A,FAM:B,HBASE_CELL_VISIBILITY\",\n+            \"-D\" + ImportTsv.SEPARATOR_CONF_KEY + \"=\\u001b\", tableName.getNameAsString() };\n+\n+    // 2 Data rows, one with valid label and one with invalid label\n+    String data =\n+        \"KEY\\u001bVALUE1\\u001bVALUE2\\u001bprivate\\nKEY1\\u001bVALUE1\\u001bVALUE2\\u001binvalid\\n\";\n+    util.createTable(tableName, FAMILY);\n+    doMROnTableTest(util, FAMILY, data, args, 1, 2);\n+    util.deleteTable(tableName);\n+  }\n+\n+  protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, String data,\n+      String[] args, int valueMultiplier) throws Exception {\n+    return doMROnTableTest(util, family, data, args, valueMultiplier, -1);\n+  }\n+\n   /**\n    * Run an ImportTsv job and perform basic validation on the results. Returns\n    * the ImportTsv <code>Tool</code> instance so that other tests can inspect it\n@@ -282,10 +326,13 @@ public void testMRWithOutputFormat() throws Exception {\n    *\n    * @param args\n    *          Any arguments to pass BEFORE inputFile path is appended.\n+   *\n+   * @param expectedKVCount Expected KV count. pass -1 to skip the kvcount check\n+   *\n    * @return The Tool instance used to run the test.\n    */\n   protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, String data,\n-      String[] args, int valueMultiplier) throws Exception {\n+      String[] args, int valueMultiplier,int expectedKVCount) throws Exception {\n     TableName table = TableName.valueOf(args[args.length - 1]);\n     Configuration conf = new Configuration(util.getConfiguration());\n \n@@ -328,7 +375,7 @@ protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, S\n     }\n     LOG.debug(\"validating the table \" + createdHFiles);\n     if (createdHFiles)\n-     validateHFiles(fs, outputPath, family);\n+     validateHFiles(fs, outputPath, family,expectedKVCount);\n     else\n       validateTable(conf, table, family, valueMultiplier);\n \n@@ -342,14 +389,15 @@ protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, S\n   /**\n    * Confirm ImportTsv via HFiles on fs.\n    */\n-  private static void validateHFiles(FileSystem fs, String outputPath, String family)\n-      throws IOException {\n+  private static void validateHFiles(FileSystem fs, String outputPath, String family,\n+      int expectedKVCount) throws IOException {\n \n     // validate number and content of output columns\n     LOG.debug(\"Validating HFiles.\");\n     Set<String> configFamilies = new HashSet<String>();\n     configFamilies.add(family);\n     Set<String> foundFamilies = new HashSet<String>();\n+    int actualKVCount = 0;\n     for (FileStatus cfStatus : fs.listStatus(new Path(outputPath), new OutputFilesFilter())) {\n       LOG.debug(\"The output path has files\");\n       String[] elements = cfStatus.getPath().toString().split(Path.SEPARATOR);\n@@ -361,8 +409,16 @@ private static void validateHFiles(FileSystem fs, String outputPath, String fami\n       for (FileStatus hfile : fs.listStatus(cfStatus.getPath())) {\n         assertTrue(String.format(\"HFile %s appears to contain no data.\", hfile.getPath()),\n             hfile.getLen() > 0);\n+        if (expectedKVCount > -1) {\n+          actualKVCount += getKVCountFromHfile(fs, hfile.getPath());\n+        }\n       }\n     }\n+    if (expectedKVCount > -1) {\n+      assertTrue(String.format(\n+        \"KV count in output hfile=<%d> doesn't match with expected KV count=<%d>\", actualKVCount,\n+        expectedKVCount), actualKVCount == expectedKVCount);\n+    }\n   }\n \n   /**\n@@ -412,4 +468,25 @@ private static void validateTable(Configuration conf, TableName tableName, Strin\n     assertTrue(verified);\n   }\n \n+  /**\n+   * Method returns the total KVs in given hfile\n+   * @param fs File System\n+   * @param p HFile path\n+   * @return KV count in the given hfile\n+   * @throws IOException\n+   */\n+  private static int getKVCountFromHfile(FileSystem fs, Path p) throws IOException {\n+    Configuration conf = util.getConfiguration();\n+    HFile.Reader reader = HFile.createReader(fs, p, new CacheConfig(conf), conf);\n+    reader.loadFileInfo();\n+    HFileScanner scanner = reader.getScanner(false, false);\n+    scanner.seekTo();\n+    int count = 0;\n+    do {\n+      count++;\n+    } while (scanner.next());\n+    reader.close();\n+    return count;\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "sha": "3aec6694d76d832177a66534c90e9cf364ddaaef",
                "status": "modified"
            }
        ],
        "message": "HBASE-14366 NPE in case visibility expression is not present in labels table during importtsv run. (Bhupendra)",
        "parent": "https://github.com/apache/hbase/commit/2e593a9d3801a42751244ab4478650a581437875",
        "patched_files": [
            "VisibilityLabelsCache.java",
            "DefaultVisibilityExpressionResolver.java",
            "TsvImporterMapper.java",
            "VisibilityConstants.java",
            "TextSortReducer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestImportTSVWithVisibilityLabels.java"
        ]
    },
    "hbase_4cb444e": {
        "bug_id": "hbase_4cb444e",
        "commit": "https://github.com/apache/hbase/commit/4cb444e77b41cdb733544770a471068256d65bbe",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/4cb444e77b41cdb733544770a471068256d65bbe/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=4cb444e77b41cdb733544770a471068256d65bbe",
                "deletions": 11,
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "patch": "@@ -264,9 +264,31 @@ protected void periodicExecute(final TEnvironment env) {\n   private final CopyOnWriteArrayList<ProcedureExecutorListener> listeners = new CopyOnWriteArrayList<>();\n \n   private Configuration conf;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Destroyed in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private ThreadGroup threadGroup;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Terminated in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private CopyOnWriteArrayList<WorkerThread> workerThreads;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Terminated in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private TimeoutExecutorThread timeoutExecutor;\n+\n   private int corePoolSize;\n   private int maxPoolSize;\n \n@@ -299,6 +321,7 @@ public ProcedureExecutor(final Configuration conf, final TEnvironment environmen\n     this.conf = conf;\n     this.checkOwnerSet = conf.getBoolean(CHECK_OWNER_SET_CONF_KEY, DEFAULT_CHECK_OWNER_SET);\n     refreshConfiguration(conf);\n+\n   }\n \n   private void load(final boolean abortOnCorruption) throws IOException {\n@@ -510,11 +533,8 @@ public void start(int numThreads, boolean abortOnCorruption) throws IOException\n     LOG.info(\"Starting {} core workers (bigger of cpus/4 or 16) with max (burst) worker count={}\",\n         corePoolSize, maxPoolSize);\n \n-    // Create the Thread Group for the executors\n-    threadGroup = new ThreadGroup(\"PEWorkerGroup\");\n-\n-    // Create the timeout executor\n-    timeoutExecutor = new TimeoutExecutorThread(this, threadGroup);\n+    this.threadGroup = new ThreadGroup(\"PEWorkerGroup\");\n+    this.timeoutExecutor = new TimeoutExecutorThread(this, threadGroup);\n \n     // Create the workers\n     workerId.set(0);\n@@ -576,22 +596,21 @@ public void join() {\n \n     // stop the timeout executor\n     timeoutExecutor.awaitTermination();\n-    timeoutExecutor = null;\n \n     // stop the worker threads\n     for (WorkerThread worker: workerThreads) {\n       worker.awaitTermination();\n     }\n-    workerThreads = null;\n \n     // Destroy the Thread Group for the executors\n+    // TODO: Fix. #join is not place to destroy resources.\n     try {\n       threadGroup.destroy();\n     } catch (IllegalThreadStateException e) {\n-      LOG.error(\"ThreadGroup \" + threadGroup + \" contains running threads; \" + e.getMessage());\n-      threadGroup.list();\n-    } finally {\n-      threadGroup = null;\n+      LOG.error(\"ThreadGroup {} contains running threads; {}: See STDOUT\",\n+          this.threadGroup, e.getMessage());\n+      // This dumps list of threads on STDOUT.\n+      this.threadGroup.list();\n     }\n \n     // reset the in-memory state for testing",
                "raw_url": "https://github.com/apache/hbase/raw/4cb444e77b41cdb733544770a471068256d65bbe/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "sha": "3a75d33dd261aeac09b1859c3347f2a4d40a6216",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/4cb444e77b41cdb733544770a471068256d65bbe/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java?ref=4cb444e77b41cdb733544770a471068256d65bbe",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hbase.client.Table;\n import org.apache.hadoop.hbase.client.TableDescriptor;\n import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n import org.apache.hadoop.hbase.master.assignment.AssignmentManager;\n import org.apache.hadoop.hbase.test.MetricsAssertHelper;\n import org.apache.hadoop.hbase.testclassification.MasterTests;\n@@ -91,6 +92,8 @@ public static void startCluster() throws Exception {\n     // set a small interval for updating rit metrics\n     conf.setInt(AssignmentManager.RIT_CHORE_INTERVAL_MSEC_CONF_KEY, MSG_INTERVAL);\n \n+    // keep rs online so it can report the failed opens.\n+    conf.setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n     TEST_UTIL.startMiniCluster(1);\n     CLUSTER = TEST_UTIL.getHBaseCluster();\n     MASTER = CLUSTER.getMaster();\n@@ -148,6 +151,9 @@ public void testRITAssignmentManagerMetrics() throws Exception {\n       }\n \n       // Sleep 3 seconds, wait for doMetrics chore catching up\n+      // the rit count consists of rit and failed opens. see RegionInTransitionStat#update\n+      // Waiting for the completion of rit makes the assert stable.\n+      TEST_UTIL.waitUntilNoRegionsInTransition();\n       Thread.sleep(MSG_INTERVAL * 3);\n       METRICS_HELPER.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 1, amSource);\n       METRICS_HELPER.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 1,",
                "raw_url": "https://github.com/apache/hbase/raw/4cb444e77b41cdb733544770a471068256d65bbe/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "sha": "4c9d0e3c33d4d3ef91c9f32c31d2ce99be50e65f",
                "status": "modified"
            }
        ],
        "message": "HBASE-20169 NPE when calling HBTU.shutdownMiniCluster (TestAssignmentManagerMetrics is flakey); AMENDMENT",
        "parent": "https://github.com/apache/hbase/commit/c4b4023b60dd51a7e0ad83883afec569037e5329",
        "patched_files": [
            "ProcedureExecutor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManagerMetrics.java",
            "TestProcedureExecutor.java"
        ]
    },
    "hbase_4ecbf3c": {
        "bug_id": "hbase_4ecbf3c",
        "commit": "https://github.com/apache/hbase/commit/4ecbf3c8c08e4b78592e2b5826059d927924cfed",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -689,6 +689,7 @@ Release 0.90.5 - Unreleased\n    HBASE-4295  rowcounter does not return the correct number of rows in\n                certain circumstances (David Revell)\n    HBASE-4515  User.getCurrent() can fail to initialize the current user\n+   HBASE-4473  NPE when executors are down but events are still coming in\n \n   IMPROVEMENT\n    HBASE-4205  Enhance HTable javadoc (Eric Charles)",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/CHANGES.txt",
                "sha": "e393f4e5a4e1dc61348146a63a969d523e24baa5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "deletions": 4,
                "filename": "src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "patch": "@@ -213,9 +213,6 @@ Executor getExecutor(final ExecutorType type) {\n \n   Executor getExecutor(String name) {\n     Executor executor = this.executorMap.get(name);\n-    if (executor == null) {\n-      LOG.debug(\"Executor service [\" + name + \"] not found in \" + this.executorMap);\n-    }\n     return executor;\n   }\n \n@@ -231,7 +228,16 @@ public void startExecutorService(final ExecutorType type, final int maxThreads)\n   }\n \n   public void submit(final EventHandler eh) {\n-    getExecutor(getExecutorServiceType(eh.getEventType())).submit(eh);\n+    Executor executor = getExecutor(getExecutorServiceType(eh.getEventType()));\n+    if (executor == null) {\n+      // This happens only when events are submitted after shutdown() was\n+      // called, so dropping them should be \"ok\" since it means we're\n+      // shutting down.\n+      LOG.error(\"Cannot submit [\" + eh + \"] because the executor is missing.\" +\n+        \" Is this process shutting down?\");\n+    } else {\n+      executor.submit(eh);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "sha": "7fb4266ac6e1c0d143dee6a43078f95b09d98a25",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "patch": "@@ -124,6 +124,15 @@ public void testExecutorService() throws Exception {\n     // Make sure threads are still around even after their timetolive expires.\n     Thread.sleep(executor.keepAliveTimeInMillis * 2);\n     assertEquals(maxThreads, pool.getPoolSize());\n+\n+    executorService.shutdown();\n+\n+    assertEquals(0, executorService.getAllExecutorStatuses().size());\n+\n+    // Test that submit doesn't throw NPEs\n+    executorService.submit(\n+      new TestEventHandler(mockedServer, EventType.M_SERVER_SHUTDOWN,\n+            lock, counter));\n   }\n \n   private void checkStatusDump(ExecutorStatus status) throws IOException {",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "sha": "cb10fff74f72e299dfdc19c8b4b841f6ef5a863d",
                "status": "modified"
            }
        ],
        "message": "HBASE-4473  NPE when executors are down but events are still coming in\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1178520 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cb845c04d8f8f0365b6fdff40cdbcb5f938c3202",
        "patched_files": [
            "CHANGES.java",
            "ExecutorService.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestExecutorService.java"
        ]
    },
    "hbase_5375ff0": {
        "bug_id": "hbase_5375ff0",
        "commit": "https://github.com/apache/hbase/commit/5375ff07bcb6451e45c09f23f010a4d051968896",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1377,6 +1377,9 @@ public void assign(Map<HRegionInfo, ServerName> regions)\n     // Reuse existing assignment info\n     Map<ServerName, List<HRegionInfo>> bulkPlan =\n       balancer.retainAssignment(regions, servers);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n     assign(regions.size(), servers.size(),\n       \"retainAssignment=true\", bulkPlan);\n@@ -1404,8 +1407,11 @@ public void assign(List<HRegionInfo> regions)\n     // Generate a round-robin bulk assignment plan\n     Map<ServerName, List<HRegionInfo>> bulkPlan\n       = balancer.roundRobinAssignment(regions, servers);\n-    processFavoredNodes(regions);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n+    processFavoredNodes(regions);\n     assign(regions.size(), servers.size(),\n       \"round-robin=true\", bulkPlan);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "77c32c439177ac460afbda679f7c911edf6fd83d",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -1079,15 +1079,30 @@ void move(final byte[] encodedRegionName,\n       final List<ServerName> destServers = this.serverManager.createDestinationServersList(\n         regionState.getServerName());\n       dest = balancer.randomAssignment(hri, destServers);\n+      if (dest == null) {\n+        LOG.debug(\"Unable to determine a plan to assign \" + hri);\n+        return;\n+      }\n     } else {\n       dest = ServerName.valueOf(Bytes.toString(destServerName));\n-      if (dest.equals(regionState.getServerName())) {\n+      if (dest.equals(serverName) && balancer instanceof BaseLoadBalancer\n+          && !((BaseLoadBalancer)balancer).shouldBeOnMaster(hri)) {\n+        // To avoid unnecessary region moving later by balancer. Don't put user\n+        // regions on master. Regions on master could be put on other region\n+        // server intentionally by test however.\n         LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n-          + \" because region already assigned to the same server \" + dest + \".\");\n+          + \" to avoid unnecessary region moving later by load balancer,\"\n+          + \" because it should not be on master\");\n         return;\n       }\n     }\n \n+    if (dest.equals(regionState.getServerName())) {\n+      LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n+        + \" because region already assigned to the same server \" + dest + \".\");\n+      return;\n+    }\n+\n     // Now we can do the move\n     RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), dest);\n ",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "eef0a09f3d0e54b1d239562e1d5ee8c7434f5b72",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -858,7 +858,7 @@ protected void setSlop(Configuration conf) {\n    * Check if a region belongs to some small system table.\n    * If so, it may be expected to be put on the master regionserver.\n    */\n-  protected boolean shouldBeOnMaster(HRegionInfo region) {\n+  public boolean shouldBeOnMaster(HRegionInfo region) {\n     return tablesOnMaster.contains(region.getTable().getNameAsString());\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "a5e110b6d0018255ba9e82e11e956d9693d14129",
                "status": "modified"
            }
        ],
        "message": "HBASE-12167 NPE in AssignmentManager",
        "parent": "https://github.com/apache/hbase/commit/d8a7b67d798ab5fec399d4a0b97a025d5bff531c",
        "patched_files": [
            "BaseLoadBalancer.java",
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java",
            "TestAssignmentManager.java"
        ]
    },
    "hbase_56e19a7": {
        "bug_id": "hbase_56e19a7",
        "commit": "https://github.com/apache/hbase/commit/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -449,6 +449,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4719  HBase script assumes pre-Hadoop 0.21 layout of jar files\n                (Roman Shposhnik)\n    HBASE-4553  The update of .tableinfo is not atomic; we remove then rename\n+   HBASE-4725  NPE in AM#updateTimers\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "sha": "352113449e879f44dd5781b44af30c9dd2be3e61",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -61,7 +61,6 @@\n import org.apache.hadoop.hbase.executor.ExecutorService;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n-import org.apache.hadoop.hbase.master.AssignmentManager.RegionState;\n import org.apache.hadoop.hbase.master.handler.ClosedRegionHandler;\n import org.apache.hadoop.hbase.master.handler.DisableTableHandler;\n import org.apache.hadoop.hbase.master.handler.EnableTableHandler;\n@@ -1056,6 +1055,7 @@ private void updateTimers(final ServerName sn) {\n       copy.putAll(this.regionPlans);\n     }\n     for (Map.Entry<String, RegionPlan> e: copy.entrySet()) {\n+      if (e.getValue() == null || e.getValue().getDestination() == null) continue;\n       if (!e.getValue().getDestination().equals(sn)) continue;\n       RegionState rs = null;\n       synchronized (this.regionsInTransition) {",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "b5a2cc730e60dd8c7eca9b7a93ac9705e11a9f5f",
                "status": "modified"
            }
        ],
        "message": "HBASE-4725 NPE in AM#updateTimers\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1197815 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/bb0c9a11d8843344f287ac5919e4bea3f34fb240",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_57c8671": {
        "bug_id": "hbase_57c8671",
        "commit": "https://github.com/apache/hbase/commit/57c86717285d74f4604a277ee034878398568d81",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java?ref=57c86717285d74f4604a277ee034878398568d81",
                "deletions": 4,
                "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "patch": "@@ -393,10 +393,10 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n             \" failed when creating the node for \" + destServerName,\n           e);\n     }\n+    String newQueueId = queueId + \"-\" + sourceServerName;\n     try {\n       String oldQueueNode = getQueueNode(sourceServerName, queueId);\n       List<String> wals = ZKUtil.listChildrenNoWatch(zookeeper, oldQueueNode);\n-      String newQueueId = queueId + \"-\" + sourceServerName;\n       if (CollectionUtils.isEmpty(wals)) {\n         ZKUtil.deleteNodeFailSilent(zookeeper, oldQueueNode);\n         LOG.info(\"Removed empty {}/{}\", sourceServerName, queueId);\n@@ -427,11 +427,12 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n       return new Pair<>(newQueueId, logQueue);\n     } catch (NoNodeException | NodeExistsException | NotEmptyException | BadVersionException e) {\n       // Multi call failed; it looks like some other regionserver took away the logs.\n-      // These exceptions mean that zk tells us the request can not be execute so it is safe to just\n-      // return a null. For other types of exception should be thrown out to notify the upper layer.\n+      // These exceptions mean that zk tells us the request can not be execute. So return an empty\n+      // queue to tell the upper layer that claim nothing. For other types of exception should be\n+      // thrown out to notify the upper layer.\n       LOG.info(\"Claim queue queueId={} from {} to {} failed with {}, someone else took the log?\",\n           queueId,sourceServerName, destServerName, e.toString());\n-      return null;\n+      return new Pair<>(newQueueId, Collections.emptySortedSet());\n     } catch (KeeperException | InterruptedException e) {\n       throw new ReplicationException(\"Claim queue queueId=\" + queueId + \" from \" +\n         sourceServerName + \" to \" + destServerName + \" failed\", e);",
                "raw_url": "https://github.com/apache/hbase/raw/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "sha": "cca8bfcab3da50d5d8440f2bc5d7ace8e18140a0",
                "status": "modified"
            }
        ],
        "message": "HBASE-20678 NPE in ReplicationSourceManager#NodeFailoverWorker",
        "parent": "https://github.com/apache/hbase/commit/a45763df553edd006c2168df3a64f0f2cdf2366f",
        "patched_files": [
            "ZKReplicationQueueStorage.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestZKReplicationQueueStorage.java"
        ]
    },
    "hbase_5994c50": {
        "bug_id": "hbase_5994c50",
        "commit": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -118,6 +118,8 @@ public boolean visit(Result r) throws IOException {\n         Pair<HRegionInfo, ServerName> region = parseCatalogResult(r);\n         if (region == null) return true;\n         HRegionInfo hri = region.getFirst();\n+        if (hri  == null) return true;\n+        if (hri.getTableNameAsString() == null) return true;\n         if (disabledTables.contains(\n             hri.getTableNameAsString())) return true;\n         // Are we to include split parents in the list?",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "77a121b33a40b228fb2040f57e61a71b170b8000",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -2254,6 +2254,7 @@ boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regi\n       if (region == null) continue;\n       HRegionInfo regionInfo = region.getFirst();\n       ServerName regionLocation = region.getSecond();\n+      if (regionInfo == null) continue;\n       String tableName = regionInfo.getTableNameAsString();\n       if (regionLocation == null) {\n         // regionLocation could be null if createTable didn't finish properly.",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "6748e5c87260cc06ccf9b8c14e45258555b5f798",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -4123,6 +4123,8 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n+    Scan scan = new Scan(get);\n+\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "4c5b1dcad734d1117a8736f8e11d8d29291f7960",
                "status": "modified"
            }
        ],
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245767 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7160ecd133e687887091c5b62e498db960f4b6ac",
        "patched_files": [
            "HRegion.java",
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestAssignmentManager.java"
        ]
    },
    "hbase_5c0d3a5": {
        "bug_id": "hbase_5c0d3a5",
        "commit": "https://github.com/apache/hbase/commit/5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -124,6 +124,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1100  HBASE-1062 broke TestForceSplit\n    HBASE-1191  shell tools -> close_region does not work for regions that did\n                not deploy properly on startup\n+   HBASE-1093  NPE in HStore#compact\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "sha": "44c33ae9a93dc28e5824bb4ea1a1ec2464519470",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "patch": "@@ -866,6 +866,10 @@ StoreSize compact(final boolean majorCompaction) throws IOException {\n       for (int i = 0; i < countOfFiles; i++) {\n         HStoreFile file = filesToCompact.get(i);\n         Path path = file.getMapFilePath();\n+        if (path == null) {\n+          LOG.warn(\"Path is null for \" + file);\n+          return null;\n+        }\n         int len = 0;\n         for (FileStatus fstatus:fs.listStatus(path)) {\n           len += fstatus.getLen();",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "sha": "c6ad3b76e9e70a981c8190083c277301f4c1122c",
                "status": "modified"
            }
        ],
        "message": "HBASE-1093 NPE in HStore#compact\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@730068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/9f9198e21eee3bbf73c609cfd2fb9d8978d46bc3",
        "patched_files": [
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_60d7a81": {
        "bug_id": "hbase_60d7a81",
        "commit": "https://github.com/apache/hbase/commit/60d7a81833369262e9b21af31d88ed9c81398f42",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java?ref=60d7a81833369262e9b21af31d88ed9c81398f42",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "patch": "@@ -356,8 +356,12 @@ public static ScannerModel fromScan(Scan scan) throws Exception {\n     Map<byte [], NavigableSet<byte []>> families = scan.getFamilyMap();\n     if (families != null) {\n       for (Map.Entry<byte [], NavigableSet<byte []>> entry : families.entrySet()) {\n-        for (byte[] qualifier : entry.getValue()) {\n-          model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+        if (entry.getValue() != null) {\n+          for (byte[] qualifier: entry.getValue()) {\n+            model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+          }\n+        } else {\n+          model.addColumn(entry.getKey());\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "sha": "7768612dceb9633478b047eec78c784e2a384092",
                "status": "modified"
            }
        ],
        "message": "HBASE-3912 [Stargate] Columns not handle by Scan; fix NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1126556 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/981f414cd3b6ef7c1b846e139977c06ff781802f",
        "patched_files": [
            "ScannerModel.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestScannerModel.java"
        ]
    },
    "hbase_612f446": {
        "bug_id": "hbase_612f446",
        "commit": "https://github.com/apache/hbase/commit/612f446dbdd151a02700839705c5ebfd1fc33e2a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -60,6 +60,7 @@ Trunk (unreleased changes)\n                (Bryan Duxbury via Stack)\n    HADOOP-2365 Result of HashFunction.hash() contains all identical values\n    HADOOP-2362 Leaking hdfs file handle on region split\n+   HADOOP-2338 Fix NullPointerException in master server.\n \n   IMPROVEMENTS\n    HADOOP-2401 Add convenience put method that takes writable",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/CHANGES.txt",
                "sha": "af507998fcbeb2ce17577cf3a90faa65bf492f24",
                "status": "modified"
            },
            {
                "additions": 383,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMaster.java",
                "changes": 733,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 350,
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "patch": "@@ -31,8 +31,6 @@\n import java.util.Random;\n import java.util.Set;\n import java.util.SortedMap;\n-import java.util.Timer;\n-import java.util.TimerTask;\n import java.util.TreeMap;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -74,6 +72,7 @@\n   HMasterRegionInterface {\n   \n   static final Log LOG = LogFactory.getLog(HMaster.class.getName());\n+  static final Long ZERO_L = Long.valueOf(0L);\n \n   /** {@inheritDoc} */\n   public long getProtocolVersion(String protocol,\n@@ -93,6 +92,8 @@ public long getProtocolVersion(String protocol,\n   // started here in HMaster rather than have them have to know about the\n   // hosting class\n   volatile AtomicBoolean closed = new AtomicBoolean(true);\n+  volatile AtomicBoolean shutdownRequested = new AtomicBoolean(false);\n+  volatile AtomicInteger quiescedMetaServers = new AtomicInteger(0);\n   volatile boolean fsOk;\n   Path dir;\n   HBaseConfiguration conf;\n@@ -102,9 +103,9 @@ public long getProtocolVersion(String protocol,\n   int numRetries;\n   long maxRegionOpenTime;\n \n-  DelayQueue<ProcessServerShutdown> shutdownQueue =\n-    new DelayQueue<ProcessServerShutdown>();\n-  BlockingQueue<RegionServerOperation> msgQueue =\n+  DelayQueue<RegionServerOperation> delayedToDoQueue =\n+    new DelayQueue<RegionServerOperation>();\n+  BlockingQueue<RegionServerOperation> toDoQueue =\n     new LinkedBlockingQueue<RegionServerOperation>();\n \n   int leaseTimeout;\n@@ -424,8 +425,7 @@ protected void checkAssigned(final HRegionInfo info,\n           || killedRegions.contains(info.getRegionName()) // queued for offline\n           || regionsToDelete.contains(info.getRegionName())) { // queued for delete\n \n-        unassignedRegions.remove(info.getRegionName());\n-        assignAttempts.remove(info.getRegionName());\n+        unassignedRegions.remove(info);\n         return;\n       }\n       HServerInfo storedInfo = null;\n@@ -458,7 +458,7 @@ protected void checkAssigned(final HRegionInfo info,\n       if (!deadServer &&\n           ((storedInfo != null && storedInfo.getStartCode() != startCode) ||\n               (storedInfo == null &&\n-                  !unassignedRegions.containsKey(info.getRegionName()) &&\n+                  !unassignedRegions.containsKey(info) &&\n                   !pendingRegions.contains(info.getRegionName())\n               )\n           )\n@@ -495,8 +495,7 @@ protected void checkAssigned(final HRegionInfo info,\n           }\n         }\n         // Now get the region assigned\n-        unassignedRegions.put(info.getRegionName(), info);\n-        assignAttempts.put(info.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(info, ZERO_L);\n       }\n     }\n   }\n@@ -818,24 +817,18 @@ synchronized boolean waitForMetaRegionsOrClose() {\n     new ConcurrentHashMap<String, HServerLoad>();\n \n   /**\n-   * The 'unassignedRegions' table maps from a region name to a HRegionInfo \n-   * record, which includes the region's table, its id, and its start/end keys.\n+   * The 'unassignedRegions' table maps from a HRegionInfo to a timestamp that\n+   * indicates the last time we *tried* to assign the region to a RegionServer.\n+   * If the timestamp is out of date, then we can try to reassign it. \n    * \n    * We fill 'unassignedRecords' by scanning ROOT and META tables, learning the\n    * set of all known valid regions.\n    * \n    * <p>Items are removed from this list when a region server reports in that\n    * the region has been deployed.\n    */\n-  final SortedMap<Text, HRegionInfo> unassignedRegions =\n-    Collections.synchronizedSortedMap(new TreeMap<Text, HRegionInfo>());\n-\n-  /**\n-   * The 'assignAttempts' table maps from regions to a timestamp that indicates\n-   * the last time we *tried* to assign the region to a RegionServer. If the \n-   * timestamp is out of date, then we can try to reassign it.\n-   */\n-  final Map<Text, Long> assignAttempts = new ConcurrentHashMap<Text, Long>();\n+  final SortedMap<HRegionInfo, Long> unassignedRegions =\n+    Collections.synchronizedSortedMap(new TreeMap<HRegionInfo, Long>());\n \n   /**\n    * Regions that have been assigned, and the server has reported that it has\n@@ -978,10 +971,7 @@ public HMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n    */\n   void unassignRootRegion() {\n     this.rootRegionLocation.set(null);\n-    this.unassignedRegions.put(HRegionInfo.rootRegionInfo.getRegionName(),\n-        HRegionInfo.rootRegionInfo);\n-    this.assignAttempts.put(HRegionInfo.rootRegionInfo.getRegionName(),\n-        Long.valueOf(0L));\n+    this.unassignedRegions.put(HRegionInfo.rootRegionInfo, ZERO_L);\n   }\n \n   /**\n@@ -1030,7 +1020,11 @@ public Path getRootDir() {\n    * @return Location of the <code>-ROOT-</code> region.\n    */\n   public HServerAddress getRootRegionLocation() {\n-    return this.rootRegionLocation.get();\n+    HServerAddress rootServer = null;\n+    if (!shutdownRequested.get() && !closed.get()) {\n+      rootServer = this.rootRegionLocation.get();\n+    }\n+    return rootServer;\n   }\n   \n   /**\n@@ -1054,11 +1048,11 @@ public void run() {\n         if (rootRegionLocation.get() != null) {\n           // We can't process server shutdowns unless the root region is online \n \n-          op = this.shutdownQueue.poll();\n+          op = this.delayedToDoQueue.poll();\n         }\n         if (op == null ) {\n           try {\n-            op = msgQueue.poll(threadWakeFrequency, TimeUnit.MILLISECONDS);\n+            op = toDoQueue.poll(threadWakeFrequency, TimeUnit.MILLISECONDS);\n           } catch (InterruptedException e) {\n             // continue\n           }\n@@ -1077,7 +1071,7 @@ public void run() {\n             // for the missing meta region(s) to come back online, but since it\n             // is waiting, it cannot process the meta region online operation it\n             // is waiting for. So put this operation back on the queue for now.\n-            if (msgQueue.size() == 0) {\n+            if (toDoQueue.size() == 0) {\n               // The queue is currently empty so wait for a while to see if what\n               // we need comes in first\n               sleeper.sleep();\n@@ -1086,9 +1080,10 @@ public void run() {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Put \" + op.toString() + \" back on queue\");\n               }\n-              msgQueue.put(op);\n+              toDoQueue.put(op);\n             } catch (InterruptedException e) {\n-              throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+              throw new RuntimeException(\n+                  \"Putting into toDoQueue was interrupted.\", e);\n             }\n           }\n         } catch (Exception ex) {\n@@ -1106,9 +1101,10 @@ public void run() {\n           }\n           LOG.warn(\"Processing pending operations: \" + op.toString(), ex);\n           try {\n-            msgQueue.put(op);\n+            toDoQueue.put(op);\n           } catch (InterruptedException e) {\n-            throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+            throw new RuntimeException(\n+                \"Putting into toDoQueue was interrupted.\", e);\n           }\n         }\n       }\n@@ -1255,7 +1251,7 @@ public MapWritable regionServerStartup(HServerInfo serverInfo)\n       if (root != null && root.equals(storedInfo.getServerAddress())) {\n         unassignRootRegion();\n       }\n-      shutdownQueue.put(new ProcessServerShutdown(storedInfo));\n+      delayedToDoQueue.put(new ProcessServerShutdown(storedInfo));\n     }\n \n     // record new server\n@@ -1302,48 +1298,70 @@ private long getServerLabel(final String s) {\n   throws IOException {\n     String serverName = serverInfo.getServerAddress().toString().trim();\n     long serverLabel = getServerLabel(serverName);\n-    if (msgs.length > 0 && msgs[0].getMsg() == HMsg.MSG_REPORT_EXITING) {\n-      synchronized (serversToServerInfo) {\n-        try {\n-          // HRegionServer is shutting down. Cancel the server's lease.\n-          // Note that canceling the server's lease takes care of updating\n-          // serversToServerInfo, etc.\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Region server \" + serverName +\n-            \": MSG_REPORT_EXITING -- cancelling lease\");\n-          }\n-\n-          if (cancelLease(serverName, serverLabel)) {\n-            // Only process the exit message if the server still has a lease.\n-            // Otherwise we could end up processing the server exit twice.\n-            LOG.info(\"Region server \" + serverName +\n-            \": MSG_REPORT_EXITING -- lease cancelled\");\n-            // Get all the regions the server was serving reassigned\n-            // (if we are not shutting down).\n-            if (!closed.get()) {\n-              for (int i = 1; i < msgs.length; i++) {\n-                HRegionInfo info = msgs[i].getRegionInfo();\n-                if (info.getTableDesc().getName().equals(ROOT_TABLE_NAME)) {\n-                  rootRegionLocation.set(null);\n-                } else if (info.getTableDesc().getName().equals(META_TABLE_NAME)) {\n-                  onlineMetaRegions.remove(info.getStartKey());\n-                }\n+//    if (LOG.isDebugEnabled()) {\n+//      LOG.debug(\"received heartbeat from \" + serverName);\n+//    }\n+    if (msgs.length > 0) {\n+      if (msgs[0].getMsg() == HMsg.MSG_REPORT_EXITING) {\n+        synchronized (serversToServerInfo) {\n+          try {\n+            // HRegionServer is shutting down. Cancel the server's lease.\n+            // Note that canceling the server's lease takes care of updating\n+            // serversToServerInfo, etc.\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Region server \" + serverName +\n+              \": MSG_REPORT_EXITING -- cancelling lease\");\n+            }\n+\n+            if (cancelLease(serverName, serverLabel)) {\n+              // Only process the exit message if the server still has a lease.\n+              // Otherwise we could end up processing the server exit twice.\n+              LOG.info(\"Region server \" + serverName +\n+              \": MSG_REPORT_EXITING -- lease cancelled\");\n+              // Get all the regions the server was serving reassigned\n+              // (if we are not shutting down).\n+              if (!closed.get()) {\n+                for (int i = 1; i < msgs.length; i++) {\n+                  HRegionInfo info = msgs[i].getRegionInfo();\n+                  if (info.isRootRegion()) {\n+                    rootRegionLocation.set(null);\n+                  } else if (info.isMetaTable()) {\n+                    onlineMetaRegions.remove(info.getStartKey());\n+                  }\n \n-                this.unassignedRegions.put(info.getRegionName(), info);\n-                this.assignAttempts.put(info.getRegionName(), Long.valueOf(0L));\n+                  this.unassignedRegions.put(info, ZERO_L);\n+                }\n               }\n             }\n-          }\n \n-          // We don't need to return anything to the server because it isn't\n-          // going to do any more work.\n-          return new HMsg[0];\n-        } finally {\n-          serversToServerInfo.notifyAll();\n+            // We don't need to return anything to the server because it isn't\n+            // going to do any more work.\n+            return new HMsg[0];\n+          } finally {\n+            serversToServerInfo.notifyAll();\n+          }\n+        }\n+      } else if (msgs[0].getMsg() == HMsg.MSG_REPORT_QUIESCED) {\n+        LOG.info(\"Region server \" + serverName + \" quiesced\");\n+        if(quiescedMetaServers.incrementAndGet() == serversToServerInfo.size()) {\n+          // If the only servers we know about are meta servers, then we can\n+          // proceed with shutdown\n+          LOG.info(\"All user tables quiesced. Proceeding with shutdown\");\n+          closed.set(true);\n+          synchronized(toDoQueue) {\n+            toDoQueue.clear();                         // Empty the queue\n+            delayedToDoQueue.clear();                  // Empty shut down queue\n+            toDoQueue.notifyAll();                     // Wake main thread\n+          }\n         }\n       }\n     }\n \n+    if (shutdownRequested.get() && !closed.get()) {\n+      // Tell the server to stop serving any user regions\n+      return new HMsg[]{new HMsg(HMsg.MSG_REGIONSERVER_QUIESCE)};\n+    }\n+\n     if (closed.get()) {\n       // Tell server to shut down if we are shutting down.  This should\n       // happen after check of MSG_REPORT_EXITING above, since region server\n@@ -1476,62 +1494,86 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n       switch (incomingMsgs[i].getMsg()) {\n \n       case HMsg.MSG_REPORT_PROCESS_OPEN:\n-        synchronized (this.assignAttempts) {\n+        synchronized (unassignedRegions) {\n           // Region server has acknowledged request to open region.\n-          // Extend region open time by 1/2 max region open time.\n-          assignAttempts.put(region.getRegionName(), \n-              Long.valueOf(assignAttempts.get(\n-                  region.getRegionName()).longValue() +\n-                  (this.maxRegionOpenTime / 2)));\n+          // Extend region open time by max region open time.\n+          unassignedRegions.put(region,\n+              System.currentTimeMillis() + this.maxRegionOpenTime);\n         }\n         break;\n         \n       case HMsg.MSG_REPORT_OPEN:\n-        HRegionInfo regionInfo = unassignedRegions.get(region.getRegionName());\n-\n-        if (regionInfo == null) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"region server \" + info.getServerAddress().toString()\n-                + \" should not have opened region \" + region.getRegionName());\n+        boolean duplicateAssignment = false;\n+        synchronized (unassignedRegions) {\n+          if (unassignedRegions.remove(region) == null) {\n+            if (region.getRegionName().compareTo(\n+                HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n+              // Root region\n+              HServerAddress rootServer = rootRegionLocation.get();\n+              if (rootServer != null) {\n+                if (rootServer.toString().compareTo(serverName) == 0) {\n+                  // A duplicate open report from the correct server\n+                  break;\n+                }\n+                // We received an open report on the root region, but it is\n+                // assigned to a different server\n+                duplicateAssignment = true;\n+              }\n+            } else {\n+              // Not root region. If it is not a pending region, then we are\n+              // going to treat it as a duplicate assignment\n+              if (pendingRegions.contains(region.getRegionName())) {\n+                // A duplicate report from the correct server\n+                break;\n+              }\n+              // Although we can't tell for certain if this is a duplicate\n+              // report from the correct server, we are going to treat it\n+              // as such\n+              duplicateAssignment = true;\n+            }\n           }\n+          if (duplicateAssignment) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"region server \" + info.getServerAddress().toString()\n+                  + \" should not have opened region \" + region.getRegionName());\n+            }\n \n-          // This Region should not have been opened.\n-          // Ask the server to shut it down, but don't report it as closed.  \n-          // Otherwise the HMaster will think the Region was closed on purpose, \n-          // and then try to reopen it elsewhere; that's not what we want.\n+            // This Region should not have been opened.\n+            // Ask the server to shut it down, but don't report it as closed.  \n+            // Otherwise the HMaster will think the Region was closed on purpose, \n+            // and then try to reopen it elsewhere; that's not what we want.\n \n-          returnMsgs.add(new HMsg(HMsg.MSG_REGION_CLOSE_WITHOUT_REPORT, region)); \n+            returnMsgs.add(\n+                new HMsg(HMsg.MSG_REGION_CLOSE_WITHOUT_REPORT, region)); \n \n-        } else {\n-          LOG.info(info.getServerAddress().toString() + \" serving \" +\n-              region.getRegionName());\n-\n-          if (region.getRegionName().compareTo(\n-              HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n-            // Store the Root Region location (in memory)\n-            synchronized (rootRegionLocation) {\n-              this.rootRegionLocation.set(\n-                  new HServerAddress(info.getServerAddress()));\n-              this.rootRegionLocation.notifyAll();\n-            }\n           } else {\n-            // Note that the table has been assigned and is waiting for the meta\n-            // table to be updated.\n+            LOG.info(info.getServerAddress().toString() + \" serving \" +\n+                region.getRegionName());\n+\n+            if (region.getRegionName().compareTo(\n+                HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n+              // Store the Root Region location (in memory)\n+              synchronized (rootRegionLocation) {\n+                this.rootRegionLocation.set(\n+                    new HServerAddress(info.getServerAddress()));\n+                this.rootRegionLocation.notifyAll();\n+              }\n+            } else {\n+              // Note that the table has been assigned and is waiting for the\n+              // meta table to be updated.\n \n-            pendingRegions.add(region.getRegionName());\n+              pendingRegions.add(region.getRegionName());\n \n-            // Queue up an update to note the region location.\n+              // Queue up an update to note the region location.\n \n-            try {\n-              msgQueue.put(new ProcessRegionOpen(info, region));\n-            } catch (InterruptedException e) {\n-              throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n-            }\n-          } \n-          // Remove from unassigned list so we don't assign it to someone else\n-          this.unassignedRegions.remove(region.getRegionName());\n-          this.assignAttempts.remove(region.getRegionName());\n+              try {\n+                toDoQueue.put(new ProcessRegionOpen(info, region));\n+              } catch (InterruptedException e) {\n+                throw new RuntimeException(\n+                    \"Putting into toDoQueue was interrupted.\", e);\n+              }\n+            } \n+          }\n         }\n         break;\n \n@@ -1559,19 +1601,24 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n             deleteRegion = true;\n           }\n \n+          if (region.isMetaTable()) {\n+            // Region is part of the meta table. Remove it from onlineMetaRegions\n+            onlineMetaRegions.remove(region.getStartKey());\n+          }\n+\n           // NOTE: we cannot put the region into unassignedRegions as that\n           //       could create a race with the pending close if it gets \n           //       reassigned before the close is processed.\n \n-          unassignedRegions.remove(region.getRegionName());\n-          assignAttempts.remove(region.getRegionName());\n+          unassignedRegions.remove(region);\n \n           try {\n-            msgQueue.put(new ProcessRegionClose(region, reassignRegion,\n+            toDoQueue.put(new ProcessRegionClose(region, reassignRegion,\n                 deleteRegion));\n \n           } catch (InterruptedException e) {\n-            throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+            throw new RuntimeException(\n+                \"Putting into toDoQueue was interrupted.\", e);\n           }\n         }\n         break;\n@@ -1580,12 +1627,10 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n         // A region has split.\n \n         HRegionInfo newRegionA = incomingMsgs[++i].getRegionInfo();\n-        unassignedRegions.put(newRegionA.getRegionName(), newRegionA);\n-        assignAttempts.put(newRegionA.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(newRegionA, ZERO_L);\n \n         HRegionInfo newRegionB = incomingMsgs[++i].getRegionInfo();\n-        unassignedRegions.put(newRegionB.getRegionName(), newRegionB);\n-        assignAttempts.put(newRegionB.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(newRegionB, ZERO_L);\n \n         LOG.info(\"region \" + region.getRegionName() +\n             \" split. New regions are: \" + newRegionA.getRegionName() + \", \" +\n@@ -1631,15 +1676,22 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n   private void assignRegions(HServerInfo info, String serverName,\n       ArrayList<HMsg> returnMsgs) {\n     \n-    synchronized (this.assignAttempts) {\n+    synchronized (this.unassignedRegions) {\n       \n       // We need to hold a lock on assign attempts while we figure out what to\n       // do so that multiple threads do not execute this method in parallel\n       // resulting in assigning the same region to multiple servers.\n       \n       long now = System.currentTimeMillis();\n-      Set<Text> regionsToAssign = new HashSet<Text>();\n-      for (Map.Entry<Text, Long> e: this.assignAttempts.entrySet()) {\n+      Set<HRegionInfo> regionsToAssign = new HashSet<HRegionInfo>();\n+      for (Map.Entry<HRegionInfo, Long> e: this.unassignedRegions.entrySet()) {\n+        HRegionInfo i = e.getKey();\n+        if (numberOfMetaRegions.get() != onlineMetaRegions.size() &&\n+            !i.isMetaRegion()) {\n+          // Can't assign user regions until all meta regions have been assigned\n+          // and are on-line\n+          continue;\n+        }\n         long diff = now - e.getValue().longValue();\n         if (diff > this.maxRegionOpenTime) {\n           regionsToAssign.add(e.getKey());\n@@ -1720,11 +1772,10 @@ private void assignRegions(HServerInfo info, String serverName,\n         }\n \n         now = System.currentTimeMillis();\n-        for (Text regionName: regionsToAssign) {\n-          HRegionInfo regionInfo = this.unassignedRegions.get(regionName);\n-          LOG.info(\"assigning region \" + regionName + \" to server \" +\n-              serverName);\n-          this.assignAttempts.put(regionName, Long.valueOf(now));\n+        for (HRegionInfo regionInfo: regionsToAssign) {\n+          LOG.info(\"assigning region \" + regionInfo.getRegionName() +\n+              \" to server \" + serverName);\n+          this.unassignedRegions.put(regionInfo, Long.valueOf(now));\n           returnMsgs.add(new HMsg(HMsg.MSG_REGION_OPEN, regionInfo));\n           if (--nregions <= 0) {\n             break;\n@@ -1773,14 +1824,13 @@ private int regionsPerServer(final int nRegionsToAssign,\n    * @param serverName\n    * @param returnMsgs\n    */\n-  private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n+  private void assignRegionsToOneServer(final Set<HRegionInfo> regionsToAssign,\n       final String serverName, final ArrayList<HMsg> returnMsgs) {\n     long now = System.currentTimeMillis();\n-    for (Text regionName: regionsToAssign) {\n-      HRegionInfo regionInfo = this.unassignedRegions.get(regionName);\n-      LOG.info(\"assigning region \" + regionName + \" to the only server \" +\n-          serverName);\n-      this.assignAttempts.put(regionName, Long.valueOf(now));\n+    for (HRegionInfo regionInfo: regionsToAssign) {\n+      LOG.info(\"assigning region \" + regionInfo.getRegionName() +\n+          \" to the only server \" + serverName);\n+      this.unassignedRegions.put(regionInfo, Long.valueOf(now));\n       returnMsgs.add(new HMsg(HMsg.MSG_REGION_OPEN, regionInfo));\n     }\n   }\n@@ -1789,26 +1839,77 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n    * Some internal classes to manage msg-passing and region server operations\n    */\n \n-  private abstract class RegionServerOperation {\n-    RegionServerOperation() {}\n+  private abstract class RegionServerOperation implements Delayed {\n+    private long expire;\n \n-    abstract boolean process() throws IOException;\n+    protected RegionServerOperation() {\n+      // Set the future time at which we expect to be released from the\n+      // DelayQueue we're inserted in on lease expiration.\n+      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n+    }\n+    \n+    /** {@inheritDoc} */\n+    public long getDelay(TimeUnit unit) {\n+      return unit.convert(this.expire - System.currentTimeMillis(),\n+        TimeUnit.MILLISECONDS);\n+    }\n+    \n+    /** {@inheritDoc} */\n+    public int compareTo(Delayed o) {\n+      return Long.valueOf(getDelay(TimeUnit.MILLISECONDS)\n+          - o.getDelay(TimeUnit.MILLISECONDS)).intValue();\n+    }\n+    \n+    protected void requeue() {\n+      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n+      delayedToDoQueue.put(this);\n+    }\n+    \n+    protected boolean rootAvailable() {\n+      boolean available = true;\n+      if (rootRegionLocation.get() == null) {\n+        available = false;\n+        requeue();\n+      }\n+      return available;\n+    }\n+\n+    protected boolean metaTableAvailable() {\n+      boolean available = true;\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"numberOfMetaRegions: \" + numberOfMetaRegions.get() +\n+            \", onlineMetaRegions.size(): \" + onlineMetaRegions.size());\n+      }\n+      if (numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n+        // We can't proceed because not all of the meta regions are online.\n+        // We can't block either because that would prevent the meta region\n+        // online message from being processed. In order to prevent spinning\n+        // in the run queue, put this request on the delay queue to give\n+        // other threads the opportunity to get the meta regions on-line.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Requeuing because not all meta regions are online\");\n+        }\n+        available = false;\n+        requeue();\n+      }\n+      return available;\n+    }\n+    \n+    protected abstract boolean process() throws IOException;\n   }\n \n   /** \n    * Instantiated when a server's lease has expired, meaning it has crashed.\n    * The region server's log file needs to be split up for each region it was\n    * serving, and the regions need to get reassigned.\n    */\n-  private class ProcessServerShutdown extends RegionServerOperation\n-  implements Delayed {\n-    private long expire;\n+  private class ProcessServerShutdown extends RegionServerOperation {\n     private HServerAddress deadServer;\n     private String deadServerName;\n     private Path oldLogDir;\n-    private transient boolean logSplit;\n-    private transient boolean rootChecked;\n-    private transient boolean rootRescanned;\n+    private boolean logSplit;\n+    private boolean rootChecked;\n+    private boolean rootRescanned;\n \n     private class ToDoEntry {\n       boolean deleteRegion;\n@@ -1824,7 +1925,10 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n       }\n     }\n \n-    ProcessServerShutdown(HServerInfo serverInfo) {\n+    /**\n+     * @param serverInfo\n+     */\n+    public ProcessServerShutdown(HServerInfo serverInfo) {\n       super();\n       this.deadServer = serverInfo.getServerAddress();\n       this.deadServerName = this.deadServer.toString();\n@@ -1838,23 +1942,8 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n       dirName.append(\"_\");\n       dirName.append(deadServer.getPort());\n       this.oldLogDir = new Path(dir, dirName.toString());\n-      // Set the future time at which we expect to be released from the\n-      // DelayQueue we're inserted in on lease expiration.\n-      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n     }\n \n-    /** {@inheritDoc} */\n-    public long getDelay(TimeUnit unit) {\n-      return unit.convert(this.expire - System.currentTimeMillis(),\n-        TimeUnit.MILLISECONDS);\n-    }\n-    \n-    /** {@inheritDoc} */\n-    public int compareTo(Delayed o) {\n-      return Long.valueOf(getDelay(TimeUnit.MILLISECONDS)\n-          - o.getDelay(TimeUnit.MILLISECONDS)).intValue();\n-    }\n-    \n     /** {@inheritDoc} */\n     @Override\n     public String toString() {\n@@ -1866,7 +1955,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n         Text regionName) throws IOException {\n \n       ArrayList<ToDoEntry> toDoList = new ArrayList<ToDoEntry>();\n-      TreeMap<Text, HRegionInfo> regions = new TreeMap<Text, HRegionInfo>();\n+      HashSet<HRegionInfo> regions = new HashSet<HRegionInfo>();\n \n       try {\n         while (true) {\n@@ -1958,8 +2047,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n             if (regionsToKill.containsKey(info.getRegionName())) {\n               regionsToKill.remove(info.getRegionName());\n               killList.put(deadServerName, regionsToKill);\n-              unassignedRegions.remove(info.getRegionName());\n-              assignAttempts.remove(info.getRegionName());\n+              unassignedRegions.remove(info);\n               synchronized (regionsToDelete) {\n                 if (regionsToDelete.contains(info.getRegionName())) {\n                   // Delete this region\n@@ -1974,7 +2062,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n \n           } else {\n             // Get region reassigned\n-            regions.put(info.getRegionName(), info);\n+            regions.add(info);\n \n             // If it was pending, remove.\n             // Otherwise will obstruct its getting reassigned.\n@@ -2008,16 +2096,13 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n       }\n \n       // Get regions reassigned\n-      for (Map.Entry<Text, HRegionInfo> e: regions.entrySet()) {\n-        Text region = e.getKey();\n-        HRegionInfo regionInfo = e.getValue();\n-        unassignedRegions.put(region, regionInfo);\n-        assignAttempts.put(region, Long.valueOf(0L));\n+      for (HRegionInfo info: regions) {\n+        unassignedRegions.put(info, ZERO_L);\n       }\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       LOG.info(\"process shutdown of server \" + deadServer + \": logSplit: \" +\n           this.logSplit + \", rootChecked: \" + this.rootChecked +\n           \", rootRescanned: \" + this.rootRescanned + \", numberOfMetaRegions: \" +\n@@ -2040,30 +2125,12 @@ boolean process() throws IOException {\n       }\n \n       if (!rootChecked) {\n-        boolean rootRegionUnavailable = false;\n-        if (rootRegionLocation.get() == null) {\n-          rootRegionUnavailable = true;\n-\n-        } else if (deadServer.equals(rootRegionLocation.get())) {\n-          // We should never get here because whenever an object of this type\n-          // is created, a check is made to see if it is the root server.\n-          // and unassignRootRegion() is called then. However, in the\n-          // unlikely event that we do end up here, let's do the right thing.\n-          unassignRootRegion();\n-          rootRegionUnavailable = true;\n-        }\n-        if (rootRegionUnavailable) {\n-          // We can't do anything until the root region is on-line, put\n-          // us back on the delay queue. Reset the future time at which\n-          // we expect to be released from the DelayQueue we're inserted\n-          // in on lease expiration.\n-          this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n-          shutdownQueue.put(this);\n-          \n-          // Return true so run() does not put us back on the msgQueue\n+        if (!rootAvailable()) {\n+          // Return true so that worker does not put this request back on the\n+          // toDoQueue.\n+          // rootAvailable() has already put it on the delayedToDoQueue\n           return true;\n         }\n-        rootChecked = true;\n       }\n \n       if (!rootRescanned) {\n@@ -2114,27 +2181,14 @@ boolean process() throws IOException {\n         }\n         rootRescanned = true;\n       }\n-\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"numberOfMetaRegions: \" + numberOfMetaRegions.get() +\n-            \", onlineMetaRegions.size(): \" + onlineMetaRegions.size());\n-      }\n-      if (numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-        // We can't proceed because not all of the meta regions are online.\n-        // We can't block either because that would prevent the meta region\n-        // online message from being processed. In order to prevent spinning\n-        // in the run queue, put this request on the delay queue to give\n-        // other threads the opportunity to get the meta regions on-line.\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\n-              \"Requeuing shutdown because not all meta regions are online\");\n-        }\n-        this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n-        shutdownQueue.put(this);\n-        \n-        // Return true so run() does not put us back on the msgQueue\n+      \n+      if (!metaTableAvailable()) {\n+        // We can't proceed because not all meta regions are online.\n+        // metaAvailable() has put this request on the delayedToDoQueue\n+        // Return true so that worker does not put this on the toDoQueue\n         return true;\n       }\n+\n       for (int tries = 0; tries < numRetries; tries++) {\n         try {\n           if (closed.get()) {\n@@ -2180,34 +2234,96 @@ boolean process() throws IOException {\n     }\n   }\n \n+  /**\n+   * Abstract class that performs common operations for \n+   * @see #ProcessRegionClose and @see #ProcessRegionOpen\n+   */\n+  private abstract class ProcessRegionStatusChange\n+    extends RegionServerOperation {\n+\n+    protected final boolean isMetaTable;\n+    protected final HRegionInfo regionInfo;\n+    private MetaRegion metaRegion;\n+    protected Text metaRegionName;\n+    \n+    /**\n+     * @param regionInfo\n+     */\n+    public ProcessRegionStatusChange(HRegionInfo regionInfo) {\n+      super();\n+      this.regionInfo = regionInfo;\n+      this.isMetaTable = regionInfo.isMetaTable();\n+      this.metaRegion = null;\n+      this.metaRegionName = null;\n+    }\n+    \n+    protected boolean metaRegionAvailable() {\n+      boolean available = true;\n+      if (isMetaTable) {\n+        // This operation is for the meta table\n+        if (!rootAvailable()) {\n+          // But we can't proceed unless the root region is available\n+          available = false;\n+        }\n+      } else {\n+        if (!rootScanned || !metaTableAvailable()) {\n+          // The root region has not been scanned or the meta table is not\n+          // available so we can't proceed.\n+          // Put the operation on the delayedToDoQueue\n+          requeue();\n+          available = false;\n+        }\n+      }\n+      return available;\n+    }\n+    \n+    protected HRegionInterface getMetaServer() throws IOException {\n+      if (this.isMetaTable) {\n+        this.metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n+      } else {\n+        if (this.metaRegion == null) {\n+          synchronized (onlineMetaRegions) {\n+            metaRegion = onlineMetaRegions.size() == 1 ? \n+                onlineMetaRegions.get(onlineMetaRegions.firstKey()) :\n+                  onlineMetaRegions.containsKey(regionInfo.getRegionName()) ?\n+                      onlineMetaRegions.get(regionInfo.getRegionName()) :\n+                        onlineMetaRegions.get(onlineMetaRegions.headMap(\n+                            regionInfo.getRegionName()).lastKey());\n+          }\n+          this.metaRegionName = metaRegion.getRegionName();\n+        }\n+      }\n+\n+      HServerAddress server = null;\n+      if (isMetaTable) {\n+        server = rootRegionLocation.get();\n+        \n+      } else {\n+        server = metaRegion.getServer();\n+      }\n+      return connection.getHRegionConnection(server);\n+    }\n+    \n+  }\n   /**\n    * ProcessRegionClose is instantiated when a region server reports that it\n    * has closed a region.\n    */\n-  private class ProcessRegionClose extends RegionServerOperation {\n-    private HRegionInfo regionInfo;\n+  private class ProcessRegionClose extends ProcessRegionStatusChange {\n     private boolean reassignRegion;\n     private boolean deleteRegion;\n-    private boolean rootRegion;\n \n-    ProcessRegionClose(HRegionInfo regionInfo, boolean reassignRegion,\n+    /**\n+     * @param regionInfo\n+     * @param reassignRegion\n+     * @param deleteRegion\n+     */\n+    public ProcessRegionClose(HRegionInfo regionInfo, boolean reassignRegion,\n         boolean deleteRegion) {\n \n-      super();\n-\n-      this.regionInfo = regionInfo;\n+      super(regionInfo);\n       this.reassignRegion = reassignRegion;\n       this.deleteRegion = deleteRegion;\n-\n-      // If the region closing down is a meta region then we need to update\n-      // the ROOT table\n-\n-      if (this.regionInfo.getTableDesc().getName().equals(META_TABLE_NAME)) {\n-        this.rootRegion = true;\n-\n-      } else {\n-        this.rootRegion = false;\n-      }\n     }\n \n     /** {@inheritDoc} */\n@@ -2217,7 +2333,7 @@ public String toString() {\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       for (int tries = 0; tries < numRetries; tries++) {\n         if (closed.get()) {\n           return true;\n@@ -2226,50 +2342,15 @@ boolean process() throws IOException {\n \n         // Mark the Region as unavailable in the appropriate meta table\n \n-        Text metaRegionName;\n-        HRegionInterface server;\n-        if (rootRegion) {\n-          if (rootRegionLocation.get() == null || !rootScanned) {\n-            // We can't proceed until the root region is online and has been\n-            // scanned\n-            return false;\n-          }\n-          metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n-          server = connection.getHRegionConnection(rootRegionLocation.get());\n-          onlineMetaRegions.remove(regionInfo.getStartKey());\n-\n-        } else {\n-          if (!rootScanned ||\n-              numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-            \n-            // We can't proceed because not all of the meta regions are online.\n-            // We can't block either because that would prevent the meta region\n-            // online message from being processed. So return false to have this\n-            // operation requeued.\n-            \n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Requeuing close because rootScanned=\" +\n-                  rootScanned + \", numberOfMetaRegions=\" +\n-                  numberOfMetaRegions.get() + \", onlineMetaRegions.size()=\" +\n-                  onlineMetaRegions.size());\n-            }\n-            return false;\n-          }\n-\n-          MetaRegion r = null;\n-          synchronized (onlineMetaRegions) {\n-            if (onlineMetaRegions.containsKey(regionInfo.getRegionName())) {\n-              r = onlineMetaRegions.get(regionInfo.getRegionName());\n-\n-            } else {\n-              r = onlineMetaRegions.get(onlineMetaRegions.headMap(\n-                  regionInfo.getRegionName()).lastKey());\n-            }\n-          }\n-          metaRegionName = r.getRegionName();\n-          server = connection.getHRegionConnection(r.getServer());\n+        if (!metaRegionAvailable()) {\n+          // We can't proceed unless the meta region we are going to update\n+          // is online. metaRegionAvailable() has put this operation on the\n+          // delayedToDoQueue, so return true so the operation is not put \n+          // back on the toDoQueue\n+          return true;\n         }\n \n+        HRegionInterface server = getMetaServer();\n         try {\n           BatchUpdate b = new BatchUpdate(rand.nextLong());\n           long lockid = b.startUpdate(regionInfo.getRegionName());\n@@ -2298,8 +2379,7 @@ boolean process() throws IOException {\n       if (reassignRegion) {\n         LOG.info(\"reassign region: \" + regionInfo.getRegionName());\n \n-        unassignedRegions.put(regionInfo.getRegionName(), regionInfo);\n-        assignAttempts.put(regionInfo.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(regionInfo, ZERO_L);\n \n       } else if (deleteRegion) {\n         try {\n@@ -2320,19 +2400,18 @@ boolean process() throws IOException {\n    * serving a region. This applies to all meta and user regions except the \n    * root region which is handled specially.\n    */\n-  private class ProcessRegionOpen extends RegionServerOperation {\n-    private final boolean rootRegion;\n-    private final HRegionInfo region;\n+  private class ProcessRegionOpen extends ProcessRegionStatusChange {\n     private final HServerAddress serverAddress;\n     private final byte [] startCode;\n \n-    ProcessRegionOpen(HServerInfo info, HRegionInfo region)\n+    /**\n+     * @param info\n+     * @param regionInfo\n+     * @throws IOException\n+     */\n+    public ProcessRegionOpen(HServerInfo info, HRegionInfo regionInfo)\n     throws IOException {\n-      // If true, the region which just came on-line is a META region.\n-      // We need to look in the ROOT region for its information.  Otherwise,\n-      // its just an ordinary region. Look for it in the META table.\n-      this.rootRegion = region.getTableDesc().getName().equals(META_TABLE_NAME);\n-      this.region = region;\n+      super(regionInfo);\n       this.serverAddress = info.getServerAddress();\n       this.startCode = Writables.longToBytes(info.getStartCode());\n     }\n@@ -2344,72 +2423,40 @@ public String toString() {\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       for (int tries = 0; tries < numRetries; tries++) {\n         if (closed.get()) {\n           return true;\n         }\n-        LOG.info(region.toString() + \" open on \" + \n+        LOG.info(regionInfo.toString() + \" open on \" + \n             this.serverAddress.toString());\n \n-        // Register the newly-available Region's location.\n-        Text metaRegionName;\n-        HRegionInterface server;\n-        if (this.rootRegion) {\n-          if (rootRegionLocation.get() == null || !rootScanned) {\n-            // We can't proceed until root region is online and scanned\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"root region: \" + \n-                ((rootRegionLocation.get() != null)?\n-                  rootRegionLocation.get().toString(): \"null\") +\n-                \", rootScanned: \" + rootScanned);\n-            }\n-            return false;\n-          }\n-          metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n-          server = connection.getHRegionConnection(rootRegionLocation.get());\n-        } else {\n-          if (!rootScanned ||\n-              numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-            // We can't proceed because not all of the meta regions are online.\n-            // We can't block either because that would prevent the meta region\n-            // online message from being processed. So return false to have this\n-            // operation requeued.\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Requeuing open because rootScanned: \" +\n-                  rootScanned + \", numberOfMetaRegions: \" +\n-                  numberOfMetaRegions.get() + \", onlineMetaRegions.size(): \" +\n-                  onlineMetaRegions.size());\n-            }\n-            return false;\n-          }\n-\n-          MetaRegion r = null;\n-          synchronized (onlineMetaRegions) {\n-            r = onlineMetaRegions.containsKey(region.getRegionName()) ?\n-                onlineMetaRegions.get(region.getRegionName()) :\n-                  onlineMetaRegions.get(onlineMetaRegions.headMap(\n-                      region.getRegionName()).lastKey());\n-          }\n-          metaRegionName = r.getRegionName();\n-          server = connection.getHRegionConnection(r.getServer());\n+        if (!metaRegionAvailable()) {\n+          // We can't proceed unless the meta region we are going to update\n+          // is online. metaRegionAvailable() has put this operation on the\n+          // delayedToDoQueue, so return true so the operation is not put \n+          // back on the toDoQueue\n+          return true;\n         }\n+\n+        // Register the newly-available Region's location.\n         \n-        LOG.info(\"updating row \" + region.getRegionName() + \" in table \" +\n+        HRegionInterface server = getMetaServer();\n+        LOG.info(\"updating row \" + regionInfo.getRegionName() + \" in table \" +\n           metaRegionName + \" with startcode \" +\n           Writables.bytesToLong(this.startCode) + \" and server \"+\n           serverAddress.toString());\n         try {\n           BatchUpdate b = new BatchUpdate(rand.nextLong());\n-          long lockid = b.startUpdate(region.getRegionName());\n+          long lockid = b.startUpdate(regionInfo.getRegionName());\n           b.put(lockid, COL_SERVER,\n             Writables.stringToBytes(serverAddress.toString()));\n           b.put(lockid, COL_STARTCODE, startCode);\n           server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n-          if (region.getTableDesc().getName().equals(META_TABLE_NAME)) {\n+          if (isMetaTable) {\n             // It's a meta region.\n             MetaRegion m = new MetaRegion(this.serverAddress,\n-              this.region.getRegionName(), this.region.getStartKey());\n+              this.regionInfo.getRegionName(), this.regionInfo.getStartKey());\n             if (!initialMetaScanComplete) {\n               // Put it on the queue to be scanned for the first time.\n               try {\n@@ -2422,11 +2469,11 @@ boolean process() throws IOException {\n             } else {\n               // Add it to the online meta regions\n               LOG.debug(\"Adding to onlineMetaRegions: \" + m.toString());\n-              onlineMetaRegions.put(this.region.getStartKey(), m);\n+              onlineMetaRegions.put(this.regionInfo.getStartKey(), m);\n             }\n           }\n           // If updated successfully, remove from pending list.\n-          pendingRegions.remove(region.getRegionName());\n+          pendingRegions.remove(regionInfo.getRegionName());\n           break;\n         } catch (IOException e) {\n           if (tries == numRetries - 1) {\n@@ -2449,19 +2496,8 @@ public boolean isMasterRunning() {\n \n   /** {@inheritDoc} */\n   public void shutdown() {\n-    TimerTask tt = new TimerTask() {\n-      @Override\n-      public void run() {\n-        closed.set(true);\n-        synchronized(msgQueue) {\n-          msgQueue.clear();                         // Empty the queue\n-          shutdownQueue.clear();                    // Empty shut down queue\n-          msgQueue.notifyAll();                     // Wake main thread\n-        }\n-      }\n-    };\n-    Timer t = new Timer(getName() + \"-Shutdown\");\n-    t.schedule(tt, 10);\n+    LOG.info(\"Cluster shutdown requested. Starting to quiesce servers\");\n+    this.shutdownRequested.set(true);\n   }\n \n   /** {@inheritDoc} */\n@@ -2563,8 +2599,7 @@ private void createTable(final HRegionInfo newRegion) throws IOException {\n \n       // 5. Get it assigned to a server\n \n-      this.unassignedRegions.put(regionName, info);\n-      this.assignAttempts.put(regionName, Long.valueOf(0L));\n+      this.unassignedRegions.put(info, ZERO_L);\n \n     } finally {\n       tableInCreation.remove(newRegion.getTableDesc().getName());\n@@ -2838,14 +2873,12 @@ protected void postProcessMeta(MetaRegion m, HRegionInterface server)\n         }\n \n         if (online) {                         // Bring offline regions on-line\n-          if (!unassignedRegions.containsKey(i.getRegionName())) {\n-            unassignedRegions.put(i.getRegionName(), i);\n-            assignAttempts.put(i.getRegionName(), Long.valueOf(0L));\n+          if (!unassignedRegions.containsKey(i)) {\n+            unassignedRegions.put(i, ZERO_L);\n           }\n \n         } else {                              // Prevent region from getting assigned.\n-          unassignedRegions.remove(i.getRegionName());\n-          assignAttempts.remove(i.getRegionName());\n+          unassignedRegions.remove(i);\n         }\n       }\n \n@@ -3069,7 +3102,7 @@ public void leaseExpired() {\n       // here because the new server will start serving the root region before\n       // the ProcessServerShutdown operation has a chance to split the log file.\n       if (info != null) {\n-        shutdownQueue.put(new ProcessServerShutdown(info));\n+        delayedToDoQueue.put(new ProcessServerShutdown(info));\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMaster.java",
                "sha": "097eb39102ad5c68da1d56b53e922fd86af66f53",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMsg.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMsg.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/HMsg.java",
                "patch": "@@ -48,6 +48,9 @@\n \n   /** Stop serving the specified region and don't report back that it's closed */\n   public static final byte MSG_REGION_CLOSE_WITHOUT_REPORT = 6;\n+  \n+  /** Stop serving user regions */\n+  public static final byte MSG_REGIONSERVER_QUIESCE = 7;\n \n   // Messages sent from the region server to the master\n   \n@@ -72,9 +75,12 @@\n    * region server is shutting down\n    * \n    * note that this message is followed by MSG_REPORT_CLOSE messages for each\n-   * region the region server was serving.\n+   * region the region server was serving, unless it was told to quiesce.\n    */\n   public static final byte MSG_REPORT_EXITING = 104;\n+  \n+  /** region server has closed all user regions but is still serving meta regions */\n+  public static final byte MSG_REPORT_QUIESCED = 105;\n \n   byte msg;\n   HRegionInfo info;\n@@ -148,6 +154,10 @@ public String toString() {\n       message.append(\"MSG_REGION_CLOSE_WITHOUT_REPORT : \");\n       break;\n       \n+    case MSG_REGIONSERVER_QUIESCE:\n+      message.append(\"MSG_REGIONSERVER_QUIESCE : \");\n+      break;\n+      \n     case MSG_REPORT_PROCESS_OPEN:\n       message.append(\"MSG_REPORT_PROCESS_OPEN : \");\n       break;\n@@ -168,6 +178,10 @@ public String toString() {\n       message.append(\"MSG_REPORT_EXITING : \");\n       break;\n       \n+    case MSG_REPORT_QUIESCED:\n+      message.append(\"MSG_REPORT_QUIESCED : \");\n+      break;\n+      \n     default:\n       message.append(\"unknown message code (\");\n       message.append(msg);",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMsg.java",
                "sha": "e9137ba962ae47ffcf2f302b69f2e2c564357999",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionInfo.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "patch": "@@ -193,6 +193,21 @@ public HTableDescriptor getTableDesc(){\n     return tableDesc;\n   }\n   \n+  /** @return true if this is the root region */\n+  public boolean isRootRegion() {\n+    return this.tableDesc.isRootRegion();\n+  }\n+  \n+  /** @return true if this is the meta table */\n+  public boolean isMetaTable() {\n+    return this.tableDesc.isMetaTable();\n+  }\n+\n+  /** @return true if this region is a meta region */\n+  public boolean isMetaRegion() {\n+    return this.tableDesc.isMetaRegion();\n+  }\n+  \n   /**\n    * @return True if has been split and has daughters.\n    */",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "sha": "cb881c7a31b4471668cb7bd6b8825b239480eb60",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 10,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "patch": "@@ -81,6 +81,8 @@\n   // Chore threads need to know about the hosting class.\n   protected final AtomicBoolean stopRequested = new AtomicBoolean(false);\n   \n+  protected final AtomicBoolean quiesced = new AtomicBoolean(false);\n+  \n   // Go down hard.  Used if file system becomes unavailable and also in\n   // debugging and unit tests.\n   protected volatile boolean abortRequested;\n@@ -652,6 +654,7 @@ public HRegionServer(HServerAddress address, HBaseConfiguration conf)\n    * load/unload instructions.\n    */\n   public void run() {\n+    boolean quiesceRequested = false;\n     try {\n       init(reportForDuty());\n       long lastMsg = 0;\n@@ -682,16 +685,24 @@ public void run() {\n               HMsg msgs[] =\n                 this.hbaseMaster.regionServerReport(serverInfo, outboundArray);\n               lastMsg = System.currentTimeMillis();\n+              \n+              if (this.quiesced.get() && onlineRegions.size() == 0) {\n+                // We've just told the master we're exiting because we aren't\n+                // serving any regions. So set the stop bit and exit.\n+                LOG.info(\"Server quiesced and not serving any regions. \" +\n+                    \"Starting shutdown\");\n+                stopRequested.set(true);\n+                continue;\n+              }\n+              \n               // Queue up the HMaster's instruction stream for processing\n               boolean restart = false;\n               for(int i = 0; i < msgs.length && !stopRequested.get() &&\n                   !restart; i++) {\n                 switch(msgs[i].getMsg()) {\n                 \n                 case HMsg.MSG_CALL_SERVER_STARTUP:\n-                  if (LOG.isDebugEnabled()) {\n-                    LOG.debug(\"Got call server startup message\");\n-                  }\n+                  LOG.info(\"Got call server startup message\");\n                   // We the MSG_CALL_SERVER_STARTUP on startup but we can also\n                   // get it when the master is panicing because for instance\n                   // the HDFS has been yanked out from under it.  Be wary of\n@@ -725,11 +736,22 @@ public void run() {\n                   break;\n \n                 case HMsg.MSG_REGIONSERVER_STOP:\n-                  if (LOG.isDebugEnabled()) {\n-                    LOG.debug(\"Got regionserver stop message\");\n-                  }\n+                  LOG.info(\"Got regionserver stop message\");\n                   stopRequested.set(true);\n                   break;\n+                  \n+                case HMsg.MSG_REGIONSERVER_QUIESCE:\n+                  if (!quiesceRequested) {\n+                    LOG.info(\"Got quiesce server message\");\n+                    try {\n+                      toDo.put(new ToDoEntry(msgs[i]));\n+                    } catch (InterruptedException e) {\n+                      throw new RuntimeException(\"Putting into msgQueue was \" +\n+                        \"interrupted.\", e);\n+                    }\n+                    quiesceRequested = true;\n+                  }\n+                  break;\n \n                 default:\n                   if (fsOk) {\n@@ -1101,6 +1123,10 @@ public void run() {\n           try {\n             LOG.info(e.msg.toString());\n             switch(e.msg.getMsg()) {\n+            \n+            case HMsg.MSG_REGIONSERVER_QUIESCE:\n+              closeUserRegions();\n+              break;\n \n             case HMsg.MSG_REGION_OPEN:\n               // Open a region\n@@ -1149,12 +1175,19 @@ public void run() {\n     }\n   }\n   \n-  void openRegion(final HRegionInfo regionInfo) throws IOException {\n+  void openRegion(final HRegionInfo regionInfo) {\n     HRegion region = onlineRegions.get(regionInfo.getRegionName());\n     if(region == null) {\n-      region = new HRegion(new Path(this.conf.get(HConstants.HBASE_DIR)),\n-        this.log, FileSystem.get(conf), conf, regionInfo, null,\n-        this.cacheFlusher);\n+      try {\n+        region = new HRegion(new Path(this.conf.get(HConstants.HBASE_DIR)),\n+            this.log, FileSystem.get(conf), conf, regionInfo, null,\n+            this.cacheFlusher);\n+        \n+      } catch (IOException e) {\n+        LOG.error(\"error opening region \" + regionInfo.getRegionName(), e);\n+        reportClose(region);\n+        return;\n+      }\n       this.lock.writeLock().lock();\n       try {\n         this.log.setSequenceNumber(region.getMinSequenceId());\n@@ -1208,6 +1241,45 @@ void closeRegion(final HRegionInfo hri, final boolean reportWhenCompleted)\n     return regionsToClose;\n   }\n \n+  /** Called as the first stage of cluster shutdown. */\n+  void closeUserRegions() {\n+    ArrayList<HRegion> regionsToClose = new ArrayList<HRegion>();\n+    this.lock.writeLock().lock();\n+    try {\n+      synchronized (onlineRegions) {\n+        for (Iterator<Map.Entry<Text, HRegion>> i =\n+          onlineRegions.entrySet().iterator();\n+        i.hasNext();) {\n+          Map.Entry<Text, HRegion> e = i.next();\n+          HRegion r = e.getValue();\n+          if (!r.getRegionInfo().isMetaRegion()) {\n+            regionsToClose.add(r);\n+            i.remove();\n+          }\n+        }\n+      }\n+    } finally {\n+      this.lock.writeLock().unlock();\n+    }\n+    for(HRegion region: regionsToClose) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"closing region \" + region.getRegionName());\n+      }\n+      try {\n+        region.close(false);\n+      } catch (IOException e) {\n+        LOG.error(\"error closing region \" + region.getRegionName(),\n+          RemoteExceptionHandler.checkIOException(e));\n+      }\n+    }\n+    this.quiesced.set(true);\n+    if (onlineRegions.size() == 0) {\n+      outboundMsgs.add(new HMsg(HMsg.MSG_REPORT_EXITING));\n+    } else {\n+      outboundMsgs.add(new HMsg(HMsg.MSG_REPORT_QUIESCED));\n+    }\n+  }\n+\n   //\n   // HRegionInterface\n   //",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "sha": "94c0ab3ee1e3e764a5627633be6acbaff83518bc",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "patch": "@@ -52,7 +52,8 @@\n             HColumnDescriptor.CompressionType.NONE, false, Integer.MAX_VALUE,\n             null));\n   \n-\n+  private boolean rootregion;\n+  private boolean metaregion;\n   private Text name;\n   // TODO: Does this need to be a treemap?  Can it be a HashMap?\n   private final TreeMap<Text, HColumnDescriptor> families;\n@@ -69,6 +70,8 @@\n \n   /** Used to construct the table descriptors for root and meta tables */\n   private HTableDescriptor(Text name, HColumnDescriptor family) {\n+    rootregion = name.equals(HConstants.ROOT_TABLE_NAME);\n+    this.metaregion = true;\n     this.name = new Text(name);\n     this.families = new TreeMap<Text, HColumnDescriptor>();\n     families.put(family.getName(), family);\n@@ -92,13 +95,30 @@ public HTableDescriptor() {\n    * <code>[a-zA-Z_0-9]\n    */\n   public HTableDescriptor(String name) {\n+    this();\n     Matcher m = LEGAL_TABLE_NAME.matcher(name);\n     if (m == null || !m.matches()) {\n       throw new IllegalArgumentException(\n           \"Table names can only contain 'word characters': i.e. [a-zA-Z_0-9\");\n     }\n-    this.name = new Text(name);\n-    this.families = new TreeMap<Text, HColumnDescriptor>();\n+    this.name.set(name);\n+    this.rootregion = false;\n+    this.metaregion = false;\n+  }\n+  \n+  /** @return true if this is the root region */\n+  public boolean isRootRegion() {\n+    return rootregion;\n+  }\n+  \n+  /** @return true if table is the meta table */\n+  public boolean isMetaTable() {\n+    return metaregion && !rootregion;\n+  }\n+  \n+  /** @return true if this is a meta region (part of the root or meta tables) */\n+  public boolean isMetaRegion() {\n+    return metaregion;\n   }\n \n   /** @return name of table */\n@@ -165,6 +185,8 @@ public int hashCode() {\n \n   /** {@inheritDoc} */\n   public void write(DataOutput out) throws IOException {\n+    out.writeBoolean(rootregion);\n+    out.writeBoolean(metaregion);\n     name.write(out);\n     out.writeInt(families.size());\n     for(Iterator<HColumnDescriptor> it = families.values().iterator();\n@@ -175,6 +197,8 @@ public void write(DataOutput out) throws IOException {\n \n   /** {@inheritDoc} */\n   public void readFields(DataInput in) throws IOException {\n+    this.rootregion = in.readBoolean();\n+    this.metaregion = in.readBoolean();\n     this.name.readFields(in);\n     int numCols = in.readInt();\n     families.clear();",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "sha": "03a97dc76f81bff3746ce80293db6545ee39a609",
                "status": "modified"
            }
        ],
        "message": "HADOOP-2338 Fix NullPointerException in master server.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@602226 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/bccf1dc26f0af20b0ec70d420990e7fd9e01c4d7",
        "patched_files": [
            "HTableDescriptor.java",
            "HRegionInfo.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegionInfo.java",
            "TestHTableDescriptor.java"
        ]
    },
    "hbase_621dc88": {
        "bug_id": "hbase_621dc88",
        "commit": "https://github.com/apache/hbase/commit/621dc88c7940ac8ab3719872e4cc1643dcf87da8",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=621dc88c7940ac8ab3719872e4cc1643dcf87da8",
                "deletions": 21,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "patch": "@@ -32,12 +32,15 @@\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.CellUtil;\n+import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.TableDescriptors;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.client.AsyncClusterConnection;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionReplicaUtil;\n import org.apache.hadoop.hbase.client.TableDescriptor;\n import org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint;\n import org.apache.hadoop.hbase.replication.WALEntryFilter;\n@@ -162,9 +165,9 @@ private void getRegionLocations(CompletableFuture<RegionLocations> future,\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches, and also there is no null elements in the returned RegionLocations\n+        // matches.\n         if (locs.size() == tableDesc.getRegionReplication() &&\n-          locs.size() == locs.numNonNullElements() &&\n+          locs.getDefaultRegionLocation() != null &&\n           Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n             encodedRegionName)) {\n           future.complete(locs);\n@@ -182,16 +185,16 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n       future.complete(Long.valueOf(entries.size()));\n       return;\n     }\n-    if (!Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n-      encodedRegionName)) {\n+    RegionInfo defaultReplica = locs.getDefaultRegionLocation().getRegion();\n+    if (!Bytes.equals(defaultReplica.getEncodedNameAsBytes(), encodedRegionName)) {\n       // the region name is not equal, this usually means the region has been split or merged, so\n       // give up replicating as the new region(s) should already have all the data of the parent\n       // region(s).\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\n           \"Skipping {} entries in table {} because located region {} is different than\" +\n             \" the original region {} from WALEdit\",\n-          tableDesc.getTableName(), locs.getDefaultRegionLocation().getRegion().getEncodedName(),\n+          tableDesc.getTableName(), defaultReplica.getEncodedName(),\n           Bytes.toStringBinary(encodedRegionName));\n       }\n       future.complete(Long.valueOf(entries.size()));\n@@ -202,24 +205,26 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n     AtomicLong skippedEdits = new AtomicLong(0);\n \n     for (int i = 1, n = locs.size(); i < n; i++) {\n-      final int replicaId = i;\n-      FutureUtils.addListener(connection.replay(tableDesc.getTableName(),\n-        locs.getRegionLocation(replicaId).getRegion().getEncodedNameAsBytes(), row, entries,\n-        replicaId, numRetries, operationTimeoutNs), (r, e) -> {\n-          if (e != null) {\n-            LOG.warn(\"Failed to replicate to {}\", locs.getRegionLocation(replicaId), e);\n-            error.compareAndSet(null, e);\n-          } else {\n-            AtomicUtils.updateMax(skippedEdits, r.longValue());\n-          }\n-          if (remainingTasks.decrementAndGet() == 0) {\n-            if (error.get() != null) {\n-              future.completeExceptionally(error.get());\n+      // Do not use the elements other than the default replica as they may be null. We will fail\n+      // earlier if the location for default replica is null.\n+      final RegionInfo replica = RegionReplicaUtil.getRegionInfoForReplica(defaultReplica, i);\n+      FutureUtils\n+        .addListener(connection.replay(tableDesc.getTableName(), replica.getEncodedNameAsBytes(),\n+          row, entries, replica.getReplicaId(), numRetries, operationTimeoutNs), (r, e) -> {\n+            if (e != null) {\n+              LOG.warn(\"Failed to replicate to {}\", replica, e);\n+              error.compareAndSet(null, e);\n             } else {\n-              future.complete(skippedEdits.get());\n+              AtomicUtils.updateMax(skippedEdits, r.longValue());\n             }\n-          }\n-        });\n+            if (remainingTasks.decrementAndGet() == 0) {\n+              if (error.get() != null) {\n+                future.completeExceptionally(error.get());\n+              } else {\n+                future.complete(skippedEdits.get());\n+              }\n+            }\n+          });\n     }\n   }\n \n@@ -245,6 +250,10 @@ private void logSkipped(TableName tableName, List<Entry> entries, String reason)\n     FutureUtils.addListener(locateFuture, (locs, error) -> {\n       if (error != null) {\n         future.completeExceptionally(error);\n+      } else if (locs.getDefaultRegionLocation() == null) {\n+        future.completeExceptionally(\n+          new HBaseIOException(\"No location found for default replica of table=\" +\n+            tableDesc.getTableName() + \" row='\" + Bytes.toStringBinary(row) + \"'\"));\n       } else {\n         replicate(future, locs, tableDesc, encodedRegionName, row, entries);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "sha": "2c3b19b6a45c3aa8820b49bc600b04efd7a37ef3",
                "status": "modified"
            }
        ],
        "message": "HBASE-22553 NPE in RegionReplicaReplicationEndpoint",
        "parent": "https://github.com/apache/hbase/commit/6278c98f5d0b19ff830368c47204232760e2b4ea",
        "patched_files": [
            "RegionReplicaReplicationEndpoint.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRegionReplicaReplicationEndpoint.java"
        ]
    },
    "hbase_62b33ba": {
        "bug_id": "hbase_62b33ba",
        "commit": "https://github.com/apache/hbase/commit/62b33ba91052ee138c8110f2ebb858accd8da4ac",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -123,6 +123,8 @@ Release 0.20.0 - Unreleased\n                Purtell)\n    HBASE-1311  ZooKeeperWrapper: Failed to set watcher on ZNode /hbase/master\n                (Nitay Joffe via Stack)\n+   HBASE-1391  NPE in TableInputFormatBase$TableRecordReader.restart if zoo.cfg\n+               is wrong or missing on task trackers\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/CHANGES.txt",
                "sha": "e8e1fac4b9b20c9926d6a972c546e15c03620ba3",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.mapred.FileInputFormat;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.JobConfigurable;\n+import org.apache.hadoop.util.StringUtils;\n \n /**\n  * Convert HBase tabular data into a format that is consumable by Map/Reduce.\n@@ -58,7 +59,7 @@ public void configure(JobConf job) {\n     try {\n       setHTable(new HTable(new HBaseConfiguration(job), tableNames[0].getName()));\n     } catch (Exception e) {\n-      LOG.error(e);\n+      LOG.error(StringUtils.stringifyException(e));\n     }\n   }\n \n@@ -69,6 +70,12 @@ public void validateInput(JobConf job) throws IOException {\n       throw new IOException(\"expecting one table name\");\n     }\n \n+    // connected to table?\n+    if (getHTable() == null) {\n+      throw new IOException(\"could not connect to table '\" +\n+        tableNames[0].getName() + \"'\");\n+    }\n+\n     // expecting at least one column\n     String colArg = job.get(COLUMN_LIST);\n     if (colArg == null || colArg.length() == 0) {",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "sha": "38d56b8596d64d2b6a3e2432464610cb2f1114b7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "patch": "@@ -304,6 +304,13 @@ protected void setInputColumns(byte [][] inputColumns) {\n     this.inputColumns = inputColumns;\n   }\n \n+  /**\n+   * Allows subclasses to get the {@link HTable}.\n+   */\n+  protected HTable getHTable() {\n+    return this.table;\n+  }\n+\n   /**\n    * Allows subclasses to set the {@link HTable}.\n    *",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "sha": "ba82b7a9f9bedf25a24d15e2e4a100ec0780311f",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "patch": "@@ -20,6 +20,8 @@\n package org.apache.hadoop.hbase.zookeeper;\n \n import java.io.IOException;\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Properties;\n@@ -32,6 +34,7 @@\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.HServerInfo;\n import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.Watcher;\n@@ -158,6 +161,7 @@ private static void loadZooKeeperConfig() {\n \n     // The clientPort option may come after the server.X hosts, so we need to\n     // grab everything and then create the final host:port comma separated list.\n+    boolean anyValid = false;\n     for (Entry<Object,Object> property : properties.entrySet()) {\n       String key = property.getKey().toString().trim();\n       String value = property.getValue().toString().trim();\n@@ -167,9 +171,20 @@ private static void loadZooKeeperConfig() {\n       else if (key.startsWith(\"server.\")) {\n         String host = value.substring(0, value.indexOf(':'));\n         servers.add(host);\n+        try {\n+          InetAddress.getByName(host);\n+          anyValid = true;\n+        } catch (UnknownHostException e) {\n+          LOG.warn(StringUtils.stringifyException(e));\n+        }\n       }\n     }\n \n+    if (!anyValid) {\n+      LOG.error(\"no valid quorum servers found in \" + ZOOKEEPER_CONFIG_NAME);\n+      return;\n+    }\n+\n     if (clientPort == null) {\n       LOG.error(\"no clientPort found in \" + ZOOKEEPER_CONFIG_NAME);\n       return;",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "sha": "a8e1f5e314119f1662e0ff5ded6514a33b664ab9",
                "status": "modified"
            }
        ],
        "message": "HBASE-1391 NPE in TableInputFormatBase.restart if zoo.cfg is wrong or missing on tasktrackers\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@773825 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/58e206e9c80955372853227225d736ea1e992394",
        "patched_files": [
            "TableInputFormat.java",
            "TableInputFormatBase.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java",
            "TestTableInputFormat.java"
        ]
    },
    "hbase_64c6a07": {
        "bug_id": "hbase_64c6a07",
        "commit": "https://github.com/apache/hbase/commit/64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -373,6 +373,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2657  TestTableResource is broken in trunk\n    HBASE-2662  TestScannerResource.testScannerResource broke in trunk\n    HBASE-2667  TestHLog.testSplit failing in trunk\n+   HBASE-2614  killing server in TestMasterTransitions causes NPEs and test deadlock\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/CHANGES.txt",
                "sha": "ab2232bdbc131b00fa5e8b5a8b9ea2b56526178b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "patch": "@@ -589,6 +589,7 @@ public void interruptAndStop() {\n     synchronized(scannerLock){\n       if (isAlive()) {\n         super.interrupt();\n+        LOG.info(\"Interrupted\");\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "sha": "c3935accd4900434d7dbae90e43cfc4cd5e77dad",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -447,6 +447,9 @@ public void run() {\n           if (this.serverManager.numServers() == 0) {\n             startShutdown();\n             break;\n+          } else {\n+            LOG.debug(\"Waiting on \" +\n+              this.serverManager.getServersToServerInfo().keySet().toString());\n           }\n         }\n         final HServerAddress root = this.regionManager.getRootRegionLocation();",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "5946eee5a65901b22187ce672639e6aac8eb9354",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 12,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "patch": "@@ -592,17 +592,8 @@ public boolean metaRegionsInTransition() {\n    * regions can shut down.\n    */\n   public void stopScanners() {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"telling root scanner to stop\");\n-    }\n-    rootScannerThread.interruptAndStop();\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"telling meta scanner to stop\");\n-    }\n-    metaScannerThread.interruptAndStop();\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"meta and root scanners notified\");\n-    }\n+    this.rootScannerThread.interruptAndStop();\n+    this.metaScannerThread.interruptAndStop();\n   }\n \n   /** Stop the region assigner */\n@@ -1152,7 +1143,8 @@ public HServerAddress getRootRegionLocation() {\n    */\n   public void waitForRootRegionLocation() {\n     synchronized (rootRegionLocation) {\n-      while (!master.isClosed() && rootRegionLocation.get() == null) {\n+      while (!master.getShutdownRequested().get() &&\n+          !master.isClosed() && rootRegionLocation.get() == null) {\n         // rootRegionLocation will be filled in when we get an 'open region'\n         // regionServerReport message from the HRegionServer that has been\n         // allocated the ROOT region below.",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "sha": "71597afcb476b335fe76ff0eee1da36dc6c23f2f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -1121,7 +1121,9 @@ public void stop() {\n   public void abort() {\n     this.abortRequested = true;\n     this.reservedSpace.clear();\n-    LOG.info(\"Dump of metrics: \" + this.metrics.toString());\n+    if (this.metrics != null) {\n+      LOG.info(\"Dump of metrics: \" + this.metrics.toString());\n+    }\n     stop();\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "9909f2c10a72c580b2974d37d5bb6c7c24345c0d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "patch": "@@ -55,7 +55,12 @@ public HRegionServer getRegionServer() {\n      * to be used.\n      */\n     public void waitForServerOnline() {\n-      while (!regionServer.isOnline()) {\n+      // The server is marked online after the init method completes inside of\n+      // the HRS#run method.  HRS#init can fail for whatever region.  In those\n+      // cases, we'll jump out of the run without setting online flag.  Check\n+      // stopRequested so we don't wait here a flag that will never be flipped.\n+      while (!this.regionServer.isOnline() &&\n+          !this.regionServer.isStopRequested()) {\n         try {\n           Thread.sleep(1000);\n         } catch (InterruptedException e) {",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "sha": "280b91da4a97bead9e81390d9bf0bf510f769c0c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "patch": "@@ -74,6 +74,10 @@\n    */\n   @BeforeClass public static void beforeAllTests() throws Exception {\n     TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n+    // Parcel out the regions, don't give them out in big lumps.  We've only\n+    // a few in this test.  Let a couple of cycles pass is more realistic and\n+    // gives stuff a chance to work.\n+    TEST_UTIL.getConfiguration().setInt(\"hbase.regions.percheckin\", 2);\n     // Start a cluster of two regionservers.\n     TEST_UTIL.startMiniCluster(2);\n     // Create a table of three families.  This will assign a region.",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "sha": "30333cde654dfb17f27b92db8d83f6df049f9a4c",
                "status": "modified"
            }
        ],
        "message": "HBASE-2614 killing server in TestMasterTransitions causes NPEs and test deadlock\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@951652 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/9cacbb074cb070d7565e8215df12771a56265d31",
        "patched_files": [
            "CHANGES.java",
            "JVMClusterUtil.java",
            "BaseScanner.java",
            "HRegionServer.java",
            "HMaster.java",
            "RegionManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMasterTransitions.java"
        ]
    },
    "hbase_65d961a": {
        "bug_id": "hbase_65d961a",
        "commit": "https://github.com/apache/hbase/commit/65d961ae783475b04e2b25b13d38871c4f4d13b7",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -103,6 +103,7 @@ Hbase Change Log\n    HBASE-515   At least double default timeouts between regionserver and master\r\n    HBASE-529   RegionServer needs to recover if datanode goes down\r\n    HBASE-456   Clearly state which ports need to be opened in order to run HBase\r\n+   HBASE-536   Remove MiniDFS startup from MiniHBaseCluster\r\n    \r\n Branch 0.1\r\n \r",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/CHANGES.txt",
                "sha": "cfd14ca6256c2d3d4e650c8d0da46b542579bc5b",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/bin/hbase",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/bin/hbase?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 8,
                "filename": "bin/hbase",
                "patch": "@@ -97,6 +97,18 @@ if [ \"$HBASE_HEAPSIZE\" != \"\" ]; then\n   #echo $JAVA_HEAP_MAX\n fi\n \n+# so that filenames w/ spaces are handled correctly in loops below\n+IFS=\n+\n+# Add libs to CLASSPATH\n+# Do this early so hadoop jar comes before hbase classes; otherwise\n+# complaint because way webapps are loaded, expectation is that\n+# loading class is from same jar as webapps themselves (only true\n+# if hadoop comes first).\n+for f in $HBASE_HOME/lib/*.jar; do\n+  CLASSPATH=${CLASSPATH}:$f;\n+done\n+\n # CLASSPATH initially contains $HBASE_CONF_DIR\n CLASSPATH=\"${CLASSPATH}:${HBASE_CONF_DIR}\"\n CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar\n@@ -112,9 +124,6 @@ if [ -d \"$HBASE_HOME/build/webapps\" ]; then\n   CLASSPATH=${CLASSPATH}:$HBASE_HOME/build\n fi\n \n-# so that filenames w/ spaces are handled correctly in loops below\n-IFS=\n-\n # for releases, add hbase, hadoop & webapps to CLASSPATH\n for f in $HBASE_HOME/hbase*.jar; do\n   if [ -f $f ]; then\n@@ -125,11 +134,6 @@ if [ -d \"$HBASE_HOME/webapps\" ]; then\n   CLASSPATH=${CLASSPATH}:$HBASE_HOME\n fi\n \n-# add libs to CLASSPATH\n-for f in $HBASE_HOME/lib/*.jar; do\n-  CLASSPATH=${CLASSPATH}:$f;\n-done\n-\n for f in $HBASE_HOME/lib/jetty-ext/*.jar; do\n   CLASSPATH=${CLASSPATH}:$f;\n done",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/bin/hbase",
                "sha": "044c07560490bb2c63948e0e70b6b5d90866c5dd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "patch": "@@ -308,7 +308,7 @@ private boolean hasReferences(final Text metaRegionName,\n   throws IOException {\n     boolean result = false;\n     HRegionInfo split =\n-      Writables.getHRegionInfoOrNull(rowContent.get(splitColumn).getValue());\n+      Writables.getHRegionInfo(rowContent.get(splitColumn));\n     if (split == null) {\n       return result;\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "sha": "dcdced9f2a3c475ac7bd4cf17236629a4f555afd",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 55,
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "patch": "@@ -31,18 +31,25 @@\n import org.apache.hadoop.hbase.regionserver.HRegion;\n \n /** Abstract base class for merge tests */\n-public abstract class AbstractMergeTestBase extends HBaseTestCase {\n+public abstract class AbstractMergeTestBase extends HBaseClusterTestCase {\n   static final Logger LOG =\n     Logger.getLogger(AbstractMergeTestBase.class.getName());\n   protected static final Text COLUMN_NAME = new Text(\"contents:\");\n   protected final Random rand = new Random();\n   protected HTableDescriptor desc;\n   protected ImmutableBytesWritable value;\n-\n-  /** constructor */\n+  protected boolean startMiniHBase;\n+  \n   public AbstractMergeTestBase() {\n+    this(true);\n+  }\n+  \n+  /** constructor */\n+  public AbstractMergeTestBase(boolean startMiniHBase) {\n     super();\n     \n+    this.startMiniHBase = startMiniHBase;\n+    \n     // We will use the same value for the rows as that is not really important here\n     \n     String partialValue = String.valueOf(System.currentTimeMillis());\n@@ -61,72 +68,53 @@ public AbstractMergeTestBase() {\n     desc.addFamily(new HColumnDescriptor(COLUMN_NAME.toString()));\n   }\n \n-  protected MiniDFSCluster dfsCluster = null;\n+  @Override\n+  protected void HBaseClusterSetup() throws Exception {\n+    if (startMiniHBase) {\n+      super.HBaseClusterSetup();\n+    }\n+  }\n \n   /**\n    * {@inheritDoc}\n    */\n   @Override\n-  public void setUp() throws Exception {\n+  public void preHBaseClusterSetup() throws Exception {\n     conf.setLong(\"hbase.hregion.max.filesize\", 64L * 1024L * 1024L);\n-    dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-    \n-    // Note: we must call super.setUp after starting the mini cluster or\n-    // we will end up with a local file system\n-    \n-    super.setUp();\n-      \n+\n     // We create three data regions: The first is too large to merge since it \n     // will be > 64 MB in size. The second two will be smaller and will be \n     // selected for merging.\n     \n     // To ensure that the first region is larger than 64MB we need to write at\n     // least 65536 rows. We will make certain by writing 70000\n \n-    try {\n-      Text row_70001 = new Text(\"row_70001\");\n-      Text row_80001 = new Text(\"row_80001\");\n-      \n-      HRegion[] regions = {\n-        createAregion(null, row_70001, 1, 70000),\n-        createAregion(row_70001, row_80001, 70001, 10000),\n-        createAregion(row_80001, null, 80001, 10000)\n-      };\n-      \n-      // Now create the root and meta regions and insert the data regions\n-      // created above into the meta\n-      \n-      HRegion root = HRegion.createHRegion(HRegionInfo.rootRegionInfo,\n-          testDir, this.conf);\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.firstMetaRegionInfo,\n-        testDir, this.conf);\n-      HRegion.addRegionToMETA(root, meta);\n-      \n-      for(int i = 0; i < regions.length; i++) {\n-        HRegion.addRegionToMETA(meta, regions[i]);\n-      }\n-      \n-      root.close();\n-      root.getLog().closeAndDelete();\n-      meta.close();\n-      meta.getLog().closeAndDelete();\n-      \n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n+    Text row_70001 = new Text(\"row_70001\");\n+    Text row_80001 = new Text(\"row_80001\");\n+    \n+    HRegion[] regions = {\n+      createAregion(null, row_70001, 1, 70000),\n+      createAregion(row_70001, row_80001, 70001, 10000),\n+      createAregion(row_80001, null, 80001, 10000)\n+    };\n+    \n+    // Now create the root and meta regions and insert the data regions\n+    // created above into the meta\n+    \n+    HRegion root = HRegion.createHRegion(HRegionInfo.rootRegionInfo,\n+      testDir, this.conf);\n+    HRegion meta = HRegion.createHRegion(HRegionInfo.firstMetaRegionInfo,\n+      testDir, this.conf);\n+    HRegion.addRegionToMETA(root, meta);\n+    \n+    for(int i = 0; i < regions.length; i++) {\n+      HRegion.addRegionToMETA(meta, regions[i]);\n     }\n-  }\n-\n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n+    \n+    root.close();\n+    root.getLog().closeAndDelete();\n+    meta.close();\n+    meta.getLog().closeAndDelete();\n   }\n \n   private HRegion createAregion(Text startKey, Text endKey, int firstRow,",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "sha": "51564c78b5ab0865a307de3893921ddb3b879df1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/DFSAbort.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "patch": "@@ -57,7 +57,7 @@ public void testDFSAbort() throws Exception {\n     try {\n       // By now the Mini DFS is running, Mini HBase is running and we have\n       // created a table. Now let's yank the rug out from HBase\n-      cluster.getDFSCluster().shutdown();\n+      dfsCluster.shutdown();\n       threadDumpingJoin();\n     } catch (Exception e) {\n       e.printStackTrace();",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "sha": "cba88debb3cd72b6f2309266b8ccb83a7101e63c",
                "status": "modified"
            },
            {
                "additions": 90,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 35,
                "filename": "src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "patch": "@@ -21,10 +21,16 @@\n \n import java.io.PrintWriter;\n \n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.util.ReflectionUtils;\n import org.apache.hadoop.hbase.client.HConnectionManager;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * Abstract base class for HBase cluster junit tests.  Spins up an hbase\n@@ -35,67 +41,116 @@\n     LogFactory.getLog(HBaseClusterTestCase.class.getName());\n   \n   protected MiniHBaseCluster cluster;\n-  final boolean miniHdfs;\n-  int regionServers;\n+  protected MiniDFSCluster dfsCluster;\n+  protected int regionServers;\n+  protected boolean startDfs;\n   \n-  /**\n-   * constructor\n-   */\n   public HBaseClusterTestCase() {\n-    this(true);\n+    this(1);\n   }\n   \n   /**\n-   * @param regionServers\n-   */\n+   * Start a MiniHBaseCluster with regionServers region servers in-process to\n+   * start with. Also, start a MiniDfsCluster before starting the hbase cluster.\n+   * The configuration used will be edited so that this works correctly.\n+   */  \n   public HBaseClusterTestCase(int regionServers) {\n-    this(true);\n-    this.regionServers = regionServers;\n+    this(regionServers, true);\n   }\n-\n+  \n   /**\n-   * @param name\n+   * Start a MiniHBaseCluster with regionServers region servers in-process to\n+   * start with. Optionally, startDfs indicates if a MiniDFSCluster should be\n+   * started. If startDfs is false, the assumption is that an external DFS is\n+   * configured in hbase-site.xml and is already started, or you have started a\n+   * MiniDFSCluster on your own and edited the configuration in memory. (You \n+   * can modify the config used by overriding the preHBaseClusterSetup method.)\n    */\n-  public HBaseClusterTestCase(String name) {\n-    this(name, true);\n+  public HBaseClusterTestCase(int regionServers, boolean startDfs) {\n+    super();\n+    this.startDfs = startDfs;\n+    this.regionServers = regionServers;\n   }\n   \n   /**\n-   * @param miniHdfs\n+   * Run after dfs is ready but before hbase cluster is started up.\n    */\n-  public HBaseClusterTestCase(final boolean miniHdfs) {\n-    super();\n-    this.miniHdfs = miniHdfs;\n-    this.regionServers = 1;\n-  }\n+  protected void preHBaseClusterSetup() throws Exception {\n+  } \n \n   /**\n-   * @param name\n-   * @param miniHdfs\n+   * Actually start the MiniHBase instance.\n    */\n-  public HBaseClusterTestCase(String name, final boolean miniHdfs) {\n-    super(name);\n-    this.miniHdfs = miniHdfs;\n-    this.regionServers = 1;\n+  protected void HBaseClusterSetup() throws Exception {\n+    // start the mini cluster\n+    this.cluster = new MiniHBaseCluster(conf, regionServers);\n+    HTable meta = new HTable(conf, new Text(\".META.\"));\n   }\n+  \n+  /**\n+   * Run after hbase cluster is started up.\n+   */\n+  protected void postHBaseClusterSetup() throws Exception {\n+  } \n \n   @Override\n   protected void setUp() throws Exception {\n-    this.cluster =\n-      new MiniHBaseCluster(this.conf, this.regionServers, this.miniHdfs);\n-    super.setUp();\n+    try {\n+      if (startDfs) {\n+        // start up the dfs\n+        dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n+\n+        // mangle the conf so that the fs parameter points to the minidfs we\n+        // just started up\n+        FileSystem fs = dfsCluster.getFileSystem();\n+        conf.set(\"fs.default.name\", fs.getName());      \n+        Path parentdir = fs.getHomeDirectory();\n+        conf.set(HConstants.HBASE_DIR, parentdir.toString());\n+        fs.mkdirs(parentdir);\n+        FSUtils.setVersion(fs, parentdir);\n+      }\n+\n+      // do the super setup now. if we had done it first, then we would have\n+      // gotten our conf all mangled and a local fs started up.\n+      super.setUp();\n+    \n+      // run the pre-cluster setup\n+      preHBaseClusterSetup();    \n+    \n+      // start the instance\n+      HBaseClusterSetup();\n+      \n+      // run post-cluster setup\n+      postHBaseClusterSetup();\n+    } catch (Exception e) {\n+      LOG.error(\"Exception in setup!\", e);\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+      if (dfsCluster != null) {\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n+      }\n+      throw e;\n+    }\n   }\n \n   @Override\n   protected void tearDown() throws Exception {\n     super.tearDown();\n-    HConnectionManager.deleteConnection(conf);\n-    if (this.cluster != null) {\n-      try {\n-        this.cluster.shutdown();\n-      } catch (Exception e) {\n-        LOG.warn(\"Closing mini dfs\", e);\n+    try {\n+      HConnectionManager.deleteConnection(conf);\n+      if (this.cluster != null) {\n+        try {\n+          this.cluster.shutdown();\n+        } catch (Exception e) {\n+          LOG.warn(\"Closing mini dfs\", e);\n+        }\n+      }\n+      if (startDfs) {\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n       }\n+    } catch (Exception e) {\n+      LOG.error(e);\n     }\n     // ReflectionUtils.printThreadInfo(new PrintWriter(System.out),\n     //  \"Temporary end-of-test thread dump debugging HADOOP-2040: \" + getName());",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "sha": "bb8cf975447bcae722d3130a4c1205db6c216b84",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 114,
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "patch": "@@ -1,5 +1,5 @@\n /**\n- * Copyright 2007 The Apache Software Foundation\n+ * Copyright 2008 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -23,7 +23,6 @@\n import java.io.IOException;\n import java.util.List;\n \n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.log4j.Logger;\n@@ -42,111 +41,23 @@\n     Logger.getLogger(MiniHBaseCluster.class.getName());\n   \n   private HBaseConfiguration conf;\n-  private MiniDFSCluster cluster;\n-  private FileSystem fs;\n-  private boolean shutdownDFS;\n-  private Path parentdir;\n   private LocalHBaseCluster hbaseCluster;\n-  private boolean deleteOnExit = true;\n \n   /**\n-   * Starts a MiniHBaseCluster on top of a new MiniDFSCluster\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @throws IOException\n+   * Start a MiniHBaseCluster. conf is assumed to contain a valid fs name to \n+   * hook up to.\n    */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes)\n+  public MiniHBaseCluster(HBaseConfiguration conf, int numRegionServers) \n   throws IOException {\n-    this(conf, nRegionNodes, true, true, true);\n-  }\n-\n-  /**\n-   * Start a MiniHBaseCluster. Use the native file system unless\n-   * miniHdfsFilesystem is set to true.\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param miniHdfsFilesystem\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      final boolean miniHdfsFilesystem) throws IOException {\n-    this(conf, nRegionNodes, miniHdfsFilesystem, true, true);\n-  }\n-\n-  /**\n-   * Starts a MiniHBaseCluster on top of an existing HDFSCluster\n-   *<pre>\n-   ****************************************************************************\n-   *            *  *  *  *  *  N O T E  *  *  *  *  *\n-   *\n-   * If you use this constructor, you should shut down the mini dfs cluster\n-   * in your test case.\n-   *\n-   *            *  *  *  *  *  N O T E  *  *  *  *  *\n-   ****************************************************************************\n-   *</pre>\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param dfsCluster\n-   * @param deleteOnExit\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      MiniDFSCluster dfsCluster, boolean deleteOnExit) throws IOException {\n-\n     this.conf = conf;\n-    this.fs = dfsCluster.getFileSystem();\n-    this.cluster = dfsCluster;\n-    this.shutdownDFS = false;\n-    this.deleteOnExit = deleteOnExit;\n-    init(nRegionNodes);\n-  }\n-\n-  /**\n-   * Constructor.\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param miniHdfsFilesystem If true, set the hbase mini\n-   * cluster atop a mini hdfs cluster.  Otherwise, use the\n-   * filesystem configured in <code>conf</code>.\n-   * @param format the mini hdfs cluster\n-   * @param deleteOnExit clean up mini hdfs files\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      final boolean miniHdfsFilesystem, boolean format, boolean deleteOnExit)\n-    throws IOException {\n-\n-    this.conf = conf;\n-    this.deleteOnExit = deleteOnExit;\n-    this.shutdownDFS = false;\n-    if (miniHdfsFilesystem) {\n-      try {\n-        this.cluster = new MiniDFSCluster(this.conf, 2, format, (String[])null);\n-        this.fs = cluster.getFileSystem();\n-        this.shutdownDFS = true;\n-      } catch (IOException e) {\n-        StaticTestEnvironment.shutdownDfs(cluster);\n-        throw e;\n-      }\n-    } else {\n-      this.cluster = null;\n-      this.fs = FileSystem.get(conf);\n-    }\n-    init(nRegionNodes);\n+    init(numRegionServers);\n   }\n \n   private void init(final int nRegionNodes) throws IOException {\n     try {\n-      this.parentdir = this.fs.getHomeDirectory();\n-      this.conf.set(HConstants.HBASE_DIR, this.parentdir.toString());\n-      this.fs.mkdirs(parentdir);\n-      FSUtils.setVersion(fs, parentdir);\n-      this.hbaseCluster = new LocalHBaseCluster(this.conf, nRegionNodes);\n-      this.hbaseCluster.startup();\n+      // start up a LocalHBaseCluster\n+      hbaseCluster = new LocalHBaseCluster(conf, nRegionNodes);\n+      hbaseCluster.startup();\n     } catch(IOException e) {\n       shutdown();\n       throw e;\n@@ -166,15 +77,6 @@ public String startRegionServer() throws IOException {\n     return t.getName();\n   }\n \n-  /**\n-   * Get the cluster on which this HBase cluster is running\n-   *\n-   * @return MiniDFSCluster\n-   */\n-  public MiniDFSCluster getDFSCluster() {\n-    return cluster;\n-  }\n-\n   /**\n    * @return Returns the rpc address actually used by the master server, because\n    * the supplied port is not necessarily the actual port used.\n@@ -240,14 +142,6 @@ public void shutdown() {\n     if (this.hbaseCluster != null) {\n       this.hbaseCluster.shutdown();\n     }\n-    if (shutdownDFS) {\n-      StaticTestEnvironment.shutdownDfs(cluster);\n-    }\n-    // Delete all DFS files\n-    if(deleteOnExit) {\n-      deleteFile(new File(System.getProperty(\n-          StaticTestEnvironment.TEST_DIRECTORY_KEY), \"dfs\"));\n-    }\n   }\n \n   private void deleteFile(File f) {",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "sha": "4c169387fec623ebf48cf5df728e64cd3f274bf0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 5,
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "patch": "@@ -39,16 +39,14 @@\n /**\n  * Utility class to build a table of multiple regions.\n  */\n-public class MultiRegionTable extends HBaseTestCase {\n+public class MultiRegionTable extends HBaseClusterTestCase {\n   static final Log LOG = LogFactory.getLog(MultiRegionTable.class.getName());\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n+  public MultiRegionTable() {\n+    super();\n     // These are needed for the new and improved Map/Reduce framework\n     System.setProperty(\"hadoop.log.dir\", conf.get(\"hadoop.log.dir\"));\n     conf.set(\"mapred.output.dir\", conf.get(\"hadoop.tmp.dir\"));\n-    super.setUp();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "sha": "716e5c8dbb2d7fc8cd72f8743d90b2c808a2e85f",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -31,6 +31,8 @@\n import java.util.regex.Pattern;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.LongWritable;\n@@ -561,7 +563,18 @@ private void runTest(final String cmd) throws IOException {\n     }\n     \n     MiniHBaseCluster hbaseMiniCluster = null;\n+    MiniDFSCluster dfsCluster = null;\n     if (this.miniCluster) {\n+      dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n+      // mangle the conf so that the fs parameter points to the minidfs we\n+      // just started up\n+      FileSystem fs = dfsCluster.getFileSystem();\n+      conf.set(\"fs.default.name\", fs.getName());      \n+      Path parentdir = fs.getHomeDirectory();\n+      conf.set(HConstants.HBASE_DIR, parentdir.toString());\n+      fs.mkdirs(parentdir);\n+      FSUtils.setVersion(fs, parentdir);\n+      \n       hbaseMiniCluster = new MiniHBaseCluster(this.conf, N);\n     }\n     \n@@ -577,6 +590,7 @@ private void runTest(final String cmd) throws IOException {\n     } finally {\n       if(this.miniCluster && hbaseMiniCluster != null) {\n         hbaseMiniCluster.shutdown();\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "bc321395a5c76200406cae3cd171a30c9d11f6ce",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "patch": "@@ -40,7 +40,7 @@\n \n   /** constructor */\n   public TestHBaseCluster() {\n-    super(true);\n+    super();\n     this.desc = null;\n     this.admin = null;\n     this.table = null;",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "sha": "f057aa4a78a199ea39927ff2b386f86b4b9b7e48",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestInfoServers.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 30,
                "filename": "src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "patch": "@@ -26,49 +26,37 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.client.HTable;\n \n /**\n  * Testing, info servers are disabled.  This test enables then and checks that\n  * they serve pages.\n  */\n-public class TestInfoServers extends HBaseTestCase {\n+public class TestInfoServers extends HBaseClusterTestCase {\n   static final Log LOG = LogFactory.getLog(TestInfoServers.class);\n-\n-  @Override  \n-  protected void setUp() throws Exception {\n-    super.setUp();\n-  }\n-\n+  \n   @Override\n-  protected void tearDown() throws Exception {\n-    super.tearDown();\n+  protected void preHBaseClusterSetup() {\n+    // Bring up info servers on 'odd' port numbers in case the test is not\n+    // sourcing the src/test/hbase-default.xml.\n+    conf.setInt(\"hbase.master.info.port\", 60011);\n+    conf.setInt(\"hbase.regionserver.info.port\", 60031);\n   }\n   \n   /**\n    * @throws Exception\n    */\n   public void testInfoServersAreUp() throws Exception {\n-    // Bring up info servers on 'odd' port numbers in case the test is not\n-    // sourcing the src/test/hbase-default.xml.\n-    this.conf.setInt(\"hbase.master.info.port\", 60011);\n-    this.conf.setInt(\"hbase.regionserver.info.port\", 60031);\n-    MiniHBaseCluster miniHbase = new MiniHBaseCluster(this.conf, 1);\n-    // Create table so info servers are given time to spin up.\n-    HBaseAdmin a = new HBaseAdmin(conf);\n-    a.createTable(new HTableDescriptor(getName()));\n-    assertTrue(a.tableExists(new Text(getName())));\n-    try {\n-      int port = miniHbase.getMaster().getInfoServer().getPort();\n-      assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n-        \"/index.html\"), \"Master\");\n-      port = miniHbase.getRegionThreads().get(0).getRegionServer().\n-        getInfoServer().getPort();\n-      assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n-        \"/index.html\"), \"Region Server\");\n-    } finally {\n-      miniHbase.shutdown();\n-    }\n+    // give the cluster time to start up\n+    HTable table = new HTable(conf, new Text(\".META.\"));\n+    \n+    int port = cluster.getMaster().getInfoServer().getPort();\n+    assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n+      \"/index.html\"), \"Master\");\n+    port = cluster.getRegionThreads().get(0).getRegionServer().\n+      getInfoServer().getPort();\n+    assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n+      \"/index.html\"), \"Region Server\");\n   }\n   \n   private void assertHasExpectedContent(final URL u, final String expected)",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "sha": "97c3e327a9cddea0b1994d479cb61716dde2ecd5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "patch": "@@ -40,7 +40,7 @@\n \n   /** constructor */\n   public TestMasterAdmin() {\n-    super(true);\n+    super();\n     admin = null;\n \n     // Make the thread wake frequency a little slower so other threads",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "sha": "57af6176243b4babc1f9ea1fa2c02351eab78eab",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMergeMeta.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 2,
                "filename": "src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "patch": "@@ -25,8 +25,8 @@\n public class TestMergeMeta extends AbstractMergeTestBase {\n \n   /** constructor */\n-  public TestMergeMeta() {\n-    super();\n+  public TestMergeMeta() throws Exception {\n+    super(false);\n     conf.setLong(\"hbase.client.pause\", 1 * 1000);\n     conf.setInt(\"hbase.client.retries.number\", 2);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "sha": "69418abfcfce61dcc72ccb3d68c6911195819847",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMergeTable.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 6,
                "filename": "src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "patch": "@@ -32,11 +32,6 @@\n    */\n   public void testMergeTable() throws IOException {\n     assertNotNull(dfsCluster);\n-    MiniHBaseCluster hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-    try {\n-      HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n-    } finally {\n-      hCluster.shutdown();\n-    }\n+    HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "sha": "ec37f9bd74eaf440ecb5d24ce2e6fbeb12e284e2",
                "status": "modified"
            },
            {
                "additions": 253,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "changes": 253,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TimestampTestBase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "patch": "@@ -0,0 +1,253 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.TreeMap;\n+\n+import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HScannerInterface;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseTestCase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Tests user specifiable time stamps putting, getting and scanning.  Also\n+ * tests same in presence of deletes.  Test cores are written so can be\n+ * run against an HRegion and against an HTable: i.e. both local and remote.\n+ */\n+public class TimestampTestBase extends HBaseTestCase {\n+  private static final Log LOG =\n+    LogFactory.getLog(TimestampTestBase.class.getName());\n+\n+  private static final long T0 = 10L;\n+  private static final long T1 = 100L;\n+  private static final long T2 = 200L;\n+  \n+  private static final String COLUMN_NAME = \"contents:\";\n+  \n+  private static final Text COLUMN = new Text(COLUMN_NAME);\n+  private static final Text ROW = new Text(\"row\");\n+  \n+  // When creating column descriptor, how many versions of a cell to allow.\n+  private static final int VERSIONS = 3;\n+  \n+    /*\n+   * Run test that delete works according to description in <a\n+   * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n+   * @param incommon\n+   * @param flusher\n+   * @throws IOException\n+   */\n+  public static void doTestDelete(final Incommon incommon, FlushCache flusher)\n+  throws IOException {\n+    // Add values at various timestamps (Values are timestampes as bytes).\n+    put(incommon, T0);\n+    put(incommon, T1);\n+    put(incommon, T2);\n+    put(incommon);\n+    // Verify that returned versions match passed timestamps.\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n+    // If I delete w/o specifying a timestamp, this means I'm deleting the\n+    // latest.\n+    delete(incommon);\n+    // Verify that I get back T2 through T1 -- that the latest version has\n+    // been deleted.\n+    assertVersions(incommon, new long [] {T2, T1, T0});\n+    \n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertVersions(incommon, new long [] {T2, T1, T0});\n+    \n+    // Now add, back a latest so I can test remove other than the latest.\n+    put(incommon);\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n+    delete(incommon, T2);\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n+    \n+    // Now try deleting all from T2 back inclusive (We first need to add T2\n+    // back into the mix and to make things a little interesting, delete and\n+    // then readd T1.\n+    put(incommon, T2);\n+    delete(incommon, T1);\n+    put(incommon, T1);\n+    incommon.deleteAll(ROW, COLUMN, T2);\n+    // Should only be current value in set.  Assert this is so\n+    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n+    \n+    // Flush everything out to disk and then redo above tests\n+    flusher.flushcache();\n+    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n+  }\n+  \n+  private static void assertOnlyLatest(final Incommon incommon,\n+    final long currentTime)\n+  throws IOException {\n+    Cell[] cellValues = incommon.get(ROW, COLUMN, 3/*Ask for too much*/);\n+    assertEquals(1, cellValues.length);\n+    long time = Writables.bytesToLong(cellValues[0].getValue());\n+    assertEquals(time, currentTime);\n+    assertNull(incommon.get(ROW, COLUMN, T1, 3 /*Too many*/));\n+    assertTrue(assertScanContentTimestamp(incommon, T1) == 0);\n+  }\n+  \n+  /*\n+   * Assert that returned versions match passed in timestamps and that results\n+   * are returned in the right order.  Assert that values when converted to\n+   * longs match the corresponding passed timestamp.\n+   * @param r\n+   * @param tss\n+   * @throws IOException\n+   */\n+  public static void assertVersions(final Incommon incommon, final long [] tss)\n+  throws IOException {\n+    // Assert that 'latest' is what we expect.\n+    byte [] bytes = incommon.get(ROW, COLUMN).getValue();\n+    assertEquals(Writables.bytesToLong(bytes), tss[0]);\n+    // Now assert that if we ask for multiple versions, that they come out in\n+    // order.\n+    Cell[] cellValues = incommon.get(ROW, COLUMN, tss.length);\n+    assertEquals(tss.length, cellValues.length);\n+    for (int i = 0; i < cellValues.length; i++) {\n+      long ts = Writables.bytesToLong(cellValues[i].getValue());\n+      assertEquals(ts, tss[i]);\n+    }\n+    // Specify a timestamp get multiple versions.\n+    cellValues = incommon.get(ROW, COLUMN, tss[0], cellValues.length - 1);\n+    for (int i = 1; i < cellValues.length; i++) {\n+      long ts = Writables.bytesToLong(cellValues[i].getValue());\n+      assertEquals(ts, tss[i]);\n+    }\n+    // Test scanner returns expected version\n+    assertScanContentTimestamp(incommon, tss[0]);\n+  }\n+  \n+  /*\n+   * Run test scanning different timestamps.\n+   * @param incommon\n+   * @param flusher\n+   * @throws IOException\n+   */\n+  public static void doTestTimestampScanning(final Incommon incommon,\n+    final FlushCache flusher)\n+  throws IOException {\n+    // Add a couple of values for three different timestamps.\n+    put(incommon, T0);\n+    put(incommon, T1);\n+    put(incommon, HConstants.LATEST_TIMESTAMP);\n+    // Get count of latest items.\n+    int count = assertScanContentTimestamp(incommon,\n+      HConstants.LATEST_TIMESTAMP);\n+    // Assert I get same count when I scan at each timestamp.\n+    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n+    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n+    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n+  }\n+  \n+  /*\n+   * Assert that the scan returns only values < timestamp. \n+   * @param r\n+   * @param ts\n+   * @return Count of items scanned.\n+   * @throws IOException\n+   */\n+  public static int assertScanContentTimestamp(final Incommon in, final long ts)\n+  throws IOException {\n+    HScannerInterface scanner =\n+      in.getScanner(COLUMNS, HConstants.EMPTY_START_ROW, ts);\n+    int count = 0;\n+    try {\n+      HStoreKey key = new HStoreKey();\n+      TreeMap<Text, byte []>value = new TreeMap<Text, byte[]>();\n+      while (scanner.next(key, value)) {\n+        assertTrue(key.getTimestamp() <= ts);\n+        // Content matches the key or HConstants.LATEST_TIMESTAMP.\n+        // (Key does not match content if we 'put' with LATEST_TIMESTAMP).\n+        long l = Writables.bytesToLong(value.get(COLUMN));\n+        assertTrue(key.getTimestamp() == l ||\n+          HConstants.LATEST_TIMESTAMP == l);\n+        count++;\n+        value.clear();\n+      }\n+    } finally {\n+      scanner.close(); \n+    }\n+    return count;\n+  }\n+  \n+  public static void put(final Incommon loader, final long ts)\n+  throws IOException {\n+    put(loader, Writables.longToBytes(ts), ts);\n+  }\n+  \n+  public static void put(final Incommon loader)\n+  throws IOException {\n+    long ts = HConstants.LATEST_TIMESTAMP;\n+    put(loader, Writables.longToBytes(ts), ts);\n+  }\n+  \n+  /*\n+   * Put values.\n+   * @param loader\n+   * @param bytes\n+   * @param ts\n+   * @throws IOException\n+   */\n+  public static void put(final Incommon loader, final byte [] bytes,\n+    final long ts)\n+  throws IOException {\n+    long lockid = loader.startUpdate(ROW);\n+    loader.put(lockid, COLUMN, bytes);\n+    if (ts == HConstants.LATEST_TIMESTAMP) {\n+      loader.commit(lockid);\n+    } else {\n+      loader.commit(lockid, ts);\n+    }\n+  }\n+  \n+  public static void delete(final Incommon loader) throws IOException {\n+    delete(loader, HConstants.LATEST_TIMESTAMP);\n+  }\n+\n+  public static void delete(final Incommon loader, final long ts) throws IOException {\n+    long lockid = loader.startUpdate(ROW);\n+    loader.delete(lockid, COLUMN);\n+    if (ts == HConstants.LATEST_TIMESTAMP) {\n+      loader.commit(lockid);\n+    } else {\n+      loader.commit(lockid, ts);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "sha": "73ab1006bdd43040f370ce6a0d2a627a56d5608b",
                "status": "added"
            },
            {
                "additions": 95,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "changes": 95,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "patch": "@@ -0,0 +1,95 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.client;\n+\n+import java.io.IOException;\n+import java.util.TreeMap;\n+\n+import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HScannerInterface;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseClusterTestCase;\n+import org.apache.hadoop.hbase.TimestampTestBase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Tests user specifiable time stamps putting, getting and scanning.  Also\n+ * tests same in presence of deletes.  Test cores are written so can be\n+ * run against an HRegion and against an HTable: i.e. both local and remote.\n+ */\n+public class TestTimestamp extends HBaseClusterTestCase {\n+  private static final Log LOG =\n+    LogFactory.getLog(TestTimestamp.class.getName());\n+  \n+  private static final String COLUMN_NAME = \"contents:\";\n+  private static final Text COLUMN = new Text(COLUMN_NAME);\n+  // When creating column descriptor, how many versions of a cell to allow.\n+  private static final int VERSIONS = 3;\n+  \n+  /** constructor */\n+  public TestTimestamp() {\n+    super();\n+  }\n+\n+  /**\n+   * Basic test of timestamps.\n+   * Do the above tests from client side.\n+   * @throws IOException\n+   */\n+  public void testTimestamps() throws IOException {\n+    HTable t = createTable();\n+    Incommon incommon = new HTableIncommon(t);\n+    TimestampTestBase.doTestDelete(incommon, new FlushCache() {\n+      public void flushcache() throws IOException {\n+        cluster.flushcache();\n+      }\n+     });\n+    \n+    // Perhaps drop and readd the table between tests so the former does\n+    // not pollute this latter?  Or put into separate tests.\n+    TimestampTestBase.doTestTimestampScanning(incommon, new FlushCache() {\n+      public void flushcache() throws IOException {\n+        cluster.flushcache();\n+      }\n+    });\n+  }\n+  \n+  /* \n+   * Create a table named TABLE_NAME.\n+   * @return An instance of an HTable connected to the created table.\n+   * @throws IOException\n+   */\n+  private HTable createTable() throws IOException {\n+    HTableDescriptor desc = new HTableDescriptor(getName());\n+    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));\n+    HBaseAdmin admin = new HBaseAdmin(conf);\n+    admin.createTable(desc);\n+    return new HTable(conf, new Text(getName()));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "sha": "3e597c521f8b47785d993c84e44b1347932cc612",
                "status": "added"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 48,
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "patch": "@@ -30,7 +30,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -42,7 +41,6 @@\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.HTableDescriptor;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n import org.apache.hadoop.io.Text;\n@@ -79,13 +77,9 @@\n \n   private HTableDescriptor desc;\n \n-  private MiniDFSCluster dfsCluster = null;\n   private Path dir;\n-  private MiniHBaseCluster hCluster = null;\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n+  public TestTableIndex() {\n     // Enable DEBUG-level MR logging.\n     Logger.getLogger(\"org.apache.hadoop.mapred\").setLevel(Level.DEBUG);\n     \n@@ -103,52 +97,23 @@ public void setUp() throws Exception {\n     desc = new HTableDescriptor(TABLE_NAME);\n     desc.addFamily(new HColumnDescriptor(INPUT_COLUMN));\n     desc.addFamily(new HColumnDescriptor(OUTPUT_COLUMN));\n-\n-    dfsCluster = new MiniDFSCluster(conf, 1, true, (String[]) null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-\n-    // Must call super.setUp after mini dfs cluster is started or else\n-    // filesystem ends up being local\n-    \n-    super.setUp();\n-\n-    try {\n-      dir = new Path(\"/hbase\");\n-      fs.mkdirs(dir);\n-\n-      // Start up HBase cluster\n-      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-\n-      // Create a table.\n-      HBaseAdmin admin = new HBaseAdmin(conf);\n-      admin.createTable(desc);\n-\n-      // Populate a table into multiple regions\n-      makeMultiRegionTable(conf, hCluster, this.fs, TABLE_NAME, INPUT_COLUMN);\n-\n-      // Verify table indeed has multiple regions\n-      HTable table = new HTable(conf, new Text(TABLE_NAME));\n-      Text[] startKeys = table.getStartKeys();\n-      assertTrue(startKeys.length > 1);\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n-    }\n-    LOG.debug(\"\\n\\n\\n\\n\\t\\t\\tSetup Complete\\n\\n\\n\\n\");\n   }\n \n   /** {@inheritDoc} */\n   @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n+  protected void postHBaseClusterSetup() throws Exception {\n+    // Create a table.\n+    HBaseAdmin admin = new HBaseAdmin(conf);\n+    admin.createTable(desc);\n \n-    if (hCluster != null) {\n-      hCluster.shutdown();\n-    }\n+    // Populate a table into multiple regions\n+    makeMultiRegionTable(conf, cluster, dfsCluster.getFileSystem(), TABLE_NAME,\n+      INPUT_COLUMN);\n \n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n+    // Verify table indeed has multiple regions\n+    HTable table = new HTable(conf, new Text(TABLE_NAME));\n+    Text[] startKeys = table.getStartKeys();\n+    assertTrue(startKeys.length > 1);\n   }\n \n   /**\n@@ -260,7 +225,7 @@ private void scanTable(boolean printResults)\n   private void verify() throws IOException {\n     // Force a cache flush for every online region to ensure that when the\n     // scanner takes its snapshot, all the updates have made it into the cache.\n-    for (HRegion r : hCluster.getRegionThreads().get(0).getRegionServer().\n+    for (HRegion r : cluster.getRegionThreads().get(0).getRegionServer().\n         getOnlineRegions().values()) {\n       HRegionIncommon region = new HRegionIncommon(r);\n       region.flushcache();",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "sha": "c2559f9932e54c9fdceb92fb900023c992665a70",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 46,
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "patch": "@@ -26,7 +26,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n@@ -35,7 +34,6 @@\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.HTableDescriptor;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n@@ -68,9 +66,7 @@\n     TEXT_OUTPUT_COLUMN\n   };\n \n-  private MiniDFSCluster dfsCluster = null;\n   private Path dir;\n-  private MiniHBaseCluster hCluster = null;\n   \n   private static byte[][] values = null;\n   \n@@ -110,46 +106,6 @@ public TestTableMapReduce() {\n     conf.setInt(\"hbase.client.pause\", 10 * 1000);\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void setUp() throws Exception {\n-    dfsCluster = new MiniDFSCluster(conf, 1, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-\n-    // Must call super.setup() after starting mini dfs cluster. Otherwise\n-    // we get a local file system instead of hdfs\n-    \n-    super.setUp();\n-    try {\n-      dir = new Path(\"/hbase\");\n-      fs.mkdirs(dir);\n-      // Start up HBase cluster\n-      // Only one region server.  MultiRegionServer manufacturing code below\n-      // depends on there being one region server only.\n-      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-      LOG.info(\"Master is at \" + this.conf.get(HConstants.MASTER_ADDRESS));\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n-    }\n-  }\n-\n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n-    if(hCluster != null) {\n-      hCluster.shutdown();\n-    }\n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n-  }\n-\n   /**\n    * Pass the given key and processed record reduce\n    */\n@@ -276,8 +232,8 @@ private void localTestMultiRegionTable() throws IOException {\n     admin.createTable(desc);\n \n     // Populate a table into multiple regions\n-    makeMultiRegionTable(conf, hCluster, fs, MULTI_REGION_TABLE_NAME,\n-        INPUT_COLUMN);\n+    makeMultiRegionTable(conf, cluster, dfsCluster.getFileSystem(), \n+      MULTI_REGION_TABLE_NAME, INPUT_COLUMN);\n     \n     // Verify table indeed has multiple regions\n     HTable table = new HTable(conf, new Text(MULTI_REGION_TABLE_NAME));",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "sha": "cbee64752eeee9b8878c8b5ccf84dcc838ffc58b",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "changes": 95,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 66,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "patch": "@@ -28,8 +28,7 @@\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.HBaseTestCase;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n+import org.apache.hadoop.hbase.HBaseClusterTestCase;\n \n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n@@ -40,10 +39,8 @@\n /**\n  * Test log deletion as logs are rolled.\n  */\n-public class TestLogRolling extends HBaseTestCase {\n+public class TestLogRolling extends HBaseClusterTestCase {\n   private static final Log LOG = LogFactory.getLog(TestLogRolling.class);\n-  private MiniDFSCluster dfs;\n-  private MiniHBaseCluster cluster;\n   private HRegionServer server;\n   private HLog log;\n   private String tableName;\n@@ -54,37 +51,13 @@\n    * @throws Exception\n    */\n   public TestLogRolling() throws Exception {\n+    // start one regionserver and a minidfs.\n     super();\n     try {\n-      this.dfs = null;\n-      this.cluster = null;\n       this.server = null;\n       this.log = null;\n       this.tableName = null;\n       this.value = null;\n-\n-      // Force a region split after every 768KB\n-      conf.setLong(\"hbase.hregion.max.filesize\", 768L * 1024L);\n-\n-      // We roll the log after every 32 writes\n-      conf.setInt(\"hbase.regionserver.maxlogentries\", 32);\n-\n-      // For less frequently updated regions flush after every 2 flushes\n-      conf.setInt(\"hbase.hregion.memcache.optionalflushcount\", 2);\n-\n-      // We flush the cache after every 8192 bytes\n-      conf.setInt(\"hbase.hregion.memcache.flush.size\", 8192);\n-\n-      // Make lease timeout longer, lease checks less frequent\n-      conf.setInt(\"hbase.master.lease.period\", 10 * 1000);\n-      conf.setInt(\"hbase.master.lease.thread.wakefrequency\", 5 * 1000);\n-\n-      // Increase the amount of time between client retries\n-      conf.setLong(\"hbase.client.pause\", 15 * 1000);\n-\n-      // Reduce thread wake frequency so that other threads can get\n-      // a chance to run.\n-      conf.setInt(HConstants.THREAD_WAKE_FREQUENCY, 2 * 1000);\n       \n       String className = this.getClass().getName();\n       StringBuilder v = new StringBuilder(className);\n@@ -99,50 +72,40 @@ public TestLogRolling() throws Exception {\n     }\n   }\n \n-  /** {@inheritDoc} */\n+  // Need to override this setup so we can edit the config before it gets sent\n+  // to the cluster startup.\n   @Override\n-  public void setUp() throws Exception {\n-    try {\n-      dfs = new MiniDFSCluster(conf, 2, true, (String[]) null);\n-      // Set the hbase.rootdir to be the home directory in mini dfs.\n-      this.conf.set(HConstants.HBASE_DIR,\n-        this.dfs.getFileSystem().getHomeDirectory().toString());\n-      super.setUp();\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfs);\n-      LOG.fatal(\"error during setUp: \", e);\n-      throw e;\n-    }\n-  }\n+  protected void preHBaseClusterSetup() {\n+    // Force a region split after every 768KB\n+    conf.setLong(\"hbase.hregion.max.filesize\", 768L * 1024L);\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    try {\n-      super.tearDown();\n-      if (cluster != null) {                      // shutdown mini HBase cluster\n-        cluster.shutdown();\n-      }\n-      StaticTestEnvironment.shutdownDfs(dfs);\n-    } catch (Exception e) {\n-      LOG.fatal(\"error in tearDown\", e);\n-      throw e;\n-    }\n+    // We roll the log after every 32 writes\n+    conf.setInt(\"hbase.regionserver.maxlogentries\", 32);\n+\n+    // For less frequently updated regions flush after every 2 flushes\n+    conf.setInt(\"hbase.hregion.memcache.optionalflushcount\", 2);\n+\n+    // We flush the cache after every 8192 bytes\n+    conf.setInt(\"hbase.hregion.memcache.flush.size\", 8192);\n+\n+    // Make lease timeout longer, lease checks less frequent\n+    conf.setInt(\"hbase.master.lease.period\", 10 * 1000);\n+    conf.setInt(\"hbase.master.lease.thread.wakefrequency\", 5 * 1000);\n+\n+    // Increase the amount of time between client retries\n+    conf.setLong(\"hbase.client.pause\", 15 * 1000);\n+\n+    // Reduce thread wake frequency so that other threads can get\n+    // a chance to run.\n+    conf.setInt(HConstants.THREAD_WAKE_FREQUENCY, 2 * 1000);\n   }\n   \n   private void startAndWriteData() throws Exception {\n-    cluster = new MiniHBaseCluster(conf, 1, dfs, true);\n-    try {\n-      Thread.sleep(10 * 1000);                  // Wait for region server to start\n-    } catch (InterruptedException e) {\n-      // continue\n-    }\n+    // When the META table can be opened, the region servers are running\n+    new HTable(conf, HConstants.META_TABLE_NAME);\n \n     this.server = cluster.getRegionThreads().get(0).getRegionServer();\n     this.log = server.getLog();\n-\n-    // When the META table can be opened, the region servers are running\n-    new HTable(conf, HConstants.META_TABLE_NAME);\n     \n     // Create the test table and open it\n     HTableDescriptor desc = new HTableDescriptor(tableName);",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "sha": "984a1983c86347da0220dcb7451a1749a19446b8",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 8,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "patch": "@@ -72,13 +72,8 @@ public TestSplit() {\n    * @throws Exception\n    */\n   public void testBasicSplit() throws Exception {\n-    MiniDFSCluster cluster = null;\n     HRegion region = null;\n     try {\n-      cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-      // Set the hbase.rootdir to be the home directory in mini dfs.\n-      this.conf.set(HConstants.HBASE_DIR,\n-        cluster.getFileSystem().getHomeDirectory().toString());\n       HTableDescriptor htd = createTableDescriptor(getName());\n       region = createNewHRegion(htd, null, null);\n       basicSplit(region);\n@@ -87,9 +82,6 @@ public void testBasicSplit() throws Exception {\n         region.close();\n         region.getLog().closeAndDelete();\n       }\n-      if (cluster != null) {\n-        StaticTestEnvironment.shutdownDfs(cluster);\n-      }\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "sha": "4dd408013791bb96eb2093dc70fc75ba33ad8132",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "changes": 283,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "deletions": 271,
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "patch": "@@ -21,67 +21,41 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.dfs.MiniDFSCluster;\n-\n import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.HBaseTestCase;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HStoreKey;\n-import org.apache.hadoop.hbase.StaticTestEnvironment;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.HScannerInterface;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseTestCase;\n+import org.apache.hadoop.hbase.TimestampTestBase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n /**\n  * Tests user specifiable time stamps putting, getting and scanning.  Also\n  * tests same in presence of deletes.  Test cores are written so can be\n  * run against an HRegion and against an HTable: i.e. both local and remote.\n  */\n public class TestTimestamp extends HBaseTestCase {\n-  private static final long T0 = 10L;\n-  private static final long T1 = 100L;\n-  private static final long T2 = 200L;\n-  \n+  private static final Log LOG =\n+    LogFactory.getLog(TestTimestamp.class.getName());\n+\n   private static final String COLUMN_NAME = \"contents:\";\n-  \n   private static final Text COLUMN = new Text(COLUMN_NAME);\n-  private static final Text ROW = new Text(\"row\");\n-  \n-  // When creating column descriptor, how many versions of a cell to allow.\n   private static final int VERSIONS = 3;\n   \n-  private MiniDFSCluster cluster;\n-\n   /** constructor */\n   public TestTimestamp() {\n     super();\n-    this.cluster = null;\n-  }\n-\n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n-    this.cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.cluster.getFileSystem().getHomeDirectory().toString());\n-    super.setUp();\n   }\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    if (this.cluster != null) {\n-      StaticTestEnvironment.shutdownDfs(cluster);\n-    }\n-  }\n-  \n   /**\n    * Test that delete works according to description in <a\n    * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n@@ -91,11 +65,12 @@ public void testDelete() throws IOException {\n     final HRegion r = createRegion();\n     try {\n       final HRegionIncommon region = new HRegionIncommon(r);\n-      doTestDelete(region, region);\n+      TimestampTestBase.doTestDelete(region, region);\n     } finally {\n       r.close();\n       r.getLog().closeAndDelete();\n     }\n+    LOG.info(\"testDelete() finished\");    \n   }\n \n   /**\n@@ -106,246 +81,12 @@ public void testTimestampScanning() throws IOException {\n     final HRegion r = createRegion();\n     try {\n       final HRegionIncommon region = new HRegionIncommon(r);\n-      doTestTimestampScanning(region, region);\n+      TimestampTestBase.doTestTimestampScanning(region, region);\n     } finally {\n       r.close();\n       r.getLog().closeAndDelete();\n     }\n-  }\n-\n-  /**\n-   * Basic test of timestamps.\n-   * Do the above tests from client side.\n-   * @throws IOException\n-   */\n-  public void testTimestamps() throws IOException {\n-    final MiniHBaseCluster cluster =\n-      new MiniHBaseCluster(this.conf, 1, this.cluster, true);\n-    try {\n-      HTable t = createTable();\n-      Incommon incommon = new HTableIncommon(t);\n-      doTestDelete(incommon, new FlushCache() {\n-        public void flushcache() throws IOException {\n-          cluster.flushcache();\n-        }\n-       });\n-      \n-      // Perhaps drop and readd the table between tests so the former does\n-      // not pollute this latter?  Or put into separate tests.\n-      doTestTimestampScanning(incommon, new FlushCache() {\n-        public void flushcache() throws IOException {\n-          cluster.flushcache();\n-        }\n-       });\n-    } catch (Exception e) {\n-      cluster.shutdown();\n-    }\n-  }\n-  \n-  /*\n-   * Run test that delete works according to description in <a\n-   * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n-   * @param incommon\n-   * @param flusher\n-   * @throws IOException\n-   */\n-  private void doTestDelete(final Incommon incommon, FlushCache flusher)\n-  throws IOException {\n-    // Add values at various timestamps (Values are timestampes as bytes).\n-    put(incommon, T0);\n-    put(incommon, T1);\n-    put(incommon, T2);\n-    put(incommon);\n-    // Verify that returned versions match passed timestamps.\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n-    // If I delete w/o specifying a timestamp, this means I'm deleting the\n-    // latest.\n-    delete(incommon);\n-    // Verify that I get back T2 through T1 -- that the latest version has\n-    // been deleted.\n-    assertVersions(incommon, new long [] {T2, T1, T0});\n-    \n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertVersions(incommon, new long [] {T2, T1, T0});\n-    \n-    // Now add, back a latest so I can test remove other than the latest.\n-    put(incommon);\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n-    delete(incommon, T2);\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n-    \n-    // Now try deleting all from T2 back inclusive (We first need to add T2\n-    // back into the mix and to make things a little interesting, delete and\n-    // then readd T1.\n-    put(incommon, T2);\n-    delete(incommon, T1);\n-    put(incommon, T1);\n-    incommon.deleteAll(ROW, COLUMN, T2);\n-    // Should only be current value in set.  Assert this is so\n-    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n-    \n-    // Flush everything out to disk and then redo above tests\n-    flusher.flushcache();\n-    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n-  }\n-  \n-  private void assertOnlyLatest(final Incommon incommon,\n-      final long currentTime)\n-  throws IOException {\n-    Cell[] cellValues = incommon.get(ROW, COLUMN, 3/*Ask for too much*/);\n-    assertEquals(1, cellValues.length);\n-    long time = Writables.bytesToLong(cellValues[0].getValue());\n-    assertEquals(time, currentTime);\n-    assertNull(incommon.get(ROW, COLUMN, T1, 3 /*Too many*/));\n-    assertTrue(assertScanContentTimestamp(incommon, T1) == 0);\n-  }\n-  \n-  /*\n-   * Assert that returned versions match passed in timestamps and that results\n-   * are returned in the right order.  Assert that values when converted to\n-   * longs match the corresponding passed timestamp.\n-   * @param r\n-   * @param tss\n-   * @throws IOException\n-   */\n-  private void assertVersions(final Incommon incommon, final long [] tss)\n-  throws IOException {\n-    // Assert that 'latest' is what we expect.\n-    byte [] bytes = incommon.get(ROW, COLUMN).getValue();\n-    assertEquals(Writables.bytesToLong(bytes), tss[0]);\n-    // Now assert that if we ask for multiple versions, that they come out in\n-    // order.\n-    Cell[] cellValues = incommon.get(ROW, COLUMN, tss.length);\n-    assertEquals(tss.length, cellValues.length);\n-    for (int i = 0; i < cellValues.length; i++) {\n-      long ts = Writables.bytesToLong(cellValues[i].getValue());\n-      assertEquals(ts, tss[i]);\n-    }\n-    // Specify a timestamp get multiple versions.\n-    cellValues = incommon.get(ROW, COLUMN, tss[0], cellValues.length - 1);\n-    for (int i = 1; i < cellValues.length; i++) {\n-      long ts = Writables.bytesToLong(cellValues[i].getValue());\n-      assertEquals(ts, tss[i]);\n-    }\n-    // Test scanner returns expected version\n-    assertScanContentTimestamp(incommon, tss[0]);\n-  }\n-  \n-  /*\n-   * Run test scanning different timestamps.\n-   * @param incommon\n-   * @param flusher\n-   * @throws IOException\n-   */\n-  private void doTestTimestampScanning(final Incommon incommon,\n-      final FlushCache flusher)\n-  throws IOException {\n-    // Add a couple of values for three different timestamps.\n-    put(incommon, T0);\n-    put(incommon, T1);\n-    put(incommon, HConstants.LATEST_TIMESTAMP);\n-    // Get count of latest items.\n-    int count = assertScanContentTimestamp(incommon,\n-      HConstants.LATEST_TIMESTAMP);\n-    // Assert I get same count when I scan at each timestamp.\n-    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n-    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n-    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n-  }\n-  \n-  /*\n-   * Assert that the scan returns only values < timestamp. \n-   * @param r\n-   * @param ts\n-   * @return Count of items scanned.\n-   * @throws IOException\n-   */\n-  private int assertScanContentTimestamp(final Incommon in, final long ts)\n-  throws IOException {\n-    HScannerInterface scanner =\n-      in.getScanner(COLUMNS, HConstants.EMPTY_START_ROW, ts);\n-    int count = 0;\n-    try {\n-      HStoreKey key = new HStoreKey();\n-      TreeMap<Text, byte []>value = new TreeMap<Text, byte[]>();\n-      while (scanner.next(key, value)) {\n-        assertTrue(key.getTimestamp() <= ts);\n-        // Content matches the key or HConstants.LATEST_TIMESTAMP.\n-        // (Key does not match content if we 'put' with LATEST_TIMESTAMP).\n-        long l = Writables.bytesToLong(value.get(COLUMN));\n-        assertTrue(key.getTimestamp() == l ||\n-          HConstants.LATEST_TIMESTAMP == l);\n-        count++;\n-        value.clear();\n-      }\n-    } finally {\n-      scanner.close(); \n-    }\n-    return count;\n-  }\n-  \n-  private void put(final Incommon loader, final long ts)\n-  throws IOException {\n-    put(loader, Writables.longToBytes(ts), ts);\n-  }\n-  \n-  private void put(final Incommon loader)\n-  throws IOException {\n-    long ts = HConstants.LATEST_TIMESTAMP;\n-    put(loader, Writables.longToBytes(ts), ts);\n-  }\n-  \n-  /*\n-   * Put values.\n-   * @param loader\n-   * @param bytes\n-   * @param ts\n-   * @throws IOException\n-   */\n-  private void put(final Incommon loader, final byte [] bytes,\n-    final long ts)\n-  throws IOException {\n-    long lockid = loader.startUpdate(ROW);\n-    loader.put(lockid, COLUMN, bytes);\n-    if (ts == HConstants.LATEST_TIMESTAMP) {\n-      loader.commit(lockid);\n-    } else {\n-      loader.commit(lockid, ts);\n-    }\n-  }\n-  \n-  private void delete(final Incommon loader) throws IOException {\n-    delete(loader, HConstants.LATEST_TIMESTAMP);\n-  }\n-\n-  private void delete(final Incommon loader, final long ts) throws IOException {\n-    long lockid = loader.startUpdate(ROW);\n-    loader.delete(lockid, COLUMN);\n-    if (ts == HConstants.LATEST_TIMESTAMP) {\n-      loader.commit(lockid);\n-    } else {\n-      loader.commit(lockid, ts);\n-    }\n-  }\n-  \n-  /* \n-   * Create a table named TABLE_NAME.\n-   * @return An instance of an HTable connected to the created table.\n-   * @throws IOException\n-   */\n-  private HTable createTable() throws IOException {\n-    HTableDescriptor desc = new HTableDescriptor(getName());\n-    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));\n-    HBaseAdmin admin = new HBaseAdmin(conf);\n-    admin.createTable(desc);\n-    return new HTable(conf, new Text(getName()));\n+    LOG.info(\"testTimestampScanning() finished\");\n   }\n   \n   private HRegion createRegion() throws IOException {",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "sha": "54b7fbba9cbe848b5183d9a13ce7e7ffc39fd20b",
                "status": "modified"
            }
        ],
        "message": "HBASE-536 Remove MiniDFS startup from MiniHBaseCluster\n-Changed MiniHBaseCluster to not start up a MiniDFS\n-Changed HBaseClusterTestCase to do the work of starting up a MiniDFS.\n-Added pre and post setup method to HBaseClusterTestCase so you can control what happen before MiniHBaseCluster is booted up\n-Converted AbstractMergeTestCase to be a HBaseClusterTestCase\n-Converted any test that used a raw MIniDFS or MiniHBaseCluster to use HBaseClusterTestCase instead\n-Split TestTimestamp into two tests - one for clientside (now in o.a.h.h.client) and one for serverside (o.a.h.h.regionserver)\n-Merged in Stack's changes to make bin/hbase have hadoop jars first on the classpath\n-Updated PerformanceEvaluation (in --miniCluster mode) to start up a DFS first\n-Fixed a bug in BaseScanner that would have allowed NPEs to be generated\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@640526 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/aabe9f09a91b5612170975d7085c73636f51d2d1",
        "patched_files": [
            "PerformanceEvaluation.java",
            "MiniHBaseCluster.java",
            "hbase.java",
            "HBaseClusterTestCase.java",
            "BaseScanner.java",
            "MultiRegionTable.java",
            "CHANGES.java",
            "DFSAbort.java",
            "TimestampTestBase.java",
            "AbstractMergeTestBase.java",
            "HBaseCluster.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestInfoServers.java",
            "TestTableMapReduce.java",
            "TestTableIndex.java",
            "TestMasterAdmin.java",
            "TestSplit.java",
            "TestMergeTable.java",
            "TestPerformanceEvaluation.java",
            "TestHBaseCluster.java",
            "TestTimestamp.java",
            "TestLogRolling.java",
            "TestMergeMeta.java"
        ]
    },
    "hbase_69431c7": {
        "bug_id": "hbase_69431c7",
        "commit": "https://github.com/apache/hbase/commit/69431c75c16d8d863932815f0460322153a25dbb",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=69431c75c16d8d863932815f0460322153a25dbb",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.master.RackManager;\n import org.apache.hadoop.hbase.master.RegionPlan;\n+import org.apache.hadoop.hbase.master.assignment.RegionStates;\n import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.Action.Type;\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hbase.thirdparty.com.google.common.base.Joiner;\n@@ -1457,8 +1458,9 @@ public ServerName randomAssignment(RegionInfo regionInfo, List<ServerName> serve\n       // In the current set of regions even if one has region replica let us go with\n       // getting the entire snapshot\n       if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n-        if (!hasRegionReplica && this.services.getAssignmentManager().getRegionStates()\n-            .isReplicaAvailableForRegion(region)) {\n+        RegionStates states = this.services.getAssignmentManager().getRegionStates();\n+        if (!hasRegionReplica && states != null &&\n+            states.isReplicaAvailableForRegion(region)) {\n           hasRegionReplica = true;\n         }\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "6cca59fa7837a0c8b2677604ef5f5e30243d224b",
                "status": "modified"
            }
        ],
        "message": "HBASE-21102 ServerCrashProcedure should select target server where no other replicas exist for the current region - addendum fixes NPE",
        "parent": "https://github.com/apache/hbase/commit/cebb725a9f4ab8dd9d3f306d21d53d6f56161c51",
        "patched_files": [
            "BaseLoadBalancer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_6b2437d": {
        "bug_id": "hbase_6b2437d",
        "commit": "https://github.com/apache/hbase/commit/6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 1,
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "patch": "@@ -32,7 +32,7 @@ if (master.isActiveMaster()) {\n   ClusterStatus status = master.getClusterStatus();\n   masters = status.getBackupMasters();\n } else{\n-  ServerName sn = master.getMasterAddressManager().getMasterAddress();\n+  ServerName sn = master.getMasterAddressTracker().getMasterAddress();\n   assert sn != null : \"Failed to retreive master's ServerName!\";\n   masters = Collections.singletonList(sn);\n }",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "sha": "e6c86136ceaeaf1723d40d6ee57ce3513f2a6293",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 3,
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "patch": "@@ -24,13 +24,12 @@ String format = \"html\";\n <%import>\n java.util.*;\n org.apache.hadoop.hbase.regionserver.HRegionServer;\n-org.apache.hadoop.hbase.util.Bytes;\n org.apache.hadoop.hbase.HRegionInfo;\n org.apache.hadoop.hbase.ServerName;\n org.apache.hadoop.hbase.HBaseConfiguration;\n org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo;\n-org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n+org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;\n </%import>\n <%if format.equals(\"json\") %>\n   <& ../common/TaskMonitorTmpl; filter = filter; format = \"json\" &>\n@@ -41,6 +40,9 @@ org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n   ServerName serverName = ProtobufUtil.toServerName(serverInfo.getServerName());\n   List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(regionServer);\n   int masterInfoPort = regionServer.getConfiguration().getInt(\"hbase.master.info.port\", 16010);\n+  MasterAddressTracker masterAddressTracker = regionServer.getMasterAddressTracker();\n+  ServerName masterServerName = masterAddressTracker == null ? null\n+    : masterAddressTracker.getMasterAddress();\n </%java>\n <!--[if IE]>\n <!DOCTYPE html>\n@@ -146,9 +148,11 @@ org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n             <td>\n                 <%if (masterInfoPort < 0) %>\n                 No hbase.master.info.port found\n+                <%elseif masterServerName == null %>\n+                No master found\n                 <%else>\n                 <%java>\n-                String host = regionServer.getMasterAddressManager().getMasterAddress().getHostname() + \":\" + masterInfoPort;\n+                String host = masterServerName.getHostname() + \":\" + masterInfoPort;\n                 String url = \"//\" + host + \"/\";\n                 </%java>\n                 <a href=\"<% url %>\"><% host %></a>",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "sha": "1efb0b011e5b9e0f3e683211428cae6e60affb15",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -294,8 +294,8 @@\n   private DrainingServerTracker drainingServerTracker;\n   // Tracker for load balancer state\n   private LoadBalancerTracker loadBalancerTracker;\n-  // master address manager and watcher\n-  private MasterAddressTracker masterAddressManager;\n+  // master address tracker\n+  private MasterAddressTracker masterAddressTracker;\n \n   // RPC server for the HMaster\n   private final RpcServerInterface rpcServer;\n@@ -586,8 +586,8 @@ public void run() {\n     startupStatus.setDescription(\"Master startup\");\n     masterStartTime = System.currentTimeMillis();\n     try {\n-      this.masterAddressManager = new MasterAddressTracker(getZooKeeperWatcher(), this);\n-      this.masterAddressManager.start();\n+      this.masterAddressTracker = new MasterAddressTracker(getZooKeeperWatcher(), this);\n+      this.masterAddressTracker.start();\n \n       // Put up info server.\n       int port = this.conf.getInt(\"hbase.master.info.port\", HConstants.DEFAULT_MASTER_INFOPORT);\n@@ -1164,8 +1164,8 @@ public ActiveMasterManager getActiveMasterManager() {\n     return this.activeMasterManager;\n   }\n \n-  public MasterAddressTracker getMasterAddressManager() {\n-    return this.masterAddressManager;\n+  public MasterAddressTracker getMasterAddressTracker() {\n+    return this.masterAddressTracker;\n   }\n \n   /*",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "a3f873f4d638622e0da7c9f5be781cac1cebc14b",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 9,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -429,8 +429,8 @@\n   // zookeeper connection and watcher\n   private ZooKeeperWatcher zooKeeper;\n \n-  // master address manager and watcher\n-  private MasterAddressTracker masterAddressManager;\n+  // master address tracker\n+  private MasterAddressTracker masterAddressTracker;\n \n   // Cluster Status Tracker\n   private ClusterStatusTracker clusterStatusTracker;\n@@ -723,12 +723,12 @@ private void initializeZooKeeper() throws IOException, InterruptedException {\n     this.zooKeeper = new ZooKeeperWatcher(conf, REGIONSERVER + \":\" +\n       this.isa.getPort(), this);\n \n-    // Create the master address manager, register with zk, and start it.  Then\n+    // Create the master address tracker, register with zk, and start it.  Then\n     // block until a master is available.  No point in starting up if no master\n     // running.\n-    this.masterAddressManager = new MasterAddressTracker(this.zooKeeper, this);\n-    this.masterAddressManager.start();\n-    blockAndCheckIfStopped(this.masterAddressManager);\n+    this.masterAddressTracker = new MasterAddressTracker(this.zooKeeper, this);\n+    this.masterAddressTracker.start();\n+    blockAndCheckIfStopped(this.masterAddressTracker);\n \n     // Wait on cluster being up.  Master will set this flag up in zookeeper\n     // when ready.\n@@ -1588,8 +1588,8 @@ public MetricsRegionServer getMetrics() {\n   /**\n    * @return Master address tracker instance.\n    */\n-  public MasterAddressTracker getMasterAddressManager() {\n-    return this.masterAddressManager;\n+  public MasterAddressTracker getMasterAddressTracker() {\n+    return this.masterAddressTracker;\n   }\n \n   /*\n@@ -1960,7 +1960,7 @@ ReplicationSinkService getReplicationSinkService() {\n     boolean interrupted = false;\n     try {\n       while (keepLooping() && master == null) {\n-        sn = this.masterAddressManager.getMasterAddress(refresh);\n+        sn = this.masterAddressTracker.getMasterAddress(refresh);\n         if (sn == null) {\n           if (!keepLooping()) {\n             // give up with no connection.",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "73bf86ba5bcb7b8527eaab70437dfbfec1b291bb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "patch": "@@ -53,7 +53,6 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       tmpl.setFormat(req.getParameter(\"format\"));\n     if (req.getParameter(\"filter\") != null)\n       tmpl.setFilter(req.getParameter(\"filter\"));\n-    if (hrs != null) tmpl.render(resp.getWriter(), hrs);\n+    tmpl.render(resp.getWriter(), hrs);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "sha": "64ae85983385b3cc62c81e37e4dded1e53743cb7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "patch": "@@ -388,7 +388,7 @@ public void testCreateSilentIsReallySilent() throws InterruptedException,\n     // Assumes the  root of the ZooKeeper space is writable as it creates a node\n     // wherever the cluster home is defined.\n     ZooKeeperWatcher zk2 = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n-      \"testMasterAddressManagerFromZK\", null);\n+      \"testCreateSilentIsReallySilent\", null);\n \n     // Save the previous ACL\n     Stat s =  null;",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "sha": "e2b4f0cea8aac97326ba802866d42492680c6dc6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "patch": "@@ -98,7 +98,7 @@ public void setupBasicMocks() {\n \n     // Fake MasterAddressTracker\n     MasterAddressTracker tracker = Mockito.mock(MasterAddressTracker.class);\n-    Mockito.doReturn(tracker).when(master).getMasterAddressManager();\n+    Mockito.doReturn(tracker).when(master).getMasterAddressTracker();\n     Mockito.doReturn(FAKE_HOST).when(tracker).getMasterAddress();\n \n     // Mock admin",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "sha": "b351d944c0ec21e30eebfbbfad17dc8be699c597",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "patch": "@@ -58,7 +58,7 @@\n import org.apache.hadoop.hbase.Waiter;\n import org.apache.hadoop.hbase.master.SplitLogManager.Task;\n import org.apache.hadoop.hbase.master.SplitLogManager.TaskBatch;\n-import org.apache.hadoop.hbase.regionserver.TestMasterAddressManager.NodeCreationListener;\n+import org.apache.hadoop.hbase.regionserver.TestMasterAddressTracker.NodeCreationListener;\n import org.apache.hadoop.hbase.zookeeper.ZKSplitLog;\n import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "sha": "82b8673742a4c34f7e35d9bf6523cdd92d91b524",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 10,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "patch": "@@ -36,8 +36,8 @@\n import org.junit.experimental.categories.Category;\n \n @Category(MediumTests.class)\n-public class TestMasterAddressManager {\n-  private static final Log LOG = LogFactory.getLog(TestMasterAddressManager.class);\n+public class TestMasterAddressTracker {\n+  private static final Log LOG = LogFactory.getLog(TestMasterAddressTracker.class);\n \n   private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n \n@@ -56,17 +56,17 @@ public static void tearDownAfterClass() throws Exception {\n    * @throws Exception\n    */\n   @Test\n-  public void testMasterAddressManagerFromZK() throws Exception {\n+  public void testMasterAddressTrackerFromZK() throws Exception {\n \n     ZooKeeperWatcher zk = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n-        \"testMasterAddressManagerFromZK\", null);\n+        \"testMasterAddressTrackerFromZK\", null);\n     ZKUtil.createAndFailSilent(zk, zk.baseZNode);\n \n     // Should not have a master yet\n-    MasterAddressTracker addressManager = new MasterAddressTracker(zk, null);\n-    addressManager.start();\n-    assertFalse(addressManager.hasMaster());\n-    zk.registerListener(addressManager);\n+    MasterAddressTracker addressTracker = new MasterAddressTracker(zk, null);\n+    addressTracker.start();\n+    assertFalse(addressTracker.hasMaster());\n+    zk.registerListener(addressTracker);\n \n     // Use a listener to capture when the node is actually created\n     NodeCreationListener listener = new NodeCreationListener(zk, zk.getMasterAddressZNode());\n@@ -83,8 +83,8 @@ public void testMasterAddressManagerFromZK() throws Exception {\n     LOG.info(\"Waiting for master address manager to be notified\");\n     listener.waitForCreation();\n     LOG.info(\"Master node created\");\n-    assertTrue(addressManager.hasMaster());\n-    ServerName pulledAddress = addressManager.getMasterAddress();\n+    assertTrue(addressTracker.hasMaster());\n+    ServerName pulledAddress = addressTracker.getMasterAddress();\n     assertTrue(pulledAddress.equals(sn));\n \n   }",
                "previous_filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "sha": "cb5b556820c33d4a75a654c74c49ddf04874f763",
                "status": "renamed"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "patch": "@@ -78,7 +78,7 @@ public void setupBasicMocks() throws IOException, ServiceException {\n     // Fake MasterAddressTracker\n     MasterAddressTracker mat = Mockito.mock(MasterAddressTracker.class);\n     Mockito.doReturn(fakeMasterAddress).when(mat).getMasterAddress();\n-    Mockito.doReturn(mat).when(rs).getMasterAddressManager();\n+    Mockito.doReturn(mat).when(rs).getMasterAddressTracker();\n \n     MetricsRegionServer rms = Mockito.mock(MetricsRegionServer.class);\n     Mockito.doReturn(new MetricsRegionServerWrapperStub()).when(rms).getRegionServerWrapper();",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "sha": "15e55bbbfb3735efd5197b15ee18fdd17e510014",
                "status": "modified"
            }
        ],
        "message": "HBASE-10739 RS web UI NPE if master shuts down sooner\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1577415 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/1160171e645da5e18391523f76d2c673ba98a97f",
        "patched_files": [
            "SplitLogManager.java",
            "MasterAddressTracker.java",
            "HMaster.java",
            "RSStatusServlet.java",
            "HRegionServer.java",
            "MasterStatusServlet.java",
            "BackupMasterStatusTmpl.java",
            "RSStatusTmpl.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitLogManager.java",
            "TestRSStatusServlet.java",
            "TestMasterAddressTracker.java",
            "TestMasterStatusServlet.java",
            "TestZooKeeper.java"
        ]
    },
    "hbase_6c1e484": {
        "bug_id": "hbase_6c1e484",
        "commit": "https://github.com/apache/hbase/commit/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "patch": "@@ -68,7 +68,11 @@ public ReplicationHLogReaderManager(FileSystem fs, Configuration conf) {\n       this.reader = HLogFactory.createReader(this.fs, path, this.conf);\n       this.lastPath = path;\n     } else {\n-      this.reader.reset();\n+      try {\n+        this.reader.reset();\n+      } catch (NullPointerException npe) {\n+        throw new IOException(\"NPE resetting reader, likely HDFS-4380\", npe);\n+      }\n     }\n     return this.reader;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "sha": "fe6924c91e092e19f439abae654c6e783c82eb2f",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -622,9 +622,14 @@ protected boolean openReader(int sleepMultiplier) {\n     } catch (IOException ioe) {\n       LOG.warn(peerClusterZnode + \" Got: \", ioe);\n       this.reader = null;\n-      // TODO Need a better way to determinate if a file is really gone but\n-      // TODO without scanning all logs dir\n-      if (sleepMultiplier == this.maxRetriesMultiplier) {\n+      if (ioe.getCause() instanceof NullPointerException) {\n+        // Workaround for race condition in HDFS-4380\n+        // which throws a NPE if we open a file before any data node has the most recent block\n+        // Just sleep and retry. Will require re-reading compressed HLogs for compressionContext.\n+        LOG.warn(\"Got NPE opening reader, will retry.\");\n+      } else if (sleepMultiplier == this.maxRetriesMultiplier) {\n+        // TODO Need a better way to determine if a file is really gone but\n+        // TODO without scanning all logs dir\n         LOG.warn(\"Waited too long for this file, considering dumping\");\n         return !processEndOfFile();\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "bde35466ac747db05ddc1997f13bec4b2bb9ec87",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "patch": "@@ -428,7 +428,7 @@ public void loadTesting() throws Exception {\n     Result[] res = scanner.next(NB_ROWS_IN_BIG_BATCH);\n     scanner.close();\n \n-    assertEquals(NB_ROWS_IN_BATCH *10, res.length);\n+    assertEquals(NB_ROWS_IN_BIG_BATCH, res.length);\n \n     scan = new Scan();\n ",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "sha": "b9219668db4df29c2bbc42996ef6573692e10552",
                "status": "modified"
            }
        ],
        "message": "HBASE-8096 [replication] NPE while replicating a log that is acquiring a new block from HDFS\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1467662 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/64863bb03e4ca4e747503209794cdc1c85c396c7",
        "patched_files": [
            "ReplicationHLogReaderManager.java",
            "ReplicationSource.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java",
            "TestReplicationSmallTests.java"
        ]
    },
    "hbase_6d46b8d": {
        "bug_id": "hbase_6d46b8d",
        "commit": "https://github.com/apache/hbase/commit/6d46b8d256bcd63349ea83e4a588b879a122854a",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=6d46b8d256bcd63349ea83e4a588b879a122854a",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.hbase.replication.ReplicationStorageFactory;\n import org.apache.hadoop.hbase.replication.ReplicationTracker;\n import org.apache.hadoop.hbase.replication.ReplicationUtils;\n+import org.apache.hadoop.hbase.replication.SyncReplicationState;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.wal.SyncReplicationWALProvider;\n import org.apache.hadoop.hbase.wal.WALProvider;\n@@ -137,6 +138,16 @@ public void initialize(Server server, FileSystem fs, Path logDir, Path oldLogDir\n         SyncReplicationWALProvider syncWALProvider = (SyncReplicationWALProvider) walProvider;\n         peerActionListener = syncWALProvider;\n         syncWALProvider.setPeerInfoProvider(syncReplicationPeerInfoProvider);\n+        // for sync replication state change, we need to reload the state twice, you can see the\n+        // code in PeerProcedureHandlerImpl, so here we need to go over the sync replication peers\n+        // to see if any of them are in the middle of the two refreshes, if so, we need to manually\n+        // repeat the action we have done in the first refresh, otherwise when the second refresh\n+        // comes we will be in trouble, such as NPE.\n+        replicationPeers.getAllPeerIds().stream().map(replicationPeers::getPeer)\n+            .filter(p -> p.getPeerConfig().isSyncReplication())\n+            .filter(p -> p.getNewSyncReplicationState() != SyncReplicationState.NONE)\n+            .forEach(p -> syncWALProvider.peerSyncReplicationStateChange(p.getId(),\n+              p.getSyncReplicationState(), p.getNewSyncReplicationState(), 0));\n       }\n     }\n     this.statsThreadPeriod =",
                "raw_url": "https://github.com/apache/hbase/raw/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "sha": "799d9750edaf0b5ae0ccdc5a33bca038a8d00ba5",
                "status": "modified"
            },
            {
                "additions": 125,
                "blob_url": "https://github.com/apache/hbase/blob/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java?ref=6d46b8d256bcd63349ea83e4a588b879a122854a",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "patch": "@@ -0,0 +1,125 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.PeerSyncReplicationStateTransitionState.REOPEN_ALL_REGIONS_IN_PEER_VALUE;\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.CountDownLatch;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerObserver;\n+import org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Testcase for HBASE-21441.\n+ */\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationNewRSJoinBetweenRefreshes extends SyncReplicationTestBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestSyncReplicationNewRSJoinBetweenRefreshes.class);\n+\n+  private static boolean HALT;\n+\n+  private static CountDownLatch ARRIVE;\n+\n+  private static CountDownLatch RESUME;\n+\n+  public static final class HaltCP implements RegionServerObserver, RegionServerCoprocessor {\n+\n+    @Override\n+    public Optional<RegionServerObserver> getRegionServerObserver() {\n+      return Optional.of(this);\n+    }\n+\n+    @Override\n+    public void postExecuteProcedures(ObserverContext<RegionServerCoprocessorEnvironment> ctx)\n+        throws IOException {\n+      synchronized (HaltCP.class) {\n+        if (!HALT) {\n+          return;\n+        }\n+        UTIL1.getMiniHBaseCluster().getMaster().getProcedures().stream()\n+          .filter(p -> p instanceof TransitPeerSyncReplicationStateProcedure)\n+          .filter(p -> !p.isFinished()).map(p -> (TransitPeerSyncReplicationStateProcedure) p)\n+          .findFirst().ifPresent(proc -> {\n+            // this is the next state of REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN_VALUE\n+            if (proc.getCurrentStateId() == REOPEN_ALL_REGIONS_IN_PEER_VALUE) {\n+              // tell the main thread to start a new region server\n+              ARRIVE.countDown();\n+              try {\n+                // wait for the region server to online\n+                RESUME.await();\n+              } catch (InterruptedException e) {\n+                throw new RuntimeException(e);\n+              }\n+              HALT = false;\n+            }\n+          });\n+      }\n+    }\n+  }\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    UTIL1.getConfiguration().setClass(CoprocessorHost.REGIONSERVER_COPROCESSOR_CONF_KEY,\n+      HaltCP.class, RegionServerObserver.class);\n+    SyncReplicationTestBase.setUp();\n+  }\n+\n+  @Test\n+  public void test() throws IOException, InterruptedException {\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.STANDBY);\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.ACTIVE);\n+\n+    ARRIVE = new CountDownLatch(1);\n+    RESUME = new CountDownLatch(1);\n+    HALT = true;\n+    Thread t = new Thread(() -> {\n+      try {\n+        UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    });\n+    t.start();\n+    ARRIVE.await();\n+    UTIL1.getMiniHBaseCluster().startRegionServer();\n+    RESUME.countDown();\n+    t.join();\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL1.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "sha": "86ad8c0c3f0b3baddf9d0459945188a2646e7840",
                "status": "added"
            }
        ],
        "message": "HBASE-21441 NPE if RS restarts between REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN and TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE",
        "parent": "https://github.com/apache/hbase/commit/86cbbdea9ec9bde73397619fceb3bac9625813a9",
        "patched_files": [
            "Replication.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "ReplicationTests.java",
            "TestSyncReplicationNewRSJoinBetweenRefreshes.java"
        ]
    },
    "hbase_6eb8f1e": {
        "bug_id": "hbase_6eb8f1e",
        "commit": "https://github.com/apache/hbase/commit/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -131,6 +131,8 @@ Release 0.20.0 - Unreleased\n    HBASE-1323  hbase-1234 broke TestThriftServer; fix and reenable\n    HBASE-1425  ColumnValueFilter and WhileMatchFilter fixes on trunk\n                (Clint Morgan via Stack)\n+   HBASE-1431  NPE in HTable.checkAndSave when row doesn't exist (Guilherme\n+               Mauro Germoglio Barbosa via Andrew Purtell)\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/CHANGES.txt",
                "sha": "bd4122c7ee7d44cb32f28a2db351f6a3c1ed1e2f",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -1396,13 +1396,17 @@ public boolean checkAndSave(BatchUpdate b,\n         Map<byte[],Cell> actualValues = getFull(row, keySet,\n           HConstants.LATEST_TIMESTAMP, 1,lid);\n         for (byte[] key : keySet) {\n-          // If test fails exit\n-          if(!Bytes.equals(actualValues.get(key).getValue(),\n-              expectedValues.get(key))) {\n-            success = false;\n-            break;\n-          }\n-        }\n+\t  // If test fails exit\n+\t  Cell cell = actualValues.get(key);\n+\t  byte[] actualValue = new byte[] {};\n+\t  if (cell != null) \n+\t    actualValue = cell.getValue();\n+\t  if(!Bytes.equals(actualValue,\n+\t\t\t   expectedValues.get(key))) {\n+\t    success = false;\n+\t    break;\n+\t  }\n+\t}\n         if (success) {\n           long commitTime = (b.getTimestamp() == LATEST_TIMESTAMP)?\n             now: b.getTimestamp();",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "8b2342cdb324872190ef643f85ef4e9c219d13db",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestHTable.java?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "patch": "@@ -235,12 +235,24 @@ public void testCheckAndSave() throws IOException {\n     BatchUpdate batchUpdate = new BatchUpdate(row);\n     BatchUpdate batchUpdate2 = new BatchUpdate(row);\n     BatchUpdate batchUpdate3 = new BatchUpdate(row);\n+\n+    // this row doesn't exist when checkAndSave is invoked\n+    byte [] row1 = Bytes.toBytes(\"row1\");\n+    BatchUpdate batchUpdate4 = new BatchUpdate(row1);\n     \n+    // to be used for a checkAndSave for expected empty columns\n+    BatchUpdate batchUpdate5 = new BatchUpdate(row);\n+\n     HbaseMapWritable<byte[],byte[]> expectedValues =\n       new HbaseMapWritable<byte[],byte[]>();\n     HbaseMapWritable<byte[],byte[]> badExpectedValues =\n       new HbaseMapWritable<byte[],byte[]>();\n-    \n+    HbaseMapWritable<byte[],byte[]> expectedNoValues =\n+      new HbaseMapWritable<byte[],byte[]>();\n+    // the columns used here must not be updated on batchupate\n+    HbaseMapWritable<byte[],byte[]> expectedNoValues1 =\n+      new HbaseMapWritable<byte[],byte[]>();\n+\n     for(int i = 0; i < 5; i++) {\n       // This batchupdate is our initial batch update,\n       // As such we also set our expected values to the same values\n@@ -250,14 +262,27 @@ public void testCheckAndSave() throws IOException {\n       \n       badExpectedValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i),\n         Bytes.toBytes(500));\n-      \n+\n+      expectedNoValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i), new byte[] {});\n+      // the columns used here must not be updated on batchupate\n+      expectedNoValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+i+\",\"+i), new byte[] {});\n+\n+\n       // This is our second batchupdate that we will use to update the initial\n       // batchupdate\n       batchUpdate2.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+1));\n       \n       // This final batch update is to check that our expected values (which\n       // are now wrong)\n       batchUpdate3.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+2));\n+\n+      // Batch update that will not happen because it is to happen with some \n+      // expected values, but the row doesn't exist\n+      batchUpdate4.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i));\n+\n+      // Batch update will happen: the row exists, but the expected columns don't,\n+      // just as the condition\n+      batchUpdate5.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+3));\n     }\n     \n     // Initialize rows\n@@ -279,6 +304,58 @@ public void testCheckAndSave() throws IOException {\n     \n     // make sure that the old expected values fail\n     assertFalse(table.checkAndSave(batchUpdate3, expectedValues,null));\n+\n+    // row doesn't exist, so doesn't matter the expected \n+    // values (unless they are empty) \n+    assertFalse(table.checkAndSave(batchUpdate4, badExpectedValues, null));\n+\n+    assertTrue(table.checkAndSave(batchUpdate4, expectedNoValues, null));\n+    // make sure check and save saves the data when expected values were empty and the row\n+    // didn't exist\n+    r = table.getRow(row1);\n+    columns = batchUpdate4.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate4.get(columns[i])));\n+    }  \n+\n+    // since the row isn't empty anymore, those expected (empty) values \n+    // are not valid anymore, so check and save method doesn't save. \n+    assertFalse(table.checkAndSave(batchUpdate4, expectedNoValues, null));\n+    \n+    // the row exists, but the columns don't. since the expected values are \n+    // for columns without value, checkAndSave must be successful. \n+    assertTrue(table.checkAndSave(batchUpdate5, expectedNoValues1, null));\n+    // make sure checkAndSave saved values for batchUpdate5.\n+    r = table.getRow(row);\n+    columns = batchUpdate5.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate5.get(columns[i])));\n+    }  \n+\n+    // since the condition wasn't changed, the following checkAndSave \n+    // must also be successful.\n+    assertTrue(table.checkAndSave(batchUpdate, expectedNoValues1, null));\n+    // make sure checkAndSave saved values for batchUpdate1\n+    r = table.getRow(row);\n+    columns = batchUpdate.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate.get(columns[i])));\n+    }\n+\n+    // one failing condition must make the following checkAndSave fail\n+    // the failing condition is a column to be empty, however, it has a value.\n+    HbaseMapWritable<byte[],byte[]> expectedValues1 =\n+      new HbaseMapWritable<byte[],byte[]>();\n+    expectedValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+0), new byte[] {});\n+    expectedValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+\"EMPTY+ROW\"), new byte[] {});\n+    assertFalse(table.checkAndSave(batchUpdate5, expectedValues1, null));\n+\n+    // assure the values on the row remain the same\n+    r = table.getRow(row);\n+    columns = batchUpdate.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate.get(columns[i])));\n+    }    \n   }\n \n   /**\n@@ -373,5 +450,5 @@ public void testTableNotFoundExceptionWithATable() {\n      fail(\"Should have thrown a TableNotFoundException instead of a \" +\n        e.getClass());\n    }\n-  }\n+   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "sha": "fbc9c1d8c475cf145eb65ae731d28cda8d864bf2",
                "status": "modified"
            }
        ],
        "message": " HBASE-1431 NPE in HTable.checkAndSave when row doesn't exist\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@775739 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/755ea927b4c381d01025e9b66f05d780451b0f23",
        "patched_files": [
            "HRegion.java",
            "CHANGES.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestHTable.java"
        ]
    },
    "hbase_6fb44f7": {
        "bug_id": "hbase_6fb44f7",
        "commit": "https://github.com/apache/hbase/commit/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "patch": "@@ -388,7 +388,8 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    istream.seek(seekPoint);\n+    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n+\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "sha": "185423603628a26c1bd271dfaeddb4b81621ff86",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          istream.seek(fileOffset);\n+          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "sha": "0b140b6e301eff851c9c78f30252f64fc4eced2e",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "patch": "@@ -0,0 +1,43 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.io.hfile;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+public class HFileUtil {\n+\n+  /** guards against NullPointer\n+   * utility which tries to seek on the DFSIS and will try an alternative source\n+   * if the FSDataInputStream throws an NPE HBASE-17501\n+   * @param istream\n+   * @param offset\n+   * @throws IOException\n+   */\n+  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n+    try {\n+      // attempt to seek inside of current blockReader\n+      istream.seek(offset);\n+    } catch (NullPointerException e) {\n+      // retry the seek on an alternate copy of the data\n+      // this can occur if the blockReader on the DFSInputStream is null\n+      istream.seekToNewSource(offset);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "status": "added"
            }
        ],
        "message": "HBASE-17501 Revert \"Revert \"guard against NPE while reading FileTrailer and HFileBlock\"\"\n\nThis reverts commit 9a4068dcf8caec644e6703ffa365a8649bbd336e.\n\nThis is a revert of a revert -- i.e. a restore -- just so I can add the\nJIRA issue to the commit message.",
        "parent": "https://github.com/apache/hbase/commit/9a4068dcf8caec644e6703ffa365a8649bbd336e",
        "patched_files": [
            "HFileBlock.java",
            "FixedFileTrailer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFixedFileTrailer.java",
            "TestHFileBlock.java"
        ]
    },
    "hbase_70379d1": {
        "bug_id": "hbase_70379d1",
        "commit": "https://github.com/apache/hbase/commit/70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -50,6 +50,7 @@ Release 0.91.0 - Unreleased\n                when HRegionInterface#get is invoked (Mingjie Lai via\n                Andrew Purtell)\n    HBASE-3688  Setters of class HTableDescriptor do not work properly\n+   HBASE-3702  Fix NPE in Exec method parameter serialization\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/CHANGES.txt",
                "sha": "2f062db150fe04bfcffaddb0224be67a7da2cd87",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "patch": "@@ -88,8 +88,9 @@ public void write(DataOutput out) throws IOException {\n     out.writeUTF(this.methodName);\n     out.writeInt(parameterClasses.length);\n     for (int i = 0; i < parameterClasses.length; i++) {\n-      HbaseObjectWritable.writeObject(out, parameters[i], parameters[i].getClass(),\n-                                 conf);\n+      HbaseObjectWritable.writeObject(out, parameters[i],\n+          parameters[i] != null ? parameters[i].getClass() : parameterClasses[i],\n+          conf);\n       out.writeUTF(parameterClasses[i].getName());\n     }\n     // fields for Exec",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "sha": "504bd77980564077fd31eeea73f331e89720deb0",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "patch": "@@ -81,6 +81,9 @@ public int incrementCount(int diff) {\n \n     @Override\n     public String hello(String name) {\n+      if (name == null) {\n+        return \"Who are you?\";\n+      }\n       return \"Hello, \"+name;\n     }\n \n@@ -149,6 +152,8 @@ public void testSingleProxy() throws Exception {\n     assertEquals(\"Invalid custom protocol response\", \"pong\", result);\n     result = pinger.hello(\"George\");\n     assertEquals(\"Invalid custom protocol response\", \"Hello, George\", result);\n+    result = pinger.hello(null);\n+    assertEquals(\"Should handle NULL parameter\", \"Who are you?\", result);\n     int cnt = pinger.getPingCount();\n     assertTrue(\"Count should be incremented\", cnt > 0);\n     int newcnt = pinger.incrementCount(5);\n@@ -275,6 +280,23 @@ public String call(PingProtocol instance) {\n     verifyRegionResults(table, results, \"Hello, pong\", ROW_C);\n   }\n \n+  @Test\n+  public void testNullCall() throws Throwable {\n+    HTable table = new HTable(util.getConfiguration(), TEST_TABLE);\n+\n+    Map<byte[],String> results = table.coprocessorExec(PingProtocol.class,\n+        ROW_A, ROW_C,\n+        new Batch.Call<PingProtocol,String>() {\n+          public String call(PingProtocol instance) {\n+            return instance.hello(null);\n+          }\n+        });\n+\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_A);\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_B);\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_C);\n+  }\n+\n   private void verifyRegionResults(HTable table,\n       Map<byte[],String> results, byte[] row) throws Exception {\n     verifyRegionResults(table, results, \"pong\", row);",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "sha": "3ca03aa27a3fd889ed2e1e482c9f86a432fe1275",
                "status": "modified"
            }
        ],
        "message": "HBASE-3702  Fix NPE in Exec method parameter serialization\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085818 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/561deab8fc7b46e65c96743dac285dce67f52323",
        "patched_files": [
            "CHANGES.java",
            "Exec.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestServerCustomProtocol.java"
        ]
    },
    "hbase_70ecf18": {
        "bug_id": "hbase_70ecf18",
        "commit": "https://github.com/apache/hbase/commit/70ecf18817ef219389a9e024ff21ffb99b6615d9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java?ref=70ecf18817ef219389a9e024ff21ffb99b6615d9",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "patch": "@@ -149,8 +149,8 @@ public SplitLogManager(Server server, Configuration conf, Stoppable stopper,\n       Set<String> failedDeletions = Collections.synchronizedSet(new HashSet<String>());\n       SplitLogManagerDetails details =\n           new SplitLogManagerDetails(tasks, master, failedDeletions, serverName);\n-      coordination.init();\n       coordination.setDetails(details);\n+      coordination.init();\n       // Determine recovery mode\n     }\n     this.unassignedTimeout =",
                "raw_url": "https://github.com/apache/hbase/raw/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "sha": "c93ecf6b639ffc45bf4c9ca015667e948df3bde3",
                "status": "modified"
            }
        ],
        "message": "HBASE-NPE when running TestSplitLogManager (Andrey Stepachev and Zhangduo)",
        "parent": "https://github.com/apache/hbase/commit/dad2474f08d201d09989e36f5cf1c25d3fa4acee",
        "patched_files": [
            "SplitLogManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitLogManager.java"
        ]
    },
    "hbase_72bf55b": {
        "bug_id": "hbase_72bf55b",
        "commit": "https://github.com/apache/hbase/commit/72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -553,6 +553,7 @@ protected boolean openReader(int sleepMultiplier) {\n       }\n     } catch (IOException ioe) {\n       LOG.warn(peerClusterZnode + \" Got: \", ioe);\n+      this.reader = null;\n       // TODO Need a better way to determinate if a file is really gone but\n       // TODO without scanning all logs dir\n       if (sleepMultiplier == this.maxRetriesMultiplier) {",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "f18c265c28d19a9010cc45471362b3529eb07f1e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -1260,6 +1260,18 @@ public int createMultiRegions(HTable table, byte[] columnFamily)\n     Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\")\n   };\n \n+  public static final byte[][] KEYS_FOR_HBA_CREATE_TABLE = {\n+      Bytes.toBytes(\"bbb\"),\n+      Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"),\n+      Bytes.toBytes(\"fff\"), Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"),\n+      Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"), Bytes.toBytes(\"kkk\"),\n+      Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n+      Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"),\n+      Bytes.toBytes(\"rrr\"), Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"),\n+      Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"), Bytes.toBytes(\"www\"),\n+      Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\"), Bytes.toBytes(\"zzz\")\n+  };\n+\n   /**\n    * Creates many regions names \"aaa\" to \"zzz\".\n    * @param c Configuration to use.",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "c35012396d0cb0d501e126c8447914781c28907b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "deletions": 6,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "patch": "@@ -91,9 +91,8 @@\n   @BeforeClass\n   public static void setUpBeforeClass() throws Exception {\n     conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/1\");\n-    // smaller block size and capacity to trigger more operations\n-    // and test them\n-    conf1.setInt(\"hbase.regionserver.hlog.blocksize\", 1024*20);\n+    // smaller log roll size to trigger more events\n+    conf1.setFloat(\"hbase.regionserver.logroll.multiplier\", 0.0003f);\n     conf1.setInt(\"replication.source.size.capacity\", 1024);\n     conf1.setLong(\"replication.source.sleepforretries\", 100);\n     conf1.setInt(\"hbase.regionserver.maxlogs\", 10);\n@@ -142,7 +141,7 @@ public static void setUpBeforeClass() throws Exception {\n     table.addFamily(fam);\n     HBaseAdmin admin1 = new HBaseAdmin(conf1);\n     HBaseAdmin admin2 = new HBaseAdmin(conf2);\n-    admin1.createTable(table);\n+    admin1.createTable(table, HBaseTestingUtility.KEYS_FOR_HBA_CREATE_TABLE);\n     admin2.createTable(table);\n     htable1 = new HTable(conf1, tableName);\n     htable1.setWriteBufferSize(1024);\n@@ -716,8 +715,6 @@ public void testVerifyRepJob() throws Exception {\n    */\n   @Test(timeout=300000)\n   public void queueFailover() throws Exception {\n-    utility1.createMultiRegions(htable1, famName);\n-\n     // killing the RS with .META. can result into failed puts until we solve\n     // IO fencing\n     int rsToKill1 =",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "sha": "07ebc61c5c5225915ed3683e58c98d5aedb90238",
                "status": "modified"
            }
        ],
        "message": "HBASE-7530  [replication] Work around HDFS-4380 else we get NPEs\nHBASE-7531  [replication] NPE in SequenceFileLogReader because\n            ReplicationSource doesn't nullify the reader\nHBASE-7534  [replication] TestReplication.queueFailover can fail\n            because HBaseTestingUtility.createMultiRegions is dangerous\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1431768 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2eb9249ac233008ff4e280a1f5a20bbb43a97ab2",
        "patched_files": [
            "Replication.java",
            "HBaseTestingUtility.java",
            "ReplicationSource.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java",
            "TestHBaseTestingUtility.java",
            "TestReplication.java"
        ]
    },
    "hbase_748aaef": {
        "bug_id": "hbase_748aaef",
        "commit": "https://github.com/apache/hbase/commit/748aaefb12dee675adc89a7fbf08790f311525fb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java?ref=748aaefb12dee675adc89a7fbf08790f311525fb",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "patch": "@@ -53,7 +53,7 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       tmpl.setFormat(req.getParameter(\"format\"));\n     if (req.getParameter(\"filter\") != null)\n       tmpl.setFilter(req.getParameter(\"filter\"));\n-    tmpl.render(resp.getWriter(), hrs);\n+    if (hrs != null) tmpl.render(resp.getWriter(), hrs);\n   }\n \n }",
                "raw_url": "https://github.com/apache/hbase/raw/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "sha": "f94f53f412bae5bcba8709191fb6d0b61cca3c24",
                "status": "modified"
            }
        ],
        "message": "HBASE-9294 NPE in /rs-status during RS shutdown\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1576953 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d8ce8d050636dd7432dab8698098bb7ec3c10215",
        "patched_files": [
            "RSStatusServlet.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRSStatusServlet.java"
        ]
    },
    "hbase_791659b": {
        "bug_id": "hbase_791659b",
        "commit": "https://github.com/apache/hbase/commit/791659b27efdbfc3b9d6785f026856e1ea8047aa",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -343,6 +343,8 @@ Release 0.90.4 - Unreleased\n                (Li Pi)\n    HBASE-3988  Infinite loop for secondary master (Liyin Tang)\n    HBASE-3995  HBASE-3946 broke TestMasterFailover\n+   HBASE-2077  NullPointerException with an open scanner that expired causing\n+               an immediate region server shutdown -- part 2.\n \n \n   IMPROVEMENT",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/CHANGES.txt",
                "sha": "d2acf053e50656ca32a3e0a81c3b4691e066dc83",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1783,7 +1783,7 @@ private void processDeadServers(\n         Result result = region.getSecond();\n         // If region was in transition (was in zk) force it offline for reassign\n         try {\n-          // Process with existing RS shutdown code  \n+          // Process with existing RS shutdown code\n           boolean assign =\n             ServerShutdownHandler.processDeadRegion(regionInfo, result, this,\n             this.catalogTracker);",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "a6f375729968bf069b53f4d3defb818378eea5f7",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "deletions": 18,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -1943,26 +1943,27 @@ public Result next(final long scannerId) throws IOException {\n   }\n \n   public Result[] next(final long scannerId, int nbRows) throws IOException {\n+    String scannerName = String.valueOf(scannerId);\n+    InternalScanner s = this.scanners.get(scannerName);\n+    if (s == null) throw new UnknownScannerException(\"Name: \" + scannerName);\n     try {\n-      String scannerName = String.valueOf(scannerId);\n-      InternalScanner s = this.scanners.get(scannerName);\n-      if (s == null) {\n-        throw new UnknownScannerException(\"Name: \" + scannerName);\n-      }\n+      checkOpen();\n+    } catch (IOException e) {\n+      // If checkOpen failed, server not running or filesystem gone,\n+      // cancel this lease; filesystem is gone or we're closing or something.\n       try {\n-        checkOpen();\n-      } catch (IOException e) {\n-        // If checkOpen failed, server not running or filesystem gone,\n-        // cancel this lease; filesystem is gone or we're closing or something.\n-        try {\n-          this.leases.cancelLease(scannerName);\n-        } catch (LeaseException le) {\n-          LOG.info(\"Server shutting down and client tried to access missing scanner \" +\n-            scannerName);\n-        }\n-        throw e;\n+        this.leases.cancelLease(scannerName);\n+      } catch (LeaseException le) {\n+        LOG.info(\"Server shutting down and client tried to access missing scanner \" +\n+          scannerName);\n       }\n-      this.leases.renewLease(scannerName);\n+      throw e;\n+    }\n+    Leases.Lease lease = null;\n+    try {\n+      // Remove lease while its being processed in server; protects against case\n+      // where processing of request takes > lease expiration time.\n+      lease = this.leases.removeLease(scannerName);\n       List<Result> results = new ArrayList<Result>(nbRows);\n       long currentScanResultSize = 0;\n       List<KeyValue> values = new ArrayList<KeyValue>();\n@@ -2024,10 +2025,15 @@ public Result next(final long scannerId) throws IOException {\n           : results.toArray(new Result[0]);\n     } catch (Throwable t) {\n       if (t instanceof NotServingRegionException) {\n-        String scannerName = String.valueOf(scannerId);\n         this.scanners.remove(scannerName);\n       }\n       throw convertThrowableToIOE(cleanup(t));\n+    } finally {\n+      // We're done. On way out readd the above removed lease.  Adding resets\n+      // expiration time on lease.\n+      if (this.scanners.containsKey(scannerName)) {\n+        if (lease != null) this.leases.addLease(lease);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "bbfdcbe8fdcdfd06c056d200cc660171cbb040c8",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "deletions": 20,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "patch": "@@ -140,16 +140,24 @@ public void close() {\n    */\n   public void createLease(String leaseName, final LeaseListener listener)\n   throws LeaseStillHeldException {\n-    if (stopRequested) {\n+    addLease(new Lease(leaseName, listener));\n+  }\n+\n+  /**\n+   * Inserts lease.  Resets expiration before insertion.\n+   * @param lease\n+   * @throws LeaseStillHeldException\n+   */\n+  public void addLease(final Lease lease) throws LeaseStillHeldException {\n+    if (this.stopRequested) {\n       return;\n     }\n-    Lease lease = new Lease(leaseName, listener,\n-        System.currentTimeMillis() + leasePeriod);\n+    lease.setExpirationTime(System.currentTimeMillis() + this.leasePeriod);\n     synchronized (leaseQueue) {\n-      if (leases.containsKey(leaseName)) {\n-        throw new LeaseStillHeldException(leaseName);\n+      if (leases.containsKey(lease.getLeaseName())) {\n+        throw new LeaseStillHeldException(lease.getLeaseName());\n       }\n-      leases.put(leaseName, lease);\n+      leases.put(lease.getLeaseName(), lease);\n       leaseQueue.add(lease);\n     }\n   }\n@@ -189,7 +197,7 @@ public void renewLease(final String leaseName) throws LeaseException {\n       // in a corrupt leaseQueue.\n       if (lease == null || !leaseQueue.remove(lease)) {\n         throw new LeaseException(\"lease '\" + leaseName +\n-                \"' does not exist or has already expired\");\n+        \"' does not exist or has already expired\");\n       }\n       lease.setExpirationTime(System.currentTimeMillis() + leasePeriod);\n       leaseQueue.add(lease);\n@@ -198,26 +206,44 @@ public void renewLease(final String leaseName) throws LeaseException {\n \n   /**\n    * Client explicitly cancels a lease.\n-   *\n    * @param leaseName name of lease\n    * @throws LeaseException\n    */\n   public void cancelLease(final String leaseName) throws LeaseException {\n+    removeLease(leaseName);\n+  }\n+\n+  /**\n+   * Remove named lease.\n+   * Lease is removed from the list of leases and removed from the delay queue.\n+   * Lease can be resinserted using {@link #addLease(Lease)}\n+   *\n+   * @param leaseName name of lease\n+   * @throws LeaseException\n+   * @return Removed lease\n+   */\n+  Lease removeLease(final String leaseName) throws LeaseException {\n+    Lease lease =  null;\n     synchronized (leaseQueue) {\n-      Lease lease = leases.remove(leaseName);\n+      lease = leases.remove(leaseName);\n       if (lease == null) {\n         throw new LeaseException(\"lease '\" + leaseName + \"' does not exist\");\n       }\n       leaseQueue.remove(lease);\n     }\n+    return lease;\n   }\n \n   /** This class tracks a single Lease. */\n-  private static class Lease implements Delayed {\n+  static class Lease implements Delayed {\n     private final String leaseName;\n     private final LeaseListener listener;\n     private long expirationTime;\n \n+    Lease(final String leaseName, LeaseListener listener) {\n+      this(leaseName, listener, 0);\n+    }\n+\n     Lease(final String leaseName, LeaseListener listener, long expirationTime) {\n       this.leaseName = leaseName;\n       this.listener = listener;\n@@ -269,14 +295,5 @@ public int compareTo(Delayed o) {\n     public void setExpirationTime(long expirationTime) {\n       this.expirationTime = expirationTime;\n     }\n-\n-    /**\n-     * Get the expiration time for that lease\n-     * @return expiration time\n-     */\n-    public long getExpirationTime() {\n-      return this.expirationTime;\n-    }\n-\n   }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "sha": "8756efb222abc4a1619c99e846a251a4d779a196",
                "status": "modified"
            }
        ],
        "message": "HBASE-2077 NullPointerException with an open scanner that expired causing an immediate region server shutdown -- part 2\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1138180 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/34d2fa085f8119c6d48043a54d2d3c4a22654299",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_7924ba3": {
        "bug_id": "hbase_7924ba3",
        "commit": "https://github.com/apache/hbase/commit/7924ba39e7ce573369deda84f55e2a0e6ecb4872",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java?ref=7924ba39e7ce573369deda84f55e2a0e6ecb4872",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "patch": "@@ -570,6 +570,9 @@ private void removeLocationFromCache(HRegionLocation loc) {\n     byte[] startKey = loc.getRegion().getStartKey();\n     for (;;) {\n       RegionLocations oldLocs = tableCache.cache.get(startKey);\n+      if (oldLocs == null) {\n+        return;\n+      }\n       HRegionLocation oldLoc = oldLocs.getRegionLocation(loc.getRegion().getReplicaId());\n       if (!canUpdateOnError(loc, oldLoc)) {\n         return;",
                "raw_url": "https://github.com/apache/hbase/raw/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "sha": "0cdfcdd083c1dd424b922e503bd3183eb39855f8",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java?ref=7924ba39e7ce573369deda84f55e2a0e6ecb4872",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "patch": "@@ -399,4 +399,14 @@ public void testLocateBeforeInOnlyRegion() throws IOException, InterruptedExcept\n     assertArrayEquals(loc.getRegion().getStartKey(), EMPTY_START_ROW);\n     assertArrayEquals(loc.getRegion().getEndKey(), EMPTY_END_ROW);\n   }\n+\n+  @Test\n+  public void testConcurrentUpdateCachedLocationOnError() throws Exception {\n+    createSingleRegionTable();\n+    HRegionLocation loc =\n+        getDefaultRegionLocation(TABLE_NAME, EMPTY_START_ROW, RegionLocateType.CURRENT, false)\n+            .get();\n+    IntStream.range(0, 100).parallel()\n+        .forEach(i -> LOCATOR.updateCachedLocationOnError(loc, new NotServingRegionException()));\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "sha": "d1ed5b7cda88c789024da3e952e27c16c9e2c0c2",
                "status": "modified"
            }
        ],
        "message": "HBASE-23155 May NPE when concurrent AsyncNonMetaRegionLocator#updateCachedLocationOnError (#718)",
        "parent": "https://github.com/apache/hbase/commit/6aec958d66fe88be8cbc175f52162645d22e996f",
        "patched_files": [
            "AsyncNonMetaRegionLocator.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAsyncNonMetaRegionLocator.java"
        ]
    },
    "hbase_79607bd": {
        "bug_id": "hbase_79607bd",
        "commit": "https://github.com/apache/hbase/commit/79607bd9f821618e25baea53a625aec1d7985bd8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=79607bd9f821618e25baea53a625aec1d7985bd8",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -6982,6 +6982,7 @@ public Result append(Append mutate, long nonceGroup, long nonce) throws IOExcept\n     Operation op = Operation.APPEND;\n     byte[] row = mutate.getRow();\n     checkRow(row, op.toString());\n+    checkFamilies(mutate.getFamilyCellMap().keySet());\n     boolean flush = false;\n     Durability durability = getEffectiveDurability(mutate.getDurability());\n     boolean writeToWAL = durability != Durability.SKIP_WAL;\n@@ -7224,6 +7225,7 @@ public Result increment(Increment mutation, long nonceGroup, long nonce)\n     Operation op = Operation.INCREMENT;\n     byte [] row = mutation.getRow();\n     checkRow(row, op.toString());\n+    checkFamilies(mutation.getFamilyCellMap().keySet());\n     boolean flush = false;\n     Durability durability = getEffectiveDurability(mutation.getDurability());\n     boolean writeToWAL = durability != Durability.SKIP_WAL;",
                "raw_url": "https://github.com/apache/hbase/raw/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "f8cbee216a33c2eef1d0debe4c252bdf17ecc838",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hbase/blob/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java?ref=79607bd9f821618e25baea53a625aec1d7985bd8",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "patch": "@@ -134,6 +134,43 @@ public void testAppend() throws IOException {\n     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v2+v1), result.getValue(fam1, qual2)));\n   }\n \n+  @Test\n+  public void testAppendWithNonExistingFamily() throws IOException {\n+    initHRegion(tableName, name.getMethodName(), fam1);\n+    final String v1 = \"Value\";\n+    final Append a = new Append(row);\n+    a.add(fam1, qual1, Bytes.toBytes(v1));\n+    a.add(fam2, qual2, Bytes.toBytes(v1));\n+    Result result = null;\n+    try {\n+      result = region.append(a, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+      fail(\"Append operation should fail with NoSuchColumnFamilyException.\");\n+    } catch (NoSuchColumnFamilyException e) {\n+      assertEquals(null, result);\n+    } catch (Exception e) {\n+      fail(\"Append operation should fail with NoSuchColumnFamilyException.\");\n+    }\n+  }\n+\n+  @Test\n+  public void testIncrementWithNonExistingFamily() throws IOException {\n+    initHRegion(tableName, name.getMethodName(), fam1);\n+    final Increment inc = new Increment(row);\n+    inc.addColumn(fam1, qual1, 1);\n+    inc.addColumn(fam2, qual2, 1);\n+    inc.setDurability(Durability.ASYNC_WAL);\n+    try {\n+      region.increment(inc, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+    } catch (NoSuchColumnFamilyException e) {\n+      final Get g = new Get(row);\n+      final Result result = region.get(g);\n+      assertEquals(null, result.getValue(fam1, qual1));\n+      assertEquals(null, result.getValue(fam2, qual2));\n+    } catch (Exception e) {\n+      fail(\"Increment operation should fail with NoSuchColumnFamilyException.\");\n+    }\n+  }\n+\n   /**\n    * Test multi-threaded increments.\n    */",
                "raw_url": "https://github.com/apache/hbase/raw/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "sha": "49f36d6b1abc96a5be20df10c66ed6363153d6f8",
                "status": "modified"
            }
        ],
        "message": "HBASE-14525 Append and increment operation throws NullPointerException on non-existing column families.(Abhishek)",
        "parent": "https://github.com/apache/hbase/commit/a77f830198815db3927fc02646d2167c370a028f",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAtomicOperation.java",
            "TestHRegion.java"
        ]
    },
    "hbase_7cdb525": {
        "bug_id": "hbase_7cdb525",
        "commit": "https://github.com/apache/hbase/commit/7cdb52519236966a7cb6dff7fbd0609c87545f75",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java?ref=7cdb52519236966a7cb6dff7fbd0609c87545f75",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "patch": "@@ -32,6 +32,8 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.Stoppable;\n import org.apache.hadoop.hbase.io.HFileLink;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.regionserver.StoreFileInfo;\n import org.apache.hadoop.hbase.util.StealJobQueue;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -44,6 +46,7 @@\n  */\n @InterfaceAudience.Private\n public class HFileCleaner extends CleanerChore<BaseHFileCleanerDelegate> {\n+  private MasterServices master;\n \n   public static final String MASTER_HFILE_CLEANER_PLUGINS = \"hbase.master.hfilecleaner.plugins\";\n \n@@ -496,4 +499,10 @@ public synchronized void cancel(boolean mayInterruptIfRunning) {\n       t.interrupt();\n     }\n   }\n+\n+  public void init(Map<String, Object> params) {\n+    if (params != null && params.containsKey(HMaster.MASTER)) {\n+      this.master = (MasterServices) params.get(HMaster.MASTER);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "sha": "7ad6177764c2ddbaff105b3a3b97f4cb2000f126",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hbase/blob/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java?ref=7cdb52519236966a7cb6dff7fbd0609c87545f75",
                "deletions": 17,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "patch": "@@ -19,26 +19,33 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.PathFilter;\n-import org.apache.hadoop.hbase.*;\n+import org.apache.hadoop.hbase.ChoreService;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Stoppable;\n+import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;\n-import org.apache.hadoop.hbase.regionserver.CompactingMemStore;\n import org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n import org.apache.hadoop.hbase.regionserver.HRegionServer;\n-import org.apache.hadoop.hbase.regionserver.Region;\n import org.apache.hadoop.hbase.testclassification.MediumTests;\n import org.apache.hadoop.hbase.testclassification.MiscTests;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -177,10 +184,11 @@ public boolean accept(Path p) {\n   /**\n    * Test that the region directory is removed when we archive a region without store files, but\n    * still has hidden files.\n-   * @throws Exception\n+   * @throws IOException throws an IOException if there's problem creating a table\n+   *   or if there's an issue with accessing FileSystem.\n    */\n   @Test\n-  public void testDeleteRegionWithNoStoreFiles() throws Exception {\n+  public void testDeleteRegionWithNoStoreFiles() throws IOException {\n     final TableName tableName = TableName.valueOf(name.getMethodName());\n     UTIL.createTable(tableName, TEST_FAM);\n \n@@ -209,7 +217,7 @@ public void testDeleteRegionWithNoStoreFiles() throws Exception {\n     PathFilter nonHidden = new PathFilter() {\n       @Override\n       public boolean accept(Path file) {\n-        return dirFilter.accept(file) && !file.getName().toString().startsWith(\".\");\n+        return dirFilter.accept(file) && !file.getName().startsWith(\".\");\n       }\n     };\n     FileStatus[] storeDirs = FSUtils.listStatus(fs, regionDir, nonHidden);\n@@ -271,12 +279,14 @@ public void testArchiveOnTableDelete() throws Exception {\n     assertArchiveFiles(fs, storeFiles, 30000);\n   }\n \n-  private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long timeout) throws IOException {\n+  private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long timeout)\n+          throws IOException {\n     long end = System.currentTimeMillis() + timeout;\n     Path archiveDir = HFileArchiveUtil.getArchivePath(UTIL.getConfiguration());\n     List<String> archivedFiles = new ArrayList<>();\n \n-    // We have to ensure that the DeleteTableHandler is finished. HBaseAdmin.deleteXXX() can return before all files\n+    // We have to ensure that the DeleteTableHandler is finished. HBaseAdmin.deleteXXX()\n+    // can return before all files\n     // are archived. We should fix HBASE-5487 and fix synchronous operations from admin.\n     while (System.currentTimeMillis() < end) {\n       archivedFiles = getAllFileNames(fs, archiveDir);\n@@ -304,10 +314,11 @@ private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long tim\n \n   /**\n    * Test that the store files are archived when a column family is removed.\n-   * @throws Exception\n+   * @throws java.io.IOException if there's a problem creating a table.\n+   * @throws java.lang.InterruptedException problem getting a RegionServer.\n    */\n   @Test\n-  public void testArchiveOnTableFamilyDelete() throws Exception {\n+  public void testArchiveOnTableFamilyDelete() throws IOException, InterruptedException {\n     final TableName tableName = TableName.valueOf(name.getMethodName());\n     UTIL.createTable(tableName, new byte[][] {TEST_FAM, Bytes.toBytes(\"fam2\")});\n \n@@ -374,11 +385,10 @@ public void testCleaningRace() throws Exception {\n     Stoppable stoppable = new StoppableImplementation();\n \n     // The cleaner should be looping without long pauses to reproduce the race condition.\n-    HFileCleaner cleaner = new HFileCleaner(1, stoppable, conf, fs, archiveDir);\n-    assertFalse(\"cleaner should not be null\", cleaner == null);\n+    HFileCleaner cleaner = getHFileCleaner(stoppable, conf, fs, archiveDir);\n+    assertNotNull(\"cleaner should not be null\", cleaner);\n     try {\n       choreService.scheduleChore(cleaner);\n-\n       // Keep creating/archiving new files while the cleaner is running in the other thread\n       long startTime = System.currentTimeMillis();\n       for (long fid = 0; (System.currentTimeMillis() - startTime) < TEST_TIME; ++fid) {\n@@ -418,6 +428,16 @@ public void testCleaningRace() throws Exception {\n     }\n   }\n \n+  // Avoid passing a null master to CleanerChore, see HBASE-21175\n+  private HFileCleaner getHFileCleaner(Stoppable stoppable, Configuration conf,\n+        FileSystem fs, Path archiveDir) throws IOException {\n+    Map<String, Object> params = new HashMap<>();\n+    params.put(HMaster.MASTER, UTIL.getMiniHBaseCluster().getMaster());\n+    HFileCleaner cleaner = new HFileCleaner(1, stoppable, conf, fs, archiveDir);\n+    cleaner.init(params);\n+    return Objects.requireNonNull(cleaner);\n+  }\n+\n   private void clearArchiveDirectory() throws IOException {\n     UTIL.getTestFileSystem().delete(\n       new Path(UTIL.getDefaultRootDirPath(), HConstants.HFILE_ARCHIVE_DIRECTORY), true);\n@@ -428,9 +448,9 @@ private void clearArchiveDirectory() throws IOException {\n    * @param fs the file system to inspect\n    * @param archiveDir the directory in which to look\n    * @return a list of all files in the directory and sub-directories\n-   * @throws IOException\n+   * @throws java.io.IOException throws IOException in case FS is unavailable\n    */\n-  private List<String> getAllFileNames(final FileSystem fs, Path archiveDir) throws IOException {\n+  private List<String> getAllFileNames(final FileSystem fs, Path archiveDir) throws IOException  {\n     FileStatus[] files = FSUtils.listStatus(fs, archiveDir, new PathFilter() {\n       @Override\n       public boolean accept(Path p) {\n@@ -446,12 +466,16 @@ public boolean accept(Path p) {\n   /** Recursively lookup all the file names under the file[] array **/\n   private List<String> recurseOnFiles(FileSystem fs, FileStatus[] files, List<String> fileNames)\n       throws IOException {\n-    if (files == null || files.length == 0) return fileNames;\n+    if (files == null || files.length == 0) {\n+      return fileNames;\n+    }\n \n     for (FileStatus file : files) {\n       if (file.isDirectory()) {\n         recurseOnFiles(fs, FSUtils.listStatus(fs, file.getPath(), null), fileNames);\n-      } else fileNames.add(file.getPath().getName());\n+      } else {\n+        fileNames.add(file.getPath().getName());\n+      }\n     }\n     return fileNames;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "sha": "2d32f9ecb420907e40198b1534e78b291d131db2",
                "status": "modified"
            }
        ],
        "message": "HBASE-21175 Partially initialized SnapshotHFileCleaner leads to NPE during TestHFileArchiving\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/e5ba79816a21036654a62e2f51bd2fbcb6b34ca0",
        "patched_files": [
            "HFileCleaner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHFileArchiving.java",
            "TestHFileCleaner.java"
        ]
    },
    "hbase_7d1f7b7": {
        "bug_id": "hbase_7d1f7b7",
        "commit": "https://github.com/apache/hbase/commit/7d1f7b7f37a6262c564531247e82570c9a3297a7",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/7d1f7b7f37a6262c564531247e82570c9a3297a7/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=7d1f7b7f37a6262c564531247e82570c9a3297a7",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -24,6 +24,7 @@\n import java.io.BufferedOutputStream;\n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n+import java.io.EOFException;\n import java.io.FilterInputStream;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -563,6 +564,11 @@ protected void receiveResponse() {\n \n         // Read the call id.\n         RpcResponse response = RpcResponse.parseDelimitedFrom(in);\n+        if (response == null) {\n+          // When the stream is closed, protobuf doesn't raise an EOFException,\n+          // instead, it returns a null message object. \n+          throw new EOFException();\n+        }\n         int id = response.getCallId();\n \n         if (LOG.isDebugEnabled())",
                "raw_url": "https://github.com/apache/hbase/raw/7d1f7b7f37a6262c564531247e82570c9a3297a7/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "dc5d1c8ff9dd3576b7ca12bd997789219a302751",
                "status": "modified"
            }
        ],
        "message": "HBASE-5759 HBaseClient throws NullPointerException when EOFException should be used.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1311899 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/f4ea4f4f07d0dc31b42122449cea9d8fd17ef4e8",
        "patched_files": [
            "HBaseClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_7d7f84b": {
        "bug_id": "hbase_7d7f84b",
        "commit": "https://github.com/apache/hbase/commit/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -103,6 +103,7 @@ Release 0.19.0 - Unreleased\n                and no stop flag given.\n    HBASE-900   Regionserver memory leak causing OOME during relatively\n                modest bulk importing; part 1\n+   HBASE-1054  Index NPE on scanning (Clint Morgan via Andrew Purtell)\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/CHANGES.txt",
                "sha": "3a453ff5b388cabf22df1b733e9af1eb72893639",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "patch": "@@ -188,6 +188,7 @@ public HTableDescriptor(final HTableDescriptor desc) {\n         desc.values.entrySet()) {\n       this.values.put(e.getKey(), e.getValue());\n     }\n+    this.indexes.putAll(desc.indexes);\n   }\n \n   /*\n@@ -494,6 +495,11 @@ public String toString() {\n     s.append(FAMILIES);\n     s.append(\" => \");\n     s.append(families.values());\n+\n+    s.append(\", \");\n+    s.append(\"INDEXES\");\n+    s.append(\" => \");\n+    s.append(indexes.values());\n     s.append('}');\n     return s.toString();\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "sha": "79e0db58699c1c516fa4a5177103205477c4da57",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "patch": "@@ -175,4 +175,14 @@ public void write(DataOutput out) throws IOException {\n         .writeObject(out, keyGenerator, IndexKeyGenerator.class, conf);\n   }\n \n+  /** {@inheritDoc} */\n+  @Override\n+  public String toString() {\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"ID => \");\n+    sb.append(indexId);\n+    return sb.toString();\n+  }\n+  \n+  \n }",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "sha": "0e469d7c1f1d52b9758b700c85a137d9dcd0125c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "patch": "@@ -164,7 +164,10 @@ public RowResult next() throws IOException {\n         if (columns != null && columns.length > 0) {\n           LOG.debug(\"Going to base table for remaining columns\");\n           RowResult baseResult = IndexedTable.this.getRow(baseRow, columns);\n-          colValues.putAll(baseResult);\n+          \n+          if (baseResult != null) {\n+            colValues.putAll(baseResult);\n+          }\n         }\n         for (Entry<byte[], Cell> entry : row.entrySet()) {\n           byte[] col = entry.getKey();",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "sha": "d384c4aef0da7287dd28511361150208c8e8e47e",
                "status": "modified"
            }
        ],
        "message": "HBASE-1054 Index NPE on scanning\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@725324 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/db520ca53cdef9d7f3e2b6232484b137e1844ce9",
        "patched_files": [
            "HTableDescriptor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHTableDescriptor.java"
        ]
    },
    "hbase_7e995b6": {
        "bug_id": "hbase_7e995b6",
        "commit": "https://github.com/apache/hbase/commit/7e995b6496a44c057337535363aee778f52318b9",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=7e995b6496a44c057337535363aee778f52318b9",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -1206,6 +1206,21 @@ public HTable createTable(HTableDescriptor htd, byte[][] families, Configuration\n     return new HTable(c, htd.getTableName());\n   }\n \n+  /**\n+   * Create a table.\n+   * @param htd\n+   * @param splitRows\n+   * @return An HTable instance for the created table.\n+   * @throws IOException\n+   */\n+  public HTable createTable(HTableDescriptor htd, byte[][] splitRows)\n+      throws IOException {\n+    getHBaseAdmin().createTable(htd, splitRows);\n+    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are assigned\n+    waitUntilAllRegionsAssigned(htd.getTableName());\n+    return new HTable(getConfiguration(), htd.getTableName());\n+  }\n+\n   /**\n    * Create a table.\n    * @param tableName",
                "raw_url": "https://github.com/apache/hbase/raw/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "450e805028ef63d24dc67f8468ed5cc840c68871",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java?ref=7e995b6496a44c057337535363aee778f52318b9",
                "deletions": 5,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "patch": "@@ -45,8 +45,10 @@\n import org.apache.hadoop.hbase.TableDescriptor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotEnabledException;\n+import org.apache.hadoop.hbase.Waiter;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Put;\n import org.apache.hadoop.hbase.client.Table;\n@@ -633,27 +635,32 @@ public static void waitForTableToBeOnline(final HBaseTestingUtility util,\n     for (HRegion region : onlineRegions) {\n       region.waitForFlushesAndCompactions();\n     }\n-    util.getHBaseAdmin().isTableAvailable(tableName);\n+    // Wait up to 60 seconds for a table to be available.\n+    final HBaseAdmin hBaseAdmin = util.getHBaseAdmin();\n+    util.waitFor(60000, new Waiter.Predicate<IOException>() {\n+      @Override\n+      public boolean evaluate() throws IOException {\n+        return hBaseAdmin.isTableAvailable(tableName);\n+      }\n+    });\n   }\n \n   public static void createTable(final HBaseTestingUtility util, final TableName tableName,\n       int regionReplication, final byte[]... families) throws IOException, InterruptedException {\n     HTableDescriptor htd = new HTableDescriptor(tableName);\n     htd.setRegionReplication(regionReplication);\n-    for (byte[] family: families) {\n+    for (byte[] family : families) {\n       HColumnDescriptor hcd = new HColumnDescriptor(family);\n       htd.addFamily(hcd);\n     }\n     byte[][] splitKeys = getSplitKeys();\n-    util.getHBaseAdmin().createTable(htd, splitKeys);\n-    waitForTableToBeOnline(util, tableName);\n+    util.createTable(htd, splitKeys);\n     assertEquals((splitKeys.length + 1) * regionReplication,\n         util.getHBaseAdmin().getTableRegions(tableName).size());\n   }\n \n   public static byte[][] getSplitKeys() {\n     byte[][] splitKeys = new byte[KEYS.length-2][];\n-    byte[] hex = Bytes.toBytes(\"123456789abcde\");\n     for (int i = 0; i < splitKeys.length; ++i) {\n       splitKeys[i] = new byte[] { KEYS[i+1] };\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "sha": "f0070d8dbc4c5cb6b554de9c6867baad41502a79",
                "status": "modified"
            }
        ],
        "message": "HBASE-12229 NullPointerException in SnapshotTestingUtils\n\n* Implemented the waitUntilAllRegionsOnline in HBaseTestingUtility#createTable\n* Add waitFor around #isTableAvailable call that previously didn't do anything\n* Remove unused byte[] hex\n\nSigned-off-by: stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/349a56ae2c88e62917ba8923283ad4ca0c83841c",
        "patched_files": [
            "HBaseTestingUtility.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_80b40a3": {
        "bug_id": "hbase_80b40a3",
        "commit": "https://github.com/apache/hbase/commit/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "patch": "@@ -643,6 +643,12 @@ private HRegionLocation createRegionForReopen(RegionStateNode node) {\n    */\n   public HRegionLocation checkReopened(HRegionLocation oldLoc) {\n     RegionStateNode node = getRegionStateNode(oldLoc.getRegion());\n+    // HBASE-20921\n+    // if the oldLoc's state node does not exist, that means the region is\n+    // merged or split, no need to check it\n+    if (node == null) {\n+      return null;\n+    }\n     synchronized (node) {\n       if (oldLoc.getSeqNum() >= 0) {\n         // in OPEN state before",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "sha": "9f012932c482f268fa72cc73dbd19d16433cd985",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "patch": "@@ -124,7 +124,7 @@ protected Flow executeFromState(MasterProcedureEnv env, ReopenTableRegionsState\n   @Override\n   protected void rollbackState(MasterProcedureEnv env, ReopenTableRegionsState state)\n       throws IOException, InterruptedException {\n-    throw new UnsupportedOperationException();\n+    throw new UnsupportedOperationException(\"unhandled state=\" + state);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "sha": "8f3aa22357624681b7f13fc9794665325bb1ca00",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "patch": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.assignment;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureConstants;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure;\n+import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;\n+\n+\n+@Category({MasterTests.class, MediumTests.class})\n+public class TestModifyTableWhileMerging {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestModifyTableWhileMerging.class);\n+\n+  private static final Logger LOG = LoggerFactory\n+      .getLogger(TestModifyTableWhileMerging.class);\n+\n+  protected static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n+  private static TableName TABLE_NAME = TableName.valueOf(\"test\");\n+  private static Admin admin;\n+  private static Table client;\n+  private static byte[] CF = Bytes.toBytes(\"cf\");\n+  private static byte[] SPLITKEY = Bytes.toBytes(\"bbbbbbb\");\n+\n+  @BeforeClass\n+  public static void setupCluster() throws Exception {\n+    //Set procedure executor thread to 1, making reproducing this issue of HBASE-20921 easier\n+    UTIL.getConfiguration().setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n+    UTIL.startMiniCluster(1);\n+    admin = UTIL.getHBaseAdmin();\n+    byte[][] splitKeys = new byte[1][];\n+    splitKeys[0] = SPLITKEY;\n+    client = UTIL.createTable(TABLE_NAME, CF, splitKeys);\n+    UTIL.waitTableAvailable(TABLE_NAME);\n+  }\n+\n+  @AfterClass\n+  public static void cleanupTest() throws Exception {\n+    try {\n+      UTIL.shutdownMiniCluster();\n+    } catch (Exception e) {\n+      LOG.warn(\"failure shutting down cluster\", e);\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    TableDescriptor tableDescriptor = client.getDescriptor();\n+    ProcedureExecutor<MasterProcedureEnv> executor = UTIL.getMiniHBaseCluster().getMaster()\n+        .getMasterProcedureExecutor();\n+    MasterProcedureEnv env = executor.getEnvironment();\n+    List<RegionInfo> regionInfos = admin.getRegions(TABLE_NAME);\n+    MergeTableRegionsProcedure mergeTableRegionsProcedure = new MergeTableRegionsProcedure(\n+      UTIL.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor()\n+        .getEnvironment(), regionInfos.get(0), regionInfos.get(1));\n+    ModifyTableProcedure modifyTableProcedure = new ModifyTableProcedure(env, tableDescriptor);\n+    long procModify = executor.submitProcedure(modifyTableProcedure);\n+    UTIL.waitFor(30000, () -> executor.getProcedures().stream()\n+      .filter(p -> p instanceof ModifyTableProcedure)\n+      .map(p -> (ModifyTableProcedure) p)\n+      .anyMatch(p -> TABLE_NAME.equals(p.getTableName())));\n+    long proc = executor.submitProcedure(mergeTableRegionsProcedure);\n+    UTIL.waitFor(3000000, () -> UTIL.getMiniHBaseCluster().getMaster()\n+        .getMasterProcedureExecutor().isFinished(procModify));\n+    Assert.assertEquals(\"Modify Table procedure should success!\",\n+        ProcedureProtos.ProcedureState.SUCCESS, modifyTableProcedure.getState());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "sha": "16ad37315940351590d36ecd88b12970972459bc",
                "status": "added"
            }
        ],
        "message": "HBASE-20921 Possible NPE in ReopenTableRegionsProcedure",
        "parent": "https://github.com/apache/hbase/commit/d43e28dc8269d19596aaf801de8e63c8bbd8b68f",
        "patched_files": [
            "RegionStates.java",
            "ReopenTableRegionsProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRegionStates.java",
            "TestModifyTableWhileMerging.java"
        ]
    },
    "hbase_80fa336": {
        "bug_id": "hbase_80fa336",
        "commit": "https://github.com/apache/hbase/commit/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -62,6 +62,9 @@ Hbase Change Log\n                (Rong-En Fan via Stack)\r\n    HBASE-699   Fix TestMigrate up on Hudson\r\n    HBASE-615   Region balancer oscillates during cluster startup\r\n+   HBASE-613   Timestamp-anchored scanning fails to find all records\r\n+   HBASE-681   NPE in Memcache\r\n+   \r\n    \r\n   IMPROVEMENTS\r\n    HBASE-559   MR example job to count table rows\r",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/CHANGES.txt",
                "sha": "9a04347c467c2b28609a9aee7ef7dae06bb277df",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "patch": "@@ -21,11 +21,9 @@\n \n import java.io.IOException;\n import java.util.HashMap;\n-import java.util.Iterator;\n import java.util.Map;\n import java.util.SortedMap;\n import java.util.Vector;\n-import java.util.Map.Entry;\n import java.util.regex.Pattern;\n \n import org.apache.commons.logging.Log;\n@@ -183,12 +181,9 @@ public boolean isMultipleMatchScanner() {\n     return this.multipleMatchers;\n   }\n \n+  /** {@inheritDoc} */\n   public abstract boolean next(HStoreKey key,\n-    SortedMap<byte [], byte []> results)\n+      SortedMap<byte [], byte []> results)\n   throws IOException;\n   \n-  public Iterator<Entry<HStoreKey, SortedMap<byte [], byte[]>>> iterator() {\n-    throw new UnsupportedOperationException(\"Unimplemented serverside. \" +\n-      \"next(HStoreKey, StortedMap(...) is more efficient\");\n-  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "sha": "213f969d8f8dc1c231b06da474bf66f71bd3e0a8",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 30,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -23,15 +23,14 @@\n import java.io.UnsupportedEncodingException;\n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.Iterator;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.TreeSet;\n-import java.util.Map.Entry;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -64,7 +63,6 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.util.Writables;\n-import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.WritableUtils;\n import org.apache.hadoop.util.Progressable;\n import org.apache.hadoop.util.StringUtils;\n@@ -1268,20 +1266,15 @@ public InternalScanner getScanner(byte[][] cols, byte [] firstRow,\n       if (this.closed.get()) {\n         throw new IOException(\"Region \" + this + \" closed\");\n       }\n-      TreeSet<byte []> families = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);\n+      HashSet<HStore> storeSet = new HashSet<HStore>();\n       for (int i = 0; i < cols.length; i++) {\n-        families.add(HStoreKey.getFamily(cols[i]));\n-      }\n-      List<HStore> storelist = new ArrayList<HStore>();\n-      for (byte [] family: families) {\n-        HStore s = stores.get(Bytes.mapKey(family));\n-        if (s == null) {\n-          continue;\n+        HStore s = stores.get(Bytes.mapKey(HStoreKey.getFamily(cols[i])));\n+        if (s != null) {\n+          storeSet.add(s);\n         }\n-        storelist.add(s);\n       }\n       return new HScanner(cols, firstRow, timestamp,\n-        storelist.toArray(new HStore [storelist.size()]), filter);\n+        storeSet.toArray(new HStore [storeSet.size()]), filter);\n     } finally {\n       splitsAndClosesLock.readLock().unlock();\n     }\n@@ -1750,15 +1743,26 @@ public Path getBaseDir() {\n       this.scanners = new InternalScanner[stores.length];\n       try {\n         for (int i = 0; i < stores.length; i++) {\n-          // TODO: The cols passed in here can include columns from other\n-          // stores; add filter so only pertinent columns are passed.\n-          //\n-          // Also, if more than one store involved, need to replicate filters.\n-          // At least WhileMatchRowFilter will mess up the scan if only\n-          // one shared across many rows. See HADOOP-2467.\n-          scanners[i] = stores[i].getScanner(timestamp, cols, firstRow,\n-            filter != null ?\n-              (RowFilterInterface)WritableUtils.clone(filter, conf) : filter);\n+          \n+          // Only pass relevant columns to each store\n+          \n+          List<byte[]> columns = new ArrayList<byte[]>();\n+          for (int j = 0; j < cols.length; j++) {\n+            if (Bytes.equals(HStoreKey.getFamily(cols[j]),\n+                stores[i].getFamily().getName())) {\n+              columns.add(cols[j]);\n+            }\n+          }\n+\n+          RowFilterInterface f = filter;\n+          if (f != null) {\n+            // Need to replicate filters.\n+            // At least WhileMatchRowFilter will mess up the scan if only\n+            // one shared across many rows. See HADOOP-2467.\n+            f = (RowFilterInterface)WritableUtils.clone(filter, conf);\n+          }\n+          scanners[i] = stores[i].getScanner(timestamp,\n+              columns.toArray(new byte[columns.size()][]), firstRow, f);\n         }\n       } catch (IOException e) {\n         for (int i = 0; i < this.scanners.length; i++) {\n@@ -1928,14 +1932,6 @@ public void close() {\n       }\n     }\n \n-    /**\n-     * @return an iterator for the scanner\n-     */\n-    public Iterator<Entry<HStoreKey, SortedMap<Text, byte[]>>> iterator() {\n-      throw new UnsupportedOperationException(\"Unimplemented serverside. \" +\n-        \"next(HStoreKey, StortedMap(...) is more efficient\");\n-    }\n-    \n     /** {@inheritDoc} */\n     public boolean isWildcardScanner() {\n       throw new UnsupportedOperationException(\"Unimplemented on HScanner\");",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "b20efd3bfe1007ffc4ad1835c2799143b188fe84",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "patch": "@@ -584,7 +584,7 @@ static HStoreKey stripTimestamp(HStoreKey key) {\n       HStoreKey key = es.getKey();\n   \n       // if there's no column name, then compare rows and timestamps\n-      if (origin.getColumn().length == 0) {\n+      if (origin.getColumn() != null && origin.getColumn().length == 0) {\n         // if the current and origin row don't match, then we can jump\n         // out of the loop entirely.\n         if (!Bytes.equals(key.getRow(), origin.getRow())) {\n@@ -697,6 +697,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], byte []> results)\n       if (results.size() > 0) {\n         results.clear();\n       }\n+      long latestTimestamp = -1;\n       while (results.size() <= 0 && this.currentRow != null) {\n         if (deletes.size() > 0) {\n           deletes.clear();\n@@ -723,11 +724,23 @@ public boolean next(HStoreKey key, SortedMap<byte [], byte []> results)\n               continue;\n             }\n           }\n+          // We should never return HConstants.LATEST_TIMESTAMP as the time for\n+          // the row. As a compromise, we return the largest timestamp for the\n+          // entries that we find that match.\n+          if (c.getTimestamp() != HConstants.LATEST_TIMESTAMP &&\n+              c.getTimestamp() > latestTimestamp) {\n+            latestTimestamp = c.getTimestamp();\n+          }\n           results.put(column, c.getValue());\n         }\n         this.currentRow = getNextRow(this.currentRow);\n \n       }\n+      // Set the timestamp to the largest one for the row if we would otherwise\n+      // return HConstants.LATEST_TIMESTAMP\n+      if (key.getTimestamp() == HConstants.LATEST_TIMESTAMP) {\n+        key.setVersion(latestTimestamp);\n+      }\n       return results.size() > 0;\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "sha": "9caa00579adf4148065635229f632e7a805800d6",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "patch": "@@ -49,6 +49,13 @@\n   // Used around replacement of Readers if they change while we're scanning.\n   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n   \n+  /**\n+   * @param store\n+   * @param timestamp\n+   * @param targetCols\n+   * @param firstRow\n+   * @throws IOException\n+   */\n   public StoreFileScanner(final HStore store, final long timestamp,\n     final byte [][] targetCols, final byte [] firstRow)\n   throws IOException {\n@@ -209,7 +216,7 @@ long getTimestamp() {\n       return this.ts;\n     }\n   }\n-  \n+\n   /*\n    * @return An instance of <code>ViableRow</code>\n    * @throws IOException\n@@ -221,9 +228,21 @@ private ViableRow getNextViableRow() throws IOException {\n     long now = System.currentTimeMillis();\n     long ttl = store.ttl;\n     for(int i = 0; i < keys.length; i++) {\n+      // The first key that we find that matches may have a timestamp greater\n+      // than the one we're looking for. We have to advance to see if there\n+      // is an older version present, since timestamps are sorted descending\n+      while (keys[i] != null &&\n+          keys[i].getTimestamp() > this.timestamp &&\n+          columnMatch(i) &&\n+          getNext(i)) {\n+        if (columnMatch(i)) {\n+          break;\n+        }\n+      }\n       if((keys[i] != null)\n-          && (columnMatch(i))\n-          && (keys[i].getTimestamp() <= this.timestamp)\n+          // If we get here and keys[i] is not null, we already know that the\n+          // column matches and the timestamp of the row is less than or equal\n+          // to this.timestamp, so we do not need to test that here\n           && ((viableRow == null)\n               || (Bytes.compareTo(keys[i].getRow(), viableRow) < 0)\n               || ((Bytes.compareTo(keys[i].getRow(), viableRow) == 0)\n@@ -293,10 +312,9 @@ private boolean getNext(int i) throws IOException {\n           vals[i] = ibw.get();\n           result = true;\n           break;\n-        } else {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"getNext: \" + keys[i] + \": expired, skipped\");\n-          }\n+        }\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"getNext: \" + keys[i] + \": expired, skipped\");\n         }\n       }\n     }\n@@ -343,6 +361,8 @@ public void close() {\n   }\n \n   // Implementation of ChangedReadersObserver\n+  \n+  /** {@inheritDoc} */\n   public void updateReaders() throws IOException {\n     this.lock.writeLock().lock();\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "sha": "24e9d5699de25c16f9922d118b276ce3bd9a4a07",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 11,
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "patch": "@@ -99,21 +99,14 @@ public void preHBaseClusterSetup() throws Exception {\n     \n     // Now create the root and meta regions and insert the data regions\n     // created above into the meta\n-    \n-    HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-      testDir, this.conf);\n-    HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-      testDir, this.conf);\n-    HRegion.addRegionToMETA(root, meta);\n+\n+    createRootAndMetaRegions();\n     \n     for(int i = 0; i < regions.length; i++) {\n       HRegion.addRegionToMETA(meta, regions[i]);\n     }\n-    \n-    root.close();\n-    root.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n+\n+    closeRootAndMeta();\n   }\n \n   private HRegion createAregion(byte [] startKey, byte [] endKey, int firstRow,",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "sha": "c703705a16f1b2506a2ef6a7f3de732945871094",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "patch": "@@ -60,6 +60,8 @@\n   private boolean localfs = false;\n   protected Path testDir = null;\n   protected FileSystem fs = null;\n+  protected HRegion root = null;\n+  protected HRegion meta = null;\n   protected static final char FIRST_CHAR = 'a';\n   protected static final char LAST_CHAR = 'z';\n   protected static final String PUNCTUATION = \"~`@#$%^&*()-_+=:;',.<>/?[]{}|\";\n@@ -626,4 +628,22 @@ public static void shutdownDfs(MiniDFSCluster cluster) {\n       }\n     }\n   }\n+  \n+  protected void createRootAndMetaRegions() throws IOException {\n+    root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO, testDir, conf);\n+    meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO, testDir, \n+        conf);\n+    HRegion.addRegionToMETA(root, meta);\n+  }\n+  \n+  protected void closeRootAndMeta() throws IOException {\n+    if (meta != null) {\n+      meta.close();\n+      meta.getLog().closeAndDelete();\n+    }\n+    if (root != null) {\n+      root.close();\n+      root.getLog().closeAndDelete();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "sha": "d4a38900cec855b1e5b178fc7ae05ef8d07686c8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 7,
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "patch": "@@ -85,16 +85,14 @@ protected void preHBaseClusterSetup() throws Exception {\n \n       // Now create the root and meta regions and insert the data regions\n       // created above into the meta\n-      HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-          testDir, this.conf);\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-          testDir, this.conf);\n-      HRegion.addRegionToMETA(root, meta);\n+\n+      createRootAndMetaRegions();\n+\n       for(int i = 0; i < regions.length; i++) {\n         HRegion.addRegionToMETA(meta, regions[i]);\n       }\n-      closeRegionAndDeleteLog(root);\n-      closeRegionAndDeleteLog(meta);\n+      \n+      closeRootAndMeta();\n     } catch (Exception e) {\n       shutdownDfs(dfsCluster);\n       throw e;",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "sha": "17d48dca011a13abaee36cded4ffc20a5ce23145",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 9,
                "filename": "src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "patch": "@@ -84,20 +84,13 @@ public void preHBaseClusterSetup() throws IOException {\n     // Now create the root and meta regions and insert the data regions\n     // created above into the meta\n     \n-    HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-      testDir, conf);\n-    HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-      testDir, conf);\n-    HRegion.addRegionToMETA(root, meta);\n+    createRootAndMetaRegions();\n     \n     for (HRegion region : regions) {\n       HRegion.addRegionToMETA(meta, region);\n     }\n     \n-    root.close();\n-    root.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n+    closeRootAndMeta();\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "sha": "f3b59ad008ccf7a92e2c9bf2b2130cab617dc17d",
                "status": "modified"
            },
            {
                "additions": 167,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "changes": 167,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "patch": "@@ -0,0 +1,167 @@\n+/**\n+ * Copyright 2008 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.io.BatchUpdate;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * Regression test for HBASE-613\n+ */\n+public class TestScanMultipleVersions extends HBaseClusterTestCase {\n+  private final byte[] TABLE_NAME = Bytes.toBytes(\"TestScanMultipleVersions\");\n+  private final HRegionInfo[] INFOS = new HRegionInfo[2];\n+  private final HRegion[] REGIONS = new HRegion[2];\n+  private final byte[][] ROWS = new byte[][] {\n+      Bytes.toBytes(\"row_0200\"),\n+      Bytes.toBytes(\"row_0800\")\n+  };\n+  private final long[] TIMESTAMPS = new long[] {\n+      100L,\n+      1000L\n+  };\n+  private HTableDescriptor desc = null;\n+\n+  @Override\n+  protected void preHBaseClusterSetup() throws Exception {\n+    testDir = new Path(conf.get(HConstants.HBASE_DIR));\n+    \n+    // Create table description\n+    \n+    this.desc = new HTableDescriptor(TABLE_NAME);\n+    this.desc.addFamily(new HColumnDescriptor(HConstants.COLUMN_FAMILY));\n+\n+    // Region 0 will contain the key range [,row_0500)\n+    INFOS[0] = new HRegionInfo(this.desc, HConstants.EMPTY_START_ROW,\n+        Bytes.toBytes(\"row_0500\"));\n+    // Region 1 will contain the key range [row_0500,)\n+    INFOS[1] = new HRegionInfo(this.desc, Bytes.toBytes(\"row_0500\"),\n+        HConstants.EMPTY_END_ROW);\n+\n+    // Create root and meta regions\n+    createRootAndMetaRegions();\n+    // Create the regions\n+    for (int i = 0; i < REGIONS.length; i++) {\n+      REGIONS[i] =\n+        HRegion.createHRegion(this.INFOS[i], this.testDir, this.conf);\n+      // Insert data\n+      for (int j = 0; j < TIMESTAMPS.length; j++) {\n+        BatchUpdate b = new BatchUpdate(ROWS[i], TIMESTAMPS[j]);\n+        b.put(HConstants.COLUMN_FAMILY, Bytes.toBytes(TIMESTAMPS[j]));\n+        REGIONS[i].batchUpdate(b);\n+      }\n+      // Insert the region we created into the meta\n+      HRegion.addRegionToMETA(meta, REGIONS[i]);\n+      // Close region\n+      REGIONS[i].close();\n+      REGIONS[i].getLog().closeAndDelete();\n+    }\n+    // Close root and meta regions\n+    closeRootAndMeta();\n+  }\n+  \n+  /**\n+   * @throws Exception\n+   */\n+  public void testScanMultipleVersions() throws Exception {\n+    // At this point we have created multiple regions and both HDFS and HBase\n+    // are running. There are 5 cases we have to test. Each is described below.\n+\n+    HTable t = new HTable(conf, TABLE_NAME);\n+    \n+    // Case 1: scan with LATEST_TIMESTAMP. Should get two rows\n+    \n+    int count = 0;\n+    Scanner s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+\n+    // Case 2: Scan with a timestamp greater than most recent timestamp\n+    // (in this case > 1000 and < LATEST_TIMESTAMP. Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        10000L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 3: scan with timestamp equal to most recent timestamp\n+    // (in this case == 1000. Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        1000L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 4: scan with timestamp greater than first timestamp but less than\n+    // second timestamp (100 < timestamp < 1000). Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        500L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 5: scan with timestamp equal to first timestamp (100)\n+    // Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        100L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "sha": "0af704708df469e36c8fd271418c15ac7704d29e",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "deletions": 18,
                "filename": "src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "patch": "@@ -47,7 +47,6 @@\n   private final HRegion[] regions = new HRegion[5];\n   private HTableDescriptor desc;\n   private byte [][][] rows;\n-  private Path rootdir = null;\n   private MiniDFSCluster dfsCluster = null;\n   private FileSystem fs;\n   \n@@ -102,9 +101,6 @@ public void setUp() throws Exception {\n     // Start up dfs\n     this.dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n     this.fs = this.dfsCluster.getFileSystem();\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.rootdir = new Path(this.fs.getHomeDirectory(), \"hbase\");\n-    this.conf.set(HConstants.HBASE_DIR, this.rootdir.toString());\n     \n     // Note: we must call super.setUp after starting the mini cluster or\n     // we will end up with a local file system\n@@ -117,7 +113,7 @@ public void setUp() throws Exception {\n        */\n       for (int i = 0; i < sourceRegions.length; i++) {\n         regions[i] =\n-          HRegion.createHRegion(this.sourceRegions[i], this.rootdir, this.conf);\n+          HRegion.createHRegion(this.sourceRegions[i], this.testDir, this.conf);\n         /*\n          * Insert data\n          */\n@@ -128,23 +124,14 @@ public void setUp() throws Exception {\n           regions[i].batchUpdate(b);\n         }\n       }\n-      // Create root region\n-      HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-          this.rootdir, this.conf);\n-      // Create meta region\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-          this.rootdir, this.conf);\n-      // Insert meta into root region\n-      HRegion.addRegionToMETA(root, meta);\n+      // Create root and meta regions\n+      createRootAndMetaRegions();\n       // Insert the regions we created into the meta\n       for(int i = 0; i < regions.length; i++) {\n         HRegion.addRegionToMETA(meta, regions[i]);\n       }\n       // Close root and meta regions\n-      root.close();\n-      root.getLog().closeAndDelete();\n-      meta.close();\n-      meta.getLog().closeAndDelete();\n+      closeRootAndMeta();\n       \n     } catch (Exception e) {\n       shutdownDfs(dfsCluster);\n@@ -182,7 +169,7 @@ private HRegion mergeAndVerify(final String msg, final String regionName1,\n     // Now verify that we can read all the rows from regions 0, 1\n     // in the new merged region.\n     HRegion merged =\n-      HRegion.openHRegion(mergedInfo, this.rootdir, log, this.conf);\n+      HRegion.openHRegion(mergedInfo, this.testDir, log, this.conf);\n     verifyMerge(merged, upperbound);\n     merged.close();\n     LOG.info(\"Verified \" + msg);",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "sha": "e9892d98a5a5dcbd35a3177576bbf5fdb26e8754",
                "status": "modified"
            }
        ],
        "message": "HBASE-613 Timestamp-anchored scanning fails to find all records\nHBASE-681 NPE in Memcache\n\nHAbstractScanner\n- remove HAbstactScanner.iterator() - iterator is not a method on InternalScanner\n\nHRegion\n- make getScanner more efficient by iterating only once to find the stores we need to scan\n- only pass columns relevant to a store to a HStoreScanner\n- remove HScanner.iterator() - iterator is not a method on InternalScanner\n\nMemcache, MemcacheScanner\n- Fix NPE in Memcache\n- never return HConstants.LATEST_TIMESTAMP as the timestamp value for a row. Instead use the largest timestamp from the cells being returned. This allows a scanner to determine a timestamp that can be used to fetch the same data again should new versions be inserted later.\n\nStoreFileScanner\n- getNextViableRow would find a row that matched the row key, but did not consider the requested timestamp. Now if the row it finds has a timestamp greater than the one desired it advances to determine if a row with a timestamp less than or equal to the requested one exists since timestamps are sorted descending.\n- removed an unnecessary else\n\ntestScanMultipleVersions\n- Test program that fails on current trunk but passes when this patch is applied.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@670124 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d5b1dfe30c20f697105a89124042ba11ea7e3657",
        "patched_files": [
            "MultiRegionTable.java",
            "HBaseTestCase.java",
            "HRegion.java",
            "CHANGES.java",
            "Memcache.java",
            "HAbstractScanner.java",
            "StoreFileScanner.java",
            "AbstractMergeTestBase.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestScanMultipleVersions.java",
            "TestRegionRebalancing.java",
            "TestMergeTool.java"
        ]
    },
    "hbase_81e2aba": {
        "bug_id": "hbase_81e2aba",
        "commit": "https://github.com/apache/hbase/commit/81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java?ref=81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
                "deletions": 9,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "patch": "@@ -210,15 +210,16 @@ public void write(ImmutableBytesWritable row, V cell)\n           if (conf.getBoolean(LOCALITY_SENSITIVE_CONF_KEY, DEFAULT_LOCALITY_SENSITIVE)) {\n             HRegionLocation loc = null;\n             String tableName = conf.get(OUTPUT_TABLE_NAME_CONF_KEY);\n-\n-            try (Connection connection = ConnectionFactory.createConnection(conf);\n-                   RegionLocator locator =\n-                     connection.getRegionLocator(TableName.valueOf(tableName))) {\n-              loc = locator.getRegionLocation(rowKey);\n-            } catch (Throwable e) {\n-              LOG.warn(\"there's something wrong when locating rowkey: \" +\n-                Bytes.toString(rowKey), e);\n-              loc = null;\n+            if (tableName != null) {\n+              try (Connection connection = ConnectionFactory.createConnection(conf);\n+                     RegionLocator locator =\n+                       connection.getRegionLocator(TableName.valueOf(tableName))) {\n+                loc = locator.getRegionLocation(rowKey);\n+              } catch (Throwable e) {\n+                LOG.warn(\"there's something wrong when locating rowkey: \" +\n+                  Bytes.toString(rowKey), e);\n+                loc = null;\n+              }\n             }\n \n             if (null == loc) {",
                "raw_url": "https://github.com/apache/hbase/raw/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "sha": "a18377305f6d9feab9fcf2c30c1c86eb904284ea",
                "status": "modified"
            }
        ],
        "message": "HBASE-14662 Fix NPE in HFileOutputFormat2 (Heng Chen)",
        "parent": "https://github.com/apache/hbase/commit/5363f371bb7cd83902e8027b221fe7bf690f9eec",
        "patched_files": [
            "HFileOutputFormat2.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHFileOutputFormat2.java"
        ]
    },
    "hbase_842f480": {
        "bug_id": "hbase_842f480",
        "commit": "https://github.com/apache/hbase/commit/842f4808db3a4951f43d958f7933e12ca9bd4830",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -480,8 +480,12 @@ private void finishActiveMasterInitialization(MonitoredTask status)\n     ZKClusterId.setClusterId(this.zooKeeper, fileSystemManager.getClusterId());\n     this.serverManager = createServerManager(this, this);\n \n-    metaTableLocator = new MetaTableLocator();\n-    shortCircuitConnection = createShortCircuitConnection();\n+    synchronized (this) {\n+      if (shortCircuitConnection == null) {\n+        shortCircuitConnection = createShortCircuitConnection();\n+        metaTableLocator = new MetaTableLocator();\n+      }\n+    }\n \n     // Invalidate all write locks held previously\n     this.tableLockManager.reapWriteLocks();",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "714b5a8aede574f53bc0ce160f540b6c6347820b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "deletions": 4,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "patch": "@@ -27,8 +27,6 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.ServerName;\n@@ -43,7 +41,6 @@\n  */\n @InterfaceAudience.Private\n public class MasterStatusServlet extends HttpServlet {\n-  private static final Log LOG = LogFactory.getLog(MasterStatusServlet.class);\n   private static final long serialVersionUID = 1L;\n \n   @Override\n@@ -88,7 +85,9 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n   }\n \n   private ServerName getMetaLocationOrNull(HMaster master) {\n-    return master.getMetaTableLocator().getMetaRegionLocation(master.getZooKeeper());\n+    MetaTableLocator metaTableLocator = master.getMetaTableLocator();\n+    return metaTableLocator == null ? null :\n+      metaTableLocator.getMetaRegionLocation(master.getZooKeeper());\n   }\n \n   private Map<String, Integer> getFragmentationInfo(",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "sha": "3059096380943f801394cf1a88e2e8aacfcbbf81",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -637,8 +637,12 @@ private void initializeZooKeeper() throws IOException, InterruptedException {\n       this.abort(\"Failed to retrieve Cluster ID\",e);\n     }\n \n-    shortCircuitConnection = createShortCircuitConnection();\n-    metaTableLocator = new MetaTableLocator();\n+    synchronized (this) {\n+      if (shortCircuitConnection == null) {\n+        shortCircuitConnection = createShortCircuitConnection();\n+        metaTableLocator = new MetaTableLocator();\n+      }\n+    }\n \n     // watch for snapshots and other procedures\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "7947c4a6c55c6942df6642fe31efdbde958579b1",
                "status": "modified"
            }
        ],
        "message": "HBASE-11880 NPE in MasterStatusServlet",
        "parent": "https://github.com/apache/hbase/commit/211c1e8ad439f2dfbbd2f07ce099c0f948e06df3",
        "patched_files": [
            "MasterStatusServlet.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMasterStatusServlet.java"
        ]
    },
    "hbase_8525e1e": {
        "bug_id": "hbase_8525e1e",
        "commit": "https://github.com/apache/hbase/commit/8525e1ee23a269ba64821d26060a3ef724e307eb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -44,6 +44,7 @@ Release 0.21.0 - Unreleased\n                split (Cosmin Lehane via Stack)\n    HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n    HBASE-1859  Misc shell fixes patch (Kyle Oba via Stack)\n+   HBASE-1865  0.20.0 TableInputFormatBase NPE\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "sha": "3df60cc6deb9fe206c30fa04129779713aca22ea",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -269,13 +269,13 @@ public float getProgress() {\n    */\n   @Override\n   public List<InputSplit> getSplits(JobContext context) throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"No table was provided.\");\n+    }\n     byte [][] startKeys = table.getStartKeys();\n     if (startKeys == null || startKeys.length == 0) {\n       throw new IOException(\"Expecting at least one region.\");\n     }\n-    if (table == null) {\n-      throw new IOException(\"No table was provided.\");\n-    }\n     int realNumSplits = startKeys.length;\n     InputSplit[] splits = new InputSplit[realNumSplits];\n     int middle = startKeys.length / realNumSplits;",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "660b27171d7f900bd3fafe31fce4300b5c4d3c4b",
                "status": "modified"
            }
        ],
        "message": "HBASE-1865 0.20.0 TableInputFormatBase NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@818651 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/dadeb0bdde6461f9fc061c1a316d85045038eca8",
        "patched_files": [
            "TableInputFormatBase.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_8620d2c": {
        "bug_id": "hbase_8620d2c",
        "commit": "https://github.com/apache/hbase/commit/8620d2c6971807d96b6a14db3f5822ae1465f7ce",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8620d2c6971807d96b6a14db3f5822ae1465f7ce/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8620d2c6971807d96b6a14db3f5822ae1465f7ce",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -812,6 +812,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3383  [0.90RC1] bin/hbase script displays \"no such file\" warning on\n                target/cached_classpath.txt\n    HBASE-3344  Master aborts after RPC to server that was shutting down\n+   HBASE-3408  AssignmentManager NullPointerException\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/8620d2c6971807d96b6a14db3f5822ae1465f7ce/CHANGES.txt",
                "sha": "fd626a18cc16c1df0ae6df75f4fd82098005e727",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/8620d2c6971807d96b6a14db3f5822ae1465f7ce/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8620d2c6971807d96b6a14db3f5822ae1465f7ce",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -21,8 +21,8 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n-import java.io.IOException;\n import java.io.EOFException;\n+import java.io.IOException;\n import java.net.ConnectException;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -1005,8 +1005,9 @@ RegionPlan getRegionPlan(final RegionState state,\n     RegionPlan existingPlan = null;\n     synchronized (this.regionPlans) {\n       existingPlan = this.regionPlans.get(encodedName);\n-      if (existingPlan == null || forceNewPlan ||\n-          (existingPlan != null && existingPlan.getDestination().equals(serverToExclude))) {\n+      if (forceNewPlan || existingPlan == null \n+              || existingPlan.getDestination() == null \n+              || existingPlan.getDestination().equals(serverToExclude)) {\n         newPlan = true;\n         this.regionPlans.put(encodedName, randomPlan);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/8620d2c6971807d96b6a14db3f5822ae1465f7ce/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "675810d8e37abfe1e64423d87c1705eaff374887",
                "status": "modified"
            }
        ],
        "message": "HBASE-3408 AssignmentManager NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1055248 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/46a56a74db2a423557584c3ed1c2027299204326",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8979464": {
        "bug_id": "hbase_8979464",
        "commit": "https://github.com/apache/hbase/commit/89794648e6e8a2588e0748fb1b6770e37816f008",
        "file": [
            {
                "additions": 227,
                "blob_url": "https://github.com/apache/hbase/blob/89794648e6e8a2588e0748fb1b6770e37816f008/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "changes": 227,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java?ref=89794648e6e8a2588e0748fb1b6770e37816f008",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "patch": "@@ -0,0 +1,227 @@\n+/**\n+ * Copyright 2011 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver.handler;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HServerInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ZooKeeperConnectionException;\n+import org.apache.hadoop.hbase.catalog.CatalogTracker;\n+import org.apache.hadoop.hbase.ipc.HBaseRpcMetrics;\n+import org.apache.hadoop.hbase.regionserver.CompactionRequestor;\n+import org.apache.hadoop.hbase.regionserver.FlushRequester;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.regionserver.RegionServerServices;\n+import org.apache.hadoop.hbase.regionserver.wal.HLog;\n+import org.apache.hadoop.hbase.zookeeper.ZKAssign;\n+import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.KeeperException.NodeExistsException;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Test of the {@link OpenRegionHandler}.\n+ */\n+public class TestOpenRegionHandler {\n+  private static final Log LOG = LogFactory.getLog(TestOpenRegionHandler.class);\n+  private final static HBaseTestingUtility HTU = new HBaseTestingUtility();\n+\n+  @BeforeClass public static void before() throws Exception {\n+    HTU.startMiniZKCluster();\n+  }\n+\n+  @AfterClass public static void after() throws IOException {\n+    HTU.shutdownMiniZKCluster();\n+  }\n+\n+  /**\n+   * Basic mock Server\n+   */\n+  static class MockServer implements Server {\n+    boolean stopped = false;\n+    final static String NAME = \"MockServer\";\n+    final ZooKeeperWatcher zk;\n+\n+    MockServer() throws ZooKeeperConnectionException, IOException {\n+      this.zk =  new ZooKeeperWatcher(HTU.getConfiguration(), NAME, this);\n+    }\n+\n+    @Override\n+    public void abort(String why, Throwable e) {\n+      LOG.fatal(\"Abort why=\" + why, e);\n+      this.stopped = true;\n+    }\n+\n+    @Override\n+    public void stop(String why) {\n+      LOG.debug(\"Stop why=\" + why);\n+      this.stopped = true;\n+    }\n+\n+    @Override\n+    public boolean isStopped() {\n+      return this.stopped;\n+    }\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+      return HTU.getConfiguration();\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeper() {\n+      return this.zk;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      // TODO Auto-generated method stub\n+      return null;\n+    }\n+\n+    @Override\n+    public String getServerName() {\n+      return NAME;\n+    }\n+  }\n+\n+  /**\n+   * Basic mock region server services.\n+   */\n+  static class MockRegionServerServices implements RegionServerServices {\n+    final Map<String, HRegion> regions = new HashMap<String, HRegion>();\n+    boolean stopping = false;\n+\n+    @Override\n+    public boolean removeFromOnlineRegions(String encodedRegionName) {\n+      return this.regions.remove(encodedRegionName) != null;\n+    }\n+    \n+    @Override\n+    public HRegion getFromOnlineRegions(String encodedRegionName) {\n+      return this.regions.get(encodedRegionName);\n+    }\n+    \n+    @Override\n+    public void addToOnlineRegions(HRegion r) {\n+      this.regions.put(r.getRegionInfo().getEncodedName(), r);\n+    }\n+    \n+    @Override\n+    public void postOpenDeployTasks(HRegion r, CatalogTracker ct, boolean daughter)\n+        throws KeeperException, IOException {\n+    }\n+    \n+    @Override\n+    public boolean isStopping() {\n+      return this.stopping;\n+    }\n+    \n+    @Override\n+    public HLog getWAL() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public HServerInfo getServerInfo() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public HBaseRpcMetrics getRpcMetrics() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public FlushRequester getFlushRequester() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public CompactionRequestor getCompactionRequester() {\n+      return null;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      return null;\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeperWatcher() {\n+      return null;\n+    }\n+  };\n+\n+  /**\n+   * Test the openregionhandler can deal with its znode being yanked out from\n+   * under it.\n+   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-3627\">HBASE-3627</a>\n+   * @throws IOException\n+   * @throws NodeExistsException\n+   * @throws KeeperException\n+   */\n+  @Test public void testOpenRegionHandlerYankingRegionFromUnderIt()\n+  throws IOException, NodeExistsException, KeeperException {\n+    final Server server = new MockServer();\n+    final RegionServerServices rss = new MockRegionServerServices();\n+\n+    HTableDescriptor htd =\n+      new HTableDescriptor(\"testOpenRegionHandlerYankingRegionFromUnderIt\");\n+    final HRegionInfo hri =\n+      new HRegionInfo(htd, HConstants.EMPTY_END_ROW, HConstants.EMPTY_END_ROW);\n+    OpenRegionHandler handler = new OpenRegionHandler(server, rss, hri) {\n+      HRegion openRegion() {\n+        // Open region first, then remove znode as though it'd been hijacked.\n+        HRegion region = super.openRegion();\n+        // Don't actually open region BUT remove the znode as though it'd\n+        // been hijacked on us.\n+        ZooKeeperWatcher zkw = this.server.getZooKeeper();\n+        String node = ZKAssign.getNodeName(zkw, hri.getEncodedName());\n+        try {\n+          ZKUtil.deleteNodeFailSilent(zkw, node);\n+        } catch (KeeperException e) {\n+          throw new RuntimeException(\"Ugh failed delete of \" + node, e);\n+        }\n+        return region;\n+      }\n+    };\n+    // Call process without first creating OFFLINE region in zk, see if\n+    // exception or just quiet return (expected).\n+    handler.process();\n+    ZKAssign.createNodeOffline(server.getZooKeeper(), hri, server.getServerName());\n+    // Call process again but this time yank the zk znode out from under it\n+    // post OPENING; again will expect it to come back w/o NPE or exception.\n+    handler.process();\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/89794648e6e8a2588e0748fb1b6770e37816f008/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "sha": "d1607cd275aa8f08acf623cf4d00a300c64e2ca0",
                "status": "added"
            }
        ],
        "message": "HBASE-3627 NPE in EventHandler when region already reassigned\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085076 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
        "patched_files": [
            "OpenRegionHandler.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestOpenRegionHandler.java"
        ]
    },
    "hbase_8aeb3ac": {
        "bug_id": "hbase_8aeb3ac",
        "commit": "https://github.com/apache/hbase/commit/8aeb3acaf959e2905191fd6c92fa56300f7d3597",
        "file": [
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hbase/blob/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java?ref=8aeb3acaf959e2905191fd6c92fa56300f7d3597",
                "deletions": 14,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "patch": "@@ -26,14 +26,14 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n-import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.client.TableState;\n import org.apache.hadoop.hbase.executor.EventHandler;\n import org.apache.hadoop.hbase.executor.EventType;\n@@ -207,19 +207,26 @@ private void handleEnableTable() throws IOException,\n     List<ServerName> onlineServers = serverManager.createDestinationServersList();\n     Map<ServerName, List<HRegionInfo>> bulkPlan =\n         this.assignmentManager.getBalancer().retainAssignment(regionsToAssign, onlineServers);\n-    LOG.info(\"Bulk assigning \" + regionsCount + \" region(s) across \" + bulkPlan.size()\n-      + \" server(s), retainAssignment=true\");\n+    if (bulkPlan != null) {\n+      LOG.info(\"Bulk assigning \" + regionsCount + \" region(s) across \" + bulkPlan.size()\n+          + \" server(s), retainAssignment=true\");\n \n-    BulkAssigner ba = new GeneralBulkAssigner(this.server, bulkPlan, this.assignmentManager, true);\n-    try {\n-      if (ba.bulkAssign()) {\n-        done = true;\n+      BulkAssigner ba =\n+          new GeneralBulkAssigner(this.server, bulkPlan, this.assignmentManager, true);\n+      try {\n+        if (ba.bulkAssign()) {\n+          done = true;\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"Enable operation was interrupted when enabling table '\"\n+            + this.tableName + \"'\");\n+        // Preserve the interrupt.\n+        Thread.currentThread().interrupt();\n       }\n-    } catch (InterruptedException e) {\n-      LOG.warn(\"Enable operation was interrupted when enabling table '\"\n-        + this.tableName + \"'\");\n-      // Preserve the interrupt.\n-      Thread.currentThread().interrupt();\n+    } else {\n+      LOG.info(\"Balancer was unable to find suitable servers for table \" + tableName\n+          + \", leaving unassigned\");\n+      done = true;\n     }\n     if (done) {\n       // Flip the table to enabled.",
                "raw_url": "https://github.com/apache/hbase/raw/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "sha": "c4969be90659a87315b51ae5489032c909533def",
                "status": "modified"
            },
            {
                "additions": 101,
                "blob_url": "https://github.com/apache/hbase/blob/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java?ref=8aeb3acaf959e2905191fd6c92fa56300f7d3597",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.handler;\n+\n+import java.util.List;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.MiniHBaseCluster;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+@Category({ MasterTests.class, MediumTests.class })\n+public class TestEnableTableHandler {\n+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Log LOG = LogFactory.getLog(TestEnableTableHandler.class);\n+  private static final byte[] FAMILYNAME = Bytes.toBytes(\"fam\");\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    TEST_UTIL.startMiniCluster(1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test(timeout = 300000)\n+  public void testEnableTableWithNoRegionServers() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testEnableTableWithNoRegionServers\");\n+    final MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n+    final HMaster m = cluster.getMaster();\n+    final HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();\n+    final HTableDescriptor desc = new HTableDescriptor(tableName);\n+    desc.addFamily(new HColumnDescriptor(FAMILYNAME));\n+    admin.createTable(desc);\n+    admin.disableTable(tableName);\n+    TEST_UTIL.waitTableDisabled(tableName.getName());\n+\n+    admin.enableTable(tableName);\n+    TEST_UTIL.waitTableEnabled(tableName);\n+\n+    // disable once more\n+    admin.disableTable(tableName);\n+\n+    // now stop region servers\n+    JVMClusterUtil.RegionServerThread rs = cluster.getRegionServerThreads().get(0);\n+    rs.getRegionServer().stop(\"stop\");\n+    cluster.waitForRegionServerToStop(rs.getRegionServer().getServerName(), 10000);\n+\n+    EnableTableHandler handler =\n+        new EnableTableHandler(m, tableName, m.getAssignmentManager(), m.getTableLockManager(),\n+            true);\n+    handler.prepare();\n+    handler.process();\n+\n+    assertTrue(admin.isTableEnabled(tableName));\n+\n+    JVMClusterUtil.RegionServerThread rs2 = cluster.startRegionServer();\n+    m.getAssignmentManager().assign(admin.getTableRegions(tableName));\n+    TEST_UTIL.waitUntilAllRegionsAssigned(tableName);\n+    List<HRegionInfo> onlineRegions = admin.getOnlineRegions(\n+        rs2.getRegionServer().getServerName());\n+    assertEquals(1, onlineRegions.size());\n+    assertEquals(tableName, onlineRegions.get(0).getTable());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "sha": "f5f2cd056df54c620a9f3aecf2bc882fc1fcd932",
                "status": "added"
            }
        ],
        "message": "HBASE-12966 NPE in HMaster while recovering tables in Enabling state (Andrey Stepachev)",
        "parent": "https://github.com/apache/hbase/commit/4f472062a436726f686608023126dd8d5cc9c9bc",
        "patched_files": [
            "EnableTableHandler.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestEnableTableHandler.java"
        ]
    },
    "hbase_8b693bf": {
        "bug_id": "hbase_8b693bf",
        "commit": "https://github.com/apache/hbase/commit/8b693bf547bc055a83de6aa0f239f481961173a1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -9,6 +9,8 @@ Release 0.91.0 - Unreleased\n    HBASE-3280  YouAreDeadException being swallowed in HRS getMaster\n    HBASE-3282  Need to retain DeadServers to ensure we don't allow\n                previously expired RS instances to rejoin cluster\n+   HBASE-3283  NPE in AssignmentManager if processing shutdown of RS who\n+               doesn't have any regions assigned to it\n \n   IMPROVEMENTS\n    HBASE-2001  Coprocessors: Colocate user code with regions (Mingjie Lai via",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "sha": "eae17ae9baa7ad371f81f204b9d1b038eba6b9ae",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1597,16 +1597,21 @@ protected void chore() {\n     // Remove this server from map of servers to regions, and remove all regions\n     // of this server from online map of regions.\n     Set<HRegionInfo> deadRegions = null;\n+    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (this.regions) {\n-      deadRegions = new TreeSet<HRegionInfo>(this.servers.remove(hsi));\n+      List<HRegionInfo> assignedRegions = this.servers.remove(hsi);\n+      if (assignedRegions == null || assignedRegions.isEmpty()) {\n+        // No regions on this server, we are done, return empty list of RITs\n+        return rits;\n+      }\n+      deadRegions = new TreeSet<HRegionInfo>(assignedRegions);\n       for (HRegionInfo region : deadRegions) {\n         this.regions.remove(region);\n       }\n     }\n     // See if any of the regions that were online on this server were in RIT\n     // If they are, normal timeouts will deal with them appropriately so\n     // let's skip a manual re-assignment.\n-    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (regionsInTransition) {\n       for (RegionState region : this.regionsInTransition.values()) {\n         if (deadRegions.remove(region.getRegion())) {",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "6811cdc6438af677ce02985f27d830f49b032905",
                "status": "modified"
            }
        ],
        "message": "HBASE-3283 NPE in AssignmentManager if processing shutdown of RS who doesn't have any regions assigned to it\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1040302 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2778fced7580043d58116f83badc8629b290505f",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8cef99e": {
        "bug_id": "hbase_8cef99e",
        "commit": "https://github.com/apache/hbase/commit/8cef99e5062d889a748c8442595a0e0644e11458",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/8cef99e5062d889a748c8442595a0e0644e11458/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java?ref=8cef99e5062d889a748c8442595a0e0644e11458",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "patch": "@@ -586,7 +586,8 @@ public int getCacheCount() {\n     // the caller will receive a result back where the number of cells in the result is less than\n     // the batch size even though it may not be the last group of cells for that row.\n     if (allowPartials || isBatchSet) {\n-      addResultsToList(resultsToAddToCache, resultsFromServer, 0, resultsFromServer.length);\n+      addResultsToList(resultsToAddToCache, resultsFromServer, 0,\n+          (null == resultsFromServer ? 0 : resultsFromServer.length));\n       return resultsToAddToCache;\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/8cef99e5062d889a748c8442595a0e0644e11458/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "sha": "c013a4d89b93efbafb35e5abc17c5402565c6097",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hbase/blob/8cef99e5062d889a748c8442595a0e0644e11458/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java?ref=8cef99e5062d889a748c8442595a0e0644e11458",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "patch": "@@ -73,6 +73,8 @@\n import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;\n import org.apache.hadoop.hbase.filter.Filter;\n import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.InclusiveStopFilter;\n import org.apache.hadoop.hbase.filter.KeyOnlyFilter;\n import org.apache.hadoop.hbase.filter.LongComparator;\n import org.apache.hadoop.hbase.filter.PrefixFilter;\n@@ -6320,4 +6322,18 @@ public void testGetStartEndKeysWithRegionReplicas() throws IOException {\n       }\n     }\n   }\n+\n+  @Test\n+  public void testFilterAllRecords() throws IOException {\n+    Scan scan = new Scan();\n+    scan.setBatch(1);\n+    scan.setCaching(1);\n+    // Filter out any records\n+    scan.setFilter(new FilterList(new FirstKeyOnlyFilter(), new InclusiveStopFilter(new byte[0])));\n+    try (Table table = TEST_UTIL.getConnection().getTable(TableName.NAMESPACE_TABLE_NAME)) {\n+      try (ResultScanner s = table.getScanner(scan)) {\n+        assertNull(s.next());\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/8cef99e5062d889a748c8442595a0e0644e11458/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "sha": "027a3482beb573223936b9a8a7ab32e017e3eece",
                "status": "modified"
            }
        ],
        "message": "HBASE-13892 NPE in ClientScanner on null results array\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>\nAmending-Author: Andrew Purtell <apurtell@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/9d3422ed16004da1b0f9a874a98bd140b46b7a6f",
        "patched_files": [
            "ClientScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFromClientSide.java"
        ]
    },
    "hbase_8d84537": {
        "bug_id": "hbase_8d84537",
        "commit": "https://github.com/apache/hbase/commit/8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -402,6 +402,13 @@ void processDeadServersAndRegionsInTransition(\n   throws KeeperException, IOException, InterruptedException {\n     List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(watcher,\n       watcher.assignmentZNode);\n+    \n+    if (nodes == null) {\n+      String errorMessage = \"Failed to get the children from ZK\";\n+      master.abort(errorMessage, new IOException(errorMessage));\n+      return;\n+    }\n+    \n     // Run through all regions.  If they are not assigned and not in RIT, then\n     // its a clean cluster startup, else its a failover.\n     for (Map.Entry<HRegionInfo, ServerName> e: this.regions.entrySet()) {",
                "raw_url": "https://github.com/apache/hbase/raw/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "6e214c8f0927f7c2380c57cab8d06c188f522141",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/hbase/blob/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java?ref=8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
                "deletions": 3,
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "patch": "@@ -17,8 +17,10 @@\n  */\n package org.apache.hadoop.hbase.master;\n \n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotSame;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.IOException;\n import java.util.ArrayList;\n@@ -27,6 +29,7 @@\n import java.util.Map;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -39,10 +42,10 @@\n import org.apache.hadoop.hbase.client.HConnection;\n import org.apache.hadoop.hbase.client.HConnectionTestingUtility;\n import org.apache.hadoop.hbase.client.Result;\n-import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n import org.apache.hadoop.hbase.executor.ExecutorService;\n-import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorType;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n+import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n+import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorType;\n import org.apache.hadoop.hbase.master.handler.ServerShutdownHandler;\n import org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.protobuf.ClientProtocol;\n@@ -54,10 +57,12 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.util.Threads;\n+import org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper;\n import org.apache.hadoop.hbase.zookeeper.ZKAssign;\n import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.Watcher;\n import org.apache.zookeeper.KeeperException.NodeExistsException;\n import org.junit.After;\n import org.junit.AfterClass;\n@@ -477,6 +482,39 @@ public void testUnassignWithSplitAtSameTime() throws KeeperException, IOExceptio\n     }\n   }\n \n+  /**\n+   * Tests the processDeadServersAndRegionsInTransition should not fail with NPE\n+   * when it failed to get the children. Let's abort the system in this\n+   * situation\n+   * @throws ServiceException \n+   */\n+  @Test(timeout = 5000)\n+  public void testProcessDeadServersAndRegionsInTransitionShouldNotFailWithNPE()\n+      throws IOException, KeeperException, InterruptedException, ServiceException {\n+    final RecoverableZooKeeper recoverableZk = Mockito\n+        .mock(RecoverableZooKeeper.class);\n+    AssignmentManagerWithExtrasForTesting am = setUpMockedAssignmentManager(\n+        this.server, this.serverManager);\n+    Watcher zkw = new ZooKeeperWatcher(HBaseConfiguration.create(), \"unittest\",\n+        null) {\n+      public RecoverableZooKeeper getRecoverableZooKeeper() {\n+        return recoverableZk;\n+      }\n+    };\n+    ((ZooKeeperWatcher) zkw).registerListener(am);\n+    Mockito.doThrow(new InterruptedException()).when(recoverableZk)\n+        .getChildren(\"/hbase/unassigned\", zkw);\n+    am.setWatcher((ZooKeeperWatcher) zkw);\n+    try {\n+      am.processDeadServersAndRegionsInTransition();\n+      fail(\"Expected to abort\");\n+    } catch (NullPointerException e) {\n+      fail(\"Should not throw NPE\");\n+    } catch (RuntimeException e) {\n+      assertEquals(\"Aborted\", e.getLocalizedMessage());\n+    }\n+  }\n+  \n   /**\n    * Creates a new ephemeral node in the SPLITTING state for the specified region.\n    * Create it ephemeral in case regionserver dies mid-split.\n@@ -610,7 +648,12 @@ void processRegionsInTransition(final RegionTransitionData data,\n       while (this.gate.get()) Threads.sleep(1);\n       super.processRegionsInTransition(data, regionInfo, deadServers, expectedVersion);\n     }\n-\n+    \n+    /** reset the watcher */\n+    void setWatcher(ZooKeeperWatcher watcher) {\n+      this.watcher = watcher;\n+    }\n+    \n     /**\n      * @return ExecutorService used by this instance.\n      */",
                "raw_url": "https://github.com/apache/hbase/raw/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "sha": "14830ef9b076e411e1e9b252340e168b95d94eb5",
                "status": "modified"
            }
        ],
        "message": "HBASE-5733 AssignmentManager#processDeadServersAndRegionsInTransition can fail with NPE (Uma Maheswara Rao G)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1327364 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/34d88b8e234a0748df4d89f84921cb8a555c8ac5",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8f92a14": {
        "bug_id": "hbase_8f92a14",
        "commit": "https://github.com/apache/hbase/commit/8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "patch": "@@ -124,6 +124,14 @@\n    */\n   List<TableDescriptor> listTableDescriptors() throws IOException;\n \n+  /**\n+   * List all userspace tables and whether or not include system tables.\n+   *\n+   * @return a list of TableDescriptors\n+   * @throws IOException if a remote or network exception occurs\n+   */\n+  List<TableDescriptor> listTableDescriptors(boolean includeSysTables) throws IOException;\n+\n   /**\n    * List all the userspace tables that match the given pattern.\n    *",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "sha": "5419c169830538b768288340d49a76ccc95230cf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "patch": "@@ -131,6 +131,12 @@ public boolean tableExists(TableName tableName) throws IOException {\n     return get(admin.listTableDescriptors());\n   }\n \n+  @Override\n+  public List<TableDescriptor> listTableDescriptors(boolean includeSysTables)\n+      throws IOException {\n+    return get(admin.listTableDescriptors(includeSysTables));\n+  }\n+\n   @Override\n   public List<TableDescriptor> listTableDescriptors(Pattern pattern, boolean includeSysTables)\n       throws IOException {",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "sha": "f1f5b2ae2d0d3ad95a912bda745285ddd25f2520",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 2,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "patch": "@@ -493,15 +493,17 @@ public void run(PRESP resp) {\n   public CompletableFuture<List<TableDescriptor>> listTableDescriptors(Pattern pattern,\n       boolean includeSysTables) {\n     Preconditions.checkNotNull(pattern,\n-      \"pattern is null. If you don't specify a pattern, use listTables(boolean) instead\");\n+      \"pattern is null. If you don't specify a pattern, \"\n+          + \"use listTableDescriptors(boolean) instead\");\n     return getTableDescriptors(RequestConverter.buildGetTableDescriptorsRequest(pattern,\n       includeSysTables));\n   }\n \n   @Override\n   public CompletableFuture<List<TableDescriptor>> listTableDescriptors(List<TableName> tableNames) {\n     Preconditions.checkNotNull(tableNames,\n-      \"tableNames is null. If you don't specify tableNames, \" + \"use listTables(boolean) instead\");\n+      \"tableNames is null. If you don't specify tableNames, \"\n+          + \"use listTableDescriptors(boolean) instead\");\n     if (tableNames.isEmpty()) {\n       return CompletableFuture.completedFuture(Collections.emptyList());\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "sha": "8c6ec0de8a35404e6bfb57aef16e5baa11764e76",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 1,
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "patch": "@@ -437,7 +437,7 @@\n     <% if (rsGroupTables != null && rsGroupTables.size() > 0) {\n         List<TableDescriptor> tables;\n         try (Admin admin = master.getConnection().getAdmin()) {\n-            tables = master.isInitialized() ? admin.listTableDescriptors((Pattern)null, true) : null;\n+            tables = master.isInitialized() ? admin.listTableDescriptors(true) : null;\n         }\n          Map<TableName, HTableDescriptor> tableDescriptors\n             = tables.stream().collect(Collectors.toMap(TableDescriptor::getTableName, p -> new HTableDescriptor(p)));",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "sha": "86260347d17e488bcbc66ee8c121038ac4e30523",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "patch": "@@ -57,6 +57,18 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(TestAdmin.class);\n \n+  @Test\n+  public void testListTableDescriptors() throws IOException{\n+    TableDescriptor metaTableDescriptor =  TEST_UTIL.getAdmin().\n+        getDescriptor(TableName.META_TABLE_NAME);\n+    List<TableDescriptor> tableDescriptors = TEST_UTIL.getAdmin().\n+        listTableDescriptors(true);\n+    assertTrue(tableDescriptors.contains(metaTableDescriptor));\n+    tableDescriptors = TEST_UTIL.getAdmin().\n+        listTableDescriptors(false);\n+    assertFalse(tableDescriptors.contains(metaTableDescriptor));\n+  }\n+\n   @Test\n   public void testCreateTable() throws IOException {\n     List<TableDescriptor> tables = ADMIN.listTableDescriptors();",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "sha": "d1ba1a7107574449f2c7fd5c8dd9f9fea13b0edf",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "deletions": 0,
                "filename": "hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "patch": "@@ -139,6 +139,11 @@ public Connection getConnection() {\n     return listTableDescriptors((Pattern) null);\n   }\n \n+  @Override\n+  public List<TableDescriptor> listTableDescriptors(boolean includeSysTables) throws IOException {\n+    return listTableDescriptors(null, includeSysTables);\n+  }\n+\n   @Override\n   public List<TableDescriptor> listTableDescriptors(Pattern pattern) throws IOException {\n     return listTableDescriptors(pattern, false);",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "sha": "f3fdde72667aa1e37bb5e7aa2d87f5cb1877e0cf",
                "status": "modified"
            }
        ],
        "message": "HBASE-23203 NPE in RSGroup info (#747)\n\nSigned-off-by: Duo Zhang <zhangduo@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/65ee17086a04f61c9cf407596fdc7efd9690d801",
        "patched_files": [
            "Admin.java",
            "AdminOverAsyncAdmin.java",
            "RawAsyncHBaseAdmin.java",
            "rsgroup.java",
            "ThriftAdmin.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAdmin.java"
        ]
    },
    "hbase_8fd9db6": {
        "bug_id": "hbase_8fd9db6",
        "commit": "https://github.com/apache/hbase/commit/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -2298,7 +2298,7 @@ public void unassign(HRegionInfo region, boolean force, ServerName dest) {\n         // Region is not in transition.\n         // We can unassign it only if it's not SPLIT/MERGED.\n         state = regionStates.getRegionState(encodedName);\n-        if (state.isMerged() || state.isSplit()) {\n+        if (state != null && (state.isMerged() || state.isSplit())) {\n           LOG.info(\"Attempting to unassign \" + state + \", ignored\");\n           return;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "2b72fd3cbff33e1672e025db20e43bb781a29c91",
                "status": "modified"
            }
        ],
        "message": "HBASE-9554 TestOfflineMetaRebuildOverlap#testMetaRebuildOverlapFail fails due to NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523870 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/80ab9fe15c9751ddb54039e97c96d84c4e1c841c",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8fe805c": {
        "bug_id": "hbase_8fe805c",
        "commit": "https://github.com/apache/hbase/commit/8fe805ce290bfd9ceffc900509b36af4928bdf34",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -450,6 +450,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4877  TestHCM failing sporadically on jenkins and always for me on an\n                ubuntu machine\n    HBASE-4878  Master crash when splitting hlog may cause data loss (Chunhui Shen)\n+   HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "sha": "4ff8cbecae4d2615ad948e26f0c60013a7029f66",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "deletions": 11,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -2977,20 +2977,19 @@ public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths)\n               \"No such column family \" + Bytes.toStringBinary(familyName));\n           ioes.add(ioe);\n           failures.add(p);\n-        }\n-\n-        try {\n-          store.assertBulkLoadHFileOk(new Path(path));\n-        } catch (WrongRegionException wre) {\n-          // recoverable (file doesn't fit in region)\n-          failures.add(p);\n-        } catch (IOException ioe) {\n-          // unrecoverable (hdfs problem)\n-          ioes.add(ioe);\n+        } else {\n+          try {\n+            store.assertBulkLoadHFileOk(new Path(path));\n+          } catch (WrongRegionException wre) {\n+            // recoverable (file doesn't fit in region)\n+            failures.add(p);\n+          } catch (IOException ioe) {\n+            // unrecoverable (hdfs problem)\n+            ioes.add(ioe);\n+          }\n         }\n       }\n \n-\n       // validation failed, bail out before doing anything permanent.\n       if (failures.size() != 0) {\n         StringBuilder list = new StringBuilder();",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "e26e213a628728a20bd018ef98f2e9c9b1e5c21b",
                "status": "modified"
            }
        ],
        "message": "HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1210212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/f547de07c84137d865ac45518d6824f8577f0cab",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_9090855": {
        "bug_id": "hbase_9090855",
        "commit": "https://github.com/apache/hbase/commit/90908553e987654fb891f6315787639f01fc8b95",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=90908553e987654fb891f6315787639f01fc8b95",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -456,6 +456,7 @@ Release 0.21.0 - Unreleased\n                that are in offline state in meta after a split\n    HBASE-2815  not able to run the test suite in background because TestShell\n                gets suspended on tty output (Alexey Kovyrin via Stack)\n+   HBASE-2852  Bloom filter NPE (pranav via jgray)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/CHANGES.txt",
                "sha": "49b0ee641a36fa0cd150c14191b7fa2355cabaf9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java?ref=90908553e987654fb891f6315787639f01fc8b95",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "patch": "@@ -930,7 +930,7 @@ private boolean passesBloomFilter(Scan scan, final SortedSet<byte[]> columns) {\n           key = row;\n           break;\n         case ROWCOL:\n-          if (columns.size() == 1) {\n+          if (columns != null && columns.size() == 1) {\n             byte[] col = columns.first();\n             key = Bytes.add(row, col);\n             break;",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "sha": "757a50c44f95f47f569330df22ca16851f6e0a27",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=90908553e987654fb891f6315787639f01fc8b95",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "patch": "@@ -2658,7 +2658,52 @@ public void testIndexesScanWithOneDeletedRow() throws IOException {\n \n   }\n \n+  //////////////////////////////////////////////////////////////////////////////\n+  // Bloom filter test\n+  //////////////////////////////////////////////////////////////////////////////\n+\n+  public void testAllColumnsWithBloomFilter() throws IOException {\n+    byte [] TABLE = Bytes.toBytes(\"testAllColumnsWithBloomFilter\");\n+    byte [] FAMILY = Bytes.toBytes(\"family\");\n+\n+    //Create table\n+    HColumnDescriptor hcd = new HColumnDescriptor(FAMILY, Integer.MAX_VALUE,\n+        HColumnDescriptor.DEFAULT_COMPRESSION,\n+        HColumnDescriptor.DEFAULT_IN_MEMORY,\n+        HColumnDescriptor.DEFAULT_BLOCKCACHE,\n+        Integer.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL,\n+        \"rowcol\",\n+        HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n+    HTableDescriptor htd = new HTableDescriptor(TABLE);\n+    htd.addFamily(hcd);\n+    HRegionInfo info = new HRegionInfo(htd, null, null, false);\n+    Path path = new Path(DIR + \"testAllColumnsWithBloomFilter\");\n+    region = HRegion.createHRegion(info, path, conf);\n+\n+    // For row:0, col:0: insert versions 1 through 5.\n+    byte row[] = Bytes.toBytes(\"row:\" + 0);\n+    byte column[] = Bytes.toBytes(\"column:\" + 0);\n+    Put put = new Put(row);\n+    for (long idx = 1; idx <= 4; idx++) {\n+      put.add(FAMILY, column, idx, Bytes.toBytes(\"value-version-\" + idx));\n+    }\n+    region.put(put);\n+\n+    //Flush\n+    region.flushcache();\n \n+    //Get rows\n+    Get get = new Get(row);\n+    get.setMaxVersions();\n+    KeyValue[] kvs = region.get(get, null).raw();\n+\n+    //Check if rows are correct\n+    assertEquals(4, kvs.length);\n+    checkOneCell(kvs[0], FAMILY, 0, 0, 4);\n+    checkOneCell(kvs[1], FAMILY, 0, 0, 3);\n+    checkOneCell(kvs[2], FAMILY, 0, 0, 2);\n+    checkOneCell(kvs[3], FAMILY, 0, 0, 1);\n+  }\n \n   private void putData(int startRow, int numRows, byte [] qf,\n       byte [] ...families)\n@@ -2784,4 +2829,24 @@ private void initHRegion (byte [] tableName, String callingMethod,\n     Path path = new Path(DIR + callingMethod);\n     region = HRegion.createHRegion(info, path, conf);\n   }\n+\n+  /**\n+   * Assert that the passed in KeyValue has expected contents for the\n+   * specified row, column & timestamp.\n+   */\n+  private void checkOneCell(KeyValue kv, byte[] cf,\n+                             int rowIdx, int colIdx, long ts) {\n+    String ctx = \"rowIdx=\" + rowIdx + \"; colIdx=\" + colIdx + \"; ts=\" + ts;\n+    assertEquals(\"Row mismatch which checking: \" + ctx,\n+                 \"row:\"+ rowIdx, Bytes.toString(kv.getRow()));\n+    assertEquals(\"ColumnFamily mismatch while checking: \" + ctx,\n+                 Bytes.toString(cf), Bytes.toString(kv.getFamily()));\n+    assertEquals(\"Column qualifier mismatch while checking: \" + ctx,\n+                 \"column:\" + colIdx, Bytes.toString(kv.getQualifier()));\n+    assertEquals(\"Timestamp mismatch while checking: \" + ctx,\n+                 ts, kv.getTimestamp());\n+    assertEquals(\"Value mismatch while checking: \" + ctx,\n+                 \"value-version-\" + ts, Bytes.toString(kv.getValue()));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "sha": "98bd3e5d8fab4b7ea7324c4790269ab75e1130ea",
                "status": "modified"
            }
        ],
        "message": "HBASE-2852  Bloom filter NPE (pranav via jgray)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@979491 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/55e8b201e6d7320ea56a8d03399e3e548693cf2f",
        "patched_files": [
            "HRegion.java",
            "StoreFile.java",
            "CHANGES.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_9297767": {
        "bug_id": "hbase_9297767",
        "commit": "https://github.com/apache/hbase/commit/929776752e52e97d397930807b38f95c9bfc3c1b",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=929776752e52e97d397930807b38f95c9bfc3c1b",
                "deletions": 12,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -392,7 +392,7 @@ protected Connection createConnection(ConnectionId remoteId, final Codec codec,\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Use \" + authMethod + \" authentication for protocol \"\n-            + protocol.getSimpleName());\n+            + (protocol == null ? \"null\" : protocol.getSimpleName()));\n       }\n       reloginMaxBackoff = conf.getInt(\"hbase.security.relogin.maxbackoff\", 5000);\n       this.remoteId = remoteId;\n@@ -811,7 +811,7 @@ protected synchronized void setupIOstreams()\n                 ticket = ticket.getRealUser();\n               }\n             }\n-            boolean continueSasl = false;\n+            boolean continueSasl;\n             try {\n               if (ticket == null) {\n                 throw new NullPointerException(\"ticket is null\");\n@@ -855,7 +855,7 @@ public Boolean run() throws IOException {\n         }\n       } catch (Throwable t) {\n         failedServers.addToFailedServers(remoteId.address);\n-        IOException e = null;\n+        IOException e;\n         if (t instanceof IOException) {\n           e = (IOException)t;\n           markClosed(e);\n@@ -1007,14 +1007,16 @@ protected void readResponse() {\n             if (call != null) call.setException(re);\n           }\n         } else {\n-          Message rpcResponseType;\n-          try {\n-            // TODO: Why pb engine pollution in here in this class?  FIX.\n-            rpcResponseType =\n-              ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n-                reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n-          } catch (Exception e) {\n-            throw new RuntimeException(e); //local exception\n+          Message rpcResponseType = null;\n+          if (call != null){\n+            try {\n+              // TODO: Why pb engine pollution in here in this class?  FIX.\n+              rpcResponseType =\n+                ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n+                  reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n+            } catch (Exception e) {\n+              throw new RuntimeException(e); //local exception\n+            }\n           }\n           Message value = null;\n           if (rpcResponseType != null) {\n@@ -1474,4 +1476,4 @@ public int hashCode() {\n       return hashcode;\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "be60417f26db45f6e3daae1d0c3bf21389062e95",
                "status": "modified"
            }
        ],
        "message": "HBASE-8380 NPE in HBaseClient.readResponse\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1471271 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/374052f071f314b22eb37394c24407377beb5de7",
        "patched_files": [
            "HBaseClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_92e5efd": {
        "bug_id": "hbase_92e5efd",
        "commit": "https://github.com/apache/hbase/commit/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -109,6 +109,7 @@ Release 0.20.0 - Unreleased\n    HBASE-1368  HBASE-1279 broke the build\n    HBASE-1264  Wrong return values of comparators for ColumnValueFilter\n                (Thomas Schneider via Andrew Purtell)\n+   HBASE-1374  NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "sha": "7be5e34ccd72f18b79c5b63074ed051843fba487",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "patch": "@@ -76,6 +76,9 @@ public static void main(String[] args) {\n   public static Properties parseZooKeeperConfig() throws IOException {\n     ClassLoader cl = HQuorumPeer.class.getClassLoader();\n     InputStream inputStream = cl.getResourceAsStream(ZOOKEEPER_CONFIG_NAME);\n+    if (inputStream == null) {\n+      throw new IOException(ZOOKEEPER_CONFIG_NAME + \" not found\");\n+    }\n     return parseConfig(inputStream);\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "sha": "9d17b36f0b15e5bdb5a8ae49d115c6a14188a62c",
                "status": "modified"
            }
        ],
        "message": "HBASE-1374 NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@771996 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/1a2a1764adad96d79b678f047c5960b8b72f8810",
        "patched_files": [
            "HQuorumPeer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHQuorumPeer.java"
        ]
    },
    "hbase_94aa409": {
        "bug_id": "hbase_94aa409",
        "commit": "https://github.com/apache/hbase/commit/94aa409410ef21f107e67c5c7fa5931570d8c941",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/94aa409410ef21f107e67c5c7fa5931570d8c941/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java?ref=94aa409410ef21f107e67c5c7fa5931570d8c941",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "patch": "@@ -1888,7 +1888,8 @@ public void testMissingRegionInfoQualifier() throws Exception {\n \n         @Override\n         public boolean processRow(Result rowResult) throws IOException {\n-          if(!MetaScanner.getHRegionInfo(rowResult).getTable().isSystemTable()) {\n+          HRegionInfo hri = MetaScanner.getHRegionInfo(rowResult);\n+          if (hri != null && !hri.getTable().isSystemTable()) {\n             Delete delete = new Delete(rowResult.getRow());\n             delete.deleteColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n             deletes.add(delete);",
                "raw_url": "https://github.com/apache/hbase/raw/94aa409410ef21f107e67c5c7fa5931570d8c941/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "sha": "2b200dbe374c545ef01c0f2f0b8b206a479f453b",
                "status": "modified"
            }
        ],
        "message": "HBASE-9860 Intermittent TestHBaseFsck#testMissingRegionInfoQualifier failure due to NullPointerException\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1537371 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/0077c822f719ea1d90350503f3d10a274631aebd",
        "patched_files": [
            "HBaseFsck.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseFsck.java"
        ]
    },
    "hbase_94bd157": {
        "bug_id": "hbase_94bd157",
        "commit": "https://github.com/apache/hbase/commit/94bd157d214f05e820d2ea7521e685f4e00f24af",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=94bd157d214f05e820d2ea7521e685f4e00f24af",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -847,11 +847,17 @@ public Boolean run() throws IOException {\n           start();\n           return;\n         }\n-      } catch (IOException e) {\n+      } catch (Throwable t) {\n         failedServers.addToFailedServers(remoteId.address);\n-        markClosed(e);\n+        IOException e = null;\n+        if (t instanceof IOException) {\n+          e = (IOException)t;\n+          markClosed(e);\n+        } else {\n+          e = new IOException(\"Coundn't set up IO Streams\", t);\n+          markClosed(e);\n+        }\n         close();\n-\n         throw e;\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "bbb91d3ef722d0cd0733bb223e38ca6e1f753fd9",
                "status": "modified"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/hbase/blob/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java?ref=94bd157d214f05e820d2ea7521e685f4e00f24af",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "patch": "@@ -0,0 +1,100 @@\n+/**\n+  *\n+  * Licensed to the Apache Software Foundation (ASF) under one\n+  * or more contributor license agreements.  See the NOTICE file\n+  * distributed with this work for additional information\n+  * regarding copyright ownership.  The ASF licenses this file\n+  * to you under the Apache License, Version 2.0 (the\n+  * \"License\"); you may not use this file except in compliance\n+  * with the License.  You may obtain a copy of the License at\n+  *\n+  *     http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+package org.apache.hadoop.hbase.ipc;\n+\n+import java.io.IOException;\n+import java.net.Socket;\n+import java.net.InetSocketAddress;\n+import java.net.SocketTimeoutException;\n+import javax.net.SocketFactory;\n+import java.lang.reflect.Method;\n+import java.util.*;\n+\n+import static org.junit.Assert.*;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.mockito.Mockito.*;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler;\n+import org.apache.hadoop.hbase.SmallTests;\n+\n+import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RpcRequestBody;\n+import com.google.protobuf.Message;\n+import org.apache.hadoop.hbase.security.User;\n+\n+import org.apache.commons.logging.*;\n+import org.apache.log4j.Logger;\n+\n+@Category(SmallTests.class)\n+public class TestIPC {\n+  public static final Log LOG = LogFactory.getLog(TestIPC.class);\n+  private static final Random RANDOM = new Random();\n+\n+  private static class TestRpcServer extends HBaseServer {\n+    TestRpcServer() throws IOException {\n+      super(\"0.0.0.0\", 0, 1, 1, HBaseConfiguration.create(), \"TestRpcServer\", 0);\n+    }\n+\n+    @Override\n+    public Message call(Class<? extends VersionedProtocol> protocol,\n+        RpcRequestBody rpcRequest, \n+        long receiveTime, \n+        MonitoredRPCHandler status) throws IOException {\n+      return rpcRequest;\n+    }\n+  }\n+\n+  @Test\n+  public void testRTEDuringConnectionSetup() throws Exception {\n+    Configuration conf = HBaseConfiguration.create();\n+    SocketFactory spyFactory = spy(NetUtils.getDefaultSocketFactory(conf));\n+    Mockito.doAnswer(new Answer<Socket>() {\n+      @Override\n+      public Socket answer(InvocationOnMock invocation) throws Throwable {\n+        Socket s = spy((Socket)invocation.callRealMethod());\n+        doThrow(new RuntimeException(\"Injected fault\")).when(s).setSoTimeout(anyInt());\n+        return s;\n+      }\n+    }).when(spyFactory).createSocket();\n+\n+    TestRpcServer rpcServer = new TestRpcServer();\n+    rpcServer.start();\n+\n+    HBaseClient client = new HBaseClient(\n+        conf,\n+        spyFactory);\n+    InetSocketAddress address = rpcServer.getListenerAddress();\n+\n+    try {\n+      client.call(RpcRequestBody.getDefaultInstance(), address, User.getCurrent(), 0);\n+      fail(\"Expected an exception to have been thrown!\");\n+    } catch (Exception e) {\n+      LOG.info(\"Caught expected exception: \" + e.toString());\n+      assertTrue(StringUtils.stringifyException(e).contains(\"Injected fault\"));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "sha": "a1fa7b6a2e68a192d19370c9b96aaf9c0979590e",
                "status": "added"
            }
        ],
        "message": "HBASE-7450 orphan RPC connection in HBaseClient leaves \"null\" out member, causing NPE in HCM (Zavier Gao)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1429017 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/e107a640877bf0bfdefc8efaa08b1a94bb670f01",
        "patched_files": [
            "HBaseClient.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestIPC.java",
            "TestHBaseClient.java"
        ]
    },
    "hbase_96c6b98": {
        "bug_id": "hbase_96c6b98",
        "commit": "https://github.com/apache/hbase/commit/96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "patch": "@@ -601,7 +601,7 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName, final StoreF\n       }\n     }\n \n-    f.getReader().close(true);\n+    f.closeReader(true);\n \n     Path splitDir = new Path(getSplitsDir(hri), familyName);\n     // A reference to the bottom half of the hsf store file.",
                "raw_url": "https://github.com/apache/hbase/raw/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "sha": "0751634263ab838e9df321af31e671060abdaa7d",
                "status": "modified"
            }
        ],
        "message": "HBASE-12696 Possible NPE in HRegionFileSystem#splitStoreFile when skipStoreFileRangeCheck in splitPolicy return true(Rajeshbabu)",
        "parent": "https://github.com/apache/hbase/commit/110c5f593057366509b8480e396cc750d5fd782b",
        "patched_files": [
            "HRegionFileSystem.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegionFileSystem.java"
        ]
    },
    "hbase_98a6a20": {
        "bug_id": "hbase_98a6a20",
        "commit": "https://github.com/apache/hbase/commit/98a6a203c0630e63ba497211308240aebb708c67",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/98a6a203c0630e63ba497211308240aebb708c67/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=98a6a203c0630e63ba497211308240aebb708c67",
                "deletions": 10,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "patch": "@@ -256,7 +256,7 @@ public void testDeleteVersionsMixedAndMultipleVersionReturn() throws IOException\n         KeyValueTestUtil.create(\"R2\", \"cf\", \"z\", now, KeyValue.Type.Put, \"dont-care\")\n     };\n     List<KeyValueScanner> scanners = scanFixture(kvs1, kvs2);\n-    \n+\n     Scan scanSpec = new Scan(Bytes.toBytes(\"R1\")).setMaxVersions(2);\n     StoreScanner scan =\n       new StoreScanner(scanSpec, CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n@@ -378,9 +378,7 @@ public void testDeleteColumn() throws IOException {\n     };\n \n   public void testSkipColumn() throws IOException {\n-    KeyValueScanner [] scanners = new KeyValueScanner[] {\n-        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n-    };\n+    List<KeyValueScanner> scanners = scanFixture(kvs);\n     StoreScanner scan =\n       new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\", \"d\"), scanners);\n@@ -441,9 +439,7 @@ public void testWildCardTtlScan() throws IOException {\n   }\n \n   public void testScannerReseekDoesntNPE() throws Exception {\n-    KeyValueScanner [] scanners = new KeyValueScanner[] {\n-        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n-    };\n+    List<KeyValueScanner> scanners = scanFixture(kvs);\n     StoreScanner scan =\n         new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n             getCols(\"a\", \"d\"), scanners);\n@@ -457,8 +453,8 @@ public void testScannerReseekDoesntNPE() throws Exception {\n \n     scan.updateReaders();\n   }\n-    \n-  \n+\n+\n   /**\n    * TODO this fails, since we don't handle deletions, etc, in peek\n    */\n@@ -472,6 +468,6 @@ public void SKIP_testPeek() throws Exception {\n     StoreScanner scan =\n       new StoreScanner(scanSpec, CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\"), scanners);\n-    assertNull(scan.peek());    \n+    assertNull(scan.peek());\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/98a6a203c0630e63ba497211308240aebb708c67/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "sha": "695bffbe9635decbba908edb31d53147dfd70b8b",
                "status": "modified"
            }
        ],
        "message": "HBASE-2740 NPE in ReadWriteConsistencyControl (build fix)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@955800 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
        "patched_files": [
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStoreScanner.java"
        ]
    },
    "hbase_998cd16": {
        "bug_id": "hbase_998cd16",
        "commit": "https://github.com/apache/hbase/commit/998cd1642f4ae3819f18061468755bf992257a52",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java?ref=998cd1642f4ae3819f18061468755bf992257a52",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "patch": "@@ -329,6 +329,9 @@ public void clearCache(final HRegionLocation location) {\n     TableName tableName = location.getRegionInfo().getTable();\n     ConcurrentMap<byte[], RegionLocations> tableLocations = getTableLocations(tableName);\n     RegionLocations rll = tableLocations.get(location.getRegionInfo().getStartKey());\n+    if (rll == null) {\n+      return;\n+    }\n     RegionLocations updatedLocations = rll.remove(location);\n     if (updatedLocations.isEmpty()) {\n       tableLocations.remove(location.getRegionInfo().getStartKey(), rll);",
                "raw_url": "https://github.com/apache/hbase/raw/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "sha": "10a48ae28ac57acf57271d308a75467b865f302c",
                "status": "modified"
            }
        ],
        "message": "HBASE-10517 NPE in MetaCache.clearCache()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1567827 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/481a116e26faea7472328420469b6aa72ce378f1",
        "patched_files": [
            "MetaCache.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetaCache.java"
        ]
    },
    "hbase_9a4068d": {
        "bug_id": "hbase_9a4068d",
        "commit": "https://github.com/apache/hbase/commit/9a4068dcf8caec644e6703ffa365a8649bbd336e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=9a4068dcf8caec644e6703ffa365a8649bbd336e",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "patch": "@@ -388,8 +388,7 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n-\n+    istream.seek(seekPoint);\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "raw_url": "https://github.com/apache/hbase/raw/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "sha": "7eac9c6eca6be97c4d380c5fe0e1d5a03b14e92f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=9a4068dcf8caec644e6703ffa365a8649bbd336e",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n+          istream.seek(fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "raw_url": "https://github.com/apache/hbase/raw/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "sha": "fba15baa0976a0952ed4375277ac1a090b9bc0f7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/edbd0e494d7abaf50319b7650e350d52b195fcc9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=edbd0e494d7abaf50319b7650e350d52b195fcc9",
                "deletions": 43,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "patch": "@@ -1,43 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.io.hfile;\n-\n-import java.io.IOException;\n-\n-import org.apache.hadoop.fs.FSDataInputStream;\n-\n-public class HFileUtil {\n-\n-  /** guards against NullPointer\n-   * utility which tries to seek on the DFSIS and will try an alternative source\n-   * if the FSDataInputStream throws an NPE HBASE-17501\n-   * @param istream\n-   * @param offset\n-   * @throws IOException\n-   */\n-  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n-    try {\n-      // attempt to seek inside of current blockReader\n-      istream.seek(offset);\n-    } catch (NullPointerException e) {\n-      // retry the seek on an alternate copy of the data\n-      // this can occur if the blockReader on the DFSInputStream is null\n-      istream.seekToNewSource(offset);\n-    }\n-  }\n-}",
                "raw_url": "https://github.com/apache/hbase/raw/edbd0e494d7abaf50319b7650e350d52b195fcc9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "status": "removed"
            }
        ],
        "message": "Revert \"guard against NPE while reading FileTrailer and HFileBlock\"\n\nThis reverts commit 201c8382508da1266d11e04d3c7cbef42e0a256a.\n\nReverted because missing JIRA number. Fixing...",
        "parent": "https://github.com/apache/hbase/commit/edbd0e494d7abaf50319b7650e350d52b195fcc9",
        "patched_files": [
            "HFileBlock.java",
            "FixedFileTrailer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFixedFileTrailer.java",
            "TestHFileBlock.java"
        ]
    },
    "hbase_9be624f": {
        "bug_id": "hbase_9be624f",
        "commit": "https://github.com/apache/hbase/commit/9be624fc933db5f47af82652fc0d8bf6a6db6398",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -26,6 +26,7 @@ Trunk (unreleased changes)\n                always kill the region server for the META region. This makes\n                the test more deterministic and getting META reassigned was\n                problematic.\n+   HADOOP-2155 Method expecting HBaseConfiguration throws NPE when given Configuration\n \n   IMPROVEMENTS\n     HADOOP-2401 Add convenience put method that takes writable",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/CHANGES.txt",
                "sha": "1a96b8a1690877eff1cf4e668b9164268f9b9721",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseAdmin.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "patch": "@@ -20,22 +20,19 @@\n package org.apache.hadoop.hbase;\n \n import java.io.IOException;\n-import java.util.NoSuchElementException;\n import java.util.Map;\n+import java.util.NoSuchElementException;\n import java.util.SortedMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-\n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.MapWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.Writable;\n import org.apache.hadoop.ipc.RemoteException;\n \n-import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.util.Writables;\n-\n /**\n  * Provides administrative functions for HBase\n  */\n@@ -53,7 +50,7 @@\n    * @param conf Configuration object\n    * @throws MasterNotRunningException\n    */\n-  public HBaseAdmin(Configuration conf) throws MasterNotRunningException {\n+  public HBaseAdmin(HBaseConfiguration conf) throws MasterNotRunningException {\n     this.connection = HConnectionManager.getConnection(conf);\n     this.pause = conf.getLong(\"hbase.client.pause\", 30 * 1000);\n     this.numRetries = conf.getInt(\"hbase.client.retries.number\", 5);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "sha": "6496b7fb519e0bf3b856fb9e38332d375a63681d",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "patch": "@@ -28,8 +28,7 @@\n   /** constructor */\n   public HBaseConfiguration() {\n     super();\n-    addResource(\"hbase-default.xml\");\n-    addResource(\"hbase-site.xml\");\n+    addHbaseResources();\n   }\n   \n   /**\n@@ -38,5 +37,13 @@ public HBaseConfiguration() {\n    */\n   public HBaseConfiguration(final Configuration c) {\n     super(c);\n+    if (!(c instanceof HBaseConfiguration)) {\n+      addHbaseResources();\n+    }\n+  }\n+  \n+  private void addHbaseResources() {\n+    addResource(\"hbase-default.xml\");\n+    addResource(\"hbase-site.xml\");\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "sha": "d3323db461ba1f82312641d67624839d0d7c426a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 10,
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "patch": "@@ -30,15 +30,13 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.MapWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.Writable;\n-\n-import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.ipc.RPC;\n+import org.apache.hadoop.ipc.RemoteException;\n \n /**\n  * A non-instantiable class that manages connections to multiple tables in\n@@ -65,7 +63,7 @@ private HConnectionManager() {\n    * @param conf\n    * @return HConnection object for the instance specified by the configuration\n    */\n-  public static HConnection getConnection(Configuration conf) {\n+  public static HConnection getConnection(HBaseConfiguration conf) {\n     HConnection connection;\n     synchronized (HBASE_INSTANCES) {\n       String instanceName = conf.get(HBASE_DIR, DEFAULT_HBASE_DIR);\n@@ -84,7 +82,7 @@ public static HConnection getConnection(Configuration conf) {\n    * Delete connection information for the instance specified by the configuration\n    * @param conf\n    */\n-  public static void deleteConnection(Configuration conf) {\n+  public static void deleteConnection(HBaseConfiguration conf) {\n     synchronized (HBASE_INSTANCES) {\n       HBASE_INSTANCES.remove(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR));\n     }    \n@@ -106,7 +104,7 @@ public static void deleteConnection(Configuration conf) {\n     private final Integer rootRegionLock = new Integer(0);\n     private final Integer metaRegionLock = new Integer(0);\n     \n-    private volatile Configuration conf;\n+    private volatile HBaseConfiguration conf;\n \n     // Map tableName -> (Map startRow -> (HRegionInfo, HServerAddress)\n     private Map<Text, SortedMap<Text, HRegionLocation>> tablesToServers;\n@@ -125,7 +123,7 @@ public static void deleteConnection(Configuration conf) {\n      * @param conf Configuration object\n      */\n     @SuppressWarnings(\"unchecked\")\n-    public TableServers(Configuration conf) {\n+    public TableServers(HBaseConfiguration conf) {\n       this.conf = LocalHBaseCluster.doLocal(new HBaseConfiguration(conf));\n       \n       String serverClassName =",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "sha": "0300ff7dba653c03c7005d03ee0c873405dcbb5b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMaster.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "patch": "@@ -94,7 +94,7 @@ public long getProtocolVersion(String protocol,\n   volatile AtomicBoolean closed = new AtomicBoolean(true);\n   volatile boolean fsOk;\n   Path dir;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   FileSystem fs;\n   Random rand;\n   int threadWakeFrequency; \n@@ -868,7 +868,7 @@ synchronized boolean waitForMetaRegionsOrClose() {\n    * @param conf - Configuration object\n    * @throws IOException\n    */\n-  public HMaster(Configuration conf) throws IOException {\n+  public HMaster(HBaseConfiguration conf) throws IOException {\n     this(new Path(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR)),\n         new HServerAddress(conf.get(MASTER_ADDRESS, DEFAULT_MASTER_ADDRESS)),\n         conf);\n@@ -882,7 +882,7 @@ public HMaster(Configuration conf) throws IOException {\n    * \n    * @throws IOException\n    */\n-  public HMaster(Path dir, HServerAddress address, Configuration conf)\n+  public HMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n   throws IOException {\n     this.fsOk = true;\n     this.dir = dir;\n@@ -3044,7 +3044,7 @@ protected static void doMain(String [] args,\n       printUsageAndExit();\n     }\n \n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n \n     // Process command-line args. TODO: Better cmd-line processing\n     // (but hopefully something not as painful as cli options).",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMaster.java",
                "sha": "6f45ce9ee9f92f114dc78299b550b245809dc09b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMerge.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMerge.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 8,
                "filename": "src/java/org/apache/hadoop/hbase/HMerge.java",
                "patch": "@@ -28,14 +28,12 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n \n-import org.apache.hadoop.hbase.util.Writables;\n-\n /** \n  * A non-instantiable class that has a static method capable of compacting\n  * a table by merging adjacent regions that have grown too small.\n@@ -61,7 +59,7 @@ private HMerge() {\n    * @param tableName   - Table to be compacted\n    * @throws IOException\n    */\n-  public static void merge(Configuration conf, FileSystem fs, Text tableName)\n+  public static void merge(HBaseConfiguration conf, FileSystem fs, Text tableName)\n       throws IOException {\n     HConnection connection = HConnectionManager.getConnection(conf);\n     boolean masterIsRunning = connection.isMasterRunning();\n@@ -82,7 +80,7 @@ public static void merge(Configuration conf, FileSystem fs, Text tableName)\n   }\n \n   private static abstract class Merger {\n-    protected Configuration conf;\n+    protected HBaseConfiguration conf;\n     protected FileSystem fs;\n     protected Text tableName;\n     protected Path dir;\n@@ -93,7 +91,7 @@ public static void merge(Configuration conf, FileSystem fs, Text tableName)\n     protected HStoreKey key;\n     protected HRegionInfo info;\n     \n-    protected Merger(Configuration conf, FileSystem fs, Text tableName)\n+    protected Merger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n         throws IOException {\n       \n       this.conf = conf;\n@@ -200,7 +198,7 @@ protected abstract void updateMeta(Text oldRegion1, Text oldRegion2,\n     private HScannerInterface metaScanner;\n     private HRegionInfo latestRegion;\n     \n-    OnlineMerger(Configuration conf, FileSystem fs, Text tableName)\n+    OnlineMerger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n     throws IOException {\n       \n       super(conf, fs, tableName);\n@@ -315,7 +313,7 @@ protected void updateMeta(Text oldRegion1, Text oldRegion2,\n     private TreeSet<HRegionInfo> metaRegions;\n     private TreeMap<Text, byte []> results;\n     \n-    OfflineMerger(Configuration conf, FileSystem fs, Text tableName)\n+    OfflineMerger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n         throws IOException {\n       \n       super(conf, fs, tableName);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMerge.java",
                "sha": "ab83fbb9285249eda92683fc87f4ce0ffd9643bf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -111,7 +111,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n       throw new IOException(\"Cannot merge non-adjacent regions\");\n     }\n \n-    Configuration conf = a.getConf();\n+    HBaseConfiguration conf = a.getConf();\n     HTableDescriptor tabledesc = a.getTableDesc();\n     HLog log = a.getLog();\n     Path rootDir = a.getRootDir();\n@@ -194,7 +194,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   Path rootDir;\n   HLog log;\n   FileSystem fs;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   HRegionInfo regionInfo;\n   Path regiondir;\n \n@@ -242,7 +242,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n    * \n    * @throws IOException\n    */\n-  public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf, \n+  public HRegion(Path rootDir, HLog log, FileSystem fs, HBaseConfiguration conf, \n       HRegionInfo regionInfo, Path initialFiles)\n   throws IOException {\n     this.rootDir = rootDir;\n@@ -559,7 +559,7 @@ public HLog getLog() {\n   }\n \n   /** @return Configuration object */\n-  public Configuration getConf() {\n+  public HBaseConfiguration getConf() {\n     return this.conf;\n   }\n \n@@ -1834,7 +1834,8 @@ public void close() {\n    * @throws IOException\n    */\n   static HRegion createHRegion(final HRegionInfo info, final Path rootDir,\n-      final Configuration conf, final Path initialFiles) throws IOException {\n+      final HBaseConfiguration conf, final Path initialFiles)\n+  throws IOException {\n     Path regionDir = HRegion.getRegionDir(rootDir,\n         HRegionInfo.encodeRegionName(info.getRegionName()));\n     FileSystem fs = FileSystem.get(conf);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "fb9355298578af70881de68e9f95aa0e7350d928",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "patch": "@@ -85,7 +85,7 @@\n   protected volatile boolean fsOk;\n   \n   protected final HServerInfo serverInfo;\n-  protected final Configuration conf;\n+  protected final HBaseConfiguration conf;\n   private final Random rand = new Random();\n   \n   // region name -> HRegion\n@@ -373,7 +373,7 @@ private void checkForLogRoll() {\n    * @param conf\n    * @throws IOException\n    */\n-  public HRegionServer(Configuration conf) throws IOException {\n+  public HRegionServer(HBaseConfiguration conf) throws IOException {\n     this(new HServerAddress(conf.get(REGIONSERVER_ADDRESS,\n         DEFAULT_REGIONSERVER_ADDRESS)), conf);\n   }\n@@ -384,7 +384,7 @@ public HRegionServer(Configuration conf) throws IOException {\n    * @param conf\n    * @throws IOException\n    */\n-  public HRegionServer(HServerAddress address, Configuration conf)\n+  public HRegionServer(HServerAddress address, HBaseConfiguration conf)\n   throws IOException {  \n     this.abortRequested = false;\n     this.fsOk = true;",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "sha": "a1825902f7e2775d38b3e128545f4bcc997dc6ef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -37,7 +37,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n@@ -77,7 +76,7 @@\n   Text familyName;\n   SequenceFile.CompressionType compression;\n   FileSystem fs;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   Path mapdir;\n   Path loginfodir;\n   Path filterDir;\n@@ -141,7 +140,7 @@\n    */\n   HStore(Path dir, Text regionName, String encodedName,\n       HColumnDescriptor family, FileSystem fs, Path reconstructionLog,\n-      Configuration conf)\n+      HBaseConfiguration conf)\n   throws IOException {  \n     this.dir = dir;\n     this.compactionDir = new Path(dir, \"compaction.dir\");",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "c152b7b84201d57c52f25a8165130dc263404d39",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreFile.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "patch": "@@ -122,11 +122,11 @@\n   private String encodedRegionName;\n   private Text colFamily;\n   private long fileId;\n-  private final Configuration conf;\n+  private final HBaseConfiguration conf;\n   private Reference reference;\n \n   /** Shutdown constructor used by Writable */\n-  HStoreFile(Configuration conf) {\n+  HStoreFile(HBaseConfiguration conf) {\n     this(conf, new Path(Path.CUR_DIR), \"\", new Text(), 0);\n   }\n   \n@@ -138,7 +138,7 @@\n    * @param colFamily name of the column family\n    * @param fileId file identifier\n    */\n-  HStoreFile(final Configuration conf, final Path dir, \n+  HStoreFile(final HBaseConfiguration conf, final Path dir, \n       final String encodedRegionName, final Text colFamily, final long fileId) {\n     this(conf, dir, encodedRegionName, colFamily, fileId, null);\n   }\n@@ -152,7 +152,7 @@\n    * @param fileId file identifier\n    * @param ref Reference to another HStoreFile.\n    */\n-  HStoreFile(Configuration conf, Path dir, String encodedRegionName, \n+  HStoreFile(HBaseConfiguration conf, Path dir, String encodedRegionName, \n       Text colFamily, long fileId, final Reference ref) {\n     this.conf = conf;\n     this.dir = dir;\n@@ -348,7 +348,7 @@ static Path getHStoreDir(Path dir, String encodedRegionName, Text colFamily) {\n    * Checks the filesystem to determine if the file already exists. If so, it\n    * will keep generating names until it generates a name that does not exist.\n    */\n-  static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir, \n+  static HStoreFile obtainNewHStoreFile(HBaseConfiguration conf, Path dir, \n       String encodedRegionName, Text colFamily, FileSystem fs)\n       throws IOException {\n     \n@@ -378,7 +378,7 @@ static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir,\n    * @return List of store file instances loaded from passed dir.\n    * @throws IOException\n    */\n-  static List<HStoreFile> loadHStoreFiles(Configuration conf, Path dir, \n+  static List<HStoreFile> loadHStoreFiles(HBaseConfiguration conf, Path dir, \n       String encodedRegionName, Text colFamily, FileSystem fs)\n   throws IOException {\n     // Look first at info files.  If a reference, these contain info we need",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "sha": "175312be228205af7ea37bedaa4d11dbbf70c2c5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTable.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/HTable.java",
                "patch": "@@ -33,7 +33,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.filter.RowFilterInterface;\n import org.apache.hadoop.hbase.filter.StopRowFilter;\n import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n@@ -76,7 +75,7 @@ protected void checkClosed() {\n    * @param tableName name of the table\n    * @throws IOException\n    */\n-  public HTable(Configuration conf, Text tableName) throws IOException {\n+  public HTable(HBaseConfiguration conf, Text tableName) throws IOException {\n     closed = true;\n     this.connection = HConnectionManager.getConnection(conf);\n     this.tableName = tableName;",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HTable.java",
                "sha": "697b70e33d255b46f452a842e1b18dabd1469b53",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "patch": "@@ -55,14 +55,14 @@\n   private final static int DEFAULT_NO = 1;\n   public static final String LOCAL = \"local\";\n   public static final String LOCAL_COLON = LOCAL + \":\";\n-  private final Configuration conf;\n+  private final HBaseConfiguration conf;\n \n   /**\n    * Constructor.\n    * @param conf\n    * @throws IOException\n    */\n-  public LocalHBaseCluster(final Configuration conf)\n+  public LocalHBaseCluster(final HBaseConfiguration conf)\n   throws IOException {\n     this(conf, DEFAULT_NO);\n   }\n@@ -74,7 +74,8 @@ public LocalHBaseCluster(final Configuration conf)\n    * @param noRegionServers Count of regionservers to start.\n    * @throws IOException\n    */\n-  public LocalHBaseCluster(final Configuration conf, final int noRegionServers)\n+  public LocalHBaseCluster(final HBaseConfiguration conf,\n+    final int noRegionServers)\n   throws IOException {\n     super();\n     this.conf = conf;\n@@ -234,7 +235,7 @@ public void shutdown() {\n    * @return The passed <code>c</code> configuration modified if hbase.master\n    * value was 'local' otherwise, unaltered.\n    */\n-  static Configuration doLocal(final Configuration c) {\n+  static HBaseConfiguration doLocal(final HBaseConfiguration c) {\n     if (!isLocal(c)) {\n       return c;\n     }\n@@ -263,7 +264,7 @@ public static boolean isLocal(final Configuration c) {\n    * @throws IOException\n    */\n   public static void main(String[] args) throws IOException {\n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n     LocalHBaseCluster cluster = new LocalHBaseCluster(conf);\n     cluster.startup();\n     HBaseAdmin admin = new HBaseAdmin(conf);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "sha": "8a2c50f68c546b7aa810058b066ec72182f82b0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/Shell.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/Shell.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/Shell.java",
                "patch": "@@ -74,7 +74,7 @@ public static String executeTime(boolean watch, long start, long end) {\n    */\n   public static void main(@SuppressWarnings(\"unused\") String args[])\n   throws IOException {\n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n     ConsoleReader reader = new ConsoleReader();\n     reader.setBellEnabled(conf.getBoolean(\"hbaseshell.jline.bell.enabled\",\n       DEFAULT_BELL_ENABLED));",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/Shell.java",
                "sha": "ed47f9543f18d8e0415333ef8f140e523af96fca",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.hadoop.mapred.RecordReader;\n import org.apache.hadoop.mapred.Reporter;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.hbase.HScannerInterface;\n import org.apache.hadoop.hbase.HStoreKey;\n@@ -195,7 +196,7 @@ public void configure(JobConf job) {\n       m_cols[i] = new Text(colNames[i]);\n     }\n     try {\n-      m_table = new HTable(job, m_tableName);\n+      m_table = new HTable(new HBaseConfiguration(job), m_tableName);\n     } catch (Exception e) {\n       LOG.error(e);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "sha": "24d016b9a8f42dd25673f4a200567b75077c26c8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.util.Progressable;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n \n@@ -51,7 +52,9 @@\n   static final Logger LOG = Logger.getLogger(TableOutputFormat.class.getName());\n \n   /** constructor */\n-  public TableOutputFormat() {}\n+  public TableOutputFormat() {\n+    super();\n+  }\n \n   /**\n    * Convert Reduce output (key, value) to (HStoreKey, KeyedDataArrayWritable) \n@@ -71,7 +74,9 @@ public TableRecordWriter(HTable table) {\n     }\n \n     /** {@inheritDoc} */\n-    public void close(@SuppressWarnings(\"unused\") Reporter reporter) {}\n+    public void close(@SuppressWarnings(\"unused\") Reporter reporter) {\n+      // Nothing to do.\n+    }\n \n     /** {@inheritDoc} */\n     public void write(Text key, MapWritable value) throws IOException {\n@@ -99,7 +104,7 @@ public RecordWriter getRecordWriter(\n     Text tableName = new Text(job.get(OUTPUT_TABLE));\n     HTable table = null;\n     try {\n-      table = new HTable(job, tableName);\n+      table = new HTable(new HBaseConfiguration(job), tableName);\n     } catch(IOException e) {\n       LOG.error(e);\n       throw e;",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "sha": "a53f24238816e429b8dffaac1218bd3b986fdeb6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "patch": "@@ -25,8 +25,8 @@\n import java.util.Map;\n import java.util.Set;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.io.Text;\n \n@@ -45,7 +45,7 @@ public AlterCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);\n       Set<String> columns = null;",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "sha": "dcfd4385c8c3162caf8c5484eab92d5c68818cdc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "patch": "@@ -22,7 +22,7 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n /**\n  * Clears the console screen. \n@@ -32,7 +32,7 @@ public ClearCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     clear();\n     return null;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "sha": "0d580fe055a6e8b06047983b32ab09bfbe27e67c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/Command.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/Command.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/Command.java",
                "patch": "@@ -19,7 +19,7 @@\n  */\n package org.apache.hadoop.hbase.shell;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n public interface Command {\n   /** family indicator */\n@@ -31,7 +31,7 @@\n    * @param conf Configuration\n    * @return Result of command execution\n    */\n-  public ReturnMsg execute(final Configuration conf);\n+  public ReturnMsg execute(final HBaseConfiguration conf);\n \n   /**\n    * @return Type of this command whether DDL, SELECT, INSERT, UPDATE, DELETE,",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/Command.java",
                "sha": "e59631c97ac3f2ca59fed3bf6c0e403e688245a8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "patch": "@@ -24,8 +24,8 @@\n import java.util.Map;\n import java.util.Set;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n \n@@ -41,7 +41,7 @@ public CreateCommand(Writer o) {\n     super(o);\n   }\n   \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);\n       HTableDescriptor tableDesc = new HTableDescriptor(tableName);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "sha": "ef6927323721ee90a3321bf4aef4cce838332948",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "patch": "@@ -24,8 +24,8 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.io.Text;\n \n@@ -41,7 +41,7 @@ public DeleteCommand(Writer o) {\n   private String rowKey;\n   private List<String> columnList;\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (columnList == null) {\n       throw new IllegalArgumentException(\"Column list is null\");\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "sha": "bcde0a85c7fb5ef0c6ce5ad15b1b546a0c34b45d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DescCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "patch": "@@ -22,7 +22,7 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConnection;\n import org.apache.hadoop.hbase.HConnectionManager;\n@@ -49,7 +49,7 @@ public DescCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n   \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.tableName == null) \n       return new ReturnMsg(0, \"Syntax error : Please check 'Describe' syntax\");\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "sha": "7b03a9266300c66a5a7314a3e28656d7ab36735b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -36,7 +36,7 @@ public DisableCommand(Writer o) {\n     super(o);\n   }\n  \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     assert tableName != null;\n     \n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "sha": "dec1ff9bb3f4e43e1bae2344753118b31be507df",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DropCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "patch": "@@ -23,8 +23,8 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -37,7 +37,7 @@ public DropCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (tableList == null) {\n       throw new IllegalArgumentException(\"List of tables is null\");\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "sha": "87266f0f2b1ec88f46ba057a95fcbf890237ec46",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -36,7 +36,7 @@ public EnableCommand(Writer o) {\n     super(o);\n   }\n  \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     assert tableName != null;\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "sha": "b4c5356f5ff221ae9edc0d73fdf3951b10ba0599",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "patch": "@@ -21,14 +21,14 @@\n \n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n public class ExitCommand extends BasicCommand {\n   public ExitCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     // TOD: Is this the best way to exit?  Would be a problem if shell is run\n     // inside another program -- St.Ack 09/11/2007\n     System.exit(1);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "sha": "abd7d7966e67be29906b81b46d70e361035c39f7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/FsCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FsShell;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n /**\n@@ -36,7 +36,7 @@ public FsCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     // This commmand will write the \n     FsShell shell = new FsShell();\n     try {",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "sha": "a9fe413dcd64475c416afe556f7fc61006de4f71",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "patch": "@@ -52,7 +52,7 @@ public HelpCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     try {\n       printHelp(this.argument);\n     } catch (IOException e) {",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "sha": "4ee582f5d456b30808b6282d60e7cc0dd88ba773",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "patch": "@@ -23,7 +23,7 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.io.Text;\n \n@@ -40,7 +40,7 @@ public InsertCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (this.tableName == null || this.values == null || this.rowKey == null)\n       return new ReturnMsg(0, \"Syntax error : Please check 'Insert' syntax.\");\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "sha": "e352f27ad8671543b67daf443f299ed571c01f66",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/JarCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "patch": "@@ -35,6 +35,7 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.util.RunJar;\n \n /**\n@@ -48,7 +49,7 @@ public JarCommand(Writer o) {\n   }\n \n   @SuppressWarnings(\"deprecation\")\n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     \n     try {\n       String[] args = getQuery();",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "sha": "fa621871d3efff0b7efbe58b430530c22894bddb",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "patch": "@@ -19,9 +19,11 @@\n  */\n package org.apache.hadoop.hbase.shell;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+\n /**\n  * Message returned when a {@link Command} is\n- * {@link Command#execute(org.apache.hadoop.conf.Configuration)}'ed.\n+ * {@link Command#execute(HBaseConfiguration)}'ed.\n  */\n public class ReturnMsg {\n   private final String msg;",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "sha": "9465749f67a10e2f39ae28b9aafd986563427761",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "patch": "@@ -28,7 +28,6 @@\n import java.util.Map;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HConstants;\n@@ -75,7 +74,7 @@ public SelectCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.tableName.equals(\"\") || this.rowKey == null ||\n         this.columns.size() == 0) {\n       return new ReturnMsg(0, \"Syntax error : Please check 'Select' syntax.\");",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "sha": "37558b4234f3156e9761b68edbde606a56272a02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTableDescriptor;\n \n /**\n@@ -51,7 +51,7 @@ public ShowCommand(final Writer o, final TableFormatter f,\n     this.command = argument;\n   }\n \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.command == null) {\n       return new ReturnMsg(0, \"Syntax error : Please check 'Show' syntax\");\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "sha": "b4da0b85549ccba1706a653e2a834abb4d0b5011",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "patch": "@@ -51,7 +51,7 @@\n     StaticTestEnvironment.initialize();\n   }\n   \n-  protected volatile Configuration conf;\n+  protected volatile HBaseConfiguration conf;\n \n   /**\n    * constructor",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "sha": "9a263b9ca03c46daace6d7c89f1d16faab053aa9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 6,
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "patch": "@@ -23,7 +23,6 @@\n import java.io.IOException;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -37,7 +36,7 @@\n   static final Logger LOG =\n     Logger.getLogger(MiniHBaseCluster.class.getName());\n   \n-  private Configuration conf;\n+  private HBaseConfiguration conf;\n   private MiniDFSCluster cluster;\n   private FileSystem fs;\n   private boolean shutdownDFS;\n@@ -52,7 +51,7 @@\n    * @param nRegionNodes\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes)\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes)\n   throws IOException {\n     this(conf, nRegionNodes, true, true, true);\n   }\n@@ -66,7 +65,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes)\n    * @param miniHdfsFilesystem\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       final boolean miniHdfsFilesystem) throws IOException {\n     this(conf, nRegionNodes, miniHdfsFilesystem, true, true);\n   }\n@@ -89,7 +88,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n    * @param dfsCluster\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       MiniDFSCluster dfsCluster) throws IOException {\n \n     this.conf = conf;\n@@ -110,7 +109,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n    * @param deleteOnExit clean up mini hdfs files\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       final boolean miniHdfsFilesystem, boolean format, boolean deleteOnExit)\n     throws IOException {\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "sha": "272f12685e2cc00814b5d41a6151ec89cf395ab4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 2,
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "patch": "@@ -27,7 +27,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.util.Writables;\n@@ -52,7 +51,7 @@\n    * @throws IOException\n    */\n   @SuppressWarnings(\"null\")\n-  public static void makeMultiRegionTable(Configuration conf,\n+  public static void makeMultiRegionTable(HBaseConfiguration conf,\n       MiniHBaseCluster cluster, FileSystem localFs, String tableName,\n       String columnName)\n   throws IOException {  ",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "sha": "6e6f21421743c18c6f4ed01659cdae60b405b980",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/OOMEHMaster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "patch": "@@ -23,7 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n \n /**\n@@ -35,11 +34,11 @@\n public class OOMEHMaster extends HMaster {\n   private List<byte []> retainer = new ArrayList<byte[]>();\n   \n-  public OOMEHMaster(Configuration conf) throws IOException {\n+  public OOMEHMaster(HBaseConfiguration conf) throws IOException {\n     super(conf);\n   }\n \n-  public OOMEHMaster(Path dir, HServerAddress address, Configuration conf)\n+  public OOMEHMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n       throws IOException {\n     super(dir, address, conf);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "sha": "67bc573ecc45dd2092238d6b6bd1608a79bf868a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/OOMERegionServer.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "patch": "@@ -23,7 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.io.Text;\n \n@@ -37,11 +36,11 @@\n public class OOMERegionServer extends HRegionServer {\n   private List<BatchUpdate> retainer = new ArrayList<BatchUpdate>();\n \n-  public OOMERegionServer(Configuration conf) throws IOException {\n+  public OOMERegionServer(HBaseConfiguration conf) throws IOException {\n     super(conf);\n   }\n \n-  public OOMERegionServer(HServerAddress address, Configuration conf)\n+  public OOMERegionServer(HServerAddress address, HBaseConfiguration conf)\n   throws IOException {\n     super(address, conf);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "sha": "638a3beb522bec8a91d2c7b8f86608391dd2db40",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 10,
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -103,7 +103,7 @@\n       SEQUENTIAL_WRITE,\n       SCAN});\n   \n-  volatile Configuration conf;\n+  volatile HBaseConfiguration conf;\n   private boolean miniCluster = false;\n   private int N = 1;\n   private int R = ROWS_PER_GB;\n@@ -131,7 +131,7 @@\n    * Constructor\n    * @param c Configuration object\n    */\n-  public PerformanceEvaluation(final Configuration c) {\n+  public PerformanceEvaluation(final HBaseConfiguration c) {\n     this.conf = c;\n   }\n   \n@@ -163,7 +163,7 @@ public PerformanceEvaluation(final Configuration c) {\n     public void configure(JobConf j) {\n       this.cmd = j.get(CMD_KEY);\n \n-      this.pe = new PerformanceEvaluation(j);\n+      this.pe = new PerformanceEvaluation(new HBaseConfiguration(j));\n     }\n     \n     /** {@inheritDoc} */\n@@ -292,9 +292,9 @@ private Path writeInputFile(final Configuration c) throws IOException {\n     private final Status status;\n     protected HBaseAdmin admin;\n     protected HTable table;\n-    protected volatile Configuration conf;\n+    protected volatile HBaseConfiguration conf;\n     \n-    Test(final Configuration conf, final int startRow,\n+    Test(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super();\n       this.startRow = startRow;\n@@ -383,7 +383,7 @@ Text getRandomRow() {\n   }\n   \n   class RandomReadTest extends Test {\n-    RandomReadTest(final Configuration conf, final int startRow,\n+    RandomReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -406,7 +406,7 @@ String getTestName() {\n   }\n   \n   class RandomWriteTest extends Test {\n-    RandomWriteTest(final Configuration conf, final int startRow,\n+    RandomWriteTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -430,7 +430,7 @@ String getTestName() {\n     private HStoreKey key = new HStoreKey();\n     private TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n     \n-    ScanTest(final Configuration conf, final int startRow,\n+    ScanTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -464,7 +464,7 @@ String getTestName() {\n   }\n   \n   class SequentialReadTest extends Test {\n-    SequentialReadTest(final Configuration conf, final int startRow,\n+    SequentialReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -481,7 +481,7 @@ String getTestName() {\n   }\n   \n   class SequentialWriteTest extends Test {\n-    SequentialWriteTest(final Configuration conf, final int startRow,\n+    SequentialWriteTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "4abc871aed1095e3b1231ea2715efd9cd6a93407",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanner.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 4,
                "filename": "src/test/org/apache/hadoop/hbase/TestScanner.java",
                "patch": "@@ -24,13 +24,11 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.io.Text;\n-\n import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * Test of a long-lived scanner validating as we go.\n@@ -135,7 +133,7 @@ public void testScanner() throws IOException {\n       \n       // Initialization\n       \n-      Configuration conf = new HBaseConfiguration();\n+      HBaseConfiguration conf = new HBaseConfiguration();\n       cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n       fs = cluster.getFileSystem();\n       Path dir = new Path(\"/hbase\");",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "sha": "0fce6de5bcf761fe3c9dbda076fdfbe8fcf0024f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "patch": "@@ -30,11 +30,11 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestCase;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n@@ -211,7 +211,8 @@ private String createIndexConfContent() {\n     return c.toString();\n   }\n \n-  private void scanTable(Configuration c, long firstK) throws IOException {\n+  private void scanTable(HBaseConfiguration c, long firstK)\n+  throws IOException {\n     HTable table = new HTable(c, new Text(TABLE_NAME));\n     Text[] columns = { TEXT_INPUT_COLUMN, TEXT_OUTPUT_COLUMN };\n     HScannerInterface scanner = table.obtainScanner(columns,\n@@ -235,7 +236,7 @@ private void scanTable(Configuration c, long firstK) throws IOException {\n     }\n   }\n \n-  private void verify(Configuration c) throws IOException {\n+  private void verify(HBaseConfiguration c) throws IOException {\n     Path localDir = new Path(this.testDir, \"index_\" +\n       Integer.toString(new Random().nextInt()));\n     this.fs.copyToLocalFile(new Path(INDEX_DIR), localDir);",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "sha": "b3e8a8ea3c7b13229f19440c70d3825d88fc707d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "deletions": 13,
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "patch": "@@ -26,17 +26,11 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.io.MapWritable;\n-import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.mapred.JobClient;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HScannerInterface;\n@@ -46,10 +40,12 @@\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.mapred.TableMap;\n-import org.apache.hadoop.hbase.mapred.TableOutputCollector;\n-import org.apache.hadoop.hbase.mapred.TableReduce;\n-import org.apache.hadoop.hbase.mapred.IdentityTableReduce;\n+import org.apache.hadoop.io.MapWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.MiniMRCluster;\n+import org.apache.hadoop.mapred.Reporter;\n \n /**\n  * Test Map/Reduce job over HBase tables\n@@ -314,7 +310,7 @@ private void localTestMultiRegionTable() throws IOException {\n     verify(conf, MULTI_REGION_TABLE_NAME);\n   }\n \n-  private void scanTable(Configuration conf, String tableName)\n+  private void scanTable(HBaseConfiguration conf, String tableName)\n   throws IOException {\n     HTable table = new HTable(conf, new Text(tableName));\n     \n@@ -344,7 +340,8 @@ private void scanTable(Configuration conf, String tableName)\n   }\n \n   @SuppressWarnings(\"null\")\n-  private void verify(Configuration conf, String tableName) throws IOException {\n+  private void verify(HBaseConfiguration conf, String tableName)\n+  throws IOException {\n     HTable table = new HTable(conf, new Text(tableName));\n     \n     Text[] columns = {",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "sha": "448d4262796312a4ca8996d1f02085ce833ccce5",
                "status": "modified"
            }
        ],
        "message": "HADOOP-2155 Method expecting HBaseConfiguration throw NPE when given Configuration\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@592549 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cda8c597fc7c51050a154d42b38d23dffdfb2e1e",
        "patched_files": [
            "FsCommand.java",
            "DisableCommand.java",
            "ExitCommand.java",
            "MiniHBaseCluster.java",
            "MultiRegionTable.java",
            "HRegion.java",
            "HBaseAdmin.java",
            "TableInputFormat.java",
            "JarCommand.java",
            "HConnectionManager.java",
            "ShowCommand.java",
            "InsertCommand.java",
            "SelectCommand.java",
            "HRegionServer.java",
            "DropCommand.java",
            "Command.java",
            "LocalHBaseCluster.java",
            "DeleteCommand.java",
            "HStoreFile.java",
            "HBaseConfiguration.java",
            "HMaster.java",
            "ReturnMsg.java",
            "HStore.java",
            "Shell.java",
            "OOMEHMaster.java",
            "EnableCommand.java",
            "HBaseTestCase.java",
            "CHANGES.java",
            "HMerge.java",
            "CreateCommand.java",
            "HelpCommand.java",
            "DescCommand.java",
            "PerformanceEvaluation.java",
            "HTable.java",
            "ClearCommand.java",
            "AlterCommand.java",
            "TableOutputFormat.java",
            "OOMERegionServer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableMapReduce.java",
            "TestLocalHBaseCluster.java",
            "TestTableIndex.java",
            "TestHBaseConfiguration.java",
            "TestShell.java",
            "TestHStoreFile.java",
            "TestHRegion.java",
            "TestPerformanceEvaluation.java",
            "TestScanner.java",
            "TestHStore.java",
            "TestTableInputFormat.java"
        ]
    },
    "hbase_9cfe7cf": {
        "bug_id": "hbase_9cfe7cf",
        "commit": "https://github.com/apache/hbase/commit/9cfe7cfa3c58e189256a83d764067612832b3daf",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -82,6 +82,9 @@\n   /** Cluster is fully-distributed */\n   public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n \n+  /** Default value for cluster distributed mode */  \n+  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+\n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "07041b5dccbb4cfc370f2e8c6ab614dfc065890e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "patch": "@@ -163,7 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED).equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n+        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n+              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n             && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "sha": "b6fef32ed5e359d9b81acd326988a2cb1d54c203",
                "status": "modified"
            }
        ],
        "message": "HBASE-5633 NPE reading ZK config in HBase\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1304924 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/accf8ee8621010ce76169a99ce0daee76469c0af",
        "patched_files": [
            "ZKConfig.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestZKConfig.java"
        ]
    },
    "hbase_9e628b7": {
        "bug_id": "hbase_9e628b7",
        "commit": "https://github.com/apache/hbase/commit/9e628b715d68c180a73f89b82ef8203fd66ef39d",
        "file": [
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hbase/blob/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=9e628b715d68c180a73f89b82ef8203fd66ef39d",
                "deletions": 11,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "patch": "@@ -2177,24 +2177,37 @@ public long getHFilesSize() {\n   }\n \n   private long getTotalUmcompressedBytes(List<HStoreFile> files) {\n-    return files.stream().filter(f -> f != null && f.getReader() != null)\n-        .mapToLong(f -> f.getReader().getTotalUncompressedBytes()).sum();\n+    return files.stream().filter(f -> f != null).mapToLong(f -> {\n+      StoreFileReader reader = f.getReader();\n+      if (reader == null) {\n+        return 0;\n+      } else {\n+        return reader.getTotalUncompressedBytes();\n+      }\n+    }).sum();\n   }\n \n   private long getStorefilesSize(Collection<HStoreFile> files, Predicate<HStoreFile> predicate) {\n     return files.stream().filter(f -> f != null && f.getReader() != null).filter(predicate)\n-        .mapToLong(f -> f.getReader().length()).sum();\n+        .mapToLong(f -> {\n+          StoreFileReader reader = f.getReader();\n+          if (reader == null) {\n+            return 0;\n+          } else {\n+            return reader.length();\n+          }\n+        }).sum();\n   }\n \n   private long getStoreFileFieldSize(ToLongFunction<StoreFileReader> f) {\n-    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n-      if (sf.getReader() == null) {\n-        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n-        return false;\n-      } else {\n-        return true;\n-      }\n-    }).map(HStoreFile::getReader).mapToLong(f).sum();\n+    return this.storeEngine.getStoreFileManager().getStorefiles().stream()\n+        .map(HStoreFile::getReader).filter(reader -> {\n+          if (reader == null) {\n+            return false;\n+          } else {\n+            return true;\n+          }\n+        }).mapToLong(f).sum();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "sha": "10bb030babe3c437ce8bc4474acd410afb78e865",
                "status": "modified"
            }
        ],
        "message": "HBASE-23159 HStore#getStorefilesSize may throw NPE",
        "parent": "https://github.com/apache/hbase/commit/73d69c6157515c28479fc78f224c8065d0c00abc",
        "patched_files": [
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_9eb369c": {
        "bug_id": "hbase_9eb369c",
        "commit": "https://github.com/apache/hbase/commit/9eb369c26699fc265c2a0e780318513bd01b85a5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -45,3 +45,4 @@ Trunk (unreleased changes)\n  26. HADOOP-1543 [hbase] Add HClient.tableExists\n  27. HADOOP-1519 [hbase] map/reduce interface for HBase\n  28. HADOOP-1523 Hung region server waiting on write locks \n+ 29. HADOOP-1560 NPE in MiniHBaseCluster on Windows",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/CHANGES.txt",
                "sha": "b1fc395ae1593ac704bdd754a011ef47ad806542",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "patch": "@@ -32,7 +32,7 @@\n   protected HTableDescriptor desc;\n   protected ImmutableBytesWritable value;\n \n-  protected MiniDFSCluster dfsCluster;\n+  protected MiniDFSCluster dfsCluster = null;\n   protected FileSystem fs;\n   protected Path dir;\n \n@@ -104,6 +104,9 @@ public void setUp() throws Exception {\n       \n     } catch(Throwable t) {\n       t.printStackTrace();\n+      if(dfsCluster != null) {\n+        dfsCluster.shutdown();\n+      }\n       fail();\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "sha": "8664c53314018736217cede68ac2b7dee71a9c89",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 9,
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "patch": "@@ -37,8 +37,8 @@\n   private MiniDFSCluster cluster;\n   private FileSystem fs;\n   private Path parentdir;\n-  private HMaster master;\n-  private Thread masterThread;\n+  private HMaster master = null;\n+  private Thread masterThread = null;\n   List<HRegionServer> regionServers;\n   List<Thread> regionThreads;\n   private boolean deleteOnExit = true;\n@@ -83,6 +83,8 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n \n     this.conf = conf;\n     this.cluster = dfsCluster;\n+    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n+    this.regionThreads = new ArrayList<Thread>(nRegionNodes);\n     init(nRegionNodes);\n   }\n \n@@ -102,6 +104,8 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n   throws IOException {\n     this.conf = conf;\n     this.deleteOnExit = deleteOnExit;\n+    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n+    this.regionThreads = new ArrayList<Thread>(nRegionNodes);\n \n     if (miniHdfsFilesystem) {\n       try {\n@@ -167,8 +171,6 @@ public MiniDFSCluster getDFSCluster() {\n \n   private void startRegionServers(final int nRegionNodes)\n   throws IOException {\n-    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n-    this.regionThreads = new ArrayList<Thread>(nRegionNodes);    \n     for(int i = 0; i < nRegionNodes; i++) {\n       startRegionServer();\n     }\n@@ -239,7 +241,9 @@ public void shutdown() {\n     for(HRegionServer hsr: this.regionServers) {\n       hsr.stop();\n     }\n-    master.shutdown();\n+    if(master != null) {\n+      master.shutdown();\n+    }\n     for(Thread t: this.regionThreads) {\n       if (t.isAlive()) {\n         try {\n@@ -249,11 +253,13 @@ public void shutdown() {\n         }\n       }\n     }\n-    try {\n-      masterThread.join();\n+    if (masterThread != null) {\n+      try {\n+        masterThread.join();\n \n-    } catch(InterruptedException e) {\n-      // continue\n+      } catch(InterruptedException e) {\n+        // continue\n+      }\n     }\n     LOG.info(\"HBase Cluster shutdown complete\");\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "sha": "183db5dd0f00945c5e79a49f8eea1177346f2961",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHRegion.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "patch": "@@ -61,6 +61,9 @@ public void testHRegion() {\n       cleanup();\n       \n     } catch(Exception e) {\n+      if(cluster != null) {\n+        cluster.shutdown();\n+      }\n       e.printStackTrace();\n       fail();\n     }\n@@ -798,6 +801,7 @@ private void cleanup() {\n     // Shut down the mini cluster\n \n     cluster.shutdown();\n+    cluster = null;\n \n     // Delete all the DFS files\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "sha": "f370bb7fbd2a3deb59997f8cbc39a5facaa9e9bb",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanner.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 3,
                "filename": "src/test/org/apache/hadoop/hbase/TestScanner.java",
                "patch": "@@ -260,9 +260,6 @@ public void testScanner() throws IOException {\n       throw e;\n       \n     } finally {\n-      if(fs != null) {\n-        fs.close();\n-      }\n       if(cluster != null) {\n         cluster.shutdown();\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "sha": "48adb45db23f6493c145bbf14266e58138d7901e",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "deletions": 26,
                "filename": "src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "patch": "@@ -75,39 +75,46 @@ public void setUp() throws Exception {\n     desc.addFamily(new HColumnDescriptor(OUTPUT_COLUMN));\n     \n     dfsCluster = new MiniDFSCluster(conf, 1, true, (String[])null);\n-    fs = dfsCluster.getFileSystem();\n-    dir = new Path(\"/hbase\");\n-    fs.mkdirs(dir);\n+    try {\n+      fs = dfsCluster.getFileSystem();\n+      dir = new Path(\"/hbase\");\n+      fs.mkdirs(dir);\n \n-    // create the root and meta regions and insert the data region into the meta\n+      // create the root and meta regions and insert the data region into the meta\n \n-    HRegion root = createNewHRegion(fs, dir, conf, HGlobals.rootTableDesc, 0L, null, null);\n-    HRegion meta = createNewHRegion(fs, dir, conf, HGlobals.metaTableDesc, 1L, null, null);\n-    HRegion.addRegionToMETA(root, meta);\n+      HRegion root = createNewHRegion(fs, dir, conf, HGlobals.rootTableDesc, 0L, null, null);\n+      HRegion meta = createNewHRegion(fs, dir, conf, HGlobals.metaTableDesc, 1L, null, null);\n+      HRegion.addRegionToMETA(root, meta);\n \n-    HRegion region = createNewHRegion(fs, dir, conf, desc, rand.nextLong(), null, null);\n-    HRegion.addRegionToMETA(meta, region);\n+      HRegion region = createNewHRegion(fs, dir, conf, desc, rand.nextLong(), null, null);\n+      HRegion.addRegionToMETA(meta, region);\n \n-    // insert some data into the test table\n+      // insert some data into the test table\n \n-    for(int i = 0; i < values.length; i++) {\n-      long lockid = region.startUpdate(new Text(\"row_\"\n-          + String.format(\"%1$05d\", i)));\n+      for(int i = 0; i < values.length; i++) {\n+        long lockid = region.startUpdate(new Text(\"row_\"\n+            + String.format(\"%1$05d\", i)));\n \n-      region.put(lockid, TEXT_INPUT_COLUMN, values[i]);\n-      region.commit(lockid);\n-    }\n+        region.put(lockid, TEXT_INPUT_COLUMN, values[i]);\n+        region.commit(lockid);\n+      }\n \n-    region.close();\n-    region.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n-    root.close();\n-    root.getLog().closeAndDelete();\n-  \n-    // Start up HBase cluster\n-    \n-    hCluster = new MiniHBaseCluster(conf, 1, dfsCluster);\n+      region.close();\n+      region.getLog().closeAndDelete();\n+      meta.close();\n+      meta.getLog().closeAndDelete();\n+      root.close();\n+      root.getLog().closeAndDelete();\n+\n+      // Start up HBase cluster\n+\n+      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster);\n+      \n+    } catch (Exception e) {\n+      if (dfsCluster != null) {\n+        dfsCluster.shutdown();\n+      }\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "sha": "f6b51b54316e7c19c35247e6e86ba98443d70000",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1560 NPE in MiniHBaseCluster on Windows\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@553080 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/655728f3bf87f21104a6f02f4564901d79cf6bf2",
        "patched_files": [
            "HRegion.java",
            "MiniHBaseCluster.java",
            "AbstractMergeTestBase.java",
            "CHANGES.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestTableMapReduce.java",
            "TestScanner.java"
        ]
    },
    "hbase_a195d0a": {
        "bug_id": "hbase_a195d0a",
        "commit": "https://github.com/apache/hbase/commit/a195d0af7eeb54d12588aa3b51baa053a830c70e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -75,6 +75,7 @@ Release 0.91.0 - Unreleased\n    HBASE-3598  Broken formatting in LRU stats output (Erik Onnen)\n    HBASE-3758  Delete triggers pre/postScannerOpen upcalls of RegionObserver\n                (Mingjie Lai via garyh)\n+   HBASE-3790  Fix NPE in ExecResult.write() with null return value\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/CHANGES.txt",
                "sha": "714f4b075c24d031edfc6ac456132c66600dde3e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "patch": "@@ -72,7 +72,7 @@ public Object getValue() {\n   public void write(DataOutput out) throws IOException {\n     Bytes.writeByteArray(out, regionName);\n     HbaseObjectWritable.writeObject(out, value,\n-        value.getClass(), null);\n+        value != null ? value.getClass() : valueType, null);\n     Class<?> alternativeSerializationClass;\n     if(value instanceof Writable){\n       alternativeSerializationClass = Writable.class;",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "sha": "eb107ed0a77bb0eb12c87e173c784f28278398ff",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "deletions": 7,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "patch": "@@ -1,5 +1,5 @@\n /*\n- * Copyright 2010 The Apache Software Foundation\n+ * Copyright 2011 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -19,10 +19,7 @@\n  */\n package org.apache.hadoop.hbase.regionserver;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n \n import java.io.IOException;\n import java.util.List;\n@@ -83,6 +80,8 @@ public int incrementCount(int diff) {\n     public String hello(String name) {\n       if (name == null) {\n         return \"Who are you?\";\n+      } else if (\"nobody\".equals(name)) {\n+        return null;\n       }\n       return \"Hello, \"+name;\n     }\n@@ -154,6 +153,8 @@ public void testSingleProxy() throws Exception {\n     assertEquals(\"Invalid custom protocol response\", \"Hello, George\", result);\n     result = pinger.hello(null);\n     assertEquals(\"Should handle NULL parameter\", \"Who are you?\", result);\n+    result = pinger.hello(\"nobody\");\n+    assertNull(result);\n     int cnt = pinger.getPingCount();\n     assertTrue(\"Count should be incremented\", cnt > 0);\n     int newcnt = pinger.incrementCount(5);\n@@ -297,6 +298,23 @@ public String call(PingProtocol instance) {\n     verifyRegionResults(table, results, \"Who are you?\", ROW_C);\n   }\n \n+  @Test\n+  public void testNullReturn() throws Throwable {\n+    HTable table = new HTable(util.getConfiguration(), TEST_TABLE);\n+\n+    Map<byte[],String> results = table.coprocessorExec(PingProtocol.class,\n+        ROW_A, ROW_C,\n+        new Batch.Call<PingProtocol,String>(){\n+          public String call(PingProtocol instance) {\n+            return instance.hello(\"nobody\");\n+          }\n+        });\n+\n+    verifyRegionResults(table, results, null, ROW_A);\n+    verifyRegionResults(table, results, null, ROW_B);\n+    verifyRegionResults(table, results, null, ROW_C);\n+  }\n+\n   private void verifyRegionResults(HTable table,\n       Map<byte[],String> results, byte[] row) throws Exception {\n     verifyRegionResults(table, results, \"pong\", row);\n@@ -307,9 +325,9 @@ private void verifyRegionResults(HTable table,\n   throws Exception {\n     HRegionLocation loc = table.getRegionLocation(row);\n     byte[] region = loc.getRegionInfo().getRegionName();\n-    assertNotNull(\"Results should contain region \" +\n+    assertTrue(\"Results should contain region \" +\n         Bytes.toStringBinary(region)+\" for row '\"+Bytes.toStringBinary(row)+\"'\",\n-        results.get(region));\n+        results.containsKey(region));\n     assertEquals(\"Invalid result for row '\"+Bytes.toStringBinary(row)+\"'\",\n         expected, results.get(region));\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "sha": "f522bdb229ef1f729de5a11f3a2dafae27575191",
                "status": "modified"
            }
        ],
        "message": "HBASE-3790  Fix NPE in ExecResult.write() with null return value\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1092854 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cac85a9833bd640ff63fa0b9dcb322fe20e00313",
        "patched_files": [
            "CHANGES.java",
            "ExecResult.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestServerCustomProtocol.java"
        ]
    },
    "hbase_a2af3a7": {
        "bug_id": "hbase_a2af3a7",
        "commit": "https://github.com/apache/hbase/commit/a2af3a76a22bf4e8e07396606927b58c6da7c771",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=a2af3a76a22bf4e8e07396606927b58c6da7c771",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -105,10 +105,15 @@ protected Cluster(Map<ServerName, List<HRegionInfo>> clusterState,  Map<String,\n       regionLocations = new int[numRegions][];\n \n       int tableIndex = 0, serverIndex = 0, regionIndex = 0, regionPerServerIndex = 0;\n+      // populate serversToIndex first\n       for (Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n         servers[serverIndex] = entry.getKey();\n         regionsPerServer[serverIndex] = new int[entry.getValue().size()];\n         serversToIndex.put(servers[serverIndex], Integer.valueOf(serverIndex));\n+        serverIndex++;\n+      }\n+      serverIndex = 0;\n+      for (Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n         regionPerServerIndex = 0;\n         for (HRegionInfo region : entry.getValue()) {\n           byte[] tableName = region.getTableName();\n@@ -142,7 +147,9 @@ protected Cluster(Map<ServerName, List<HRegionInfo>> clusterState,  Map<String,\n             List<ServerName> loc = regionFinder.getTopBlockLocations(region);\n             regionLocations[regionIndex] = new int[loc.size()];\n             for (int i=0; i < loc.size(); i++) {\n-              regionLocations[regionIndex][i] = serversToIndex.get(loc.get(i));\n+              regionLocations[regionIndex][i] =\n+                  loc.get(i) == null ? -1 :\n+                    (serversToIndex.get(loc.get(i)) == null ? -1 : serversToIndex.get(loc.get(i)));\n             }\n           }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "94149a5dbb5d128786b44e277bc3a6666e161003",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java?ref=a2af3a76a22bf4e8e07396606927b58c6da7c771",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "patch": "@@ -505,7 +505,7 @@ protected double computeCost(Cluster cluster) {\n \n       int index = -1;\n       for (int j = 0; j < regionLocations.length; j++) {\n-        if (regionLocations[j] == serverIndex) {\n+        if (regionLocations[j] >= 0 && regionLocations[j] == serverIndex) {\n           index = j;\n           break;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "sha": "330e9b18ad7720ed0f9a9b4190c2a359ee067c3b",
                "status": "modified"
            }
        ],
        "message": "HBASE-8374 NullPointerException when launching the balancer due to unknown region location (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1469954 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/e946c2871f1c1dea9b767845f1c529682c6ecd5b",
        "patched_files": [
            "BaseLoadBalancer.java",
            "StochasticLoadBalancer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStochasticLoadBalancer.java",
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_a4a4458": {
        "bug_id": "hbase_a4a4458",
        "commit": "https://github.com/apache/hbase/commit/a4a44587b3f2236750569f5c032b283dc77942f6",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=a4a44587b3f2236750569f5c032b283dc77942f6",
                "deletions": 8,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "status": "modified"
            }
        ],
        "message": "Revert \"Revert \"HBASE-14909 NPE testing for RIT\"\"\nReverted the wrong patch... putting it back (revert of a revert)\n\nThis reverts commit 157a60f1b396ab9adc7f934a15352f2dbc5493a9.",
        "parent": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "patched_files": [
            "HBaseTestingUtility.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_a95eb65": {
        "bug_id": "hbase_a95eb65",
        "commit": "https://github.com/apache/hbase/commit/a95eb6559d69781c20e05b88b5b9ba773328c691",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=a95eb6559d69781c20e05b88b5b9ba773328c691",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "patch": "@@ -151,21 +151,23 @@ private boolean requiresReplication(Optional<TableDescriptor> tableDesc, Entry e\n   private void getRegionLocations(CompletableFuture<RegionLocations> future,\n       TableDescriptor tableDesc, byte[] encodedRegionName, byte[] row, boolean reload) {\n     FutureUtils.addListener(connection.getRegionLocations(tableDesc.getTableName(), row, reload),\n-      (r, e) -> {\n+      (locs, e) -> {\n         if (e != null) {\n           future.completeExceptionally(e);\n           return;\n         }\n         // if we are not loading from cache, just return\n         if (reload) {\n-          future.complete(r);\n+          future.complete(locs);\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches\n-        if (r.size() == tableDesc.getRegionReplication() && Bytes.equals(\n-          r.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(), encodedRegionName)) {\n-          future.complete(r);\n+        // matches, and also there is no null elements in the returned RegionLocations\n+        if (locs.size() == tableDesc.getRegionReplication() &&\n+          locs.size() == locs.numNonNullElements() &&\n+          Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n+            encodedRegionName)) {\n+          future.complete(locs);\n         } else {\n           // reload again as the information in cache maybe stale\n           getRegionLocations(future, tableDesc, encodedRegionName, row, true);",
                "raw_url": "https://github.com/apache/hbase/raw/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "sha": "cc2650f803f2aa165c353c752d2ff15b14ad4960",
                "status": "modified"
            }
        ],
        "message": "HBASE-22328 NPE in RegionReplicaReplicationEndpoint",
        "parent": "https://github.com/apache/hbase/commit/6855d5837955c31f4774d42694d64a6d921bc1f5",
        "patched_files": [
            "RegionReplicaReplicationEndpoint.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRegionReplicaReplicationEndpoint.java"
        ]
    },
    "hbase_a97739b": {
        "bug_id": "hbase_a97739b",
        "commit": "https://github.com/apache/hbase/commit/a97739bd3fe6654c26057630ea5055e97a6eb487",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java?ref=a97739bd3fe6654c26057630ea5055e97a6eb487",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "patch": "@@ -63,7 +63,11 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n     List<ServerName> servers = null;\n     Set<ServerName> deadServers = null;\n     \n-    if(master.isActiveMaster()){\n+    if(master.isActiveMaster()) {\n+      if (master.getServerManager() == null) {\n+        response.sendError(503, \"Master not ready\");\n+        return;\n+      }\n       metaLocation = getMetaLocationOrNull(master);\n       //ServerName metaLocation = master.getCatalogTracker().getMetaLocation();\n       servers = master.getServerManager().getOnlineServersList();",
                "raw_url": "https://github.com/apache/hbase/raw/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "sha": "49ab9d6bd80d43653032b346ec2f383e4c5c115a",
                "status": "modified"
            }
        ],
        "message": "HBASE-8975  NPE/HTTP 500 when opening the master's web UI too early\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1504955 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/581c3f2634e82343ed0d27a4a0ce5d407f445567",
        "patched_files": [
            "MasterStatusServlet.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMasterStatusServlet.java"
        ]
    },
    "hbase_aba565a": {
        "bug_id": "hbase_aba565a",
        "commit": "https://github.com/apache/hbase/commit/aba565a22873439f7ee8f6d53091b3ce709fc96c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "patch": "@@ -58,9 +58,7 @@\n   /**\n    * Constructor\n    */\n-  public HMemcache() {\n-    super();\n-  }\n+  public HMemcache() {}\n \n   /** represents the state of the memcache at a specified point in time */\n   static class Snapshot {\n@@ -320,7 +318,7 @@ HInternalScannerInterface getScanner(long timestamp,\n         // Generate list of iterators\n         HStoreKey firstKey = new HStoreKey(firstRow);\n         for(int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = (firstRow.getLength() != 0)?\n+          keyIterators[i] = (/*firstRow != null &&*/ firstRow.getLength() != 0)?\n             backingMaps[i].tailMap(firstKey).keySet().iterator():\n             backingMaps[i].keySet().iterator();\n           while(getNext(i)) {",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "sha": "163c015136543d62b576f34adbde4ad701262830",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "deletions": 12,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -208,6 +208,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n \n   final int memcacheFlushSize;\n   final int blockingMemcacheSize;\n+  protected final long threadWakeFrequency;\n   private final HLocking lock = new HLocking();\n   private long desiredMaxFileSize;\n   private final long maxSequenceId;\n@@ -244,6 +245,7 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     this.conf = conf;\n     this.regionInfo = regionInfo;\n     this.memcache = new HMemcache();\n+    this.threadWakeFrequency = conf.getLong(THREAD_WAKE_FREQUENCY, 10 * 1000);\n \n     // Declare the regionName.  This is a unique string for the region, used to \n     // build a unique filename.\n@@ -1055,24 +1057,28 @@ public long startUpdate(Text row) throws IOException {\n    * the notify.\n    */\n   private synchronized void checkResources() {\n-    if (checkCommitsSinceFlush()) {\n-      return;\n-    }\n+    boolean blocked = false;\n     \n-    LOG.warn(\"Blocking updates for '\" + Thread.currentThread().getName() +\n-      \"': Memcache size \" +\n-      StringUtils.humanReadableInt(this.memcache.getSize()) +\n-      \" is >= than blocking \" +\n-      StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n     while (!checkCommitsSinceFlush()) {\n+      if (!blocked) {\n+        LOG.info(\"Blocking updates for '\" + Thread.currentThread().getName() +\n+            \"': Memcache size \" +\n+            StringUtils.humanReadableInt(this.memcache.getSize()) +\n+            \" is >= than blocking \" +\n+            StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n+      }\n+\n+      blocked = true;\n       try {\n-        wait();\n+        wait(threadWakeFrequency);\n       } catch (InterruptedException e) {\n         // continue;\n       }\n     }\n-    LOG.warn(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n-      \"'\");\n+    if (blocked) {\n+      LOG.info(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n+          \"'\");\n+    }\n   }\n   \n   /*\n@@ -1635,4 +1641,4 @@ static boolean deleteRegion(FileSystem fs, Path baseDirectory,\n   public static Path getRegionDir(final Path dir, final Text regionName) {\n     return new Path(dir, new Path(HREGIONDIR_PREFIX + regionName));\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "f7e4b55bc4cf3ae43ec07a5f904d8de64c0574be",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1797 Fix NPEs in MetaScanner constructor\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@571333 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/12a62a6333dbe317af987a303e4e3c9405608ee3",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_abaeeac": {
        "bug_id": "hbase_abaeeac",
        "commit": "https://github.com/apache/hbase/commit/abaeeace004caa73a585ba1120e21d162f4556fb",
        "file": [
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "deletions": 40,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "patch": "@@ -75,50 +75,40 @@\n     @Override\n     public void preGetOp(ObserverContext<RegionCoprocessorEnvironment> e, Get get,\n         List<Cell> results) throws IOException {\n-      if (!active || !isMetaTableOp(e)) {\n-        return;\n-      }\n-      tableMetricRegisterAndMark(e, get);\n-      clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, get);\n-      opMetricRegisterAndMark(e, get);\n-      opWithClientMetricRegisterAndMark(e, get);\n+      registerAndMarkMetrics(e, get);\n     }\n \n     @Override\n     public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit,\n         Durability durability) throws IOException {\n-      if (!active || !isMetaTableOp(e)) {\n-        return;\n-      }\n-      tableMetricRegisterAndMark(e, put);\n-      clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, put);\n-      opMetricRegisterAndMark(e, put);\n-      opWithClientMetricRegisterAndMark(e, put);\n+      registerAndMarkMetrics(e, put);\n     }\n \n     @Override\n     public void preDelete(ObserverContext<RegionCoprocessorEnvironment> e, Delete delete,\n         WALEdit edit, Durability durability) throws IOException {\n+      registerAndMarkMetrics(e, delete);\n+    }\n+\n+    private void registerAndMarkMetrics(ObserverContext<RegionCoprocessorEnvironment> e, Row row){\n       if (!active || !isMetaTableOp(e)) {\n         return;\n       }\n-      tableMetricRegisterAndMark(e, delete);\n+      tableMetricRegisterAndMark(e, row);\n       clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, delete);\n-      opMetricRegisterAndMark(e, delete);\n-      opWithClientMetricRegisterAndMark(e, delete);\n+      regionMetricRegisterAndMark(e, row);\n+      opMetricRegisterAndMark(e, row);\n+      opWithClientMetricRegisterAndMark(e, row);\n     }\n \n     private void markMeterIfPresent(String requestMeter) {\n       if (requestMeter.isEmpty()) {\n         return;\n       }\n-      Metric metric =\n-          requestsMap.get(requestMeter).isPresent() ? requestsMap.get(requestMeter).get() : null;\n-      if (metric != null) {\n-        ((Meter) metric).mark();\n+\n+      if (requestsMap.containsKey(requestMeter) && requestsMap.get(requestMeter).isPresent()) {\n+        Meter metric = (Meter) requestsMap.get(requestMeter).get();\n+        metric.mark();\n       }\n     }\n \n@@ -137,7 +127,7 @@ private void registerMeterIfNotPresent(ObserverContext<RegionCoprocessorEnvironm\n     /**\n      * Registers and counts lossyCount for Meters that kept by lossy counting.\n      * By using lossy count to maintain meters, at most 7 / e meters will be kept  (e is error rate)\n-     * e.g. when e is 0.02 by default, at most 50 Clients request metrics will be kept\n+     * e.g. when e is 0.02 by default, at most 350 Clients request metrics will be kept\n      *      also, all kept elements have frequency higher than e * N. (N is total count)\n      * @param e Region coprocessor environment\n      * @param requestMeter meter to be registered\n@@ -202,6 +192,7 @@ private boolean isMetaTableOp(ObserverContext<RegionCoprocessorEnvironment> e) {\n     }\n \n     private void clientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e) {\n+      // Mark client metric\n       String clientIP = RpcServer.getRemoteIp() != null ? RpcServer.getRemoteIp().toString() : \"\";\n \n       String clientRequestMeter = clientRequestMeterName(clientIP);\n@@ -211,37 +202,43 @@ private void clientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnviro\n \n     private void tableMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n-      // Mark the meta table meter whenever the coprocessor is called\n+      // Mark table metric\n       String tableName = getTableNameFromOp(op);\n       String tableRequestMeter = tableMeterName(tableName);\n-      registerMeterIfNotPresent(e, tableRequestMeter);\n-      markMeterIfPresent(tableRequestMeter);\n+      registerAndMarkMeterIfNotPresent(e, tableRequestMeter);\n     }\n \n     private void regionMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n-      // Mark the meta table meter whenever the coprocessor is called\n+      // Mark region metric\n       String regionId = getRegionIdFromOp(op);\n       String regionRequestMeter = regionMeterName(regionId);\n-      registerMeterIfNotPresent(e, regionRequestMeter);\n-      markMeterIfPresent(regionRequestMeter);\n+      registerAndMarkMeterIfNotPresent(e, regionRequestMeter);\n     }\n \n     private void opMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n+      // Mark access type [\"get\", \"put\", \"delete\"] metric\n       String opMeterName = opMeterName(op);\n-      registerMeterIfNotPresent(e, opMeterName);\n-      markMeterIfPresent(opMeterName);\n+      registerAndMarkMeterIfNotPresent(e, opMeterName);\n     }\n \n     private void opWithClientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Object op) {\n+      // // Mark client + access type metric\n       String opWithClientMeterName = opWithClientMeterName(op);\n-      registerMeterIfNotPresent(e, opWithClientMeterName);\n-      markMeterIfPresent(opWithClientMeterName);\n+      registerAndMarkMeterIfNotPresent(e, opWithClientMeterName);\n+    }\n+\n+    // Helper function to register and mark meter if not present\n+    private void registerAndMarkMeterIfNotPresent(ObserverContext<RegionCoprocessorEnvironment> e,\n+        String name) {\n+      registerMeterIfNotPresent(e, name);\n+      markMeterIfPresent(name);\n     }\n \n     private String opWithClientMeterName(Object op) {\n+      // Extract meter name containing the client IP\n       String clientIP = RpcServer.getRemoteIp() != null ? RpcServer.getRemoteIp().toString() : \"\";\n       if (clientIP.isEmpty()) {\n         return \"\";\n@@ -265,6 +262,7 @@ private String opWithClientMeterName(Object op) {\n     }\n \n     private String opMeterName(Object op) {\n+      // Extract meter name containing the access type\n       MetaTableOps ops = opsNameMap.get(op.getClass());\n       String opMeterName = \"\";\n       switch (ops) {\n@@ -284,17 +282,20 @@ private String opMeterName(Object op) {\n     }\n \n     private String tableMeterName(String tableName) {\n+      // Extract meter name containing the table name\n       return String.format(\"MetaTable_table_%s_request\", tableName);\n     }\n \n     private String clientRequestMeterName(String clientIP) {\n+      // Extract meter name containing the client IP\n       if (clientIP.isEmpty()) {\n         return \"\";\n       }\n       return String.format(\"MetaTable_client_%s_request\", clientIP);\n     }\n \n     private String regionMeterName(String regionId) {\n+      // Extract meter name containing the region ID\n       return String.format(\"MetaTable_region_%s_request\", regionId);\n     }\n   }\n@@ -306,30 +307,27 @@ private String regionMeterName(String regionId) {\n \n   @Override\n   public void start(CoprocessorEnvironment env) throws IOException {\n+    observer = new ExampleRegionObserverMeta();\n     if (env instanceof RegionCoprocessorEnvironment\n         && ((RegionCoprocessorEnvironment) env).getRegionInfo().getTable() != null\n         && ((RegionCoprocessorEnvironment) env).getRegionInfo().getTable()\n           .equals(TableName.META_TABLE_NAME)) {\n       regionCoprocessorEnv = (RegionCoprocessorEnvironment) env;\n-      observer = new ExampleRegionObserverMeta();\n       requestsMap = new ConcurrentHashMap<>();\n       clientMetricsLossyCounting = new LossyCounting();\n       // only be active mode when this region holds meta table.\n       active = true;\n-    } else {\n-      observer = new ExampleRegionObserverMeta();\n     }\n   }\n \n   @Override\n   public void stop(CoprocessorEnvironment env) throws IOException {\n     // since meta region can move around, clear stale metrics when stop.\n     if (requestsMap != null) {\n+      MetricRegistry registry = regionCoprocessorEnv.getMetricRegistryForRegionServer();\n       for (String meterName : requestsMap.keySet()) {\n-        MetricRegistry registry = regionCoprocessorEnv.getMetricRegistryForRegionServer();\n         registry.remove(meterName);\n       }\n     }\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "sha": "d08bae6762c030da0842ae84182bca0c0fbeb873",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "deletions": 16,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "patch": "@@ -24,7 +24,6 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -61,27 +60,16 @@ public LossyCounting(double errorRate) {\n     this.bucketSize = (long) Math.ceil(1 / errorRate);\n     this.currentTerm = 1;\n     this.totalDataCount = 0;\n-    this.errorRate = errorRate;\n     this.data = new ConcurrentHashMap<>();\n     calculateCurrentTerm();\n   }\n \n   public LossyCounting() {\n-    Configuration conf = HBaseConfiguration.create();\n-    this.errorRate = conf.getDouble(HConstants.DEFAULT_LOSSY_COUNTING_ERROR_RATE, 0.02);\n-    this.bucketSize = (long) Math.ceil(1.0 / errorRate);\n-    this.currentTerm = 1;\n-    this.totalDataCount = 0;\n-    this.data = new ConcurrentHashMap<>();\n-    calculateCurrentTerm();\n+    this(HBaseConfiguration.create().getDouble(HConstants.DEFAULT_LOSSY_COUNTING_ERROR_RATE, 0.02));\n   }\n \n   public Set<String> addByOne(String key) {\n-    if(data.containsKey(key)) {\n-      data.put(key, data.get(key) +1);\n-    } else {\n-      data.put(key, 1);\n-    }\n+    data.put(key, data.getOrDefault(key, 0) + 1);\n     totalDataCount++;\n     calculateCurrentTerm();\n     Set<String> dataToBeSwept = new HashSet<>();\n@@ -105,7 +93,7 @@ public LossyCounting() {\n     for(String key : dataToBeSwept) {\n       data.remove(key);\n     }\n-    LOG.debug(String.format(\"Swept %d of elements.\", dataToBeSwept.size()));\n+    LOG.debug(String.format(\"Swept %d elements.\", dataToBeSwept.size()));\n     return dataToBeSwept;\n   }\n \n@@ -116,7 +104,7 @@ private void calculateCurrentTerm() {\n     this.currentTerm = (int) Math.ceil(1.0 * totalDataCount / bucketSize);\n   }\n \n-  public long getBuketSize(){\n+  public long getBucketSize(){\n     return bucketSize;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "sha": "839bb90acf4cb765ab5f62d99df60ba104dde9b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "deletions": 4,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "patch": "@@ -222,10 +222,6 @@ public void test() throws IOException, InterruptedException {\n             jmxMetrics.stream().filter(metric -> metric.matches(putWithClientMetricNameRegex))\n                     .count();\n     assertEquals(5L, putWithClientMetricsCount);\n-\n-\n-\n-\n   }\n \n }",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "sha": "bbbeb9e5273d6e474a41f455bf3ff98951f0cca9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "patch": "@@ -39,9 +39,9 @@\n   @Test\n   public void testBucketSize() {\n     LossyCounting lossyCounting = new LossyCounting(0.01);\n-    assertEquals(100L, lossyCounting.getBuketSize());\n+    assertEquals(100L, lossyCounting.getBucketSize());\n     LossyCounting lossyCounting2 = new LossyCounting();\n-    assertEquals(50L, lossyCounting2.getBuketSize());\n+    assertEquals(50L, lossyCounting2.getBucketSize());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "sha": "11758be7f5e1c765d130ff0ca84477b0cf93727b",
                "status": "modified"
            }
        ],
        "message": "HBASE-21800: RegionServer aborted due to NPE from MetaTableMetrics coprocessor\n\nHave included code refactoring in MetaTableMetrics & LossyCounting",
        "parent": "https://github.com/apache/hbase/commit/6f16836c20802ff50ed3b0ab4bc4f1bfa791465e",
        "patched_files": [
            "LossyCounting.java",
            "MetaTableMetrics.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestLossyCounting.java",
            "TestMetaTableMetrics.java"
        ]
    },
    "hbase_acbdb86": {
        "bug_id": "hbase_acbdb86",
        "commit": "https://github.com/apache/hbase/commit/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 1,
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "patch": "@@ -74,7 +74,8 @@ protected final int getCycles() {\n    */\n   private int previousState;\n \n-  protected enum Flow {\n+  @VisibleForTesting\n+  public enum Flow {\n     HAS_MORE_STATE,\n     NO_MORE_STATE,\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "sha": "0e3a4cd65479476bb331adcd75bad812a0ffd66b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 0,
                "filename": "hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "patch": "@@ -307,6 +307,7 @@ enum ServerCrashState {\n }\n \n enum RecoverMetaState {\n+  RECOVER_META_PREPARE = 0;\n   RECOVER_META_SPLIT_LOGS = 1;\n   RECOVER_META_ASSIGN_REGIONS = 2;\n }",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "sha": "fa6fa757a80b2ceec55439ec8d61f42c529f4778",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "patch": "@@ -235,12 +235,12 @@ protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n           break;\n         }\n         if (task != null) {\n-          LOG.debug(\"Removing: {} from archive\", task.filePath);\n+          LOG.debug(\"Removing {}\", task.filePath);\n           boolean succeed;\n           try {\n             succeed = this.fs.delete(task.filePath, false);\n           } catch (IOException e) {\n-            LOG.warn(\"Failed to delete file {}\", task.filePath, e);\n+            LOG.warn(\"Failed to delete {}\", task.filePath, e);\n             succeed = false;\n           }\n           task.setResult(succeed);\n@@ -250,7 +250,7 @@ protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n         }\n       }\n     } finally {\n-      LOG.debug(\"Exit thread: {}\", Thread.currentThread());\n+      LOG.debug(\"Exit {}\", Thread.currentThread());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "sha": "b85b56963b69e8ee0e69db0ec3e7fd82accff12a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "patch": "@@ -104,6 +104,21 @@ protected Flow executeFromState(MasterProcedureEnv env,\n \n     try {\n       switch (state) {\n+        case RECOVER_META_PREPARE:\n+          // If Master is going down or cluster is up, skip this assign by returning NO_MORE_STATE\n+          if (!master.isClusterUp()) {\n+            String msg = \"Cluster not up! Skipping hbase:meta assign.\";\n+            LOG.warn(msg);\n+            return Flow.NO_MORE_STATE;\n+          }\n+          if (master.isStopping() || master.isStopped()) {\n+            String msg = \"Master stopping=\" + master.isStopping() + \", stopped=\" +\n+                master.isStopped() + \"; skipping hbase:meta assign.\";\n+            LOG.warn(msg);\n+            return Flow.NO_MORE_STATE;\n+          }\n+          setNextState(RecoverMetaState.RECOVER_META_SPLIT_LOGS);\n+          break;\n         case RECOVER_META_SPLIT_LOGS:\n           LOG.info(\"Start \" + this);\n           if (shouldSplitWal) {\n@@ -202,7 +217,7 @@ protected int getStateId(MasterProcedureProtos.RecoverMetaState recoverMetaState\n \n   @Override\n   protected MasterProcedureProtos.RecoverMetaState getInitialState() {\n-    return RecoverMetaState.RECOVER_META_SPLIT_LOGS;\n+    return RecoverMetaState.RECOVER_META_PREPARE;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "sha": "2234a1bb8983d916712d6df64b2a2db29869c755",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "patch": "@@ -424,7 +424,7 @@ private void logStats() {\n         long created = chunkCount.get();\n         long reused = reusedChunkCount.sum();\n         long total = created + reused;\n-        LOG.debug(\"{} Stats (chunk size={}): current pool size={}, created chunk count={}, \" +\n+        LOG.debug(\"{} stats (chunk size={}): current pool size={}, created chunk count={}, \" +\n                 \"reused chunk count={}, reuseRatio={}\", label, chunkSize, reclaimedChunks.size(),\n             created, reused,\n             (total == 0? \"0\": StringUtils.formatPercent((float)reused/(float)total,2)));",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "sha": "5577be47ad25444ec983a524e78373264bd47d61",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "patch": "@@ -98,6 +98,7 @@ public static void startCluster() throws Exception {\n \n   @AfterClass\n   public static void after() throws Exception {\n+    LOG.info(\"AFTER {} <= IS THIS NULL?\", TEST_UTIL);\n     TEST_UTIL.shutdownMiniCluster();\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "sha": "87f6fa4e25fae42c8c7f0a5e4c75da99567c1c7c",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "patch": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.procedure;\n+\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.master.assignment.MockMasterServices;\n+import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n+import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.mockito.Mockito;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+@Category({MasterTests.class, SmallTests.class})\n+public class TestRecoverMetaProcedure {\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRecoverMetaProcedure.class);\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestRecoverMetaProcedure.class);\n+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n+\n+  /**\n+   * Test the new prepare step.\n+   * Here we test that our Mock is faking out the precedure well-enough for it to progress past the\n+   * first prepare stage.\n+   */\n+  @Test\n+  public void testPrepare() throws ProcedureSuspendedException, ProcedureYieldException,\n+      InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices =\n+        new MockMasterServices(UTIL.getConfiguration(), null);\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.HAS_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+    int stateId = rmp.getCurrentStateId();\n+    assertEquals(MasterProcedureProtos.RecoverMetaState.RECOVER_META_SPLIT_LOGS_VALUE,\n+        rmp.getCurrentStateId());\n+  }\n+\n+  /**\n+   * Test the new prepare step.\n+   * If Master is stopping, procedure should skip the assign by returning NO_MORE_STATE\n+   */\n+  @Test\n+  public void testPrepareWithMasterStopping() throws ProcedureSuspendedException,\n+      ProcedureYieldException, InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices = new MockMasterServices(UTIL.getConfiguration(), null) {\n+      @Override\n+      public boolean isStopping() {\n+        return true;\n+      }\n+    };\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.NO_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+  }\n+\n+  /**\n+   * Test the new prepare step.\n+   * If cluster is down, procedure should skip the assign by returning NO_MORE_STATE\n+   */\n+  @Test\n+  public void testPrepareWithNoCluster() throws ProcedureSuspendedException,\n+      ProcedureYieldException, InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices = new MockMasterServices(UTIL.getConfiguration(), null) {\n+      @Override\n+      public boolean isClusterUp() {\n+        return false;\n+      }\n+    };\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.NO_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "sha": "dc939f50f00e01b79e8ac058e10608cdea475352",
                "status": "added"
            }
        ],
        "message": "HBASE-20169 NPE when calling HBTU.shutdownMiniCluster\n\nAdds a prepare step to RecoverMetaProcedure in which we test for\ncluster up and master being up. If not up, we fail the run.\n\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java\n Minor log cleanup.\n\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java\n Add pepare step.\n\nModified hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java\n Debug for the failing test....\n\nAdded hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java\n Test the prepare step goes down if master or cluster are down.",
        "parent": "https://github.com/apache/hbase/commit/74c28bdf4448c25612df71bb3bebf0420eadd719",
        "patched_files": [
            "ChunkCreator.java",
            "StateMachineProcedure.java",
            "RecoverMetaProcedure.java",
            "HFileCleaner.java",
            "MasterProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManagerMetrics.java",
            "TestHFileCleaner.java",
            "TestStateMachineProcedure.java",
            "TestRecoverMetaProcedure.java"
        ]
    },
    "hbase_afac14f": {
        "bug_id": "hbase_afac14f",
        "commit": "https://github.com/apache/hbase/commit/afac14fcf052219063b338b18db936bd69f22233",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/afac14fcf052219063b338b18db936bd69f22233/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=afac14fcf052219063b338b18db936bd69f22233",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -17,6 +17,8 @@ Release 0.91.0 - Unreleased\n                the HBase shell (Igor Ranitovic via Stack)\n    HBASE-3317  Javadoc and Throws Declaration for Bytes.incrementBytes() is\n                Wrong (Ed Kohlwey via Stack)\n+   HBASE-1888  KeyValue methods throw NullPointerException instead of\n+               IllegalArgumentException during parameter sanity check\n \n   IMPROVEMENTS\n    HBASE-2001  Coprocessors: Colocate user code with regions (Mingjie Lai via",
                "raw_url": "https://github.com/apache/hbase/raw/afac14fcf052219063b338b18db936bd69f22233/CHANGES.txt",
                "sha": "4ab8cf58ef0d7932f1b9b18264e061fdf687eae7",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/afac14fcf052219063b338b18db936bd69f22233/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=afac14fcf052219063b338b18db936bd69f22233",
                "deletions": 20,
                "filename": "src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "patch": "@@ -36,35 +36,35 @@\n import org.apache.hadoop.io.Writable;\n \n /**\n- * An HBase Key/Value.\n+ * An HBase Key/Value.  This is the fundamental HBase Type.\n  *\n  * <p>If being used client-side, the primary methods to access individual fields\n  * are {@link #getRow()}, {@link #getFamily()}, {@link #getQualifier()},\n  * {@link #getTimestamp()}, and {@link #getValue()}.  These methods allocate new\n- * byte arrays and return copies so they should be avoided server-side.\n+ * byte arrays and return copies. Avoid their use server-side.\n  *\n- * <p>Instances of this class are immutable.  They are not\n- * comparable but Comparators are provided.  Comparators change with context,\n- * whether user table or a catalog table comparison context.  Its\n- * important that you use the appropriate comparator comparing rows in\n- * particular.  There are Comparators for KeyValue instances and then for\n- * just the Key portion of a KeyValue used mostly in {@link HFile}.\n+ * <p>Instances of this class are immutable.  They do not implement Comparable\n+ * but Comparators are provided.  Comparators change with context,\n+ * whether user table or a catalog table comparison.  Its critical you use the\n+ * appropriate comparator.  There are Comparators for KeyValue instances and\n+ * then for just the Key portion of a KeyValue used mostly by {@link HFile}.\n  *\n- * <p>KeyValue wraps a byte array and has offset and length for passed array\n- * at where to start interpreting the content as a KeyValue blob.  The KeyValue\n- * blob format inside the byte array is:\n+ * <p>KeyValue wraps a byte array and takes offsets and lengths into passed\n+ * array at where to start interpreting the content as KeyValue.  The KeyValue\n+ * format inside a byte array is:\n  * <code>&lt;keylength> &lt;valuelength> &lt;key> &lt;value></code>\n- * Key is decomposed as:\n+ * Key is further decomposed as:\n  * <code>&lt;rowlength> &lt;row> &lt;columnfamilylength> &lt;columnfamily> &lt;columnqualifier> &lt;timestamp> &lt;keytype></code>\n- * Rowlength maximum is Short.MAX_SIZE, column family length maximum is\n- * Byte.MAX_SIZE, and column qualifier + key length must be < Integer.MAX_SIZE.\n- * The column does not contain the family/qualifier delimiter.\n- *\n- * <p>TODO: Group Key-only comparators and operations into a Key class, just\n- * for neatness sake, if can figure what to call it.\n+ * The <code>rowlength</code> maximum is <code>Short.MAX_SIZE</code>,\n+ * column family length maximum is\n+ * <code>Byte.MAX_SIZE</code>, and column qualifier + key length must\n+ * be < <code>Integer.MAX_SIZE</code>.\n+ * The column does not contain the family/qualifier delimiter, {@link #COLUMN_FAMILY_DELIMITER}\n  */\n public class KeyValue implements Writable, HeapSize {\n   static final Log LOG = LogFactory.getLog(KeyValue.class);\n+  // TODO: Group Key-only comparators and operations into a Key class, just\n+  // for neatness sake, if can figure what to call it.\n \n   /**\n    * Colon character in UTF-8\n@@ -1293,7 +1293,7 @@ static int getRequiredDelimiterInReverse(final byte [] b,\n   public static int getDelimiter(final byte [] b, int offset, final int length,\n       final int delimiter) {\n     if (b == null) {\n-      throw new NullPointerException();\n+      throw new IllegalArgumentException(\"Passed buffer is null\");\n     }\n     int result = -1;\n     for (int i = offset; i < length + offset; i++) {\n@@ -1314,7 +1314,7 @@ public static int getDelimiter(final byte [] b, int offset, final int length,\n   public static int getDelimiterInReverse(final byte [] b, final int offset,\n       final int length, final int delimiter) {\n     if (b == null) {\n-      throw new NullPointerException();\n+      throw new IllegalArgumentException(\"Passed buffer is null\");\n     }\n     int result = -1;\n     for (int i = (offset + length) - 1; i >= offset; i--) {",
                "raw_url": "https://github.com/apache/hbase/raw/afac14fcf052219063b338b18db936bd69f22233/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "sha": "91221effb73f6120ecd05a03bbb2cdde21b550eb",
                "status": "modified"
            }
        ],
        "message": "HBASE-1888 KeyValue methods throw NullPointerException instead of IllegalArgumentException during parameter sanity check\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1043218 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/584e9b5c26e78668d3ccf02d7dc65e5fd97871dd",
        "patched_files": [
            "KeyValue.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestKeyValue.java"
        ]
    },
    "hbase_b329e6e": {
        "bug_id": "hbase_b329e6e",
        "commit": "https://github.com/apache/hbase/commit/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -778,7 +778,6 @@ protected void initializeZKBasedSystemTrackers()\n     this.splitOrMergeTracker.start();\n \n     this.replicationPeerManager = ReplicationPeerManager.create(zooKeeper, conf);\n-    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n \n     this.drainingServerTracker = new DrainingServerTracker(zooKeeper, this, this.serverManager);\n     this.drainingServerTracker.start();\n@@ -949,7 +948,10 @@ private void finishActiveMasterInitialization(MonitoredTask status) throws IOExc\n     }\n \n     status.setStatus(\"Initialize ServerManager and schedule SCP for crash servers\");\n+    // The below two managers must be created before loading procedures, as they will be used during\n+    // loading.\n     this.serverManager = createServerManager(this);\n+    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n     createProcedureExecutor();\n     @SuppressWarnings(\"rawtypes\")\n     Map<Class<? extends Procedure>, List<Procedure<MasterProcedureEnv>>> procsByType =",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "e1d374090e0ef6e52e6dd93d20196ee6fbc32cb3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "patch": "@@ -146,12 +146,12 @@ public SyncReplicationReplayWALManager(MasterServices services)\n     this.fs = services.getMasterFileSystem().getWALFileSystem();\n     this.walRootDir = services.getMasterFileSystem().getWALRootDir();\n     this.remoteWALDir = new Path(this.walRootDir, ReplicationUtils.REMOTE_WAL_DIR_NAME);\n-    MasterProcedureScheduler scheduler =\n-      services.getMasterProcedureExecutor().getEnvironment().getProcedureScheduler();\n     serverManager.registerListener(new ServerListener() {\n \n       @Override\n       public void serverAdded(ServerName serverName) {\n+        MasterProcedureScheduler scheduler =\n+          services.getMasterProcedureExecutor().getEnvironment().getProcedureScheduler();\n         for (UsedReplayWorkersForPeer usedWorkers : usedWorkersByPeer.values()) {\n           synchronized (usedWorkers) {\n             usedWorkers.wake(scheduler);",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "sha": "ae624b145709c22f9a0088fed9a287ba1fc9d240",
                "status": "modified"
            },
            {
                "additions": 127,
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "changes": 127,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "patch": "@@ -0,0 +1,127 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState.DISPATCH_WALS_VALUE;\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState.UNREGISTER_PEER_FROM_WORKER_STORAGE_VALUE;\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n+import org.apache.hadoop.hbase.replication.SyncReplicationState;\n+import org.apache.hadoop.hbase.replication.SyncReplicationTestBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;\n+import org.apache.zookeeper.KeeperException;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Testcase for HBASE-21494.\n+ */\n+@Category({ MasterTests.class, LargeTests.class })\n+public class TestRegisterPeerWorkerWhenRestarting extends SyncReplicationTestBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegisterPeerWorkerWhenRestarting.class);\n+\n+  private static volatile boolean FAIL = false;\n+\n+  public static final class HMasterForTest extends HMaster {\n+\n+    public HMasterForTest(Configuration conf) throws IOException, KeeperException {\n+      super(conf);\n+    }\n+\n+    @Override\n+    public void remoteProcedureCompleted(long procId) {\n+      if (FAIL && getMasterProcedureExecutor()\n+        .getProcedure(procId) instanceof SyncReplicationReplayWALRemoteProcedure) {\n+        throw new RuntimeException(\"Inject error\");\n+      }\n+      super.remoteProcedureCompleted(procId);\n+    }\n+  }\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    UTIL2.getConfiguration().setClass(HConstants.MASTER_IMPL, HMasterForTest.class, HMaster.class);\n+    SyncReplicationTestBase.setUp();\n+  }\n+\n+  @Test\n+  public void testRestart() throws Exception {\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.STANDBY);\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.ACTIVE);\n+\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, 100);\n+    Thread.sleep(2000);\n+    // peer is disabled so no data have been replicated\n+    verifyNotReplicatedThroughRegion(UTIL2, 0, 100);\n+\n+    // transit the A to DA first to avoid too many error logs.\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.DOWNGRADE_ACTIVE);\n+    HMaster master = UTIL2.getHBaseCluster().getMaster();\n+    // make sure the transiting can not succeed\n+    FAIL = true;\n+    ProcedureExecutor<MasterProcedureEnv> procExec = master.getMasterProcedureExecutor();\n+    Thread t = new Thread() {\n+\n+      @Override\n+      public void run() {\n+        try {\n+          UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+            SyncReplicationState.DOWNGRADE_ACTIVE);\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+    };\n+    t.start();\n+    // wait until we are in the states where we need to register peer worker when restarting\n+    UTIL2.waitFor(60000,\n+      () -> procExec.getProcedures().stream().filter(p -> p instanceof RecoverStandbyProcedure)\n+        .map(p -> (RecoverStandbyProcedure) p)\n+        .anyMatch(p -> p.getCurrentStateId() == DISPATCH_WALS_VALUE ||\n+          p.getCurrentStateId() == UNREGISTER_PEER_FROM_WORKER_STORAGE_VALUE));\n+    // failover to another master\n+    MasterThread mt = UTIL2.getMiniHBaseCluster().getMasterThread();\n+    mt.getMaster().abort(\"for testing\");\n+    mt.join();\n+    FAIL = false;\n+    t.join();\n+    // make sure the new master can finish the transiting\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+    verify(UTIL2, 0, 100);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "sha": "72aa32d22e4a799c3dc6f6b3861e00754638f286",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.master.replication;\n \n+import static org.junit.Assert.assertEquals;\n+\n import java.io.IOException;\n import java.io.UncheckedIOException;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n@@ -90,5 +92,8 @@ public void run() {\n       .mapToLong(Procedure::getProcId).min().getAsLong();\n     MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId);\n     ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+    verify(UTIL2, 0, 100);\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "sha": "9b73039c180c24887b58cac85c8a9b7c1cf42194",
                "status": "modified"
            }
        ],
        "message": "HBASE-21494 NPE when loading RecoverStandByProcedure",
        "parent": "https://github.com/apache/hbase/commit/f555258e7abab1337ee4d39aaa1dafff72be287b",
        "patched_files": [
            "HMaster.java",
            "SyncReplicationReplayWALManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSyncReplicationReplayWALManager.java",
            "TestRegisterPeerWorkerWhenRestarting.java",
            "TestTransitPeerSyncReplicationStateProcedureRetry.java"
        ]
    },
    "hbase_b5a5aa6": {
        "bug_id": "hbase_b5a5aa6",
        "commit": "https://github.com/apache/hbase/commit/b5a5aa63814b93b567410867b6824f3b33124c4e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -429,6 +429,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2806  DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress\n                (Benoit Sigoure via Stack)\n    HBASE-2806  (small compile fix via jgray)\n+   HBASE-2797  Another NPE in ReadWriteConsistencyControl\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/CHANGES.txt",
                "sha": "7ef4321545145fb8b1eb1dabe76fb8d53402ac15",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "deletions": 7,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -169,15 +169,9 @@\n   }\n \n   public synchronized KeyValue peek() {\n-    try {\n-      checkReseek();\n-    } catch (IOException e) {\n-      throw new RuntimeException(\"IOE conversion\", e);\n-    }\n     if (this.heap == null) {\n-      return null;\n+      return this.lastTop;\n     }\n-\n     return this.heap.peek();\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "91eafd062760f240392eca91db77849a2ee92b0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "patch": "@@ -452,6 +452,8 @@ public void testScannerReseekDoesntNPE() throws Exception {\n     scan.updateReaders();\n \n     scan.updateReaders();\n+\n+    scan.peek();\n   }\n \n ",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "sha": "1b5fb25ee7036cc0c42b4f76f7f52e70ad357f03",
                "status": "modified"
            }
        ],
        "message": "HBASE-2797  Another NPE in ReadWriteConsistencyControl\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@961549 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/c2c41f0439fe2c8c72f78949494ecfdcd9de63ae",
        "patched_files": [
            "CHANGES.java",
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStoreScanner.java"
        ]
    },
    "hbase_b5e2145": {
        "bug_id": "hbase_b5e2145",
        "commit": "https://github.com/apache/hbase/commit/b5e2145879254acb416e8414924ab9256d55bd31",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b5e2145879254acb416e8414924ab9256d55bd31",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "patch": "@@ -762,7 +762,7 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n     }\n \n     if (null == jar || jar.isEmpty()) {\n-      throw new IOException(\"Cannot locate resource for class \" + my_class.getName());\n+      return null;\n     }\n \n     LOG.debug(String.format(\"For class %s, using jar %s\", my_class.getName(), jar));\n@@ -776,6 +776,9 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n    * @param packagedClasses map[class -> jar]\n    */\n   private static void updateMap(String jar, Map<String, String> packagedClasses) throws IOException {\n+    if (null == jar || jar.isEmpty()) {\n+      return;\n+    }\n     ZipFile zip = null;\n     try {\n       zip = new ZipFile(jar);",
                "raw_url": "https://github.com/apache/hbase/raw/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "sha": "cc66882c03aaedf21611960675482825e2c6e04d",
                "status": "modified"
            }
        ],
        "message": "HBASE-10061 TableMapReduceUtil.findOrCreateJar calls updateMap(null, ) resulting in thrown NPE (Amit Sela)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1548747 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/656b5ca9f73ee9cb806ec39fc32001fa9f32df3c",
        "patched_files": [
            "TableMapReduceUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableMapReduceUtil.java"
        ]
    },
    "hbase_b7d932f": {
        "bug_id": "hbase_b7d932f",
        "commit": "https://github.com/apache/hbase/commit/b7d932f4dc1d1cee73142376baaee212df9d6ae0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -15,6 +15,7 @@ Trunk (unreleased changes)\n   BUG FIXES\r\n    HADOOP-2731 Under load, regions become extremely large and eventually cause\r\n                region servers to become unresponsive\r\n+   HADOOP-2693 NPE in getClosestRowBefore (Bryan Duxbury & Stack)\r\n \r\n   IMPROVEMENTS\r\n    HADOOP-2555 Refactor the HTable#get and HTable#getRow methods to avoid\r",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "sha": "1e60afff71fd9da9c9ff74959202eecdcd388ab1",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "deletions": 14,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -1830,20 +1830,13 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n       for(int i = maparray.length - 1; i >= 0; i--) {\n         Text row_from_mapfile = \n           rowAtOrBeforeFromMapFile(maparray[i], row, timestamp);\n-\n-        // for when we have MapFile.Reader#getClosest before functionality\n-/*        Text row_from_mapfile = null;\n-        WritableComparable value = null; \n-        \n-        HStoreKey hskResult = \n-          (HStoreKey)maparray[i].getClosest(rowKey, value, true);\n-        \n-        if (hskResult != null) {\n-          row_from_mapfile = hskResult.getRow();\n-        }*/\n-                \n-/*        LOG.debug(\"Best from this mapfile was \" + row_from_mapfile);*/\n         \n+        // if the result from the mapfile is null, then we know that\n+        // the mapfile was empty and can move on to the next one.\n+        if (row_from_mapfile == null) {\n+          continue;\n+        }\n+      \n         // short circuit on an exact match\n         if (row.equals(row_from_mapfile)) {\n           return row;\n@@ -1855,7 +1848,6 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n         }\n       }\n       \n-/*      LOG.debug(\"Went searching for \" + row + \", found \" + bestSoFar);*/\n       return bestSoFar;\n     } finally {\n       this.lock.readLock().unlock();",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "ab5f7f1fe7f29d44504fe9d7e5b3e653f3cb66ab",
                "status": "modified"
            }
        ],
        "message": "HADOOP-2693 NPE in getClosestRowBefore\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk/src/contrib/hbase@617724 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/3e351091b6695c6344ad382ee28db8af0627e2e5",
        "patched_files": [
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_b8f999b": {
        "bug_id": "hbase_b8f999b",
        "commit": "https://github.com/apache/hbase/commit/b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "patch": "@@ -38,6 +38,8 @@\n import org.apache.hadoop.hbase.shaded.protobuf.generated.HFileProtos;\n import org.apache.hadoop.hbase.util.Bytes;\n \n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * The {@link HFile} has a fixed trailer which contains offsets to other\n@@ -53,6 +55,8 @@\n  */\n @InterfaceAudience.Private\n public class FixedFileTrailer {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedFileTrailer.class);\n+\n   /**\n    * We store the comparator class name as a fixed-length field in the trailer.\n    */\n@@ -623,7 +627,13 @@ private String getHBase1CompatibleName(final String comparator) {\n   public static CellComparator createComparator(\n       String comparatorClassName) throws IOException {\n     try {\n-      return getComparatorClass(comparatorClassName).getDeclaredConstructor().newInstance();\n+\n+      Class<? extends CellComparator> comparatorClass = getComparatorClass(comparatorClassName);\n+      if(comparatorClass != null){\n+        return comparatorClass.getDeclaredConstructor().newInstance();\n+      }\n+      LOG.warn(\"No Comparator class for \" + comparatorClassName + \". Returning Null.\");\n+      return null;\n     } catch (Exception e) {\n       throw new IOException(\"Comparator class \" + comparatorClassName +\n         \" is not instantiable\", e);",
                "raw_url": "https://github.com/apache/hbase/raw/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "sha": "3c74d114e1e8d80903c292786ce1c7fd8f2e4657",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hbase/blob/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java?ref=b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hbase.io.hfile;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -33,6 +34,7 @@\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n import org.apache.hadoop.hbase.CellComparatorImpl;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n@@ -43,8 +45,10 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.Before;\n import org.junit.ClassRule;\n+import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n+import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n import org.junit.runners.Parameterized.Parameters;\n@@ -82,6 +86,9 @@ public TestFixedFileTrailer(int version) {\n     this.version = version;\n   }\n \n+  @Rule\n+  public ExpectedException expectedEx = ExpectedException.none();\n+\n   @Parameters\n   public static Collection<Object[]> getParameters() {\n     List<Object[]> versionsToTest = new ArrayList<>();\n@@ -108,6 +115,34 @@ public void testComparatorIsHBase1Compatible() {\n         pb.getComparatorClassName());\n   }\n \n+  @Test\n+  public void testCreateComparator() throws IOException {\n+    FixedFileTrailer t = new FixedFileTrailer(version, HFileReaderImpl.PBUF_TRAILER_MINOR_VERSION);\n+    try {\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(KeyValue.COMPARATOR.getLegacyKeyComparatorName()).getClass());\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(KeyValue.COMPARATOR.getClass().getName()).getClass());\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(CellComparator.class.getName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class,\n+          t.createComparator(KeyValue.META_COMPARATOR.getLegacyKeyComparatorName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class,\n+          t.createComparator(KeyValue.META_COMPARATOR.getClass().getName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class, t.createComparator(\n+          CellComparatorImpl.MetaCellComparator.META_COMPARATOR.getClass().getName()).getClass());\n+      assertNull(t.createComparator(Bytes.BYTES_RAWCOMPARATOR.getClass().getName()));\n+      assertNull(t.createComparator(\"org.apache.hadoop.hbase.KeyValue$RawBytesComparator\"));\n+    } catch (IOException e) {\n+      fail(\"Unexpected exception while testing FixedFileTrailer#createComparator()\");\n+    }\n+\n+    // Test an invalid comparatorClassName\n+    expectedEx.expect(IOException.class);\n+    t.createComparator(\"\");\n+\n+  }\n+\n   @Test\n   public void testTrailer() throws IOException {\n     FixedFileTrailer t = new FixedFileTrailer(version,",
                "raw_url": "https://github.com/apache/hbase/raw/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "sha": "d25ce4796d7ccbd9ac19e3f48b6bcba8525776a9",
                "status": "modified"
            }
        ],
        "message": "HBASE-20135 Fixed NullPointerException during reading bloom filter when upgraded from hbase-1 to hbase-2",
        "parent": "https://github.com/apache/hbase/commit/65bd0881e25981a41373ba70a5929dc15cf1c037",
        "patched_files": [
            "FixedFileTrailer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFixedFileTrailer.java"
        ]
    },
    "hbase_b94b0e9": {
        "bug_id": "hbase_b94b0e9",
        "commit": "https://github.com/apache/hbase/commit/b94b0e9ca76f5ab2fad648dae052dce85e578628",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hbase/blob/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b94b0e9ca76f5ab2fad648dae052dce85e578628",
                "deletions": 16,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "patch": "@@ -900,25 +900,28 @@ private static void updateMap(String jar, Map<String, String> packagedClasses) t\n   private static String findContainingJar(Class<?> my_class, Map<String, String> packagedClasses)\n       throws IOException {\n     ClassLoader loader = my_class.getClassLoader();\n+\n     String class_file = my_class.getName().replaceAll(\"\\\\.\", \"/\") + \".class\";\n \n-    // first search the classpath\n-    for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n-      URL url = itr.nextElement();\n-      if (\"jar\".equals(url.getProtocol())) {\n-        String toReturn = url.getPath();\n-        if (toReturn.startsWith(\"file:\")) {\n-          toReturn = toReturn.substring(\"file:\".length());\n+    if (loader != null) {\n+      // first search the classpath\n+      for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n+        URL url = itr.nextElement();\n+        if (\"jar\".equals(url.getProtocol())) {\n+          String toReturn = url.getPath();\n+          if (toReturn.startsWith(\"file:\")) {\n+            toReturn = toReturn.substring(\"file:\".length());\n+          }\n+          // URLDecoder is a misnamed class, since it actually decodes\n+          // x-www-form-urlencoded MIME type rather than actual\n+          // URL encoding (which the file path has). Therefore it would\n+          // decode +s to ' 's which is incorrect (spaces are actually\n+          // either unencoded or encoded as \"%20\"). Replace +s first, so\n+          // that they are kept sacred during the decoding process.\n+          toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n+          toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n+          return toReturn.replaceAll(\"!.*$\", \"\");\n         }\n-        // URLDecoder is a misnamed class, since it actually decodes\n-        // x-www-form-urlencoded MIME type rather than actual\n-        // URL encoding (which the file path has). Therefore it would\n-        // decode +s to ' 's which is incorrect (spaces are actually\n-        // either unencoded or encoded as \"%20\"). Replace +s first, so\n-        // that they are kept sacred during the decoding process.\n-        toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n-        toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n-        return toReturn.replaceAll(\"!.*$\", \"\");\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "sha": "b5149415a5e6abbb9cdfaec75604bd2c6c6056b3",
                "status": "modified"
            }
        ],
        "message": "HBASE-12491 TableMapReduceUtil.findContainingJar() NPE",
        "parent": "https://github.com/apache/hbase/commit/555e78005df2b1f211e013acc5f743df9c7b80ab",
        "patched_files": [
            "TableMapReduceUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableMapReduceUtil.java"
        ]
    },
    "hbase_bd7c8e3": {
        "bug_id": "hbase_bd7c8e3",
        "commit": "https://github.com/apache/hbase/commit/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -127,6 +127,7 @@ Release 0.91.0 - Unreleased\n    HBASE-3983  list command in shell seems broken\n    HBASE-3793  HBASE-3468 Broke checkAndPut with null value (Ming Ma)\n    HBASE-3995  HBASE-3946 broke TestMasterFailover\n+   HBASE-3889  NPE in Distributed Log Splitting (Anirudh Todi)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/CHANGES.txt",
                "sha": "d481c9dec001d2d34ee0c1295bd9988685009164",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "patch": "@@ -136,6 +136,7 @@ public Status exec(String filename, CancelableProgressable p) {\n \n   @Override\n   public void run() {\n+   try {\n     LOG.info(\"SplitLogWorker \" + this.serverName + \" starting\");\n     this.watcher.registerListener(this);\n     int res;\n@@ -162,8 +163,13 @@ public void run() {\n     }\n \n     taskLoop();\n-\n-    LOG.info(\"SplitLogWorker \" + this.serverName + \" exiting\");\n+   } catch (Throwable t) {\n+\t   // only a logical error can cause here. Printing it out \n+\t   // to make debugging easier\n+\t   LOG.error(\"unexpected error \", t);\n+   } finally {\n+\t   LOG.info(\"SplitLogWorker \" + this.serverName + \" exiting\");\n+   }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "sha": "843873e8fa58dd51a56fb76945ff685d4141b8f7",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "deletions": 2,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "patch": "@@ -152,7 +152,8 @@ public void tearDown() throws Exception {\n    * @throws IOException\n    * @see https://issues.apache.org/jira/browse/HBASE-3020\n    */\n-  @Test public void testRecoveredEditsPathForMeta() throws IOException {\n+  @Test \n+  public void testRecoveredEditsPathForMeta() throws IOException {\n     FileSystem fs = FileSystem.get(TEST_UTIL.getConfiguration());\n     byte [] encoded = HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes();\n     Path tdir = new Path(hbaseDir, Bytes.toString(HConstants.META_TABLE_NAME));\n@@ -952,7 +953,35 @@ public void testSplitLogFileWithOneRegion() throws IOException {\n \n     assertEquals(true, logsAreEqual(originalLog, splitLog));\n   }\n+  \n+  @Test\n+  public void testSplitLogFileDeletedRegionDir()\n+  throws IOException {\n+\tLOG.info(\"testSplitLogFileDeletedRegionDir\");\n+\tfinal String REGION = \"region__1\";\n+    regions.removeAll(regions);\n+    regions.add(REGION);\n+\n+\n+    generateHLogs(1, 10, -1);\n+    FileStatus logfile = fs.listStatus(hlogDir)[0];\n+    fs.initialize(fs.getUri(), conf);\n+    \n+    Path regiondir = new Path(tabledir, REGION);\n+    LOG.info(\"Region directory is\" + regiondir);\n+    fs.delete(regiondir, true);\n+    \n+    HLogSplitter.splitLogFileToTemp(hbaseDir, \"tmpdir\", logfile, fs,\n+        conf, reporter);\n+    HLogSplitter.moveRecoveredEditsFromTemp(\"tmpdir\", hbaseDir, oldLogDir,\n+        logfile.getPath().toString(), conf);\n+    \n+    assertTrue(!fs.exists(regiondir));\n+    assertTrue(true);\n+  }\n \n+  \n+  \n   @Test\n   public void testSplitLogFileEmpty() throws IOException {\n     LOG.info(\"testSplitLogFileEmpty\");\n@@ -1009,7 +1038,6 @@ public void testSplitLogFileFirstLineCorruptionLog()\n     assertEquals(1, fs.listStatus(corruptDir).length);\n   }\n \n-\n   private void flushToConsole(String s) {\n     System.out.println(s);\n     System.out.flush();",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "sha": "de28418efea0a7db8cc0a0ad709cc31f70236956",
                "status": "modified"
            }
        ],
        "message": "HBASE-3889  NPE in Distributed Log Splitting\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1136659 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/0f15fcac46b9ffb520ca81e736f855bde2731e6f",
        "patched_files": [
            "CHANGES.java",
            "SplitLogWorker.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitLogWorker.java",
            "TestHLogSplit.java"
        ]
    },
    "hbase_be33a24": {
        "bug_id": "hbase_be33a24",
        "commit": "https://github.com/apache/hbase/commit/be33a241ce1a2926fb72a961b8d02213057d14a8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -95,3 +95,4 @@ Trunk (unreleased changes)\n  58. HADOOP-1710 All updates should be batch updates\n  59. HADOOP-1711 HTable API should use interfaces instead of concrete classes as\n      method parameters and return values\n+ 60. HADOOP-1644 Compactions should not block updates",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/CHANGES.txt",
                "sha": "97ec53b7fce109c5b065f4c454ee3050d0a1732f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/conf/hbase-default.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/conf/hbase-default.xml?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 1,
                "filename": "conf/hbase-default.xml",
                "patch": "@@ -147,7 +147,7 @@\n     </description>\n   </property>\n   <property>\n-    <name>hbase.hregion.compactionThreshold</name>\n+    <name>hbase.hstore.compactionThreshold</name>\n     <value>3</value>\n     <description>\n     If more than this number of HStoreFiles in any one HStore",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/conf/hbase-default.xml",
                "sha": "9a4316c22f6ef33640e948c765df8462d6e9ea26",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnection.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnection.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HConnection.java",
                "patch": "@@ -38,6 +38,7 @@\n   public boolean isMasterRunning();\n   \n   /**\n+   * Checks if <code>tableName</code> exists.\n    * @param tableName Table to check.\n    * @return True if table exists already.\n    */",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnection.java",
                "sha": "442b904f8c8ac4e3f1f5902748c115dab4e36597",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 23,
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "patch": "@@ -44,7 +44,7 @@\n  * multiple HBase instances\n  */\n public class HConnectionManager implements HConstants {\n-  private HConnectionManager(){}                        // Not instantiable\n+  private HConnectionManager() {}                        // Not instantiable\n   \n   // A Map of master HServerAddress -> connection information for that instance\n   // Note that although the Map is synchronized, the objects it contains\n@@ -209,15 +209,19 @@ public boolean isMasterRunning() {\n \n     /** {@inheritDoc} */\n     public boolean tableExists(final Text tableName) {\n-      boolean exists = true;\n+      if (tableName == null) {\n+        throw new IllegalArgumentException(\"Table name cannot be null\");\n+      }\n+      boolean exists = false;\n       try {\n-        SortedMap<Text, HRegionLocation> servers = getTableServers(tableName);\n-        if (servers == null || servers.size() == 0) {\n-          exists = false;\n+        HTableDescriptor[] tables = listTables();\n+        for (int i = 0; i < tables.length; i++) {\n+          if (tables[i].getName().equals(tableName)) {\n+            exists = true;\n+          }\n         }\n-\n       } catch (IOException e) {\n-        exists = false;\n+        LOG.warn(\"Testing for table existence threw exception\", e);\n       }\n       return exists;\n     }\n@@ -400,7 +404,6 @@ public void close(Text tableName) {\n     throws IOException {\n       \n       // Wipe out everything we know about this table\n-\n       if (this.tablesToServers.remove(tableName) != null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Wiping out all we know of \" + tableName);\n@@ -524,9 +527,10 @@ public void close(Text tableName) {\n       }\n       this.tablesToServers.put(tableName, servers);\n       if (LOG.isDebugEnabled()) {\n+        int count = 0;\n         for (Map.Entry<Text, HRegionLocation> e: servers.entrySet()) {\n-          LOG.debug(\"Server \" + e.getKey() + \" is serving: \" + e.getValue() +\n-              \" for table \" + tableName);\n+          LOG.debug(\"Region \" + (1 + count++) + \" of \" + servers.size() +\n+            \": \" + e.getValue());\n         }\n       }\n       return servers;\n@@ -650,40 +654,47 @@ public void close(Text tableName) {\n         new TreeMap<Text, HRegionLocation>();\n       \n       for (int tries = 0; servers.size() == 0 && tries < numRetries; tries++) {\n-\n         long scannerId = -1L;\n         try {\n-          scannerId =\n-            server.openScanner(t.getRegionInfo().getRegionName(),\n-                COLUMN_FAMILY_ARRAY, tableName, System.currentTimeMillis(), null);\n+          scannerId = server.openScanner(t.getRegionInfo().getRegionName(),\n+            COLUMN_FAMILY_ARRAY, tableName, System.currentTimeMillis(), null);\n \n           while (true) {\n-            HRegionInfo regionInfo = null;\n-            String serverAddress = null;\n             KeyedData[] values = server.next(scannerId);\n             if (values.length == 0) {\n               if (servers.size() == 0) {\n                 // If we didn't find any servers then the table does not exist\n                 throw new TableNotFoundException(\"table '\" + tableName +\n-                    \"' does not exist in \" + t);\n+                  \"' does not exist in \" + t);\n               }\n \n               // We found at least one server for the table and now we're done.\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Found \" + servers.size() + \" server(s) for \" +\n-                    \"location: \" + t + \" for tablename \" + tableName);\n+                  tableName + \" at \" + t);\n               }\n               break;\n             }\n \n-            byte[] bytes = null;\n             TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n             for (int i = 0; i < values.length; i++) {\n               results.put(values[i].getKey().getColumn(), values[i].getData());\n             }\n-            regionInfo = new HRegionInfo();\n-            regionInfo = (HRegionInfo) Writables.getWritable(\n-                results.get(COL_REGIONINFO), regionInfo);\n+            \n+            byte[] bytes = results.get(COL_REGIONINFO);\n+            if (bytes == null || bytes.length == 0) {\n+              // This can be null.  Looks like an info:splitA or info:splitB\n+              // is only item in the row.\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(COL_REGIONINFO.toString() + \" came back empty: \" +\n+                  results.toString());\n+              }\n+              servers.clear();\n+              break;\n+            }\n+            \n+            HRegionInfo regionInfo = (HRegionInfo) Writables.getWritable(\n+              results.get(COL_REGIONINFO), new HRegionInfo());\n \n             if (!regionInfo.tableDesc.getName().equals(tableName)) {\n               // We're done\n@@ -707,7 +718,8 @@ public void close(Text tableName) {\n               servers.clear();\n               break;\n             }\n-            serverAddress = Writables.bytesToString(bytes);\n+            \n+            String serverAddress = Writables.bytesToString(bytes);\n             servers.put(regionInfo.startKey, new HRegionLocation(\n                 regionInfo, new HServerAddress(serverAddress)));\n           }",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "sha": "56215760df66eb0aee8fa3e002fbdee9808b44a4",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HLogEdit.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "patch": "@@ -34,6 +34,7 @@\n   private Text column = new Text();\n   private byte [] val;\n   private long timestamp;\n+  private final int MAX_VALUE_LEN = 128;\n \n   /**\n    * Default constructor used by Writable\n@@ -69,17 +70,23 @@ public long getTimestamp() {\n     return this.timestamp;\n   }\n \n-  /** {@inheritDoc} */\n+  /**\n+   * @return First column name, timestamp, and first 128 bytes of the value\n+   * bytes as a String.\n+   */\n   @Override\n   public String toString() {\n     String value = \"\";\n     try {\n-      value = new String(getVal(), HConstants.UTF8_ENCODING);\n-      \n+      value = (this.val.length > MAX_VALUE_LEN)?\n+        new String(this.val, 0, MAX_VALUE_LEN, HConstants.UTF8_ENCODING) +\n+          \"...\":\n+        new String(getVal(), HConstants.UTF8_ENCODING);\n     } catch (UnsupportedEncodingException e) {\n       throw new RuntimeException(\"UTF8 encoding not present?\", e);\n     }\n-    return \"(\" + getColumn().toString() + \"/\" + getTimestamp() + \"/\" + value + \")\";\n+    return \"(\" + getColumn().toString() + \"/\" + getTimestamp() + \"/\" +\n+      value + \")\";\n   }\n   \n   // Writable\n@@ -99,4 +106,4 @@ public void readFields(DataInput in) throws IOException {\n     in.readFully(this.val);\n     this.timestamp = in.readLong();\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "sha": "6e5b6aa818bf79f38a000dd9ed30995611c1202e",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMaster.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 29,
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "patch": "@@ -312,18 +312,16 @@ private boolean cleanupSplits(final Text metaRegionName,\n       boolean noReferencesB = splitB == null;\n       \n       if (!noReferencesA) {\n-        noReferencesA =\n-          hasReferences(metaRegionName, server, info.getRegionName(), splitA, COL_SPLITA);\n+        noReferencesA = hasReferences(metaRegionName, server,\n+          info.getRegionName(), splitA, COL_SPLITA);\n       }\n       if (!noReferencesB) {\n-        noReferencesB =\n-          hasReferences(metaRegionName, server, info.getRegionName(), splitB, COL_SPLITB);\n+        noReferencesB = hasReferences(metaRegionName, server,\n+          info.getRegionName(), splitB, COL_SPLITB);\n       }\n-      if (!(noReferencesA && noReferencesB)) {\n-        \n+      if (!noReferencesA && !noReferencesB) {\n         // No references.  Remove this item from table and deleted region on\n         // disk.\n-        \n         LOG.info(\"Deleting region \" + info.getRegionName() +\n         \" because daughter splits no longer hold references\");\n         \n@@ -337,7 +335,6 @@ private boolean cleanupSplits(final Text metaRegionName,\n         b.delete(lockid, COL_SERVER);\n         b.delete(lockid, COL_STARTCODE);\n         server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n-        \n         result = true;\n       }\n       \n@@ -361,8 +358,8 @@ protected boolean hasReferences(final Text metaRegionName,\n         \n         Path [] ps = fs.listPaths(p,\n             new PathFilter () {\n-              public boolean accept(Path path) {\n-                return HStoreFile.isReference(path);\n+              public boolean accept(Path p) {\n+                return HStoreFile.isReference(p);\n               }\n             }\n         );\n@@ -394,18 +391,11 @@ protected void checkAssigned(final HRegionInfo info,\n         final String serverName, final long startCode) {\n       \n       // Skip region - if ...\n-      \n-      if(info.offLine                                       // offline\n-          || killedRegions.contains(info.regionName)        // queued for offline\n-          || regionsToDelete.contains(info.regionName)) {   // queued for delete\n-\n+      if(info.offLine                                     // offline\n+          || killedRegions.contains(info.regionName)      // queued for offline\n+          || regionsToDelete.contains(info.regionName)) { // queued for delete\n         unassignedRegions.remove(info.regionName);\n         assignAttempts.remove(info.regionName);\n-\n-        if(LOG.isDebugEnabled()) {\n-          LOG.debug(\"not assigning region: \" + info.regionName + \" (offline: \" +\n-              info.isOffline() + \", split: \" + info.isSplit() + \")\");\n-        }\n         return;\n       }\n \n@@ -416,7 +406,6 @@ protected void checkAssigned(final HRegionInfo info,\n             regionsToKill.containsKey(info.regionName)) {\n           \n           // Skip if region is on kill list\n-\n           if(LOG.isDebugEnabled()) {\n             LOG.debug(\"not assigning region (on kill list): \" + info.regionName);\n           }\n@@ -431,14 +420,8 @@ protected void checkAssigned(final HRegionInfo info,\n           && (storedInfo == null || storedInfo.getStartCode() != startCode)) {\n         \n         // The current assignment is no good; load the region.\n-\n         unassignedRegions.put(info.regionName, info);\n         assignAttempts.put(info.regionName, Long.valueOf(0L));\n-      \n-      } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Finished if \" + info.getRegionName() + \" is assigned: \" +\n-            \"unassigned: \" + unassignedRegions.containsKey(info.regionName) +\n-            \", pending: \" + pendingRegions.contains(info.regionName));\n       }\n     }\n   }\n@@ -2155,8 +2138,10 @@ boolean process() throws IOException {\n           if (rootRegionLocation.get() == null || !rootScanned) {\n             // We can't proceed until the root region is online and has been scanned\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"root region=\" + rootRegionLocation.get().toString() +\n-                  \", rootScanned=\" + rootScanned);\n+              LOG.debug(\"root region: \" + \n+                ((rootRegionLocation != null)?\n+                  rootRegionLocation.toString(): \"null\") +\n+                \", rootScanned: \" + rootScanned);\n             }\n             return false;\n           }",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMaster.java",
                "sha": "aa008764b82a3dac20df3374d1a995da96ddb68c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "patch": "@@ -243,6 +243,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    *\n    * TODO - This is kinda slow.  We need a data structure that allows for \n    * proximity-searches, not just precise-matches.\n+   * \n    * @param map\n    * @param key\n    * @param numVersions\n@@ -251,13 +252,19 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n   ArrayList<byte []> get(final TreeMap<HStoreKey, byte []> map,\n       final HStoreKey key, final int numVersions) {\n     ArrayList<byte []> result = new ArrayList<byte []>();\n-    HStoreKey curKey =\n-      new HStoreKey(key.getRow(), key.getColumn(), key.getTimestamp());\n+    // TODO: If get is of a particular version -- numVersions == 1 -- we\n+    // should be able to avoid all of the tailmap creations and iterations\n+    // below.\n+    HStoreKey curKey = new HStoreKey(key);\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(curKey);\n     for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n       HStoreKey itKey = es.getKey();\n       if (itKey.matchesRowCol(curKey)) {\n         if(HConstants.DELETE_BYTES.compareTo(es.getValue()) == 0) {\n+          // TODO: Shouldn't this be a continue rather than a break?  Perhaps\n+          // the intent is that this DELETE_BYTES is meant to suppress older\n+          // info -- see 5.4 Compactions in BigTable -- but how does this jibe\n+          // with being able to remove one version only?\n           break;\n         }\n         result.add(tailMap.get(itKey));",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "sha": "16cb3968041aaa2fb0b1328f7cfb42c8da73cc81",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 232,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 162,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -92,15 +92,13 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     // Make sure that srcA comes first; important for key-ordering during\n     // write of the merged file.\n     FileSystem fs = srcA.getFilesystem();\n-    if(srcA.getStartKey() == null) {\n-      if(srcB.getStartKey() == null) {\n+    if (srcA.getStartKey() == null) {\n+      if (srcB.getStartKey() == null) {\n         throw new IOException(\"Cannot merge two regions with null start key\");\n       }\n       // A's start key is null but B's isn't. Assume A comes before B\n-      \n-    } else if((srcB.getStartKey() == null)         // A is not null but B is\n+    } else if ((srcB.getStartKey() == null)         // A is not null but B is\n         || (srcA.getStartKey().compareTo(srcB.getStartKey()) > 0)) { // A > B\n-      \n       a = srcB;\n       b = srcA;\n     }\n@@ -113,106 +111,29 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     HTableDescriptor tabledesc = a.getTableDesc();\n     HLog log = a.getLog();\n     Path rootDir = a.getRootDir();\n-\n     Text startKey = a.getStartKey();\n     Text endKey = b.getEndKey();\n-\n     Path merges = new Path(a.getRegionDir(), MERGEDIR);\n     if(! fs.exists(merges)) {\n       fs.mkdirs(merges);\n     }\n     \n     HRegionInfo newRegionInfo\n       = new HRegionInfo(Math.abs(rand.nextLong()), tabledesc, startKey, endKey);\n-    \n     Path newRegionDir = HRegion.getRegionDir(merges, newRegionInfo.regionName);\n-\n     if(fs.exists(newRegionDir)) {\n-      throw new IOException(\"Cannot merge; target file collision at \" + newRegionDir);\n+      throw new IOException(\"Cannot merge; target file collision at \" +\n+        newRegionDir);\n     }\n \n     LOG.info(\"starting merge of regions: \" + a.getRegionName() + \" and \" +\n       b.getRegionName() + \" into new region \" + newRegionInfo.toString());\n-    \n-    // Flush each of the sources, and merge their files into a single \n-    // target for each column family.    \n-    TreeSet<HStoreFile> alreadyMerged = new TreeSet<HStoreFile>();\n-    TreeMap<Text, Vector<HStoreFile>> filesToMerge =\n-      new TreeMap<Text, Vector<HStoreFile>>();\n-    \n-    for(HStoreFile src: a.flushcache(true)) {\n-      Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-      if(v == null) {\n-        v = new Vector<HStoreFile>();\n-        filesToMerge.put(src.getColFamily(), v);\n-      }\n-      v.add(src);\n-    }\n-    \n-    for(HStoreFile src: b.flushcache(true)) {\n-      Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-      if(v == null) {\n-        v = new Vector<HStoreFile>();\n-        filesToMerge.put(src.getColFamily(), v);\n-      }\n-      v.add(src);\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"merging stores\");\n-    }\n-    \n-    for (Map.Entry<Text, Vector<HStoreFile>> es: filesToMerge.entrySet()) {\n-      Text colFamily = es.getKey();\n-      Vector<HStoreFile> srcFiles = es.getValue();\n-      HStoreFile dst = new HStoreFile(conf, merges, newRegionInfo.regionName, \n-        colFamily, Math.abs(rand.nextLong()));\n-      dst.mergeStoreFiles(srcFiles, fs, conf);\n-      alreadyMerged.addAll(srcFiles);\n-    }\n \n-    // That should have taken care of the bulk of the data.\n-    // Now close the source HRegions for good, and repeat the above to take care\n-    // of any last-minute inserts\n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"flushing changes since start of merge for region \" \n-          + a.getRegionName());\n-    }\n-\n-    filesToMerge.clear();\n-    \n-    for(HStoreFile src: a.close()) {\n-      if(! alreadyMerged.contains(src)) {\n-        Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-        if(v == null) {\n-          v = new Vector<HStoreFile>();\n-          filesToMerge.put(src.getColFamily(), v);\n-        }\n-        v.add(src);\n-      }\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"flushing changes since start of merge for region \" \n-          + b.getRegionName());\n-    }\n-    \n-    for(HStoreFile src: b.close()) {\n-      if(! alreadyMerged.contains(src)) {\n-        Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-        if(v == null) {\n-          v = new Vector<HStoreFile>();\n-          filesToMerge.put(src.getColFamily(), v);\n-        }\n-        v.add(src);\n-      }\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"merging changes since start of merge\");\n-    }\n-    \n-    for (Map.Entry<Text, Vector<HStoreFile>> es : filesToMerge.entrySet()) {\n+    Map<Text, Vector<HStoreFile>> byFamily =\n+      new TreeMap<Text, Vector<HStoreFile>>();\n+    byFamily = filesByFamily(byFamily, a.close());\n+    byFamily = filesByFamily(byFamily, b.close());\n+    for (Map.Entry<Text, Vector<HStoreFile>> es : byFamily.entrySet()) {\n       Text colFamily = es.getKey();\n       Vector<HStoreFile> srcFiles = es.getValue();\n       HStoreFile dst = new HStoreFile(conf, merges, newRegionInfo.regionName,\n@@ -233,6 +154,25 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     \n     return dstRegion;\n   }\n+  \n+  /*\n+   * Fills a map with a vector of store files keyed by column family. \n+   * @param byFamily Map to fill.\n+   * @param storeFiles Store files to process.\n+   * @return Returns <code>byFamily</code>\n+   */\n+  private static Map<Text, Vector<HStoreFile>> filesByFamily(\n+      Map<Text, Vector<HStoreFile>> byFamily, Vector<HStoreFile> storeFiles) {\n+    for(HStoreFile src: storeFiles) {\n+      Vector<HStoreFile> v = byFamily.get(src.getColFamily());\n+      if(v == null) {\n+        v = new Vector<HStoreFile>();\n+        byFamily.put(src.getColFamily(), v);\n+      }\n+      v.add(src);\n+    }\n+    return byFamily;\n+  }\n \n   //////////////////////////////////////////////////////////////////////////////\n   // Members\n@@ -254,19 +194,19 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   Path regiondir;\n \n   static class WriteState {\n-    volatile boolean writesOngoing;\n-    volatile boolean writesEnabled;\n-    WriteState() {\n-      this.writesOngoing = true;\n-      this.writesEnabled = true;\n-    }\n+    // Set while a memcache flush is happening.\n+    volatile boolean flushing = false;\n+    // Set while a compaction is running.\n+    volatile boolean compacting = false;\n+    // Gets set by last flush before close.  If set, cannot compact or flush\n+    // again.\n+    volatile boolean writesEnabled = true;\n   }\n   \n   volatile WriteState writestate = new WriteState();\n \n   final int memcacheFlushSize;\n   final int blockingMemcacheSize;\n-  int compactionThreshold = 0;\n   private final HLocking lock = new HLocking();\n   private long desiredMaxFileSize;\n   private final long maxSequenceId;\n@@ -297,15 +237,12 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf, \n       HRegionInfo regionInfo, Path initialFiles)\n   throws IOException {\n-    \n     this.rootDir = rootDir;\n     this.log = log;\n     this.fs = fs;\n     this.conf = conf;\n     this.regionInfo = regionInfo;\n     this.memcache = new HMemcache();\n-    this.writestate.writesOngoing = true;\n-    this.writestate.writesEnabled = true;\n \n     // Declare the regionName.  This is a unique string for the region, used to \n     // build a unique filename.\n@@ -319,7 +256,6 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     }\n \n     // Load in all the HStores.\n-\n     long maxSeqId = -1;\n     for(Map.Entry<Text, HColumnDescriptor> e :\n         this.regionInfo.tableDesc.families().entrySet()) {\n@@ -357,17 +293,12 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     this.blockingMemcacheSize = this.memcacheFlushSize *\n       conf.getInt(\"hbase.hregion.memcache.block.multiplier\", 2);\n     \n-    // By default, we compact the region if an HStore has more than\n-    // MIN_COMMITS_FOR_COMPACTION map files\n-    this.compactionThreshold =\n-      conf.getInt(\"hbase.hregion.compactionThreshold\", 3);\n-    \n     // By default we split region if a file > DEFAULT_MAX_FILE_SIZE.\n     this.desiredMaxFileSize =\n       conf.getLong(\"hbase.hregion.max.filesize\", DEFAULT_MAX_FILE_SIZE);\n \n     // HRegion is ready to go!\n-    this.writestate.writesOngoing = false;\n+    this.writestate.compacting = false;\n     LOG.info(\"region \" + this.regionInfo.regionName + \" available\");\n   }\n   \n@@ -411,56 +342,48 @@ boolean isClosed() {\n    * \n    * @param abort true if server is aborting (only during testing)\n    * @return Vector of all the storage files that the HRegion's component \n-   * HStores make use of.  It's a list of HStoreFile objects.\n+   * HStores make use of.  It's a list of HStoreFile objects.  Can be null if\n+   * we are not to close at this time or we are already closed.\n    * \n    * @throws IOException\n    */\n   Vector<HStoreFile> close(boolean abort) throws IOException {\n     if (isClosed()) {\n       LOG.info(\"region \" + this.regionInfo.regionName + \" already closed\");\n-      return new Vector<HStoreFile>();\n+      return null;\n     }\n     lock.obtainWriteLock();\n     try {\n-      boolean shouldClose = false;\n       synchronized(writestate) {\n-        while(writestate.writesOngoing) {\n+        while(writestate.compacting || writestate.flushing) {\n           try {\n             writestate.wait();\n           } catch (InterruptedException iex) {\n             // continue\n           }\n         }\n-        writestate.writesOngoing = true;\n-        shouldClose = true;\n-      }\n-\n-      if(!shouldClose) {\n-        return null;\n+        // Disable compacting and flushing by background threads for this\n+        // region.\n+        writestate.writesEnabled = false;\n       }\n       \n       // Write lock means no more row locks can be given out.  Wait on\n       // outstanding row locks to come in before we close so we do not drop\n       // outstanding updates.\n       waitOnRowLocks();\n-\n-      Vector<HStoreFile> allHStoreFiles = null;\n+      \n       if (!abort) {\n         // Don't flush the cache if we are aborting during a test.\n-        allHStoreFiles = internalFlushcache();\n+        internalFlushcache();\n       }\n+      \n+      Vector<HStoreFile> result = new Vector<HStoreFile>();\n       for (HStore store: stores.values()) {\n-        store.close();\n-      }\n-      try {\n-        return allHStoreFiles;\n-      } finally {\n-        synchronized (writestate) {\n-          writestate.writesOngoing = false;\n-        }\n-        this.closed.set(true);\n-        LOG.info(\"closed \" + this.regionInfo.regionName);\n+        result.addAll(store.close());\n       }\n+      this.closed.set(true);\n+      LOG.info(\"closed \" + this.regionInfo.regionName);\n+      return result;\n     } finally {\n       lock.releaseWriteLock();\n     }\n@@ -527,6 +450,7 @@ boolean isClosed() {\n       HStoreFile a = new HStoreFile(this.conf, splits,\n         regionAInfo.regionName, h.getColFamily(), Math.abs(rand.nextLong()),\n         aReference);\n+      // Reference to top half of the hsf store file.\n       HStoreFile.Reference bReference = new HStoreFile.Reference(\n         getRegionName(), h.getFileId(), new HStoreKey(midKey),\n         HStoreFile.Range.top);\n@@ -721,12 +645,10 @@ boolean needsCompaction() {\n     boolean needsCompaction = false;\n     this.lock.obtainReadLock();\n     try {\n-      for(HStore store: stores.values()) {\n-        if(store.getNMaps() > this.compactionThreshold) {\n+      for (HStore store: stores.values()) {\n+        if (store.needsCompaction()) {\n           needsCompaction = true;\n-          LOG.info(getRegionName().toString() + \" needs compaction because \" +\n-            store.getNMaps() + \" store files present and threshold is \" +\n-            this.compactionThreshold);\n+          LOG.info(store.toString() + \" needs compaction\");\n           break;\n         }\n       }\n@@ -756,9 +678,9 @@ boolean compactStores() throws IOException {\n     lock.obtainReadLock();\n     try {\n       synchronized (writestate) {\n-        if ((!writestate.writesOngoing) &&\n+        if ((!writestate.compacting) &&\n             writestate.writesEnabled) {\n-          writestate.writesOngoing = true;\n+          writestate.compacting = true;\n           shouldCompact = true;\n         }\n       }\n@@ -783,7 +705,7 @@ boolean compactStores() throws IOException {\n     } finally {\n       lock.releaseReadLock();\n       synchronized (writestate) {\n-        writestate.writesOngoing = false;\n+        writestate.compacting = false;\n         writestate.notifyAll();\n       }\n     }\n@@ -825,23 +747,17 @@ void optionallyFlush() throws IOException {\n    * close() the HRegion shortly, so the HRegion should not take on any new and \n    * potentially long-lasting disk operations. This flush() should be the final\n    * pre-close() disk operation.\n-   * \n-   * @return List of store files including new flushes, if any.  If no flushes\n-   * because  memcache is null, returns all current store files.  Returns\n-   * null if no flush (Writes are going on elsewhere -- concurrently we are\n-   * compacting or splitting).\n    */\n-  Vector<HStoreFile> flushcache(boolean disableFutureWrites)\n+  void flushcache(boolean disableFutureWrites)\n   throws IOException {\n     if (this.closed.get()) {\n-      return null;\n+      return;\n     }\n     this.noFlushCount = 0;\n     boolean shouldFlush = false;\n     synchronized(writestate) {\n-      if((!writestate.writesOngoing) &&\n-          writestate.writesEnabled) {\n-        writestate.writesOngoing = true;\n+      if((!writestate.flushing) && writestate.writesEnabled) {\n+        writestate.flushing = true;\n         shouldFlush = true;\n         if(disableFutureWrites) {\n           writestate.writesEnabled = false;\n@@ -854,14 +770,14 @@ void optionallyFlush() throws IOException {\n         LOG.debug(\"NOT flushing memcache for region \" +\n           this.regionInfo.regionName);\n       }\n-      return null;  \n+      return;  \n     }\n     \n     try {\n-      return internalFlushcache();\n+      internalFlushcache();\n     } finally {\n       synchronized (writestate) {\n-        writestate.writesOngoing = false;\n+        writestate.flushing = false;\n         writestate.notifyAll();\n       }\n     }\n@@ -892,11 +808,8 @@ void optionallyFlush() throws IOException {\n    * routes.\n    * \n    * <p> This method may block for some time.\n-   * \n-   * @return List of store files including just-made new flushes per-store. If\n-   * not flush, returns list of all store files.\n    */\n-  Vector<HStoreFile> internalFlushcache() throws IOException {\n+  void internalFlushcache() throws IOException {\n     long startTime = -1;\n     if(LOG.isDebugEnabled()) {\n       startTime = System.currentTimeMillis();\n@@ -917,7 +830,7 @@ void optionallyFlush() throws IOException {\n     HMemcache.Snapshot retval = memcache.snapshotMemcacheForLog(log);\n     if(retval == null || retval.memcacheSnapshot == null) {\n       LOG.debug(\"Finished memcache flush; empty snapshot\");\n-      return getAllStoreFiles();\n+      return;\n     }\n     long logCacheFlushId = retval.sequenceId;\n     if(LOG.isDebugEnabled()) {\n@@ -929,11 +842,8 @@ void optionallyFlush() throws IOException {\n     // A.  Flush memcache to all the HStores.\n     // Keep running vector of all store files that includes both old and the\n     // just-made new flush store file.\n-    Vector<HStoreFile> allHStoreFiles = new Vector<HStoreFile>();\n     for(HStore hstore: stores.values()) {\n-      Vector<HStoreFile> hstoreFiles\n-        = hstore.flushCache(retval.memcacheSnapshot, retval.sequenceId);\n-      allHStoreFiles.addAll(0, hstoreFiles);\n+      hstore.flushCache(retval.memcacheSnapshot, retval.sequenceId);\n     }\n \n     // B.  Write a FLUSHCACHE-COMPLETE message to the log.\n@@ -958,13 +868,12 @@ void optionallyFlush() throws IOException {\n         this.regionInfo.regionName + \" in \" +\n           (System.currentTimeMillis() - startTime) + \"ms\");\n     }\n-    return allHStoreFiles;\n   }\n   \n   private Vector<HStoreFile> getAllStoreFiles() {\n     Vector<HStoreFile> allHStoreFiles = new Vector<HStoreFile>();\n     for(HStore hstore: stores.values()) {\n-      Vector<HStoreFile> hstoreFiles = hstore.getAllMapFiles();\n+      Vector<HStoreFile> hstoreFiles = hstore.getAllStoreFiles();\n       allHStoreFiles.addAll(0, hstoreFiles);\n     }\n     return allHStoreFiles;\n@@ -1020,7 +929,6 @@ void optionallyFlush() throws IOException {\n       }\n \n       // If unavailable in memcache, check the appropriate HStore\n-\n       Text colFamily = HStoreKey.extractFamily(key.getColumn());\n       HStore targetStore = stores.get(colFamily);\n       if(targetStore == null) {",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "0debc8323e320bcc1c5dfad44c853376a4daeaaf",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 9,
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "patch": "@@ -158,7 +158,8 @@ public void run() {\n           try {\n             for(HRegion cur: regionsToCheck) {\n               if(cur.isClosed()) {\n-                continue;                               // Skip if closed\n+                // Skip if closed\n+                continue;\n               }\n               if (cur.needsCompaction()) {\n                 cur.compactStores();\n@@ -272,10 +273,6 @@ private void split(final HRegion region, final Text midKey)\n   protected final Integer cacheFlusherLock = new Integer(0);\n   \n   /* Runs periodically to flush memcache.\n-   * \n-   * Memcache flush is also called just before compaction and just before\n-   * split so memcache is best prepared for the the long trip across\n-   * compactions/splits during which it will not be able to flush to disk.\n    */\n   class Flusher implements Runnable {\n     /**\n@@ -286,9 +283,7 @@ public void run() {\n         long startTime = System.currentTimeMillis();\n \n         synchronized(cacheFlusherLock) {\n-\n           // Grab a list of items to flush\n-\n           Vector<HRegion> toFlush = new Vector<HRegion>();\n           lock.readLock().lock();\n           try {\n@@ -837,6 +832,7 @@ void reportSplit(HRegionInfo oldRegion, HRegionInfo newRegionA,\n   BlockingQueue<ToDoEntry> toDo;\n   private Worker worker;\n   private Thread workerThread;\n+  \n   /** Thread that performs long running requests from the master */\n   class Worker implements Runnable {\n     void stop() {\n@@ -910,7 +906,6 @@ void openRegion(HRegionInfo regionInfo) throws IOException {\n     HRegion region = onlineRegions.get(regionInfo.regionName);\n     if(region == null) {\n       region = new HRegion(rootDir, log, fs, conf, regionInfo, null);\n-\n       this.lock.writeLock().lock();\n       try {\n         this.log.setSequenceNumber(region.getMaxSequenceId());\n@@ -1193,7 +1188,7 @@ protected HRegion getRegion(final Text regionName)\n    * @return {@link HRegion} for <code>regionName</code>\n    * @throws NotServingRegionException\n    */\n-  protected HRegion getRegion(final Text regionName,\n+  protected HRegion getRegion(final Text regionName, \n       final boolean checkRetiringRegions)\n   throws NotServingRegionException {\n     HRegion region = null;",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "sha": "ab64197d5042d0ea86389be87781d356c0bb4d60",
                "status": "modified"
            },
            {
                "additions": 346,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 509,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 163,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -30,6 +30,7 @@\n import java.util.Random;\n import java.util.TreeMap;\n import java.util.Vector;\n+import java.util.Map.Entry;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -42,6 +43,7 @@\n import org.apache.hadoop.io.MapFile;\n import org.apache.hadoop.io.SequenceFile;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n import org.apache.hadoop.io.WritableComparable;\n import org.apache.hadoop.util.StringUtils;\n import org.onelab.filter.BloomFilter;\n@@ -92,6 +94,8 @@\n   Random rand = new Random();\n   \n   private long maxSeqId;\n+  \n+  private int compactionThreshold;\n \n   /**\n    * An HStore is a set of zero or more MapFiles, which stretch backwards over \n@@ -164,7 +168,7 @@\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"starting \" + this.storeName +\n-        ((reconstructionLog == null)?\n+        ((reconstructionLog == null || !fs.exists(reconstructionLog))?\n           \" (no reconstruction log)\": \" with reconstruction log: \" +\n           reconstructionLog.toString()));\n     }\n@@ -215,19 +219,19 @@\n     }\n     \n     doReconstructionLog(reconstructionLog, maxSeqId);\n-    this.maxSeqId += 1;\n \n-    // Compact all the MapFiles into a single file.  The resulting MapFile \n-    // should be \"timeless\"; that is, it should not have an associated seq-ID, \n-    // because all log messages have been reflected in the TreeMaps at this\n-    // point.  \n-    //\n-    // TODO: Only do the compaction if we are over a threshold, not\n-    // every time. Not necessary if only two or three store files.  Fix after\n-    // revamp of compaction.\n-    if(storefiles.size() > 1) {\n-      compactHelper(true);\n-    }\n+    // By default, we compact if an HStore has more than\n+    // MIN_COMMITS_FOR_COMPACTION map files\n+    this.compactionThreshold =\n+      conf.getInt(\"hbase.hstore.compactionThreshold\", 3);\n+    \n+    // We used to compact in here before bringing the store online.  Instead\n+    // get it online quick even if it needs compactions so we can start\n+    // taking updates as soon as possible (Once online, can take updates even\n+    // during a compaction).\n+\n+    // Move maxSeqId on by one. Why here?  And not in HRegion?\n+    this.maxSeqId += 1;\n     \n     // Finally, start up all the map readers! (There should be just one at this \n     // point, as we've compacted them all.)\n@@ -253,10 +257,6 @@ private void doReconstructionLog(final Path reconstructionLog,\n       final long maxSeqID)\n   throws UnsupportedEncodingException, IOException {\n     if (reconstructionLog == null || !fs.exists(reconstructionLog)) {\n-      if (reconstructionLog != null && !fs.exists(reconstructionLog)) {\n-        LOG.warn(\"Passed reconstruction log \" + reconstructionLog +\n-          \" does not exist\");\n-      }\n       // Nothing to do.\n       return;\n     }\n@@ -397,15 +397,18 @@ private void flushBloomFilter() throws IOException {\n    * Close all the MapFile readers\n    * @throws IOException\n    */\n-  void close() throws IOException {\n+  Vector<HStoreFile> close() throws IOException {\n+    Vector<HStoreFile> result = null;\n     this.lock.obtainWriteLock();\n     try {\n       for (MapFile.Reader reader: this.readers.values()) {\n         reader.close();\n       }\n       this.readers.clear();\n+      result = new Vector<HStoreFile>(storefiles.values());\n       this.storefiles.clear();\n       LOG.info(\"closed \" + this.storeName);\n+      return result;\n     } finally {\n       this.lock.releaseWriteLock();\n     }\n@@ -428,16 +431,15 @@ void close() throws IOException {\n    *\n    * @param inputCache memcache to flush\n    * @param logCacheFlushId flush sequence number\n-   * @return Vector of all the HStoreFiles in use\n    * @throws IOException\n    */\n-  Vector<HStoreFile> flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n     final long logCacheFlushId)\n   throws IOException {\n-    return flushCacheHelper(inputCache, logCacheFlushId, true);\n+    flushCacheHelper(inputCache, logCacheFlushId, true);\n   }\n   \n-  Vector<HStoreFile> flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n       long logCacheFlushId, boolean addToAvailableMaps)\n   throws IOException {\n     synchronized(flushLock) {\n@@ -447,12 +449,31 @@ void close() throws IOException {\n       String name = flushedFile.toString();\n       MapFile.Writer out = flushedFile.getWriter(this.fs, this.compression,\n         this.bloomFilter);\n+      \n+      // hbase.hstore.compact.on.flush=true enables picking up an existing\n+      // HStoreFIle from disk interlacing the memcache flush compacting as we\n+      // go.  The notion is that interlacing would take as long as a pure\n+      // flush with the added benefit of having one less file in the store. \n+      // Experiments show that it takes two to three times the amount of time\n+      // flushing -- more column families makes it so the two timings come\n+      // closer together -- but it also complicates the flush. Disabled for\n+      // now.  Needs work picking which file to interlace (favor references\n+      // first, etc.)\n+      //\n+      // Related, looks like 'merging compactions' in BigTable paper interlaces\n+      // a memcache flush.  We don't.\n       try {\n-        for (Map.Entry<HStoreKey, byte []> es: inputCache.entrySet()) {\n-          HStoreKey curkey = es.getKey();\n-          if (this.familyName.\n-              equals(HStoreKey.extractFamily(curkey.getColumn()))) {\n-            out.append(curkey, new ImmutableBytesWritable(es.getValue()));\n+        if (this.conf.getBoolean(\"hbase.hstore.compact.on.flush\", false) &&\n+            this.storefiles.size() > 0) {\n+          compact(out, inputCache.entrySet().iterator(),\n+              this.readers.get(this.storefiles.firstKey()));\n+        } else {\n+          for (Map.Entry<HStoreKey, byte []> es: inputCache.entrySet()) {\n+            HStoreKey curkey = es.getKey();\n+            if (this.familyName.\n+                equals(HStoreKey.extractFamily(curkey.getColumn()))) {\n+              out.append(curkey, new ImmutableBytesWritable(es.getValue()));\n+            }\n           }\n         }\n       } finally {\n@@ -486,14 +507,14 @@ void close() throws IOException {\n           this.lock.releaseWriteLock();\n         }\n       }\n-      return getAllMapFiles();\n+      return;\n     }\n   }\n \n   /**\n    * @return - vector of all the HStore files in use\n    */\n-  Vector<HStoreFile> getAllMapFiles() {\n+  Vector<HStoreFile> getAllStoreFiles() {\n     this.lock.obtainReadLock();\n     try {\n       return new Vector<HStoreFile>(storefiles.values());\n@@ -505,6 +526,14 @@ void close() throws IOException {\n   //////////////////////////////////////////////////////////////////////////////\n   // Compaction\n   //////////////////////////////////////////////////////////////////////////////\n+  \n+  /**\n+   * @return True if this store needs compaction.\n+   */\n+  public boolean needsCompaction() {\n+    return this.storefiles != null &&\n+    this.storefiles.size() >= this.compactionThreshold;\n+  }\n \n   /**\n    * Compact the back-HStores.  This method may take some time, so the calling \n@@ -528,11 +557,24 @@ void compact() throws IOException {\n     compactHelper(false);\n   }\n   \n-  void compactHelper(boolean deleteSequenceInfo) throws IOException {\n+  void compactHelper(final boolean deleteSequenceInfo)\n+  throws IOException {\n+    compactHelper(deleteSequenceInfo, -1);\n+  }\n+  \n+  /* \n+   * @param deleteSequenceInfo True if we are to set the sequence number to -1\n+   * on compacted file.\n+   * @param maxSeenSeqID We may have already calculated the maxSeenSeqID.  If\n+   * so, pass it here.  Otherwise, pass -1 and it will be calculated inside in\n+   * this method.\n+   * @throws IOException\n+   */\n+  void compactHelper(final boolean deleteSequenceInfo, long maxSeenSeqID)\n+  throws IOException {\n     synchronized(compactLock) {\n       Path curCompactStore =\n         HStoreFile.getHStoreDir(compactdir, regionName, familyName);\n-      fs.mkdirs(curCompactStore);\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"started compaction of \" + storefiles.size() + \" files in \" +\n           curCompactStore.toString());\n@@ -547,28 +589,32 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n           this.lock.releaseWriteLock();\n         }\n \n-        // Compute the max-sequenceID seen in any of the to-be-compacted\n-        // TreeMaps\n-        long maxSeenSeqID = -1;\n-        for (HStoreFile hsf: toCompactFiles) {\n-          long seqid = hsf.loadInfo(fs);\n-          if(seqid > 0) {\n-            if(seqid > maxSeenSeqID) {\n-              maxSeenSeqID = seqid;\n-            }\n-          }\n-        }\n-\n-        HStoreFile compactedOutputFile \n-          = new HStoreFile(conf, compactdir, regionName, familyName, -1);\n-        if(toCompactFiles.size() == 1) {\n-          // TODO: Only rewrite if NOT a HSF reference file.\n-          if(LOG.isDebugEnabled()) {\n+        HStoreFile compactedOutputFile =\n+          new HStoreFile(conf, compactdir, regionName, familyName, -1);\n+        if (toCompactFiles.size() < 1 ||\n+            (toCompactFiles.size() == 1 &&\n+              !toCompactFiles.get(0).isReference())) {\n+          if (LOG.isDebugEnabled()) {\n             LOG.debug(\"nothing to compact for \" + this.storeName);\n           }\n-          HStoreFile hsf = toCompactFiles.elementAt(0);\n-          if(hsf.loadInfo(fs) == -1) {\n-            return;\n+          if (deleteSequenceInfo && toCompactFiles.size() == 1) {\n+            toCompactFiles.get(0).writeInfo(fs, -1);\n+          }\n+          return;\n+        }\n+        \n+        fs.mkdirs(curCompactStore);\n+        \n+        // Compute the max-sequenceID seen in any of the to-be-compacted\n+        // TreeMaps if it hasn't been passed in to us.\n+        if (maxSeenSeqID == -1) {\n+          for (HStoreFile hsf: toCompactFiles) {\n+            long seqid = hsf.loadInfo(fs);\n+            if(seqid > 0) {\n+              if(seqid > maxSeenSeqID) {\n+                maxSeenSeqID = seqid;\n+              }\n+            }\n           }\n         }\n \n@@ -577,108 +623,11 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n           compactedOutputFile.getWriter(this.fs, this.compression,\n             this.bloomFilter);\n         try {\n-          // We create a new set of MapFile.Reader objects so we don't screw up \n-          // the caching associated with the currently-loaded ones.\n-          //\n-          // Our iteration-based access pattern is practically designed to ruin \n-          // the cache.\n-          //\n-          // We work by opening a single MapFile.Reader for each file, and \n-          // iterating through them in parallel.  We always increment the \n-          // lowest-ranked one.  Updates to a single row/column will appear \n-          // ranked by timestamp.  This allows us to throw out deleted values or\n-          // obsolete versions.\n-          MapFile.Reader[] rdrs = new MapFile.Reader[toCompactFiles.size()];\n-          HStoreKey[] keys = new HStoreKey[toCompactFiles.size()];\n-          ImmutableBytesWritable[] vals =\n-            new ImmutableBytesWritable[toCompactFiles.size()];\n-          boolean[] done = new boolean[toCompactFiles.size()];\n-          int pos = 0;\n-          for(HStoreFile hsf: toCompactFiles) {\n-            rdrs[pos] = hsf.getReader(this.fs, this.bloomFilter);\n-            keys[pos] = new HStoreKey();\n-            vals[pos] = new ImmutableBytesWritable();\n-            done[pos] = false;\n-            pos++;\n-          }\n-\n-          // Now, advance through the readers in order.  This will have the\n-          // effect of a run-time sort of the entire dataset.\n-          int numDone = 0;\n-          for(int i = 0; i < rdrs.length; i++) {\n-            rdrs[i].reset();\n-            done[i] = ! rdrs[i].next(keys[i], vals[i]);\n-            if(done[i]) {\n-              numDone++;\n-            }\n-          }\n-          \n-          int timesSeen = 0;\n-          Text lastRow = new Text();\n-          Text lastColumn = new Text();\n-          while(numDone < done.length) {\n-            // Find the reader with the smallest key\n-            int smallestKey = -1;\n-            for(int i = 0; i < rdrs.length; i++) {\n-              if(done[i]) {\n-                continue;\n-              }\n-              \n-              if(smallestKey < 0) {\n-                smallestKey = i;\n-              } else {\n-                if(keys[i].compareTo(keys[smallestKey]) < 0) {\n-                  smallestKey = i;\n-                }\n-              }\n-            }\n-\n-            // Reflect the current key/val in the output\n-            HStoreKey sk = keys[smallestKey];\n-            if(lastRow.equals(sk.getRow())\n-                && lastColumn.equals(sk.getColumn())) {\n-              timesSeen++;\n-            } else {\n-              timesSeen = 1;\n-            }\n-            \n-            if(timesSeen <= family.getMaxVersions()) {\n-              // Keep old versions until we have maxVersions worth.\n-              // Then just skip them.\n-              if(sk.getRow().getLength() != 0\n-                  && sk.getColumn().getLength() != 0) {\n-                // Only write out objects which have a non-zero length key and\n-                // value\n-                compactedOut.append(sk, vals[smallestKey]);\n-              }\n-            }\n-\n-            // TODO: I don't know what to do about deleted values.  I currently \n-            // include the fact that the item was deleted as a legitimate \n-            // \"version\" of the data.  Maybe it should just drop the deleted\n-            // val?\n-\n-            // Update last-seen items\n-            lastRow.set(sk.getRow());\n-            lastColumn.set(sk.getColumn());\n-\n-            // Advance the smallest key.  If that reader's all finished, then \n-            // mark it as done.\n-            if(! rdrs[smallestKey].next(keys[smallestKey],\n-                vals[smallestKey])) {\n-              done[smallestKey] = true;\n-              rdrs[smallestKey].close();\n-              numDone++;\n-            }\n-          }\n+          compact(compactedOut, toCompactFiles);\n         } finally {\n           compactedOut.close();\n         }\n \n-        if(LOG.isDebugEnabled()) {\n-          LOG.debug(\"writing new compacted HStore \" + compactedOutputFile);\n-        }\n-\n         // Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.\n         if((! deleteSequenceInfo) && maxSeenSeqID >= 0) {\n           compactedOutputFile.writeInfo(fs, maxSeenSeqID);\n@@ -691,8 +640,7 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n         DataOutputStream out = new DataOutputStream(fs.create(filesToReplace));\n         try {\n           out.writeInt(toCompactFiles.size());\n-          for(Iterator<HStoreFile> it = toCompactFiles.iterator(); it.hasNext(); ) {\n-            HStoreFile hsf = it.next();\n+          for(HStoreFile hsf: toCompactFiles) {\n             hsf.write(out);\n           }\n         } finally {\n@@ -706,7 +654,207 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n         // Move the compaction into place.\n         processReadyCompaction();\n       } finally {\n-        fs.delete(compactdir);\n+        if (fs.exists(compactdir)) {\n+          fs.delete(compactdir);\n+        }\n+      }\n+    }\n+  }\n+  \n+  /*\n+   * Compact passed <code>toCompactFiles</code> into <code>compactedOut</code>. \n+   * We create a new set of MapFile.Reader objects so we don't screw up \n+   * the caching associated with the currently-loaded ones. Our\n+   * iteration-based access pattern is practically designed to ruin \n+   * the cache.\n+   *\n+   * We work by opening a single MapFile.Reader for each file, and \n+   * iterating through them in parallel.  We always increment the \n+   * lowest-ranked one.  Updates to a single row/column will appear \n+   * ranked by timestamp.  This allows us to throw out deleted values or\n+   * obsolete versions.\n+   * @param compactedOut\n+   * @param toCompactFiles\n+   * @throws IOException\n+   */\n+  void compact(final MapFile.Writer compactedOut,\n+      final Vector<HStoreFile> toCompactFiles)\n+  throws IOException {\n+    int size = toCompactFiles.size();\n+    CompactionReader[] rdrs = new CompactionReader[size];\n+    int index = 0;\n+    for (HStoreFile hsf: toCompactFiles) {\n+      try {\n+        rdrs[index++] =\n+          new MapFileCompactionReader(hsf.getReader(fs, bloomFilter));\n+      } catch (IOException e) {\n+        // Add info about which file threw exception. It may not be in the\n+        // exception message so output a message here where we know the\n+        // culprit.\n+        LOG.warn(\"Failed with \" + e.toString() + \": \" + hsf.toString() +\n+          (hsf.isReference()? \" \" + hsf.getReference().toString(): \"\"));\n+        throw e;\n+      }\n+    }\n+    try {\n+      compact(compactedOut, rdrs);\n+    } finally {\n+      for (int i = 0; i < rdrs.length; i++) {\n+        if (rdrs[i] != null) {\n+          try {\n+            rdrs[i].close();\n+          } catch (IOException e) {\n+            LOG.warn(\"Exception closing reader\", e);\n+          }\n+        }\n+      }\n+    }\n+  }\n+  \n+  interface CompactionReader {\n+    public void close() throws IOException;\n+    public boolean next(WritableComparable key, Writable val)\n+      throws IOException;\n+    public void reset() throws IOException;\n+  }\n+  \n+  class MapFileCompactionReader implements CompactionReader {\n+    final MapFile.Reader reader;\n+    \n+    MapFileCompactionReader(final MapFile.Reader r) {\n+      this.reader = r;\n+    }\n+    \n+    public void close() throws IOException {\n+      this.reader.close();\n+    }\n+\n+    public boolean next(WritableComparable key, Writable val)\n+    throws IOException {\n+      return this.reader.next(key, val);\n+    }\n+\n+    public void reset() throws IOException {\n+      this.reader.reset();\n+    }\n+  }\n+  \n+  void compact(final MapFile.Writer compactedOut,\n+      final Iterator<Entry<HStoreKey, byte []>> iterator,\n+      final MapFile.Reader reader)\n+  throws IOException {\n+    // Make an instance of a CompactionReader that wraps the iterator.\n+    CompactionReader cr = new CompactionReader() {\n+      public boolean next(WritableComparable key, Writable val)\n+          throws IOException {\n+        boolean result = false;\n+        while (iterator.hasNext()) {\n+          Entry<HStoreKey, byte []> e = iterator.next();\n+          HStoreKey hsk = e.getKey();\n+          if (familyName.equals(HStoreKey.extractFamily(hsk.getColumn()))) {\n+            ((HStoreKey)key).set(hsk);\n+            ((ImmutableBytesWritable)val).set(e.getValue());\n+            result = true;\n+            break;\n+          }\n+        }\n+        return result;\n+      }\n+\n+      @SuppressWarnings(\"unused\")\n+      public void reset() throws IOException {\n+        // noop.\n+      }\n+      \n+      @SuppressWarnings(\"unused\")\n+      public void close() throws IOException {\n+        // noop.\n+      }\n+    };\n+    \n+    compact(compactedOut,\n+      new CompactionReader [] {cr, new MapFileCompactionReader(reader)});\n+  }\n+  \n+  void compact(final MapFile.Writer compactedOut,\n+      final CompactionReader [] rdrs)\n+  throws IOException {\n+    HStoreKey[] keys = new HStoreKey[rdrs.length];\n+    ImmutableBytesWritable[] vals = new ImmutableBytesWritable[rdrs.length];\n+    boolean[] done = new boolean[rdrs.length];\n+    for(int i = 0; i < rdrs.length; i++) {\n+      keys[i] = new HStoreKey();\n+      vals[i] = new ImmutableBytesWritable();\n+      done[i] = false;\n+    }\n+\n+    // Now, advance through the readers in order.  This will have the\n+    // effect of a run-time sort of the entire dataset.\n+    int numDone = 0;\n+    for(int i = 0; i < rdrs.length; i++) {\n+      rdrs[i].reset();\n+      done[i] = ! rdrs[i].next(keys[i], vals[i]);\n+      if(done[i]) {\n+        numDone++;\n+      }\n+    }\n+\n+    int timesSeen = 0;\n+    Text lastRow = new Text();\n+    Text lastColumn = new Text();\n+    while(numDone < done.length) {\n+      // Find the reader with the smallest key\n+      int smallestKey = -1;\n+      for(int i = 0; i < rdrs.length; i++) {\n+        if(done[i]) {\n+          continue;\n+        }\n+        if(smallestKey < 0) {\n+          smallestKey = i;\n+        } else {\n+          if(keys[i].compareTo(keys[smallestKey]) < 0) {\n+            smallestKey = i;\n+          }\n+        }\n+      }\n+\n+      // Reflect the current key/val in the output\n+      HStoreKey sk = keys[smallestKey];\n+      if(lastRow.equals(sk.getRow())\n+          && lastColumn.equals(sk.getColumn())) {\n+        timesSeen++;\n+      } else {\n+        timesSeen = 1;\n+      }\n+\n+      if(timesSeen <= family.getMaxVersions()) {\n+        // Keep old versions until we have maxVersions worth.\n+        // Then just skip them.\n+        if(sk.getRow().getLength() != 0\n+            && sk.getColumn().getLength() != 0) {\n+          // Only write out objects which have a non-zero length key and\n+          // value\n+          compactedOut.append(sk, vals[smallestKey]);\n+        }\n+      }\n+\n+      // TODO: I don't know what to do about deleted values.  I currently \n+      // include the fact that the item was deleted as a legitimate \n+      // \"version\" of the data.  Maybe it should just drop the deleted\n+      // val?\n+\n+      // Update last-seen items\n+      lastRow.set(sk.getRow());\n+      lastColumn.set(sk.getColumn());\n+\n+      // Advance the smallest key.  If that reader's all finished, then \n+      // mark it as done.\n+      if(!rdrs[smallestKey].next(keys[smallestKey],\n+          vals[smallestKey])) {\n+        done[smallestKey] = true;\n+        rdrs[smallestKey].close();\n+        rdrs[smallestKey] = null;\n+        numDone++;\n       }\n     }\n   }\n@@ -773,21 +921,19 @@ void processReadyCompaction() throws IOException {\n         }\n       }\n \n+      Vector<HStoreFile> toDelete = new Vector<HStoreFile>(keys.size());\n       for (Long key: keys) {\n         MapFile.Reader reader = this.readers.remove(key);\n         if (reader != null) {\n           reader.close();\n         }\n         HStoreFile hsf = this.storefiles.remove(key);\n-        // 4. Delete all old files, no longer needed\n-        hsf.delete();\n-      }\n-      if(LOG.isDebugEnabled()) {\n-        LOG.debug(\"deleted \" + toCompactFiles.size() + \" old file(s)\");\n+        // 4. Add to the toDelete files all old files, no longer needed\n+        toDelete.add(hsf);\n       }\n       \n-      // What if we fail now?  The above deletes will fail silently. We'd better\n-      // make sure not to write out any new files with the same names as \n+      // What if we fail now?  The above deletes will fail silently. We'd\n+      // better make sure not to write out any new files with the same names as \n       // something we delete, though.\n \n       // 5. Moving the new MapFile into place\n@@ -800,17 +946,30 @@ void processReadyCompaction() throws IOException {\n           compactdir.toString() +\n           \" to \" + finalCompactedFile.toString() + \" in \" + dir.toString());\n       }\n-      compactedFile.rename(this.fs, finalCompactedFile);\n+      if (!compactedFile.rename(this.fs, finalCompactedFile)) {\n+        LOG.error(\"Failed move of compacted file \" +\n+          finalCompactedFile.toString());\n+        return;\n+      }\n+      \n+      // Safe to delete now compaction has been moved into place.\n+      for (HStoreFile hsf: toDelete) {\n+        if (hsf.getFileId() == finalCompactedFile.getFileId()) {\n+          // Be careful we do not delte the just compacted file.\n+          LOG.warn(\"Weird. File to delete has same name as one we are \" +\n+            \"about to delete (skipping): \" + hsf.getFileId());\n+          continue;\n+        }\n+        hsf.delete();\n+      }\n \n-      // Fail here?  No worries.\n       Long orderVal = Long.valueOf(finalCompactedFile.loadInfo(fs));\n \n       // 6. Loading the new TreeMap.\n       this.readers.put(orderVal,\n         finalCompactedFile.getReader(this.fs, this.bloomFilter));\n       this.storefiles.put(orderVal, finalCompactedFile);\n     } finally {\n-      \n       // 7. Releasing the write-lock\n       this.lock.releaseWriteLock();\n     }\n@@ -838,6 +997,9 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n           map.reset();\n           ImmutableBytesWritable readval = new ImmutableBytesWritable();\n           HStoreKey readkey = (HStoreKey)map.getClosest(key, readval);\n+          if (readkey == null) {\n+            continue;\n+          }\n           do {\n             Text readcol = readkey.getColumn();\n             if (results.get(readcol) == null\n@@ -1004,7 +1166,7 @@ HStoreSize size(Text midKey) {\n   /**\n    * @return    Returns the number of map files currently in use\n    */\n-  int getNMaps() {\n+  int countOfStoreFiles() {\n     this.lock.obtainReadLock();\n     try {\n       return storefiles.size();\n@@ -1014,6 +1176,22 @@ int getNMaps() {\n     }\n   }\n   \n+  boolean hasReferences() {\n+    boolean result = false;\n+    this.lock.obtainReadLock();\n+    try {\n+        for (HStoreFile hsf: this.storefiles.values()) {\n+          if (hsf.isReference()) {\n+            break;\n+          }\n+        }\n+      \n+    } finally {\n+      this.lock.releaseReadLock();\n+    }\n+    return result;\n+  }\n+  \n   //////////////////////////////////////////////////////////////////////////////\n   // File administration\n   //////////////////////////////////////////////////////////////////////////////\n@@ -1038,6 +1216,11 @@ HInternalScannerInterface getScanner(long timestamp, Text targetCols[],\n     \n     return new HStoreScanner(timestamp, targetCols, firstRow);\n   }\n+  \n+  @Override\n+  public String toString() {\n+    return this.storeName;\n+  }\n \n   //////////////////////////////////////////////////////////////////////////////\n   // This class implements the HScannerInterface.",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "93ed53055824ddd67111446a0037d96e16c7171a",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreFile.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "patch": "@@ -399,7 +399,13 @@ static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir,\n       Path mapfile = curfile.getMapFilePath();\n       if (!fs.exists(mapfile)) {\n         fs.delete(curfile.getInfoFilePath());\n+        LOG.warn(\"Mapfile \" + mapfile.toString() + \" does not exist. \" +\n+          \"Cleaned up info file.  Continuing...\");\n+        continue;\n       }\n+      \n+      // TODO: Confirm referent exists.\n+      \n       // Found map and sympathetic info file.  Add this hstorefile to result.\n       results.add(curfile);\n       // Keep list of sympathetic data mapfiles for cleaning info dir in next\n@@ -537,8 +543,7 @@ void mergeStoreFiles(Vector<HStoreFile> srcFiles, FileSystem fs,\n     \n     try {\n       for(HStoreFile src: srcFiles) {\n-        MapFile.Reader in =\n-          new MapFile.Reader(fs, src.getMapFilePath().toString(), conf);\n+        MapFile.Reader in = src.getReader(fs, null);\n         try {\n           HStoreKey readkey = new HStoreKey();\n           ImmutableBytesWritable readval = new ImmutableBytesWritable();\n@@ -627,12 +632,23 @@ private void delete(final Path p) throws IOException {\n    * <code>hsf</code> directory.\n    * @param fs\n    * @param hsf\n+   * @return True if succeeded.\n    * @throws IOException\n    */\n-  public void rename(final FileSystem fs, final HStoreFile hsf)\n+  public boolean rename(final FileSystem fs, final HStoreFile hsf)\n   throws IOException {\n-    fs.rename(getMapFilePath(), hsf.getMapFilePath());\n-    fs.rename(getInfoFilePath(), hsf.getInfoFilePath());\n+    boolean success = fs.rename(getMapFilePath(), hsf.getMapFilePath());\n+    if (!success) {\n+      LOG.warn(\"Failed rename of \" + getMapFilePath() + \" to \" +\n+        hsf.getMapFilePath());\n+      return success;\n+    }\n+    success = fs.rename(getInfoFilePath(), hsf.getInfoFilePath());\n+    if (!success) {\n+      LOG.warn(\"Failed rename of \" + getInfoFilePath() + \" to \" +\n+        hsf.getInfoFilePath());\n+    }\n+    return success;\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "sha": "a9e3c15c6f03180f6a4cd3ed956261f76cde39ba",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "patch": "@@ -301,6 +301,10 @@ public int compareTo(Object o) {\n     if(result == 0) {\n       result = this.column.compareTo(other.column);\n       if(result == 0) {\n+        // The below older timestamps sorting ahead of newer timestamps looks\n+        // wrong but it is intentional.  This way, newer timestamps are first\n+        // found when we iterate over a memcache and newer versions are the\n+        // first we trip over when reading from a store file.\n         if(this.timestamp < other.timestamp) {\n           result = 1;\n         } else if(this.timestamp > other.timestamp) {",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "sha": "452894ae1e8d6dd85a1d1614556d6b7260093da7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "patch": "@@ -87,6 +87,13 @@ public ImmutableBytesWritable(final byte[] newData, final int offset,\n     return this.bytes;\n   }\n   \n+  /**\n+   * @param b Use passed bytes as backing array for this instance.\n+   */\n+  public void set(final byte [] b) {\n+    this.bytes = b;\n+  }\n+  \n   /**\n    * @return the current size of the buffer.\n    */",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "sha": "3f90032df5364799ed87ceb02d7048b33510458d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Writables.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/util/Writables.java",
                "patch": "@@ -66,15 +66,16 @@\n    * @param w An empty Writable (usually made by calling the null-arg\n    * constructor).\n    * @return The passed Writable after its readFields has been called fed\n-   * by the passed <code>bytes</code> array or null if passed null or\n-   * empty <code>bytes</code>.\n+   * by the passed <code>bytes</code> array or IllegalArgumentException\n+   * if passed null or an empty <code>bytes</code> array.\n    * @throws IOException\n+   * @throws IllegalArgumentException\n    */\n   public static Writable getWritable(final byte [] bytes, final Writable w)\n   throws IOException {\n     if (bytes == null || bytes.length == 0) {\n-      throw new IllegalArgumentException(\n-          \"Con't build a writable with empty bytes array\");\n+      throw new IllegalArgumentException(\"Can't build a writable with empty \" +\n+        \"bytes array\");\n     }\n     if (w == null) {\n       throw new IllegalArgumentException(\"Writable cannot be null\");",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "sha": "0ad4598168e7b170c0c7b70f39f5cdbc07e61485",
                "status": "modified"
            },
            {
                "additions": 139,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "changes": 139,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "patch": "@@ -32,6 +32,14 @@\n  * Abstract base class for test cases. Performs all static initialization\n  */\n public abstract class HBaseTestCase extends TestCase {\n+  public final static String COLFAMILY_NAME1 = \"colfamily1:\";\n+  public final static String COLFAMILY_NAME2 = \"colfamily2:\";\n+  public final static String COLFAMILY_NAME3 = \"colfamily3:\";\n+  protected Path testDir = null;\n+  protected FileSystem localFs = null;\n+  public static final char FIRST_CHAR = 'a';\n+  public static final char LAST_CHAR = 'z';\n+  \n   static {\n     StaticTestEnvironment.initialize();\n   }\n@@ -47,6 +55,29 @@ protected HBaseTestCase(String name) {\n     super(name);\n     conf = new HBaseConfiguration();\n   }\n+  \n+  @Override\n+  protected void setUp() throws Exception {\n+    super.setUp();\n+    this.testDir = getUnitTestdir(getName());\n+    this.localFs = FileSystem.getLocal(this.conf);\n+    if (localFs.exists(testDir)) {\n+      localFs.delete(testDir);\n+    }\n+  }\n+  \n+  @Override\n+  protected void tearDown() throws Exception {\n+    try {\n+      if (this.localFs != null && this.testDir != null &&\n+          this.localFs.exists(testDir)) {\n+        this.localFs.delete(testDir);\n+      }\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+    }\n+    super.tearDown();\n+  }\n \n   protected Path getUnitTestdir(String testName) {\n     return new Path(StaticTestEnvironment.TEST_DIRECTORY_KEY, testName);\n@@ -63,4 +94,112 @@ protected HRegion createNewHRegion(Path dir, Configuration c,\n       new HLog(fs, new Path(regionDir, HConstants.HREGION_LOGDIR_NAME), conf),\n       fs, conf, info, null);\n   }\n+  \n+  protected HTableDescriptor createTableDescriptor(final String name) {\n+    HTableDescriptor htd = new HTableDescriptor(name);\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1));\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2));\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3));\n+    return htd;\n+  }\n+  \n+  protected void addContent(final HRegion r, final String column)\n+  throws IOException {\n+    Text startKey = r.getRegionInfo().getStartKey();\n+    Text endKey = r.getRegionInfo().getEndKey();\n+    byte [] startKeyBytes = startKey.getBytes();\n+    if (startKeyBytes == null || startKeyBytes.length == 0) {\n+      startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n+    }\n+    addContent(new HRegionLoader(r), column, startKeyBytes, endKey);\n+  }\n+  \n+  protected void addContent(final Loader updater, final String column)\n+  throws IOException {\n+    addContent(updater, column,\n+      new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR}, null);\n+  }\n+  \n+  protected void addContent(final Loader updater, final String column,\n+      final byte [] startKeyBytes, final Text endKey)\n+  throws IOException {\n+    // Add rows of three characters.  The first character starts with the\n+    // 'a' character and runs up to 'z'.  Per first character, we run the\n+    // second character over same range.  And same for the third so rows\n+    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n+    char secondCharStart = (char)startKeyBytes[1];\n+    char thirdCharStart = (char)startKeyBytes[2];\n+    EXIT: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n+      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n+        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n+          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n+          Text t = new Text(new String(bytes));\n+          if (endKey != null && endKey.getLength() > 0\n+              && endKey.compareTo(t) <= 0) {\n+            break EXIT;\n+          }\n+          long lockid = updater.startBatchUpdate(t);\n+          try {\n+            updater.put(lockid, new Text(column), bytes);\n+            updater.commit(lockid);\n+            lockid = -1;\n+          } finally {\n+            if (lockid != -1) {\n+              updater.abort(lockid);\n+            }\n+          }\n+        }\n+        // Set start character back to FIRST_CHAR after we've done first loop.\n+        thirdCharStart = FIRST_CHAR;\n+      }\n+      secondCharStart = FIRST_CHAR;\n+    }\n+  }\n+  \n+  public interface Loader {\n+    public long startBatchUpdate(final Text row) throws IOException;\n+    public void put(long lockid, Text column, byte val[]) throws IOException;\n+    public void commit(long lockid) throws IOException;\n+    public void abort(long lockid) throws IOException;\n+  }\n+  \n+  public class HRegionLoader implements Loader {\n+    final HRegion region;\n+    public HRegionLoader(final HRegion HRegion) {\n+      super();\n+      this.region = HRegion;\n+    }\n+    public void abort(long lockid) throws IOException {\n+      this.region.abort(lockid);\n+    }\n+    public void commit(long lockid) throws IOException {\n+      this.region.commit(lockid, System.currentTimeMillis());\n+    }\n+    public void put(long lockid, Text column, byte[] val) throws IOException {\n+      this.region.put(lockid, column, val);\n+    }\n+    public long startBatchUpdate(Text row) throws IOException {\n+      return this.region.startUpdate(row);\n+    }\n+  }\n+  \n+  public class HTableLoader implements Loader {\n+    final HTable table;\n+    public HTableLoader(final HTable table) {\n+      super();\n+      this.table = table;\n+    }\n+    public void abort(long lockid) throws IOException {\n+      this.table.abort(lockid);\n+    }\n+    public void commit(long lockid) throws IOException {\n+      this.table.commit(lockid);\n+    }\n+    public void put(long lockid, Text column, byte[] val) throws IOException {\n+      this.table.put(lockid, column, val);\n+    }\n+    public long startBatchUpdate(Text row) {\n+      return this.table.startBatchUpdate(row);\n+    }\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "sha": "27218d361850436192ab6c2bc519600dacb1c830",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "patch": "@@ -365,7 +365,9 @@ void shutdown() {\n     shutdown(this.masterThread, this.regionThreads);\n     // Close the file system.  Will complain if files open so helps w/ leaks.\n     try {\n-      this.cluster.getFileSystem().close();\n+      if (this.cluster != null && this.cluster.getFileSystem() != null) {\n+        this.cluster.getFileSystem().close();\n+      }\n     } catch (IOException e) {\n       LOG.error(\"Closing down dfs\", e);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "sha": "d1902f72ed96c7a6111830ef1de5e047deb5633e",
                "status": "modified"
            },
            {
                "additions": 101,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompaction.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Test compactions\n+ */\n+public class TestCompaction extends HBaseTestCase {\n+  static final Log LOG = LogFactory.getLog(TestCompaction.class.getName());\n+\n+  protected void setUp() throws Exception {\n+    super.setUp();\n+  }\n+  \n+  protected void tearDown() throws Exception {\n+    super.tearDown();\n+  }\n+  \n+  /**\n+   * Run compaction and flushing memcache\n+   * @throws Exception\n+   */\n+  public void testCompaction() throws Exception {\n+    HLog hlog = new HLog(this.localFs, this.testDir, this.conf);\n+    HTableDescriptor htd = createTableDescriptor(getName());\n+    HRegionInfo hri = new HRegionInfo(1, htd, null, null);\n+    final HRegion r =\n+      new HRegion(testDir, hlog, this.localFs, this.conf, hri, null);\n+    try {\n+      createStoreFile(r);\n+      assertFalse(r.needsCompaction());\n+      int compactionThreshold =\n+        this.conf.getInt(\"hbase.hstore.compactionThreshold\", 3);\n+      for (int i = 0; i < compactionThreshold; i++) {\n+        createStoreFile(r);\n+      }\n+      assertTrue(r.needsCompaction());\n+      // Try to run compaction concurrent with a thread flush.\n+      addContent(new HRegionLoader(r), COLFAMILY_NAME1);\n+      Thread t1 = new Thread() {\n+        @Override\n+        public void run() {\n+          try {\n+            r.flushcache(false);\n+          } catch (IOException e) {\n+            e.printStackTrace();\n+          }\n+        }\n+      };\n+      Thread t2 = new Thread() {\n+        @Override\n+        public void run() {\n+          try {\n+            assertTrue(r.compactStores());\n+          } catch (IOException e) {\n+            e.printStackTrace();\n+          }\n+        }\n+      };\n+      t1.setDaemon(true);\n+      t1.start();\n+      t2.setDaemon(true);\n+      t2.start();\n+      t1.join();\n+      t2.join();\n+    } finally {\n+      r.close();\n+      hlog.closeAndDelete();\n+    }\n+  }\n+  \n+  private void createStoreFile(final HRegion r) throws IOException {\n+    HRegionLoader loader = new HRegionLoader(r);\n+    for (int i = 0; i < 3; i++) {\n+      addContent(loader, COLFAMILY_NAME1);\n+    }\n+    r.flushcache(false);\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "sha": "caa68ebbd9a46c4c169512e4e09df51aaec9b802",
                "status": "added"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompare.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 1,
                "filename": "src/test/org/apache/hadoop/hbase/TestCompare.java",
                "patch": "@@ -26,7 +26,27 @@\n  * Test comparing HBase objects.\n  */\n public class TestCompare extends TestCase {\n-  /** test case */\n+  \n+  /**\n+   * HStoreKey sorts as you would expect in the row and column portions but\n+   * for the timestamps, it sorts in reverse with the newest sorting before\n+   * the oldest (This is intentional so we trip over the latest first when\n+   * iterating or looking in store files).\n+   */\n+  public void testHStoreKey() {\n+    long timestamp = System.currentTimeMillis();\n+    Text a = new Text(\"a\");\n+    HStoreKey past = new HStoreKey(a, a, timestamp - 10);\n+    HStoreKey now = new HStoreKey(a, a, timestamp);\n+    HStoreKey future = new HStoreKey(a, a, timestamp + 10);\n+    assertTrue(past.compareTo(now) > 0);\n+    assertTrue(now.compareTo(now) == 0);\n+    assertTrue(future.compareTo(now) < 0);\n+  }\n+  \n+  /**\n+   * Sort of HRegionInfo.\n+   */\n   public void testHRegionInfo() {\n     HRegionInfo a = new HRegionInfo(1, new HTableDescriptor(\"a\"), null, null);\n     HRegionInfo b = new HRegionInfo(2, new HTableDescriptor(\"b\"), null, null);",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "sha": "0bd3c1dfc2e0135b09680cc3872c4427e20325db",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestSplit.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "deletions": 126,
                "filename": "src/test/org/apache/hadoop/hbase/TestSplit.java",
                "patch": "@@ -38,61 +38,35 @@\n  * split and manufactures odd-ball split scenarios.\n  */\n public class TestSplit extends HBaseTestCase {\n-  static final Log LOG = LogFactory.getLog(TestSplit.class);\n-  private final static String COLFAMILY_NAME1 = \"colfamily1:\";\n-  private final static String COLFAMILY_NAME2 = \"colfamily2:\";\n-  private final static String COLFAMILY_NAME3 = \"colfamily3:\";\n-  private Path testDir = null;\n-  private FileSystem fs = null;\n-  private static final char FIRST_CHAR = 'a';\n-  private static final char LAST_CHAR = 'z';\n-\n+  static final Log LOG = LogFactory.getLog(TestSplit.class.getName());\n+  \n   /** constructor */\n   public TestSplit() {\n     Logger.getRootLogger().setLevel(Level.WARN);\n-    Logger.getLogger(this.getClass().getPackage().getName()).setLevel(Level.DEBUG);\n+    Logger.getLogger(this.getClass().getPackage().getName()).\n+      setLevel(Level.DEBUG);\n   }\n \n   /** {@inheritDoc} */\n   @Override\n   public void setUp() throws Exception {\n     super.setUp();\n-    this.testDir = getUnitTestdir(getName());\n-    this.fs = FileSystem.getLocal(this.conf);\n-    if (fs.exists(testDir)) {\n-      fs.delete(testDir);\n-    }\n     // This size should make it so we always split using the addContent\n     // below.  After adding all data, the first region is 1.3M\n     conf.setLong(\"hbase.hregion.max.filesize\", 1024 * 128);\n   }\n   \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    if (fs != null) {\n-      try {\n-        if (this.fs.exists(testDir)) {\n-          this.fs.delete(testDir);\n-        }\n-      } catch (Exception e) {\n-        e.printStackTrace();\n-      }\n-    }\n-    super.tearDown();\n-  }\n-  \n   /**\n    * Splits twice and verifies getting from each of the split regions.\n    * @throws Exception\n    */\n   public void testBasicSplit() throws Exception {\n     HRegion region = null;\n-    HLog hlog = new HLog(this.fs, this.testDir, this.conf);\n+    HLog hlog = new HLog(this.localFs, this.testDir, this.conf);\n     try {\n       HTableDescriptor htd = createTableDescriptor(getName());\n       HRegionInfo hri = new HRegionInfo(1, htd, null, null);\n-      region = new HRegion(testDir, hlog, fs, this.conf, hri, null);\n+      region = new HRegion(testDir, hlog, this.localFs, this.conf, hri, null);\n       basicSplit(region);\n     } finally {\n       if (region != null) {\n@@ -102,14 +76,6 @@ public void testBasicSplit() throws Exception {\n     }\n   }\n   \n-  private HTableDescriptor createTableDescriptor(final String name) {\n-    HTableDescriptor htd = new HTableDescriptor(name);\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1));\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2));\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3));\n-    return htd;\n-  }\n-  \n   private void basicSplit(final HRegion region) throws Exception {\n     addContent(region, COLFAMILY_NAME3);\n     region.internalFlushcache();\n@@ -184,13 +150,13 @@ private void basicSplit(final HRegion region) throws Exception {\n    * @throws Exception\n    */\n   public void testSplitRegionIsDeleted() throws Exception {\n-    final int retries = 10;\n-    this.testDir = null;\n-    this.fs = null;\n+    final int retries = 10; \n     // Start up a hbase cluster\n-    MiniHBaseCluster cluster = new MiniHBaseCluster(conf, 1);\n-    Path testDir = cluster.regionThreads.get(0).getRegionServer().rootDir;\n-    FileSystem fs = cluster.getDFSCluster().getFileSystem();\n+    MiniHBaseCluster cluster = new MiniHBaseCluster(conf, 1, true);\n+    Path d = cluster.regionThreads.get(0).getRegionServer().rootDir;\n+    FileSystem fs = (cluster.getDFSCluster() == null)?\n+      this.localFs:\n+      cluster.getDFSCluster().getFileSystem();\n     HTable meta = null;\n     HTable t = null;\n     try {\n@@ -201,7 +167,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n       meta = new HTable(this.conf, HConstants.META_TABLE_NAME);\n       int count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       t = new HTable(this.conf, new Text(getName()));\n-      addContent(t, COLFAMILY_NAME3);\n+      addContent(new HTableLoader(t), COLFAMILY_NAME3);\n       // All is running in the one JVM so I should be able to get the\n       // region instance and bring on a split.\n       HRegionInfo hri =\n@@ -223,8 +189,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n       }\n       HRegionInfo parent = getSplitParent(meta);\n       assertTrue(parent.isOffline());\n-      Path parentDir =\n-        HRegion.getRegionDir(testDir, parent.getRegionName());\n+      Path parentDir = HRegion.getRegionDir(d, parent.getRegionName());\n       assertTrue(fs.exists(parentDir));\n       LOG.info(\"Split happened and parent \" + parent.getRegionName() + \" is \" +\n       \"offline\");\n@@ -263,7 +228,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n         for (int i = 0; i < 10; i++) {\n           try {\n             for (HRegion online: regions.values()) {\n-              if (online.getRegionName().toString().startsWith(getName())) {\n+              if (online.getTableDesc().getName().toString().equals(getName())) {\n                 online.compactStores();\n               }\n             }\n@@ -403,79 +368,4 @@ private void assertScan(final HRegion r, final String column,\n     assertEquals(regions.length, 2);\n     return regions;\n   }\n-  \n-  private void addContent(final HRegion r, final String column)\n-  throws IOException {\n-    Text startKey = r.getRegionInfo().getStartKey();\n-    Text endKey = r.getRegionInfo().getEndKey();\n-    byte [] startKeyBytes = startKey.getBytes();\n-    if (startKeyBytes == null || startKeyBytes.length == 0) {\n-      startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n-    }\n-    // Add rows of three characters.  The first character starts with the\n-    // 'a' character and runs up to 'z'.  Per first character, we run the\n-    // second character over same range.  And same for the third so rows\n-    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n-    char secondCharStart = (char)startKeyBytes[1];\n-    char thirdCharStart = (char)startKeyBytes[2];\n-    EXIT_ALL_LOOPS: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n-      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n-        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          Text t = new Text(new String(bytes));\n-          if (endKey != null && endKey.getLength() > 0\n-              && endKey.compareTo(t) <= 0) {\n-            break EXIT_ALL_LOOPS;\n-          }\n-          long lockid = r.startUpdate(t);\n-          try {\n-            r.put(lockid, new Text(column), bytes);\n-            r.commit(lockid, System.currentTimeMillis());\n-            lockid = -1;\n-          } finally {\n-            if (lockid != -1) {\n-              r.abort(lockid);\n-            }\n-          }\n-        }\n-        // Set start character back to FIRST_CHAR after we've done first loop.\n-        thirdCharStart = FIRST_CHAR;\n-      }\n-      secondCharStart = FIRST_CHAR;\n-    }\n-  }\n-  \n-  // TODO: Have HTable and HRegion implement interface that has in it\n-  // startUpdate, put, delete, commit, abort, etc.\n-  private void addContent(final HTable table, final String column)\n-  throws IOException {\n-    byte [] startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n-    // Add rows of three characters.  The first character starts with the\n-    // 'a' character and runs up to 'z'.  Per first character, we run the\n-    // second character over same range.  And same for the third so rows\n-    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n-    char secondCharStart = (char)startKeyBytes[1];\n-    char thirdCharStart = (char)startKeyBytes[2];\n-    for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n-      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n-        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          Text t = new Text(new String(bytes));\n-          long lockid = table.startUpdate(t);\n-          try {\n-            table.put(lockid, new Text(column), bytes);\n-            table.commit(lockid, System.currentTimeMillis());\n-            lockid = -1;\n-          } finally {\n-            if (lockid != -1) {\n-              table.abort(lockid);\n-            }\n-          }\n-        }\n-        // Set start character back to FIRST_CHAR after we've done first loop.\n-        thirdCharStart = FIRST_CHAR;\n-      }\n-      secondCharStart = FIRST_CHAR;\n-    }\n-  }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "sha": "7f5819e237c666fe4ceac057c700a26bba960f81",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1644 [hbase] Compactions should not block updates\n\nDisentangles flushes and compactions; flushes can proceed while a\ncompaction is happening.  Also, don't compact unless we hit\ncompaction threshold: i.e. don't automatically compact on HRegion\nstartup so regions can come online the faster.\n\nM src/contrib/hbase/conf/hbase-default.xml\n    (hbase.hregion.compactionThreashold): Moved to be a hstore property\n    as part of encapsulating compaction decision inside hstore.\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/HBaseTestCase.java\n    Refactored.  Moved here generalized content loading code that can\n    be shared by tests.  Add to setup and teardown the setup and removal\n    of local test dir (if it exists).\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestCompare.java\n    Added test of HStoreKey compare (It works other than one would at\n    first expect).\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestSplit.java\n    Bulk of content loading code has been moved up into the parent class.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HConnectionManager.java\n    (tableExists): Restore to a check of if the asked-for table is in list of\n    tables.  As it was, a check for tableExists would just wait on all timeouts\n    and retries to expire and then report table does not exist..  Fixed up\n    debug message listing regions of a table.  Added protection against meta\n    table not having a COL_REGINFO (Seen in cluster testing -- probably a bug\n    in row removal).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStoreFile.java\n    Loading store files, even if it was noticed that there was no corresponding\n    map file, was still counting file as valid.  Also fix merger -- was\n    constructing MapFile.Reader directly rather than asking HStoreFile for\n    the reader (HStoreFile knows how to do MapFile references)\n    (rename): Added check that move succeeded and logging.  In cluster-testing,\n    the hdfs move of compacted file into place has failed on occasion (Need\n    more info).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStore.java\n    Encapsulate ruling on whether a compaction should take place inside HStore.\n    Added reading of the compactionThreshold her.  Compaction threshold is\n    currently just number of store files.  Later may include other factors such\n    as count of reference files.  Cleaned up debug messages around\n    reconstruction log.  Removed compaction if size > 1 from constructor.  Let\n    compaction happen after we've been deployed (Compactions that happen while\n    we are online can continue to take updates.  Compaction in the constructor\n    puts off our being able to take in updates).\n    (close): Changed so it now returns set of store files.  This used to be done\n    by calls to flush. Since flush and compaction have been disentangled, a\n    compaction can come in after flush and the list of files could be off.\n    Having it done by close, can be sure list of files is complete.\n    (flushCache): No longer returns set of store files.  Added 'merging compaction'\n    where we pick an arbitrary store file from disk and merge into it the content\n    of memcache (Needs work).\n    (getAllMapFiles): Renamed getAllStoreFiles.\n    (needsCompaction): Added.\n    (compactHelper): Added passing of maximum sequence number if already\n    calculated. If compacting one file only, we used skip without rewriting\n    the info file.  Fixed.\n    Refactored.  Moved guts to new  compact(outFile, listOfStores)  method.\n    (compact, CompactionReader): Added overrides and interface  to support\n    'merging compaction' that takes files and memcache.  In compaction,\n    if we failed the move of the compacted file, all data had already been\n    deleted.  Changing, so deletion happens after confirmed move of\n    compacted file.\n    (getFull): Fixed bug where NPE when read of maps came back null.\n    Revealed by our NOT compacting stores on startup.  Meant could be two\n    backing stores one of which had no data regards queried key.\n    (getNMaps): Renamed countOfStoreFiles.\n    (toString): Added.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStoreKey.java\n    Added comment on 'odd'-looking comparison.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegionServer.java\n    Javadoc edit. \nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HLogEdit.java\n    Only return first 128 bytes of value when toStringing (On cluster,\n    was returning complete web pages in log).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMaster.java\n    Removed confusing debug message (made sense once -- but not now).\n    Test rootRegionLocation for null before using it (can be null).\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMemcache.java\n    Added comment that delete behavior needs study.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegion.java\n    Fixed merge so it doesn't do the incremental based off files\n    returned by flush.  Instead all is done in the one go after\n    region closes (using files returned by close).\n    Moved duplicated code to new filesByFamily method.\n    (WriteState): Removed writesOngoing in favor of compacting and\n    flushing flags.\n    (flushCache): No longer returns list of files.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/util/Writables.java\n    Fix javadoc.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@566459 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6c8e71367141628445b4cc843ad60afdfb5bb7c7",
        "patched_files": [
            "HStoreFile.java",
            "MiniHBaseCluster.java",
            "ImmutableBytesWritable.java",
            "HMaster.java",
            "HStore.java",
            "HConnection.java",
            "Writables.java",
            "HBaseTestCase.java",
            "HRegion.java",
            "CHANGES.java",
            "HLogEdit.java",
            "HMemcache.java",
            "HStoreKey.java",
            "HConnectionManager.java",
            "hbase-default.java",
            "HRegionServer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSplit.java",
            "TestImmutableBytesWritable.java",
            "TestCompaction.java",
            "TestCompare.java",
            "TestHRegion.java",
            "TestHStoreFile.java",
            "TestHStore.java"
        ]
    },
    "hbase_bfe27f5": {
        "bug_id": "hbase_bfe27f5",
        "commit": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -802,6 +802,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3374  Our jruby jar has *GPL jars in it; fix\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n+   HBASE-3386  NPE in TableRecordReaderImpl.restart\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "sha": "f73e9a409d5fcda41688c031400a385842c7105e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -94,6 +94,11 @@\n   public RecordReader<ImmutableBytesWritable, Result> createRecordReader(\n       InputSplit split, TaskAttemptContext context)\n   throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"Cannot create a record reader because of a\" +\n+          \" previous error. Please look at the previous logs lines from\" +\n+          \" the task's full log for more details.\");\n+    }\n     TableSplit tSplit = (TableSplit) split;\n     TableRecordReader trr = this.tableRecordReader;\n     // if no table record reader was provided use default",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "c813c490a365c2445725af3c8122cd5dc7b329ca",
                "status": "modified"
            }
        ],
        "message": "HBASE-3386  NPE in TableRecordReaderImpl.restart\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052051 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/94cc5237b0576d9f36f51b7142ef913cc032d9bc",
        "patched_files": [
            "TableInputFormatBase.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_c0598dc": {
        "bug_id": "hbase_c0598dc",
        "commit": "https://github.com/apache/hbase/commit/c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -7585,7 +7585,7 @@ private WriteEntry doWALAppend(WALEdit walEdit, Durability durability, List<UUID\n       }\n       writeEntry = walKey.getWriteEntry();\n     } catch (IOException ioe) {\n-      if (walKey != null) {\n+      if (walKey != null && walKey.getWriteEntry() != null) {\n         mvcc.complete(walKey.getWriteEntry());\n       }\n       throw ioe;",
                "raw_url": "https://github.com/apache/hbase/raw/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "6401a8b957ef6473da0c93384c534e19c876126e",
                "status": "modified"
            }
        ],
        "message": "HBASE-19593 Possible NPE if wal is closed during waledit append.(Rajeshabbu)",
        "parent": "https://github.com/apache/hbase/commit/448ba3a78f50df2ffac874c3768e9f50d52b15f6",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_c1ac4bb": {
        "bug_id": "hbase_c1ac4bb",
        "commit": "https://github.com/apache/hbase/commit/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "deletions": 3,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "patch": "@@ -23,6 +23,8 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.TimeUnit;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.metrics.BaseSourceImpl;\n import org.apache.hadoop.metrics2.MetricsCollector;\n@@ -35,6 +37,8 @@\n public class MetricsRegionAggregateSourceImpl extends BaseSourceImpl\n     implements MetricsRegionAggregateSource {\n \n+  private static final Log LOG = LogFactory.getLog(MetricsRegionAggregateSourceImpl.class);\n+\n   private final MetricsExecutorImpl executor = new MetricsExecutorImpl();\n \n   private final Set<MetricsRegionSource> regionSources =\n@@ -54,7 +58,7 @@ public MetricsRegionAggregateSourceImpl(String metricsName,\n     // Every few mins clean the JMX cache.\n     executor.getExecutor().scheduleWithFixedDelay(new Runnable() {\n       public void run() {\n-        JmxCacheBuster.clearJmxCache(true);\n+        JmxCacheBuster.clearJmxCache();\n       }\n     }, 5, 5, TimeUnit.MINUTES);\n   }\n@@ -67,12 +71,20 @@ public void register(MetricsRegionSource source) {\n \n   @Override\n   public void deregister(MetricsRegionSource toRemove) {\n-    regionSources.remove(toRemove);\n+    try {\n+      regionSources.remove(toRemove);\n+    } catch (Exception e) {\n+      // Ignored. If this errors out it means that someone is double\n+      // closing the region source and the region is already nulled out.\n+      LOG.info(\n+          \"Error trying to remove \" + toRemove + \" from \" + this.getClass().getSimpleName(),\n+          e);\n+    }\n     clearCache();\n   }\n \n   private synchronized void clearCache() {\n-    JmxCacheBuster.clearJmxCache(true);\n+    JmxCacheBuster.clearJmxCache();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "sha": "1835f6b6dbd88fd6ada7d7cc06625c4fbba74c8b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "deletions": 6,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "patch": "@@ -68,28 +68,28 @@ public MetricsRegionServerSourceImpl(String metricsName,\n     this.rsWrap = rsWrap;\n \n     putHisto = getMetricsRegistry().newHistogram(MUTATE_KEY);\n-    slowPut = getMetricsRegistry().newCounter(SLOW_MUTATE_KEY, SLOW_MUTATE_DESC, 0l);\n+    slowPut = getMetricsRegistry().newCounter(SLOW_MUTATE_KEY, SLOW_MUTATE_DESC, 0L);\n \n     deleteHisto = getMetricsRegistry().newHistogram(DELETE_KEY);\n-    slowDelete = getMetricsRegistry().newCounter(SLOW_DELETE_KEY, SLOW_DELETE_DESC, 0l);\n+    slowDelete = getMetricsRegistry().newCounter(SLOW_DELETE_KEY, SLOW_DELETE_DESC, 0L);\n \n     getHisto = getMetricsRegistry().newHistogram(GET_KEY);\n-    slowGet = getMetricsRegistry().newCounter(SLOW_GET_KEY, SLOW_GET_DESC, 0l);\n+    slowGet = getMetricsRegistry().newCounter(SLOW_GET_KEY, SLOW_GET_DESC, 0L);\n \n     incrementHisto = getMetricsRegistry().newHistogram(INCREMENT_KEY);\n     slowIncrement = getMetricsRegistry().newCounter(SLOW_INCREMENT_KEY, SLOW_INCREMENT_DESC, 0L);\n \n     appendHisto = getMetricsRegistry().newHistogram(APPEND_KEY);\n-    slowAppend = getMetricsRegistry().newCounter(SLOW_APPEND_KEY, SLOW_APPEND_DESC, 0l);\n+    slowAppend = getMetricsRegistry().newCounter(SLOW_APPEND_KEY, SLOW_APPEND_DESC, 0L);\n     \n     replayHisto = getMetricsRegistry().newHistogram(REPLAY_KEY);\n     scanNextHisto = getMetricsRegistry().newHistogram(SCAN_NEXT_KEY);\n \n     splitTimeHisto = getMetricsRegistry().newHistogram(SPLIT_KEY);\n     flushTimeHisto = getMetricsRegistry().newHistogram(FLUSH_KEY);\n \n-    splitRequest = getMetricsRegistry().newCounter(SPLIT_REQUEST_KEY, SPLIT_REQUEST_DESC, 0l);\n-    splitSuccess = getMetricsRegistry().newCounter(SPLIT_SUCCESS_KEY, SPLIT_SUCCESS_DESC, 0l);\n+    splitRequest = getMetricsRegistry().newCounter(SPLIT_REQUEST_KEY, SPLIT_REQUEST_DESC, 0L);\n+    splitSuccess = getMetricsRegistry().newCounter(SPLIT_SUCCESS_KEY, SPLIT_SUCCESS_DESC, 0L);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "sha": "a4891da1a9b6d41bdcdb3dc38f67985cebc91c47",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "deletions": 19,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "patch": "@@ -20,16 +20,12 @@\n \n import java.util.Map;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReadWriteLock;\n-import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.math.stat.descriptive.DescriptiveStatistics;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n-import org.apache.hadoop.metrics2.impl.JmxCacheBuster;\n import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n import org.apache.hadoop.metrics2.lib.Interns;\n import org.apache.hadoop.metrics2.lib.MutableCounterLong;\n@@ -65,6 +61,7 @@\n   private final MutableCounterLong regionAppend;\n   private final MutableHistogram regionGet;\n   private final MutableHistogram regionScanNext;\n+  private final int hashCode;\n \n   public MetricsRegionSourceImpl(MetricsRegionWrapper regionWrapper,\n                                  MetricsRegionAggregateSourceImpl aggregate) {\n@@ -101,6 +98,8 @@ public MetricsRegionSourceImpl(MetricsRegionWrapper regionWrapper,\n \n     regionScanNextKey = regionNamePrefix + MetricsRegionServerSource.SCAN_NEXT_KEY;\n     regionScanNext = registry.newHistogram(regionScanNextKey);\n+\n+    hashCode = regionWrapper.getRegionHashCode();\n   }\n \n   @Override\n@@ -173,12 +172,16 @@ public MetricsRegionAggregateSource getAggregateSource() {\n \n   @Override\n   public int compareTo(MetricsRegionSource source) {\n-    if (!(source instanceof MetricsRegionSourceImpl))\n+    if (!(source instanceof MetricsRegionSourceImpl)) {\n       return -1;\n+    }\n \n     MetricsRegionSourceImpl impl = (MetricsRegionSourceImpl) source;\n-    return this.regionWrapper.getRegionName()\n-        .compareTo(impl.regionWrapper.getRegionName());\n+    if (impl == null) {\n+      return -1;\n+    }\n+\n+    return Long.compare(hashCode, impl.hashCode);\n   }\n \n   void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n@@ -203,31 +206,40 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n       }\n \n       mrb.addGauge(\n-          Interns.info(regionNamePrefix + MetricsRegionServerSource.STORE_COUNT,\n+          Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STORE_COUNT,\n               MetricsRegionServerSource.STORE_COUNT_DESC),\n           this.regionWrapper.getNumStores());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.STOREFILE_COUNT,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STOREFILE_COUNT,\n               MetricsRegionServerSource.STOREFILE_COUNT_DESC),\n           this.regionWrapper.getNumStoreFiles());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.MEMSTORE_SIZE,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.MEMSTORE_SIZE,\n               MetricsRegionServerSource.MEMSTORE_SIZE_DESC),\n           this.regionWrapper.getMemstoreSize());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.STOREFILE_SIZE,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STOREFILE_SIZE,\n               MetricsRegionServerSource.STOREFILE_SIZE_DESC),\n           this.regionWrapper.getStoreFileSize());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.COMPACTIONS_COMPLETED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.COMPACTIONS_COMPLETED_COUNT,\n               MetricsRegionSource.COMPACTIONS_COMPLETED_DESC),\n           this.regionWrapper.getNumCompactionsCompleted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.NUM_BYTES_COMPACTED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.NUM_BYTES_COMPACTED_COUNT,\n               MetricsRegionSource.NUM_BYTES_COMPACTED_DESC),\n           this.regionWrapper.getNumBytesCompacted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.NUM_FILES_COMPACTED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.NUM_FILES_COMPACTED_COUNT,\n               MetricsRegionSource.NUM_FILES_COMPACTED_DESC),\n           this.regionWrapper.getNumFilesCompacted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionServerSource.READ_REQUEST_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.READ_REQUEST_COUNT,\n               MetricsRegionServerSource.READ_REQUEST_COUNT_DESC),\n           this.regionWrapper.getReadRequestCount());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionServerSource.WRITE_REQUEST_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.WRITE_REQUEST_COUNT,\n               MetricsRegionServerSource.WRITE_REQUEST_COUNT_DESC),\n           this.regionWrapper.getWriteRequestCount());\n \n@@ -265,12 +277,12 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n \n   @Override\n   public int hashCode() {\n-    return regionWrapper.getRegionHashCode();\n+    return hashCode;\n   }\n \n   @Override\n   public boolean equals(Object obj) {\n-    if (obj == this) return true;\n-    return obj instanceof MetricsRegionSourceImpl && compareTo((MetricsRegionSourceImpl) obj) == 0;\n+    return obj == this ||\n+        (obj instanceof MetricsRegionSourceImpl && compareTo((MetricsRegionSourceImpl) obj) == 0);\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "sha": "0ecf2b2a5743649df27b2141ca42c011b18efb7c",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "deletions": 8,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "patch": "@@ -49,24 +49,18 @@ private JmxCacheBuster() {\n   /**\n    * For JMX to forget about all previously exported metrics.\n    */\n-\n   public static void clearJmxCache() {\n-    clearJmxCache(false);\n-  }\n-\n-  public static synchronized void clearJmxCache(boolean force) {\n     //If there are more then 100 ms before the executor will run then everything should be merged.\n     ScheduledFuture future = fut.get();\n-    if (!force &&\n-        (future == null || (!future.isDone() && future.getDelay(TimeUnit.MILLISECONDS) > 100))) {\n+    if ((future == null || (!future.isDone() && future.getDelay(TimeUnit.MILLISECONDS) > 100))) {\n       // BAIL OUT\n       return;\n     }\n     future = executor.getExecutor().schedule(new JmxCacheBusterRunnable(), 5, TimeUnit.SECONDS);\n     fut.set(future);\n   }\n \n-  static class JmxCacheBusterRunnable implements Runnable {\n+  final static class JmxCacheBusterRunnable implements Runnable {\n     @Override\n     public void run() {\n       if (LOG.isTraceEnabled()) {\n@@ -78,6 +72,9 @@ public void run() {\n       try {\n         if (DefaultMetricsSystem.instance() != null) {\n           DefaultMetricsSystem.instance().stop();\n+          // Sleep some time so that the rest of the hadoop metrics\n+          // system knows that things are done\n+          Thread.sleep(500);\n           DefaultMetricsSystem.instance().start();\n         }\n       }  catch (Exception exception)  {",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "sha": "95734ba4e6f95c420a1cbb34a80b08c911b0c209",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "deletions": 4,
                "filename": "hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "patch": "@@ -42,8 +42,10 @@ public void testCompareToHashCodeEquals() throws Exception {\n     assertEquals(one.hashCode(), oneClone.hashCode());\n     assertNotEquals(one, two);\n \n-    assertTrue( one.compareTo(two) < 0);\n-    assertTrue( two.compareTo(one) > 0);\n+    assertTrue( one.compareTo(two) != 0);\n+    assertTrue( two.compareTo(one) != 0);\n+    assertTrue( two.compareTo(one) != one.compareTo(two));\n+    assertTrue( two.compareTo(two) == 0);\n   }\n \n \n@@ -59,8 +61,6 @@ public void testNoGetRegionServerMetricsSourceImpl() throws Exception {\n     private String regionName;\n \n     public RegionWrapperStub(String regionName) {\n-\n-\n       this.regionName = regionName;\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "sha": "9d60a8ff2c8dabae9506ef16dece50c90ddc198d",
                "status": "modified"
            }
        ],
        "message": "HBASE-14278 Fix NPE that is showing up since HBASE-14274 went in",
        "parent": "https://github.com/apache/hbase/commit/7b08f4c8be60582cd02ba31161be214c9c9d40f9",
        "patched_files": [
            "MetricsRegionServerSourceImpl.java",
            "MetricsRegionSourceImpl.java",
            "JmxCacheBuster.java",
            "MetricsRegionAggregateSourceImpl.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetricsRegionServerSourceImpl.java",
            "TestMetricsRegionSourceImpl.java"
        ]
    },
    "hbase_c24810e": {
        "bug_id": "hbase_c24810e",
        "commit": "https://github.com/apache/hbase/commit/c24810eebe1f52bbe42bd0f4e0bf512425295aad",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=c24810eebe1f52bbe42bd0f4e0bf512425295aad",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "patch": "@@ -304,10 +304,18 @@ public boolean seekBefore(Cell key) throws IOException {\n           // The equals sign isn't strictly necessary just here to be consistent\n           // with seekTo\n           if (getComparator().compareOnlyKeyPortion(key, splitCell) >= 0) {\n-            return this.delegate.seekBefore(splitCell);\n+            boolean ret = this.delegate.seekBefore(splitCell);\n+            if (ret) {\n+              atEnd = false;\n+            }\n+            return ret;\n           }\n         }\n-        return this.delegate.seekBefore(key);\n+        boolean ret = this.delegate.seekBefore(key);\n+        if (ret) {\n+          atEnd = false;\n+        }\n+        return ret;\n       }\n     };\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "sha": "05c996f50523653a92681413461db8c4d4d8fcc8",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hbase/blob/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "changes": 102,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=c24810eebe1f52bbe42bd0f4e0bf512425295aad",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "patch": "@@ -5009,6 +5009,7 @@ private void checkOneCell(Cell kv, byte[] cf, int rowIdx, int colIdx, long ts) {\n         Bytes.toString(CellUtil.cloneValue(kv)));\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_Normal()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5064,6 +5065,7 @@ public void testReverseScanner_FromMemStore_SingleCF_Normal()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_LargerKey()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5120,6 +5122,7 @@ public void testReverseScanner_FromMemStore_SingleCF_LargerKey()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_FullScan()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5173,6 +5176,7 @@ public void testReverseScanner_FromMemStore_SingleCF_FullScan()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_moreRowsMayExistAfter() throws IOException {\n     // case for \"INCLUDE_AND_SEEK_NEXT_ROW & SEEK_NEXT_ROW\" endless loop\n     byte[] rowA = Bytes.toBytes(\"rowA\");\n@@ -5250,6 +5254,7 @@ public void testReverseScanner_moreRowsMayExistAfter() throws IOException {\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_smaller_blocksize() throws IOException {\n     // case to ensure no conflict with HFile index optimization\n     byte[] rowA = Bytes.toBytes(\"rowA\");\n@@ -5329,6 +5334,7 @@ public void testReverseScanner_smaller_blocksize() throws IOException {\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs1()\n       throws IOException {\n     byte[] row0 = Bytes.toBytes(\"row0\"); // 1 kv\n@@ -5489,6 +5495,7 @@ public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs1()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs2()\n       throws IOException {\n     byte[] row1 = Bytes.toBytes(\"row1\");\n@@ -5562,6 +5569,101 @@ public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs2()\n     }\n   }\n \n+  @Test (timeout=60000)\n+  public void testSplitRegionWithReverseScan() throws IOException {\n+    byte [] tableName = Bytes.toBytes(\"testSplitRegionWithReverseScan\");\n+    byte [] qualifier = Bytes.toBytes(\"qualifier\");\n+    Configuration hc = initSplit();\n+    int numRows = 3;\n+    byte [][] families = {fam1};\n+\n+    //Setting up region\n+    String method = this.getName();\n+    this.region = initHRegion(tableName, method, hc, families);\n+\n+    //Put data in region\n+    int startRow = 100;\n+    putData(startRow, numRows, qualifier, families);\n+    int splitRow = startRow + numRows;\n+    putData(splitRow, numRows, qualifier, families);\n+    int endRow = splitRow + numRows;\n+    region.flushcache();\n+\n+    HRegion [] regions = null;\n+    try {\n+      regions = splitRegion(region, Bytes.toBytes(\"\" + splitRow));\n+      //Opening the regions returned.\n+      for (int i = 0; i < regions.length; i++) {\n+        regions[i] = HRegion.openHRegion(regions[i], null);\n+      }\n+      //Verifying that the region has been split\n+      assertEquals(2, regions.length);\n+\n+      //Verifying that all data is still there and that data is in the right\n+      //place\n+      verifyData(regions[0], startRow, numRows, qualifier, families);\n+      verifyData(regions[1], splitRow, numRows, qualifier, families);\n+\n+      //fire the reverse scan1:  top range, and larger than the last row\n+      Scan scan = new Scan(Bytes.toBytes(String.valueOf(startRow + 10 * numRows)));\n+      scan.setReversed(true);\n+      InternalScanner scanner = regions[1].getScanner(scan);\n+      List<Cell> currRow = new ArrayList<Cell>();\n+      boolean more = false;\n+      int verify = startRow + 2 * numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow + numRows - 1);\n+      scanner.close();\n+      //fire the reverse scan2:  top range, and equals to the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + 2 * numRows - 1)));\n+      scan.setReversed(true);\n+      scanner = regions[1].getScanner(scan);\n+      verify = startRow + 2 * numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow + numRows - 1);\n+      scanner.close();\n+      //fire the reverse scan3:  bottom range, and larger than the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + numRows)));\n+      scan.setReversed(true);\n+      scanner = regions[0].getScanner(scan);\n+      verify = startRow + numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, 99);\n+      scanner.close();\n+      //fire the reverse scan4:  bottom range, and equals to the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + numRows - 1)));\n+      scan.setReversed(true);\n+      scanner = regions[0].getScanner(scan);\n+      verify = startRow + numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow - 1);\n+      scanner.close();\n+    } finally {\n+      HRegion.closeHRegion(this.region);\n+      this.region = null;\n+    }\n+  }\n+\n   @Test\n   public void testWriteRequestsCounter() throws IOException {\n     byte[] fam = Bytes.toBytes(\"info\");",
                "raw_url": "https://github.com/apache/hbase/raw/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "sha": "992a978acd038a767481ca81287678f0b483d0cb",
                "status": "modified"
            }
        ],
        "message": "HBASE-12767 Fix a StoreFileScanner NPE in reverse scan flow",
        "parent": "https://github.com/apache/hbase/commit/02b03326506d5ffeb191a61a598763a99380e2a0",
        "patched_files": [
            "HRegion.java",
            "HalfStoreFileReader.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java",
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_c4911d3": {
        "bug_id": "hbase_c4911d3",
        "commit": "https://github.com/apache/hbase/commit/c4911d3230749947f107f2188b4da1615bb1fab1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=c4911d3230749947f107f2188b4da1615bb1fab1",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "patch": "@@ -154,7 +154,9 @@ public void stopReplicationService() {\n   public void join() {\n     if (this.replication) {\n       this.replicationManager.join();\n-      this.replicationSink.stopReplicationSinkServices();\n+      if (this.replicationSink != null) {\n+        this.replicationSink.stopReplicationSinkServices();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "sha": "71aa2c76ea8241a2dcc70c4610759916782be527",
                "status": "modified"
            }
        ],
        "message": "HBASE-8230 Possible NPE on regionserver abort if replication service has not been started\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1464280 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/e7bebd470650488cafd94a37462e6c1fd7833edf",
        "patched_files": [
            "Replication.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "ReplicationTests.java"
        ]
    },
    "hbase_c58e80f": {
        "bug_id": "hbase_c58e80f",
        "commit": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=c58e80fbe6db6426e652ec363149b92f9e33fbb0",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "patch": "@@ -492,7 +492,8 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region.getRegionNameAsString(), serverName.getHostname());\n+          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+              serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {\n             rs.close();\n@@ -1579,6 +1580,10 @@ private void createWriteTable(int numberOfServers) throws IOException {\n       try (RegionLocator regionLocator =\n                admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n         for (HRegionLocation location: regionLocator.getAllRegionLocations()) {\n+          if (location == null) {\n+            LOG.warn(\"Null location\");\n+            continue;\n+          }\n           ServerName rs = location.getServerName();\n           RegionInfo region = location.getRegion();\n           tasks.add(new RegionTask(admin.getConnection(), region, rs, (RegionStdOutSink)sink,\n@@ -1795,6 +1800,10 @@ private void monitorRegionServers(Map<String, List<RegionInfo>> rsAndRMap, Regio\n           try (RegionLocator regionLocator =\n                    this.admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n             for (HRegionLocation location : regionLocator.getAllRegionLocations()) {\n+              if (location == null) {\n+                LOG.warn(\"Null location\");\n+                continue;\n+              }\n               ServerName rs = location.getServerName();\n               String rsName = rs.getHostname();\n               RegionInfo r = location.getRegion();",
                "raw_url": "https://github.com/apache/hbase/raw/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "sha": "a967dab81750d974a570f908a02de69bd8fc4666",
                "status": "modified"
            }
        ],
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nSigned-off-by: Viraj Jasani <virajjasani007@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/b8a4504a2609d6d6d26e0b839a368912b3276feb",
        "patched_files": [
            "CanaryTool.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCanaryTool.java"
        ]
    },
    "hbase_c6b8e6f": {
        "bug_id": "hbase_c6b8e6f",
        "commit": "https://github.com/apache/hbase/commit/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
                "deletions": 15,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -345,19 +344,13 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive the mob data if there is a mob-enabled column\n-    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n-    boolean hasMob = MobUtils.hasMobColumns(htd);\n-    Path mobTableDir = null;\n-    if (hasMob) {\n-      // Archive mob data\n-      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-              tableName);\n-      Path regionDir =\n-              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-      if (fs.exists(regionDir)) {\n-        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n-      }\n+    // Archive mob data\n+    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+            tableName);\n+    Path regionDir =\n+            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+    if (fs.exists(regionDir)) {\n+      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -366,7 +359,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "sha": "1e86254a061de41e730c62f0b5bf28a69db51462",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hbase/blob/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
                "deletions": 24,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "patch": "@@ -195,32 +195,15 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n+    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n+  }\n \n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n+  @Test(timeout=90000)\n+  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n+    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n+    testRollbackAndDoubleExecution(htd);\n   }\n \n   @Test(timeout=90000)\n@@ -282,4 +265,31 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n+\n+  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+    TableName tableName = htd.getTableName();\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "sha": "676a3f4dec947a85be7f243cf086f23b976c233e",
                "status": "modified"
            }
        ],
        "message": "HBASE-14907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)\nPut back HBASE-14907 but with right JIRA number (by doing a revert of a revert)\n\nThis reverts commit 35a7b56e530f3e4a12f1968df5aee9d3b63815bb.",
        "parent": "https://github.com/apache/hbase/commit/a4a44587b3f2236750569f5c032b283dc77942f6",
        "patched_files": [
            "CreateTableProcedure.java",
            "DeleteTableProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCreateTableProcedure.java",
            "TestDeleteTableProcedure.java"
        ]
    },
    "hbase_c74cf12": {
        "bug_id": "hbase_c74cf12",
        "commit": "https://github.com/apache/hbase/commit/c74cf12925b810b7a59c5b639834508f00054053",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=c74cf12925b810b7a59c5b639834508f00054053",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -2223,6 +2223,12 @@ public int run(String[] args) throws Exception {\n         throw new IllegalArgumentException(\"Number of clients must be > 0\");\n       }\n \n+      // cmdName should not be null, print help and exit\n+      if (opts.cmdName == null) {\n+        printUsage();\n+        return errCode;\n+      }\n+\n       Class<? extends Test> cmdClass = determineCommandClass(opts.cmdName);\n       if (cmdClass != null) {\n         runTest(cmdClass, opts);",
                "raw_url": "https://github.com/apache/hbase/raw/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "7f1c6408c550319b449c4c78b58845919952b284",
                "status": "modified"
            }
        ],
        "message": "HBASE-17357 FIX PerformanceEvaluation parameters parsing triggers NPE.\n\ncheck command name is not null, if null print usage and exit\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/79018056f542cde5850b1d1fc2fe248f0007fd66",
        "patched_files": [
            "PerformanceEvaluation.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_c8e9a29": {
        "bug_id": "hbase_c8e9a29",
        "commit": "https://github.com/apache/hbase/commit/c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hbase/blob/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java?ref=c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
                "deletions": 5,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "patch": "@@ -298,23 +298,29 @@ public void updateRegionStats(ServerName serverName, byte[] regionName,\n   private final ConcurrentMap<String, Counter> cacheDroppingExceptions =\n     new ConcurrentHashMap<>(CAPACITY, LOAD_FACTOR, CONCURRENCY_LEVEL);\n \n-  public MetricsConnection(final ConnectionImplementation conn) {\n+  MetricsConnection(final ConnectionImplementation conn) {\n     this.scope = conn.toString();\n     this.registry = new MetricRegistry();\n-    final ThreadPoolExecutor batchPool = (ThreadPoolExecutor) conn.getCurrentBatchPool();\n-    final ThreadPoolExecutor metaPool = (ThreadPoolExecutor) conn.getCurrentMetaLookupPool();\n \n-    this.registry.register(name(this.getClass(), \"executorPoolActiveThreads\", scope),\n+    this.registry.register(getExecutorPoolName(),\n         new RatioGauge() {\n           @Override\n           protected Ratio getRatio() {\n+            ThreadPoolExecutor batchPool = (ThreadPoolExecutor) conn.getCurrentBatchPool();\n+            if (batchPool == null) {\n+              return Ratio.of(0, 0);\n+            }\n             return Ratio.of(batchPool.getActiveCount(), batchPool.getMaximumPoolSize());\n           }\n         });\n-    this.registry.register(name(this.getClass(), \"metaPoolActiveThreads\", scope),\n+    this.registry.register(getMetaPoolName(),\n         new RatioGauge() {\n           @Override\n           protected Ratio getRatio() {\n+            ThreadPoolExecutor metaPool = (ThreadPoolExecutor) conn.getCurrentMetaLookupPool();\n+            if (metaPool == null) {\n+              return Ratio.of(0, 0);\n+            }\n             return Ratio.of(metaPool.getActiveCount(), metaPool.getMaximumPoolSize());\n           }\n         });\n@@ -337,6 +343,21 @@ protected Ratio getRatio() {\n     this.reporter.start();\n   }\n \n+  @VisibleForTesting\n+  final String getExecutorPoolName() {\n+    return name(getClass(), \"executorPoolActiveThreads\", scope);\n+  }\n+\n+  @VisibleForTesting\n+  final String getMetaPoolName() {\n+    return name(getClass(), \"metaPoolActiveThreads\", scope);\n+  }\n+\n+  @VisibleForTesting\n+  MetricRegistry getMetricRegistry() {\n+    return registry;\n+  }\n+\n   public void shutdown() {\n     this.reporter.stop();\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "sha": "36627bda40938193636c5292a2753f799f5e2f06",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hbase/blob/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java?ref=c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
                "deletions": 6,
                "filename": "hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.client;\n \n+import com.codahale.metrics.RatioGauge;\n+import com.codahale.metrics.RatioGauge.Ratio;\n import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;\n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.ClientService;\n@@ -32,24 +34,28 @@\n import org.apache.hadoop.hbase.testclassification.SmallTests;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.AfterClass;\n-import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.mockito.Mockito;\n \n import java.io.IOException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import static org.junit.Assert.assertEquals;\n \n @Category({ClientTests.class, MetricsTests.class, SmallTests.class})\n public class TestMetricsConnection {\n \n   private static MetricsConnection METRICS;\n-\n+  private static final ExecutorService BATCH_POOL = Executors.newFixedThreadPool(2);\n   @BeforeClass\n   public static void beforeClass() {\n     ConnectionImplementation mocked = Mockito.mock(ConnectionImplementation.class);\n     Mockito.when(mocked.toString()).thenReturn(\"mocked-connection\");\n-    METRICS = new MetricsConnection(Mockito.mock(ConnectionImplementation.class));\n+    Mockito.when(mocked.getCurrentBatchPool()).thenReturn(BATCH_POOL);\n+    METRICS = new MetricsConnection(mocked);\n   }\n \n   @AfterClass\n@@ -112,9 +118,15 @@ public void testStaticMetrics() throws IOException {\n         METRICS.getTracker, METRICS.scanTracker, METRICS.multiTracker, METRICS.appendTracker,\n         METRICS.deleteTracker, METRICS.incrementTracker, METRICS.putTracker\n     }) {\n-      Assert.assertEquals(\"Failed to invoke callTimer on \" + t, loop, t.callTimer.getCount());\n-      Assert.assertEquals(\"Failed to invoke reqHist on \" + t, loop, t.reqHist.getCount());\n-      Assert.assertEquals(\"Failed to invoke respHist on \" + t, loop, t.respHist.getCount());\n+      assertEquals(\"Failed to invoke callTimer on \" + t, loop, t.callTimer.getCount());\n+      assertEquals(\"Failed to invoke reqHist on \" + t, loop, t.reqHist.getCount());\n+      assertEquals(\"Failed to invoke respHist on \" + t, loop, t.respHist.getCount());\n     }\n+    RatioGauge executorMetrics = (RatioGauge) METRICS.getMetricRegistry()\n+            .getMetrics().get(METRICS.getExecutorPoolName());\n+    RatioGauge metaMetrics = (RatioGauge) METRICS.getMetricRegistry()\n+            .getMetrics().get(METRICS.getMetaPoolName());\n+    assertEquals(Ratio.of(0, 3).getValue(), executorMetrics.getValue(), 0);\n+    assertEquals(Double.NaN, metaMetrics.getValue(), 0);\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "sha": "854ecc50221937dbae0a289af37cca42d7b9dd96",
                "status": "modified"
            }
        ],
        "message": "HBASE-16855 Avoid NPE in MetricsConnection\u2019s construction (ChiaPing Tsai)",
        "parent": "https://github.com/apache/hbase/commit/278625312047a2100f4dbb2d2eaa4e2219d00e14",
        "patched_files": [
            "MetricsConnection.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetricsConnection.java"
        ]
    },
    "hbase_c920eb3": {
        "bug_id": "hbase_c920eb3",
        "commit": "https://github.com/apache/hbase/commit/c920eb3c4c3fba8a52a6be8794e63d865a38196b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 4,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "patch": "@@ -93,9 +93,8 @@ public ClientScanner(final Configuration conf, final Scan scan,\n     public ClientScanner(final Configuration conf, final Scan scan,\n       final byte[] tableName, HConnection connection) throws IOException {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Creating scanner over \"\n-            + Bytes.toString(tableName)\n-            + \" starting at key '\" + Bytes.toStringBinary(scan.getStartRow()) + \"'\");\n+        LOG.debug(\"Scan table=\" + Bytes.toString(tableName)\n+            + \", startRow=\" + Bytes.toStringBinary(scan.getStartRow()));\n       }\n       this.scan = scan;\n       this.tableName = tableName;\n@@ -192,7 +191,7 @@ private boolean nextScanner(final boolean done)\n             done) {\n           close();\n           if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Finished scanning region \" + this.currentRegion);\n+            LOG.debug(\"Finished region=\" + this.currentRegion);\n           }\n           return false;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "sha": "ebab79921ae1b73336b4d76ed5a0d6c0445fef5e",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 14,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "patch": "@@ -1188,14 +1188,18 @@ private void cacheLocation(final byte [] tableName, final HRegionLocation source\n         }\n       }\n       if (isNewCacheEntry) {\n-        LOG.debug(\"Cached location for \" +\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Cached location for \" +\n             location.getRegionInfo().getRegionNameAsString() +\n             \" is \" + location.getHostnamePort());\n+        }\n       } else if (isStaleUpdate && !location.equals(oldLocation)) {\n-        LOG.debug(\"Ignoring stale location update for \"\n-          + location.getRegionInfo().getRegionNameAsString() + \": \"\n-          + location.getHostnamePort() + \" at \" + location.getSeqNum() + \"; local \"\n-          + oldLocation.getHostnamePort() + \" at \" + oldLocation.getSeqNum());\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Ignoring stale location update for \"\n+            + location.getRegionInfo().getRegionNameAsString() + \": \"\n+            + location.getHostnamePort() + \" at \" + location.getSeqNum() + \"; local \"\n+            + oldLocation.getHostnamePort() + \" at \" + oldLocation.getSeqNum());\n+        }\n       }\n     }\n \n@@ -1388,7 +1392,7 @@ Object makeStub() throws MasterNotRunningException {\n                 // tries at this point is 1 or more; decrement to start from 0.\n                 long pauseTime = ConnectionUtils.getPauseTime(pause, tries - 1);\n                 LOG.info(\"getMaster attempt \" + tries + \" of \" + numTries +\n-                    \" failed; retrying after sleep of \" +pauseTime + \", exception=\" +\n+                    \" failed; retrying after sleep of \" + pauseTime + \", exception=\" +\n                   exceptionCaught);\n \n                 try {\n@@ -2217,10 +2221,11 @@ private void submit(List<Action<R>> actionsList, final boolean isRetry) throws I\n           if (LOG.isTraceEnabled() && isRetry) {\n             StringBuilder sb = new StringBuilder();\n             for (Action<R> action : e.getValue().allActions()) {\n-              sb.append(Bytes.toStringBinary(action.getAction().getRow())).append(';');\n+              if (sb.length() > 0) sb.append(' ');\n+              sb.append(Bytes.toStringBinary(action.getAction().getRow()));\n             }\n-            LOG.trace(\"Will retry requests to [\" + e.getKey().getHostnamePort()\n-              + \"] after delay of [\" + backoffTime + \"] for rows [\" + sb.toString() + \"]\");\n+            LOG.trace(\"Attempt #\" + this.curNumRetries + \" against \" + e.getKey().getHostnamePort()\n+              + \" after=\" + backoffTime + \"ms, row(s)=\" + sb.toString());\n           }\n           Triple<MultiAction<R>, HRegionLocation, Future<MultiResponse>> p =\n             new Triple<MultiAction<R>, HRegionLocation, Future<MultiResponse>>(\n@@ -2280,11 +2285,10 @@ private void processBatchCallback() throws IOException, InterruptedException {\n         //  we had more than numRetries for any action\n         //  In this case, we will finish the current retries but we won't start new ones.\n         boolean lastRetry = false;\n-        // despite its name numRetries means number of tries. So if numRetries == 1 it means we\n-        //  won't retry. And we compare vs. 2 in case someone set it to zero.\n+        // If hci.numTries is 1 or 0, we do not retry.\n         boolean noRetry = (hci.numTries < 2);\n \n-        // Analyze and resubmit until all actions are done successfully or failed after numRetries\n+        // Analyze and resubmit until all actions are done successfully or failed after numTries\n         while (!this.inProgress.isEmpty()) {\n           // We need the original multi action to find out what actions to replay if\n           //  we have a 'total' failure of the Future<MultiResponse>\n@@ -2355,8 +2359,9 @@ private void processBatchCallback() throws IOException, InterruptedException {\n           // Retry all actions in toReplay then clear it.\n           if (!noRetry && !toReplay.isEmpty()) {\n             if (isTraceEnabled) {\n-              LOG.trace(\"Retrying due to errors\" + (lastRetry ? \" (one last time)\" : \"\")\n-                   + \": \" + retriedErrors.getDescriptionAndClear());\n+              LOG.trace(\"Retrying #\" + this.curNumRetries +\n+                (lastRetry ? \" (one last time)\": \"\") + \" because \" +\n+                retriedErrors.getDescriptionAndClear());\n             }\n             doRetry();\n             if (lastRetry) {",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "sha": "35d44dac9af6c8938a49195894860451d5a16489",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 2,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "patch": "@@ -156,8 +156,8 @@ public static void metaScan(Configuration configuration,\n       int rows = Math.min(rowLimit, configuration.getInt(HConstants.HBASE_META_SCANNER_CACHING,\n         HConstants.DEFAULT_HBASE_META_SCANNER_CACHING));\n       scan.setCaching(rows);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Scanning \" + Bytes.toString(metaTableName) + \" starting at row=\" +\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"Scanning \" + Bytes.toString(metaTableName) + \" starting at row=\" +\n           Bytes.toStringBinary(startRow) + \" for max=\" + rowUpperLimit + \" with caching=\" + rows);\n       }\n       // Run the scan",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "sha": "bdcf32638cb48eff090340718b4cf302b281d46a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 3,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "patch": "@@ -126,13 +126,14 @@ public String getExhaustiveDescription() {\n       Throwable t = this.exceptions.get(i);\n       Row action = this.actions.get(i);\n       String server = this.hostnameAndPort.get(i);\n-      pw.append(\"Error\");\n+      pw.append(\"exception\");\n       if (this.exceptions.size() > 1) {\n         pw.append(\" #\" + i);\n       }\n-      pw.append(\" from [\" + server + \"] for [\"\n-        + ((action == null) ? \"unknown key\" : Bytes.toStringBinary(action.getRow())) + \"]\");\n+      pw.append(\" from \" + server + \" for \"\n+        + ((action == null) ? \"unknown key\" : Bytes.toStringBinary(action.getRow())));\n       if (t != null) {\n+        pw.println();\n         t.printStackTrace(pw);\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "sha": "a1e22858d490f3e29d5f46d2616aeb2aaf2f67a0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "patch": "@@ -94,6 +94,10 @@ public ServerCallable(HConnection connection, byte [] tableName, byte [] row, in\n    */\n   public void prepare(final boolean reload) throws IOException {\n     this.location = connection.getRegionLocation(tableName, row, reload);\n+    if (this.location == null) {\n+      throw new IOException(\"Failed to find location, tableName=\" + tableName + \", row=\" +\n+        Bytes.toString(row) + \", reload=\" + reload);\n+    }\n     this.stub = connection.getClient(location.getServerName());\n   }\n \n@@ -169,7 +173,7 @@ public T withRetries()\n         prepare(tries != 0); // if called with false, check table status on ZK\n         return call();\n       } catch (Throwable t) {\n-        LOG.warn(\"Call exception, tries=\" + tries + \", numRetries=\" + numRetries + \": \" + t);\n+        LOG.warn(\"Call exception, tries=\" + tries + \", numRetries=\" + numRetries, t);\n \n         t = translateException(t);\n         // translateException throws an exception when we should not retry, i.e. when it's the",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "sha": "d98e461d4d2c4aeeb689a59058124837aff68ab5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 1,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -574,7 +574,7 @@\n   /**\n    * Default value of {@link #HBASE_CLIENT_RETRIES_NUMBER}.\n    */\n-  public static int DEFAULT_HBASE_CLIENT_RETRIES_NUMBER = 10;\n+  public static int DEFAULT_HBASE_CLIENT_RETRIES_NUMBER = 20;\n \n   /**\n    * Parameter name for client prefetch limit, used as the maximum number of regions",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "e18d9da64a149607eb10f053ada4e8414b3b4173",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 2,
                "filename": "hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "patch": "@@ -31,13 +31,13 @@\n /**\n  * A system test which does large data ingestion and verify using {@link LoadTestTool}.\n  * It performs a set of actions deterministically using ChaosMonkey, then starts killing\n- * things randomly. You can configure how long should the load test run by using \n+ * things randomly. You can configure how long should the load test run by using\n  * \"hbase.IntegrationTestDataIngestSlowDeterministic.runtime\" configuration parameter.\n  */\n @Category(IntegrationTests.class)\n public class IntegrationTestDataIngestSlowDeterministic extends IngestIntegrationTestBase {\n   private static final int SERVER_COUNT = 4; // number of slaves for the smallest cluster\n-  private static final long DEFAULT_RUN_TIME = 30 * 60 * 1000;\n+  private static final long DEFAULT_RUN_TIME = 10 * 60 * 1000;\n   private static final long CHAOS_EVERY_MS = 150 * 1000; // Chaos every 2.5 minutes.\n \n   private ChaosMonkey monkey;",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "sha": "9d91a58a9e4003ae99638a3b62149ae0ca066a77",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 4,
                "filename": "hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "patch": "@@ -22,13 +22,11 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n-import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n import java.util.Queue;\n import java.util.Random;\n-import java.util.Set;\n \n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.logging.Log;\n@@ -38,8 +36,8 @@\n import org.apache.hadoop.hbase.HBaseCluster;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.IntegrationTestingUtility;\n import org.apache.hadoop.hbase.IntegrationTestDataIngestWithChaosMonkey;\n+import org.apache.hadoop.hbase.IntegrationTestingUtility;\n import org.apache.hadoop.hbase.ServerLoad;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.Stoppable;\n@@ -49,7 +47,6 @@\n \n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n-import com.google.protobuf.ServiceException;\n \n /**\n  * A utility to injects faults in a running cluster.\n@@ -158,6 +155,7 @@ void init(ActionContext context) throws Exception {\n     /** Returns current region servers */\n     protected ServerName[] getCurrentServers() throws IOException {\n       Collection<ServerName> regionServers = cluster.getClusterStatus().getServers();\n+      if (regionServers == null || regionServers.size() <= 0) return new ServerName [] {};\n       return regionServers.toArray(new ServerName[regionServers.size()]);\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "sha": "aa252ee0a621bf02846066e507807d1f4e5d0ae2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "deletions": 3,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "patch": "@@ -173,8 +173,8 @@ public void run() {\n   }\n \n   public void insert(HTable table, Put put, long keyBase) {\n+    long start = System.currentTimeMillis();\n     try {\n-      long start = System.currentTimeMillis();\n       table.put(put);\n       totalOpTimeMs.addAndGet(System.currentTimeMillis() - start);\n     } catch (IOException e) {\n@@ -190,8 +190,8 @@ public void insert(HTable table, Put put, long keyBase) {\n         pw.flush();\n         exceptionInfo = StringUtils.stringifyException(e);\n       }\n-      LOG.error(\"Failed to insert: \" + keyBase + \"; region information: \"\n-          + getRegionDebugInfoSafe(table, put.getRow()) + \"; errors: \"\n+      LOG.error(\"Failed to insert: \" + keyBase + \" after \" + (System.currentTimeMillis() - start) +\n+        \"ms; region information: \" + getRegionDebugInfoSafe(table, put.getRow()) + \"; errors: \"\n           + exceptionInfo);\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "sha": "41ba1262641a61281c78267b9cd2c30f5617c64d",
                "status": "modified"
            }
        ],
        "message": "HBASE-8657 Miscellaneous log fixups for hbase-it; tidier logging, fix a few NPEs\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1487945 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/957f580d8390ee77673280003bfd5a51b22504c6",
        "patched_files": [
            "RetriesExhaustedWithDetailsException.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRetriesExhaustedWithDetailsException.java"
        ]
    },
    "hbase_ca6e67a": {
        "bug_id": "hbase_ca6e67a",
        "commit": "https://github.com/apache/hbase/commit/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "patch": "@@ -136,7 +136,7 @@\n  *                             columns: info:merge0001, info:merge0002. You make also see 'mergeA',\n  *                             and 'mergeB'. This is old form replaced by the new format that allows\n  *                             for more than two parents to be merged at a time.\n- * TODO: Add rep_barrier for serial replication explaination.\n+ * TODO: Add rep_barrier for serial replication explaination. See SerialReplicationChecker.\n  * </pre>\n  * </p>\n  * <p>\n@@ -607,6 +607,7 @@ private static Scan getMetaScan(Connection connection, int rowUpperLimit) {\n    * @param excludeOfflinedSplitParents don't return split parents\n    * @return Return list of regioninfos and server addresses.\n    */\n+  // What happens here when 1M regions in hbase:meta? This won't scale?\n   public static List<Pair<RegionInfo, ServerName>> getTableRegionsAndLocations(\n       Connection connection, @Nullable final TableName tableName,\n       final boolean excludeOfflinedSplitParents) throws IOException {\n@@ -1928,6 +1929,9 @@ public static Put makePutForReplicationBarrier(RegionInfo regionInfo, long openS\n     return put;\n   }\n \n+  /**\n+   * See class comment on SerialReplicationChecker\n+   */\n   public static void addReplicationBarrier(Put put, long openSeqNum) throws IOException {\n     put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n       .setRow(put.getRow())",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "sha": "f93c3cce9fa6168ec5fe87d6e373f97c9f6f525e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 2,
                "filename": "hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "patch": "@@ -270,7 +270,7 @@ protected void doGet(final HttpServletRequest req, final HttpServletResponse res\n             resp.getWriter().write(\n               \"Started [\" + event.getInternalName() +\n               \"] profiling. This page will automatically redirect to \" +\n-              relativeUrl + \" after \" + duration + \" seconds.\\n\\ncommand:\\n\" +\n+              relativeUrl + \" after \" + duration + \" seconds.\\n\\nCommand:\\n\" +\n               Joiner.on(\" \").join(cmd));\n \n             // to avoid auto-refresh by ProfileOutputServlet, refreshDelay can be specified\n@@ -395,4 +395,4 @@ protected void doGet(final HttpServletRequest req, final HttpServletResponse res\n \n   }\n \n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "sha": "fc75530cc50fcb5c6937df2798e98c308b0bbdb5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 2,
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "patch": "@@ -407,7 +407,15 @@ void updateState(long procId, boolean isDeleted) {\n     int wordIndex = bitmapIndex >> ADDRESS_BITS_PER_WORD;\n     long value = (1L << bitmapIndex);\n \n-    modified[wordIndex] |= value;\n+    try {\n+      modified[wordIndex] |= value;\n+    } catch (ArrayIndexOutOfBoundsException aioobe) {\n+      // We've gotten a AIOOBE in here; add detail to help debug.\n+      ArrayIndexOutOfBoundsException aioobe2 =\n+          new ArrayIndexOutOfBoundsException(\"pid=\" + procId + \", deleted=\" + isDeleted);\n+      aioobe2.initCause(aioobe);\n+      throw aioobe2;\n+    }\n     if (isDeleted) {\n       deleted[wordIndex] |= value;\n     } else {\n@@ -431,4 +439,4 @@ private static long alignUp(final long x) {\n   private static long alignDown(final long x) {\n     return x & -BITS_PER_WORD;\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "sha": "78d2d91ca8661e961539fc2f479e21bed87086ac",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 10,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "patch": "@@ -190,10 +190,10 @@ private void loadRegionsFromInMemoryState() {\n       RegionInfo regionInfo = regionState.getRegion();\n       if (master.getTableStateManager()\n           .isTableState(regionInfo.getTable(), TableState.State.DISABLED)) {\n-        disabledTableRegions.add(regionInfo.getEncodedName());\n+        disabledTableRegions.add(regionInfo.getRegionNameAsString());\n       }\n       if (regionInfo.isSplitParent()) {\n-        splitParentRegions.add(regionInfo.getEncodedName());\n+        splitParentRegions.add(regionInfo.getRegionNameAsString());\n       }\n       HbckRegionInfo.MetaEntry metaEntry =\n           new HbckRegionInfo.MetaEntry(regionInfo, regionState.getServerName(),\n@@ -212,7 +212,7 @@ private void loadRegionsFromRSReport() {\n         String encodedRegionName = RegionInfo.encodeRegionName(regionName);\n         HbckRegionInfo hri = regionInfoMap.get(encodedRegionName);\n         if (hri == null) {\n-          orphanRegionsOnRS.put(encodedRegionName, serverName);\n+          orphanRegionsOnRS.put(RegionInfo.getRegionNameAsString(regionName), serverName);\n           continue;\n         }\n         hri.addServer(hri.getMetaEntry(), serverName);\n@@ -223,29 +223,31 @@ private void loadRegionsFromRSReport() {\n         numRegions, rsReports.size(), orphanRegionsOnFS.size());\n \n     for (Map.Entry<String, HbckRegionInfo> entry : regionInfoMap.entrySet()) {\n-      String encodedRegionName = entry.getKey();\n       HbckRegionInfo hri = entry.getValue();\n       ServerName locationInMeta = hri.getMetaEntry().getRegionServer();\n       if (hri.getDeployedOn().size() == 0) {\n         if (locationInMeta == null) {\n           continue;\n         }\n         // skip the offline region which belong to disabled table.\n-        if (disabledTableRegions.contains(encodedRegionName)) {\n+        if (disabledTableRegions.contains(hri.getRegionNameAsString())) {\n           continue;\n         }\n         // skip the split parent regions\n-        if (splitParentRegions.contains(encodedRegionName)) {\n+        if (splitParentRegions.contains(hri.getRegionNameAsString())) {\n           continue;\n         }\n         // Master thought this region opened, but no regionserver reported it.\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, new LinkedList<>()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, new LinkedList<>()));\n       } else if (hri.getDeployedOn().size() > 1) {\n         // More than one regionserver reported opened this region\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, hri.getDeployedOn()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, hri.getDeployedOn()));\n       } else if (!hri.getDeployedOn().get(0).equals(locationInMeta)) {\n         // Master thought this region opened on Server1, but regionserver reported Server2\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, hri.getDeployedOn()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, hri.getDeployedOn()));\n       }\n     }\n   }\n@@ -339,4 +341,4 @@ public long getCheckingStartTimestamp() {\n   public long getCheckingEndTimestamp() {\n     return this.checkingEndTimestamp;\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "sha": "cf4368581e708413894a022108d81bcd0a25d97a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 4,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "patch": "@@ -50,12 +50,13 @@\n  * </p>\n  * <p>\n  * We record all the open sequence number for a region in a special family in meta, which is called\n- * 'barrier', so there will be a sequence of open sequence number (b1, b2, b3, ...). We call [bn,\n- * bn+1) a range, and it is obvious that a region will always be on the same RS within a range.\n+ * 'rep_barrier', so there will be a sequence of open sequence number (b1, b2, b3, ...). We call\n+ * [bn, bn+1) a range, and it is obvious that a region will always be on the same RS within a\n+ * range.\n  * <p>\n  * When split and merge, we will also record the parent for the generated region(s) in the special\n- * family in meta. And also, we will write an extra 'open sequence number' for the parent region(s),\n- * which is the max sequence id of the region plus one.\n+ * family in meta. And also, we will write an extra 'open sequence number' for the parent\n+ * region(s), which is the max sequence id of the region plus one.\n  * </p>\n  * </p>\n  * <p>",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "sha": "321bbb420bc74a5a6c9d352b4950d9286453fd10",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "patch": "@@ -277,13 +277,15 @@ public void publishReadTiming(String znode, String server, long msTime) {\n \n     public void publishReadFailure(ServerName serverName, RegionInfo region, Exception e) {\n       incReadFailureCount();\n-      LOG.error(\"Read from {} on {} failed\", region.getRegionNameAsString(), serverName, e);\n+      LOG.error(\"Read from {} on serverName={} failed\",\n+          region.getRegionNameAsString(), serverName, e);\n     }\n \n     public void publishReadFailure(ServerName serverName, RegionInfo region,\n         ColumnFamilyDescriptor column, Exception e) {\n       incReadFailureCount();\n-      LOG.error(\"Read from {} on {} {} failed\", region.getRegionNameAsString(), serverName,\n+      LOG.error(\"Read from {} on serverName={}, columnFamily={} failed\",\n+          region.getRegionNameAsString(), serverName,\n           column.getNameAsString(), e);\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "sha": "af9b879bfe625262fd0ba02a043721640d16fb22",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 3,
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "patch": "@@ -78,7 +78,7 @@\n \n   <div class=\"row\">\n     <div class=\"page-header\">\n-  <p><span>This page displays two reports: the 'HBCK Chore Report' and the 'CatalogJanitor Consistency Issues' report. Only titles show if there are no problems to report. Note some conditions are transitory as regions migrate.</span></p>\n+      <p><span>This page displays two reports: the 'HBCK Chore Report' and the 'CatalogJanitor Consistency Issues' report. Only titles show if there are no problems to report. Note some conditions are <em>transitory</em> as regions migrate.</span></p>\n     </div>\n   </div>\n   <div class=\"row\">\n@@ -119,7 +119,7 @@\n \n   <table class=\"table table-striped\">\n     <tr>\n-      <th>Region Encoded Name</th>\n+      <th>Region Name</th>\n       <th>Location in META</th>\n       <th>Reported Online RegionServers</th>\n     </tr>\n@@ -142,10 +142,18 @@\n       <h2>Orphan Regions on RegionServer</h2>\n     </div>\n   </div>\n+      <p>\n+        <span>\n+          The below are Regions we've lost account of. To be safe, run bulk load of any data found in these Region orphan directories back into the HBase cluster.\n+          First make sure hbase:meta is in healthy state; run 'hbkc2 fixMeta' to be sure. Once this is done, per Region below, run a bulk\n+          load -- '$ hbase completebulkload REGION_DIR_PATH TABLE_NAME' -- and then delete the desiccated directory content (HFiles are removed upon successful load; all that is left are empty directories\n+          and occasionally a seqid marking file).\n+        </span>\n+      </p>\n \n   <table class=\"table table-striped\">\n     <tr>\n-      <th>Region Encoded Name</th>\n+      <th>Region Name</th>\n       <th>Reported Online RegionServer</th>\n     </tr>\n     <% for (Map.Entry<String, ServerName> entry : orphanRegionsOnRS.entrySet()) { %>",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "sha": "f89aac8bce6fdf49739ba08d0c017da5cba35b2a",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 40,
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "patch": "@@ -81,11 +81,14 @@\n         <th>Errors</th>\n         <th>Parameters</th>\n     </tr>\n-    <% for (Procedure<?> proc : procedures) { \n+    <%\n+      int displayCount = 0;\n+      for (Procedure<?> proc : procedures) {\n       // Don't show SUCCESS procedures.\n       if (proc.isSuccess()) {\n         continue;\n       }\n+      displayCount++;\n     %>\n       <tr>\n         <td><%= proc.getProcId() %></td>\n@@ -99,9 +102,63 @@\n         <td><%= escapeXml(proc.toString()) %></td>\n       </tr>\n     <% } %>\n+    <%\n+    if (displayCount > 0) {\n+    %>\n+      <p><%= displayCount %> procedure(s).</p>\n+    <%\n+    }\n+    %>\n   </table>\n </div>\n <br />\n+<div class=\"container-fluid content\">\n+  <div class=\"row\">\n+      <div class=\"page-header\">\n+          <h1>Locks</h1>\n+      </div>\n+  </div>\n+    <%\n+    if (lockedResources.size() > 0) {\n+    %>\n+    <p><%= lockedResources.size() %> lock(s).</p>\n+    <%\n+    }\n+    %>\n+  <% for (LockedResource lockedResource : lockedResources) { %>\n+    <h2><%= lockedResource.getResourceType() %>: <%= lockedResource.getResourceName() %></h2>\n+    <%\n+      switch (lockedResource.getLockType()) {\n+      case EXCLUSIVE:\n+    %>\n+    <p>Lock type: EXCLUSIVE</p>\n+    <p>Owner procedure: <%= escapeXml(lockedResource.getExclusiveLockOwnerProcedure().toStringDetails()) %></p>\n+    <%\n+        break;\n+      case SHARED:\n+    %>\n+    <p>Lock type: SHARED</p>\n+    <p>Number of shared locks: <%= lockedResource.getSharedLockCount() %></p>\n+    <%\n+        break;\n+      }\n+\n+      List<Procedure<?>> waitingProcedures = lockedResource.getWaitingProcedures();\n+\n+      if (!waitingProcedures.isEmpty()) {\n+    %>\n+        <h3>Waiting procedures</h3>\n+        <table class=\"table table-striped\" width=\"90%\" >\n+        <% for (Procedure<?> proc : procedures) { %>\n+         <tr>\n+            <td><%= escapeXml(proc.toStringDetails()) %></td>\n+          </tr>\n+        <% } %>\n+        </table>\n+    <% } %>\n+  <% } %>\n+</div>\n+<br />\n <div class=\"container-fluid content\">\n   <div class=\"row\">\n     <div class=\"page-header\">\n@@ -206,44 +263,5 @@\n   </div>\n </div>\n <br />\n-<div class=\"container-fluid content\">\n-  <div class=\"row\">\n-      <div class=\"page-header\">\n-          <h1>Locks</h1>\n-      </div>\n-  </div>\n-  <% for (LockedResource lockedResource : lockedResources) { %>\n-    <h2><%= lockedResource.getResourceType() %>: <%= lockedResource.getResourceName() %></h2>\n-    <%\n-      switch (lockedResource.getLockType()) {\n-      case EXCLUSIVE:\n-    %>\n-    <p>Lock type: EXCLUSIVE</p>\n-    <p>Owner procedure: <%= escapeXml(lockedResource.getExclusiveLockOwnerProcedure().toStringDetails()) %></p>\n-    <%\n-        break;\n-      case SHARED:\n-    %>\n-    <p>Lock type: SHARED</p>\n-    <p>Number of shared locks: <%= lockedResource.getSharedLockCount() %></p>\n-    <%\n-        break;\n-      }\n-\n-      List<Procedure<?>> waitingProcedures = lockedResource.getWaitingProcedures();\n-\n-      if (!waitingProcedures.isEmpty()) {\n-    %>\n-        <h3>Waiting procedures</h3>\n-        <table class=\"table table-striped\" width=\"90%\" >\n-        <% for (Procedure<?> proc : procedures) { %>\n-         <tr>\n-            <td><%= escapeXml(proc.toStringDetails()) %></td>\n-          </tr>\n-        <% } %>\n-        </table>\n-    <% } %>\n-  <% } %>\n-</div>\n \n <jsp:include page=\"footer.jsp\" />",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "sha": "ea252cff39a1fb8c2bbc398a42b0ec08ddc25b04",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "deletions": 3,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "patch": "@@ -69,7 +69,7 @@ public void setUp() throws Exception {\n   @Test\n   public void testForMeta() {\n     byte[] metaRegionNameAsBytes = RegionInfoBuilder.FIRST_META_REGIONINFO.getRegionName();\n-    String metaRegionName = RegionInfoBuilder.FIRST_META_REGIONINFO.getEncodedName();\n+    String metaRegionName = RegionInfoBuilder.FIRST_META_REGIONINFO.getRegionNameAsString();\n     List<ServerName> serverNames = master.getServerManager().getOnlineServersList();\n     assertEquals(NSERVERS, serverNames.size());\n \n@@ -96,7 +96,7 @@ public void testForMeta() {\n   public void testForUserTable() throws Exception {\n     TableName tableName = TableName.valueOf(\"testForUserTable\");\n     RegionInfo hri = createRegionInfo(tableName, 1);\n-    String regionName = hri.getEncodedName();\n+    String regionName = hri.getRegionNameAsString();\n     rsDispatcher.setMockRsExecutor(new GoodRsExecutor());\n     Future<byte[]> future = submitProcedure(createAssignProcedure(hri));\n     waitOnFuture(future);\n@@ -154,7 +154,7 @@ public void testForUserTable() throws Exception {\n   public void testForDisabledTable() throws Exception {\n     TableName tableName = TableName.valueOf(\"testForDisabledTable\");\n     RegionInfo hri = createRegionInfo(tableName, 1);\n-    String regionName = hri.getEncodedName();\n+    String regionName = hri.getRegionNameAsString();\n     rsDispatcher.setMockRsExecutor(new GoodRsExecutor());\n     Future<byte[]> future = submitProcedure(createAssignProcedure(hri));\n     waitOnFuture(future);",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "sha": "ea7050824942762fb5f21ed9e6a272de32646e62",
                "status": "modified"
            }
        ],
        "message": "HBASE-23315 Miscellaneous HBCK Report page cleanup\n\n * Add a bit of javadoc around SerialReplicationChecker.\n * Miniscule edit to the profiler jsp page and then a bit of doc on how to make it work that might help.\n * Add some detail if NPE getting BitSetNode to help w/ debug.\n * Change HbckChore to log region names instead of encoded names; helps doing diagnostics; can take region name and query in shell to find out all about the region according to hbase:meta.\n * Add some fix-it help inline in the HBCK Report page \u2013 how to fix.\n * Add counts in procedures page so can see if making progress; move listing of WALs to end of the page.",
        "parent": "https://github.com/apache/hbase/commit/e83bb205f4fc3555abcdca7c4c985a3f76150f16",
        "patched_files": [
            "hbck.java",
            "MetaTableAccessor.java",
            "procedures.java",
            "CanaryTool.java",
            "BitSetNode.java",
            "ProfileServlet.java",
            "HbckChore.java",
            "SerialReplicationChecker.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCanaryTool.java",
            "TestHbckChore.java",
            "TestSerialReplicationChecker.java",
            "TestBitSetNode.java",
            "TestMetaTableAccessor.java"
        ]
    },
    "hbase_ccf42ac": {
        "bug_id": "hbase_ccf42ac",
        "commit": "https://github.com/apache/hbase/commit/ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -61,6 +61,7 @@ Trunk (unreleased changes)\n     HADOOP-1923, HADOOP-1924 a) tests fail sporadically because set up and tear\n                  down is inconsistent b) TestDFSAbort failed in nightly #242\n     HADOOP-1929 Add hbase-default.xml to hbase jar\n+    HADOOP-1941 StopRowFilter throws NPE when passed null row\n \n   IMPROVEMENTS\n     HADOOP-1737 Make HColumnDescriptor data publically members settable",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/CHANGES.txt",
                "sha": "291671e0d8fe0b29a0d240fabbb0de7fd6bc6c69",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "patch": "@@ -98,6 +98,12 @@ public boolean filterAllRemaining() {\n \n   /** {@inheritDoc} */\n   public boolean filter(final Text rowKey) {\n+    if (rowKey == null) {\n+      if (this.stopRowKey == null) {\n+        return true;\n+      }\n+      return false;\n+    }\n     boolean result = this.stopRowKey.compareTo(rowKey) <= 0;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Filter result for rowKey: \" + rowKey + \".  Result: \" + ",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "sha": "be638140097e79fb09007b6e1d63e99747234140",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "patch": "@@ -87,5 +87,7 @@ private void stopRowTests(RowFilterInterface filter) throws Exception {\n \n     assertFalse(\"FilterAllRemaining\", filter.filterAllRemaining());\n     assertFalse(\"FilterNotNull\", filter.filterNotNull(null));\n+    \n+    assertFalse(\"Filter a null\", filter.filter(null));\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "sha": "dc5b867741349928ae5ed1cd8f5d35116e2c8b62",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1941 StopRowFilter throws NPE when passed null row\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@579410 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/87f305b608f7449ba2b622d764dbb80ab870602b",
        "patched_files": [
            "CHANGES.java",
            "StopRowFilter.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStopRowFilter.java"
        ]
    },
    "hbase_cd161d9": {
        "bug_id": "hbase_cd161d9",
        "commit": "https://github.com/apache/hbase/commit/cd161d976ef47b84e904f2d54bac65d2f3417c2a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/Cell.proto?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "deletions": 0,
                "filename": "hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "patch": "@@ -32,6 +32,7 @@ enum CellType {\n     PUT = 4;\n \n     DELETE = 8;\n+    DELETE_FAMILY_VERSION = 10;\n     DELETE_COLUMN = 12;\n     DELETE_FAMILY = 14;\n ",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "sha": "aada35329aafa97c63beae8c7c59b2131755b072",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol/src/main/protobuf/Cell.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol/src/main/protobuf/Cell.proto?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "deletions": 0,
                "filename": "hbase-protocol/src/main/protobuf/Cell.proto",
                "patch": "@@ -32,6 +32,7 @@ enum CellType {\n     PUT = 4;\n \n     DELETE = 8;\n+    DELETE_FAMILY_VERSION = 10;\n     DELETE_COLUMN = 12;\n     DELETE_FAMILY = 14;\n ",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol/src/main/protobuf/Cell.proto",
                "sha": "e518e658f63ba53ca9e9df58bc7f350377739a23",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.client;\n \n+import static org.apache.hadoop.hbase.HConstants.RPC_CODEC_CONF_KEY;\n+import static org.apache.hadoop.hbase.ipc.RpcClient.DEFAULT_CODEC_CLASS;\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -608,6 +610,30 @@ public void testGetRowOffset() throws Exception {\n        \"Testing offset + multiple CFs + maxResults\");\n   }\n \n+  @Test\n+  public void testScanRawDeleteFamilyVersion() throws Exception {\n+    TableName tableName = TableName.valueOf(name.getMethodName());\n+    TEST_UTIL.createTable(tableName, FAMILY);\n+    Configuration conf = new Configuration(TEST_UTIL.getConfiguration());\n+    conf.set(RPC_CODEC_CONF_KEY, \"\");\n+    conf.set(DEFAULT_CODEC_CLASS, \"\");\n+    try (Connection connection = ConnectionFactory.createConnection(conf);\n+        Table table = connection.getTable(tableName)) {\n+      Delete delete = new Delete(ROW);\n+      delete.addFamilyVersion(FAMILY, 0L);\n+      table.delete(delete);\n+      Scan scan = new Scan(ROW).setRaw(true);\n+      ResultScanner scanner = table.getScanner(scan);\n+      int count = 0;\n+      while (scanner.next() != null) {\n+        count++;\n+      }\n+      assertEquals(1, count);\n+    } finally {\n+      TEST_UTIL.deleteTable(tableName);\n+    }\n+  }\n+\n   /**\n    * Test from client side for scan while the region is reopened\n    * on the same region server.",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "sha": "5e8d107d4f58b7cd31764e1a08beca16b5930283",
                "status": "modified"
            }
        ],
        "message": "HBASE-21204 NPE when scan raw DELETE_FAMILY_VERSION and codec is not set\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/dc767c06d27c57e02d8963515317d54e89ddc718",
        "patched_files": [
            "Cell.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestScannersFromClientSide.java"
        ]
    },
    "hbase_cdee1a7": {
        "bug_id": "hbase_cdee1a7",
        "commit": "https://github.com/apache/hbase/commit/cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java?ref=cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "patch": "@@ -314,7 +314,8 @@ public boolean equals(Object obj) {\n     }\n     TablePermission other = (TablePermission)obj;\n \n-    if (!(table.equals(other.getTableName()) &&\n+    if (!(((table == null && other.getTableName() == null) ||\n+           (table != null && table.equals(other.getTableName()))) &&\n         ((family == null && other.getFamily() == null) ||\n          Bytes.equals(family, other.getFamily())) &&\n         ((qualifier == null && other.getQualifier() == null) ||",
                "raw_url": "https://github.com/apache/hbase/raw/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "sha": "cf3f071a56a2dbb456a89a119f24ff7dc5648fab",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java?ref=cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "patch": "@@ -83,6 +83,8 @@ public boolean isAborted() {\n     }\n   };\n \n+  private static String TEST_NAMESPACE = \"perms_test_ns\";\n+  private static String TEST_NAMESPACE2 = \"perms_test_ns2\";\n   private static TableName TEST_TABLE =\n       TableName.valueOf(\"perms_test\");\n   private static TableName TEST_TABLE2 =\n@@ -409,6 +411,15 @@ public void testEquals() throws Exception {\n     p2 = new TablePermission(TEST_TABLE, null);\n     assertFalse(p1.equals(p2));\n     assertFalse(p2.equals(p1));\n+\n+    p1 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    p2 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    assertEquals(p1, p2);\n+\n+    p1 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    p2 = new TablePermission(TEST_NAMESPACE2, TablePermission.Action.READ);\n+    assertFalse(p1.equals(p2));\n+    assertFalse(p2.equals(p1));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hbase/raw/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "sha": "1e525e2008f9371558e4e2d20764b670665541c3",
                "status": "modified"
            }
        ],
        "message": "HBASE-17450 TablePermission#equals throws NPE after namespace support was added (huzheng)",
        "parent": "https://github.com/apache/hbase/commit/7794c530bd8dea47e16ca5329270aecc46ea9c8f",
        "patched_files": [
            "TablePermission.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTablePermissions.java"
        ]
    },
    "hbase_ce50830": {
        "bug_id": "hbase_ce50830",
        "commit": "https://github.com/apache/hbase/commit/ce50830a0af29e0ad2be24528629965923ef1cbf",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=ce50830a0af29e0ad2be24528629965923ef1cbf",
                "deletions": 9,
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "patch": "@@ -217,20 +217,22 @@ protected void periodicExecute(final TEnvironment env) {\n \n         // TODO: Select TTL based on Procedure type\n         if (retainer.isExpired(now, evictTtl, evictAckTtl)) {\n-          if (debugEnabled) {\n-            LOG.debug(\"Evict completed \" + proc);\n-          }\n-          batchIds[batchCount++] = entry.getKey();\n-          if (batchCount == batchIds.length) {\n-            store.delete(batchIds, 0, batchCount);\n-            batchCount = 0;\n+          // Failed procedures aren't persisted in WAL.\n+          if (!(proc instanceof FailedProcedure)) {\n+            batchIds[batchCount++] = entry.getKey();\n+            if (batchCount == batchIds.length) {\n+              store.delete(batchIds, 0, batchCount);\n+              batchCount = 0;\n+            }\n           }\n-          it.remove();\n-\n           final NonceKey nonceKey = proc.getNonceKey();\n           if (nonceKey != null) {\n             nonceKeysToProcIdsMap.remove(nonceKey);\n           }\n+          it.remove();\n+          if (debugEnabled) {\n+            LOG.debug(\"Evict completed \" + proc);\n+          }\n         }\n       }\n       if (batchCount > 0) {",
                "raw_url": "https://github.com/apache/hbase/raw/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "sha": "4a612998b307f2a691b03d56c1642b0780c62a64",
                "status": "modified"
            },
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/hbase/blob/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java?ref=ce50830a0af29e0ad2be24528629965923ef1cbf",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hadoop.hbase.procedure;\n+\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.MasterObserver;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.procedure2.Procedure;\n+import org.apache.hadoop.hbase.security.AccessDeniedException;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Check if CompletedProcedureCleaner cleans up failed nonce procedures.\n+ */\n+@Category(MediumTests.class)\n+public class TestFailedProcCleanup {\n+  private static final Log LOG = LogFactory.getLog(TestFailedProcCleanup.class);\n+\n+  protected static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final TableName TABLE = TableName.valueOf(\"test\");\n+  private static final byte[] FAMILY = Bytes.toBytesBinary(\"f\");\n+  private static final int evictionDelay = 10 * 1000;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setInt(\"hbase.procedure.cleaner.evict.ttl\", evictionDelay);\n+    conf.setInt(\"hbase.procedure.cleaner.evict.batch.size\", 1);\n+    conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, CreateFailObserver.class.getName());\n+    TEST_UTIL.startMiniCluster(3);\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    TEST_UTIL.cleanupTestDir();\n+    TEST_UTIL.cleanupDataTestDirOnTestFS();\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test\n+  public void testFailCreateTable() throws Exception {\n+    try {\n+      TEST_UTIL.createTable(TABLE, FAMILY);\n+      fail(\"Table shouldn't be created\");\n+    } catch (AccessDeniedException e) {\n+      LOG.debug(\"Ignoring exception: \", e);\n+      Thread.sleep(evictionDelay * 3);\n+    }\n+    List<Procedure<?>> procedureInfos =\n+        TEST_UTIL.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor().getProcedures();\n+    for (Procedure procedureInfo : procedureInfos) {\n+      if (procedureInfo.getProcName().equals(\"CreateTableProcedure\")\n+          && procedureInfo.getState() == ProcedureProtos.ProcedureState.ROLLEDBACK) {\n+        fail(\"Found procedure \" + procedureInfo + \" that hasn't been cleaned up\");\n+      }\n+    }\n+  }\n+\n+  public static class CreateFailObserver implements MasterCoprocessor, MasterObserver {\n+\n+    @Override\n+    public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> env,\n+        TableDescriptor desc, RegionInfo[] regions) throws IOException {\n+\n+      if (desc.getTableName().equals(TABLE)) {\n+        throw new AccessDeniedException(\"Don't allow creation of table\");\n+      }\n+    }\n+\n+    @Override\n+    public Optional<MasterObserver> getMasterObserver() {\n+      return Optional.of(this);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "sha": "740caea09d368d67849b7d279bd2b257e0395c87",
                "status": "added"
            }
        ],
        "message": "HBASE-19756 Master NPE during completed failed proc eviction\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/98cae45d2ba0854592956e8d7ed5976d3eaa5da6",
        "patched_files": [
            "ProcedureExecutor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestFailedProcCleanup.java",
            "TestProcedureExecutor.java"
        ]
    },
    "hbase_cf3284d": {
        "bug_id": "hbase_cf3284d",
        "commit": "https://github.com/apache/hbase/commit/cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -168,6 +168,7 @@ Release 0.90.2 - Unreleased\n    HBASE-3617  NoRouteToHostException during balancing will cause Master abort\n                (Ted Yu via Stack)\n    HBASE-3668  CatalogTracker.waitForMeta can wait forever and totally stall a RS\n+   HBASE-3627  NPE in EventHandler when region already reassigned\n \n   IMPROVEMENTS\n    HBASE-3542  MultiGet methods in Thrift",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "sha": "5beea27f13e9ef0c15545e94f587b59ab32f0cc3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.io.IOException;\n import java.net.ConnectException;\n import java.net.SocketTimeoutException;\n+import java.net.SocketException;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.commons.logging.Log;\n@@ -390,8 +391,11 @@ private HRegionInterface getCachedConnection(HServerAddress address)\n         throw e;\n       }\n     } catch (SocketTimeoutException e) {\n-      // We were passed the wrong address.  Return 'protocol' == null.\n+      // Return 'protocol' == null.\n       LOG.debug(\"Timed out connecting to \" + address);\n+    } catch (SocketException e) {\n+      // Return 'protocol' == null.\n+      LOG.debug(\"Exception connecting to \" + address);\n     } catch (IOException ioe) {\n       Throwable cause = ioe.getCause();\n       if (cause != null && cause instanceof EOFException) {",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "sha": "be311797c81a8894d9280cad6b77c618bedff118",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -308,14 +308,9 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n     } catch (java.net.SocketTimeoutException e) {\n       // Treat this exception + message as unavailable catalog table. Catch it\n       // and fall through to return a null\n-    } catch (java.net.ConnectException e) {\n-      if (e.getMessage() != null &&\n-          e.getMessage().contains(\"Connection refused\")) {\n-        // Treat this exception + message as unavailable catalog table. Catch it\n-        // and fall through to return a null\n-      } else {\n-        throw e;\n-      }\n+    } catch (java.net.SocketException e) {\n+      // Treat this exception + message as unavailable catalog table. Catch it\n+      // and fall through to return a null\n     } catch (RemoteException re) {\n       IOException ioe = re.unwrapRemoteException();\n       if (ioe instanceof NotServingRegionException) {",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "6e22cf5e44997f7b258d4666a6c3d78f262d135f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1770,6 +1770,10 @@ protected void chore() {\n                   Stat stat = new Stat();\n                   RegionTransitionData data = ZKAssign.getDataNoWatch(watcher,\n                       node, stat);\n+                  if (data == null) {\n+                    LOG.warn(\"Data is null, node \" + node + \" no longer exists\");\n+                    break;\n+                  }\n                   if (data.getEventType() == EventType.RS_ZK_REGION_OPENED) {\n                     LOG.debug(\"Region has transitioned to OPENED, allowing \" +\n                         \"watched event handlers to process\");",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "e9b2af27b6a8c96656079194719260f8ca06b037",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "patch": "@@ -87,7 +87,11 @@ public void process() throws IOException {\n \n     // If fails, just return.  Someone stole the region from under us.\n     // Calling transitionZookeeperOfflineToOpening initalizes this.version.\n-    if (!transitionZookeeperOfflineToOpening(encodedName)) return;\n+    if (!transitionZookeeperOfflineToOpening(encodedName)) {\n+      LOG.warn(\"Region was hijacked? It no longer exists, encodedName=\" +\n+        encodedName);\n+      return;\n+    }\n \n     // Open region.  After a successful open, failures in subsequent processing\n     // needs to do a close as part of cleanup.\n@@ -254,7 +258,7 @@ private boolean transitionToOpened(final HRegion r) throws IOException {\n   /**\n    * @return Instance of HRegion if successful open else null.\n    */\n-  private HRegion openRegion() {\n+  HRegion openRegion() {\n     HRegion region = null;\n     try {\n       // Instantiate the region.  This also periodically tickles our zk OPENING",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "sha": "441b48419e4b85f91a73e3b3eb54c0d96268a41b",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 5,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "patch": "@@ -396,7 +396,8 @@ public static boolean deleteNode(ZooKeeperWatcher zkw, String regionName,\n     zkw.sync(node);\n     Stat stat = new Stat();\n     byte [] bytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(bytes == null) {\n+    if (bytes == null) {\n+      // If it came back null, node does not exist.\n       throw KeeperException.create(Code.NONODE);\n     }\n     RegionTransitionData data = RegionTransitionData.fromBytes(bytes);\n@@ -674,8 +675,11 @@ public static int transitionNode(ZooKeeperWatcher zkw, HRegionInfo region,\n \n     // Read existing data of the node\n     Stat stat = new Stat();\n-    byte [] existingBytes =\n-      ZKUtil.getDataNoWatch(zkw, node, stat);\n+    byte [] existingBytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n+    if (existingBytes == null) {\n+      // Node no longer exists.  Return -1. It means unsuccessful transition.\n+      return -1;\n+    }\n     RegionTransitionData existingData =\n       RegionTransitionData.fromBytes(existingBytes);\n \n@@ -762,7 +766,7 @@ public static RegionTransitionData getData(ZooKeeperWatcher zkw,\n    * @param zkw zk reference\n    * @param pathOrRegionName fully-specified path or region name\n    * @param stat object to store node info into on getData call\n-   * @return data for the unassigned node\n+   * @return data for the unassigned node or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n@@ -771,7 +775,7 @@ public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n     String node = pathOrRegionName.startsWith(\"/\") ?\n         pathOrRegionName : getNodeName(zkw, pathOrRegionName);\n     byte [] data = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(data == null) {\n+    if (data == null) {\n       return null;\n     }\n     return RegionTransitionData.fromBytes(data);",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "sha": "34e17b60638fc698d4bbfe1e49a09312719cd4b9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "patch": "@@ -576,7 +576,7 @@ public static int getNumberOfChildren(ZooKeeperWatcher zkw, String znode)\n    * @param zkw zk reference\n    * @param znode path of node\n    * @param stat node status to set if node exists\n-   * @return data of the specified znode, or null if does not exist\n+   * @return data of the specified znode, or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static byte [] getDataNoWatch(ZooKeeperWatcher zkw, String znode,",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "sha": "08748f810272dbcc582ed381d1979fa33b238f08",
                "status": "modified"
            }
        ],
        "message": "HBASE-3627 NPE in EventHandler when region already reassigned\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085075 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/237bc82af2992a82d0f71648f68311c4dab25f3e",
        "patched_files": [
            "AssignmentManager.java",
            "ZKUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestZKUtil.java",
            "TestAssignmentManager.java"
        ]
    },
    "hbase_cfe868d": {
        "bug_id": "hbase_cfe868d",
        "commit": "https://github.com/apache/hbase/commit/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java?ref=cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "patch": "@@ -140,7 +140,9 @@ public int compare(NormalizationPlan plan, NormalizationPlan plan2) {\n     for (int i = 0; i < tableRegions.size(); i++) {\n       HRegionInfo hri = tableRegions.get(i);\n       long regionSize = getRegionSize(hri);\n-      totalSizeMb += regionSize;\n+      if (regionSize > 0) {\n+        totalSizeMb += regionSize;\n+      }\n     }\n \n     double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n@@ -204,6 +206,10 @@ private long getRegionSize(HRegionInfo hri) {\n       getRegionServerOfRegion(hri);\n     RegionLoad regionLoad = masterServices.getServerManager().getLoad(sn).\n       getRegionsLoad().get(hri.getRegionName());\n+    if (regionLoad == null) {\n+      LOG.debug(hri.getRegionNameAsString() + \" was not found in RegionsLoad\");\n+      return -1;\n+    }\n     return regionLoad.getStorefileSizeMB();\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "sha": "d209eb739468e0f0533c8ae37dc8381968bdea73",
                "status": "modified"
            }
        ],
        "message": "HBASE-15933 NullPointerException may be thrown from SimpleRegionNormalizer#getRegionSize()",
        "parent": "https://github.com/apache/hbase/commit/a0f49c988419d48f6c655f46ac78f8199c643b50",
        "patched_files": [
            "SimpleRegionNormalizer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestSimpleRegionNormalizer.java"
        ]
    },
    "hbase_d0f2d18": {
        "bug_id": "hbase_d0f2d18",
        "commit": "https://github.com/apache/hbase/commit/d0f2d18ca73737764550b319f749a51c876cca39",
        "file": [
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hbase/blob/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java?ref=d0f2d18ca73737764550b319f749a51c876cca39",
                "deletions": 36,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "patch": "@@ -195,11 +195,33 @@ private RestoreMetaChanges restoreHdfsRegions(final ThreadPoolExecutor exec) thr\n     // this instance, by removing the regions already present in the restore dir.\n     Set<String> regionNames = new HashSet<>(regionManifests.keySet());\n \n+    List<RegionInfo> tableRegions = getTableRegions();\n+\n     RegionInfo mobRegion = MobUtils.getMobRegionInfo(snapshotManifest.getTableDescriptor()\n         .getTableName());\n+    if (tableRegions != null) {\n+      // restore the mob region in case\n+      if (regionNames.contains(mobRegion.getEncodedName())) {\n+        monitor.rethrowException();\n+        status.setStatus(\"Restoring mob region...\");\n+        List<RegionInfo> mobRegions = new ArrayList<>(1);\n+        mobRegions.add(mobRegion);\n+        restoreHdfsMobRegions(exec, regionManifests, mobRegions);\n+        regionNames.remove(mobRegion.getEncodedName());\n+        status.setStatus(\"Finished restoring mob region.\");\n+      }\n+    }\n+    if (regionNames.contains(mobRegion.getEncodedName())) {\n+      // add the mob region\n+      monitor.rethrowException();\n+      status.setStatus(\"Cloning mob region...\");\n+      cloneHdfsMobRegion(regionManifests, mobRegion);\n+      regionNames.remove(mobRegion.getEncodedName());\n+      status.setStatus(\"Finished cloning mob region.\");\n+    }\n+\n     // Identify which region are still available and which not.\n     // NOTE: we rely upon the region name as: \"table name, start key, end key\"\n-    List<RegionInfo> tableRegions = getTableRegions();\n     if (tableRegions != null) {\n       monitor.rethrowException();\n       for (RegionInfo regionInfo: tableRegions) {\n@@ -213,50 +235,40 @@ private RestoreMetaChanges restoreHdfsRegions(final ThreadPoolExecutor exec) thr\n           metaChanges.addRegionToRemove(regionInfo);\n         }\n       }\n-\n-      // Restore regions using the snapshot data\n-      monitor.rethrowException();\n-      status.setStatus(\"Restoring table regions...\");\n-      if (regionNames.contains(mobRegion.getEncodedName())) {\n-        // restore the mob region in case\n-        List<RegionInfo> mobRegions = new ArrayList<>(1);\n-        mobRegions.add(mobRegion);\n-        restoreHdfsMobRegions(exec, regionManifests, mobRegions);\n-        regionNames.remove(mobRegion.getEncodedName());\n-      }\n-      restoreHdfsRegions(exec, regionManifests, metaChanges.getRegionsToRestore());\n-      status.setStatus(\"Finished restoring all table regions.\");\n-\n-      // Remove regions from the current table\n-      monitor.rethrowException();\n-      status.setStatus(\"Starting to delete excess regions from table\");\n-      removeHdfsRegions(exec, metaChanges.getRegionsToRemove());\n-      status.setStatus(\"Finished deleting excess regions from table.\");\n     }\n \n     // Regions to Add: present in the snapshot but not in the current table\n+    List<RegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());\n     if (regionNames.size() > 0) {\n-      List<RegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());\n-\n       monitor.rethrowException();\n-      // add the mob region\n-      if (regionNames.contains(mobRegion.getEncodedName())) {\n-        cloneHdfsMobRegion(regionManifests, mobRegion);\n-        regionNames.remove(mobRegion.getEncodedName());\n-      }\n       for (String regionName: regionNames) {\n         LOG.info(\"region to add: \" + regionName);\n-        regionsToAdd.add(ProtobufUtil.toRegionInfo(regionManifests.get(regionName).getRegionInfo()));\n+        regionsToAdd.add(ProtobufUtil.toRegionInfo(regionManifests.get(regionName)\n+            .getRegionInfo()));\n       }\n-\n-      // Create new regions cloning from the snapshot\n-      monitor.rethrowException();\n-      status.setStatus(\"Cloning regions...\");\n-      RegionInfo[] clonedRegions = cloneHdfsRegions(exec, regionManifests, regionsToAdd);\n-      metaChanges.setNewRegions(clonedRegions);\n-      status.setStatus(\"Finished cloning regions.\");\n     }\n \n+    // Create new regions cloning from the snapshot\n+    // HBASE-19980: We need to call cloneHdfsRegions() before restoreHdfsRegions() because\n+    // regionsMap is constructed in cloneHdfsRegions() and it can be used in restoreHdfsRegions().\n+    monitor.rethrowException();\n+    status.setStatus(\"Cloning regions...\");\n+    RegionInfo[] clonedRegions = cloneHdfsRegions(exec, regionManifests, regionsToAdd);\n+    metaChanges.setNewRegions(clonedRegions);\n+    status.setStatus(\"Finished cloning regions.\");\n+\n+    // Restore regions using the snapshot data\n+    monitor.rethrowException();\n+    status.setStatus(\"Restoring table regions...\");\n+    restoreHdfsRegions(exec, regionManifests, metaChanges.getRegionsToRestore());\n+    status.setStatus(\"Finished restoring all table regions.\");\n+\n+    // Remove regions from the current table\n+    monitor.rethrowException();\n+    status.setStatus(\"Starting to delete excess regions from table\");\n+    removeHdfsRegions(exec, metaChanges.getRegionsToRemove());\n+    status.setStatus(\"Finished deleting excess regions from table.\");\n+\n     LOG.info(\"finishing restore table regions using snapshot=\" + snapshotDesc);\n \n     return metaChanges;\n@@ -742,11 +754,16 @@ private void restoreReferenceFile(final Path familyDir, final RegionInfo regionI\n \n     // Add the daughter region to the map\n     String regionName = Bytes.toString(regionsMap.get(regionInfo.getEncodedNameAsBytes()));\n+    if (regionName == null) {\n+      regionName = regionInfo.getEncodedName();\n+    }\n     LOG.debug(\"Restore reference \" + regionName + \" to \" + clonedRegionName);\n     synchronized (parentsMap) {\n       Pair<String, String> daughters = parentsMap.get(clonedRegionName);\n       if (daughters == null) {\n-        daughters = new Pair<>(regionName, null);\n+        // In case one side of the split is already compacted, regionName is put as both first and\n+        // second of Pair\n+        daughters = new Pair<>(regionName, regionName);\n         parentsMap.put(clonedRegionName, daughters);\n       } else if (!regionName.equals(daughters.getFirst())) {\n         daughters.setSecond(regionName);",
                "raw_url": "https://github.com/apache/hbase/raw/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "sha": "c4f0e258cb77e723bbb0c145555b093ad8eb404e",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java?ref=d0f2d18ca73737764550b319f749a51c876cca39",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "patch": "@@ -23,6 +23,7 @@\n \n import java.io.IOException;\n import java.util.HashSet;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -295,6 +296,25 @@ public void testCorruptedSnapshot() throws IOException, InterruptedException {\n     }\n   }\n \n+  @Test\n+  public void testRestoreSnapshotAfterSplittingRegions() throws IOException, InterruptedException {\n+    List<RegionInfo> regionInfos = admin.getRegions(tableName);\n+    RegionReplicaUtil.removeNonDefaultRegions(regionInfos);\n+\n+    // Split the first region\n+    splitRegion(regionInfos.get(0));\n+\n+    // Take a snapshot\n+    admin.snapshot(snapshotName1, tableName);\n+\n+    // Restore the snapshot\n+    admin.disableTable(tableName);\n+    admin.restoreSnapshot(snapshotName1);\n+    admin.enableTable(tableName);\n+\n+    verifyRowCount(TEST_UTIL, tableName, snapshot1Rows);\n+  }\n+\n   // ==========================================================================\n   //  Helpers\n   // ==========================================================================",
                "raw_url": "https://github.com/apache/hbase/raw/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "sha": "eb8b20e124fbd24c01dd3c9997ed934fe4d0cc91",
                "status": "modified"
            }
        ],
        "message": "HBASE-19980 NullPointerException when restoring a snapshot after splitting a region\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/8d26736bc2b0c28efd5caa3be7d8c9037dba633a",
        "patched_files": [
            "RestoreSnapshotHelper.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestRestoreSnapshotHelper.java",
            "TestRestoreSnapshotFromClient.java"
        ]
    },
    "hbase_d64c76f": {
        "bug_id": "hbase_d64c76f",
        "commit": "https://github.com/apache/hbase/commit/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "deletions": 6,
                "filename": "CHANGES.txt",
                "patch": "@@ -256,14 +256,15 @@ Release 0.91.0 - Unreleased\n Release 0.90.4 - Unreleased\n \n   BUG FIXES\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3881 Add disable balancer in graceful_stop.sh script\n-   HBASE-3895 Fix order of parameters after HBASE-1511\n+   HBASE-3878  Hbase client throws NoSuchElementException (Ted Yu)\n+   HBASE-3881  Add disable balancer in graceful_stop.sh script\n+   HBASE-3895  Fix order of parameters after HBASE-1511\n+   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n+               region assignment\n \n   IMPROVEMENT\n-   HBASE-3882 hbase-config.sh needs to be updated so it can auto-detects the\n-              sun jre provided by RHEL6 (Roman Shaposhnik)\n+   HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the\n+               sun jre provided by RHEL6 (Roman Shaposhnik)\n \n Release 0.90.3 - Unreleased\n ",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "sha": "d48d7d7fdf89099029f2ef59d662d3ac2b07f779",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1995,7 +1995,9 @@ protected void chore() {\n       for (Iterator <Map.Entry<String, RegionPlan>> i =\n           this.regionPlans.entrySet().iterator(); i.hasNext();) {\n         Map.Entry<String, RegionPlan> e = i.next();\n-        if (e.getValue().getDestination().equals(sn)) {\n+        ServerName otherSn = e.getValue().getDestination();\n+        // The name will be null if the region is planned for a random assign.\n+        if (otherSn != null && otherSn.equals(sn)) {\n           // Use iterator's remove else we'll get CME\n           i.remove();\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "7e473095b11a15233e608aa48d6c9c431ebd6aed",
                "status": "modified"
            }
        ],
        "message": "   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n               region assignment\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1124477 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cf8be18f159b21137885e8c579e415a0cc1e323e",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_d68f697": {
        "bug_id": "hbase_d68f697",
        "commit": "https://github.com/apache/hbase/commit/d68f697f39fc0e660eb85dc9ef67a757102169f2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=d68f697f39fc0e660eb85dc9ef67a757102169f2",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "patch": "@@ -986,12 +986,11 @@ public static HRegionFileSystem createRegionOnFileSystem(final Configuration con\n \n       // Write HRI to a file in case we need to recover hbase:meta\n       regionFs.writeRegionInfoOnFilesystem(false);\n-      return regionFs;\n     } else {\n       if (LOG.isDebugEnabled())\n         LOG.debug(\"Skipping creation of .regioninfo file for \" + regionInfo);\n     }\n-    return null;\n+    return regionFs;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "sha": "904060f8013db7d094debac5a6a0b913cf00e0b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java?ref=d68f697f39fc0e660eb85dc9ef67a757102169f2",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "patch": "@@ -45,7 +45,6 @@\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.ClassRule;\n-import org.junit.Ignore;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n@@ -101,7 +100,6 @@ public void testPriorityRegionIsOpenedWithSeparateThreadPool() throws Exception\n     assertEquals(completed + 1, exec.getCompletedTaskCount());\n   }\n \n-  @Ignore // Needs rewrite since HBASE-19391 which returns null out of createRegionOnFileSystem\n   @Test\n   public void testNonExistentRegionReplica() throws Exception {\n     final TableName tableName = TableName.valueOf(name.getMethodName());",
                "raw_url": "https://github.com/apache/hbase/raw/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "sha": "7190d840a7fac8d32f66fe868584a7e01c7d212f",
                "status": "modified"
            }
        ],
        "message": "HBASE-20052 TestRegionOpen#testNonExistentRegionReplica fails due to NPE",
        "parent": "https://github.com/apache/hbase/commit/1fd2a276f6849107960acfab7a939aa2842fc9c5",
        "patched_files": [
            "HRegionFileSystem.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegionFileSystem.java",
            "TestRegionOpen.java"
        ]
    },
    "hbase_d7a6398": {
        "bug_id": "hbase_d7a6398",
        "commit": "https://github.com/apache/hbase/commit/d7a63983fc079da951290c3a0212e8fe03c35417",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -38,6 +38,7 @@ Release 0.91.0 - Unreleased\n                row are the same\n    HBASE-3416  For intra-row scanning, the update readers notification resets\n                the query matcher and can lead to incorrect behavior\n+   HBASE-3492  NPE while splitting table with empty column family store\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/CHANGES.txt",
                "sha": "6eeabc601b5fa5a7de11406ba59999f1882efbe7",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "deletions": 12,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "patch": "@@ -725,17 +725,17 @@ private boolean hasReferences(Collection<StoreFile> files) {\n    * @param dir\n    * @throws IOException\n    */\n-  private static long getLowestTimestamp(FileSystem fs, \n+  private static long getLowestTimestamp(FileSystem fs,\n       final List<StoreFile> candidates) throws IOException {\n     long minTs = Long.MAX_VALUE;\n     if (candidates.isEmpty()) {\n-      return minTs; \n+      return minTs;\n     }\n     Path[] p = new Path[candidates.size()];\n     for (int i = 0; i < candidates.size(); ++i) {\n       p[i] = candidates.get(i).getPath();\n     }\n-    \n+\n     FileStatus[] stats = fs.listStatus(p);\n     if (stats == null || stats.length == 0) {\n       return minTs;\n@@ -756,13 +756,13 @@ boolean isMajorCompaction() throws IOException {\n         return false;\n       }\n     }\n-    \n+\n     List<StoreFile> candidates = new ArrayList<StoreFile>(this.storefiles);\n \n     // exclude files above the max compaction threshold\n     // except: save all references. we MUST compact them\n     int pos = 0;\n-    while (pos < candidates.size() && \n+    while (pos < candidates.size() &&\n            candidates.get(pos).getReader().length() > this.maxCompactSize &&\n            !candidates.get(pos).isReference()) ++pos;\n     candidates.subList(0, pos).clear();\n@@ -868,7 +868,7 @@ long getNextMajorCompactTime() {\n       // do not compact old files above a configurable threshold\n       // save all references. we MUST compact them\n       int pos = 0;\n-      while (pos < filesToCompact.size() && \n+      while (pos < filesToCompact.size() &&\n              filesToCompact.get(pos).getReader().length() > maxCompactSize &&\n              !filesToCompact.get(pos).isReference()) ++pos;\n       filesToCompact.subList(0, pos).clear();\n@@ -878,7 +878,7 @@ long getNextMajorCompactTime() {\n       LOG.debug(this.storeNameStr + \": no store files to compact\");\n       return filesToCompact;\n     }\n-    \n+\n     // major compact on user action or age (caveat: we have too many files)\n     boolean majorcompaction = (forcemajor || isMajorCompaction(filesToCompact))\n       && filesToCompact.size() < this.maxFilesToCompact;\n@@ -891,7 +891,7 @@ long getNextMajorCompactTime() {\n       int start = 0;\n       double r = this.compactRatio;\n \n-      /* TODO: add sorting + unit test back in when HBASE-2856 is fixed \n+      /* TODO: add sorting + unit test back in when HBASE-2856 is fixed\n       // Sort files by size to correct when normal skew is altered by bulk load.\n       Collections.sort(filesToCompact, StoreFile.Comparators.FILE_SIZE);\n        */\n@@ -1320,10 +1320,11 @@ StoreSize checkSplit(final boolean force) {\n     this.lock.readLock().lock();\n     try {\n       // sanity checks\n-      if (!force) {\n-        if (storeSize < this.desiredMaxFileSize || this.storefiles.isEmpty()) {\n-          return null;\n-        }\n+      if (this.storefiles.isEmpty()) {\n+        return null;\n+      }\n+      if (!force && storeSize < this.desiredMaxFileSize) {\n+        return null;\n       }\n \n       if (this.region.getRegionInfo().isMetaRegion()) {",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "sha": "e13cf59375f59c7fe2191fe3e58dfaf03cbf6784",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "deletions": 1,
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "patch": "@@ -61,7 +61,7 @@\n import com.google.common.base.Joiner;\n \n /**\n- * Test class fosr the Store\n+ * Test class for the Store\n  */\n public class TestStore extends TestCase {\n   public static final Log LOG = LogFactory.getLog(TestStore.class);\n@@ -630,4 +630,15 @@ public void testMultipleTimestamps() throws IOException {\n     result = HBaseTestingUtility.getFromStoreFile(store, get);\n     assertTrue(result.size()==0);\n   }\n+\n+  /**\n+   * Test for HBASE-3492 - Test split on empty colfam (no store files).\n+   *\n+   * @throws IOException When the IO operations fail.\n+   */\n+  public void testSplitWithEmptyColFam() throws IOException {\n+    init(this.getName());\n+    assertNull(store.checkSplit(false));\n+    assertNull(store.checkSplit(true));\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "sha": "988dfe1c1674b43e2b6c200f4af9b0a7d02141ec",
                "status": "modified"
            }
        ],
        "message": "HBASE-3492  NPE while splitting table with empty column family store\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1066785 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/a3a2ce921508da46e3e8db9550cf281683fadb6c",
        "patched_files": [
            "CHANGES.java",
            "Store.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStore.java"
        ]
    },
    "hbase_d8c6ed3": {
        "bug_id": "hbase_d8c6ed3",
        "commit": "https://github.com/apache/hbase/commit/d8c6ed3be515198920394e48f01d643bccec6bf2",
        "file": [
            {
                "additions": 141,
                "blob_url": "https://github.com/apache/hbase/blob/d8c6ed3be515198920394e48f01d643bccec6bf2/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "changes": 141,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java?ref=d8c6ed3be515198920394e48f01d643bccec6bf2",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "patch": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright 2010 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+\n+public class TestHalfStoreFileReader {\n+\n+  /**\n+   * Test the scanner and reseek of a half hfile scanner. The scanner API\n+   * demands that seekTo and reseekTo() only return < 0 if the key lies\n+   * before the start of the file (with no position on the scanner). Returning\n+   * 0 if perfect match (rare), and return > 1 if we got an imperfect match.\n+   *\n+   * The latter case being the most common, we should generally be returning 1,\n+   * and if we do, there may or may not be a 'next' in the scanner/file.\n+   *\n+   * A bug in the half file scanner was returning -1 at the end of the bottom\n+   * half, and that was causing the infrastructure above to go null causing NPEs\n+   * and other problems.  This test reproduces that failure, and also tests\n+   * both the bottom and top of the file while we are at it.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testHalfScanAndReseek() throws IOException {\n+    HBaseTestingUtility test_util = new HBaseTestingUtility();\n+    String root_dir = HBaseTestingUtility.getTestDir(\"TestHalfStoreFile\").toString();\n+    Path p = new Path(root_dir, \"test\");\n+\n+    FileSystem fs = FileSystem.get(test_util.getConfiguration());\n+\n+    HFile.Writer w = new HFile.Writer(fs, p, 1024, \"none\", KeyValue.KEY_COMPARATOR);\n+\n+    // write some things.\n+    List<KeyValue> items = genSomeKeys();\n+    for (KeyValue kv : items) {\n+      w.append(kv);\n+    }\n+    w.close();\n+\n+    HFile.Reader r = new HFile.Reader(fs, p, null, false);\n+    r.loadFileInfo();\n+    byte [] midkey = r.midkey();\n+    KeyValue midKV = KeyValue.createKeyValueFromKey(midkey);\n+    midkey = midKV.getRow();\n+\n+    //System.out.println(\"midkey: \" + midKV + \" or: \" + Bytes.toStringBinary(midkey));\n+\n+    Reference bottom = new Reference(midkey, Reference.Range.bottom);\n+    doTestOfScanAndReseek(p, fs, bottom);\n+\n+    Reference top = new Reference(midkey, Reference.Range.top);\n+    doTestOfScanAndReseek(p, fs, top);\n+  }\n+\n+  private void doTestOfScanAndReseek(Path p, FileSystem fs, Reference bottom)\n+      throws IOException {\n+    final HalfStoreFileReader halfreader =\n+        new HalfStoreFileReader(fs, p, null, bottom);\n+    halfreader.loadFileInfo();\n+    final HFileScanner scanner = halfreader.getScanner(false, false);\n+\n+    scanner.seekTo();\n+    KeyValue curr;\n+    do {\n+      curr = scanner.getKeyValue();\n+      KeyValue reseekKv =\n+          getLastOnCol(curr);\n+      int ret = scanner.reseekTo(reseekKv.getKey());\n+      assertTrue(\"reseek to returned: \" + ret, ret > 0);\n+      //System.out.println(curr + \": \" + ret);\n+    } while (scanner.next());\n+\n+    int ret = scanner.reseekTo(getLastOnCol(curr).getKey());\n+    //System.out.println(\"Last reseek: \" + ret);\n+    assertTrue( ret > 0 );\n+  }\n+\n+  private KeyValue getLastOnCol(KeyValue curr) {\n+    return KeyValue.createLastOnRow(\n+        curr.getBuffer(), curr.getRowOffset(), curr.getRowLength(),\n+        curr.getBuffer(), curr.getFamilyOffset(), curr.getFamilyLength(),\n+        curr.getBuffer(), curr.getQualifierOffset(), curr.getQualifierLength());\n+  }\n+\n+  static final int SIZE = 1000;\n+\n+  static byte[] _b(String s) {\n+    return Bytes.toBytes(s);\n+  }\n+\n+  List<KeyValue> genSomeKeys() {\n+    List<KeyValue> ret = new ArrayList<KeyValue>(SIZE);\n+    for (int i = 0 ; i < SIZE; i++) {\n+      KeyValue kv =\n+          new KeyValue(\n+              _b(String.format(\"row_%04d\", i)),\n+              _b(\"family\"),\n+              _b(\"qualifier\"),\n+              1000, // timestamp\n+              _b(\"value\"));\n+      ret.add(kv);\n+    }\n+    return ret;\n+  }\n+\n+\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/d8c6ed3be515198920394e48f01d643bccec6bf2/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "sha": "47f4c3910573f2baf8497df28351a180dfd9994f",
                "status": "added"
            }
        ],
        "message": "HBASE-3224  NPE in KeyValue.compare when compacting (forgot a file)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1035129 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/069fecaeb7d128979a338e9cbbb4ad4a34726125",
        "patched_files": [
            "HalfStoreFileReader.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_da0cc59": {
        "bug_id": "hbase_da0cc59",
        "commit": "https://github.com/apache/hbase/commit/da0cc598feab995eed12527d90805dd627674035",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=da0cc598feab995eed12527d90805dd627674035",
                "deletions": 8,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "status": "modified"
            }
        ],
        "message": "HBASE-14909 NPE testing for RIT",
        "parent": "https://github.com/apache/hbase/commit/08f90f30b385bbf314f7fb014b810d86330d60b1",
        "patched_files": [
            "HBaseTestingUtility.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_db2b642": {
        "bug_id": "hbase_db2b642",
        "commit": "https://github.com/apache/hbase/commit/db2b6421ff52608f56522684461a4de3f491744f",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java?ref=db2b6421ff52608f56522684461a4de3f491744f",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "patch": "@@ -187,7 +187,11 @@ public void close() throws IOException {\n     if (null != metaProvider) {\n       metaProvider.close();\n     }\n-    provider.close();\n+    // close is called on a WALFactory with null provider in the case of contention handling\n+    // within the getInstance method.\n+    if (null != provider) {\n+      provider.close();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "sha": "3500bd0ff885a139682e4ff1105b0c2845e66844",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java?ref=db2b6421ff52608f56522684461a4de3f491744f",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "patch": "@@ -152,6 +152,11 @@ public static void tearDownAfterClass() throws Exception {\n     TEST_UTIL.shutdownMiniCluster();\n   }\n \n+  @Test\n+  public void canCloseSingleton() throws IOException {\n+    WALFactory.getInstance(conf).close();\n+  }\n+\n   /**\n    * Just write multiple logs then split.  Before fix for HADOOP-2283, this\n    * would fail.",
                "raw_url": "https://github.com/apache/hbase/raw/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "sha": "47b001aa73d9274eddc14d23761e9942df4231c5",
                "status": "modified"
            }
        ],
        "message": "HBASE-12535 NPE in WALFactory under contention for getInstance()\n\nSigned-off-by: stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/05ced20a347213c31e46c528e53b93c675f85891",
        "patched_files": [
            "WALFactory.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestWALFactory.java"
        ]
    },
    "hbase_db3ba65": {
        "bug_id": "hbase_db3ba65",
        "commit": "https://github.com/apache/hbase/commit/db3ba652f88083b0b1c57b4857f11fce7ae5b131",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "deletions": 4,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "patch": "@@ -1190,9 +1190,15 @@ private void receiveGlobalFailure(\n         byte[] row = e.getValue().iterator().next().getAction().getRow();\n         // Do not use the exception for updating cache because it might be coming from\n         // any of the regions in the MultiAction.\n-        if (tableName != null) {\n-          connection.updateCachedLocations(tableName, regionName, row,\n+        try {\n+          if (tableName != null) {\n+            connection.updateCachedLocations(tableName, regionName, row,\n               ClientExceptionsUtil.isMetaClearingException(t) ? null : t, server);\n+          }\n+        } catch (Throwable ex) {\n+          // That should never happen, but if it did, we want to make sure\n+          // we still process errors\n+          LOG.error(\"Couldn't update cached region locations: \" + ex);\n         }\n         for (Action<Row> action : e.getValue()) {\n           Retry retry = manageError(\n@@ -1317,8 +1323,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n             // Register corresponding failures once per server/once per region.\n             if (!regionFailureRegistered) {\n               regionFailureRegistered = true;\n-              connection.updateCachedLocations(\n+              try {\n+                connection.updateCachedLocations(\n                   tableName, regionName, row.getRow(), result, server);\n+              } catch (Throwable ex) {\n+                // That should never happen, but if it did, we want to make sure\n+                // we still process errors\n+                LOG.error(\"Couldn't update cached region locations: \" + ex);\n+              }\n             }\n             if (failureCount == 0) {\n               errorsByServer.reportServerError(server);\n@@ -1372,8 +1384,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n           // for every possible exception that comes through, however.\n           connection.clearCaches(server);\n         } else {\n-          connection.updateCachedLocations(\n+          try {\n+            connection.updateCachedLocations(\n               tableName, region, actions.get(0).getAction().getRow(), throwable, server);\n+          } catch (Throwable ex) {\n+            // That should never happen, but if it did, we want to make sure\n+            // we still process errors\n+            LOG.error(\"Couldn't update cached region locations: \" + ex);\n+          }\n         }\n         failureCount += actions.size();\n ",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "sha": "142e2a0647fb231f3b6f2a5ef67970b1e01db045",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "patch": "@@ -63,6 +63,7 @@\n   private static final String MEMLOAD_BASE = \"memstoreLoad_\";\n   private static final String HEAP_BASE = \"heapOccupancy_\";\n   private static final String CACHE_BASE = \"cacheDroppingExceptions_\";\n+  private static final String UNKNOWN_EXCEPTION = \"UnknownException\";\n   private static final String CLIENT_SVC = ClientService.getDescriptor().getName();\n \n   /** A container class for collecting details about the RPC call as it percolates. */\n@@ -464,7 +465,8 @@ public void updateRpc(MethodDescriptor method, Message param, CallStats stats) {\n   }\n \n   public void incrCacheDroppingExceptions(Object exception) {\n-    getMetric(CACHE_BASE + exception.getClass().getSimpleName(),\n+    getMetric(CACHE_BASE +\n+      (exception == null? UNKNOWN_EXCEPTION : exception.getClass().getSimpleName()),\n       cacheDroppingExceptions, counterFactory).inc();\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "sha": "53a33264cb41df0cf8337abc1402f7c52f2f63b5",
                "status": "modified"
            }
        ],
        "message": "HBASE-15524 Fix NPE in client-side metrics",
        "parent": "https://github.com/apache/hbase/commit/fd5c0934b60664ecdde21a994910953339c7060d",
        "patched_files": [
            "MetricsConnection.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetricsConnection.java"
        ]
    },
    "hbase_dc79029": {
        "bug_id": "hbase_dc79029",
        "commit": "https://github.com/apache/hbase/commit/dc79029966c72f6c46add8c382e118308609cc81",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "deletions": 18,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "patch": "@@ -48,19 +48,15 @@ public MetricsTableAggregateSourceImpl(String metricsName,\n   }\n \n   private void register(MetricsTableSource source) {\n-    synchronized (this) {\n-      source.registerMetrics();\n-    }\n+    source.registerMetrics();\n   }\n \n   @Override\n   public void deleteTableSource(String table) {\n     try {\n-      synchronized (this) {\n-        MetricsTableSource source = tableSources.remove(table);\n-        if (source != null) {\n-          source.close();\n-        }\n+      MetricsTableSource source = tableSources.remove(table);\n+      if (source != null) {\n+        source.close();\n       }\n     } catch (Exception e) {\n       // Ignored. If this errors out it means that someone is double\n@@ -76,17 +72,13 @@ public MetricsTableSource getOrCreateTableSource(String table,\n     if (source != null) {\n       return source;\n     }\n-    source = CompatibilitySingletonFactory.getInstance(MetricsRegionServerSourceFactory.class)\n-      .createTable(table, wrapper);\n-    MetricsTableSource prev = tableSources.putIfAbsent(table, source);\n-\n-    if (prev != null) {\n-      return prev;\n-    } else {\n+    MetricsTableSource newSource = CompatibilitySingletonFactory\n+      .getInstance(MetricsRegionServerSourceFactory.class).createTable(table, wrapper);\n+    return tableSources.computeIfAbsent(table, k -> {\n       // register the new metrics now\n-      register(source);\n-    }\n-    return source;\n+      newSource.registerMetrics();\n+      return newSource;\n+    });\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "sha": "5133a96db10ae9fe6a2ff35287baed826dade3cb",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "deletions": 21,
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "patch": "@@ -15,21 +15,8 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package org.apache.hadoop.hbase.regionserver;\n \n-import java.util.concurrent.atomic.AtomicBoolean;\n-\n-import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.metrics.Interns;\n-import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n-import org.apache.hadoop.metrics2.MetricHistogram;\n-import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n-import org.apache.hadoop.metrics2.lib.MutableFastCounter;\n-import org.apache.yetus.audience.InterfaceAudience;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_INPUT_BYTES;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_INPUT_BYTES_DESC;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_OUTPUT_BYTES;\n@@ -74,6 +61,17 @@\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.SPLIT_SUCCESS_DESC;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.SPLIT_SUCCESS_KEY;\n \n+import java.util.concurrent.atomic.AtomicBoolean;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.metrics.Interns;\n+import org.apache.hadoop.metrics2.MetricHistogram;\n+import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n+import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n+import org.apache.hadoop.metrics2.lib.MutableFastCounter;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n @InterfaceAudience.Private\n public class MetricsTableSourceImpl implements MetricsTableSource {\n \n@@ -123,7 +121,7 @@\n \n   public MetricsTableSourceImpl(String tblName,\n       MetricsTableAggregateSourceImpl aggregate, MetricsTableWrapperAggregate tblWrapperAgg) {\n-    LOG.debug(\"Creating new MetricsTableSourceImpl for table \");\n+    LOG.debug(\"Creating new MetricsTableSourceImpl for table '{}'\", tblName);\n     this.tableName = TableName.valueOf(tblName);\n     this.agg = aggregate;\n \n@@ -240,17 +238,11 @@ public int compareTo(MetricsTableSource source) {\n     if (!(source instanceof MetricsTableSourceImpl)) {\n       return -1;\n     }\n-\n     MetricsTableSourceImpl impl = (MetricsTableSourceImpl) source;\n-    if (impl == null) {\n-      return -1;\n-    }\n-\n     return Long.compare(hashCode, impl.hashCode);\n   }\n \n   void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n-\n     // If there is a close that started be double extra sure\n     // that we're not getting any locks and not putting data\n     // into the metrics that should be removed. So early out\n@@ -263,7 +255,6 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n     // This ensures that removes of the metrics\n     // can't happen while we are putting them back in.\n     synchronized (this) {\n-\n       // It's possible that a close happened between checking\n       // the closed variable and getting the lock.\n       if (closed.get()) {",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "sha": "3da16b88bcbc50ca4c3da3532f74f3159510f4c6",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "patch": "@@ -17,7 +17,14 @@\n  */\n package org.apache.hadoop.hbase.regionserver;\n \n+import static org.junit.Assert.assertTrue;\n+\n import java.io.IOException;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.CompatibilityFactory;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n@@ -29,15 +36,19 @@\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n-@Category({RegionServerTests.class, SmallTests.class})\n+@Category({ RegionServerTests.class, SmallTests.class })\n public class TestMetricsTableAggregate {\n \n   @ClassRule\n   public static final HBaseClassTestRule CLASS_RULE =\n     HBaseClassTestRule.forClass(TestMetricsTableAggregate.class);\n \n-  public static MetricsAssertHelper HELPER =\n+  private static final Logger LOG = LoggerFactory.getLogger(TestMetricsTableAggregate.class);\n+\n+  private static MetricsAssertHelper HELPER =\n     CompatibilityFactory.getInstance(MetricsAssertHelper.class);\n \n   private String tableName = \"testTableMetrics\";\n@@ -87,6 +98,7 @@ public void testRegionAndStoreMetrics() throws IOException {\n     HELPER.assertGauge(pre + \"averageRegionSize\", 88, agg);\n   }\n \n+  @Test\n   public void testFlush() {\n     rsm.updateFlush(tableName, 1, 2, 3);\n     HELPER.assertCounter(pre + \"flushTime_num_ops\", 1, agg);\n@@ -139,4 +151,32 @@ public void testCompaction() {\n     HELPER.assertCounter(pre + \"majorCompactedoutputBytes\", 500, agg);\n   }\n \n+  private void update(AtomicBoolean succ, int round, CyclicBarrier barrier) {\n+    try {\n+      for (int i = 0; i < round; i++) {\n+        String tn = tableName + \"-\" + i;\n+        barrier.await(10, TimeUnit.SECONDS);\n+        rsm.updateFlush(tn, 100, 1000, 500);\n+      }\n+    } catch (Exception e) {\n+      LOG.warn(\"Failed to update metrics\", e);\n+      succ.set(false);\n+    }\n+  }\n+\n+  @Test\n+  public void testConcurrentUpdate() throws InterruptedException {\n+    int threadNumber = 10;\n+    int round = 100;\n+    AtomicBoolean succ = new AtomicBoolean(true);\n+    CyclicBarrier barrier = new CyclicBarrier(threadNumber);\n+    Thread[] threads = IntStream.range(0, threadNumber)\n+      .mapToObj(i -> new Thread(() -> update(succ, round, barrier), \"Update-Worker-\" + i))\n+      .toArray(Thread[]::new);\n+    Stream.of(threads).forEach(Thread::start);\n+    for (Thread t : threads) {\n+      t.join();\n+    }\n+    assertTrue(succ.get());\n+  }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "sha": "71c3e71882f54f8ffbd51720ccab79de309e2b52",
                "status": "modified"
            }
        ],
        "message": "HBASE-21136 NPE in MetricsTableSourceImpl.updateFlushTime",
        "parent": "https://github.com/apache/hbase/commit/9c09efc0df4b2f60b359bad00fed27e7980cf92e",
        "patched_files": [
            "MetricsTableAggregateSourceImpl.java",
            "MetricsTableSourceImpl.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestMetricsTableSourceImpl.java",
            "TestMetricsTableAggregate.java"
        ]
    },
    "hbase_dfc29dc": {
        "bug_id": "hbase_dfc29dc",
        "commit": "https://github.com/apache/hbase/commit/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java?ref=dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "patch": "@@ -285,6 +285,6 @@ public String toString() {\n         Bytes.toStringBinary(this.columnQualifier),\n         this.dropDependentColumn,\n         this.compareOp.name(),\n-        Bytes.toStringBinary(this.comparator.getValue()));\n+        this.comparator != null ? Bytes.toStringBinary(this.comparator.getValue()) : \"null\");\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "sha": "38a8bbe0f5f9a506363eed47e1f52fac4a1ca0cb",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hbase/blob/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java?ref=dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.junit.Before;\n import org.junit.Test;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import org.junit.experimental.categories.Category;\n@@ -247,5 +248,32 @@ public void testFilterDropping() throws Exception {\n       assertEquals(\"check cell retention\", 2, accepted.size());\n   }\n \n+  /**\n+   * Test for HBASE-8794. Avoid NullPointerException in DependentColumnFilter.toString().\n+   */\n+  @Test\n+  public void testToStringWithNullComparator() {\n+    // Test constructor that implicitly sets a null comparator\n+    Filter filter = new DependentColumnFilter(FAMILIES[0], QUALIFIER);\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains 'null' as compatator is null\",\n+      filter.toString().contains(\"null\"));\n+\n+    // Test constructor with explicit null comparator\n+    filter = new DependentColumnFilter(FAMILIES[0], QUALIFIER, true, CompareOp.EQUAL, null);\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains 'null' as compatator is null\",\n+      filter.toString().contains(\"null\"));\n+  }\n+\n+  @Test\n+  public void testToStringWithNonNullComparator() {\n+    Filter filter =\n+        new DependentColumnFilter(FAMILIES[0], QUALIFIER, true, CompareOp.EQUAL,\n+            new BinaryComparator(MATCH_VAL));\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains comparator value\", filter.toString().contains(\"match\"));\n+  }\n+\n }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "sha": "277cff0e583ff3c0f3d78961fcd28919f2cb2f76",
                "status": "modified"
            }
        ],
        "message": "HBASE-8794 DependentColumnFilter.toString() throws NullPointerException (Stefan Seelmann)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1498536 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/a7655cccf27d54c9409ff42768111b9ebecd55c1",
        "patched_files": [
            "DependentColumnFilter.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestDependentColumnFilter.java"
        ]
    },
    "hbase_dfc6399": {
        "bug_id": "hbase_dfc6399",
        "commit": "https://github.com/apache/hbase/commit/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.hbase.mapreduce;\n \n import java.io.IOException;\n+import java.io.InterruptedIOException;\n import java.net.InetAddress;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -31,7 +32,6 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Result;\n@@ -128,6 +128,11 @@\n     sc.setStopRow(tSplit.getEndRow());\n     trr.setScan(sc);\n     trr.setHTable(table);\n+    try {\n+      trr.initialize(tSplit, context);\n+    } catch (InterruptedException e) {\n+      throw new InterruptedIOException(e.getMessage());\n+    }\n     return trr;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "6054b36ecf946010cc82c1c0923cf4f617e97db6",
                "status": "modified"
            }
        ],
        "message": "HBASE-6069 TableInputFormatBase#createRecordReader() doesn't initialize TableRecordReader which causes NPE (Jie Huang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1341922 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6a5244912a7a7ef2e1d9f4d10beee0ecc75216b6",
        "patched_files": [
            "TableInputFormatBase.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_e13718b": {
        "bug_id": "hbase_e13718b",
        "commit": "https://github.com/apache/hbase/commit/e13718b872dbf7266c4fc4000855441f0397657b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=e13718b872dbf7266c4fc4000855441f0397657b",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "patch": "@@ -527,6 +527,8 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName,\n       //check if larger than last key.\n       KeyValue splitKey = KeyValue.createFirstOnRow(splitRow);\n       byte[] lastKey = f.createReader().getLastKey();      \n+      // If lastKey is null means storefile is empty.\n+      if (lastKey == null) return null;\n       if (f.getReader().getComparator().compare(splitKey.getBuffer(), \n           splitKey.getKeyOffset(), splitKey.getKeyLength(), lastKey, 0, lastKey.length) > 0) {\n         return null;\n@@ -535,6 +537,8 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName,\n       //check if smaller than first key\n       KeyValue splitKey = KeyValue.createLastOnRow(splitRow);\n       byte[] firstKey = f.createReader().getFirstKey();\n+      // If firstKey is null means storefile is empty.\n+      if (firstKey == null) return null;\n       if (f.getReader().getComparator().compare(splitKey.getBuffer(), \n           splitKey.getKeyOffset(), splitKey.getKeyLength(), firstKey, 0, firstKey.length) < 0) {\n         return null;",
                "raw_url": "https://github.com/apache/hbase/raw/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "sha": "f9902cc0e44929ec86e41e7aa428a1df6ac54b59",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hbase/blob/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java?ref=e13718b872dbf7266c4fc4000855441f0397657b",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "patch": "@@ -40,8 +40,10 @@\n import org.apache.hadoop.hbase.Abortable;\n import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.LargeTests;\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.RegionTransition;\n@@ -52,6 +54,9 @@\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n@@ -75,6 +80,7 @@\n import org.apache.zookeeper.data.Stat;\n import org.junit.After;\n import org.junit.AfterClass;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -427,6 +433,69 @@ public void run() {\n     }\n   }\n \n+  @Test(timeout = 180000)\n+  public void testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles() throws Exception {\n+    Configuration conf = TESTING_UTIL.getConfiguration();\n+    ZooKeeperWatcher zkw = HBaseTestingUtility.getZooKeeperWatcher(TESTING_UTIL);\n+    String userTableName = \"testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles\";\n+    HTableDescriptor htd = new HTableDescriptor(userTableName);\n+    HColumnDescriptor hcd = new HColumnDescriptor(\"col\");\n+    htd.addFamily(hcd);\n+    admin.createTable(htd);\n+    ZKAssign.blockUntilNoRIT(zkw);\n+    HTable table = new HTable(conf, userTableName);\n+    try {\n+      for (int i = 0; i <= 5; i++) {\n+        String row = \"row\" + i;\n+        Put p = new Put(row.getBytes());\n+        String val = \"Val\" + i;\n+        p.add(\"col\".getBytes(), \"ql\".getBytes(), val.getBytes());\n+        table.put(p);\n+        admin.flush(userTableName);\n+        Delete d = new Delete(row.getBytes());\n+        // Do a normal delete\n+        table.delete(d);\n+        admin.flush(userTableName);\n+      }\n+      admin.majorCompact(userTableName);\n+      List<HRegionInfo> regionsOfTable = TESTING_UTIL.getMiniHBaseCluster()\n+          .getMaster().getAssignmentManager().getRegionStates()\n+          .getRegionsOfTable(userTableName.getBytes());\n+      HRegionInfo hRegionInfo = regionsOfTable.get(0);\n+      Put p = new Put(\"row6\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      p = new Put(\"row7\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      p = new Put(\"row8\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      admin.flush(userTableName);\n+      admin.split(hRegionInfo.getRegionName(), \"row7\".getBytes());\n+      regionsOfTable = TESTING_UTIL.getMiniHBaseCluster().getMaster()\n+          .getAssignmentManager().getRegionStates()\n+          .getRegionsOfTable(userTableName.getBytes());\n+\n+      while (regionsOfTable.size() != 2) {\n+        Thread.sleep(2000);\n+        regionsOfTable = TESTING_UTIL.getMiniHBaseCluster().getMaster()\n+            .getAssignmentManager().getRegionStates()\n+            .getRegionsOfTable(userTableName.getBytes());\n+      }\n+      Assert.assertEquals(2, regionsOfTable.size());\n+      Scan s = new Scan();\n+      ResultScanner scanner = table.getScanner(s);\n+      int mainTableCount = 0;\n+      for (Result rr = scanner.next(); rr != null; rr = scanner.next()) {\n+        mainTableCount++;\n+      }\n+      Assert.assertEquals(3, mainTableCount);\n+    } finally {\n+      table.close();\n+    }\n+  }\n+\n   /**\n    * Noop Abortable implementation used below in tests.\n    */",
                "raw_url": "https://github.com/apache/hbase/raw/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "sha": "671d3a28a384aef8d22ce989898766369ff07970",
                "status": "modified"
            }
        ],
        "message": "HBASE-8814 Possible NPE in split if a region has empty store files\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1499213 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/9ccf16db51a9ac0948459dbc8499ffea9b83632f",
        "patched_files": [
            "HRegionFileSystem.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegionFileSystem.java",
            "TestSplitTransactionOnCluster.java"
        ]
    },
    "hbase_e223639": {
        "bug_id": "hbase_e223639",
        "commit": "https://github.com/apache/hbase/commit/e2236396713cfc1cef61150a569e972c2faaca04",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "patch": "@@ -391,8 +391,8 @@ public void initialize() throws HBaseIOException {\n         HBASE_GROUP_LOADBALANCER_CLASS,\n         StochasticLoadBalancer.class, LoadBalancer.class);\n     internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);\n-    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setMasterServices(masterServices);\n+    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setConf(config);\n     internalBalancer.initialize();\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "sha": "c42c46d108ee627e8a813cbcf44e613b5d8f3067",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -742,8 +742,8 @@ private void finishActiveMasterInitialization(MonitoredTask status)\n     }\n \n     //initialize load balancer\n-    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.setMasterServices(this);\n+    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.initialize();\n \n     // Check if master is shutting down because of some issue",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "eac2fa22f8d61abefebae80ce9873f6876822169",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "patch": "@@ -233,7 +233,7 @@ public synchronized void setClusterStatus(ClusterStatus st) {\n \n       updateMetricsSize(tablesCount * (functionsCount + 1)); // +1 for overall\n     } catch (Exception e) {\n-      LOG.error(\"failed to get the size of all tables, exception = \" + e.getMessage());\n+      LOG.error(\"failed to get the size of all tables\", e);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "sha": "b02aac1aadd76aab8d03cd1ea370bb7c64e8ea81",
                "status": "modified"
            }
        ],
        "message": "HBASE-16910 Avoid NPE when starting StochasticLoadBalancer (Guanghao Zhang)",
        "parent": "https://github.com/apache/hbase/commit/0ae211eb399e5524196d89af8eac1941c8b61b60",
        "patched_files": [
            "RSGroupBasedLoadBalancer.java",
            "StochasticLoadBalancer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStochasticLoadBalancer.java",
            "TestRSGroupBasedLoadBalancer.java"
        ]
    },
    "hbase_e7f6c29": {
        "bug_id": "hbase_e7f6c29",
        "commit": "https://github.com/apache/hbase/commit/e7f6c2972dba2bc1eff8a5ae39893603508336ea",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "deletions": 7,
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "patch": "@@ -92,29 +92,29 @@\n    * break PE having it fail at various junctures. When non-null, testing is set to an instance of\n    * the below internal {@link Testing} class with flags set for the particular test.\n    */\n-  Testing testing = null;\n+  volatile Testing testing = null;\n \n   /**\n    * Class with parameters describing how to fail/die when in testing-context.\n    */\n   public static class Testing {\n-    protected boolean killIfHasParent = true;\n-    protected boolean killIfSuspended = false;\n+    protected volatile boolean killIfHasParent = true;\n+    protected volatile boolean killIfSuspended = false;\n \n     /**\n      * Kill the PE BEFORE we store state to the WAL. Good for figuring out if a Procedure is\n      * persisting all the state it needs to recover after a crash.\n      */\n-    protected boolean killBeforeStoreUpdate = false;\n-    protected boolean toggleKillBeforeStoreUpdate = false;\n+    protected volatile boolean killBeforeStoreUpdate = false;\n+    protected volatile boolean toggleKillBeforeStoreUpdate = false;\n \n     /**\n      * Set when we want to fail AFTER state has been stored into the WAL. Rarely used. HBASE-20978\n      * is about a case where memory-state was being set after store to WAL where a crash could\n      * cause us to get stuck. This flag allows killing at what was a vulnerable time.\n      */\n-    protected boolean killAfterStoreUpdate = false;\n-    protected boolean toggleKillAfterStoreUpdate = false;\n+    protected volatile boolean killAfterStoreUpdate = false;\n+    protected volatile boolean toggleKillAfterStoreUpdate = false;\n \n     protected boolean shouldKillBeforeStoreUpdate() {\n       final boolean kill = this.killBeforeStoreUpdate;",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "sha": "f0affd2b8aebcca828a4f1c2396833d634222718",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "deletions": 4,
                "filename": "hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "patch": "@@ -133,16 +133,15 @@ public static void initAndStartWorkers(ProcedureExecutor<?> procExecutor, int nu\n     if (actionBeforeStartWorker != null) {\n       actionBeforeStartWorker.call();\n     }\n+    if (avoidTestKillDuringRestart) {\n+      procExecutor.testing = testing;\n+    }\n     if (startWorkers) {\n       procExecutor.startWorkers();\n     }\n     if (startAction != null) {\n       startAction.call();\n     }\n-\n-    if (avoidTestKillDuringRestart) {\n-      procExecutor.testing = testing;\n-    }\n   }\n \n   public static void storeRestart(ProcedureStore procStore, ProcedureStore.ProcedureLoader loader)",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "sha": "452e08bc9c17d9a7b405fa1fb82068606e8e0c90",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "deletions": 23,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "patch": "@@ -275,36 +275,31 @@ public void testRollbackAndDoubleExecution() throws Exception {\n \n   @Test\n   public void testMergeWithoutPONR() throws Exception {\n-    try {\n-      final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n-      final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n \n-      List<RegionInfo> tableRegions = createTable(tableName);\n+    List<RegionInfo> tableRegions = createTable(tableName);\n \n-      ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n-      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+    ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n \n-      RegionInfo[] regionsToMerge = new RegionInfo[2];\n-      regionsToMerge[0] = tableRegions.get(0);\n-      regionsToMerge[1] = tableRegions.get(1);\n+    RegionInfo[] regionsToMerge = new RegionInfo[2];\n+    regionsToMerge[0] = tableRegions.get(0);\n+    regionsToMerge[1] = tableRegions.get(1);\n \n-      long procId = procExec.submitProcedure(\n-        new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n+    long procId = procExec.submitProcedure(\n+      new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n \n-      // Execute until step 9 of split procedure\n-      // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n-      MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n+    // Execute until step 9 of split procedure\n+    // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n+    MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n \n-      // Unset Toggle Kill and make ProcExec work correctly\n-      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n-      MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n-      ProcedureTestingUtility.waitProcedure(procExec, procId);\n+    // Unset Toggle Kill and make ProcExec work correctly\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n+    MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n+    ProcedureTestingUtility.waitProcedure(procExec, procId);\n \n-      assertRegionCount(tableName, initialRegionCount - 1);\n-    } catch (Throwable t) {\n-      LOG.error(\"error!\", t);\n-      throw t;\n-    }\n+    assertRegionCount(tableName, initialRegionCount - 1);\n   }\n \n   private List<RegionInfo> createTable(final TableName tableName) throws Exception {",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "sha": "6a8c4b3ae5f47c7dbb5b1a1c5497279314dbc29b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "patch": "@@ -110,8 +110,12 @@ public Void call() throws Exception {\n         @Override\n         public Void call() throws Exception {\n           AssignmentManager am = env.getAssignmentManager();\n-          am.joinCluster();\n-          master.setInitialized(true);\n+          try {\n+            am.joinCluster();\n+            master.setInitialized(true);\n+          } catch (Exception e) {\n+            LOG.warn(\"Failed to load meta\", e);\n+          }\n           return null;\n         }\n       });",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "sha": "98c39781432a653a9ca4a6efd8a17fe96d24e534",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.TableName;\n@@ -63,7 +64,8 @@\n   private void setupConf(Configuration conf) {\n     conf.setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n     conf.set(\"hbase.balancer.tablesOnMaster\", \"none\");\n-    conf.setInt(\"hbase.client.retries.number\", 3);\n+    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 3);\n+    conf.setInt(HConstants.HBASE_CLIENT_SERVERSIDE_RETRIES_MULTIPLIER, 3);\n   }\n \n   @Before",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "sha": "0e4a84b5c7669b210d6af06f16adc66e2bc5eb42",
                "status": "modified"
            }
        ],
        "message": "HBASE-21422 NPE in TestMergeTableRegionsProcedure.testMergeWithoutPONR",
        "parent": "https://github.com/apache/hbase/commit/ee55b558c0de0412b40ae65756b50ffb1bc49eee",
        "patched_files": [
            "ProcedureTestingUtility.java",
            "MergeTableRegionsProcedure.java",
            "MasterProcedureTestingUtility.java",
            "ProcedureExecutor.java",
            "ServerCrashProcedure.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestServerCrashProcedure.java",
            "TestMergeTableRegionsProcedure.java",
            "TestProcedureExecutor.java"
        ]
    },
    "hbase_e86b13f": {
        "bug_id": "hbase_e86b13f",
        "commit": "https://github.com/apache/hbase/commit/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -718,7 +718,8 @@ private static long getSeqNumDuringOpen(final Result r, final int replicaId) {\n \n   /**\n    * Returns an HRegionLocationList extracted from the result.\n-   * @return an HRegionLocationList containing all locations for the region range\n+   * @return an HRegionLocationList containing all locations for the region range or null if\n+   *  we can't deserialize the result.\n    */\n   public static RegionLocations getRegionLocations(final Result r) {\n     if (r == null) return null;",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "9517113fcc6763c1952dc38326a3c003c1a81159",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.apache.hadoop.hbase.HRegionLocation;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.RegionTransition;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n@@ -2760,7 +2761,13 @@ boolean waitUntilNoRegionsInTransition(final long timeout)\n     Set<ServerName> offlineServers = new HashSet<ServerName>();\n     // Iterate regions in META\n     for (Result result : results) {\n-      HRegionLocation[] locations = MetaReader.getRegionLocations(result).getRegionLocations();\n+      if (result == null && LOG.isDebugEnabled()){\n+        LOG.debug(\"null result from meta - ignoring but this is strange.\");\n+        continue;\n+      }\n+      RegionLocations rl =  MetaReader.getRegionLocations(result);\n+      if (rl == null) continue;\n+      HRegionLocation[] locations = rl.getRegionLocations();\n       if (locations == null) continue;\n       for (HRegionLocation hrl : locations) {\n         HRegionInfo regionInfo = hrl.getRegionInfo();",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "f4075698228bf80af018ad7d4283e5a923e9c74f",
                "status": "modified"
            }
        ],
        "message": "HBASE-10957 HMaster can abort with NPE in #rebuildUserRegions (Nicolas Liochon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1590184 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/48ffa4d5e615c78f6db8f6c2beddd93460887642",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_e890776": {
        "bug_id": "hbase_e890776",
        "commit": "https://github.com/apache/hbase/commit/e890776fe04697a32a71ac6a4a5dada9bc549d80",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/e890776fe04697a32a71ac6a4a5dada9bc549d80/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java?ref=e890776fe04697a32a71ac6a4a5dada9bc549d80",
                "deletions": 6,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "patch": "@@ -109,7 +109,6 @@ public String explainFailure() throws Exception {\n \n   @Test\n   public void testMergeRegionOrder() throws Exception {\n-\n     int regionCount= 20;\n \n     TableName tableName = TableName.valueOf(\"MergeRegionOrder\");\n@@ -143,14 +142,13 @@ public void testMergeRegionOrder() throws Exception {\n     RegionInfo mergedRegion = mergedRegions.get(0);\n \n     List<RegionInfo> mergeParentRegions = MetaTableAccessor.getMergeRegions(UTIL.getConnection(),\n-      mergedRegion.getEncodedNameAsBytes());\n+      mergedRegion.getRegionName());\n \n     assertEquals(mergeParentRegions.size(), regionCount);\n \n-    for (int c = 0; c < regionCount-1; c++) {\n-      assertTrue(Bytes.compareTo(\n-        mergeParentRegions.get(c).getStartKey(),\n-        mergeParentRegions.get(c+1).getStartKey()) < 0);\n+    for (int c = 0; c < regionCount - 1; c++) {\n+      assertTrue(Bytes.compareTo(mergeParentRegions.get(c).getStartKey(),\n+        mergeParentRegions.get(c + 1).getStartKey()) < 0);\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/e890776fe04697a32a71ac6a4a5dada9bc549d80/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "sha": "04389bb66833790095447f85e99e9554f0be5e41",
                "status": "modified"
            }
        ],
        "message": "HBASE-22941 Addendum fix NPE in UT",
        "parent": "https://github.com/apache/hbase/commit/49718b8b46cd9e06aeef3f74647b32c6df99f7ae",
        "patched_files": [],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitMerge.java"
        ]
    },
    "hbase_e95cf8f": {
        "bug_id": "hbase_e95cf8f",
        "commit": "https://github.com/apache/hbase/commit/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -860,7 +860,13 @@ int getLeastLoadedTopServerForRegion(int region) {\n         int leastLoadedServerIndex = -1;\n         int load = Integer.MAX_VALUE;\n         for (ServerName sn : topLocalServers) {\n-          int index = serversToIndex.get(sn);\n+          if (!serversToIndex.containsKey(sn.getHostAndPort())) {\n+            continue;\n+          }\n+          int index = serversToIndex.get(sn.getHostAndPort());\n+          if (regionsPerServer[index] == null) {\n+            continue;\n+          }\n           int tempLoad = regionsPerServer[index].length;\n           if (tempLoad <= load) {\n             leastLoadedServerIndex = index;",
                "raw_url": "https://github.com/apache/hbase/raw/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "962b241663fc320e92b62926faaea81d8435d4d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-14291 NPE On StochasticLoadBalancer Balance Involving RS With No Regions",
        "parent": "https://github.com/apache/hbase/commit/902cd172f8a1ace4ad9fb35d4dfb7700fc10de21",
        "patched_files": [
            "BaseLoadBalancer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_ea3c445": {
        "bug_id": "hbase_ea3c445",
        "commit": "https://github.com/apache/hbase/commit/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -956,7 +956,7 @@ RegionPlan getRegionPlan(final RegionState state,\n     synchronized (this.regionPlans) {\n       existingPlan = this.regionPlans.get(encodedName);\n       if (existingPlan == null || forceNewPlan ||\n-          existingPlan.getDestination().equals(serverToExclude)) {\n+          (existingPlan != null && existingPlan.getDestination().equals(serverToExclude))) {\n         newPlan = true;\n         this.regionPlans.put(encodedName, randomPlan);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "c3fd667a1411dcfb4625507ca4b7fbfc0537efd6",
                "status": "modified"
            }
        ],
        "message": "Fix possible NPE in assign\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1042885 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d547429a1442e48d9a3f69faaf9298b817a29967",
        "patched_files": [
            "AssignmentManager.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_eed3a47": {
        "bug_id": "hbase_eed3a47",
        "commit": "https://github.com/apache/hbase/commit/eed3a473435006c62f693cda66b1da3f49154963",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/eed3a473435006c62f693cda66b1da3f49154963/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java?ref=eed3a473435006c62f693cda66b1da3f49154963",
                "deletions": 2,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "patch": "@@ -442,7 +442,6 @@ public void testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles() throws Exc\n     HColumnDescriptor hcd = new HColumnDescriptor(\"col\");\n     htd.addFamily(hcd);\n     admin.createTable(htd);\n-    ZKAssign.blockUntilNoRIT(zkw);\n     HTable table = new HTable(conf, userTableName);\n     try {\n       for (int i = 0; i <= 5; i++) {\n@@ -678,12 +677,14 @@ public void testSplitBeforeSettingSplittingInZK() throws Exception,\n \n   @Test(timeout = 60000)\n   public void testTableExistsIfTheSpecifiedTableRegionIsSplitParent() throws Exception {\n+    ZooKeeperWatcher zkw = HBaseTestingUtility.getZooKeeperWatcher(TESTING_UTIL);\n     final byte[] tableName =\n         Bytes.toBytes(\"testTableExistsIfTheSpecifiedTableRegionIsSplitParent\");\n     // Create table then get the single region for our new table.\n     HTable t = createTableAndWait(tableName, Bytes.toBytes(\"cf\"));\n+    List<HRegion> regions = null;\n     try {\n-      List<HRegion> regions = cluster.getRegions(tableName);\n+      regions = cluster.getRegions(tableName);\n       int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());\n       HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);\n       insertData(tableName, admin, t);\n@@ -707,6 +708,11 @@ public void testTableExistsIfTheSpecifiedTableRegionIsSplitParent() throws Excep\n           Bytes.toString(tableName));\n       assertEquals(\"The specified table should present.\", true, tableExists);\n     } finally {\n+      if (regions != null) {\n+        String node = ZKAssign.getNodeName(zkw, regions.get(0).getRegionInfo()\n+            .getEncodedName());\n+        ZKUtil.deleteNodeFailSilent(zkw, node);\n+      }\n       admin.setBalancerRunning(true, false);\n       cluster.getMaster().setCatalogJanitorEnabled(true);\n       t.close();",
                "raw_url": "https://github.com/apache/hbase/raw/eed3a473435006c62f693cda66b1da3f49154963/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "sha": "b13324647af8863e1b40fbbf2c2ad7d2174bbe81",
                "status": "modified"
            }
        ],
        "message": "HBASE-8814 Possible NPE in split if a region has empty store files; ADDENDUM\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1499801 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/3631ba5022548377000cc2753ca29b984abc5caa",
        "patched_files": [],
        "repo": "hbase",
        "unit_tests": [
            "TestSplitTransactionOnCluster.java"
        ]
    },
    "hbase_ef4d353": {
        "bug_id": "hbase_ef4d353",
        "commit": "https://github.com/apache/hbase/commit/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
                "deletions": 3,
                "filename": "core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -1921,7 +1921,8 @@ public Path getBaseDir() {\n    * It is used to combine scanners from multiple Stores (aka column families).\n    */\n   class RegionScanner implements InternalScanner {\n-    private KeyValueHeap storeHeap = null;\n+    // Package local for testability\n+    KeyValueHeap storeHeap = null;\n     private final byte [] stopRow;\n     private Filter filter;\n     private List<KeyValue> results = new ArrayList<KeyValue>();\n@@ -2091,8 +2092,11 @@ private boolean hasResults() {\n     }\n \n     public synchronized void close() {\n-      storeHeap.close();\n-      this.filterClosed = true;\n+      if (storeHeap != null) {\n+        storeHeap.close();\n+        storeHeap = null;\n+      }\n+\t  this.filterClosed = true;\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/hbase/raw/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "a385ce1f767696da4854e33bfd0863ac04da053b",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/hbase/blob/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
                "deletions": 10,
                "filename": "core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "patch": "@@ -41,11 +41,9 @@\n import org.apache.hadoop.hbase.filter.Filter;\n import org.apache.hadoop.hbase.filter.FilterList;\n import org.apache.hadoop.hbase.filter.PrefixFilter;\n-import org.apache.hadoop.hbase.filter.RowFilter;\n import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n import org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.junit.Assert;\n \n import java.io.IOException;\n import java.util.ArrayList;\n@@ -54,6 +52,9 @@\n import java.util.List;\n import java.util.Map;\n import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n \n /**\n  * Basic stand-alone testing of HRegion.\n@@ -92,6 +93,93 @@ protected void setUp() throws Exception {\n   // /tmp/testtable\n   //////////////////////////////////////////////////////////////////////////////\n \n+  public void testGetWhileRegionClose() throws IOException {\n+    HBaseConfiguration hc = initSplit();\n+    int numRows = 100;\n+    byte [][] families = {fam1, fam2, fam3};\n+    \n+    //Setting up region\n+    String method = this.getName();\n+    initHRegion(tableName, method, hc, families);\n+\n+    // Put data in region\n+    final int startRow = 100;\n+    putData(startRow, numRows, qual1, families);\n+    putData(startRow, numRows, qual2, families);\n+    putData(startRow, numRows, qual3, families);\n+    // this.region.flushcache();\n+    final AtomicBoolean done = new AtomicBoolean(false);\n+    final AtomicInteger gets = new AtomicInteger(0);\n+    GetTillDoneOrException [] threads = new GetTillDoneOrException[10];\n+    try {\n+      // Set ten threads running concurrently getting from the region.\n+      for (int i = 0; i < threads.length / 2; i++) {\n+        threads[i] = new GetTillDoneOrException(i, Bytes.toBytes(\"\" + startRow),\n+          done, gets);\n+        threads[i].setDaemon(true);\n+        threads[i].start();\n+      }\n+      // Artificially make the condition by setting closing flag explicitly.\n+      // I can't make the issue happen with a call to region.close().\n+      this.region.closing.set(true);\n+      for (int i = threads.length / 2; i < threads.length; i++) {\n+        threads[i] = new GetTillDoneOrException(i, Bytes.toBytes(\"\" + startRow),\n+          done, gets);\n+        threads[i].setDaemon(true);\n+        threads[i].start();\n+      }\n+    } finally {\n+      if (this.region != null) {\n+        this.region.close();\n+        this.region.getLog().closeAndDelete();\n+      }\n+    }\n+    done.set(true);\n+    for (GetTillDoneOrException t: threads) {\n+      try {\n+        t.join();\n+      } catch (InterruptedException e) {\n+        e.printStackTrace();\n+      }\n+      if (t.e != null) {\n+        LOG.info(\"Exception=\" + t.e);\n+        assertFalse(\"Found a NPE in \" + t.getName(),\n+          t.e instanceof NullPointerException);\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Thread that does get on single row until 'done' flag is flipped.  If an\n+   * exception causes us to fail, it records it.\n+   */\n+  class GetTillDoneOrException extends Thread {\n+    private final Get g;\n+    private final AtomicBoolean done;\n+    private final AtomicInteger count;\n+    private Exception e;\n+\n+    GetTillDoneOrException(final int i, final byte[] r, final AtomicBoolean d,\n+        final AtomicInteger c) {\n+      super(\"getter.\" + i);\n+      this.g = new Get(r);\n+      this.done = d;\n+      this.count = c;\n+    }\n+\n+    @Override\n+    public void run() {\n+      while (!this.done.get()) {\n+        try {\n+          assertTrue(region.get(g, null).size() > 0);\n+          this.count.incrementAndGet();\n+        } catch (Exception e) {\n+          this.e = e;\n+          break;\n+        }\n+      }\n+    }\n+  }\n \n   /*\n    * An involved filter test.  Has multiple column families and deletes in mix.\n@@ -1034,13 +1122,13 @@ public void testGetScanner_WithNoFamilies() throws IOException {\n     scan.addFamily(fam4);\n     is = (RegionScanner) region.getScanner(scan);\n     is.initHeap(); // i dont like this test\n-    assertEquals(1, ((RegionScanner)is).getStoreHeap().getHeap().size());\n-\n+    assertEquals(1, ((RegionScanner)is).storeHeap.getHeap().size());\n+    \n     scan = new Scan();\n     is = (RegionScanner) region.getScanner(scan);\n     is.initHeap();\n     assertEquals(families.length -1, \n-        ((RegionScanner)is).getStoreHeap().getHeap().size());\n+        ((RegionScanner)is).storeHeap.getHeap().size());\n   }\n \n   public void testRegionScanner_Next() throws IOException {\n@@ -1921,7 +2009,7 @@ public void testFlushCacheWhileScanning() throws IOException, InterruptedExcepti\n         if (!toggle) {\n           flushThread.flush();\n         }\n-        Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n+        assertEquals(\"i=\" + i, expectedCount, res.size());\n         toggle = !toggle;\n       }\n     }\n@@ -1944,7 +2032,7 @@ public void done() {\n \n     public void checkNoError() {\n       if (error != null) {\n-        Assert.assertNull(error);\n+        assertNull(error);\n       }\n     }\n \n@@ -2082,7 +2170,7 @@ public void done() {\n \n     public void checkNoError() {\n       if (error != null) {\n-        Assert.assertNull(error);\n+        assertNull(error);\n       }\n     }\n \n@@ -2175,7 +2263,7 @@ public void testWritesWhileGetting()\n       boolean previousEmpty = result == null || result.isEmpty();\n       result = region.get(get, null);\n       if (!result.isEmpty() || !previousEmpty || i > compactInterval) {\n-        Assert.assertEquals(\"i=\" + i, expectedCount, result.size());\n+        assertEquals(\"i=\" + i, expectedCount, result.size());\n         // TODO this was removed, now what dangit?!\n         // search looking for the qualifier in question?\n         long timestamp = 0;\n@@ -2185,7 +2273,7 @@ public void testWritesWhileGetting()\n             timestamp = kv.getTimestamp();\n           }\n         }\n-        Assert.assertTrue(timestamp >= prevTimestamp);\n+        assertTrue(timestamp >= prevTimestamp);\n         prevTimestamp = timestamp;\n \n         byte [] gotValue = null;",
                "raw_url": "https://github.com/apache/hbase/raw/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "sha": "482caa900a1ae8dfc806de7f52bf630bbf42f72f",
                "status": "modified"
            }
        ],
        "message": "HBASE-2509 NPEs in various places, HRegion.get, HRS.close\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@944533 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/f65e61c7abe27fd718f1f4abb25fe59a20249aa6",
        "patched_files": [
            "HRegion.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_f19f1d9": {
        "bug_id": "hbase_f19f1d9",
        "commit": "https://github.com/apache/hbase/commit/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java?ref=f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "patch": "@@ -253,10 +253,11 @@ protected void checkActiveSize() {\n       * in exclusive mode while this method (checkActiveSize) is invoked holding updatesLock\n       * in the shared mode. */\n       InMemoryFlushRunnable runnable = new InMemoryFlushRunnable();\n-      LOG.info(\"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\n+          \"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      }\n       getPool().execute(runnable);\n-      // guard against queuing same old compactions over and over again\n-      inMemoryFlushInProgress.set(true);\n     }\n   }\n \n@@ -277,10 +278,9 @@ void flushInMemory() throws IOException {\n     }\n     // Phase II: Compact the pipeline\n     try {\n-      if (allowCompaction.get()) {\n+      if (allowCompaction.get() && inMemoryFlushInProgress.compareAndSet(false, true)) {\n         // setting the inMemoryFlushInProgress flag again for the case this method is invoked\n         // directly (only in tests) in the common path setting from true to true is idempotent\n-        inMemoryFlushInProgress.set(true);\n         // Speculative compaction execution, may be interrupted if flush is forced while\n         // compaction is in progress\n         compactor.startCompaction();",
                "raw_url": "https://github.com/apache/hbase/raw/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "sha": "ec5684dcda1fe49d6e5ff13eba43496244285e8d",
                "status": "modified"
            }
        ],
        "message": "HBASE-15999 NPE in MemstoreCompactor (Ram)",
        "parent": "https://github.com/apache/hbase/commit/158568e7806e461275406bc15856ba26e4660f4c",
        "patched_files": [
            "CompactingMemStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCompactingMemStore.java"
        ]
    },
    "hbase_f2820ea": {
        "bug_id": "hbase_f2820ea",
        "commit": "https://github.com/apache/hbase/commit/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -329,11 +329,16 @@ private void tryStartNewShipper(String walGroupId, PriorityBlockingQueue<Path> q\n       replicationDelay =\n           ReplicationLoad.calculateReplicationDelay(ageOfLastShippedOp, lastTimeStamp, queueSize);\n       Path currentPath = shipper.getCurrentPath();\n-      try {\n-        fileSize = getFileSize(currentPath);\n-      } catch (IOException e) {\n-        LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n-        fileSize = -1;\n+      fileSize = -1;\n+      if (currentPath != null) {\n+        try {\n+          fileSize = getFileSize(currentPath);\n+        } catch (IOException e) {\n+          LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n+        }\n+      } else {\n+        currentPath = new Path(\"NO_LOGS_IN_QUEUE\");\n+        LOG.warn(\"No replication ongoing, waiting for new log\");\n       }\n       ReplicationStatus.ReplicationStatusBuilder statusBuilder = ReplicationStatus.newBuilder();\n       statusBuilder.withPeerId(this.getPeerId())",
                "raw_url": "https://github.com/apache/hbase/raw/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "f1b6e766877fe499a59dd6d6fd5f84c1ab9350ee",
                "status": "modified"
            }
        ],
        "message": "HBASE-21749 RS UI may throw NPE and make rs-status page inaccessible with multiwal and replication\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/35df6147eef0baf25e127df358f6e50bbf967e2c",
        "patched_files": [
            "ReplicationSource.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java"
        ]
    },
    "hbase_f29260c": {
        "bug_id": "hbase_f29260c",
        "commit": "https://github.com/apache/hbase/commit/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java?ref=f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
                "deletions": 0,
                "filename": "hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "patch": "@@ -591,6 +591,9 @@ private static WebAppContext createWebAppContext(String name,\n     ctx.setContextPath(\"/\");\n     ctx.setWar(appDir + \"/\" + name);\n     ctx.getServletContext().setAttribute(CONF_CONTEXT_ATTRIBUTE, conf);\n+    // for org.apache.hadoop.metrics.MetricsServlet\n+    ctx.getServletContext().setAttribute(\n+      org.apache.hadoop.http.HttpServer2.CONF_CONTEXT_ATTRIBUTE, conf);\n     ctx.getServletContext().setAttribute(ADMINS_ACL, adminsAcl);\n     addNoCacheFilter(ctx);\n     return ctx;",
                "raw_url": "https://github.com/apache/hbase/raw/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "sha": "af72ab8c1e65f91310ddd479239dfdbeace4e9bf",
                "status": "modified"
            }
        ],
        "message": "HBASE-19424 Fix NPE in \"/metrics\" servlet.\n\nSigned-off-by: Apekshit Sharma <appy@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/2509a150c0792e914429264453510b9028250c29",
        "patched_files": [
            "HttpServer.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHttpServer.java"
        ]
    },
    "hbase_f440612": {
        "bug_id": "hbase_f440612",
        "commit": "https://github.com/apache/hbase/commit/f4406121af4ff600d2ff581e5dbe46ec34b06cca",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -139,6 +139,7 @@ Release 0.21.0 - Unreleased\n                its regions around\n    HBASE-2065  Cannot disable a table if any of its region is opening \n                at the same time\n+   HBASE-2026  NPE in StoreScanner on compaction\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "sha": "36591df27136788929bf6dbe97dad86e5176b692",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -255,6 +255,7 @@ public synchronized void updateReaders() throws IOException {\n \n     // Reset the state of the Query Matcher and set to top row\n     matcher.reset();\n-    matcher.setRow(heap.peek().getRow());\n+    KeyValue kv = heap.peek();\n+    matcher.setRow((kv == null ? topKey : kv).getRow());\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "d30432516294e98680fa5d4737e546cf3c0aa834",
                "status": "modified"
            }
        ],
        "message": "HBASE-2026  NPE in StoreScanner on compaction\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@894219 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/69a6ef5a9a8c09d8cf918751473240c5d77cfaae",
        "patched_files": [
            "StoreScanner.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestStoreScanner.java"
        ]
    },
    "hbase_f6d44db": {
        "bug_id": "hbase_f6d44db",
        "commit": "https://github.com/apache/hbase/commit/f6d44db9c694281ab3df6854c83033cd7dc3d29c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "deletions": 1,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "patch": "@@ -1130,9 +1130,12 @@ public String toString() {\n \n   /**\n    * @param k Key portion of a KeyValue.\n-   * @return Key as a String.\n+   * @return Key as a String, empty string if k is null. \n    */\n   public static String keyToString(final byte [] k) {\n+    if (k == null) { \n+      return \"\";\n+    }\n     return keyToString(k, 0, k.length);\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "sha": "06675b29d590c0106b0b9ad3fd47b63cae6984d0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "patch": "@@ -350,7 +350,11 @@ private void printMeta(HFile.Reader reader, Map<byte[], byte[]> fileInfo)\n       }\n     }\n \n-    System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    try {\n+      System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    } catch (Exception e) {\n+      System.out.println (\"Unable to retrieve the midkey\");\n+    }\n \n     // Printing general bloom information\n     DataInput bloomMeta = reader.getGeneralBloomFilterMetadata();",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "sha": "a7484e2c7a774e6a0dab8631beece000f4c19716",
                "status": "modified"
            }
        ],
        "message": "HBASE-9649 HFilePrettyPrinter should not throw a NPE if FirstKey or LastKey is null (Jean-Marc Spaggiari)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1526113 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/91508fd56478e73e125c2eb236338afd1c155f23",
        "patched_files": [
            "KeyValue.java",
            "HFilePrettyPrinter.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHFilePrettyPrinter.java",
            "TestKeyValue.java"
        ]
    },
    "hbase_f782846": {
        "bug_id": "hbase_f782846",
        "commit": "https://github.com/apache/hbase/commit/f78284685fc533230a0395d297ebacff32632396",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=f78284685fc533230a0395d297ebacff32632396",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "patch": "@@ -2571,7 +2571,8 @@ public GetResponse get(final RpcController controller, final GetRequest request)\n         }\n         builder.setResult(pbr);\n       }\n-      if (r != null) {\n+      //r.cells is null when an table.exists(get) call\n+      if (r != null && r.rawCells() != null) {\n         quota.addGetResult(r);\n       }\n       return builder.build();",
                "raw_url": "https://github.com/apache/hbase/raw/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "sha": "f788a86f47b2602c2d8e8040a56d1848663644da",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hbase/blob/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java?ref=f78284685fc533230a0395d297ebacff32632396",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "patch": "@@ -553,6 +553,23 @@ public void testTableReadCapacityUnitThrottle() throws Exception {\n     triggerTableCacheRefresh(true, TABLE_NAMES[0]);\n   }\n \n+  @Test\n+  public void testTableExistsGetThrottle() throws Exception {\n+    final Admin admin = TEST_UTIL.getAdmin();\n+\n+    // Add throttle quota\n+    admin.setQuota(QuotaSettingsFactory.throttleTable(TABLE_NAMES[0],\n+        ThrottleType.REQUEST_NUMBER, 100, TimeUnit.MINUTES));\n+    triggerTableCacheRefresh(false, TABLE_NAMES[0]);\n+\n+    Table table = TEST_UTIL.getConnection().getTable(TABLE_NAMES[0]);\n+    // An exists call when having throttle quota\n+    table.exists(new Get(Bytes.toBytes(\"abc\")));\n+\n+    admin.setQuota(QuotaSettingsFactory.unthrottleTable(TABLE_NAMES[0]));\n+    triggerTableCacheRefresh(true, TABLE_NAMES[0]);\n+  }\n+\n   private int doPuts(int maxOps, final Table... tables) throws Exception {\n     return doPuts(maxOps, -1, tables);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "sha": "c0694031489d738cb7be7f4d9f64c4ecff35dd34",
                "status": "modified"
            }
        ],
        "message": "HBASE-21592 quota.addGetResult(r) throw NPE\n\nSigned-off-by: huzheng <openinx@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/1971d02e725341fdee79b7ee2308a9870debe2f6",
        "patched_files": [
            "RSRpcServices.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestQuotaThrottle.java"
        ]
    },
    "hbase_fa5f75a": {
        "bug_id": "hbase_fa5f75a",
        "commit": "https://github.com/apache/hbase/commit/fa5f75ab0e799f839b8eacc8615f7142b9254110",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -231,6 +231,8 @@ Release 0.91.0 - Unreleased\n    HBASE-4310  SlabCache metrics bugfix (Li Pi)\n    HBASE-4283  HBaseAdmin never recovers from restarted cluster (Lars Hofhansl)\n    HBASE-4315  RPC logging too verbose (todd)\n+   HBASE-4273  java.lang.NullPointerException when a table is being disabled and\n+               HMaster restarts (Ming Ma)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/CHANGES.txt",
                "sha": "fe83fc650d46cc04b53f8cbb628b70b4eb75637c",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "patch": "@@ -0,0 +1,50 @@\n+/**\n+ * Copyright 2011 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * Thrown if a table should be enabled but is not\n+ */\n+public class TableNotEnabledException extends IOException {\n+  private static final long serialVersionUID = 262144L;\n+  /** default constructor */\n+  public TableNotEnabledException() {\n+    super();\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param s message\n+   */\n+  public TableNotEnabledException(String s) {\n+    super(s);\n+  }\n+\n+  /**\n+   * @param tableName Name of table that is not enabled\n+   */\n+  public TableNotEnabledException(byte[] tableName) {\n+    this(Bytes.toString(tableName));\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "sha": "d0392d52f6e3c6b7f305b1858d6dedc8b4a13bd5",
                "status": "added"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "patch": "@@ -565,8 +565,12 @@ public void enableTable(final String tableName)\n   /**\n    * Enable a table.  May timeout.  Use {@link #enableTableAsync(byte[])}\n    * and {@link #isTableEnabled(byte[])} instead.\n+   * The table has to be in disabled state for it to be enabled.\n    * @param tableName name of the table\n    * @throws IOException if a remote or network exception occurs\n+   * There could be couple types of IOException\n+   * TableNotFoundException means the table doesn't exist.\n+   * TableNotDisabledException means the table isn't in disabled state.\n    * @see #isTableEnabled(byte[])\n    * @see #disableTable(byte[])\n    * @see #enableTableAsync(byte[])\n@@ -706,8 +710,12 @@ public void disableTable(final String tableName)\n    * Disable table and wait on completion.  May timeout eventually.  Use\n    * {@link #disableTableAsync(byte[])} and {@link #isTableDisabled(String)}\n    * instead.\n+   * The table has to be in enabled state for it to be disabled.\n    * @param tableName\n    * @throws IOException\n+   * There could be couple types of IOException\n+   * TableNotFoundException means the table doesn't exist.\n+   * TableNotEnabledException means the table isn't in enabled state.\n    */\n   public void disableTable(final byte [] tableName)\n   throws IOException {",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "sha": "f17036e1eeb47f358b4560c01cc101c0e999ccde",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1830,12 +1830,20 @@ boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regi\n       ServerName regionLocation = region.getSecond();\n       String tableName = regionInfo.getTableNameAsString();\n       if (regionLocation == null) {\n-        // Region not being served, add to region map with no assignment\n-        // If this needs to be assigned out, it will also be in ZK as RIT\n-        // add if the table is not in disabled and enabling state\n-        if (false == checkIfRegionBelongsToDisabled(regionInfo)\n-            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {\n-          regions.put(regionInfo, regionLocation);\n+        // regionLocation could be null if createTable didn't finish properly.\n+        // When createTable is in progress, HMaster restarts.\n+        // Some regions have been added to .META., but have not been assigned.\n+        // When this happens, the region's table must be in ENABLING state.\n+        // It can't be in ENABLED state as that is set when all regions are\n+        // assigned.\n+        // It can't be in DISABLING state, because DISABLING state transitions\n+        // from ENABLED state when application calls disableTable.\n+        // It can't be in DISABLED state, because DISABLED states transitions\n+        // from DISABLING state.\n+        if (false == checkIfRegionsBelongsToEnabling(regionInfo)) {\n+          LOG.warn(\"Region \" + regionInfo.getEncodedName() +\n+            \" has null regionLocation.\" + \" But its table \" + tableName +\n+            \" isn't in ENABLING state.\");\n         }\n         addTheTablesInPartialState(disablingTables, enablingTables, regionInfo,\n             tableName);\n@@ -1901,7 +1909,7 @@ private boolean recoverTableInDisablingState(Set<String> disablingTables)\n             + \" is in DISABLING state.  Hence recovering by moving the table\"\n             + \" to DISABLED state.\");\n         new DisableTableHandler(this.master, tableName.getBytes(),\n-            catalogTracker, this).process();\n+            catalogTracker, this, true).process();\n       }\n     }\n     return isWatcherCreated;\n@@ -1931,7 +1939,7 @@ private void recoverTableInEnablingState(Set<String> enablingTables,\n             + \" is in ENABLING state.  Hence recovering by moving the table\"\n             + \" to ENABLED state.\");\n         new EnableTableHandler(this.master, tableName.getBytes(),\n-            catalogTracker, this).process();\n+            catalogTracker, this, true).process();\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "698c7d3518cef2dd4db136c5b0111f41aaa7b2bc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -1029,7 +1029,7 @@ public void enableTable(final byte [] tableName) throws IOException {\n       cpHost.preEnableTable(tableName);\n     }\n     this.executorService.submit(new EnableTableHandler(this, tableName,\n-      catalogTracker, assignmentManager));\n+      catalogTracker, assignmentManager, false));\n     if (cpHost != null) {\n       cpHost.postEnableTable(tableName);\n     }\n@@ -1040,7 +1040,7 @@ public void disableTable(final byte [] tableName) throws IOException {\n       cpHost.preDisableTable(tableName);\n     }\n     this.executorService.submit(new DisableTableHandler(this, tableName,\n-      catalogTracker, assignmentManager));\n+      catalogTracker, assignmentManager, false));\n     if (cpHost != null) {\n       cpHost.postDisableTable(tableName);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "de93dc546aa7759ed32ba134e14dc3b55207bd38",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 12,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableNotEnabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.catalog.CatalogTracker;\n import org.apache.hadoop.hbase.catalog.MetaReader;\n@@ -46,8 +47,9 @@\n   private final AssignmentManager assignmentManager;\n \n   public DisableTableHandler(Server server, byte [] tableName,\n-      CatalogTracker catalogTracker, AssignmentManager assignmentManager)\n-  throws TableNotFoundException, IOException {\n+      CatalogTracker catalogTracker, AssignmentManager assignmentManager,\n+      boolean skipTableStateCheck)\n+  throws TableNotFoundException, TableNotEnabledException, IOException {\n     super(server, EventType.C_M_DISABLE_TABLE);\n     this.tableName = tableName;\n     this.tableNameStr = Bytes.toString(this.tableName);\n@@ -57,7 +59,25 @@ public DisableTableHandler(Server server, byte [] tableName,\n     //       part of old master rewrite, schema to zk to check for table\n     //       existence and such\n     if (!MetaReader.tableExists(catalogTracker, this.tableNameStr)) {\n-      throw new TableNotFoundException(Bytes.toString(tableName));\n+      throw new TableNotFoundException(this.tableNameStr);\n+    }\n+\n+    // There could be multiple client requests trying to disable or enable\n+    // the table at the same time. Ensure only the first request is honored\n+    // After that, no other requests can be accepted until the table reaches\n+    // DISABLED or ENABLED.\n+    if (!skipTableStateCheck)\n+    {\n+      try {\n+        if (!this.assignmentManager.getZKTable().checkEnabledAndSetDisablingTable\n+          (this.tableNameStr)) {\n+          LOG.info(\"Table \" + tableNameStr + \" isn't enabled; skipping disable\");\n+          throw new TableNotEnabledException(this.tableNameStr);\n+        }\n+      } catch (KeeperException e) {\n+        throw new IOException(\"Unable to ensure that the table will be\" +\n+          \" disabling because of a ZooKeeper issue\", e);\n+      }\n     }\n   }\n \n@@ -67,9 +87,10 @@ public String toString() {\n     if(server != null && server.getServerName() != null) {\n       name = server.getServerName().toString();\n     }\n-    return getClass().getSimpleName() + \"-\" + name + \"-\" + getSeqid() + \"-\" + tableNameStr;\n+    return getClass().getSimpleName() + \"-\" + name + \"-\" + getSeqid() + \"-\" +\n+      tableNameStr;\n   }\n-  \n+\n   @Override\n   public void process() {\n     try {\n@@ -83,18 +104,14 @@ public void process() {\n   }\n \n   private void handleDisableTable() throws IOException, KeeperException {\n-    if (this.assignmentManager.getZKTable().isDisabledTable(this.tableNameStr)) {\n-      LOG.info(\"Table \" + tableNameStr + \" already disabled; skipping disable\");\n-      return;\n-    }\n     // Set table disabling flag up in zk.\n     this.assignmentManager.getZKTable().setDisablingTable(this.tableNameStr);\n     boolean done = false;\n     while (true) {\n       // Get list of online regions that are of this table.  Regions that are\n       // already closed will not be included in this list; i.e. the returned\n-      // list is not ALL regions in a table, its all online regions according to\n-      // the in-memory state on this master.\n+      // list is not ALL regions in a table, its all online regions according\n+      // to the in-memory state on this master.\n       final List<HRegionInfo> regions =\n         this.assignmentManager.getRegionsOfTable(tableName);\n       if (regions.size() == 0) {\n@@ -159,4 +176,4 @@ protected boolean waitUntilDone(long timeout)\n       return regions != null && regions.isEmpty();\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "sha": "5af0690207e24c78813981d1f2b15f260d56917b",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 9,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableNotDisabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.catalog.CatalogTracker;\n import org.apache.hadoop.hbase.catalog.MetaReader;\n@@ -47,8 +48,9 @@\n   private final CatalogTracker ct;\n \n   public EnableTableHandler(Server server, byte [] tableName,\n-      CatalogTracker catalogTracker, AssignmentManager assignmentManager)\n-  throws TableNotFoundException, IOException {\n+      CatalogTracker catalogTracker, AssignmentManager assignmentManager,\n+      boolean skipTableStateCheck)\n+  throws TableNotFoundException, TableNotDisabledException, IOException {\n     super(server, EventType.C_M_ENABLE_TABLE);\n     this.tableName = tableName;\n     this.tableNameStr = Bytes.toString(tableName);\n@@ -58,6 +60,24 @@ public EnableTableHandler(Server server, byte [] tableName,\n     if (!MetaReader.tableExists(catalogTracker, this.tableNameStr)) {\n       throw new TableNotFoundException(Bytes.toString(tableName));\n     }\n+\n+    // There could be multiple client requests trying to disable or enable\n+    // the table at the same time. Ensure only the first request is honored\n+    // After that, no other requests can be accepted until the table reaches\n+    // DISABLED or ENABLED.\n+    if (!skipTableStateCheck)\n+    {\n+      try {\n+        if (!this.assignmentManager.getZKTable().checkDisabledAndSetEnablingTable\n+          (this.tableNameStr)) {\n+          LOG.info(\"Table \" + tableNameStr + \" isn't disabled; skipping enable\");\n+          throw new TableNotDisabledException(this.tableNameStr);\n+        }\n+      } catch (KeeperException e) {\n+        throw new IOException(\"Unable to ensure that the table will be\" +\n+          \" enabling because of a ZooKeeper issue\", e);\n+      }\n+    }\n   }\n \n   @Override\n@@ -83,10 +103,6 @@ public void process() {\n   }\n \n   private void handleEnableTable() throws IOException, KeeperException {\n-    if (this.assignmentManager.getZKTable().isEnabledTable(this.tableNameStr)) {\n-      LOG.info(\"Table \" + tableNameStr + \" is already enabled; skipping enable\");\n-      return;\n-    }\n     // I could check table is disabling and if so, not enable but require\n     // that user first finish disabling but that might be obnoxious.\n \n@@ -121,7 +137,8 @@ private void handleEnableTable() throws IOException, KeeperException {\n       }\n     }\n     // Flip the table to enabled.\n-    if (done) this.assignmentManager.getZKTable().setEnabledTable(this.tableNameStr);\n+    if (done) this.assignmentManager.getZKTable().setEnabledTable(\n+      this.tableNameStr);\n     LOG.info(\"Enabled table is done=\" + done);\n   }\n \n@@ -131,7 +148,8 @@ private void handleEnableTable() throws IOException, KeeperException {\n    * been onlined; i.e. List of regions that need onlining.\n    * @throws IOException\n    */\n-  private List<HRegionInfo> regionsToAssign(final List<HRegionInfo> regionsInMeta)\n+  private List<HRegionInfo> regionsToAssign(\n+    final List<HRegionInfo> regionsInMeta)\n   throws IOException {\n     final List<HRegionInfo> onlineRegions =\n       this.assignmentManager.getRegionsOfTable(tableName);\n@@ -186,4 +204,4 @@ private boolean isDone(final List<HRegionInfo> regions) {\n       return regions != null && regions.size() >= this.countOfRegionsInTable;\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "sha": "b6dc1c04eb9b5d18ac5f4566a04a4f27ffd68137",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "patch": "@@ -189,6 +189,42 @@ public boolean checkAndSetEnablingTable(final String tableName)\n     }\n   }\n \n+  /**\n+   * Sets the specified table as ENABLING in zookeeper atomically\n+   * If the table isn't in DISABLED state, no operation is performed\n+   * @param tableName\n+   * @return if the operation succeeds or not\n+   * @throws KeeperException unexpected zookeeper exception\n+   */\n+  public boolean checkDisabledAndSetEnablingTable(final String tableName)\n+    throws KeeperException {\n+    synchronized (this.cache) {\n+      if (!isDisabledTable(tableName)) {\n+        return false;\n+      }\n+      setTableState(tableName, TableState.ENABLING);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Sets the specified table as DISABLING in zookeeper atomically\n+   * If the table isn't in ENABLED state, no operation is performed\n+   * @param tableName\n+   * @return if the operation succeeds or not\n+   * @throws KeeperException unexpected zookeeper exception\n+   */\n+  public boolean checkEnabledAndSetDisablingTable(final String tableName)\n+    throws KeeperException {\n+    synchronized (this.cache) {\n+      if (!isEnabledTable(tableName)) {\n+        return false;\n+      }\n+      setTableState(tableName, TableState.DISABLING);\n+      return true;\n+    }\n+  }\n+\n   private void setTableState(final String tableName, final TableState state)\n   throws KeeperException {\n     String znode = ZKUtil.joinZNode(this.watcher.tableZNode, tableName);\n@@ -366,4 +402,4 @@ public void setEnabledTable(final String tableName)\n     }\n     return disabledTables;\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "sha": "4930f66987562ee99ae7a4c1f2377c13a3ff425b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 1,
                "filename": "src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "patch": "@@ -171,7 +171,6 @@ public void testFamilyAdminAndMetadata() throws Exception {\n     impl.deleteFamily(tableAname, familyAname);\n     assertEquals(impl.describeTable(tableAname).families.size(), 0);\n \n-    impl.disableTable(tableAname);\n     impl.deleteTable(tableAname);\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "sha": "23b91e12d0fe63077c2246c696debaeb04bb24aa",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 3,
                "filename": "src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "patch": "@@ -1,5 +1,5 @@\n /**\n- * Copyright 2009 The Apache Software Foundation\n+ * Copyright 2011 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -46,6 +46,7 @@\n import org.apache.hadoop.hbase.NotServingRegionException;\n import org.apache.hadoop.hbase.TableExistsException;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n+import org.apache.hadoop.hbase.TableNotEnabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.executor.EventHandler;\n import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n@@ -916,12 +917,37 @@ public void testTableNames() throws IOException {\n    * @throws IOException\n    */\n   @Test (expected=TableExistsException.class)\n-  public void testTableNotFoundExceptionWithATable() throws IOException {\n-    final byte [] name = Bytes.toBytes(\"testTableNotFoundExceptionWithATable\");\n+  public void testTableExistsExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\"testTableExistsExceptionWithATable\");\n     TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n     TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n   }\n \n+  /**\n+   * Can't disable a table if the table isn't in enabled state\n+   * @throws IOException\n+   */\n+  @Test (expected=TableNotEnabledException.class)\n+  public void testTableNotEnabledExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\n+      \"testTableNotEnabledExceptionWithATable\");\n+    TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n+    this.admin.disableTable(name);\n+    this.admin.disableTable(name);\n+  }\n+\n+  /**\n+   * Can't enable a table if the table isn't in disabled state\n+   * @throws IOException\n+   */\n+  @Test (expected=TableNotDisabledException.class)\n+  public void testTableNotDisabledExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\n+      \"testTableNotDisabledExceptionWithATable\");\n+    TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n+    this.admin.enableTable(name);\n+  }\n+\n   /**\n    * For HADOOP-2579\n    * @throws IOException",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "sha": "c6c0d2baabfb3d36d71fa9444f1a79a0eb7cd4fc",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "deletions": 6,
                "filename": "src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "patch": "@@ -104,9 +104,6 @@ public void testTableCreateAndDeleteXML() throws IOException, JAXBException {\n     response = client.put(schemaPath, Constants.MIMETYPE_XML, toXML(model));\n     assertEquals(response.getCode(), 403);\n \n-    // make sure HBase concurs, and wait for the table to come online\n-    admin.enableTable(TABLE1);\n-\n     // retrieve the schema and validate it\n     response = client.get(schemaPath, Constants.MIMETYPE_XML);\n     assertEquals(response.getCode(), 200);\n@@ -145,9 +142,6 @@ public void testTableCreateAndDeletePB() throws IOException, JAXBException {\n       model.createProtobufOutput());\n     assertEquals(response.getCode(), 403);\n \n-    // make sure HBase concurs, and wait for the table to come online\n-    admin.enableTable(TABLE2);\n-\n     // retrieve the schema and validate it\n     response = client.get(schemaPath, Constants.MIMETYPE_PROTOBUF);\n     assertEquals(response.getCode(), 200);",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "sha": "14d24bab53f36b2076dbee7ece579f9199bf57df",
                "status": "modified"
            }
        ],
        "message": "HBASE-4273  java.lang.NullPointerException when a table is being disabled and\n               HMaster restarts (Ming Ma)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1164347 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5af490cb6bed065dee0d0e8c2984f2090919fcdf",
        "patched_files": [
            "ZKTable.java",
            "Admin.java",
            "AssignmentManager.java",
            "DisableTableHandler.java",
            "SchemaResource.java",
            "HMaster.java",
            "TableNotEnabledException.java",
            "CHANGES.java",
            "HBaseAdmin.java",
            "EnableTableHandler.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestAvroServer.java",
            "TestAdmin.java",
            "TestSchemaResource.java",
            "TestAssignmentManager.java"
        ]
    },
    "hbase_fa97600": {
        "bug_id": "hbase_fa97600",
        "commit": "https://github.com/apache/hbase/commit/fa97600ea9142dde236376fd0c0dcea343d91c5f",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hbase/blob/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java?ref=fa97600ea9142dde236376fd0c0dcea343d91c5f",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.net.InetSocketAddress;\n import java.net.Socket;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n \n@@ -35,17 +36,19 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hbase.*;\n+import org.apache.hadoop.hbase.EmptyWatcher;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Threads;\n import org.apache.zookeeper.AsyncCallback;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n-import org.apache.zookeeper.KeeperException.NoNodeException;\n import org.apache.zookeeper.Watcher;\n-import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.ZooKeeper;\n+import org.apache.zookeeper.KeeperException.NoNodeException;\n+import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.data.ACL;\n import org.apache.zookeeper.data.Stat;\n \n@@ -596,11 +599,13 @@ public static int getNumberOfChildren(ZooKeeperWatcher zkw, String znode)\n       ZooKeeperWatcher zkw, String baseNode) throws KeeperException {\n     List<String> nodes =\n       ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);\n-    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();\n-    for (String node: nodes) {\n-      String nodePath = ZKUtil.joinZNode(baseNode, node);\n-      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);\n-      newNodes.add(new NodeAndData(nodePath, data));\n+    List<NodeAndData> newNodes = Collections.emptyList();\n+    if (nodes != null) {\n+      for (String node : nodes) {\n+        String nodePath = ZKUtil.joinZNode(baseNode, node);\n+        byte[] data = ZKUtil.getDataAndWatch(zkw, nodePath);\n+        newNodes.add(new NodeAndData(nodePath, data));\n+      }\n     }\n     return newNodes;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "sha": "ba064668e3941aaf7cff593b0249e8d855e01f50",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java?ref=fa97600ea9142dde236376fd0c0dcea343d91c5f",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "patch": "@@ -342,6 +342,18 @@ public void testCreateSilentIsReallySilent() throws InterruptedException,\n \n     ZKUtil.createAndFailSilent(zk2, aclZnode);\n  }\n+  \n+  @Test\n+  /**\n+   * Test should not fail with NPE when getChildDataAndWatchForNewChildren\n+   * invoked with wrongNode\n+   */\n+  public void testGetChildDataAndWatchForNewChildrenShouldNotThrowNPE()\n+      throws Exception {\n+    ZooKeeperWatcher zkw = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n+        \"testGetChildDataAndWatchForNewChildrenShouldNotThrowNPE\", null);\n+    ZKUtil.getChildDataAndWatchForNewChildren(zkw, \"/wrongNode\");\n+  }\n \n   @org.junit.Rule\n   public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =",
                "raw_url": "https://github.com/apache/hbase/raw/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "sha": "d4f76e9605807d7d531fad4558e1954144558d2a",
                "status": "modified"
            }
        ],
        "message": "HBASE-5722 NPE in ZKUtil#getChildDataAndWatchForNewChildren when ZK not available or NW down.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1310104 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/1140530cc228a6c9b13ba3b4d1653962935739dd",
        "patched_files": [
            "ZKUtil.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestZKUtil.java",
            "TestZooKeeper.java"
        ]
    },
    "hbase_fa98df6": {
        "bug_id": "hbase_fa98df6",
        "commit": "https://github.com/apache/hbase/commit/fa98df639ead80e1d374a134ceb289fa1189114e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/fa98df639ead80e1d374a134ceb289fa1189114e/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java?ref=fa98df639ead80e1d374a134ceb289fa1189114e",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "patch": "@@ -218,11 +218,11 @@ private void connectExistingPeers() throws IOException, KeeperException {\n    */\n   public List<ServerName> getSlavesAddresses(String peerClusterId) {\n     if (this.peerClusters.size() == 0) {\n-      return new ArrayList<ServerName>(0);\n+      return Collections.emptyList();\n     }\n     ReplicationPeer peer = this.peerClusters.get(peerClusterId);\n     if (peer == null) {\n-      return new ArrayList<ServerName>(0);\n+      return Collections.emptyList();\n     }\n     \n     List<ServerName> addresses;\n@@ -281,7 +281,7 @@ private void connectExistingPeers() throws IOException, KeeperException {\n   throws KeeperException {\n     List<String> children = ZKUtil.listChildrenNoWatch(zkw, znode);\n     if(children == null) {\n-      return null;\n+      return Collections.emptyList();\n     }\n     List<ServerName> addresses = new ArrayList<ServerName>(children.size());\n     for (String child : children) {",
                "raw_url": "https://github.com/apache/hbase/raw/fa98df639ead80e1d374a134ceb289fa1189114e/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "sha": "e1a7398119ee60f995e58f98d77a74afddafebff",
                "status": "modified"
            },
            {
                "additions": 118,
                "blob_url": "https://github.com/apache/hbase/blob/fa98df639ead80e1d374a134ceb289fa1189114e/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "changes": 118,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java?ref=fa98df639ead80e1d374a134ceb289fa1189114e",
                "deletions": 0,
                "filename": "src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "patch": "@@ -0,0 +1,118 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.replication;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.MediumTests;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.catalog.CatalogTracker;\n+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.zookeeper.KeeperException;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+@Category(MediumTests.class)\n+public class TestReplicationZookeeper {\n+\n+  private static Configuration conf;\n+\n+  private static HBaseTestingUtility utility;\n+\n+  private static ZooKeeperWatcher zkw;\n+\n+  private static ReplicationZookeeper repZk;\n+\n+  private static String slaveClusterKey;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    utility = new HBaseTestingUtility();\n+    utility.startMiniZKCluster();\n+    conf = utility.getConfiguration();\n+    zkw = HBaseTestingUtility.getZooKeeperWatcher(utility);\n+    DummyServer server = new DummyServer();\n+    repZk = new ReplicationZookeeper(server, new AtomicBoolean());\n+    slaveClusterKey = conf.get(HConstants.ZOOKEEPER_QUORUM) + \":\" +\n+      conf.get(\"hbase.zookeeper.property.clientPort\") + \":/1\";\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility.shutdownMiniZKCluster();\n+  }\n+\n+  @Test\n+  public void testGetAddressesMissingSlave()\n+    throws IOException, KeeperException {\n+    repZk.addPeer(\"1\", slaveClusterKey);\n+    // HBASE-5586 used to get an NPE\n+    assertEquals(0, repZk.getSlavesAddresses(\"1\").size());\n+  }\n+\n+  static class DummyServer implements Server {\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+      return conf;\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeper() {\n+      return zkw;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      return null;\n+    }\n+\n+    @Override\n+    public ServerName getServerName() {\n+      return new ServerName(\"hostname.example.org\", 1234, -1L);\n+    }\n+\n+    @Override\n+    public void abort(String why, Throwable e) {\n+    }\n+\n+    @Override\n+    public boolean isAborted() {\n+      return false;\n+    }\n+\n+    @Override\n+    public void stop(String why) {\n+    }\n+\n+    @Override\n+    public boolean isStopped() {\n+      return false;\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/fa98df639ead80e1d374a134ceb289fa1189114e/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "sha": "553b5cd7d27ef90c08e90b503946e02b9ac70489",
                "status": "added"
            }
        ],
        "message": "   HBASE-5586  [replication] NPE in ReplicationSource when creating a stream\n               to an inexistent cluster\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1303945 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/536ff218258177d923e8771b44fce2420dc4c9e5",
        "patched_files": [
            "ReplicationZookeeper.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationZookeeper.java"
        ]
    },
    "hbase_faefb90": {
        "bug_id": "hbase_faefb90",
        "commit": "https://github.com/apache/hbase/commit/faefb9073f388663df91d0ef2db24b00d6512519",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java?ref=faefb9073f388663df91d0ef2db24b00d6512519",
                "deletions": 7,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "patch": "@@ -211,14 +211,22 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     }\n     List<Path> newDelPaths = compactDelFiles(request, delFilePaths);\n     List<StoreFile> newDelFiles = new ArrayList<StoreFile>();\n-    for (Path newDelPath : newDelPaths) {\n-      StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n-      newDelFiles.add(sf);\n+    List<Path> paths = null;\n+    try {\n+      for (Path newDelPath : newDelPaths) {\n+        StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n+        // pre-create reader of a del file to avoid race condition when opening the reader in each\n+        // partition.\n+        sf.createReader();\n+        newDelFiles.add(sf);\n+      }\n+      LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n+      // compact the mob files by partitions.\n+      paths = compactMobFiles(request, newDelFiles);\n+      LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n+    } finally {\n+      closeStoreFileReaders(newDelFiles);\n     }\n-    LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n-    // compact the mob files by partitions.\n-    List<Path> paths = compactMobFiles(request, newDelFiles);\n-    LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n     // archive the del files if all the mob files are selected.\n     if (request.type == CompactionType.ALL_FILES && !newDelPaths.isEmpty()) {\n       LOG.info(\"After a mob compaction with all files selected, archiving the del files \"\n@@ -336,6 +344,20 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     return newFiles;\n   }\n \n+  /**\n+   * Closes the readers of store files.\n+   * @param storeFiles The store files to be closed.\n+   */\n+  private void closeStoreFileReaders(List<StoreFile> storeFiles) {\n+    for (StoreFile storeFile : storeFiles) {\n+      try {\n+        storeFile.closeReader(true);\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed to close the reader on store file \" + storeFile.getPath(), e);\n+      }\n+    }\n+  }\n+\n   /**\n    * Compacts a partition of selected small mob files and all the del files in a batch.\n    * @param request The compaction request.\n@@ -415,6 +437,7 @@ private void compactMobFilesInBatch(PartitionedMobCompactionRequest request,\n     }\n     // archive the old mob files, do not archive the del files.\n     try {\n+      closeStoreFileReaders(mobFilesToCompact);\n       MobUtils\n         .removeMobFiles(conf, fs, tableName, mobTableDir, column.getName(), mobFilesToCompact);\n     } catch (IOException e) {",
                "raw_url": "https://github.com/apache/hbase/raw/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "sha": "6c2ff01dd84426f3b63d0b4aac8cdc5e48deef12",
                "status": "modified"
            }
        ],
        "message": "HBASE-13855 Race in multi threaded PartitionedMobCompactor causes NPE. (Jingcheng)",
        "parent": "https://github.com/apache/hbase/commit/26893aa451215ef0395b7df16f129414b7b86c86",
        "patched_files": [
            "PartitionedMobCompactor.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestPartitionedMobCompactor.java"
        ]
    },
    "hbase_fbfe3f2": {
        "bug_id": "hbase_fbfe3f2",
        "commit": "https://github.com/apache/hbase/commit/fbfe3f29e5911146361e04f4514689e05257a491",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -77,13 +77,13 @@\n   public static final String HBASE_MASTER_LOADBALANCER_CLASS = \"hbase.master.loadbalancer.class\";\n \n   /** Cluster is standalone or pseudo-distributed */\n-  public static final String CLUSTER_IS_LOCAL = \"false\";\n+  public static final boolean CLUSTER_IS_LOCAL = false;\n \n   /** Cluster is fully-distributed */\n-  public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n+  public static final boolean CLUSTER_IS_DISTRIBUTED = true;\n \n   /** Default value for cluster distributed mode */  \n-  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+  public static final boolean DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n \n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "21ac4bad171c7444b41cf67dc89bcd2aa5c9db4b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "patch": "@@ -436,8 +436,8 @@ public void shutdown() {\n    * @return True if a 'local' address in hbase.master value.\n    */\n   public static boolean isLocal(final Configuration c) {\n-    final String mode = c.get(HConstants.CLUSTER_DISTRIBUTED);\n-    return mode == null || mode.equals(HConstants.CLUSTER_IS_LOCAL);\n+    boolean mode = c.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+    return(mode == HConstants.CLUSTER_IS_LOCAL);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "sha": "9c9c7cc441573ef872caa1fe8a7921a67894b479",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "patch": "@@ -163,9 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n-              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n-            && value.startsWith(HConstants.LOCALHOST)) {\n+        boolean mode = conf.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+        if (mode == HConstants.CLUSTER_IS_DISTRIBUTED && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +\n               \"See \\\"Getting Started\\\" for more information.\";",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "sha": "99c04bdc75734361a057ac2b4114491a1a2b26a6",
                "status": "modified"
            }
        ],
        "message": "HBASE-5638 Readability improvements on HBASE-5633: NPE reading ZK config in HBase (Matteo Bertozzi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1307085 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa",
        "patched_files": [
            "ZKConfig.java",
            "LocalHBaseCluster.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestLocalHBaseCluster.java",
            "TestZKConfig.java"
        ]
    },
    "hbase_fc2157a": {
        "bug_id": "hbase_fc2157a",
        "commit": "https://github.com/apache/hbase/commit/fc2157ae44ed05b36127c5284f171e286ffcc4dd",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -137,6 +137,7 @@ Release 0.19.0 - Unreleased\n                IllegalStateException: Cannot set a region to be closed it it was\n                not already marked as closing, Does not recover if HRS carrying \n                -ROOT- goes down\n+   HBASE-1114  Weird NPEs compacting\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/CHANGES.txt",
                "sha": "fc95740985fea33d83a84ed8717b24631ba1e3e2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "patch": "@@ -696,8 +696,9 @@ private IOException convertThrowableToIOE(final Throwable t,\n   private boolean checkOOME(final Throwable e) {\n     boolean stop = false;\n     if (e instanceof OutOfMemoryError ||\n-        (e.getCause()!= null && e.getCause() instanceof OutOfMemoryError) ||\n-        e.getMessage().contains(\"java.lang.OutOfMemoryError\")) {\n+      (e.getCause() != null && e.getCause() instanceof OutOfMemoryError) ||\n+      (e.getMessage() != null &&\n+        e.getMessage().contains(\"java.lang.OutOfMemoryError\"))) {\n       LOG.fatal(\"OutOfMemoryError, aborting.\", e);\n       abort();\n       stop = true;",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "sha": "a3fc106846b04e500dbece1c45bfc0fbc0ef0ade",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "patch": "@@ -866,8 +866,10 @@ StoreSize compact(final boolean majorCompaction) throws IOException {\n           return null;\n         }\n         int len = 0;\n-        for (FileStatus fstatus:fs.listStatus(path)) {\n-          len += fstatus.getLen();\n+        // listStatus can come back null.\n+        FileStatus [] fss = this.fs.listStatus(path);\n+        for (int ii = 0; fss != null && i < fss.length; ii++) {\n+          len += fss[ii].getLen();\n         }\n         fileSizes[i] = len;\n         totalSize += len;",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "sha": "b56df68a801367ef0e9390d3a0cd3dbb6a80b9c1",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/webapps/master/WEB-INF/web.xml",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/webapps/master/WEB-INF/web.xml?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "deletions": 8,
                "filename": "src/webapps/master/WEB-INF/web.xml",
                "patch": "@@ -15,13 +15,13 @@ Automatically created by Tomcat JspC.\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n     </servlet>\n \n     <servlet-mapping>\n@@ -30,13 +30,13 @@ Automatically created by Tomcat JspC.\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <url-pattern>/table.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <url-pattern>/regionhistorian.jsp</url-pattern>\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <url-pattern>/regionhistorian.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <url-pattern>/table.jsp</url-pattern>\n     </servlet-mapping>\n \n </web-app>",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/webapps/master/WEB-INF/web.xml",
                "sha": "2c7766d2c19e29ef17860fad6886f058f939cba8",
                "status": "modified"
            }
        ],
        "message": "HBASE-1114 Weird NPEs compacting\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@733213 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7acb2ad4402259c1b3467153c620c53edd520e22",
        "patched_files": [
            "HStore.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_fd55431": {
        "bug_id": "hbase_fd55431",
        "commit": "https://github.com/apache/hbase/commit/fd5543190c43feb8ede2056338ccf4622e94c99a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -35,6 +35,7 @@ Release 0.19.0 - Unreleased\n                they are using HTable\n                With J-D's one line patch, test cases now appear to work and\n                PerformanceEvaluation works as before.\n+   HBASE-939   NPE in HStoreKey\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/CHANGES.txt",
                "sha": "4e027bcade37594216abf2eaab48c55c18a163a1",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "patch": "@@ -351,6 +351,11 @@ public int compareTo(Object o) {\n   \n   static int compareTo(final HRegionInfo hri, final HStoreKey left,\n       final HStoreKey right) {\n+    // We can be passed null\n+    if (left == null && right == null) return 0;\n+    if (left == null) return -1;\n+    if (right == null) return 1;\n+    \n     int result = compareTwoRowKeys(hri, left.getRow(), right.getRow());\n     if (result != 0) {\n       return result;",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "sha": "91dfde7a4fa8d23bc4932879564e8a5a24cd41a0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompare.java?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/hbase/TestCompare.java",
                "patch": "@@ -51,6 +51,11 @@ public void testHStoreKey() {\n     nocolumn = new HStoreKey(a, HConstants.LATEST_TIMESTAMP);\n     withcolumn = new HStoreKey(a, a, timestamp);\n     assertTrue(nocolumn.compareTo(withcolumn) < 0);\n+    // Test null keys.\n+    HStoreKey normal = new HStoreKey(\"a\", \"b\");\n+    assertTrue(normal.compareTo(null) > 0);\n+    assertTrue(HStoreKey.compareTo(null, null, null) == 0);\n+    assertTrue(HStoreKey.compareTo(null, null, normal) < 0);\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "sha": "b33317624f35fba48c78d33dab80c7e8e34b46b8",
                "status": "modified"
            }
        ],
        "message": "HBASE-939 NPE in HStoreKey\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@706348 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5e3815484cde31fd975423b6b1d423198dadf90d",
        "patched_files": [
            "CHANGES.java",
            "HStoreKey.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestCompare.java"
        ]
    },
    "hbase_ffe4302": {
        "bug_id": "hbase_ffe4302",
        "commit": "https://github.com/apache/hbase/commit/ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
                "deletions": 0,
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -1189,6 +1189,7 @@ void updateValueSize(final int valueSize) {\n     }\n \n     void updateScanMetrics(final ScanMetrics metrics) {\n+      if (metrics == null) return;\n       Map<String,Long> metricsMap = metrics.getMetricsMap();\n       Long rpcCalls = metricsMap.get(ScanMetrics.RPC_CALLS_METRIC_NAME);\n       if (rpcCalls != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "1a72ececfcae0344534d92990be57657566fa6f6",
                "status": "modified"
            }
        ],
        "message": "HBASE-20785 NPE getting metrics in PE testing scans",
        "parent": "https://github.com/apache/hbase/commit/952bb96c8a5c1f8a34237cab970ac41101bdd870",
        "patched_files": [
            "PerformanceEvaluation.java"
        ],
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    }
}