{
    "incubator-gobblin_032b9c2": {
        "bug_id": "incubator-gobblin_032b9c2",
        "commit": "https://github.com/apache/incubator-gobblin/commit/032b9c2c4d43f2cb06dd54f55d51d000a749515e",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/032b9c2c4d43f2cb06dd54f55d51d000a749515e/scheduler/src/main/java/com/linkedin/uif/scheduler/Task.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/scheduler/src/main/java/com/linkedin/uif/scheduler/Task.java?ref=032b9c2c4d43f2cb06dd54f55d51d000a749515e",
                "deletions": 2,
                "filename": "scheduler/src/main/java/com/linkedin/uif/scheduler/Task.java",
                "patch": "@@ -89,11 +89,21 @@ public void run() {\n         try {\n             // Build the extractor for pulling source schema and data records\n             extractor = this.taskContext.getSource().getExtractor(this.taskState);\n+            if (extractor == null) {\n+                LOG.error(\"No extractor created for task \" + this.taskId);\n+                return;\n+            }\n+\n+            // Original source schema\n+            Object sourceSchema = extractor.getSchema();\n+            if (sourceSchema == null) {\n+                LOG.error(\"No source schema extracted for task \" + this.taskId);\n+                return;\n+            }\n+\n             // If conversion is needed on the source schema and data records\n             // before they are passed to the writer\n             boolean doConversion = !this.taskContext.getConverters().isEmpty();\n-            // Original source schema\n-            Object sourceSchema = extractor.getSchema();\n             Converter converter = null;\n             // (Possibly converted) source schema ready for the writer\n             Object schemaForWriter = sourceSchema;\n@@ -245,6 +255,13 @@ public TaskState getTaskState() {\n      * Update record-level metrics.\n      */\n     public void updateRecordMetrics() {\n+        if (this.writer == null) {\n+            LOG.error(String.format(\n+                    \"Could not update record metrics for task %s: writer was not built\",\n+                    this.taskId));\n+            return;\n+        }\n+\n         this.taskState.updateRecordMetrics(this.writer.recordsWritten());\n     }\n \n@@ -256,6 +273,13 @@ public void updateRecordMetrics() {\n      * </p>\n      */\n     public void updateByteMetrics() throws IOException {\n+        if (this.writer == null) {\n+            LOG.error(String.format(\n+                    \"Could not update byte metrics for task %s: writer was not built\",\n+                    this.taskId));\n+            return;\n+        }\n+\n         this.taskState.updateByteMetrics(this.writer.bytesWritten());\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/032b9c2c4d43f2cb06dd54f55d51d000a749515e/scheduler/src/main/java/com/linkedin/uif/scheduler/Task.java",
                "sha": "40b6a39cf2748eba520dc65b90489302e2a6620b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/032b9c2c4d43f2cb06dd54f55d51d000a749515e/test/src/main/java/com/linkedin/uif/test/TestExtractor.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/test/src/main/java/com/linkedin/uif/test/TestExtractor.java?ref=032b9c2c4d43f2cb06dd54f55d51d000a749515e",
                "deletions": 3,
                "filename": "test/src/main/java/com/linkedin/uif/test/TestExtractor.java",
                "patch": "@@ -8,8 +8,8 @@\n import org.apache.avro.generic.GenericDatumReader;\n import org.apache.avro.generic.GenericRecord;\n import org.apache.avro.io.DatumReader;\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import com.linkedin.uif.configuration.WorkUnitState;\n import com.linkedin.uif.source.extractor.Extractor;\n@@ -21,7 +21,7 @@\n  */\n public class TestExtractor implements Extractor<String, String> {\n \n-    private static final Log LOG = LogFactory.getLog(TestExtractor.class);\n+    private static final Logger LOG = LoggerFactory.getLogger(TestExtractor.class);\n \n     private static final String SOURCE_FILE_KEY = \"source.file\";\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/032b9c2c4d43f2cb06dd54f55d51d000a749515e/test/src/main/java/com/linkedin/uif/test/TestExtractor.java",
                "sha": "e8945d5278af613704e41af66055b3b450ff1db8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/032b9c2c4d43f2cb06dd54f55d51d000a749515e/writer/build.gradle",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/writer/build.gradle?ref=032b9c2c4d43f2cb06dd54f55d51d000a749515e",
                "deletions": 0,
                "filename": "writer/build.gradle",
                "patch": "@@ -12,6 +12,7 @@ dependencies {\n   compile spec.external.gson\n   compile spec.external.jacksonCore\n   compile spec.external.jacksonMapper\n+  compile spec.external.slf4j\n \n   testCompile spec.external.testng\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/032b9c2c4d43f2cb06dd54f55d51d000a749515e/writer/build.gradle",
                "sha": "1ae336785807676cff91e1f16d616dcc13ae40a8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/032b9c2c4d43f2cb06dd54f55d51d000a749515e/writer/src/main/java/com/linkedin/uif/writer/AvroHdfsDataWriter.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/writer/src/main/java/com/linkedin/uif/writer/AvroHdfsDataWriter.java?ref=032b9c2c4d43f2cb06dd54f55d51d000a749515e",
                "deletions": 3,
                "filename": "writer/src/main/java/com/linkedin/uif/writer/AvroHdfsDataWriter.java",
                "patch": "@@ -4,12 +4,13 @@\n import java.net.URI;\n import java.util.concurrent.atomic.AtomicLong;\n \n+import com.google.common.base.Preconditions;\n import org.apache.avro.Schema;\n import org.apache.avro.file.DataFileWriter;\n import org.apache.avro.generic.GenericDatumWriter;\n import org.apache.avro.generic.GenericRecord;\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n@@ -28,7 +29,7 @@\n  */\n class AvroHdfsDataWriter<S> implements DataWriter<S, GenericRecord> {\n \n-    private static final Log LOG = LogFactory.getLog(AvroHdfsDataWriter.class);\n+    private static final Logger LOG = LoggerFactory.getLogger(AvroHdfsDataWriter.class);\n \n     private final FileSystem fs;\n     private final Path stagingFile;\n@@ -53,6 +54,8 @@ public AvroHdfsDataWriter(URI uri, String stagingDir, String outputDir,\n \n     @Override\n     public void write(S sourceRecord) throws IOException {\n+        Preconditions.checkNotNull(sourceRecord);\n+\n         try {\n             this.writer.append(this.dataConverter.convert(sourceRecord));\n         } catch (DataConversionException e) {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/032b9c2c4d43f2cb06dd54f55d51d000a749515e/writer/src/main/java/com/linkedin/uif/writer/AvroHdfsDataWriter.java",
                "sha": "7123ec29fea6ea173974dedff9969132943f6478",
                "status": "modified"
            }
        ],
        "message": "Fixed a bug in Task and added more defense against NPE\n\nSigned-off-by: Yinan Li <ynli@linkedin.com>\n\nRB=278948\nR=nveeramr,kgoodhop,lqiao,stakiar\nA=kgoodhop",
        "parent": "https://github.com/apache/incubator-gobblin/commit/704885afbe2e9d3a45b711d9e106d76a43e1f6a1",
        "patched_files": [
            "Task.java",
            "build.gradle",
            "Extractor.java",
            "AvroHdfsDataWriter.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "AvroHdfsDataWriterTest.java",
            "TestExtractor.java"
        ]
    },
    "incubator-gobblin_046cbca": {
        "bug_id": "incubator-gobblin_046cbca",
        "commit": "https://github.com/apache/incubator-gobblin/commit/046cbca3ecafb2314a095ca9c542ac65f282f07e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java?ref=046cbca3ecafb2314a095ca9c542ac65f282f07e",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "patch": "@@ -137,8 +137,9 @@ public String explain() {\n     /**\n      * @return a unique string identifier for this {@link DatasetAndPartition}.\n      */\n+    @SuppressWarnings(\"deprecation\")\n     public String identifier() {\n-      return Hex.encodeHexString(DigestUtils.sha1(this.dataset.toString() + this.partition));\n+      return Hex.encodeHexString(DigestUtils.sha(this.dataset.toString() + this.partition));\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "sha": "e41c312ca5986c7174c7fdacf9a58cfa528daf78",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java?ref=046cbca3ecafb2314a095ca9c542ac65f282f07e",
                "deletions": 9,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "patch": "@@ -107,12 +107,12 @@\n    * If the predicate returns true, the partition will be skipped. */\n   public static final String FAST_PARTITION_SKIP_PREDICATE =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.fast.partition.skip.predicate\";\n-  \n+\n   /** A predicate applied to non partition table before any file listing.\n    * If the predicate returns true, the table will be skipped. */\n   public static final String FAST_TABLE_SKIP_PREDICATE =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.fast.table.skip.predicate\";\n-  \n+\n   /** Method for deleting files on deregister. One of {@link DeregisterFileDeleteMethod}. */\n   public static final String DELETE_FILES_ON_DEREGISTER =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.deregister.fileDeleteMethod\";\n@@ -157,7 +157,7 @@\n   private final Optional<String> partitionFilter;\n   private final Optional<Predicate<PartitionCopy>> fastPartitionSkip;\n   private final Optional<Predicate<HiveCopyEntityHelper>> fastTableSkip;\n-  \n+\n   private final DeregisterFileDeleteMethod deleteMethod;\n \n   private final Optional<CommitStep> tableRegistrationStep;\n@@ -226,6 +226,7 @@\n       this.configuration = configuration;\n       this.targetFs = targetFs;\n \n+      this.targetPathHelper = new HiveTargetPathHelper(this.dataset);\n       this.hiveRegProps = new HiveRegProps(new State(this.dataset.getProperties()));\n       this.targetURI = Optional.fromNullable(this.dataset.getProperties().getProperty(TARGET_METASTORE_URI_KEY));\n       this.targetClientPool = HiveMetastoreClientPool.get(this.dataset.getProperties(), this.targetURI);\n@@ -305,15 +306,16 @@\n         if (HiveUtils.isPartitioned(this.dataset.table)) {\n           this.sourcePartitions = HiveUtils.getPartitionsMap(multiClient.getClient(source_client), this.dataset.table,\n               this.partitionFilter);\n+          // Note: this must be mutable, so we copy the map\n           this.targetPartitions =\n-              this.existingTargetTable.isPresent() ? HiveUtils.getPartitionsMap(multiClient.getClient(target_client),\n-                  this.existingTargetTable.get(), this.partitionFilter) : Maps.<List<String>, Partition> newHashMap();\n+              this.existingTargetTable.isPresent() ? Maps.newHashMap(\n+                  HiveUtils.getPartitionsMap(multiClient.getClient(target_client),\n+                      this.existingTargetTable.get(), this.partitionFilter)) : Maps.<List<String>, Partition> newHashMap();\n         } else {\n           this.sourcePartitions = Maps.newHashMap();\n           this.targetPartitions = Maps.newHashMap();\n         }\n \n-        this.targetPathHelper = new HiveTargetPathHelper(this.dataset);\n       } catch (TException te) {\n         closer.close();\n         throw new IOException(\"Failed to generate work units for table \" + dataset.table.getCompleteName(), te);\n@@ -653,7 +655,7 @@ private int addSharedSteps(List<CopyEntity> copyEntities, String fileSet, int in\n         HiveLocationDescriptor.forTable(this.dataset.table, this.dataset.fs, this.dataset.getProperties());\n     HiveLocationDescriptor desiredTargetLocation =\n         HiveLocationDescriptor.forTable(this.targetTable, this.targetFs, this.dataset.getProperties());\n-    \n+\n     Optional<HiveLocationDescriptor> existingTargetLocation = this.existingTargetTable.isPresent() ? Optional.of(\n         HiveLocationDescriptor.forTable(this.existingTargetTable.get(), this.targetFs, this.dataset.getProperties()))\n         : Optional.<HiveLocationDescriptor> absent();\n@@ -663,12 +665,12 @@ private int addSharedSteps(List<CopyEntity> copyEntities, String fileSet, int in\n       multiTimer.close();\n       return Lists.newArrayList();\n     }\n-    \n+\n     DiffPathSet diffPathSet = fullPathDiff(sourceLocation, desiredTargetLocation, existingTargetLocation,\n         Optional.<Partition> absent(), multiTimer, this);\n \n     multiTimer.nextStage(Stages.FULL_PATH_DIFF);\n-    \n+\n     // Could used to delete files for the existing snapshot\n     DeleteFileCommitStep deleteStep =\n         DeleteFileCommitStep.fromPaths(this.targetFs, diffPathSet.pathsToDelete, this.dataset.getProperties());",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "sha": "4547faa382944345152ff8f70ae51c7d262d9a03",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java?ref=046cbca3ecafb2314a095ca9c542ac65f282f07e",
                "deletions": 1,
                "filename": "gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "patch": "@@ -182,8 +182,10 @@ public String toString() {\n     return Hex.encodeHexString(this.sha);\n   }\n \n+  // DigestUtils.sha is deprecated for sha1, but sha1 is not available in old versions of commons codec\n+  @SuppressWarnings(\"deprecation\")\n   private static byte[] computeGuid(byte[] bytes) {\n-    return DigestUtils.sha1(bytes);\n+    return DigestUtils.sha(bytes);\n   }\n \n   static class SimpleHasGuid implements HasGuid {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/046cbca3ecafb2314a095ca9c542ac65f282f07e/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "sha": "8b279933c4fdf89bc5338baec60a3dc481ed8d62",
                "status": "modified"
            }
        ],
        "message": "Fix NPE and mutability of a map in hive target path helper and copy helper.",
        "parent": "https://github.com/apache/incubator-gobblin/commit/25b8059436d0bf07921976a62dc50970c88c5b6e",
        "patched_files": [
            "HiveCopyEntityHelper.java",
            "Guid.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "GuidTest.java",
            "HiveCopyEntityHelperTest.java"
        ]
    },
    "incubator-gobblin_056888e": {
        "bug_id": "incubator-gobblin_056888e",
        "commit": "https://github.com/apache/incubator-gobblin/commit/056888e0248ab75f9d3a789c3d14622a9a426bb6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyEntity.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyEntity.java?ref=056888e0248ab75f9d3a789c3d14622a9a426bb6",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyEntity.java",
                "patch": "@@ -118,7 +118,7 @@ public static CopyEntity deserialize(String serialized) {\n    */\n   public static String getSerializedWithNewPackage(String serialized) {\n     serialized = serialized.replace(\"\\\"gobblin.data.management.\", \"\\\"org.apache.gobblin.data.management.\");\n-    log.info(\"Serialized updated copy entity: \" + serialized);\n+    log.debug(\"Serialized updated copy entity: \" + serialized);\n     return serialized;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyEntity.java",
                "sha": "cd4b97e9d953fe87bf268558d012128832fdb74c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyableDatasetMetadata.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyableDatasetMetadata.java?ref=056888e0248ab75f9d3a789c3d14622a9a426bb6",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyableDatasetMetadata.java",
                "patch": "@@ -69,7 +69,7 @@ public static CopyableDatasetMetadata deserialize(String serialized) {\n    */\n   private static String getSerializedWithNewPackage(String serialized) {\n     serialized = serialized.replace(\"\\\"gobblin.data.management.\", \"\\\"org.apache.gobblin.data.management.\");\n-    log.info(\"Serialized updated copy entity: \" + serialized);\n+    log.debug(\"Serialized updated copy entity: \" + serialized);\n     return serialized;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/CopyableDatasetMetadata.java",
                "sha": "45b71ab8a7bb59384dfbfb9492a739204cde106e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/publisher/CopyDataPublisher.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/publisher/CopyDataPublisher.java?ref=056888e0248ab75f9d3a789c3d14622a9a426bb6",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/publisher/CopyDataPublisher.java",
                "patch": "@@ -209,7 +209,9 @@ private void publishFileSet(CopyEntity.DatasetAndPartition datasetAndPartition,\n           // Dataset Output path is injected in each copyableFile.\n           // This can be optimized by having a dataset level equivalent class for copyable entities\n           // and storing dataset related information, e.g. dataset output path, there.\n-          if (!fileSetRoot.isPresent()) {\n+\n+          // Currently datasetOutputPath is only present for hive datasets.\n+          if (!fileSetRoot.isPresent() && copyableFile.getDatasetOutputPath() != null) {\n             fileSetRoot = Optional.of(copyableFile.getDatasetOutputPath());\n           }\n         }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/056888e0248ab75f9d3a789c3d14622a9a426bb6/gobblin-data-management/src/main/java/org/apache/gobblin/data/management/copy/publisher/CopyDataPublisher.java",
                "sha": "e443271fe98dbb1fb79342457d1a9f7b6b036450",
                "status": "modified"
            }
        ],
        "message": "[GOBBLIN-286] gix bug where non hive dataset publishing gives NPE\n\nCloses #2148 from\narjun4084346/fixBugNonHiveDataset",
        "parent": "https://github.com/apache/incubator-gobblin/commit/68456c620c3a759d47bab4dff43d7b5ad6e46fdd",
        "patched_files": [
            "CopyableDatasetMetadata.java",
            "CopyDataPublisher.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "CopyableDatasetMetadataTest.java",
            "CopyDataPublisherTest.java"
        ]
    },
    "incubator-gobblin_1d2fd4c": {
        "bug_id": "incubator-gobblin_1d2fd4c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java?ref=1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
                "deletions": 13,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "patch": "@@ -15,6 +15,7 @@\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n \n+import lombok.extern.slf4j.Slf4j;\n import org.apache.avro.Schema;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -28,6 +29,7 @@\n \n import gobblin.configuration.ConfigurationKeys;\n import gobblin.configuration.State;\n+import gobblin.data.management.conversion.hive.util.HiveAvroORCQueryUtils;\n import gobblin.hive.avro.HiveAvroSerDeManager;\n import gobblin.util.AvroUtils;\n import gobblin.util.HadoopUtils;\n@@ -55,6 +57,7 @@\n  * If multiple {@link Partition}s have the same {@link Schema} a duplicate schema file in not created. Already existing\n  * {@link Schema} url for this {@link Schema} is used.\n  */\n+@Slf4j\n public class AvroSchemaManager {\n \n   private static final String HIVE_SCHEMA_TEMP_DIR_PATH_KEY = \"hive.schema.dir\";\n@@ -65,7 +68,6 @@\n    * A mapping of {@link Schema} hash to its {@link Path} on {@link FileSystem}\n    */\n   private final Map<String, Path> schemaPaths;\n-  private final Schema.Parser schemaParser;\n \n   /**\n    * A temporary directory to hold all Schema files. The path is job id specific.\n@@ -76,7 +78,6 @@\n   public AvroSchemaManager(FileSystem fs, State state) {\n     this.fs = fs;\n     this.schemaPaths = Maps.newHashMap();\n-    this.schemaParser = new Schema.Parser();\n     this.schemaDir = new Path(state.getProp(HIVE_SCHEMA_TEMP_DIR_PATH_KEY, DEFAULT_HIVE_SCHEMA_TEMP_DIR_PATH_KEY),\n             state.getProp(ConfigurationKeys.JOB_ID_KEY));\n   }\n@@ -88,13 +89,13 @@ public AvroSchemaManager(FileSystem fs, State state) {\n    * @return a {@link Path} to table's avro {@link Schema} file.\n    */\n   public Path getSchemaUrl(Table table) throws IOException {\n-    return getSchemaUrl(table.getSd());\n+    return getSchemaUrl(table.getTTable().getSd());\n   }\n \n   /**\n    * Get the url to <code>partition</code>'s avro {@link Schema} file.\n    *\n-   * @param table whose avro schema is to be returned\n+   * @param partition whose avro schema is to be returned\n    * @return a {@link Path} to table's avro {@link Schema} file.\n    */\n   public Path getSchemaUrl(Partition partition) throws IOException {\n@@ -115,16 +116,24 @@ public static Schema getSchemaFromUrl(Path schemaUrl, FileSystem fs) throws IOEx\n   private Path getSchemaUrl(StorageDescriptor sd) throws IOException {\n     if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_URL)) {\n       return new Path(sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_URL));\n-    } else if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_LITERAL)) {\n-\n-      Schema schema =\n-          this.schemaParser.parse(sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_LITERAL));\n-      return getOrGenerateSchemaFile(schema);\n-\n-    } else {\n-      Schema schema = AvroUtils.getDirectorySchema(new Path(sd.getLocation()), this.fs, true);\n-      return getOrGenerateSchemaFile(schema);\n     }\n+    String schemaLiteral = null;\n+    try {\n+      if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_LITERAL)) {\n+\n+        schemaLiteral = sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_LITERAL);\n+        log.debug(\"Schema literal is: \" + schemaLiteral);\n+        Schema schema = HiveAvroORCQueryUtils.readSchemaFromString(schemaLiteral);\n+\n+        return getOrGenerateSchemaFile(schema);\n+      }\n+    } catch (Exception e) {\n+      log.error(String.format(\"Failed to parse schema from schema literal. Falling back to HDFS schema: %s\",\n+          schemaLiteral), e);\n+    }\n+\n+    Schema schema = AvroUtils.getDirectorySchema(new Path(sd.getLocation()), this.fs, true);\n+    return getOrGenerateSchemaFile(schema);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "sha": "7f6b88bd45c836e0512605d97d01992353e83309",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java?ref=1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
                "deletions": 57,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "patch": "@@ -11,18 +11,19 @@\n  */\n package gobblin.data.management.conversion.hive.converter;\n \n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n import lombok.extern.slf4j.Slf4j;\n \n import org.apache.avro.Schema;\n import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n \n import com.google.common.base.Optional;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Splitter;\n+import com.google.common.collect.Maps;\n \n import gobblin.configuration.WorkUnitState;\n import gobblin.converter.Converter;\n@@ -33,6 +34,7 @@\n import gobblin.data.management.conversion.hive.util.HiveAvroORCQueryUtils;\n import gobblin.util.AvroFlattener;\n \n+\n /**\n  * Builds the Hive avro to Orc conversion query. The record type for this converter is {@link QueryBasedHiveConversionEntity}. A {@link QueryBasedHiveConversionEntity}\n  * can be a hive table or a hive partition.\n@@ -43,8 +45,9 @@\n \n   // TODO: Remove when topology is enabled\n   private static final String ORC_TABLE_ALTERNATE_LOCATION = \"orc.table.alternate.location\";\n+  private static final String ORC_TABLE_ALTERNATE_DATABASE = \"orc.table.alternate.database\";\n \n-  private static AvroFlattener avroFlattener = new AvroFlattener();\n+  private static AvroFlattener AVRO_FLATTENER = new AvroFlattener();\n \n   @Override\n   public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws SchemaConversionException {\n@@ -57,12 +60,12 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n   @Override\n   public Iterable<QueryBasedHiveConversionEntity> convertRecord(Schema outputSchema,\n       QueryBasedHiveConversionEntity conversionEntity, WorkUnitState workUnit) throws DataConversionException {\n-    Preconditions.checkNotNull(outputSchema);\n-    Preconditions.checkNotNull(conversionEntity);\n-    Preconditions.checkNotNull(workUnit);\n-    Preconditions.checkNotNull(conversionEntity.getHiveTable());\n+    Preconditions.checkNotNull(outputSchema, \"Output schema must not be null\");\n+    Preconditions.checkNotNull(conversionEntity, \"Conversion entity must not be null\");\n+    Preconditions.checkNotNull(workUnit, \"Workunit state must not be null\");\n+    Preconditions.checkNotNull(conversionEntity.getHiveTable(), \"Hive table within conversion entity must not be null\");\n \n-    Schema flattenedSchema = avroFlattener.flatten(outputSchema, true);\n+    Schema flattenedSchema = AVRO_FLATTENER.flatten(outputSchema, true);\n \n     // Create flattened table if not exists\n     // ORC Hive tables are named as   : {avro_table_name}_orc\n@@ -78,28 +81,75 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n             : conversionEntity.getHiveTable().getSd().getLocation();\n \n     // ORC table name and location\n+    // TODO: Define naming convention and pull it from config / topology\n     String orcTableName = avroTableName + \"_orc\";\n-    String orcTableLocation;\n+    String orcTableDatabase = getOrcTableDatabase(workUnit, conversionEntity);\n+    String orcDataLocation = getOrcDataLocation(workUnit, avroDataLocation, orcTableName);\n+\n+    // Populate optional partition info\n+    Map<String, String> partitionsDDLInfo = Maps.newHashMap();\n+    Map<String, String> partitionsDMLInfo = Maps.newHashMap();\n+    populatePartitionInfo(conversionEntity, partitionsDDLInfo, partitionsDMLInfo);\n+\n+    // Create DDL statement\n+    String createFlattenedTableDDL = HiveAvroORCQueryUtils\n+        .generateCreateTableDDL(flattenedSchema,\n+            orcTableName,\n+            orcDataLocation,\n+            Optional.of(orcTableDatabase),\n+            Optional.of(partitionsDDLInfo),\n+            Optional.<List<String>>absent(),\n+            Optional.<Map<String, HiveAvroORCQueryUtils.COLUMN_SORT_ORDER>>absent(),\n+            Optional.<Integer>absent(),\n+            Optional.<String>absent(),\n+            Optional.<String>absent(),\n+            Optional.<String>absent(),\n+            Optional.<Map<String, String>>absent());\n+    conversionEntity.getQueries().add(createFlattenedTableDDL);\n+    log.info(\"Create DDL: \" + createFlattenedTableDDL);\n+\n+    // Create DML statement\n+    String insertInORCTableDML = HiveAvroORCQueryUtils\n+        .generateTableMappingDML(outputSchema, flattenedSchema, avroTableName, orcTableName,\n+            Optional.of(conversionEntity.getHiveTable().getDbName()),\n+            Optional.of(orcTableDatabase),\n+            Optional.of(partitionsDMLInfo),\n+            Optional.<Boolean>absent(), Optional.<Boolean>absent());\n+    conversionEntity.getQueries().add(insertInORCTableDML);\n+    log.info(\"Conversion DML: \" + insertInORCTableDML);\n+\n+    log.info(\"Conversion Query \" + conversionEntity.getQueries());\n+    return new SingleRecordIterable<>(conversionEntity);\n+  }\n+\n+  private String getOrcTableDatabase(WorkUnitState workUnit, QueryBasedHiveConversionEntity conversionEntity) {\n+    String orcTableAlternateDB = workUnit.getJobState().getProp(ORC_TABLE_ALTERNATE_DATABASE);\n+    return StringUtils.isNotBlank(orcTableAlternateDB) ? orcTableAlternateDB :\n+        conversionEntity.getHiveTable().getDbName();\n+  }\n+\n+  private String getOrcDataLocation(WorkUnitState workUnit, String avroDataLocation, String orcTableName) {\n+    String orcDataLocation;\n \n     // By default ORC table creates a new directory where Avro data resides with _orc postfix, but this can be\n     // .. overridden by specifying this property\n     String orcTableAlternateLocation = workUnit.getJobState().getProp(ORC_TABLE_ALTERNATE_LOCATION);\n     if (StringUtils.isNotBlank(orcTableAlternateLocation)) {\n-      orcTableLocation = orcTableAlternateLocation.endsWith(\"/\") ?\n-           orcTableAlternateLocation + orcTableName : orcTableAlternateLocation + \"/\" + orcTableName;\n+      orcDataLocation = StringUtils.removeEnd(orcTableAlternateLocation, Path.SEPARATOR) + \"/\" + orcTableName;\n     } else {\n-      orcTableLocation = (avroDataLocation.endsWith(\"/\") ?\n-          avroDataLocation.substring(0, avroDataLocation.length() - 1) : avroDataLocation) + \"_orc\";\n+      orcDataLocation = StringUtils.removeEnd(avroDataLocation, Path.SEPARATOR) + \"_orc\";\n     }\n \n     // Each job execution further writes to a sub-directory within ORC data directory to support stagin use-case\n     // .. ie for atomic swap\n-    orcTableLocation += \"/\" + workUnit.getJobState().getId();\n+    orcDataLocation += \"/\" + workUnit.getJobState().getId();\n \n-    // Populate optional partition info\n-    Optional<Map<String, String>> optionalPartitionsDDLInfo = Optional.<Map<String, String>>absent();\n-    Optional<Map<String, String>> optionalPartitionsDMLInfo = Optional.<Map<String, String>>absent();\n+    return orcDataLocation;\n+  }\n \n+  private void populatePartitionInfo(QueryBasedHiveConversionEntity conversionEntity,\n+      Map<String, String> partitionsDDLInfo,\n+      Map<String, String> partitionsDMLInfo) {\n     String partitionsInfoString = null;\n     String partitionsTypeString = null;\n \n@@ -112,8 +162,6 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n       if (StringUtils.isBlank(partitionsInfoString) || StringUtils.isBlank(partitionsTypeString)) {\n         throw new IllegalArgumentException(\"Both partitions info and partions must be present, if one is specified\");\n       }\n-      Map<String, String> partitionDDLInfo = new HashMap<>();\n-      Map<String, String> partitionDMLInfo = new HashMap<>();\n       List<String> pInfo = Splitter.on(\",\").omitEmptyStrings().trimResults().splitToList(partitionsInfoString);\n       List<String> pType = Splitter.on(\",\").omitEmptyStrings().trimResults().splitToList(partitionsTypeString);\n       if (pInfo.size() != pType.size()) {\n@@ -127,46 +175,9 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n               String.format(\"Partition details should be of the format partitionName=partitionValue. Recieved: %s\",\n                   pInfo.get(i)));\n         }\n-        partitionDDLInfo.put(partitionInfoParts.get(0), partitionType);\n-        partitionDMLInfo.put(partitionInfoParts.get(0), partitionInfoParts.get(1));\n-      }\n-      if (partitionDDLInfo.size() > 0) {\n-        optionalPartitionsDDLInfo = Optional.of(partitionDDLInfo);\n-      }\n-      if (partitionDMLInfo.size() > 0) {\n-        optionalPartitionsDMLInfo = Optional.of(partitionDMLInfo);\n+        partitionsDDLInfo.put(partitionInfoParts.get(0), partitionType);\n+        partitionsDMLInfo.put(partitionInfoParts.get(0), partitionInfoParts.get(1));\n       }\n     }\n-\n-    // Create DDL statement\n-    String createFlattenedTableDDL = HiveAvroORCQueryUtils\n-        .generateCreateTableDDL(flattenedSchema,\n-            orcTableName,\n-            orcTableLocation,\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            optionalPartitionsDDLInfo,\n-            Optional.<List<String>>absent(),\n-            Optional.<Map<String, HiveAvroORCQueryUtils.COLUMN_SORT_ORDER>>absent(),\n-            Optional.<Integer>absent(),\n-            Optional.<String>absent(),\n-            Optional.<String>absent(),\n-            Optional.<String>absent(),\n-            Optional.<Map<String, String>>absent());\n-    conversionEntity.getQueries().add(createFlattenedTableDDL);\n-    log.info(\"Create DDL: \" + createFlattenedTableDDL);\n-\n-    // Create DML statement\n-    String insertInORCTableDML = HiveAvroORCQueryUtils\n-        .generateTableMappingDML(outputSchema, flattenedSchema, avroTableName, orcTableName,\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            optionalPartitionsDMLInfo,\n-            Optional.<Boolean>absent(), Optional.<Boolean>absent());\n-    conversionEntity.getQueries().add(insertInORCTableDML);\n-    log.info(\"Conversion DML: \" + insertInORCTableDML);\n-\n-    log.info(\"Conversion Query \" + conversionEntity.getQueries());\n-    return new SingleRecordIterable<>(conversionEntity);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "sha": "29c35339bd992593028f8b2584849d1adfb1d2fe",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java?ref=1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
                "deletions": 0,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "patch": "@@ -16,6 +16,7 @@\n import lombok.EqualsAndHashCode;\n import lombok.Getter;\n import lombok.ToString;\n+import lombok.extern.slf4j.Slf4j;\n \n import com.google.common.base.Optional;\n import com.google.common.collect.Lists;\n@@ -29,6 +30,7 @@\n import gobblin.hive.HiveTable;\n import gobblin.source.extractor.Extractor;\n \n+\n /**\n  * Represents a gobblin Record in the Hive avro to orc conversion flow.\n  * The {@link HiveConvertExtractor} extracts exactly one {@link QueryBasedHiveConversionEntity}.\n@@ -47,6 +49,7 @@\n @ToString\n @EqualsAndHashCode\n @Getter\n+@Slf4j\n public class QueryBasedHiveConversionEntity {\n \n   private final SchemaAwareHiveTable hiveTable;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "sha": "93216ca2cc03d1a41ad9518aed0c590801d92b35",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java?ref=1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
                "deletions": 16,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "patch": "@@ -14,8 +14,11 @@\n import java.io.IOException;\n import java.util.List;\n \n+import lombok.extern.slf4j.Slf4j;\n+\n import org.apache.avro.Schema;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -36,6 +39,7 @@\n import gobblin.hive.HiveMetastoreClientPool;\n import gobblin.source.extractor.DataRecordException;\n import gobblin.source.extractor.Extractor;\n+import gobblin.util.AutoReturnableObject;\n \n \n /**\n@@ -50,6 +54,7 @@\n  * to get the corresponding hive {@link org.apache.hadoop.hive.ql.metadata.Table} and hive {@link org.apache.hadoop.hive.ql.metadata.Partition}\n  * </p>\n  */\n+@Slf4j\n public class HiveConvertExtractor implements Extractor<Schema, QueryBasedHiveConversionEntity> {\n \n   private List<QueryBasedHiveConversionEntity> conversionEntities = Lists.newArrayList();\n@@ -58,31 +63,36 @@ public HiveConvertExtractor(WorkUnitState state, FileSystem fs) throws IOExcepti\n \n     HiveMetastoreClientPool pool =\n         HiveMetastoreClientPool.get(state.getJobState().getProperties(),\n-            Optional.of(state.getJobState().getProp(HiveDatasetFinder.HIVE_METASTORE_URI_KEY)));\n+            Optional.fromNullable(state.getJobState().getProp(HiveDatasetFinder.HIVE_METASTORE_URI_KEY)));\n \n     SerializableHiveTable hiveTable = HiveSourceUtils.deserializeTable(state);\n \n-    Table table = pool.getClient().get().getTable(hiveTable.getDbName(), hiveTable.getTableName());\n+    try (AutoReturnableObject<IMetaStoreClient> client = pool.getClient()) {\n+      Table table = client.get().getTable(hiveTable.getDbName(), hiveTable.getTableName());\n+\n+      SchemaAwareHiveTable schemaAwareHiveTable =\n+          new SchemaAwareHiveTable(table, AvroSchemaManager.getSchemaFromUrl(hiveTable.getSchemaUrl(), fs));\n \n-    SchemaAwareHiveTable schemaAwareHiveTable =\n-        new SchemaAwareHiveTable(table, AvroSchemaManager.getSchemaFromUrl(hiveTable.getSchemaUrl(), fs));\n+      SchemaAwareHivePartition schemaAwareHivePartition = null;\n \n-    SchemaAwareHivePartition schemaAwareHivePartition = null;\n+      if (HiveSourceUtils.hasPartition(state)) {\n \n-    if (HiveSourceUtils.hasPartition(state)) {\n+        SerializableHivePartition hivePartition = HiveSourceUtils.deserializePartition(state);\n \n-      SerializableHivePartition hivePartition = HiveSourceUtils.deserializePartition(state);\n+        Partition partition =\n+            client.get()\n+                .getPartition(hiveTable.getDbName(), hiveTable.getTableName(), hivePartition.getPartitionName());\n+        schemaAwareHivePartition =\n+            new SchemaAwareHivePartition(table, partition, AvroSchemaManager.getSchemaFromUrl(\n+                hivePartition.getSchemaUrl(), fs));\n+      }\n \n-      Partition partition =\n-          pool.getClient().get()\n-              .getPartition(hiveTable.getTableName(), hiveTable.getDbName(), hivePartition.getPartitionName());\n-      schemaAwareHivePartition =\n-          new SchemaAwareHivePartition(table, partition, AvroSchemaManager.getSchemaFromUrl(\n-              hivePartition.getSchemaUrl(), fs));\n+      QueryBasedHiveConversionEntity entity = new QueryBasedHiveConversionEntity(schemaAwareHiveTable, Optional\n+          .fromNullable(schemaAwareHivePartition));\n+      this.conversionEntities.add(entity);\n     }\n \n-    this.conversionEntities.add(new QueryBasedHiveConversionEntity(schemaAwareHiveTable, Optional\n-        .fromNullable(schemaAwareHivePartition)));\n+\n \n   }\n \n@@ -109,7 +119,6 @@ public QueryBasedHiveConversionEntity readRecord(QueryBasedHiveConversionEntity\n     }\n \n     return this.conversionEntities.remove(0);\n-\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "sha": "b81ace9dc1bb8793ab55246a9745b9457b978265",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java?ref=1d2fd4c05819f3105fef103136001f6f2ffd4f0c",
                "deletions": 3,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "patch": "@@ -12,6 +12,7 @@\n \n package gobblin.data.management.conversion.hive.util;\n \n+import java.io.IOException;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -27,6 +28,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableMap;\n \n+\n /***\n  * Generate Hive queries\n  */\n@@ -108,8 +110,7 @@ public static String generateCreateTableDDL(Schema schema,\n                                               Optional<Integer> optionalNumOfBuckets,\n                                               Optional<String> optionalRowFormatSerde,\n                                               Optional<String> optionalInputFormat,\n-                                              Optional<String> optionalOutputFormat,\n-                                              Optional<Map<String, String>> optionalTblProperties) {\n+                                              Optional<String> optionalOutputFormat, Optional<Map<String, String>> optionalTblProperties) {\n \n     Preconditions.checkNotNull(schema);\n     Preconditions.checkArgument(StringUtils.isNotBlank(tblName));\n@@ -387,7 +388,7 @@ public static String generateTableMappingDML(Schema originalSchema, Schema flatt\n     Preconditions.checkArgument(StringUtils.isNotBlank(originalTblName));\n     Preconditions.checkArgument(StringUtils.isNotBlank(flattenedTblName));\n \n-    String originalDbName = optionalOriginalDbName.isPresent() ? optionalFlattenedDbName.get() : DEFAULT_DB_NAME;\n+    String originalDbName = optionalOriginalDbName.isPresent() ? optionalOriginalDbName.get() : DEFAULT_DB_NAME;\n     String flattenedDbName = optionalFlattenedDbName.isPresent() ? optionalFlattenedDbName.get() : DEFAULT_DB_NAME;\n     boolean shouldOverwriteTable = optionalOverwriteTable.isPresent() ? optionalOverwriteTable.get() : true;\n     boolean shouldCreateIfNotExists = optionalCreateIfNotExists.isPresent() ? optionalCreateIfNotExists.get() : false;\n@@ -459,4 +460,9 @@ public static String generateTableMappingDML(Schema originalSchema, Schema flatt\n \n     return dmlQuery.toString();\n   }\n+\n+  public static Schema readSchemaFromString(String schemaStr)\n+      throws IOException {\n+    return new Schema.Parser().parse(schemaStr);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/1d2fd4c05819f3105fef103136001f6f2ffd4f0c/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "sha": "997adcdc0e1dce6ba480bcf249f8af9e13531799",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1036 from abti/master\n\nHive metastore connection pool optimization, Fixes for: backward compatibility for Hive in AvroToOrc, schema parser deserialization from schema literal, database name in Hive DDL query generation, Hive metastore connection pool initialization NPE if Hcat uri is platform provided",
        "parent": "https://github.com/apache/incubator-gobblin/commit/33bf210989db17a606f7e04c6373aba264814d7b",
        "patched_files": [
            "HiveAvroORCQueryUtils.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HiveAvroORCQueryUtilsTest.java"
        ]
    },
    "incubator-gobblin_26b2605": {
        "bug_id": "incubator-gobblin_26b2605",
        "commit": "https://github.com/apache/incubator-gobblin/commit/26b2605c73a077875a7485855d07998e8965849e",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/26b2605c73a077875a7485855d07998e8965849e/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java?ref=26b2605c73a077875a7485855d07998e8965849e",
                "deletions": 1,
                "filename": "gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "patch": "@@ -160,7 +160,11 @@ public void setActualHighWatermark(Watermark watermark) {\n    * Backoff the actual high watermark to the low watermark returned by {@link WorkUnit#getLowWatermark()}.\n    */\n   public void backoffActualHighWatermark() {\n-    setProp(ConfigurationKeys.WORK_UNIT_STATE_ACTUAL_HIGH_WATER_MARK_KEY, this.workunit.getLowWatermark().toString());\n+    JsonElement lowWatermark = this.workunit.getLowWatermark();\n+    if (lowWatermark == null) {\n+      return;\n+    }\n+    setProp(ConfigurationKeys.WORK_UNIT_STATE_ACTUAL_HIGH_WATER_MARK_KEY, lowWatermark.toString());\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/26b2605c73a077875a7485855d07998e8965849e/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "sha": "0863d69991f9128018a9f8e2bf6c0ab0ad81ea6b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/26b2605c73a077875a7485855d07998e8965849e/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java?ref=26b2605c73a077875a7485855d07998e8965849e",
                "deletions": 1,
                "filename": "gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "patch": "@@ -113,9 +113,13 @@ public Extract getExtract() {\n   /**\n    * Get the low {@link Watermark} as a {@link JsonElement}.\n    *\n-   * @return a {@link JsonElement} representing the low {@link Watermark}.\n+   * @return a {@link JsonElement} representing the low {@link Watermark} or\n+   *         {@code null} if the low {@link Watermark} is not set.\n    */\n   public JsonElement getLowWatermark() {\n+    if (!contains(ConfigurationKeys.WATERMARK_INTERVAL_VALUE_KEY)) {\n+      return null;\n+    }\n     return JSON_PARSER.parse(getProp(ConfigurationKeys.WATERMARK_INTERVAL_VALUE_KEY)).getAsJsonObject()\n         .get(WatermarkInterval.LOW_WATERMARK_TO_JSON_KEY);\n   }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/26b2605c73a077875a7485855d07998e8965849e/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "sha": "63e2f08c76e2a0e398b1ceca15a3b348116ed86b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/26b2605c73a077875a7485855d07998e8965849e/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java?ref=26b2605c73a077875a7485855d07998e8965849e",
                "deletions": 5,
                "filename": "gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "patch": "@@ -17,7 +17,6 @@\n \n import org.apache.avro.Schema;\n import org.apache.avro.file.CodecFactory;\n-import org.apache.avro.file.DataFileConstants;\n import org.apache.avro.file.DataFileWriter;\n import org.apache.avro.generic.GenericDatumWriter;\n import org.apache.avro.generic.GenericRecord;\n@@ -167,10 +166,12 @@ public long bytesWritten() throws IOException {\n   /**\n    * Create a new {@link DataFileWriter} for writing Avro records.\n    *\n-   * @param avroFile Avro file to write to\n-   * @param bufferSize Buffer size\n-   * @param codecType Compression codec type\n-   * @param deflateLevel Deflate level\n+   * @param avroFile the Avro file to write to\n+   * @param bufferSize the Avro data writer buffer size\n+   * @param codecFactory a {@link CodecFactory} object for building the compression codec\n+   * @param replication the replication factor\n+   * @param blockSize the block size\n+   * @param permissions a {@link FsPermission} object defining the Avro file permission\n    * @throws IOException if there is something wrong creating a new {@link DataFileWriter}\n    */\n   private DataFileWriter<GenericRecord> createDatumWriter(Path avroFile, int bufferSize, CodecFactory codecFactory,",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/26b2605c73a077875a7485855d07998e8965849e/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "sha": "a32860724c85fb66a81f47a76c8499e7504b1592",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/26b2605c73a077875a7485855d07998e8965849e/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java?ref=26b2605c73a077875a7485855d07998e8965849e",
                "deletions": 2,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "patch": "@@ -164,7 +164,11 @@ public void close() throws IOException {\n         this.job.killJob();\n       }\n     } finally {\n-      super.close();\n+      try {\n+        cleanUpWorkingDirectory();\n+      } finally {\n+        super.close();\n+      }\n     }\n   }\n \n@@ -235,7 +239,7 @@ protected JobLock getJobLock() throws IOException {\n   @Override\n   protected void executeCancellation() {\n     try {\n-      if (!this.job.isComplete()) {\n+      if (this.hadoopJobSubmitted && !this.job.isComplete()) {\n         LOG.info(\"Killing the Hadoop MR job for job \" + this.jobContext.getJobId());\n         this.job.killJob();\n       }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/26b2605c73a077875a7485855d07998e8965849e/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "sha": "c28801c1c76e4ce17ab5aaffb964575b0f220707",
                "status": "modified"
            }
        ],
        "message": "Fixed a potential NPE in WorkUnit.getLowWatermark()\n\nSigned-off-by: Yinan Li <liyinan926@gmail.com>",
        "parent": "https://github.com/apache/incubator-gobblin/commit/279439e24b7b1438b4c13423d02253eda70ebaee",
        "patched_files": [
            "MRJobLauncher.java",
            "WorkUnitState.java",
            "AvroHdfsDataWriter.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "WorkUnitStateTest.java",
            "AvroHdfsDataWriterTest.java",
            "MRJobLauncherTest.java"
        ]
    },
    "incubator-gobblin_29c76c9": {
        "bug_id": "incubator-gobblin_29c76c9",
        "commit": "https://github.com/apache/incubator-gobblin/commit/29c76c9e7fc87271219d3fe5be8f3f9790742614",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/29c76c9e7fc87271219d3fe5be8f3f9790742614/gobblin-service/src/main/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStore.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-service/src/main/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStore.java?ref=29c76c9e7fc87271219d3fe5be8f3f9790742614",
                "deletions": 9,
                "filename": "gobblin-service/src/main/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStore.java",
                "patch": "@@ -57,8 +57,15 @@\n   private final String dagCheckpointDir;\n   private final Gson gson;\n \n-  public FSDagStateStore(Config config) {\n+  public FSDagStateStore(Config config) throws IOException {\n     this.dagCheckpointDir = config.getString(DagManager.DAG_STATESTORE_DIR);\n+    File checkpointDir = new File(this.dagCheckpointDir);\n+    if (!checkpointDir.exists()) {\n+      if (!checkpointDir.mkdirs()) {\n+        throw new IOException(\"Could not create dag state store dir - \" + this.dagCheckpointDir);\n+      }\n+    }\n+\n     JsonSerializer<List<JobExecutionPlan>> serializer = new JobExecutionPlanListSerializer();\n     JsonDeserializer<List<JobExecutionPlan>> deserializer = new JobExecutionPlanListDeserializer();\n     this.gson = new GsonBuilder().registerTypeAdapter(LIST_JOBEXECUTIONPLAN_TYPE, serializer)\n@@ -75,13 +82,6 @@ public synchronized void writeCheckpoint(Dag<JobExecutionPlan> dag) throws IOExc\n     String fileName = DagManagerUtils.generateDagId(dag) + DAG_FILE_EXTENSION;\n     String serializedDag = serializeDag(dag);\n \n-    File checkpointDir = new File(this.dagCheckpointDir);\n-    if (!checkpointDir.exists()) {\n-      if (!checkpointDir.mkdirs()) {\n-        throw new IOException(\"Could not create dir - \" + this.dagCheckpointDir);\n-      }\n-    }\n-\n     File tmpCheckpointFile = new File(this.dagCheckpointDir, fileName + \".tmp\");\n     File checkpointFile = new File(this.dagCheckpointDir, fileName);\n \n@@ -110,8 +110,9 @@ public synchronized void cleanUp(Dag<JobExecutionPlan> dag) {\n   public List<Dag<JobExecutionPlan>> getDags() throws IOException {\n     List<Dag<JobExecutionPlan>> runningDags = Lists.newArrayList();\n     File dagCheckpointFolder = new File(this.dagCheckpointDir);\n+\n     for (File file : dagCheckpointFolder.listFiles((dir, name) -> name.endsWith(DAG_FILE_EXTENSION))) {\n-        runningDags.add(getDag(file));\n+      runningDags.add(getDag(file));\n     }\n     return runningDags;\n   }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/29c76c9e7fc87271219d3fe5be8f3f9790742614/gobblin-service/src/main/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStore.java",
                "sha": "e647fed4e02ea8b1bcccf2c7ab250d07e9e594c3",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/29c76c9e7fc87271219d3fe5be8f3f9790742614/gobblin-service/src/test/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStoreTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-service/src/test/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStoreTest.java?ref=29c76c9e7fc87271219d3fe5be8f3f9790742614",
                "deletions": 4,
                "filename": "gobblin-service/src/test/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStoreTest.java",
                "patch": "@@ -52,11 +52,11 @@\n \n   @BeforeClass\n   public void setUp() throws IOException {\n+    this.checkpointDir = new File(dagStateStoreDir);\n+    FileUtils.deleteDirectory(this.checkpointDir);\n     Config config = ConfigFactory.empty().withValue(DagManager.DAG_STATESTORE_DIR, ConfigValueFactory.fromAnyRef(\n         this.dagStateStoreDir));\n     this._dagStateStore = new FSDagStateStore(config);\n-    this.checkpointDir = new File(dagStateStoreDir);\n-    FileUtils.deleteDirectory(this.checkpointDir);\n   }\n \n   /**\n@@ -128,8 +128,8 @@ public void testCleanUp() throws IOException, URISyntaxException {\n \n   @Test (dependsOnMethods = \"testCleanUp\")\n   public void testGetDags() throws IOException, URISyntaxException {\n-    //Delete dag checkpoint dir\n-    FileUtils.deleteDirectory(this.checkpointDir);\n+    //Set up a new FSDagStateStore instance.\n+    setUp();\n     List<Long> flowExecutionIds = Lists.newArrayList(System.currentTimeMillis(), System.currentTimeMillis() + 1);\n     for (int i = 0; i < 2; i++) {\n       String flowGroupId = Integer.toString(i);",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/29c76c9e7fc87271219d3fe5be8f3f9790742614/gobblin-service/src/test/java/org/apache/gobblin/service/modules/orchestration/FSDagStateStoreTest.java",
                "sha": "073d0913938de8e3a51299ba131a8a9a6f9cd414",
                "status": "modified"
            }
        ],
        "message": "[GOBBLIN-637] Create dag checkpoint dir on initialization to avoid NPE.[]\n\nCloses #2507 from sv2000/dagStateStoreNpe",
        "parent": "https://github.com/apache/incubator-gobblin/commit/578b4f9504cf322c7efb9e36b551c52b5ba1155b",
        "patched_files": [
            "FSDagStateStore.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "FSDagStateStoreTest.java"
        ]
    },
    "incubator-gobblin_2f9ac6c": {
        "bug_id": "incubator-gobblin_2f9ac6c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/2f9ac6cfbf403930b69fa17770fa908a1bc607bb",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/2f9ac6cfbf403930b69fa17770fa908a1bc607bb/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java?ref=2f9ac6cfbf403930b69fa17770fa908a1bc607bb",
                "deletions": 1,
                "filename": "gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "patch": "@@ -465,10 +465,16 @@ public void publishMetadata(Collection<? extends WorkUnitState> states)\n             branchId,\n             getMetadataOutputFileForBranch(anyState, branchId));\n       } else {\n+        String metadataFilename = getMetadataFileNameForBranch(anyState, branchId);\n+        if (mdOutputPath == null || metadataFilename == null) {\n+          LOG.info(\"Metadata filename not set for branch \" + String.valueOf(branchId) + \": not publishing metadata.\");\n+          continue;\n+        }\n+\n         for (String partition : partitions) {\n           publishMetadata(getMergedMetadataForPartitionAndBranch(partition, branchId),\n               branchId,\n-              new Path(new Path(mdOutputPath, partition), getMetadataFileNameForBranch(anyState, branchId)));\n+              new Path(new Path(mdOutputPath, partition), metadataFilename));\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/2f9ac6cfbf403930b69fa17770fa908a1bc607bb/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "sha": "f0d0e32f1e89166c214c6a178bacf123a93c2470",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/2f9ac6cfbf403930b69fa17770fa908a1bc607bb/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java?ref=2f9ac6cfbf403930b69fa17770fa908a1bc607bb",
                "deletions": 0,
                "filename": "gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "patch": "@@ -197,6 +197,38 @@ public void testNoOutputWhenDisabled()\n     Assert.assertFalse(mdFile.exists(), \"Internal metadata from writer should not be written out if no merger is set in config\");\n   }\n \n+  @Test\n+  public void testNoOutputWhenDisabledWithPartitions()\n+      throws IOException {\n+\n+    File publishPath = Files.createTempDir();\n+\n+    State s = buildDefaultState(1);\n+    s.removeProp(ConfigurationKeys.DATA_PUBLISHER_METADATA_OUTPUT_DIR);\n+    s.removeProp(ConfigurationKeys.DATA_PUBLISHER_METADATA_OUTPUT_FILE);\n+    s.setProp(ConfigurationKeys.DATA_PUBLISHER_FINAL_DIR, publishPath.getAbsolutePath());\n+\n+    WorkUnitState wuState = new WorkUnitState();\n+    addStateToWorkunit(s, wuState);\n+\n+    wuState.setProp(ConfigurationKeys.WRITER_METADATA_KEY, \"abcdefg\");\n+\n+    FsWriterMetrics metrics1 = buildWriterMetrics(\"foo1.json\", \"1-2-3-4\", 0, 10);\n+    FsWriterMetrics metrics2 = buildWriterMetrics(\"foo1.json\", \"5-6-7-8\",10, 20);\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY, \"1-2-3-4\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY, metrics1.toJson());\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + \"_0\", \"1-2-3-4\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY + \" _0\", metrics2.toJson());\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + \"_1\", \"5-6-7-8\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY + \" _1\", metrics2.toJson());\n+\n+    BaseDataPublisher publisher = new BaseDataPublisher(s);\n+    publisher.publishMetadata(Collections.singletonList(wuState));\n+\n+    String[] filesInPublishDir = publishPath.list();\n+    Assert.assertEquals(0, filesInPublishDir.length, \"Expected 0 files to be output to publish path\");\n+  }\n+\n   @Test\n   public void testMergesExistingMetadata() throws IOException {\n     File publishPath = Files.createTempDir();",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/2f9ac6cfbf403930b69fa17770fa908a1bc607bb/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "sha": "09bc0c8d0c2cc6a0c64a4eeeb018d51a2d945a0f",
                "status": "modified"
            }
        ],
        "message": "[GOBBLIN-194] Fix a NullPointerException that can occur if a partitioned\nwriter is used and a filename for metadata output has not been set",
        "parent": "https://github.com/apache/incubator-gobblin/commit/067d42233fab93b82f62f0569159679a0b5102fa",
        "patched_files": [
            "BaseDataPublisher.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "BaseDataPublisherTest.java"
        ]
    },
    "incubator-gobblin_34ddf8e": {
        "bug_id": "incubator-gobblin_34ddf8e",
        "commit": "https://github.com/apache/incubator-gobblin/commit/34ddf8eef5ebb66d83544efd89d8485651c272b0",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/34ddf8eef5ebb66d83544efd89d8485651c272b0/gobblin-api/src/main/java/gobblin/configuration/State.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/configuration/State.java?ref=34ddf8eef5ebb66d83544efd89d8485651c272b0",
                "deletions": 15,
                "filename": "gobblin-api/src/main/java/gobblin/configuration/State.java",
                "patch": "@@ -28,7 +28,7 @@\n \n \n /**\n- * A serializable wrapper class that can be persisted for {@link java.util.Properties}.\n+ * A serializable wrapper class that can be persisted for {@link Properties}.\n  *\n  * @author kgoodhop\n  */\n@@ -47,9 +47,9 @@ public State(Properties properties) {\n   }\n \n   /**\n-   * Return a copy of the underlying {@link java.util.Properties} object.\n+   * Return a copy of the underlying {@link Properties} object.\n    *\n-   * @return A copy of the underlying {@link java.util.Properties} object.\n+   * @return A copy of the underlying {@link Properties} object.\n    */\n   public Properties getProperties() {\n     Properties props = new Properties();\n@@ -104,7 +104,7 @@ public String getId() {\n    * @param value property value\n    */\n   public void setProp(String key, Object value) {\n-    properties.put(key, value.toString());\n+    this.properties.put(key, value.toString());\n   }\n \n   /**\n@@ -149,10 +149,10 @@ public String getProp(String key, String def) {\n   }\n \n   /**\n-   * Get the value of a comma separated property as a {@link java.util.List} of strings.\n+   * Get the value of a comma separated property as a {@link List} of strings.\n    *\n    * @param key property key\n-   * @return value associated with the key as a {@link java.util.List} of strings\n+   * @return value associated with the key as a {@link List} of strings\n    */\n   public List<String> getPropAsList(String key) {\n     return Splitter.on(\",\").trimResults().omitEmptyStrings().splitToList(getProperty(key));\n@@ -170,22 +170,22 @@ public String getProp(String key, String def) {\n   }\n \n   /**\n-   * Get the value of a property as a case insensitive {@link java.util.Set} of strings.\n+   * Get the value of a property as a case insensitive {@link Set} of strings.\n    *\n    * @param key property key\n-   * @return value associated with the key as a case insensitive {@link java.util.Set} of strings\n+   * @return value associated with the key as a case insensitive {@link Set} of strings\n    */\n   public Set<String> getPropAsCaseInsensitiveSet(String key) {\n     return ImmutableSortedSet.copyOf(String.CASE_INSENSITIVE_ORDER,\n         Splitter.on(\",\").trimResults().omitEmptyStrings().split(getProperty(key)));\n   }\n \n   /**\n-   * Get the value of a property as a case insensitive {@link java.util.Set} of strings, using the given default value if the property is not set.\n+   * Get the value of a property as a case insensitive {@link Set} of strings, using the given default value if the property is not set.\n    *\n    * @param key property key\n    * @param def default value\n-   * @return value associated with the key as a case insensitive {@link java.util.Set} of strings\n+   * @return value associated with the key as a case insensitive {@link Set} of strings\n    */\n   public Set<String> getPropAsCaseInsensitiveSet(String key, String def) {\n     return ImmutableSortedSet.copyOf(String.CASE_INSENSITIVE_ORDER,\n@@ -298,17 +298,17 @@ public boolean getPropAsBoolean(String key, boolean def) {\n   }\n \n   protected String getProperty(String key) {\n-    return properties.getProperty(key);\n+    return this.properties.getProperty(key);\n   }\n \n   protected String getProperty(String key, String def) {\n-    return properties.getProperty(key, def);\n+    return this.properties.getProperty(key, def);\n   }\n \n   /**\n-   * Get the names of all the properties set in a {@link java.util.Set}.\n+   * Get the names of all the properties set in a {@link Set}.\n    *\n-   * @return names of all the properties set in a {@link java.util.Set}\n+   * @return names of all the properties set in a {@link Set}\n    */\n   public Set<String> getPropertyNames() {\n     return this.properties.stringPropertyNames();\n@@ -321,7 +321,7 @@ protected String getProperty(String key, String def) {\n    * @return <code>true</code> if the property is set or <code>false</code> otherwise\n    */\n   public boolean contains(String key) {\n-    return properties.getProperty(key) != null;\n+    return this.properties.getProperty(key) != null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/34ddf8eef5ebb66d83544efd89d8485651c272b0/gobblin-api/src/main/java/gobblin/configuration/State.java",
                "sha": "d52039eb0275e4e33f6948e179afceb69efd3bd0",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/34ddf8eef5ebb66d83544efd89d8485651c272b0/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java?ref=34ddf8eef5ebb66d83544efd89d8485651c272b0",
                "deletions": 5,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "patch": "@@ -77,7 +77,7 @@\n   private final int index;\n \n   private final Converter converter;\n-  private final Object convertedSchema;\n+  private final Optional<Object> convertedSchema;\n   private final RowLevelPolicyChecker rowLevelPolicyChecker;\n   private final RowLevelPolicyCheckResults rowLevelPolicyCheckingResult;\n \n@@ -116,7 +116,7 @@ public Fork(TaskContext taskContext, TaskState taskState, Object schema, int bra\n     this.index = index;\n \n     this.converter = this.closer.register(new MultiConverter(this.taskContext.getConverters(this.index)));\n-    this.convertedSchema = this.converter.convertSchema(schema, this.taskState);\n+    this.convertedSchema = Optional.fromNullable(this.converter.convertSchema(schema, this.taskState));\n     this.rowLevelPolicyChecker =\n         this.closer.register(this.taskContext.getRowLevelPolicyChecker(this.taskState, this.index));\n     this.rowLevelPolicyCheckingResult = new RowLevelPolicyCheckResults();\n@@ -328,7 +328,7 @@ public void close() throws IOException {\n         .writeTo(Destination.of(this.taskContext.getDestinationType(this.branches, this.index), this.taskState))\n         .writeInFormat(this.taskContext.getWriterOutputFormat(this.branches, this.index))\n         .withWriterId(this.taskId)\n-        .withSchema(this.convertedSchema)\n+        .withSchema(this.convertedSchema.get())\n         .withBranches(this.branches)\n         .forBranch(this.index)\n         .build();\n@@ -374,7 +374,7 @@ private void processRecords() throws IOException, DataConversionException {\n    *\n    * @return whether data publishing is successful and data should be committed\n    */\n-  private boolean checkDataQuality(Object schema)\n+  private boolean checkDataQuality(Optional<Object> schema)\n       throws Exception {\n     TaskState taskStateForFork = this.taskState;\n     if (this.branches > 1) {\n@@ -387,7 +387,10 @@ private boolean checkDataQuality(Object schema)\n     } else {\n       taskStateForFork.setProp(ConfigurationKeys.WRITER_ROWS_WRITTEN, 0l);\n     }\n-    taskStateForFork.setProp(ConfigurationKeys.EXTRACT_SCHEMA, schema.toString());\n+\n+    if (schema.isPresent()) {\n+      taskStateForFork.setProp(ConfigurationKeys.EXTRACT_SCHEMA, schema.get().toString());\n+    }\n \n     try {\n       // Do task-level quality checking",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/34ddf8eef5ebb66d83544efd89d8485651c272b0/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "sha": "212e671fb2612b25093a3a497836366c1fbc86e1",
                "status": "modified"
            }
        ],
        "message": "Fixing NPE when Extractor.getSchema returns null",
        "parent": "https://github.com/apache/incubator-gobblin/commit/713498e6a678d2b85fee790fd48a4fa589737a77",
        "patched_files": [
            "State.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "StateTest.java"
        ]
    },
    "incubator-gobblin_3523f3f": {
        "bug_id": "incubator-gobblin_3523f3f",
        "commit": "https://github.com/apache/incubator-gobblin/commit/3523f3f32498c41fe5ddb292b1fa216231aaf62a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java?ref=3523f3f32498c41fe5ddb292b1fa216231aaf62a",
                "deletions": 1,
                "filename": "gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "patch": "@@ -160,7 +160,11 @@ public void setActualHighWatermark(Watermark watermark) {\n    * Backoff the actual high watermark to the low watermark returned by {@link WorkUnit#getLowWatermark()}.\n    */\n   public void backoffActualHighWatermark() {\n-    setProp(ConfigurationKeys.WORK_UNIT_STATE_ACTUAL_HIGH_WATER_MARK_KEY, this.workunit.getLowWatermark().toString());\n+    JsonElement lowWatermark = this.workunit.getLowWatermark();\n+    if (lowWatermark == null) {\n+      return;\n+    }\n+    setProp(ConfigurationKeys.WORK_UNIT_STATE_ACTUAL_HIGH_WATER_MARK_KEY, lowWatermark.toString());\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-api/src/main/java/gobblin/configuration/WorkUnitState.java",
                "sha": "0863d69991f9128018a9f8e2bf6c0ab0ad81ea6b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java?ref=3523f3f32498c41fe5ddb292b1fa216231aaf62a",
                "deletions": 1,
                "filename": "gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "patch": "@@ -113,9 +113,13 @@ public Extract getExtract() {\n   /**\n    * Get the low {@link Watermark} as a {@link JsonElement}.\n    *\n-   * @return a {@link JsonElement} representing the low {@link Watermark}.\n+   * @return a {@link JsonElement} representing the low {@link Watermark} or\n+   *         {@code null} if the low {@link Watermark} is not set.\n    */\n   public JsonElement getLowWatermark() {\n+    if (!contains(ConfigurationKeys.WATERMARK_INTERVAL_VALUE_KEY)) {\n+      return null;\n+    }\n     return JSON_PARSER.parse(getProp(ConfigurationKeys.WATERMARK_INTERVAL_VALUE_KEY)).getAsJsonObject()\n         .get(WatermarkInterval.LOW_WATERMARK_TO_JSON_KEY);\n   }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-api/src/main/java/gobblin/source/workunit/WorkUnit.java",
                "sha": "63e2f08c76e2a0e398b1ceca15a3b348116ed86b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java?ref=3523f3f32498c41fe5ddb292b1fa216231aaf62a",
                "deletions": 5,
                "filename": "gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "patch": "@@ -17,7 +17,6 @@\n \n import org.apache.avro.Schema;\n import org.apache.avro.file.CodecFactory;\n-import org.apache.avro.file.DataFileConstants;\n import org.apache.avro.file.DataFileWriter;\n import org.apache.avro.generic.GenericDatumWriter;\n import org.apache.avro.generic.GenericRecord;\n@@ -167,10 +166,12 @@ public long bytesWritten() throws IOException {\n   /**\n    * Create a new {@link DataFileWriter} for writing Avro records.\n    *\n-   * @param avroFile Avro file to write to\n-   * @param bufferSize Buffer size\n-   * @param codecType Compression codec type\n-   * @param deflateLevel Deflate level\n+   * @param avroFile the Avro file to write to\n+   * @param bufferSize the Avro data writer buffer size\n+   * @param codecFactory a {@link CodecFactory} object for building the compression codec\n+   * @param replication the replication factor\n+   * @param blockSize the block size\n+   * @param permissions a {@link FsPermission} object defining the Avro file permission\n    * @throws IOException if there is something wrong creating a new {@link DataFileWriter}\n    */\n   private DataFileWriter<GenericRecord> createDatumWriter(Path avroFile, int bufferSize, CodecFactory codecFactory,",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-core/src/main/java/gobblin/writer/AvroHdfsDataWriter.java",
                "sha": "a32860724c85fb66a81f47a76c8499e7504b1592",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java?ref=3523f3f32498c41fe5ddb292b1fa216231aaf62a",
                "deletions": 2,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "patch": "@@ -164,7 +164,11 @@ public void close() throws IOException {\n         this.job.killJob();\n       }\n     } finally {\n-      super.close();\n+      try {\n+        cleanUpWorkingDirectory();\n+      } finally {\n+        super.close();\n+      }\n     }\n   }\n \n@@ -235,7 +239,7 @@ protected JobLock getJobLock() throws IOException {\n   @Override\n   protected void executeCancellation() {\n     try {\n-      if (!this.job.isComplete()) {\n+      if (this.hadoopJobSubmitted && !this.job.isComplete()) {\n         LOG.info(\"Killing the Hadoop MR job for job \" + this.jobContext.getJobId());\n         this.job.killJob();\n       }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/3523f3f32498c41fe5ddb292b1fa216231aaf62a/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "sha": "c28801c1c76e4ce17ab5aaffb964575b0f220707",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #226 from liyinan926/master\n\nFixed a potential NPE in WorkUnit.getLowWatermark()",
        "parent": "https://github.com/apache/incubator-gobblin/commit/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d",
        "patched_files": [
            "MRJobLauncher.java",
            "WorkUnitState.java",
            "AvroHdfsDataWriter.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "WorkUnitStateTest.java",
            "AvroHdfsDataWriterTest.java",
            "MRJobLauncherTest.java"
        ]
    },
    "incubator-gobblin_5c1aa34": {
        "bug_id": "incubator-gobblin_5c1aa34",
        "commit": "https://github.com/apache/incubator-gobblin/commit/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d/gobblin-api/src/main/java/gobblin/configuration/State.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-api/src/main/java/gobblin/configuration/State.java?ref=5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d",
                "deletions": 15,
                "filename": "gobblin-api/src/main/java/gobblin/configuration/State.java",
                "patch": "@@ -28,7 +28,7 @@\n \n \n /**\n- * A serializable wrapper class that can be persisted for {@link java.util.Properties}.\n+ * A serializable wrapper class that can be persisted for {@link Properties}.\n  *\n  * @author kgoodhop\n  */\n@@ -47,9 +47,9 @@ public State(Properties properties) {\n   }\n \n   /**\n-   * Return a copy of the underlying {@link java.util.Properties} object.\n+   * Return a copy of the underlying {@link Properties} object.\n    *\n-   * @return A copy of the underlying {@link java.util.Properties} object.\n+   * @return A copy of the underlying {@link Properties} object.\n    */\n   public Properties getProperties() {\n     Properties props = new Properties();\n@@ -104,7 +104,7 @@ public String getId() {\n    * @param value property value\n    */\n   public void setProp(String key, Object value) {\n-    properties.put(key, value.toString());\n+    this.properties.put(key, value.toString());\n   }\n \n   /**\n@@ -149,10 +149,10 @@ public String getProp(String key, String def) {\n   }\n \n   /**\n-   * Get the value of a comma separated property as a {@link java.util.List} of strings.\n+   * Get the value of a comma separated property as a {@link List} of strings.\n    *\n    * @param key property key\n-   * @return value associated with the key as a {@link java.util.List} of strings\n+   * @return value associated with the key as a {@link List} of strings\n    */\n   public List<String> getPropAsList(String key) {\n     return Splitter.on(\",\").trimResults().omitEmptyStrings().splitToList(getProperty(key));\n@@ -170,22 +170,22 @@ public String getProp(String key, String def) {\n   }\n \n   /**\n-   * Get the value of a property as a case insensitive {@link java.util.Set} of strings.\n+   * Get the value of a property as a case insensitive {@link Set} of strings.\n    *\n    * @param key property key\n-   * @return value associated with the key as a case insensitive {@link java.util.Set} of strings\n+   * @return value associated with the key as a case insensitive {@link Set} of strings\n    */\n   public Set<String> getPropAsCaseInsensitiveSet(String key) {\n     return ImmutableSortedSet.copyOf(String.CASE_INSENSITIVE_ORDER,\n         Splitter.on(\",\").trimResults().omitEmptyStrings().split(getProperty(key)));\n   }\n \n   /**\n-   * Get the value of a property as a case insensitive {@link java.util.Set} of strings, using the given default value if the property is not set.\n+   * Get the value of a property as a case insensitive {@link Set} of strings, using the given default value if the property is not set.\n    *\n    * @param key property key\n    * @param def default value\n-   * @return value associated with the key as a case insensitive {@link java.util.Set} of strings\n+   * @return value associated with the key as a case insensitive {@link Set} of strings\n    */\n   public Set<String> getPropAsCaseInsensitiveSet(String key, String def) {\n     return ImmutableSortedSet.copyOf(String.CASE_INSENSITIVE_ORDER,\n@@ -298,17 +298,17 @@ public boolean getPropAsBoolean(String key, boolean def) {\n   }\n \n   protected String getProperty(String key) {\n-    return properties.getProperty(key);\n+    return this.properties.getProperty(key);\n   }\n \n   protected String getProperty(String key, String def) {\n-    return properties.getProperty(key, def);\n+    return this.properties.getProperty(key, def);\n   }\n \n   /**\n-   * Get the names of all the properties set in a {@link java.util.Set}.\n+   * Get the names of all the properties set in a {@link Set}.\n    *\n-   * @return names of all the properties set in a {@link java.util.Set}\n+   * @return names of all the properties set in a {@link Set}\n    */\n   public Set<String> getPropertyNames() {\n     return this.properties.stringPropertyNames();\n@@ -321,7 +321,7 @@ protected String getProperty(String key, String def) {\n    * @return <code>true</code> if the property is set or <code>false</code> otherwise\n    */\n   public boolean contains(String key) {\n-    return properties.getProperty(key) != null;\n+    return this.properties.getProperty(key) != null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d/gobblin-api/src/main/java/gobblin/configuration/State.java",
                "sha": "d52039eb0275e4e33f6948e179afceb69efd3bd0",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java?ref=5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d",
                "deletions": 5,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "patch": "@@ -77,7 +77,7 @@\n   private final int index;\n \n   private final Converter converter;\n-  private final Object convertedSchema;\n+  private final Optional<Object> convertedSchema;\n   private final RowLevelPolicyChecker rowLevelPolicyChecker;\n   private final RowLevelPolicyCheckResults rowLevelPolicyCheckingResult;\n \n@@ -116,7 +116,7 @@ public Fork(TaskContext taskContext, TaskState taskState, Object schema, int bra\n     this.index = index;\n \n     this.converter = this.closer.register(new MultiConverter(this.taskContext.getConverters(this.index)));\n-    this.convertedSchema = this.converter.convertSchema(schema, this.taskState);\n+    this.convertedSchema = Optional.fromNullable(this.converter.convertSchema(schema, this.taskState));\n     this.rowLevelPolicyChecker =\n         this.closer.register(this.taskContext.getRowLevelPolicyChecker(this.taskState, this.index));\n     this.rowLevelPolicyCheckingResult = new RowLevelPolicyCheckResults();\n@@ -328,7 +328,7 @@ public void close() throws IOException {\n         .writeTo(Destination.of(this.taskContext.getDestinationType(this.branches, this.index), this.taskState))\n         .writeInFormat(this.taskContext.getWriterOutputFormat(this.branches, this.index))\n         .withWriterId(this.taskId)\n-        .withSchema(this.convertedSchema)\n+        .withSchema(this.convertedSchema.orNull())\n         .withBranches(this.branches)\n         .forBranch(this.index)\n         .build();\n@@ -374,7 +374,7 @@ private void processRecords() throws IOException, DataConversionException {\n    *\n    * @return whether data publishing is successful and data should be committed\n    */\n-  private boolean checkDataQuality(Object schema)\n+  private boolean checkDataQuality(Optional<Object> schema)\n       throws Exception {\n     TaskState taskStateForFork = this.taskState;\n     if (this.branches > 1) {\n@@ -387,7 +387,10 @@ private boolean checkDataQuality(Object schema)\n     } else {\n       taskStateForFork.setProp(ConfigurationKeys.WRITER_ROWS_WRITTEN, 0l);\n     }\n-    taskStateForFork.setProp(ConfigurationKeys.EXTRACT_SCHEMA, schema.toString());\n+\n+    if (schema.isPresent()) {\n+      taskStateForFork.setProp(ConfigurationKeys.EXTRACT_SCHEMA, schema.get().toString());\n+    }\n \n     try {\n       // Do task-level quality checking",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/5c1aa34f8079d49dce01ba8a716ce3e87e0c5e7d/gobblin-runtime/src/main/java/gobblin/runtime/Fork.java",
                "sha": "e8818c535e24a3854c73bb0ca9d1b0adbdab0547",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #224 from sahilTakiar/schemaNPE\n\nFixing NPE when Extractor.getSchema returns null",
        "parent": "https://github.com/apache/incubator-gobblin/commit/fdffda740d6690767b0fafefc714724493cdc3a9",
        "patched_files": [
            "State.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "StateTest.java"
        ]
    },
    "incubator-gobblin_5e19355": {
        "bug_id": "incubator-gobblin_5e19355",
        "commit": "https://github.com/apache/incubator-gobblin/commit/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff",
        "file": [
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java?ref=5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff",
                "deletions": 23,
                "filename": "gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n import org.apache.hadoop.hive.serde2.Deserializer;\n import org.apache.hadoop.hive.serde2.SerDeException;\n import org.apache.hadoop.hive.serde2.SerDeUtils;\n@@ -333,35 +334,39 @@ private static State getSerDeProps(SerDeInfo si) {\n     return fieldSchemas;\n   }\n \n+  /**\n+   * Returns a Deserializer from HiveRegistrationUnit if present and successfully initialized. Else returns null.\n+   */\n   private static Deserializer getDeserializer(HiveRegistrationUnit unit) {\n-    Deserializer deserializer = null;\n-\n     Optional<String> serdeClass = unit.getSerDeType();\n-    if (serdeClass.isPresent()) {\n-      String serde = serdeClass.get();\n-      HiveConf hiveConf = new HiveConf();\n+    if (!serdeClass.isPresent()) {\n+      return null;\n+    }\n \n-      try {\n-        deserializer = ReflectionUtils.newInstance(hiveConf.getClassByName(serde).asSubclass(Deserializer.class),\n-            hiveConf);\n-      } catch (ClassNotFoundException e) {\n-        LOG.warn(\"Serde class \" + serde + \" not found!\", e);\n-      }\n+    String serde = serdeClass.get();\n+    HiveConf hiveConf = new HiveConf();\n \n-      if (deserializer == null) {\n-        return deserializer;\n-      }\n+    Deserializer deserializer;\n+    try {\n+      deserializer = ReflectionUtils.newInstance(hiveConf.getClassByName(serde).asSubclass(Deserializer.class),\n+          hiveConf);\n+\n+    } catch (ClassNotFoundException e) {\n+      LOG.warn(\"Serde class \" + serde + \" not found!\", e);\n+      return null;\n+    }\n \n-      Properties props = new Properties();\n-      props.putAll(unit.getProps().getProperties());\n-      props.putAll(unit.getStorageProps().getProperties());\n-      props.putAll(unit.getSerDeProps().getProperties());\n+    Properties props = new Properties();\n+    props.putAll(unit.getProps().getProperties());\n+    props.putAll(unit.getStorageProps().getProperties());\n+    props.putAll(unit.getSerDeProps().getProperties());\n \n-      try {\n-        SerDeUtils.initializeSerDe(deserializer, hiveConf, props, null);\n-      } catch (SerDeException e) {\n-        LOG.warn(\"Failed to initialize serde \" + serde + \" with properties \" + props);\n-      }\n+    try {\n+      SerDeUtils.initializeSerDe(deserializer, hiveConf, props, null);\n+    } catch (SerDeException e) {\n+      LOG.warn(\"Failed to initialize serde \" + serde + \" with properties \" + props + \" for table \" + unit.getDbName() +\n+          \".\" + unit.getTableName());\n+      return null;\n     }\n \n     return deserializer;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "sha": "7a3940ecbde28970a3563adefda1629ff3ead138",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java?ref=5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff",
                "deletions": 1,
                "filename": "gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "patch": "@@ -34,7 +34,7 @@\n \n public class HiveMetaStoreUtilsTest {\n   @Test\n-  public void testGetTable() {\n+  public void testGetTableAvro() {\n     final String databaseName = \"testdb\";\n     final String tableName = \"testtable\";\n \n@@ -69,4 +69,36 @@ public void testGetTable() {\n     Assert.assertEquals(fieldA.getName(), \"a\");\n     Assert.assertEquals(fieldA.getType(), \"int\");\n   }\n+\n+  @Test\n+  public void testGetTableAvroInvalidSchema() {\n+    final String databaseName = \"testdb\";\n+    final String tableName = \"testtable\";\n+\n+    HiveTable.Builder builder = new HiveTable.Builder();\n+\n+    builder.withDbName(databaseName).withTableName(tableName);\n+\n+    State serdeProps = new State();\n+    serdeProps.setProp(\"avro.schema.literal\", \"invalid schema\");\n+    builder.withSerdeProps(serdeProps);\n+\n+    HiveTable hiveTable = builder.build();\n+    hiveTable.setInputFormat(AvroContainerInputFormat.class.getName());\n+    hiveTable.setOutputFormat(AvroContainerOutputFormat.class.getName());\n+    hiveTable.setSerDeType(AvroSerDe.class.getName());\n+\n+    Table table = HiveMetaStoreUtils.getTable(hiveTable);\n+    Assert.assertEquals(table.getDbName(), databaseName);\n+    Assert.assertEquals(table.getTableName(), tableName);\n+\n+    StorageDescriptor sd = table.getSd();\n+    Assert.assertEquals(sd.getInputFormat(), AvroContainerInputFormat.class.getName());\n+    Assert.assertEquals(sd.getOutputFormat(), AvroContainerOutputFormat.class.getName());\n+    Assert.assertNotNull(sd.getSerdeInfo());\n+    Assert.assertEquals(sd.getSerdeInfo().getSerializationLib(), AvroSerDe.class.getName());\n+\n+    List<FieldSchema> fields = sd.getCols();\n+    Assert.assertTrue(fields != null && fields.size() == 0);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "sha": "ca188daea1a960f46ad15d5863b94139fd61b56f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gradle/scripts/defaultBuildProperties.gradle",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gradle/scripts/defaultBuildProperties.gradle?ref=5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff",
                "deletions": 1,
                "filename": "gradle/scripts/defaultBuildProperties.gradle",
                "patch": "@@ -28,7 +28,7 @@ def BuildProperties BUILD_PROPERTIES = new BuildProperties(project)\n     .register(new BuildProperty(\"doNotSignArtifacts\", false, \"Do not sight Maven artifacts\"))\n     .register(new BuildProperty(\"gobblinFlavor\", \"standard\", \"Build flavor (see http://gobblin.readthedocs.io/en/latest/developer-guide/GobblinModules/)\"))\n     .register(new BuildProperty(\"hadoopVersion\", \"2.3.0\", \"Hadoop dependencies version\"))\n-    .register(new BuildProperty(\"hiveVersion\", \"1.0.1\", \"Hive dependencies version\"))\n+    .register(new BuildProperty(\"hiveVersion\", \"1.1.0\", \"Hive dependencies version\"))\n     .register(new BuildProperty(\"jdkVersion\", JavaVersion.VERSION_1_7.toString(),\n     \"Java languange compatibility; one of \" + JavaVersion.VERSION_1_7 + \", \" +\n         JavaVersion.VERSION_1_8))",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/5e19355d0a9de58d4329cf8d7a011f4c20d2c5ff/gradle/scripts/defaultBuildProperties.gradle",
                "sha": "c62e815bdac25f25dece1e3c19af2c8b711df533",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1635 from erwa/fix-npe\n\nFix NullPointerException when Deserializer is not properly initialized",
        "parent": "https://github.com/apache/incubator-gobblin/commit/408d7d3b3099dc85d9ab2ddb5c1b5f887c50c4d9",
        "patched_files": [
            "HiveMetaStoreUtils.java",
            "defaultBuildProperties.gradle"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HiveMetaStoreUtilsTest.java"
        ]
    },
    "incubator-gobblin_647319c": {
        "bug_id": "incubator-gobblin_647319c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/647319ca6e4ecc5991dcefd1794e1e64a16dd458",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/647319ca6e4ecc5991dcefd1794e1e64a16dd458/gobblin-utility/src/main/java/org/apache/gobblin/util/ConfigUtils.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-utility/src/main/java/org/apache/gobblin/util/ConfigUtils.java?ref=647319ca6e4ecc5991dcefd1794e1e64a16dd458",
                "deletions": 1,
                "filename": "gobblin-utility/src/main/java/org/apache/gobblin/util/ConfigUtils.java",
                "patch": "@@ -419,7 +419,9 @@ public static Config getConfig(Config config, String path, Config def) {\n     try {\n       valueList = config.getStringList(path);\n     } catch (ConfigException.WrongType e) {\n-\n+      if (StringUtils.isEmpty(config.getString(path))) {\n+        return Collections.emptyList();\n+      }\n       /*\n        * Using CSV Reader as values could be quoted.\n        * E.g The string \"a\",\"false\",\"b\",\"10,12\" will be split to a list of 4 elements and not 5.",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/647319ca6e4ecc5991dcefd1794e1e64a16dd458/gobblin-utility/src/main/java/org/apache/gobblin/util/ConfigUtils.java",
                "sha": "4ca747c45779f67ca1b92d72da385dc919bc61f7",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/647319ca6e4ecc5991dcefd1794e1e64a16dd458/gobblin-utility/src/test/java/org/apache/gobblin/util/ConfigUtilsTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-utility/src/test/java/org/apache/gobblin/util/ConfigUtilsTest.java?ref=647319ca6e4ecc5991dcefd1794e1e64a16dd458",
                "deletions": 0,
                "filename": "gobblin-utility/src/test/java/org/apache/gobblin/util/ConfigUtilsTest.java",
                "patch": "@@ -129,6 +129,11 @@ public void testGetStringList() throws Exception {\n     Map<String,String> configMap = Maps.newHashMap();\n     configMap.put(\"key1\", null);\n     Assert.assertEquals(ConfigUtils.getStringList(ConfigFactory.parseMap(configMap), \"key1\"), ImmutableList.of());\n+\n+    // Empty list if value is empty string\n+    configMap = Maps.newHashMap();\n+    configMap.put(\"key2\", \"\");\n+    Assert.assertEquals(ConfigUtils.getStringList(ConfigFactory.parseMap(configMap), \"key2\"), ImmutableList.of());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/647319ca6e4ecc5991dcefd1794e1e64a16dd458/gobblin-utility/src/test/java/org/apache/gobblin/util/ConfigUtilsTest.java",
                "sha": "4ac527dfc3023fdf66ea80b52e729b0a0cca80e9",
                "status": "modified"
            }
        ],
        "message": "Fix NPE while reading empty config strings\n\nCloses #2197 from pcadabam/master",
        "parent": "https://github.com/apache/incubator-gobblin/commit/47951b21d8e3379c4d731d05a109c49adcb7a925",
        "patched_files": [
            "ConfigUtils.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "ConfigUtilsTest.java"
        ]
    },
    "incubator-gobblin_6c14efe": {
        "bug_id": "incubator-gobblin_6c14efe",
        "commit": "https://github.com/apache/incubator-gobblin/commit/6c14efe7616bb3b0d3e935248b929077643bff94",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/6c14efe7616bb3b0d3e935248b929077643bff94/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java?ref=6c14efe7616bb3b0d3e935248b929077643bff94",
                "deletions": 1,
                "filename": "gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "patch": "@@ -465,10 +465,16 @@ public void publishMetadata(Collection<? extends WorkUnitState> states)\n             branchId,\n             getMetadataOutputFileForBranch(anyState, branchId));\n       } else {\n+        String metadataFilename = getMetadataFileNameForBranch(anyState, branchId);\n+        if (mdOutputPath == null || metadataFilename == null) {\n+          LOG.info(\"Metadata filename not set for branch \" + String.valueOf(branchId) + \": not publishing metadata.\");\n+          continue;\n+        }\n+\n         for (String partition : partitions) {\n           publishMetadata(getMergedMetadataForPartitionAndBranch(partition, branchId),\n               branchId,\n-              new Path(new Path(mdOutputPath, partition), getMetadataFileNameForBranch(anyState, branchId)));\n+              new Path(new Path(mdOutputPath, partition), metadataFilename));\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/6c14efe7616bb3b0d3e935248b929077643bff94/gobblin-core/src/main/java/org/apache/gobblin/publisher/BaseDataPublisher.java",
                "sha": "f0d0e32f1e89166c214c6a178bacf123a93c2470",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/6c14efe7616bb3b0d3e935248b929077643bff94/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java?ref=6c14efe7616bb3b0d3e935248b929077643bff94",
                "deletions": 0,
                "filename": "gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "patch": "@@ -197,6 +197,38 @@ public void testNoOutputWhenDisabled()\n     Assert.assertFalse(mdFile.exists(), \"Internal metadata from writer should not be written out if no merger is set in config\");\n   }\n \n+  @Test\n+  public void testNoOutputWhenDisabledWithPartitions()\n+      throws IOException {\n+\n+    File publishPath = Files.createTempDir();\n+\n+    State s = buildDefaultState(1);\n+    s.removeProp(ConfigurationKeys.DATA_PUBLISHER_METADATA_OUTPUT_DIR);\n+    s.removeProp(ConfigurationKeys.DATA_PUBLISHER_METADATA_OUTPUT_FILE);\n+    s.setProp(ConfigurationKeys.DATA_PUBLISHER_FINAL_DIR, publishPath.getAbsolutePath());\n+\n+    WorkUnitState wuState = new WorkUnitState();\n+    addStateToWorkunit(s, wuState);\n+\n+    wuState.setProp(ConfigurationKeys.WRITER_METADATA_KEY, \"abcdefg\");\n+\n+    FsWriterMetrics metrics1 = buildWriterMetrics(\"foo1.json\", \"1-2-3-4\", 0, 10);\n+    FsWriterMetrics metrics2 = buildWriterMetrics(\"foo1.json\", \"5-6-7-8\",10, 20);\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY, \"1-2-3-4\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY, metrics1.toJson());\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + \"_0\", \"1-2-3-4\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY + \" _0\", metrics2.toJson());\n+    wuState.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + \"_1\", \"5-6-7-8\");\n+    wuState.setProp(FsDataWriter.FS_WRITER_METRICS_KEY + \" _1\", metrics2.toJson());\n+\n+    BaseDataPublisher publisher = new BaseDataPublisher(s);\n+    publisher.publishMetadata(Collections.singletonList(wuState));\n+\n+    String[] filesInPublishDir = publishPath.list();\n+    Assert.assertEquals(0, filesInPublishDir.length, \"Expected 0 files to be output to publish path\");\n+  }\n+\n   @Test\n   public void testMergesExistingMetadata() throws IOException {\n     File publishPath = Files.createTempDir();",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/6c14efe7616bb3b0d3e935248b929077643bff94/gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java",
                "sha": "09bc0c8d0c2cc6a0c64a4eeeb018d51a2d945a0f",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #2046 from eogren/publisher_npe",
        "parent": "https://github.com/apache/incubator-gobblin/commit/067d42233fab93b82f62f0569159679a0b5102fa",
        "patched_files": [
            "BaseDataPublisher.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "BaseDataPublisherTest.java"
        ]
    },
    "incubator-gobblin_77653a2": {
        "bug_id": "incubator-gobblin_77653a2",
        "commit": "https://github.com/apache/incubator-gobblin/commit/77653a29f39ea08aa7b13a76263777fb74b400e6",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java?ref=77653a29f39ea08aa7b13a76263777fb74b400e6",
                "deletions": 13,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "patch": "@@ -15,6 +15,7 @@\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n \n+import lombok.extern.slf4j.Slf4j;\n import org.apache.avro.Schema;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -28,6 +29,7 @@\n \n import gobblin.configuration.ConfigurationKeys;\n import gobblin.configuration.State;\n+import gobblin.data.management.conversion.hive.util.HiveAvroORCQueryUtils;\n import gobblin.hive.avro.HiveAvroSerDeManager;\n import gobblin.util.AvroUtils;\n import gobblin.util.HadoopUtils;\n@@ -55,6 +57,7 @@\n  * If multiple {@link Partition}s have the same {@link Schema} a duplicate schema file in not created. Already existing\n  * {@link Schema} url for this {@link Schema} is used.\n  */\n+@Slf4j\n public class AvroSchemaManager {\n \n   private static final String HIVE_SCHEMA_TEMP_DIR_PATH_KEY = \"hive.schema.dir\";\n@@ -65,7 +68,6 @@\n    * A mapping of {@link Schema} hash to its {@link Path} on {@link FileSystem}\n    */\n   private final Map<String, Path> schemaPaths;\n-  private final Schema.Parser schemaParser;\n \n   /**\n    * A temporary directory to hold all Schema files. The path is job id specific.\n@@ -76,7 +78,6 @@\n   public AvroSchemaManager(FileSystem fs, State state) {\n     this.fs = fs;\n     this.schemaPaths = Maps.newHashMap();\n-    this.schemaParser = new Schema.Parser();\n     this.schemaDir = new Path(state.getProp(HIVE_SCHEMA_TEMP_DIR_PATH_KEY, DEFAULT_HIVE_SCHEMA_TEMP_DIR_PATH_KEY),\n             state.getProp(ConfigurationKeys.JOB_ID_KEY));\n   }\n@@ -88,13 +89,13 @@ public AvroSchemaManager(FileSystem fs, State state) {\n    * @return a {@link Path} to table's avro {@link Schema} file.\n    */\n   public Path getSchemaUrl(Table table) throws IOException {\n-    return getSchemaUrl(table.getSd());\n+    return getSchemaUrl(table.getTTable().getSd());\n   }\n \n   /**\n    * Get the url to <code>partition</code>'s avro {@link Schema} file.\n    *\n-   * @param table whose avro schema is to be returned\n+   * @param partition whose avro schema is to be returned\n    * @return a {@link Path} to table's avro {@link Schema} file.\n    */\n   public Path getSchemaUrl(Partition partition) throws IOException {\n@@ -115,16 +116,24 @@ public static Schema getSchemaFromUrl(Path schemaUrl, FileSystem fs) throws IOEx\n   private Path getSchemaUrl(StorageDescriptor sd) throws IOException {\n     if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_URL)) {\n       return new Path(sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_URL));\n-    } else if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_LITERAL)) {\n-\n-      Schema schema =\n-          this.schemaParser.parse(sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_LITERAL));\n-      return getOrGenerateSchemaFile(schema);\n-\n-    } else {\n-      Schema schema = AvroUtils.getDirectorySchema(new Path(sd.getLocation()), this.fs, true);\n-      return getOrGenerateSchemaFile(schema);\n     }\n+    String schemaLiteral = null;\n+    try {\n+      if (sd.getSerdeInfo().getParameters().containsKey(HiveAvroSerDeManager.SCHEMA_LITERAL)) {\n+\n+        schemaLiteral = sd.getSerdeInfo().getParameters().get(HiveAvroSerDeManager.SCHEMA_LITERAL);\n+        log.debug(\"Schema literal is: \" + schemaLiteral);\n+        Schema schema = HiveAvroORCQueryUtils.readSchemaFromString(schemaLiteral);\n+\n+        return getOrGenerateSchemaFile(schema);\n+      }\n+    } catch (Exception e) {\n+      log.error(String.format(\"Failed to parse schema from schema literal. Falling back to HDFS schema: %s\",\n+          schemaLiteral), e);\n+    }\n+\n+    Schema schema = AvroUtils.getDirectorySchema(new Path(sd.getLocation()), this.fs, true);\n+    return getOrGenerateSchemaFile(schema);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/AvroSchemaManager.java",
                "sha": "7f6b88bd45c836e0512605d97d01992353e83309",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java?ref=77653a29f39ea08aa7b13a76263777fb74b400e6",
                "deletions": 57,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "patch": "@@ -11,18 +11,19 @@\n  */\n package gobblin.data.management.conversion.hive.converter;\n \n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n import lombok.extern.slf4j.Slf4j;\n \n import org.apache.avro.Schema;\n import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n \n import com.google.common.base.Optional;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Splitter;\n+import com.google.common.collect.Maps;\n \n import gobblin.configuration.WorkUnitState;\n import gobblin.converter.Converter;\n@@ -33,6 +34,7 @@\n import gobblin.data.management.conversion.hive.util.HiveAvroORCQueryUtils;\n import gobblin.util.AvroFlattener;\n \n+\n /**\n  * Builds the Hive avro to Orc conversion query. The record type for this converter is {@link QueryBasedHiveConversionEntity}. A {@link QueryBasedHiveConversionEntity}\n  * can be a hive table or a hive partition.\n@@ -43,8 +45,9 @@\n \n   // TODO: Remove when topology is enabled\n   private static final String ORC_TABLE_ALTERNATE_LOCATION = \"orc.table.alternate.location\";\n+  private static final String ORC_TABLE_ALTERNATE_DATABASE = \"orc.table.alternate.database\";\n \n-  private static AvroFlattener avroFlattener = new AvroFlattener();\n+  private static AvroFlattener AVRO_FLATTENER = new AvroFlattener();\n \n   @Override\n   public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws SchemaConversionException {\n@@ -57,12 +60,12 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n   @Override\n   public Iterable<QueryBasedHiveConversionEntity> convertRecord(Schema outputSchema,\n       QueryBasedHiveConversionEntity conversionEntity, WorkUnitState workUnit) throws DataConversionException {\n-    Preconditions.checkNotNull(outputSchema);\n-    Preconditions.checkNotNull(conversionEntity);\n-    Preconditions.checkNotNull(workUnit);\n-    Preconditions.checkNotNull(conversionEntity.getHiveTable());\n+    Preconditions.checkNotNull(outputSchema, \"Output schema must not be null\");\n+    Preconditions.checkNotNull(conversionEntity, \"Conversion entity must not be null\");\n+    Preconditions.checkNotNull(workUnit, \"Workunit state must not be null\");\n+    Preconditions.checkNotNull(conversionEntity.getHiveTable(), \"Hive table within conversion entity must not be null\");\n \n-    Schema flattenedSchema = avroFlattener.flatten(outputSchema, true);\n+    Schema flattenedSchema = AVRO_FLATTENER.flatten(outputSchema, true);\n \n     // Create flattened table if not exists\n     // ORC Hive tables are named as   : {avro_table_name}_orc\n@@ -78,28 +81,75 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n             : conversionEntity.getHiveTable().getSd().getLocation();\n \n     // ORC table name and location\n+    // TODO: Define naming convention and pull it from config / topology\n     String orcTableName = avroTableName + \"_orc\";\n-    String orcTableLocation;\n+    String orcTableDatabase = getOrcTableDatabase(workUnit, conversionEntity);\n+    String orcDataLocation = getOrcDataLocation(workUnit, avroDataLocation, orcTableName);\n+\n+    // Populate optional partition info\n+    Map<String, String> partitionsDDLInfo = Maps.newHashMap();\n+    Map<String, String> partitionsDMLInfo = Maps.newHashMap();\n+    populatePartitionInfo(conversionEntity, partitionsDDLInfo, partitionsDMLInfo);\n+\n+    // Create DDL statement\n+    String createFlattenedTableDDL = HiveAvroORCQueryUtils\n+        .generateCreateTableDDL(flattenedSchema,\n+            orcTableName,\n+            orcDataLocation,\n+            Optional.of(orcTableDatabase),\n+            Optional.of(partitionsDDLInfo),\n+            Optional.<List<String>>absent(),\n+            Optional.<Map<String, HiveAvroORCQueryUtils.COLUMN_SORT_ORDER>>absent(),\n+            Optional.<Integer>absent(),\n+            Optional.<String>absent(),\n+            Optional.<String>absent(),\n+            Optional.<String>absent(),\n+            Optional.<Map<String, String>>absent());\n+    conversionEntity.getQueries().add(createFlattenedTableDDL);\n+    log.info(\"Create DDL: \" + createFlattenedTableDDL);\n+\n+    // Create DML statement\n+    String insertInORCTableDML = HiveAvroORCQueryUtils\n+        .generateTableMappingDML(outputSchema, flattenedSchema, avroTableName, orcTableName,\n+            Optional.of(conversionEntity.getHiveTable().getDbName()),\n+            Optional.of(orcTableDatabase),\n+            Optional.of(partitionsDMLInfo),\n+            Optional.<Boolean>absent(), Optional.<Boolean>absent());\n+    conversionEntity.getQueries().add(insertInORCTableDML);\n+    log.info(\"Conversion DML: \" + insertInORCTableDML);\n+\n+    log.info(\"Conversion Query \" + conversionEntity.getQueries());\n+    return new SingleRecordIterable<>(conversionEntity);\n+  }\n+\n+  private String getOrcTableDatabase(WorkUnitState workUnit, QueryBasedHiveConversionEntity conversionEntity) {\n+    String orcTableAlternateDB = workUnit.getJobState().getProp(ORC_TABLE_ALTERNATE_DATABASE);\n+    return StringUtils.isNotBlank(orcTableAlternateDB) ? orcTableAlternateDB :\n+        conversionEntity.getHiveTable().getDbName();\n+  }\n+\n+  private String getOrcDataLocation(WorkUnitState workUnit, String avroDataLocation, String orcTableName) {\n+    String orcDataLocation;\n \n     // By default ORC table creates a new directory where Avro data resides with _orc postfix, but this can be\n     // .. overridden by specifying this property\n     String orcTableAlternateLocation = workUnit.getJobState().getProp(ORC_TABLE_ALTERNATE_LOCATION);\n     if (StringUtils.isNotBlank(orcTableAlternateLocation)) {\n-      orcTableLocation = orcTableAlternateLocation.endsWith(\"/\") ?\n-           orcTableAlternateLocation + orcTableName : orcTableAlternateLocation + \"/\" + orcTableName;\n+      orcDataLocation = StringUtils.removeEnd(orcTableAlternateLocation, Path.SEPARATOR) + \"/\" + orcTableName;\n     } else {\n-      orcTableLocation = (avroDataLocation.endsWith(\"/\") ?\n-          avroDataLocation.substring(0, avroDataLocation.length() - 1) : avroDataLocation) + \"_orc\";\n+      orcDataLocation = StringUtils.removeEnd(avroDataLocation, Path.SEPARATOR) + \"_orc\";\n     }\n \n     // Each job execution further writes to a sub-directory within ORC data directory to support stagin use-case\n     // .. ie for atomic swap\n-    orcTableLocation += \"/\" + workUnit.getJobState().getId();\n+    orcDataLocation += \"/\" + workUnit.getJobState().getId();\n \n-    // Populate optional partition info\n-    Optional<Map<String, String>> optionalPartitionsDDLInfo = Optional.<Map<String, String>>absent();\n-    Optional<Map<String, String>> optionalPartitionsDMLInfo = Optional.<Map<String, String>>absent();\n+    return orcDataLocation;\n+  }\n \n+  private void populatePartitionInfo(QueryBasedHiveConversionEntity conversionEntity,\n+      Map<String, String> partitionsDDLInfo,\n+      Map<String, String> partitionsDMLInfo) {\n     String partitionsInfoString = null;\n     String partitionsTypeString = null;\n \n@@ -112,8 +162,6 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n       if (StringUtils.isBlank(partitionsInfoString) || StringUtils.isBlank(partitionsTypeString)) {\n         throw new IllegalArgumentException(\"Both partitions info and partions must be present, if one is specified\");\n       }\n-      Map<String, String> partitionDDLInfo = new HashMap<>();\n-      Map<String, String> partitionDMLInfo = new HashMap<>();\n       List<String> pInfo = Splitter.on(\",\").omitEmptyStrings().trimResults().splitToList(partitionsInfoString);\n       List<String> pType = Splitter.on(\",\").omitEmptyStrings().trimResults().splitToList(partitionsTypeString);\n       if (pInfo.size() != pType.size()) {\n@@ -127,46 +175,9 @@ public Schema convertSchema(Schema inputSchema, WorkUnitState workUnit) throws S\n               String.format(\"Partition details should be of the format partitionName=partitionValue. Recieved: %s\",\n                   pInfo.get(i)));\n         }\n-        partitionDDLInfo.put(partitionInfoParts.get(0), partitionType);\n-        partitionDMLInfo.put(partitionInfoParts.get(0), partitionInfoParts.get(1));\n-      }\n-      if (partitionDDLInfo.size() > 0) {\n-        optionalPartitionsDDLInfo = Optional.of(partitionDDLInfo);\n-      }\n-      if (partitionDMLInfo.size() > 0) {\n-        optionalPartitionsDMLInfo = Optional.of(partitionDMLInfo);\n+        partitionsDDLInfo.put(partitionInfoParts.get(0), partitionType);\n+        partitionsDMLInfo.put(partitionInfoParts.get(0), partitionInfoParts.get(1));\n       }\n     }\n-\n-    // Create DDL statement\n-    String createFlattenedTableDDL = HiveAvroORCQueryUtils\n-        .generateCreateTableDDL(flattenedSchema,\n-            orcTableName,\n-            orcTableLocation,\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            optionalPartitionsDDLInfo,\n-            Optional.<List<String>>absent(),\n-            Optional.<Map<String, HiveAvroORCQueryUtils.COLUMN_SORT_ORDER>>absent(),\n-            Optional.<Integer>absent(),\n-            Optional.<String>absent(),\n-            Optional.<String>absent(),\n-            Optional.<String>absent(),\n-            Optional.<Map<String, String>>absent());\n-    conversionEntity.getQueries().add(createFlattenedTableDDL);\n-    log.info(\"Create DDL: \" + createFlattenedTableDDL);\n-\n-    // Create DML statement\n-    String insertInORCTableDML = HiveAvroORCQueryUtils\n-        .generateTableMappingDML(outputSchema, flattenedSchema, avroTableName, orcTableName,\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            Optional.of(conversionEntity.getHiveTable().getDbName()),\n-            optionalPartitionsDMLInfo,\n-            Optional.<Boolean>absent(), Optional.<Boolean>absent());\n-    conversionEntity.getQueries().add(insertInORCTableDML);\n-    log.info(\"Conversion DML: \" + insertInORCTableDML);\n-\n-    log.info(\"Conversion Query \" + conversionEntity.getQueries());\n-    return new SingleRecordIterable<>(conversionEntity);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/converter/HiveAvroToOrcConverter.java",
                "sha": "29c35339bd992593028f8b2584849d1adfb1d2fe",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java?ref=77653a29f39ea08aa7b13a76263777fb74b400e6",
                "deletions": 0,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "patch": "@@ -16,6 +16,7 @@\n import lombok.EqualsAndHashCode;\n import lombok.Getter;\n import lombok.ToString;\n+import lombok.extern.slf4j.Slf4j;\n \n import com.google.common.base.Optional;\n import com.google.common.collect.Lists;\n@@ -29,6 +30,7 @@\n import gobblin.hive.HiveTable;\n import gobblin.source.extractor.Extractor;\n \n+\n /**\n  * Represents a gobblin Record in the Hive avro to orc conversion flow.\n  * The {@link HiveConvertExtractor} extracts exactly one {@link QueryBasedHiveConversionEntity}.\n@@ -47,6 +49,7 @@\n @ToString\n @EqualsAndHashCode\n @Getter\n+@Slf4j\n public class QueryBasedHiveConversionEntity {\n \n   private final SchemaAwareHiveTable hiveTable;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/entities/QueryBasedHiveConversionEntity.java",
                "sha": "93216ca2cc03d1a41ad9518aed0c590801d92b35",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java?ref=77653a29f39ea08aa7b13a76263777fb74b400e6",
                "deletions": 16,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "patch": "@@ -14,8 +14,11 @@\n import java.io.IOException;\n import java.util.List;\n \n+import lombok.extern.slf4j.Slf4j;\n+\n import org.apache.avro.Schema;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -36,6 +39,7 @@\n import gobblin.hive.HiveMetastoreClientPool;\n import gobblin.source.extractor.DataRecordException;\n import gobblin.source.extractor.Extractor;\n+import gobblin.util.AutoReturnableObject;\n \n \n /**\n@@ -50,6 +54,7 @@\n  * to get the corresponding hive {@link org.apache.hadoop.hive.ql.metadata.Table} and hive {@link org.apache.hadoop.hive.ql.metadata.Partition}\n  * </p>\n  */\n+@Slf4j\n public class HiveConvertExtractor implements Extractor<Schema, QueryBasedHiveConversionEntity> {\n \n   private List<QueryBasedHiveConversionEntity> conversionEntities = Lists.newArrayList();\n@@ -58,31 +63,36 @@ public HiveConvertExtractor(WorkUnitState state, FileSystem fs) throws IOExcepti\n \n     HiveMetastoreClientPool pool =\n         HiveMetastoreClientPool.get(state.getJobState().getProperties(),\n-            Optional.of(state.getJobState().getProp(HiveDatasetFinder.HIVE_METASTORE_URI_KEY)));\n+            Optional.fromNullable(state.getJobState().getProp(HiveDatasetFinder.HIVE_METASTORE_URI_KEY)));\n \n     SerializableHiveTable hiveTable = HiveSourceUtils.deserializeTable(state);\n \n-    Table table = pool.getClient().get().getTable(hiveTable.getDbName(), hiveTable.getTableName());\n+    try (AutoReturnableObject<IMetaStoreClient> client = pool.getClient()) {\n+      Table table = client.get().getTable(hiveTable.getDbName(), hiveTable.getTableName());\n+\n+      SchemaAwareHiveTable schemaAwareHiveTable =\n+          new SchemaAwareHiveTable(table, AvroSchemaManager.getSchemaFromUrl(hiveTable.getSchemaUrl(), fs));\n \n-    SchemaAwareHiveTable schemaAwareHiveTable =\n-        new SchemaAwareHiveTable(table, AvroSchemaManager.getSchemaFromUrl(hiveTable.getSchemaUrl(), fs));\n+      SchemaAwareHivePartition schemaAwareHivePartition = null;\n \n-    SchemaAwareHivePartition schemaAwareHivePartition = null;\n+      if (HiveSourceUtils.hasPartition(state)) {\n \n-    if (HiveSourceUtils.hasPartition(state)) {\n+        SerializableHivePartition hivePartition = HiveSourceUtils.deserializePartition(state);\n \n-      SerializableHivePartition hivePartition = HiveSourceUtils.deserializePartition(state);\n+        Partition partition =\n+            client.get()\n+                .getPartition(hiveTable.getDbName(), hiveTable.getTableName(), hivePartition.getPartitionName());\n+        schemaAwareHivePartition =\n+            new SchemaAwareHivePartition(table, partition, AvroSchemaManager.getSchemaFromUrl(\n+                hivePartition.getSchemaUrl(), fs));\n+      }\n \n-      Partition partition =\n-          pool.getClient().get()\n-              .getPartition(hiveTable.getTableName(), hiveTable.getDbName(), hivePartition.getPartitionName());\n-      schemaAwareHivePartition =\n-          new SchemaAwareHivePartition(table, partition, AvroSchemaManager.getSchemaFromUrl(\n-              hivePartition.getSchemaUrl(), fs));\n+      QueryBasedHiveConversionEntity entity = new QueryBasedHiveConversionEntity(schemaAwareHiveTable, Optional\n+          .fromNullable(schemaAwareHivePartition));\n+      this.conversionEntities.add(entity);\n     }\n \n-    this.conversionEntities.add(new QueryBasedHiveConversionEntity(schemaAwareHiveTable, Optional\n-        .fromNullable(schemaAwareHivePartition)));\n+\n \n   }\n \n@@ -109,7 +119,6 @@ public QueryBasedHiveConversionEntity readRecord(QueryBasedHiveConversionEntity\n     }\n \n     return this.conversionEntities.remove(0);\n-\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/extractor/HiveConvertExtractor.java",
                "sha": "b81ace9dc1bb8793ab55246a9745b9457b978265",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java?ref=77653a29f39ea08aa7b13a76263777fb74b400e6",
                "deletions": 3,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "patch": "@@ -12,6 +12,7 @@\n \n package gobblin.data.management.conversion.hive.util;\n \n+import java.io.IOException;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -27,6 +28,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableMap;\n \n+\n /***\n  * Generate Hive queries\n  */\n@@ -108,8 +110,7 @@ public static String generateCreateTableDDL(Schema schema,\n                                               Optional<Integer> optionalNumOfBuckets,\n                                               Optional<String> optionalRowFormatSerde,\n                                               Optional<String> optionalInputFormat,\n-                                              Optional<String> optionalOutputFormat,\n-                                              Optional<Map<String, String>> optionalTblProperties) {\n+                                              Optional<String> optionalOutputFormat, Optional<Map<String, String>> optionalTblProperties) {\n \n     Preconditions.checkNotNull(schema);\n     Preconditions.checkArgument(StringUtils.isNotBlank(tblName));\n@@ -387,7 +388,7 @@ public static String generateTableMappingDML(Schema originalSchema, Schema flatt\n     Preconditions.checkArgument(StringUtils.isNotBlank(originalTblName));\n     Preconditions.checkArgument(StringUtils.isNotBlank(flattenedTblName));\n \n-    String originalDbName = optionalOriginalDbName.isPresent() ? optionalFlattenedDbName.get() : DEFAULT_DB_NAME;\n+    String originalDbName = optionalOriginalDbName.isPresent() ? optionalOriginalDbName.get() : DEFAULT_DB_NAME;\n     String flattenedDbName = optionalFlattenedDbName.isPresent() ? optionalFlattenedDbName.get() : DEFAULT_DB_NAME;\n     boolean shouldOverwriteTable = optionalOverwriteTable.isPresent() ? optionalOverwriteTable.get() : true;\n     boolean shouldCreateIfNotExists = optionalCreateIfNotExists.isPresent() ? optionalCreateIfNotExists.get() : false;\n@@ -459,4 +460,9 @@ public static String generateTableMappingDML(Schema originalSchema, Schema flatt\n \n     return dmlQuery.toString();\n   }\n+\n+  public static Schema readSchemaFromString(String schemaStr)\n+      throws IOException {\n+    return new Schema.Parser().parse(schemaStr);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/77653a29f39ea08aa7b13a76263777fb74b400e6/gobblin-data-management/src/main/java/gobblin/data/management/conversion/hive/util/HiveAvroORCQueryUtils.java",
                "sha": "997adcdc0e1dce6ba480bcf249f8af9e13531799",
                "status": "modified"
            }
        ],
        "message": "- Hive metastore connection pool optimization\n- Fix for backward compatibility for Hive in AvroToOrc\n- Fix for schema parser deserialization from schema literal\n- Fix for database name in Hive DDL query generation\n- Fix for Hive metastore connection pool initialization NPE if Hcat uri is platform provided",
        "parent": "https://github.com/apache/incubator-gobblin/commit/33bf210989db17a606f7e04c6373aba264814d7b",
        "patched_files": [
            "HiveAvroORCQueryUtils.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HiveAvroORCQueryUtilsTest.java"
        ]
    },
    "incubator-gobblin_7d303c6": {
        "bug_id": "incubator-gobblin_7d303c6",
        "commit": "https://github.com/apache/incubator-gobblin/commit/7d303c6f421c4e171510dcb93f9cfd5dd4481e70",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/7d303c6f421c4e171510dcb93f9cfd5dd4481e70/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java?ref=7d303c6f421c4e171510dcb93f9cfd5dd4481e70",
                "deletions": 11,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "patch": "@@ -205,7 +205,7 @@ protected void runWorkUnits(List<WorkUnit> workUnits) throws Exception {\n \n       // Create a metrics set for this job run from the Hadoop counters.\n       // The metrics set is to be persisted to the metrics store later.\n-      countersToMetrics(this.job.getCounters(),\n+      countersToMetrics(Optional.fromNullable(this.job.getCounters()),\n           JobMetrics.get(jobName, this.jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY)));\n     } finally {\n       cleanUpWorkingDirectory();\n@@ -465,17 +465,19 @@ private void cleanUpWorkingDirectory() {\n   /**\n    * Create a {@link gobblin.metrics.GobblinMetrics} instance for this job run from the Hadoop counters.\n    */\n-  private void countersToMetrics(Counters counters, GobblinMetrics metrics) {\n-    // Write job-level counters\n-    CounterGroup jobCounterGroup = counters.getGroup(MetricGroup.JOB.name());\n-    for (Counter jobCounter : jobCounterGroup) {\n-      metrics.getCounter(jobCounter.getName()).inc(jobCounter.getValue());\n-    }\n+  private void countersToMetrics(Optional<Counters> counters, GobblinMetrics metrics) {\n+    if (counters.isPresent()) {\n+      // Write job-level counters\n+      CounterGroup jobCounterGroup = counters.get().getGroup(MetricGroup.JOB.name());\n+      for (Counter jobCounter : jobCounterGroup) {\n+        metrics.getCounter(jobCounter.getName()).inc(jobCounter.getValue());\n+      }\n \n-    // Write task-level counters\n-    CounterGroup taskCounterGroup = counters.getGroup(MetricGroup.TASK.name());\n-    for (Counter taskCounter : taskCounterGroup) {\n-      metrics.getCounter(taskCounter.getName()).inc(taskCounter.getValue());\n+      // Write task-level counters\n+      CounterGroup taskCounterGroup = counters.get().getGroup(MetricGroup.TASK.name());\n+      for (Counter taskCounter : taskCounterGroup) {\n+        metrics.getCounter(taskCounter.getName()).inc(taskCounter.getValue());\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/7d303c6f421c4e171510dcb93f9cfd5dd4481e70/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "sha": "20587b5de3a3ef3a1ddf9c91ae08ae44e291ac95",
                "status": "modified"
            }
        ],
        "message": "Fixing NPE in countersToMetrics",
        "parent": "https://github.com/apache/incubator-gobblin/commit/0b744be0f28c5bd0ef075010e248d004f00032ff",
        "patched_files": [
            "MRJobLauncher.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "MRJobLauncherTest.java"
        ]
    },
    "incubator-gobblin_865b0a0": {
        "bug_id": "incubator-gobblin_865b0a0",
        "commit": "https://github.com/apache/incubator-gobblin/commit/865b0a00441b76f57c86ffda5d34b4c5dc8bd091",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java?ref=865b0a00441b76f57c86ffda5d34b4c5dc8bd091",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "patch": "@@ -44,6 +44,11 @@ public Path globVersionPattern() {\n \n   @Override\n   public TimestampedDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new TimestampedDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.TimestampedDatasetVersion timestampedDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (timestampedDatasetVersion != null) {\n+      return new TimestampedDatasetVersion(timestampedDatasetVersion);\n+    }\n+    return null;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "sha": "a4dd6f3e63b6b814c821d2f8aa1814ba201e66f1",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java?ref=865b0a00441b76f57c86ffda5d34b4c5dc8bd091",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "patch": "@@ -42,7 +42,12 @@ public Path globVersionPattern() {\n \n   @Override\n   public TimestampedDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new TimestampedDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.TimestampedDatasetVersion timestampedDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (timestampedDatasetVersion != null) {\n+      return new TimestampedDatasetVersion(timestampedDatasetVersion);\n+    }\n+    return null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "sha": "c0fdd47f889488e402387203c51f4c5f4bbb2f71",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java?ref=865b0a00441b76f57c86ffda5d34b4c5dc8bd091",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "patch": "@@ -45,6 +45,11 @@ public Path globVersionPattern() {\n \n   @Override\n   public StringDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new StringDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.StringDatasetVersion stringDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (stringDatasetVersion != null) {\n+      return new StringDatasetVersion(stringDatasetVersion);\n+    }\n+    return null;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/865b0a00441b76f57c86ffda5d34b4c5dc8bd091/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "sha": "5167a2cdd887d99b1cb2263e3bac620ed80caa98",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #888 from ydai1124/fix\n\nFix NPE in datasetversion finder",
        "parent": "https://github.com/apache/incubator-gobblin/commit/7c836b9554291260c503dd84db1c04e8c82163b2",
        "patched_files": [
            "UnixTimestampVersionFinder.java",
            "WatermarkDatasetVersionFinder.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "UnixTimestampVersionFinderTest.java",
            "WatermarkDatasetVersionFinderTest.java"
        ]
    },
    "incubator-gobblin_8b1b9e3": {
        "bug_id": "incubator-gobblin_8b1b9e3",
        "commit": "https://github.com/apache/incubator-gobblin/commit/8b1b9e308e68f8277ce84cbe083cdf0d92c96dec",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/8b1b9e308e68f8277ce84cbe083cdf0d92c96dec/gobblin-core/src/main/java/gobblin/converter/csv/CsvToJsonConverterV2.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/main/java/gobblin/converter/csv/CsvToJsonConverterV2.java?ref=8b1b9e308e68f8277ce84cbe083cdf0d92c96dec",
                "deletions": 2,
                "filename": "gobblin-core/src/main/java/gobblin/converter/csv/CsvToJsonConverterV2.java",
                "patch": "@@ -72,12 +72,13 @@\n   @Override\n   public Converter<String, JsonArray, String[], JsonObject> init(WorkUnitState workUnit) {\n     super.init(workUnit);\n-    customOrder = workUnit.getPropAsList(CUSTOM_ORDERING);\n+    customOrder = workUnit.getPropAsList(CUSTOM_ORDERING, \"\");\n     if (!customOrder.isEmpty()) {\n       LOG.info(\"Will use custom order to generate JSON from CSV: \" + customOrder);\n     }\n     return this;\n   }\n+\n   @Override\n   public JsonArray convertSchema(String inputSchema, WorkUnitState workUnit) throws SchemaConversionException {\n     Preconditions.checkNotNull(inputSchema, \"inputSchema is required.\");\n@@ -183,7 +184,7 @@ JsonObject createOutput(JsonArray outputSchema, String[] inputRecord) {\n   JsonObject createOutput(JsonArray outputSchema, String[] inputRecord, List<String> customOrder) {\n \n     Preconditions.checkArgument(outputSchema.size() == customOrder.size(), \"# of columns mismatch. Input \"\n-        + inputRecord.length + \" , output: \" + outputSchema.size());\n+        + outputSchema.size() + \" , output: \" + customOrder.size());\n     JsonObject outputRecord = new JsonObject();\n     Iterator<JsonElement> outputSchemaIterator = outputSchema.iterator();\n     Iterator<String> customOrderIterator = customOrder.iterator();",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/8b1b9e308e68f8277ce84cbe083cdf0d92c96dec/gobblin-core/src/main/java/gobblin/converter/csv/CsvToJsonConverterV2.java",
                "sha": "68cda0857eee769eea6729a9b2f44036fb39c550",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/8b1b9e308e68f8277ce84cbe083cdf0d92c96dec/gobblin-core/src/test/java/gobblin/converter/csv/CsvToJsonConverterV2Test.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-core/src/test/java/gobblin/converter/csv/CsvToJsonConverterV2Test.java?ref=8b1b9e308e68f8277ce84cbe083cdf0d92c96dec",
                "deletions": 8,
                "filename": "gobblin-core/src/test/java/gobblin/converter/csv/CsvToJsonConverterV2Test.java",
                "patch": "@@ -1,9 +1,10 @@\n package gobblin.converter.csv;\n \n+import gobblin.configuration.WorkUnitState;\n+import gobblin.converter.DataConversionException;\n+\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.util.Arrays;\n-\n import org.testng.Assert;\n import org.testng.annotations.Test;\n \n@@ -27,14 +28,16 @@ public void convertOutput() throws IOException {\n     String[] inputRecord = csvParser.parseLine(row10Cols);\n \n     CsvToJsonConverterV2 converter = new CsvToJsonConverterV2();\n+    converter.init(new WorkUnitState());\n     JsonObject actual = converter.createOutput(outputSchema, inputRecord);\n     JsonObject expected = parser.parse(new InputStreamReader(getClass().getResourceAsStream(\"/converter/csv/10_fields.json\")))\n                                 .getAsJsonObject();\n \n     Assert.assertEquals(expected, actual);\n+    converter.close();\n   }\n \n-  public void convertOutputSkippingField() throws IOException {\n+  public void convertOutputSkippingField() throws IOException, DataConversionException {\n     JsonParser parser = new JsonParser();\n     JsonElement jsonElement = parser.parse(new InputStreamReader(getClass().getResourceAsStream(\"/converter/csv/schema_with_10_fields.json\")));\n \n@@ -43,12 +46,16 @@ public void convertOutputSkippingField() throws IOException {\n     String[] inputRecord = csvParser.parseLine(row11Cols);\n \n     CsvToJsonConverterV2 converter = new CsvToJsonConverterV2();\n-    String[] customOrder = {\"0\",\"1\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"};\n-    JsonObject actual = converter.createOutput(outputSchema, inputRecord, Arrays.asList(customOrder));\n+    WorkUnitState wuState = new WorkUnitState();\n+    wuState.setProp(CsvToJsonConverterV2.CUSTOM_ORDERING, \"0,1,3,4,5,6,7,8,9,10\");\n+    converter.init(wuState);\n+\n+    JsonObject actual = converter.convertRecord(outputSchema, inputRecord, wuState).iterator().next();\n     JsonObject expected = parser.parse(new InputStreamReader(getClass().getResourceAsStream(\"/converter/csv/10_fields.json\")))\n                                 .getAsJsonObject();\n \n     Assert.assertEquals(expected, actual);\n+    converter.close();\n   }\n \n   public void convertOutputMismatchFields() throws IOException {\n@@ -66,9 +73,10 @@ public void convertOutputMismatchFields() throws IOException {\n     } catch (Exception e) {\n \n     }\n+    converter.close();\n   }\n \n-  public void convertOutputAddingNull() throws IOException {\n+  public void convertOutputAddingNull() throws IOException, DataConversionException {\n     JsonParser parser = new JsonParser();\n     JsonElement jsonElement = parser.parse(new InputStreamReader(getClass().getResourceAsStream(\"/converter/csv/schema_with_11_fields.json\")));\n \n@@ -77,10 +85,14 @@ public void convertOutputAddingNull() throws IOException {\n     String[] inputRecord = csvParser.parseLine(row11Cols);\n \n     CsvToJsonConverterV2 converter = new CsvToJsonConverterV2();\n-    String[] customOrder = {\"0\",\"1\",\"-1\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"};\n-    JsonObject actual = converter.createOutput(outputSchema, inputRecord, Arrays.asList(customOrder));\n+    WorkUnitState wuState = new WorkUnitState();\n+    wuState.setProp(CsvToJsonConverterV2.CUSTOM_ORDERING, \"0,1,-1,3,4,5,6,7,8,9,10\");\n+    converter.init(wuState);\n+\n+    JsonObject actual = converter.convertRecord(outputSchema, inputRecord, wuState).iterator().next();\n     JsonObject expected = parser.parse(new InputStreamReader(getClass().getResourceAsStream(\"/converter/csv/11_fields_with_null.json\")))\n                                 .getAsJsonObject();\n     Assert.assertEquals(expected, actual);\n+    converter.close();\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/8b1b9e308e68f8277ce84cbe083cdf0d92c96dec/gobblin-core/src/test/java/gobblin/converter/csv/CsvToJsonConverterV2Test.java",
                "sha": "83d3ad2a70d984ef99bea12806d564f077b723dc",
                "status": "modified"
            }
        ],
        "message": "Fixed a bug on CsvToJsonConverterV2 throwing NPE when custom order is not provided. Updated corresponding unit test. (#1356)",
        "parent": "https://github.com/apache/incubator-gobblin/commit/3529f5fb4b11cc6e115beda95fc99c71d3e29abb",
        "patched_files": [
            "CsvToJsonConverterV2.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "CsvToJsonConverterV2Test.java"
        ]
    },
    "incubator-gobblin_8f77c41": {
        "bug_id": "incubator-gobblin_8f77c41",
        "commit": "https://github.com/apache/incubator-gobblin/commit/8f77c41e203e54d6a68c1b52a220c2a012decdd6",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/8f77c41e203e54d6a68c1b52a220c2a012decdd6/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java?ref=8f77c41e203e54d6a68c1b52a220c2a012decdd6",
                "deletions": 0,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "patch": "@@ -176,6 +176,12 @@ public void clean() throws IOException {\n     this.log.info(\"Cleaning dataset \" + this);\n \n     List<T> versions = Lists.newArrayList(getVersionFinder().findDatasetVersions(this));\n+\n+    if (versions.isEmpty()) {\n+      this.log.warn(\"No dataset version can be found. Ignoring.\");\n+      return;\n+    }\n+    \n     Collections.sort(versions, Collections.reverseOrder());\n \n     Collection<T> deletableVersions =",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/8f77c41e203e54d6a68c1b52a220c2a012decdd6/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "sha": "a7580efc57cb227d0eef29b20fef71ad0e9a4d78",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/8f77c41e203e54d6a68c1b52a220c2a012decdd6/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java?ref=8f77c41e203e54d6a68c1b52a220c2a012decdd6",
                "deletions": 2,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "patch": "@@ -61,9 +61,9 @@ public FileLevelTimestampVersionFinder(FileSystem fs, Properties props) {\n       }\n       return timestampedVersions;\n     } catch (IOException e) {\n-      LOGGER.warn(\"Failed to get ModifiedTimStamp for candidate dataset version at \" + dataset.datasetRoot()\n+      LOGGER.warn(\"Failed to get ModifiedTimeStamp for candidate dataset version at \" + dataset.datasetRoot()\n           + \". Ignoring.\");\n-      return null;\n+      return Lists.newArrayList();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/8f77c41e203e54d6a68c1b52a220c2a012decdd6/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "sha": "4a2d0ef88511eac1ee06556eaa4a22116afa4e2f",
                "status": "modified"
            }
        ],
        "message": "Fix NPE when finding dataset versions",
        "parent": "https://github.com/apache/incubator-gobblin/commit/888ebcf52780139bc14d23f0d15be10fddeae71d",
        "patched_files": [
            "DatasetBase.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "DatasetBaseTest.java"
        ]
    },
    "incubator-gobblin_9b08750": {
        "bug_id": "incubator-gobblin_9b08750",
        "commit": "https://github.com/apache/incubator-gobblin/commit/9b087509fdbce67ac1a88ba3e46681f823da525a",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/9b087509fdbce67ac1a88ba3e46681f823da525a/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java?ref=9b087509fdbce67ac1a88ba3e46681f823da525a",
                "deletions": 11,
                "filename": "gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "patch": "@@ -205,7 +205,7 @@ protected void runWorkUnits(List<WorkUnit> workUnits) throws Exception {\n \n       // Create a metrics set for this job run from the Hadoop counters.\n       // The metrics set is to be persisted to the metrics store later.\n-      countersToMetrics(this.job.getCounters(),\n+      countersToMetrics(Optional.fromNullable(this.job.getCounters()),\n           JobMetrics.get(jobName, this.jobProps.getProperty(ConfigurationKeys.JOB_ID_KEY)));\n     } finally {\n       cleanUpWorkingDirectory();\n@@ -465,17 +465,19 @@ private void cleanUpWorkingDirectory() {\n   /**\n    * Create a {@link gobblin.metrics.GobblinMetrics} instance for this job run from the Hadoop counters.\n    */\n-  private void countersToMetrics(Counters counters, GobblinMetrics metrics) {\n-    // Write job-level counters\n-    CounterGroup jobCounterGroup = counters.getGroup(MetricGroup.JOB.name());\n-    for (Counter jobCounter : jobCounterGroup) {\n-      metrics.getCounter(jobCounter.getName()).inc(jobCounter.getValue());\n-    }\n+  private void countersToMetrics(Optional<Counters> counters, GobblinMetrics metrics) {\n+    if (counters.isPresent()) {\n+      // Write job-level counters\n+      CounterGroup jobCounterGroup = counters.get().getGroup(MetricGroup.JOB.name());\n+      for (Counter jobCounter : jobCounterGroup) {\n+        metrics.getCounter(jobCounter.getName()).inc(jobCounter.getValue());\n+      }\n \n-    // Write task-level counters\n-    CounterGroup taskCounterGroup = counters.getGroup(MetricGroup.TASK.name());\n-    for (Counter taskCounter : taskCounterGroup) {\n-      metrics.getCounter(taskCounter.getName()).inc(taskCounter.getValue());\n+      // Write task-level counters\n+      CounterGroup taskCounterGroup = counters.get().getGroup(MetricGroup.TASK.name());\n+      for (Counter taskCounter : taskCounterGroup) {\n+        metrics.getCounter(taskCounter.getName()).inc(taskCounter.getValue());\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/9b087509fdbce67ac1a88ba3e46681f823da525a/gobblin-runtime/src/main/java/gobblin/runtime/mapreduce/MRJobLauncher.java",
                "sha": "20587b5de3a3ef3a1ddf9c91ae08ae44e291ac95",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #163 from sahilTakiar/countersNPE\n\nFixing NPE in countersToMetrics",
        "parent": "https://github.com/apache/incubator-gobblin/commit/0b744be0f28c5bd0ef075010e248d004f00032ff",
        "patched_files": [
            "MRJobLauncher.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "MRJobLauncherTest.java"
        ]
    },
    "incubator-gobblin_a55a572": {
        "bug_id": "incubator-gobblin_a55a572",
        "commit": "https://github.com/apache/incubator-gobblin/commit/a55a5722efe4ffc633a331e0d83c2a77eabd67d8",
        "file": [
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java?ref=a55a5722efe4ffc633a331e0d83c2a77eabd67d8",
                "deletions": 23,
                "filename": "gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n import org.apache.hadoop.hive.serde2.Deserializer;\n import org.apache.hadoop.hive.serde2.SerDeException;\n import org.apache.hadoop.hive.serde2.SerDeUtils;\n@@ -333,35 +334,39 @@ private static State getSerDeProps(SerDeInfo si) {\n     return fieldSchemas;\n   }\n \n+  /**\n+   * Returns a Deserializer from HiveRegistrationUnit if present and successfully initialized. Else returns null.\n+   */\n   private static Deserializer getDeserializer(HiveRegistrationUnit unit) {\n-    Deserializer deserializer = null;\n-\n     Optional<String> serdeClass = unit.getSerDeType();\n-    if (serdeClass.isPresent()) {\n-      String serde = serdeClass.get();\n-      HiveConf hiveConf = new HiveConf();\n+    if (!serdeClass.isPresent()) {\n+      return null;\n+    }\n \n-      try {\n-        deserializer = ReflectionUtils.newInstance(hiveConf.getClassByName(serde).asSubclass(Deserializer.class),\n-            hiveConf);\n-      } catch (ClassNotFoundException e) {\n-        LOG.warn(\"Serde class \" + serde + \" not found!\", e);\n-      }\n+    String serde = serdeClass.get();\n+    HiveConf hiveConf = new HiveConf();\n \n-      if (deserializer == null) {\n-        return deserializer;\n-      }\n+    Deserializer deserializer;\n+    try {\n+      deserializer = ReflectionUtils.newInstance(hiveConf.getClassByName(serde).asSubclass(Deserializer.class),\n+          hiveConf);\n+\n+    } catch (ClassNotFoundException e) {\n+      LOG.warn(\"Serde class \" + serde + \" not found!\", e);\n+      return null;\n+    }\n \n-      Properties props = new Properties();\n-      props.putAll(unit.getProps().getProperties());\n-      props.putAll(unit.getStorageProps().getProperties());\n-      props.putAll(unit.getSerDeProps().getProperties());\n+    Properties props = new Properties();\n+    props.putAll(unit.getProps().getProperties());\n+    props.putAll(unit.getStorageProps().getProperties());\n+    props.putAll(unit.getSerDeProps().getProperties());\n \n-      try {\n-        SerDeUtils.initializeSerDe(deserializer, hiveConf, props, null);\n-      } catch (SerDeException e) {\n-        LOG.warn(\"Failed to initialize serde \" + serde + \" with properties \" + props);\n-      }\n+    try {\n+      SerDeUtils.initializeSerDe(deserializer, hiveConf, props, null);\n+    } catch (SerDeException e) {\n+      LOG.warn(\"Failed to initialize serde \" + serde + \" with properties \" + props + \" for table \" + unit.getDbName() +\n+          \".\" + unit.getTableName());\n+      return null;\n     }\n \n     return deserializer;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "sha": "7a3940ecbde28970a3563adefda1629ff3ead138",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java?ref=a55a5722efe4ffc633a331e0d83c2a77eabd67d8",
                "deletions": 1,
                "filename": "gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "patch": "@@ -34,7 +34,7 @@\n \n public class HiveMetaStoreUtilsTest {\n   @Test\n-  public void testGetTable() {\n+  public void testGetTableAvro() {\n     final String databaseName = \"testdb\";\n     final String tableName = \"testtable\";\n \n@@ -69,4 +69,36 @@ public void testGetTable() {\n     Assert.assertEquals(fieldA.getName(), \"a\");\n     Assert.assertEquals(fieldA.getType(), \"int\");\n   }\n+\n+  @Test\n+  public void testGetTableAvroInvalidSchema() {\n+    final String databaseName = \"testdb\";\n+    final String tableName = \"testtable\";\n+\n+    HiveTable.Builder builder = new HiveTable.Builder();\n+\n+    builder.withDbName(databaseName).withTableName(tableName);\n+\n+    State serdeProps = new State();\n+    serdeProps.setProp(\"avro.schema.literal\", \"invalid schema\");\n+    builder.withSerdeProps(serdeProps);\n+\n+    HiveTable hiveTable = builder.build();\n+    hiveTable.setInputFormat(AvroContainerInputFormat.class.getName());\n+    hiveTable.setOutputFormat(AvroContainerOutputFormat.class.getName());\n+    hiveTable.setSerDeType(AvroSerDe.class.getName());\n+\n+    Table table = HiveMetaStoreUtils.getTable(hiveTable);\n+    Assert.assertEquals(table.getDbName(), databaseName);\n+    Assert.assertEquals(table.getTableName(), tableName);\n+\n+    StorageDescriptor sd = table.getSd();\n+    Assert.assertEquals(sd.getInputFormat(), AvroContainerInputFormat.class.getName());\n+    Assert.assertEquals(sd.getOutputFormat(), AvroContainerOutputFormat.class.getName());\n+    Assert.assertNotNull(sd.getSerdeInfo());\n+    Assert.assertEquals(sd.getSerdeInfo().getSerializationLib(), AvroSerDe.class.getName());\n+\n+    List<FieldSchema> fields = sd.getCols();\n+    Assert.assertTrue(fields != null && fields.size() == 0);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "sha": "ca188daea1a960f46ad15d5863b94139fd61b56f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gradle/scripts/defaultBuildProperties.gradle",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gradle/scripts/defaultBuildProperties.gradle?ref=a55a5722efe4ffc633a331e0d83c2a77eabd67d8",
                "deletions": 1,
                "filename": "gradle/scripts/defaultBuildProperties.gradle",
                "patch": "@@ -28,7 +28,7 @@ def BuildProperties BUILD_PROPERTIES = new BuildProperties(project)\n     .register(new BuildProperty(\"doNotSignArtifacts\", false, \"Do not sight Maven artifacts\"))\n     .register(new BuildProperty(\"gobblinFlavor\", \"standard\", \"Build flavor (see http://gobblin.readthedocs.io/en/latest/developer-guide/GobblinModules/)\"))\n     .register(new BuildProperty(\"hadoopVersion\", \"2.3.0\", \"Hadoop dependencies version\"))\n-    .register(new BuildProperty(\"hiveVersion\", \"1.0.1\", \"Hive dependencies version\"))\n+    .register(new BuildProperty(\"hiveVersion\", \"1.1.0\", \"Hive dependencies version\"))\n     .register(new BuildProperty(\"jdkVersion\", JavaVersion.VERSION_1_7.toString(),\n     \"Java languange compatibility; one of \" + JavaVersion.VERSION_1_7 + \", \" +\n         JavaVersion.VERSION_1_8))",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/a55a5722efe4ffc633a331e0d83c2a77eabd67d8/gradle/scripts/defaultBuildProperties.gradle",
                "sha": "c62e815bdac25f25dece1e3c19af2c8b711df533",
                "status": "modified"
            }
        ],
        "message": "Fix NullPointerException when Deserializer is not properly initialized",
        "parent": "https://github.com/apache/incubator-gobblin/commit/408d7d3b3099dc85d9ab2ddb5c1b5f887c50c4d9",
        "patched_files": [
            "HiveMetaStoreUtils.java",
            "defaultBuildProperties.gradle"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HiveMetaStoreUtilsTest.java"
        ]
    },
    "incubator-gobblin_a91e968": {
        "bug_id": "incubator-gobblin_a91e968",
        "commit": "https://github.com/apache/incubator-gobblin/commit/a91e968f320f623dcf57bd28d12cd3b951d08ba3",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/a91e968f320f623dcf57bd28d12cd3b951d08ba3/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java?ref=a91e968f320f623dcf57bd28d12cd3b951d08ba3",
                "deletions": 0,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "patch": "@@ -176,6 +176,12 @@ public void clean() throws IOException {\n     this.log.info(\"Cleaning dataset \" + this);\n \n     List<T> versions = Lists.newArrayList(getVersionFinder().findDatasetVersions(this));\n+\n+    if (versions.isEmpty()) {\n+      this.log.warn(\"No dataset version can be found. Ignoring.\");\n+      return;\n+    }\n+    \n     Collections.sort(versions, Collections.reverseOrder());\n \n     Collection<T> deletableVersions =",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/a91e968f320f623dcf57bd28d12cd3b951d08ba3/gobblin-data-management/src/main/java/gobblin/data/management/retention/dataset/DatasetBase.java",
                "sha": "a7580efc57cb227d0eef29b20fef71ad0e9a4d78",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/a91e968f320f623dcf57bd28d12cd3b951d08ba3/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java?ref=a91e968f320f623dcf57bd28d12cd3b951d08ba3",
                "deletions": 2,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "patch": "@@ -61,9 +61,9 @@ public FileLevelTimestampVersionFinder(FileSystem fs, Properties props) {\n       }\n       return timestampedVersions;\n     } catch (IOException e) {\n-      LOGGER.warn(\"Failed to get ModifiedTimStamp for candidate dataset version at \" + dataset.datasetRoot()\n+      LOGGER.warn(\"Failed to get ModifiedTimeStamp for candidate dataset version at \" + dataset.datasetRoot()\n           + \". Ignoring.\");\n-      return null;\n+      return Lists.newArrayList();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/a91e968f320f623dcf57bd28d12cd3b951d08ba3/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/FileLevelTimestampVersionFinder.java",
                "sha": "4a2d0ef88511eac1ee06556eaa4a22116afa4e2f",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #325 from ydailinkedin/master\n\nFix NPE when finding dataset versions",
        "parent": "https://github.com/apache/incubator-gobblin/commit/888ebcf52780139bc14d23f0d15be10fddeae71d",
        "patched_files": [
            "DatasetBase.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "DatasetBaseTest.java"
        ]
    },
    "incubator-gobblin_abd056c": {
        "bug_id": "incubator-gobblin_abd056c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/abd056c80a1513a39d816bb4bed97ade480b42e0",
        "file": [
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/abd056c80a1513a39d816bb4bed97ade480b42e0/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java?ref=abd056c80a1513a39d816bb4bed97ade480b42e0",
                "deletions": 1,
                "filename": "gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "patch": "@@ -17,16 +17,28 @@\n \n package gobblin.hive.metastore;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Properties;\n \n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.TableType;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.Deserializer;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeUtils;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Optional;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.primitives.Ints;\n@@ -49,6 +61,8 @@\n @Alpha\n public class HiveMetaStoreUtils {\n \n+  private static final Logger LOG = LoggerFactory.getLogger(HiveMetaStoreUtils.class);\n+\n   private static final TableType DEFAULT_TABLE_TYPE = TableType.EXTERNAL_TABLE;\n   private static final String EXTERNAL = \"EXTERNAL\";\n \n@@ -166,7 +180,7 @@ private static StorageDescriptor getStorageDescriptor(HiveRegistrationUnit unit)\n     State props = unit.getStorageProps();\n     StorageDescriptor sd = new StorageDescriptor();\n     sd.setParameters(getParameters(props));\n-    sd.setCols(getFieldSchemas(unit.getColumns()));\n+    sd.setCols(getFieldSchemas(unit));\n     if (unit.getLocation().isPresent()) {\n       sd.setLocation(unit.getLocation().get());\n     }\n@@ -297,4 +311,59 @@ private static State getSerDeProps(SerDeInfo si) {\n     return fieldSchemas;\n   }\n \n+  /**\n+   * First tries getting the {@code FieldSchema}s from the {@code HiveRegistrationUnit}'s columns, if set.\n+   * Else, gets the {@code FieldSchema}s from the deserializer.\n+   */\n+  private static List<FieldSchema> getFieldSchemas(HiveRegistrationUnit unit) {\n+    List<Column> columns = unit.getColumns();\n+    List<FieldSchema> fieldSchemas = new ArrayList<>();\n+    if (columns != null && columns.size() > 0) {\n+      fieldSchemas = getFieldSchemas(columns);\n+    } else {\n+      Deserializer deserializer = getDeserializer(unit);\n+      if (deserializer != null) {\n+        try {\n+          fieldSchemas = MetaStoreUtils.getFieldsFromDeserializer(unit.getTableName(), deserializer);\n+        } catch (SerDeException | MetaException e) {\n+          LOG.warn(\"Encountered exception while getting fields from deserializer.\", e);\n+        }\n+      }\n+    }\n+    return fieldSchemas;\n+  }\n+\n+  private static Deserializer getDeserializer(HiveRegistrationUnit unit) {\n+    Deserializer deserializer = null;\n+\n+    Optional<String> serdeClass = unit.getSerDeType();\n+    if (serdeClass.isPresent()) {\n+      String serde = serdeClass.get();\n+      HiveConf hiveConf = new HiveConf();\n+\n+      try {\n+        deserializer = ReflectionUtils.newInstance(hiveConf.getClassByName(serde).asSubclass(Deserializer.class),\n+            hiveConf);\n+      } catch (ClassNotFoundException e) {\n+        LOG.warn(\"Serde class \" + serde + \" not found!\", e);\n+      }\n+\n+      if (deserializer == null) {\n+        return deserializer;\n+      }\n+\n+      Properties props = new Properties();\n+      props.putAll(unit.getProps().getProperties());\n+      props.putAll(unit.getStorageProps().getProperties());\n+      props.putAll(unit.getSerDeProps().getProperties());\n+\n+      try {\n+        SerDeUtils.initializeSerDe(deserializer, hiveConf, props, null);\n+      } catch (SerDeException e) {\n+        LOG.warn(\"Failed to initialize serde \" + serde + \" with properties \" + props);\n+      }\n+    }\n+\n+    return deserializer;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/abd056c80a1513a39d816bb4bed97ade480b42e0/gobblin-hive-registration/src/main/java/gobblin/hive/metastore/HiveMetaStoreUtils.java",
                "sha": "d6f3d155d2e20bc86517b96101669fa45d53ab9d",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/abd056c80a1513a39d816bb4bed97ade480b42e0/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java?ref=abd056c80a1513a39d816bb4bed97ade480b42e0",
                "deletions": 0,
                "filename": "gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package gobblin.hive.metastore;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat;\n+import org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat;\n+import org.apache.hadoop.hive.serde2.avro.AvroSerDe;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import gobblin.configuration.State;\n+import gobblin.hive.HiveTable;\n+\n+\n+public class HiveMetaStoreUtilsTest {\n+  @Test\n+  public void testGetTable() {\n+    final String databaseName = \"testdb\";\n+    final String tableName = \"testtable\";\n+\n+    HiveTable.Builder builder = new HiveTable.Builder();\n+\n+    builder.withDbName(databaseName).withTableName(tableName);\n+\n+    State serdeProps = new State();\n+    serdeProps.setProp(\"avro.schema.literal\", \"{\\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"TestEvent\\\",\"\n+        + \" \\\"namespace\\\": \\\"test.namespace\\\", \\\"fields\\\": [{\\\"name\\\":\\\"a\\\",\"\n+        + \" \\\"type\\\": \\\"int\\\"}]}\");\n+    builder.withSerdeProps(serdeProps);\n+\n+    HiveTable hiveTable = builder.build();\n+    hiveTable.setInputFormat(AvroContainerInputFormat.class.getName());\n+    hiveTable.setOutputFormat(AvroContainerOutputFormat.class.getName());\n+    hiveTable.setSerDeType(AvroSerDe.class.getName());\n+\n+    Table table = HiveMetaStoreUtils.getTable(hiveTable);\n+    Assert.assertEquals(table.getDbName(), databaseName);\n+    Assert.assertEquals(table.getTableName(), tableName);\n+\n+    StorageDescriptor sd = table.getSd();\n+    Assert.assertEquals(sd.getInputFormat(), AvroContainerInputFormat.class.getName());\n+    Assert.assertEquals(sd.getOutputFormat(), AvroContainerOutputFormat.class.getName());\n+    Assert.assertNotNull(sd.getSerdeInfo());\n+    Assert.assertEquals(sd.getSerdeInfo().getSerializationLib(), AvroSerDe.class.getName());\n+\n+    List<FieldSchema> fields = sd.getCols();\n+    Assert.assertTrue(fields != null && fields.size() == 1);\n+    FieldSchema fieldA = fields.get(0);\n+    Assert.assertEquals(fieldA.getName(), \"a\");\n+    Assert.assertEquals(fieldA.getType(), \"int\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/abd056c80a1513a39d816bb4bed97ade480b42e0/gobblin-hive-registration/src/test/java/gobblin/hive/metastore/HiveMetaStoreUtilsTest.java",
                "sha": "8737b878efbb3ce188b85e5c83d5598727d0ac52",
                "status": "added"
            }
        ],
        "message": "Merge pull request #1602 from erwa/h26175-npe\n\nPopulate Table columns from Deserializer if not set in HiveRegistrationUnit columns",
        "parent": "https://github.com/apache/incubator-gobblin/commit/95d59e1c75e0637cf32405d2bbbc11755920d341",
        "patched_files": [
            "HiveMetaStoreUtils.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HiveMetaStoreUtilsTest.java"
        ]
    },
    "incubator-gobblin_b01ec9c": {
        "bug_id": "incubator-gobblin_b01ec9c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a/gobblin-modules/gobblin-crypto/src/main/java/org/apache/gobblin/crypto/GPGFileDecryptor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-modules/gobblin-crypto/src/main/java/org/apache/gobblin/crypto/GPGFileDecryptor.java?ref=b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a",
                "deletions": 0,
                "filename": "gobblin-modules/gobblin-crypto/src/main/java/org/apache/gobblin/crypto/GPGFileDecryptor.java",
                "patch": "@@ -178,6 +178,10 @@ public LazyMaterializeDecryptorInputStream(JcaPGPObjectFactory pgpFact)\n     @Override\n     public int read()\n         throws IOException {\n+      if (this.currentUnderlyingStream == null) {\n+        return -1;\n+      }\n+\n       int value = this.currentUnderlyingStream.read();\n \n       if (value != -1) {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a/gobblin-modules/gobblin-crypto/src/main/java/org/apache/gobblin/crypto/GPGFileDecryptor.java",
                "sha": "4c673c7e058cb7e2285c5b31063c790cff03219f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a/gobblin-modules/gobblin-crypto/src/test/java/org/apache/gobblin/crypto/GPGFileDecryptorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-modules/gobblin-crypto/src/test/java/org/apache/gobblin/crypto/GPGFileDecryptorTest.java?ref=b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a",
                "deletions": 0,
                "filename": "gobblin-modules/gobblin-crypto/src/test/java/org/apache/gobblin/crypto/GPGFileDecryptorTest.java",
                "patch": "@@ -86,6 +86,9 @@ public void decryptLargeFileSym() throws IOException, PGPException {\n \n       Assert.assertEquals(bytesRead, 1041981183L);\n \n+      // Make sure no error thrown if read again after reaching EOF\n+      Assert.assertEquals(is.read(), -1);\n+\n       System.gc();\n       System.gc();\n       long endHeapSize = Runtime.getRuntime().totalMemory();",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b01ec9cc991c5a2dbe422ffc2e8f17d28be1a96a/gobblin-modules/gobblin-crypto/src/test/java/org/apache/gobblin/crypto/GPGFileDecryptorTest.java",
                "sha": "05892fcfdfbf4426d2567b180c0acf29404daa83",
                "status": "modified"
            }
        ],
        "message": "[GOBBLIN-501] Fix NPE thrown from read after EOF of LazyMaterializeDecryptorInputStream\n\nCloses #2371 from zxcware/enc",
        "parent": "https://github.com/apache/incubator-gobblin/commit/88383014c483013e3035bc9ead8b15f998275155",
        "patched_files": [
            "GPGFileDecryptor.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "GPGFileDecryptorTest.java"
        ]
    },
    "incubator-gobblin_b11cfa8": {
        "bug_id": "incubator-gobblin_b11cfa8",
        "commit": "https://github.com/apache/incubator-gobblin/commit/b11cfa8b7cbd5e4763f9494915101ad26a711d57",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinClusterConfigurationKeys.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinClusterConfigurationKeys.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinClusterConfigurationKeys.java",
                "patch": "@@ -125,6 +125,8 @@\n   public static final String HELIX_WORKFLOW_EXPIRY_TIME_SECONDS = GOBBLIN_CLUSTER_PREFIX + \"workflow.expirySeconds\";\n   public static final long DEFAULT_HELIX_WORKFLOW_EXPIRY_TIME_SECONDS = 6 * 60 * 60;\n \n+  public static final String HELIX_JOB_STOP_TIMEOUT_SECONDS = GOBBLIN_CLUSTER_PREFIX + \"helix.job.stopTimeoutSeconds\";\n+  public static final long DEFAULT_HELIX_JOB_STOP_TIMEOUT_SECONDS = 10L;\n   public static final String TASK_RUNNER_SUITE_BUILDER = GOBBLIN_CLUSTER_PREFIX + \"taskRunnerSuite.builder\";\n \n   public static final String HELIX_JOB_TIMEOUT_ENABLED_KEY = \"helix.job.timeout.enabled\";",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinClusterConfigurationKeys.java",
                "sha": "b2bd6825047c2579b0208f99b72ff49c2798d3bc",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixDistributeJobExecutionLauncher.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixDistributeJobExecutionLauncher.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 2,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixDistributeJobExecutionLauncher.java",
                "patch": "@@ -30,6 +30,7 @@\n import java.util.concurrent.TimeoutException;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.helix.HelixException;\n import org.apache.helix.HelixManager;\n import org.apache.helix.task.JobConfig;\n import org.apache.helix.task.TaskConfig;\n@@ -94,6 +95,8 @@\n \n   private final long workFlowExpiryTimeSeconds;\n \n+  private final long helixJobStopTimeoutSeconds;\n+\n   private boolean jobSubmitted;\n \n   // A conditional variable for which the condition is satisfied if a cancellation is requested\n@@ -128,6 +131,10 @@ public GobblinHelixDistributeJobExecutionLauncher(Builder builder) throws Except\n         GobblinClusterConfigurationKeys.DEFAULT_HELIX_WORKFLOW_EXPIRY_TIME_SECONDS);\n     this.planningJobLauncherMetrics = builder.planningJobLauncherMetrics;\n     this.helixMetrics = builder.helixMetrics;\n+\n+    this.helixJobStopTimeoutSeconds = ConfigUtils.getLong(combined,\n+        GobblinClusterConfigurationKeys.HELIX_JOB_STOP_TIMEOUT_SECONDS,\n+        GobblinClusterConfigurationKeys.DEFAULT_HELIX_JOB_STOP_TIMEOUT_SECONDS);\n   }\n \n   @Override\n@@ -142,11 +149,16 @@ private void executeCancellation() {\n           // TODO : fix this when HELIX-1180 is completed\n           // work flow should never be deleted explicitly because it has a expiry time\n           // If cancellation is requested, we should set the job state to CANCELLED/ABORT\n-          this.helixTaskDriver.waitToStop(planningJobId, 10000L);\n+          this.helixTaskDriver.waitToStop(planningJobId, this.helixJobStopTimeoutSeconds);\n           log.info(\"Stopped the workflow \", planningJobId);\n         }\n+      } catch (HelixException e) {\n+        // Cancellation may throw an exception, but Helix set the job state to STOP and it should eventually stop\n+        // We will keep this.cancellationExecuted and this.cancellationRequested to true and not propagate the exception\n+        log.error(\"Failed to stop workflow {} in Helix\", planningJobId, e);\n       } catch (InterruptedException e) {\n-        throw new RuntimeException(\"Failed to stop workflow \" + planningJobId + \" in Helix\", e);\n+        log.error(\"Thread interrupted while trying to stop the workflow {} in Helix\", planningJobId);\n+        Thread.currentThread().interrupt();\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixDistributeJobExecutionLauncher.java",
                "sha": "bc8443d9a1f45c4b6f216a07c9febc69bdae9d0e",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixJobLauncher.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixJobLauncher.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 2,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixJobLauncher.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.helix.HelixException;\n import org.apache.helix.HelixManager;\n import org.apache.helix.task.JobConfig;\n import org.apache.helix.task.JobQueue;\n@@ -123,6 +124,7 @@\n   private final StateStores stateStores;\n   private final Config jobConfig;\n   private final long workFlowExpiryTimeSeconds;\n+  private final long helixJobStopTimeoutSeconds;\n \n   public GobblinHelixJobLauncher (Properties jobProps,\n                                   final HelixManager helixManager,\n@@ -153,6 +155,10 @@ public GobblinHelixJobLauncher (Properties jobProps,\n         GobblinClusterConfigurationKeys.HELIX_WORKFLOW_EXPIRY_TIME_SECONDS,\n         GobblinClusterConfigurationKeys.DEFAULT_HELIX_WORKFLOW_EXPIRY_TIME_SECONDS);\n \n+    this.helixJobStopTimeoutSeconds = ConfigUtils.getLong(jobConfig,\n+        GobblinClusterConfigurationKeys.HELIX_JOB_STOP_TIMEOUT_SECONDS,\n+        GobblinClusterConfigurationKeys.DEFAULT_HELIX_JOB_STOP_TIMEOUT_SECONDS);\n+\n     Config stateStoreJobConfig = ConfigUtils.propertiesToConfig(jobProps)\n         .withValue(ConfigurationKeys.STATE_STORE_FS_URI_KEY, ConfigValueFactory.fromAnyRef(\n             new URI(appWorkDir.toUri().getScheme(), null, appWorkDir.toUri().getHost(),\n@@ -242,11 +248,16 @@ protected void executeCancellation() {\n           // TODO : fix this when HELIX-1180 is completed\n           // work flow should never be deleted explicitly because it has a expiry time\n           // If cancellation is requested, we should set the job state to CANCELLED/ABORT\n-          this.helixTaskDriver.waitToStop(this.helixWorkFlowName, 10000L);\n+          this.helixTaskDriver.waitToStop(this.helixWorkFlowName, this.helixJobStopTimeoutSeconds);\n           log.info(\"stopped the workflow \", this.helixWorkFlowName);\n         }\n+      } catch (HelixException e) {\n+        // Cancellation may throw an exception, but Helix set the job state to STOP and it should eventually stop\n+        // We will keep this.cancellationExecuted and this.cancellationRequested to true and not propagate the exception\n+        log.error(\"Failed to stop workflow {} in Helix\", helixWorkFlowName, e);\n       } catch (InterruptedException e) {\n-        throw new RuntimeException(\"Failed to stop workflow \" + helixWorkFlowName + \" in Helix\", e);\n+        log.error(\"Thread interrupted while trying to stop the workflow {} in Helix\", helixWorkFlowName);\n+        Thread.currentThread().interrupt();\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixJobLauncher.java",
                "sha": "3389f841352cc5645b9e7cf6b48cabd3bcaa5e34",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTask.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 2,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTask.java",
                "patch": "@@ -76,8 +76,7 @@\n   public GobblinHelixTask(TaskRunnerSuiteBase.Builder builder,\n                           TaskCallbackContext taskCallbackContext,\n                           TaskAttemptBuilder taskAttemptBuilder,\n-                          StateStores stateStores)  throws IOException {\n-\n+                          StateStores stateStores) {\n     this.taskConfig = taskCallbackContext.getTaskConfig();\n     this.applicationName = builder.getApplicationName();\n     this.instanceName = builder.getInstanceName();\n@@ -127,6 +126,7 @@ public TaskResult run() {\n \n   @Override\n   public void cancel() {\n+    log.warn(\"Gobblin helix task cancellation invoked.\");\n     this.task.cancel();\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTask.java",
                "sha": "c93d9ac68051fd9d7a9bd9f2964d3ce2356c0dd7",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTaskFactory.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTaskFactory.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 8,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTaskFactory.java",
                "patch": "@@ -98,14 +98,9 @@ private TaskAttemptBuilder createTaskAttemptBuilder() {\n \n   @Override\n   public Task createNewTask(TaskCallbackContext context) {\n-    try {\n-      if (this.newTasksCounter.isPresent()) {\n-        this.newTasksCounter.get().inc();\n-      }\n-      return new GobblinHelixTask(builder, context, this.taskAttemptBuilder, this.stateStores);\n-    } catch (IOException ioe) {\n-      LOGGER.error(\"Failed to create a new GobblinHelixTask\", ioe);\n-      throw Throwables.propagate(ioe);\n+    if (this.newTasksCounter.isPresent()) {\n+      this.newTasksCounter.get().inc();\n     }\n+    return new GobblinHelixTask(builder, context, this.taskAttemptBuilder, this.stateStores);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/GobblinHelixTaskFactory.java",
                "sha": "73ca784ceef581a16122033e33c98fcb882bd36a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/HelixUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/HelixUtils.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 1,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/HelixUtils.java",
                "patch": "@@ -159,7 +159,7 @@ static void waitJobCompletion(HelixManager helixManager, String workFlowName, St\n           case COMPLETED:\n           return;\n           default:\n-            log.info(\"Waiting for job {} to complete...\", jobName);\n+            log.info(\"Waiting for job {} to complete... State - {}\", jobName, jobState);\n             Thread.sleep(1000);\n         }\n       } else {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/HelixUtils.java",
                "sha": "f67e77c037afdfed374b698db8124e5fd11204c0",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/SleepingTask.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/SleepingTask.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/main/java/org/apache/gobblin/cluster/SleepingTask.java",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.gobblin.cluster;\n+\n+import avro.shaded.com.google.common.base.Throwables;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import org.apache.gobblin.runtime.TaskContext;\n+import org.apache.gobblin.runtime.task.BaseAbstractTask;\n+\n+@Slf4j\n+public class SleepingTask extends BaseAbstractTask {\n+  private final long sleepTime;\n+\n+  public SleepingTask(TaskContext taskContext) {\n+    super(taskContext);\n+    sleepTime = taskContext.getTaskState().getPropAsLong(\"data.publisher.sleep.time.in.seconds\", 10L);\n+  }\n+\n+  @Override\n+  public void run() {\n+    try {\n+      long endTime = System.currentTimeMillis() + sleepTime * 1000;\n+      while (System.currentTimeMillis() <= endTime) {\n+        Thread.sleep(1000L);\n+        log.warn(\"Sleeping for {} seconds\", sleepTime);\n+      }\n+      log.info(\"Hello World!\");\n+      super.run();\n+    } catch (InterruptedException e) {\n+      log.error(\"Sleep interrupted.\");\n+      Thread.currentThread().interrupt();\n+      Throwables.propagate(e);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/main/java/org/apache/gobblin/cluster/SleepingTask.java",
                "sha": "55750c0b72e0bc31cab735d9feaba6156b0f4a6d",
                "status": "added"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/ClusterIntegrationTest.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/ClusterIntegrationTest.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 1,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/ClusterIntegrationTest.java",
                "patch": "@@ -19,17 +19,27 @@\n \n import java.io.IOException;\n \n+import org.apache.helix.HelixManager;\n+import org.apache.helix.HelixManagerFactory;\n+import org.apache.helix.InstanceType;\n+import org.apache.helix.task.TaskDriver;\n import org.testng.annotations.AfterMethod;\n import org.testng.annotations.Test;\n \n+import com.typesafe.config.Config;\n+\n+import lombok.extern.slf4j.Slf4j;\n+\n import org.apache.gobblin.cluster.suite.IntegrationBasicSuite;\n import org.apache.gobblin.cluster.suite.IntegrationDedicatedManagerClusterSuite;\n import org.apache.gobblin.cluster.suite.IntegrationDedicatedTaskDriverClusterSuite;\n+import org.apache.gobblin.cluster.suite.IntegrationJobCancelSuite;\n import org.apache.gobblin.cluster.suite.IntegrationJobFactorySuite;\n import org.apache.gobblin.cluster.suite.IntegrationJobTagSuite;\n import org.apache.gobblin.cluster.suite.IntegrationSeparateProcessSuite;\n+import org.apache.gobblin.util.ConfigUtils;\n \n-\n+@Slf4j\n public class ClusterIntegrationTest {\n \n   private IntegrationBasicSuite suite;\n@@ -41,6 +51,37 @@ public void testJobShouldComplete()\n     runAndVerify();\n   }\n \n+  @Test void testJobShouldGetCancelled() throws Exception {\n+    this.suite =new IntegrationJobCancelSuite();\n+    Config helixConfig = this.suite.getManagerConfig();\n+    String clusterName = helixConfig.getString(GobblinClusterConfigurationKeys.HELIX_CLUSTER_NAME_KEY);\n+    String instanceName = ConfigUtils.getString(helixConfig, GobblinClusterConfigurationKeys.HELIX_INSTANCE_NAME_KEY,\n+        GobblinClusterManager.class.getSimpleName());\n+    String zkConnectString = helixConfig.getString(GobblinClusterConfigurationKeys.ZK_CONNECTION_STRING_KEY);\n+    HelixManager helixManager = HelixManagerFactory.getZKHelixManager(clusterName, instanceName, InstanceType.CONTROLLER, zkConnectString);\n+\n+    suite.startCluster();\n+\n+    helixManager.connect();\n+\n+    TaskDriver taskDriver = new TaskDriver(helixManager);\n+\n+    while (TaskDriver.getWorkflowContext(helixManager, IntegrationJobCancelSuite.JOB_ID) == null) {\n+      log.warn(\"Waiting for the job to start...\");\n+      Thread.sleep(1000L);\n+    }\n+\n+    // Give the job some time to reach writer, where it sleeps\n+    Thread.sleep(2000L);\n+\n+    log.info(\"Stopping the job\");\n+    taskDriver.stop(IntegrationJobCancelSuite.JOB_ID);\n+\n+    suite.shutdownCluster();\n+\n+    suite.waitForAndVerifyOutputFiles();\n+  }\n+\n   @Test\n   public void testSeparateProcessMode()\n       throws Exception {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/ClusterIntegrationTest.java",
                "sha": "9d5d602b428dea5d11ca31272b31f05c21353458",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingCustomTaskSource.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingCustomTaskSource.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingCustomTaskSource.java",
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.gobblin.cluster;\n+\n+import java.util.List;\n+\n+import org.apache.gobblin.configuration.SourceState;\n+import org.apache.gobblin.runtime.task.TaskUtils;\n+import org.apache.gobblin.source.workunit.WorkUnit;\n+import org.apache.gobblin.util.test.HelloWorldSource;\n+\n+\n+public class SleepingCustomTaskSource extends HelloWorldSource {\n+  @Override\n+  public List<WorkUnit> getWorkunits(SourceState state) {\n+    List<WorkUnit> workUnits = super.getWorkunits(state);\n+    for (WorkUnit workUnit : workUnits) {\n+      TaskUtils.setTaskFactoryClass(workUnit, SleepingTaskFactory.class);\n+    }\n+    return workUnits;\n+  }\n+}\n+\n+",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingCustomTaskSource.java",
                "sha": "12e2224bb59a98e29965b11b1b99a9759c4b29aa",
                "status": "added"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingTaskFactory.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingTaskFactory.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingTaskFactory.java",
                "patch": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.gobblin.cluster;\n+\n+import org.apache.gobblin.publisher.DataPublisher;\n+import org.apache.gobblin.publisher.NoopPublisher;\n+import org.apache.gobblin.runtime.JobState;\n+import org.apache.gobblin.runtime.TaskContext;\n+import org.apache.gobblin.runtime.task.TaskFactory;\n+import org.apache.gobblin.runtime.task.TaskIFace;\n+\n+\n+public class SleepingTaskFactory implements TaskFactory {\n+  @Override\n+  public TaskIFace createTask(TaskContext taskContext) {\n+    return new SleepingTask(taskContext);\n+  }\n+\n+  @Override\n+  public DataPublisher createDataPublisher(JobState.DatasetState datasetState) {\n+    return new NoopPublisher(datasetState);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/SleepingTaskFactory.java",
                "sha": "1a3901f4a2d6dba58040e0626a4297a7b86f5ce9",
                "status": "added"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationBasicSuite.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationBasicSuite.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 1,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationBasicSuite.java",
                "patch": "@@ -34,6 +34,7 @@\n import java.util.Collection;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.Scanner;\n \n import org.apache.commons.io.FileUtils;\n import org.apache.curator.test.TestingServer;\n@@ -50,6 +51,7 @@\n import com.typesafe.config.ConfigRenderOptions;\n import com.typesafe.config.ConfigSyntax;\n \n+import lombok.Getter;\n import lombok.extern.slf4j.Slf4j;\n \n import org.apache.gobblin.cluster.ClusterIntegrationTest;\n@@ -81,6 +83,8 @@\n   protected Collection<GobblinTaskRunner> taskDrivers = Lists.newArrayList();\n   protected GobblinClusterManager manager;\n \n+  // This filename should match the log file specified in log4j.xml\n+  public static Path jobLogOutputFile = Paths.get(\"gobblin-integration-test-log-dir/gobblin-cluster-test.log\");;\n   protected Path workPath;\n   protected Path jobConfigPath;\n   protected Path jobOutputBasePath;\n@@ -183,7 +187,7 @@ protected Config getClusterConfig() {\n     return overrideConfig.withFallback(config);\n   }\n \n-  protected Config getManagerConfig() {\n+  public Config getManagerConfig() {\n     // manager config initialization\n     URL url = Resources.getResource(\"BasicManager.conf\");\n     Config managerConfig = ConfigFactory.parseURL(url);\n@@ -217,6 +221,18 @@ public void waitForAndVerifyOutputFiles() throws Exception {\n     asserter.assertTrue(this::hasExpectedFilesBeenCreated, \"Waiting for job-completion\");\n   }\n \n+  /**\n+   * verify if the file containts the provided message\n+   * @param logFile file to be looked inside\n+   * @param message string to look for\n+   * @return true if the file contains the message\n+   * @throws IOException\n+   */\n+  static boolean verifyFileForMessage(Path logFile, String message) throws IOException {\n+    String content = new String(Files.readAllBytes(logFile));\n+    return content.contains(message);\n+  }\n+\n   protected boolean hasExpectedFilesBeenCreated(Void input) {\n     int numOfFiles = getNumOfOutputFiles(this.jobOutputBasePath);\n     return numOfFiles == 1;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationBasicSuite.java",
                "sha": "23c18a63608a903ccf43609418d13e84c851e565",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedManagerClusterSuite.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedManagerClusterSuite.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 1,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedManagerClusterSuite.java",
                "patch": "@@ -44,7 +44,7 @@ public void createHelixCluster() throws Exception {\n   }\n \n   @Override\n-  protected Config getManagerConfig() {\n+  public Config getManagerConfig() {\n     Map<String, String> configMap = new HashMap<>();\n     configMap.put(GobblinClusterConfigurationKeys.DEDICATED_MANAGER_CLUSTER_ENABLED, \"true\");\n     configMap.put(GobblinClusterConfigurationKeys.MANAGER_CLUSTER_NAME_KEY, \"ManagerCluster\");",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedManagerClusterSuite.java",
                "sha": "37f6303badd89f283f9973d940053cb014854d02",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedTaskDriverClusterSuite.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedTaskDriverClusterSuite.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 1,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedTaskDriverClusterSuite.java",
                "patch": "@@ -59,7 +59,7 @@ public void createHelixCluster() throws Exception {\n   }\n \n   @Override\n-  protected Config getManagerConfig() {\n+  public Config getManagerConfig() {\n     Map<String, String> configMap = new HashMap<>();\n     configMap.put(GobblinClusterConfigurationKeys.DEDICATED_MANAGER_CLUSTER_ENABLED, \"true\");\n     configMap.put(GobblinClusterConfigurationKeys.MANAGER_CLUSTER_NAME_KEY, \"ManagerCluster\");",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationDedicatedTaskDriverClusterSuite.java",
                "sha": "01597fba5cb31f7034be1083ac4d2a8d07c62c29",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationJobCancelSuite.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationJobCancelSuite.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationJobCancelSuite.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.gobblin.cluster.suite;\n+\n+import java.util.Map;\n+\n+import org.junit.Assert;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.gobblin.cluster.GobblinClusterConfigurationKeys;\n+import org.apache.gobblin.configuration.ConfigurationKeys;\n+\n+\n+public class IntegrationJobCancelSuite extends IntegrationBasicSuite {\n+  public static final String JOB_ID = \"job_HelloWorldTestJob_1234\";\n+\n+  @Override\n+  protected Map<String, Config> overrideJobConfigs(Config rawJobConfig) {\n+    Config newConfig = ConfigFactory.parseMap(ImmutableMap.of(\n+        ConfigurationKeys.SOURCE_CLASS_KEY, \"org.apache.gobblin.cluster.SleepingCustomTaskSource\",\n+        ConfigurationKeys.JOB_ID_KEY, JOB_ID,\n+        GobblinClusterConfigurationKeys.HELIX_JOB_TIMEOUT_ENABLED_KEY, Boolean.TRUE,\n+        GobblinClusterConfigurationKeys.HELIX_JOB_TIMEOUT_SECONDS, 10L))\n+        .withFallback(rawJobConfig);\n+    return ImmutableMap.of(\"HelloWorldJob\", newConfig);\n+  }\n+\n+  @Override\n+  public void waitForAndVerifyOutputFiles() throws Exception {\n+    // If the job is cancelled, it should not have been able to write 'Hello World!'\n+    Assert.assertFalse(verifyFileForMessage(this.jobLogOutputFile, \"Hello World!\"));\n+    Assert.assertFalse(verifyFileForMessage(this.jobLogOutputFile, \"java.lang.NullPointerException\"));\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/java/org/apache/gobblin/cluster/suite/IntegrationJobCancelSuite.java",
                "sha": "7961bf89fb9584b0f7282beb4095936af9146817",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/resources/log4j.xml",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-cluster/src/test/resources/log4j.xml?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-cluster/src/test/resources/log4j.xml",
                "patch": "@@ -27,8 +27,20 @@\n     </layout>\n   </appender>\n \n+  <appender name=\"file\" class=\"org.apache.log4j.RollingFileAppender\">\n+    <param name=\"append\" value=\"false\" />\n+    <param name=\"maxFileSize\" value=\"1MB\" />\n+    <param name=\"maxBackupIndex\" value=\"5\" />\n+    <param name=\"file\" value=\"gobblin-integration-test-log-dir/gobblin-cluster-test.log\" />\n+    <layout class=\"org.apache.log4j.PatternLayout\">\n+      <param name=\"ConversionPattern\"\n+             value=\"%d{yyyy-MM-dd HH:mm:ss z} %-5p [%t] %C %L - %m%n\" />\n+    </layout>\n+  </appender>\n+\n   <root>\n     <level value=\"info\" />\n+    <appender-ref ref=\"file\" />\n     <appender-ref ref=\"console\" />\n   </root>\n ",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-cluster/src/test/resources/log4j.xml",
                "sha": "12b4ff999d7ed4e6aff519e61522a0944a742efa",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/Task.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/Task.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 2,
                "filename": "gobblin-runtime/src/main/java/org/apache/gobblin/runtime/Task.java",
                "patch": "@@ -45,7 +45,6 @@\n import com.typesafe.config.Config;\n import com.typesafe.config.ConfigFactory;\n \n-import javax.annotation.Nullable;\n import lombok.NoArgsConstructor;\n \n import org.apache.gobblin.Constructs;\n@@ -165,7 +164,7 @@\n   private final AtomicBoolean shutdownRequested;\n   private volatile long shutdownRequestedTime = Long.MAX_VALUE;\n   private final CountDownLatch shutdownLatch;\n-  private Future<?> taskFuture;\n+  protected Future<?> taskFuture;\n \n   /**\n    * Instantiate a new {@link Task}.",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/Task.java",
                "sha": "049a3ff629c5e337fba821d1dd4389adeb19d5f6",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/task/TaskIFaceWrapper.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/task/TaskIFaceWrapper.java?ref=b11cfa8b7cbd5e4763f9494915101ad26a711d57",
                "deletions": 0,
                "filename": "gobblin-runtime/src/main/java/org/apache/gobblin/runtime/task/TaskIFaceWrapper.java",
                "patch": "@@ -170,4 +170,20 @@ protected void submitTaskCommittedEvent() {\n   public boolean isSpeculativeExecutionSafe() {\n     return this.underlyingTask.isSpeculativeExecutionSafe();\n   }\n+\n+  /**\n+   * return true if the task is successfully cancelled.\n+   * This method is a copy of the method in parent class.\n+   * We need this copy so TaskIFaceWrapper variables are not shared between this class and its parent class\n+   * @return\n+   */\n+  @Override\n+  public synchronized boolean cancel() {\n+    if (this.taskFuture != null && this.taskFuture.cancel(true)) {\n+      this.taskStateTracker.onTaskRunCompletion(this);\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/b11cfa8b7cbd5e4763f9494915101ad26a711d57/gobblin-runtime/src/main/java/org/apache/gobblin/runtime/task/TaskIFaceWrapper.java",
                "sha": "baf1d7a345dd10b0d1cc15e38cf144482a01bc19",
                "status": "modified"
            }
        ],
        "message": "[GOBBLIN-593] fix NPE in task cancel\n\nfix NPE in task cancel\n\naddress review comments\n\naddress review comments\n\naddress review comments\n\nadd an integration task to test custom task\ncancellation\n\nmerge conflicts\n\nfix method name\n\naddress review comments\n\nCloses #2459 from arjun4084346/NPEinTaskCancel",
        "parent": "https://github.com/apache/incubator-gobblin/commit/757b25b611b195865328b52c3a4eb4cfa214e2af",
        "patched_files": [
            "TaskIFaceWrapper.java",
            "GobblinHelixDistributeJobExecutionLauncher.java",
            "IntegrationDedicatedTaskDriverClusterSuite.java",
            "IntegrationBasicSuite.java",
            "IntegrationDedicatedManagerClusterSuite.java",
            "SleepingCustomTaskSource.java",
            "SleepingTaskFactory.java",
            "GobblinHelixTask.java",
            "GobblinHelixJobLauncher.java",
            "HelixUtils.java",
            "GobblinHelixTaskFactory.java",
            "Task.java",
            "SleepingTask.java",
            "GobblinClusterConfigurationKeys.java",
            "log4j.xml",
            "IntegrationJobCancelSuite.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "HelixUtilsTest.java",
            "GobblinHelixTaskTest.java",
            "ClusterIntegrationTest.java",
            "TaskTest.java",
            "GobblinHelixJobLauncherTest.java"
        ]
    },
    "incubator-gobblin_c1c94c2": {
        "bug_id": "incubator-gobblin_c1c94c2",
        "commit": "https://github.com/apache/incubator-gobblin/commit/c1c94c2e1383f8acd13756a02533c71afb14b362",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java?ref=c1c94c2e1383f8acd13756a02533c71afb14b362",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "patch": "@@ -44,6 +44,11 @@ public Path globVersionPattern() {\n \n   @Override\n   public TimestampedDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new TimestampedDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.TimestampedDatasetVersion timestampedDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (timestampedDatasetVersion != null) {\n+      return new TimestampedDatasetVersion(timestampedDatasetVersion);\n+    }\n+    return null;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/GlobModTimeDatasetVersionFinder.java",
                "sha": "a4dd6f3e63b6b814c821d2f8aa1814ba201e66f1",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java?ref=c1c94c2e1383f8acd13756a02533c71afb14b362",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "patch": "@@ -42,7 +42,12 @@ public Path globVersionPattern() {\n \n   @Override\n   public TimestampedDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new TimestampedDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.TimestampedDatasetVersion timestampedDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (timestampedDatasetVersion != null) {\n+      return new TimestampedDatasetVersion(timestampedDatasetVersion);\n+    }\n+    return null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/UnixTimestampVersionFinder.java",
                "sha": "c0fdd47f889488e402387203c51f4c5f4bbb2f71",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java?ref=c1c94c2e1383f8acd13756a02533c71afb14b362",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "patch": "@@ -45,6 +45,11 @@ public Path globVersionPattern() {\n \n   @Override\n   public StringDatasetVersion getDatasetVersion(Path pathRelativeToDatasetRoot, Path fullPath) {\n-    return new StringDatasetVersion(this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath));\n+    gobblin.data.management.version.StringDatasetVersion stringDatasetVersion =\n+        this.realVersionFinder.getDatasetVersion(pathRelativeToDatasetRoot, fullPath);\n+    if (stringDatasetVersion != null) {\n+      return new StringDatasetVersion(stringDatasetVersion);\n+    }\n+    return null;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/c1c94c2e1383f8acd13756a02533c71afb14b362/gobblin-data-management/src/main/java/gobblin/data/management/retention/version/finder/WatermarkDatasetVersionFinder.java",
                "sha": "5167a2cdd887d99b1cb2263e3bac620ed80caa98",
                "status": "modified"
            }
        ],
        "message": "Fix NPE",
        "parent": "https://github.com/apache/incubator-gobblin/commit/23b7008349cb2aa208b778a46270937354100850",
        "patched_files": [
            "UnixTimestampVersionFinder.java",
            "WatermarkDatasetVersionFinder.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "UnixTimestampVersionFinderTest.java",
            "WatermarkDatasetVersionFinderTest.java"
        ]
    },
    "incubator-gobblin_cbadb91": {
        "bug_id": "incubator-gobblin_cbadb91",
        "commit": "https://github.com/apache/incubator-gobblin/commit/cbadb917809d24c326051939384a57ef4b79e91c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-config-management/gobblin-config-client/src/main/java/gobblin/config/client/ConfigClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-config-management/gobblin-config-client/src/main/java/gobblin/config/client/ConfigClient.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 0,
                "filename": "gobblin-config-management/gobblin-config-client/src/main/java/gobblin/config/client/ConfigClient.java",
                "patch": "@@ -301,6 +301,7 @@ private ConfigStoreAccessor createNewConfigStoreAccessor(URI configKeyURI)\n     }\n \n     String currentVersion = cs.getCurrentVersion();\n+    LOG.info(\"Current config store version number: \" + currentVersion);\n     // topology related\n     ConfigStoreBackedTopology csTopology = new ConfigStoreBackedTopology(cs, currentVersion);\n     InMemoryTopology inMemoryTopology = new InMemoryTopology(csTopology);",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-config-management/gobblin-config-client/src/main/java/gobblin/config/client/ConfigClient.java",
                "sha": "cee2d12ddfc89010b2c04f8654369cf98026161f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopySource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopySource.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/CopySource.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.net.URI;\n+import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n@@ -113,7 +114,6 @@\n   public static final String MAX_WORK_UNITS_PER_BIN = CopyConfiguration.COPY_PREFIX + \".binPacking.maxWorkUnitsPerBin\";\n \n   private static final String WORK_UNIT_WEIGHT = CopyConfiguration.COPY_PREFIX + \".workUnitWeight\";\n-\n   private final WorkUnitWeighter weighter = new FieldWeighter(WORK_UNIT_WEIGHT);\n \n   public MetricContext metricContext;",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopySource.java",
                "sha": "28e99734ac7869505db205c87f12c9c79111f5d9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 2,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java",
                "patch": "@@ -89,9 +89,9 @@ private void calculateDatasetURN(){\n       } catch (IOException e1) {\n         // ignored\n       }\n+    } else {\n+      this.datasetURN = e.toString();\n     }\n-\n-    this.datasetURN = e.toString();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java",
                "sha": "942292e3a75dcaeab4ce555763757a1296f3d646",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 16,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java",
                "patch": "@@ -17,15 +17,6 @@\n \n package gobblin.data.management.copy.replication;\n \n-import com.google.common.base.Function;\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Iterators;\n-import com.typesafe.config.Config;\n-import gobblin.data.management.copy.CopySource;\n-import gobblin.dataset.Dataset;\n-import gobblin.util.Either;\n-import gobblin.util.ExecutorsUtils;\n-import gobblin.util.executors.IteratorExecutor;\n import java.io.IOException;\n import java.net.URI;\n import java.net.URISyntaxException;\n@@ -39,17 +30,22 @@\n import java.util.Properties;\n import java.util.Set;\n import java.util.TreeSet;\n-\n import java.util.concurrent.Callable;\n import java.util.concurrent.CopyOnWriteArrayList;\n import java.util.concurrent.ExecutionException;\n+import java.util.stream.Collectors;\n+\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n import com.google.common.base.Optional;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Splitter;\n import com.google.common.collect.ImmutableSet;\n+import com.google.common.base.Function;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterators;\n+import com.typesafe.config.Config;\n \n import gobblin.config.client.ConfigClient;\n import gobblin.config.client.api.ConfigStoreFactoryDoesNotExistsException;\n@@ -59,6 +55,12 @@\n import gobblin.configuration.ConfigurationKeys;\n import gobblin.dataset.DatasetsFinder;\n import gobblin.util.PathUtils;\n+import gobblin.data.management.copy.CopyConfiguration;\n+import gobblin.data.management.copy.CopySource;\n+import gobblin.dataset.Dataset;\n+import gobblin.util.Either;\n+import gobblin.util.ExecutorsUtils;\n+import gobblin.util.executors.IteratorExecutor;\n import lombok.extern.slf4j.Slf4j;\n \n \n@@ -86,6 +88,14 @@\n   public static final String GOBBLIN_CONFIG_STORE_DATASET_COMMON_ROOT =\n       ConfigurationKeys.CONFIG_BASED_PREFIX + \".dataset.common.root\";\n \n+  // In addition to the white/blacklist tags, this configuration let the user to black/whitelist some datasets\n+  // in the job-level configuration, which is not in configStore\n+  // as to have easier approach to black/whitelist some datasets.\n+  // The semantics keep still as tag, which the blacklist override whitelist if any dataset in common.\n+  public static final String JOB_LEVEL_BLACKLIST = CopyConfiguration.COPY_PREFIX + \".configBased.blacklist\" ;\n+  public static final String JOB_LEVEL_WHITELIST = CopyConfiguration.COPY_PREFIX + \".configBased.whitelist\" ;\n+\n+\n   protected final String storeRoot;\n   protected final Path commonRoot;\n   protected final Path whitelistTag;\n@@ -94,6 +104,9 @@\n   protected final Properties props;\n   private final int threadPoolSize;\n \n+  private Optional<List<String>> blacklistURNs;\n+  private Optional<List<String>> whitelistURNs;\n+\n \n   public ConfigBasedDatasetsFinder(FileSystem fs, Properties jobProps) throws IOException {\n     // ignore the input FileSystem , the source file system could be different for different datasets\n@@ -129,14 +142,34 @@ public ConfigBasedDatasetsFinder(FileSystem fs, Properties jobProps) throws IOEx\n \n     configClient = ConfigClient.createConfigClient(VersionStabilityPolicy.WEAK_LOCAL_STABILITY);\n     this.props = jobProps;\n+\n+\n+    if (props.containsKey(JOB_LEVEL_BLACKLIST)) {\n+      this.blacklistURNs = Optional.of(Splitter.on(\",\").omitEmptyStrings().splitToList(props.getProperty(JOB_LEVEL_BLACKLIST)));\n+    } else {\n+      this.blacklistURNs = Optional.absent();\n+    }\n+\n+    if (props.containsKey(JOB_LEVEL_WHITELIST)) {\n+      this.whitelistURNs = Optional.of(Splitter.on(\",\").omitEmptyStrings().splitToList(props.getProperty(JOB_LEVEL_WHITELIST)));\n+    } else {\n+      this.whitelistURNs = Optional.absent();\n+    }\n   }\n \n   protected Set<URI> getValidDatasetURIs(Path datasetCommonRoot) {\n     Collection<URI> allDatasetURIs;\n-    Set<URI> disabledURISet = ImmutableSet.of();\n+    Set<URI> disabledURISet = new HashSet();\n+    if (this.blacklistURNs.isPresent()) {\n+      for(String urn : this.blacklistURNs.get()) {\n+        disabledURISet.add(this.datasetURNtoURI(urn));\n+      }\n+    }\n \n     try {\n-      allDatasetURIs = configClient.getImportedBy(new URI(whitelistTag.toString()), true);\n+      // get all the URIs which imports {@link #replicationTag} or all from whitelistURNs\n+      allDatasetURIs = this.whitelistURNs.isPresent()\n+          ? this.whitelistURNs.get().stream().map(u -> this.datasetURNtoURI(u)).collect(Collectors.toList()) : configClient.getImportedBy(new URI(whitelistTag.toString()), true);\n       populateDisabledURIs(disabledURISet);\n     } catch ( ConfigStoreFactoryDoesNotExistsException | ConfigStoreCreationException\n         | URISyntaxException e) {\n@@ -192,16 +225,18 @@ public int compare(URI c1, URI c2) {\n     for (URI disable : disabledURISet) {\n       if (validURISet.remove(disable)) {\n         log.info(\"skip disabled dataset \" + disable);\n+      } else {\n+        log.info(\"There's no URI \" + disable + \" available in validURISet.\");\n       }\n     }\n     return validURISet;\n   }\n \n   private void populateDisabledURIs(Set<URI> disabledURIs) throws\n-                                             URISyntaxException,\n-                                             ConfigStoreFactoryDoesNotExistsException,\n-                                             ConfigStoreCreationException,\n-                                             VersionDoesNotExistException {\n+                                                           URISyntaxException,\n+                                                           ConfigStoreFactoryDoesNotExistsException,\n+                                                           ConfigStoreCreationException,\n+                                                           VersionDoesNotExistException {\n     if (this.blacklistTags.isPresent()) {\n       disabledURIs = new HashSet<URI>();\n       for (Path s : this.blacklistTags.get()) {\n@@ -256,6 +291,18 @@ protected void executeItertorExecutor(Iterator<Callable<Void>> callableIterator)\n     }\n   }\n \n+  /**\n+   * Helper funcition for converting datasetURN into URI\n+   */\n+  private URI datasetURNtoURI(String datasetURN) {\n+    try {\n+      return new URI(PathUtils.mergePaths(new Path(this.storeRoot), new Path(datasetURN)).toString());\n+    }catch (URISyntaxException e) {\n+      log.error(\"Dataset with URN:\" + datasetURN + \" cannot be converted into URI. Skip the dataset\");\n+      return null;\n+    }\n+  }\n+\n   protected abstract Callable<Void> findDatasetsCallable(final ConfigClient confClient,\n       final URI u, final Properties p, final Collection<Dataset> datasets);\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java",
                "sha": "8208d6a8bb9d4b5577821b3523f20f219ba4d272",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/writer/FileAwareInputStreamDataWriter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/writer/FileAwareInputStreamDataWriter.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/writer/FileAwareInputStreamDataWriter.java",
                "patch": "@@ -213,9 +213,9 @@ public boolean apply(FileStatus input) {\n         os = EncryptionFactory.buildStreamCryptoProvider(encryptionConfig).encodeOutputStream(os);\n       }\n       try {\n+        FileSystem defaultFS = FileSystem.get(new Configuration());\n         StreamThrottler<GobblinScopeTypes> throttler =\n             this.taskBroker.getSharedResource(new StreamThrottler.Factory<GobblinScopeTypes>(), new EmptyKey());\n-        FileSystem defaultFS = FileSystem.get(new Configuration());\n         ThrottledInputStream throttledInputStream = throttler.throttleInputStream().inputStream(inputStream)\n             .sourceURI(copyableFile.getOrigin().getPath().makeQualified(defaultFS.getUri(), defaultFS.getWorkingDirectory()).toUri())\n             .targetURI(this.fs.makeQualified(writeAt).toUri()).build();",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/copy/writer/FileAwareInputStreamDataWriter.java",
                "sha": "4d0b42bd1e812425b20d407dc36bee48afe2cad8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/partition/CopyableDatasetRequestor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/partition/CopyableDatasetRequestor.java?ref=cbadb917809d24c326051939384a57ef4b79e91c",
                "deletions": 0,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/partition/CopyableDatasetRequestor.java",
                "patch": "@@ -29,6 +29,7 @@\n import com.google.common.base.Function;\n import com.google.common.collect.Iterators;\n import com.google.common.collect.Lists;\n+import com.google.common.base.Optional;\n \n import gobblin.data.management.copy.CopyConfiguration;\n import gobblin.data.management.copy.CopyEntity;\n@@ -59,6 +60,7 @@\n     private final CopyConfiguration copyConfiguration;\n     private final Logger log;\n \n+\n     @Nullable\n     @Override\n     public CopyableDatasetRequestor apply(CopyableDatasetBase input) {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/cbadb917809d24c326051939384a57ef4b79e91c/gobblin-data-management/src/main/java/gobblin/data/management/partition/CopyableDatasetRequestor.java",
                "sha": "d96264a1e2049f7befd9163453d9cdf3fe57e1af",
                "status": "modified"
            }
        ],
        "message": "Implemented Job-level white/black list for replication (#1975)\n\n* Implemented HAOPS feature request for /data/derived migration:\r\n- Add job-level black/whitelist in copySource.java\r\n- Enbale logging of version number of configStore\r\n- fix minor bugs in /data/management/copy/replication\r\n\r\n* fix unit test failure, NPE introduced by new config.\r\n\r\n* bracket fix, config name to be resolved\r\n\r\n* Move the blacklist into datasetfinder level\r\n\r\n* Remove redundant variables",
        "parent": "https://github.com/apache/incubator-gobblin/commit/c749df66c51d0399f45b1d79cfd29a26cdb584d1",
        "patched_files": [
            "CopySource.java",
            "FileAwareInputStreamDataWriter.java",
            "ConfigClient.java",
            "ConfigBasedDatasetsFinder.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "ConfigBasedDatasetsFinderTest.java",
            "CopySourceTest.java",
            "FileAwareInputStreamDataWriterTest.java",
            "TestConfigClient.java"
        ]
    },
    "incubator-gobblin_d11fd5f": {
        "bug_id": "incubator-gobblin_d11fd5f",
        "commit": "https://github.com/apache/incubator-gobblin/commit/d11fd5f95aa772c12fda1bfcd4d817df35fd4452",
        "file": [
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/d11fd5f95aa772c12fda1bfcd4d817df35fd4452/runtime/src/main/java/com/linkedin/uif/runtime/AbstractJobLauncher.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/runtime/src/main/java/com/linkedin/uif/runtime/AbstractJobLauncher.java?ref=d11fd5f95aa772c12fda1bfcd4d817df35fd4452",
                "deletions": 6,
                "filename": "runtime/src/main/java/com/linkedin/uif/runtime/AbstractJobLauncher.java",
                "patch": "@@ -129,18 +129,38 @@ public void launchJob(Properties jobProps) throws JobException {\n             sourceState = new SourceState(jobState, getPreviousWorkUnitStates(jobName));\n             source = initSource(jobProps, sourceState);\n         } catch (Throwable t) {\n-            String errMsg = \"Failed to initialize source for job \" + jobId;\n+            String errMsg = \"Failed to initialize the source for job \" + jobId;\n             LOG.error(errMsg, t);\n             unlockJob(jobName, jobLock);\n             throw new JobException(errMsg, t);\n         }\n \n         // Generate work units of the job from the source\n-        List<WorkUnit> workUnits = source.getWorkunits(sourceState);\n+        List<WorkUnit> workUnits;\n+        try {\n+            workUnits = source.getWorkunits(sourceState);\n+        } catch (Throwable t) {\n+            String errMsg = \"Failed to get work units for job \" + jobId;\n+            LOG.error(errMsg, t);\n+            try {\n+                source.shutdown(sourceState);\n+            } catch (Throwable t1) {\n+                // Catch any possible errors so unlockJob is guaranteed to be called below\n+                LOG.error(\"Failed to shutdown the source for job \" + jobId, t1);\n+            }\n+            unlockJob(jobName, jobLock);\n+            throw new JobException(errMsg, t);\n+        }\n+\n         // If there is no real work to do\n         if (workUnits == null || workUnits.isEmpty()) {\n             LOG.warn(\"No work units to do for job \" + jobId);\n-            source.shutdown(sourceState);\n+            try {\n+                source.shutdown(sourceState);\n+            } catch (Throwable t) {\n+                // Catch any possible errors so unlockJob is guaranteed to be called below\n+                LOG.error(\"Failed to shutdown the source for job \" + jobId, t);\n+            }\n             unlockJob(jobName, jobLock);\n             return;\n         }\n@@ -169,9 +189,15 @@ public void launchJob(Properties jobProps) throws JobException {\n             jobState.setState(JobState.RunningState.FAILED);\n             throw new JobException(errMsg, t);\n         } finally {\n-            source.shutdown(sourceState);\n-            persistJobState(jobState);\n-            cleanupStagingData(jobState);\n+            try {\n+                source.shutdown(sourceState);\n+                persistJobState(jobState);\n+                cleanupStagingData(jobState);\n+            } catch (Throwable t) {\n+                // Catch any possible errors so unlockJob is guaranteed to be called below\n+                LOG.error(\"Failed to cleanup for job \" + jobId, t);\n+            }\n+            // Finally release the job lock\n             unlockJob(jobName, jobLock);\n         }\n     }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/d11fd5f95aa772c12fda1bfcd4d817df35fd4452/runtime/src/main/java/com/linkedin/uif/runtime/AbstractJobLauncher.java",
                "sha": "bd6c5286f36f857736af43b2b6b8ba07b8cb441c",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/d11fd5f95aa772c12fda1bfcd4d817df35fd4452/runtime/src/main/java/com/linkedin/uif/runtime/local/LocalJobManager.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/runtime/src/main/java/com/linkedin/uif/runtime/local/LocalJobManager.java?ref=d11fd5f95aa772c12fda1bfcd4d817df35fd4452",
                "deletions": 36,
                "filename": "runtime/src/main/java/com/linkedin/uif/runtime/local/LocalJobManager.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Optional;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Splitter;\n import com.google.common.base.Strings;\n@@ -302,51 +303,67 @@ public void runJob(Properties jobProps, JobListener jobListener) throws JobExcep\n \n         LOG.info(\"Starting job \" + jobId);\n \n-        SourceWrapperBase source;\n-        SourceState sourceState;\n+        Optional<SourceState> sourceStateOptional = Optional.absent();\n         try {\n-            source = this.sourceWrapperMap.get(jobProps.getProperty(\n+            // Initialize the source\n+            SourceWrapperBase source = this.sourceWrapperMap.get(jobProps.getProperty(\n                     ConfigurationKeys.SOURCE_WRAPPER_CLASS_KEY,\n                     ConfigurationKeys.DEFAULT_SOURCE_WRAPPER)\n                     .toLowerCase()).newInstance();\n-            sourceState = new SourceState(jobState, getPreviousWorkUnitStates(jobName));\n+            SourceState sourceState = new SourceState(jobState, getPreviousWorkUnitStates(jobName));\n             source.init(sourceState);\n-        } catch (Exception e) {\n-            LOG.error(\"Failed to instantiate source for job \" + jobId, e);\n-            unlockJob(jobName, runOnce);\n-            throw new JobException(\"Failed to run job \" + jobId, e);\n-        }\n+            sourceStateOptional = Optional.of(sourceState);\n+\n+            // Generate work units based on all previous work unit states\n+            List<WorkUnit> workUnits = source.getWorkunits(sourceState);\n+            // If no real work to do\n+            if (workUnits == null || workUnits.isEmpty()) {\n+                LOG.warn(\"No work units have been created for job \" + jobId);\n+                source.shutdown(jobState);\n+                unlockJob(jobName, runOnce);\n+                callJobListener(jobName, jobState, runOnce);\n+                return;\n+            }\n \n-        // Generate work units based on all previous work unit states\n-        List<WorkUnit> workUnits = source.getWorkunits(sourceState);\n-        // If no real work to do\n-        if (workUnits == null || workUnits.isEmpty()) {\n-            LOG.warn(\"No work units have been created for job \" + jobId);\n-            source.shutdown(jobState);\n+            jobState.setTasks(workUnits.size());\n+            jobState.setStartTime(System.currentTimeMillis());\n+            jobState.setState(JobState.RunningState.WORKING);\n+\n+            this.jobStateMap.put(jobId, jobState);\n+            this.jobSourceMap.put(jobId, source);\n+\n+            // Add all generated work units\n+            int sequence = 0;\n+            for (WorkUnit workUnit : workUnits) {\n+                // Task ID in the form of task_<job_id_suffix>_<task_sequence_number>\n+                String taskId = String.format(\"task_%s_%d\", jobIdSuffix, sequence++);\n+                workUnit.setId(taskId);\n+                WorkUnitState workUnitState = new WorkUnitState(workUnit);\n+                workUnitState.setId(taskId);\n+                workUnitState.setProp(ConfigurationKeys.JOB_ID_KEY, jobId);\n+                workUnitState.setProp(ConfigurationKeys.TASK_ID_KEY, taskId);\n+                this.workUnitManager.addWorkUnit(workUnitState);\n+            }\n+        } catch (Throwable t) {\n+            String errMsg = \"Failed to run job \" + jobId;\n+            LOG.error(errMsg, t);\n+            // Shutdown the source if the source has already been initialized\n+            if (this.jobSourceMap.containsKey(jobId) && sourceStateOptional.isPresent()) {\n+                try {\n+                    this.jobSourceMap.remove(jobId).shutdown(sourceStateOptional.get());\n+                } catch (Throwable t1) {\n+                    // Catch any possible errors so unlockJob is guaranteed to be called below\n+                    LOG.error(\"Failed to shutdown the source for job \" + jobId, t1);\n+                }\n+            }\n+            // Remove the cached job state\n+            this.jobStateMap.remove(jobId);\n+            // Finally release the job lock\n             unlockJob(jobName, runOnce);\n-            callJobListener(jobName, jobState, runOnce);\n-            return;\n+            throw new JobException(errMsg, t);\n         }\n \n-        jobState.setTasks(workUnits.size());\n-        jobState.setStartTime(System.currentTimeMillis());\n-        jobState.setState(JobState.RunningState.WORKING);\n-\n-        this.jobStateMap.put(jobId, jobState);\n-        this.jobSourceMap.put(jobId, source);\n-\n-        // Add all generated work units\n-        int sequence = 0;\n-        for (WorkUnit workUnit : workUnits) {\n-            // Task ID in the form of task_<job_id_suffix>_<task_sequence_number>\n-            String taskId = String.format(\"task_%s_%d\", jobIdSuffix, sequence++);\n-            workUnit.setId(taskId);\n-            WorkUnitState workUnitState = new WorkUnitState(workUnit);\n-            workUnitState.setId(taskId);\n-            workUnitState.setProp(ConfigurationKeys.JOB_ID_KEY, jobId);\n-            workUnitState.setProp(ConfigurationKeys.TASK_ID_KEY, taskId);\n-            this.workUnitManager.addWorkUnit(workUnitState);\n-        }\n+\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/d11fd5f95aa772c12fda1bfcd4d817df35fd4452/runtime/src/main/java/com/linkedin/uif/runtime/local/LocalJobManager.java",
                "sha": "cbcd0e7738979ce4ec2ebfcb23e2a930a906eed5",
                "status": "modified"
            }
        ],
        "message": "Prevent a job from hanging when the source throws an NPE\n\nSigned-off-by: Yinan Li <ynli@linkedin.com>\n\nRB=323051\nR=nveeramr,kgoodhop,lqiao,stakiar\nA=kgoodhop",
        "parent": "https://github.com/apache/incubator-gobblin/commit/872b62253fff3c8ce3401d70f580cfe39ccaf137",
        "patched_files": [
            "LocalJobManager.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "LocalJobManagerTest.java"
        ]
    },
    "incubator-gobblin_fe7dc7c": {
        "bug_id": "incubator-gobblin_fe7dc7c",
        "commit": "https://github.com/apache/incubator-gobblin/commit/fe7dc7c35eebc3a4faee9987ecccaae3511118c5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java?ref=fe7dc7c35eebc3a4faee9987ecccaae3511118c5",
                "deletions": 1,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "patch": "@@ -137,8 +137,9 @@ public String explain() {\n     /**\n      * @return a unique string identifier for this {@link DatasetAndPartition}.\n      */\n+    @SuppressWarnings(\"deprecation\")\n     public String identifier() {\n-      return Hex.encodeHexString(DigestUtils.sha1(this.dataset.toString() + this.partition));\n+      return Hex.encodeHexString(DigestUtils.sha(this.dataset.toString() + this.partition));\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-data-management/src/main/java/gobblin/data/management/copy/CopyEntity.java",
                "sha": "e41c312ca5986c7174c7fdacf9a58cfa528daf78",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java?ref=fe7dc7c35eebc3a4faee9987ecccaae3511118c5",
                "deletions": 9,
                "filename": "gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "patch": "@@ -107,12 +107,12 @@\n    * If the predicate returns true, the partition will be skipped. */\n   public static final String FAST_PARTITION_SKIP_PREDICATE =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.fast.partition.skip.predicate\";\n-  \n+\n   /** A predicate applied to non partition table before any file listing.\n    * If the predicate returns true, the table will be skipped. */\n   public static final String FAST_TABLE_SKIP_PREDICATE =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.fast.table.skip.predicate\";\n-  \n+\n   /** Method for deleting files on deregister. One of {@link DeregisterFileDeleteMethod}. */\n   public static final String DELETE_FILES_ON_DEREGISTER =\n       HiveDatasetFinder.HIVE_DATASET_PREFIX + \".copy.deregister.fileDeleteMethod\";\n@@ -157,7 +157,7 @@\n   private final Optional<String> partitionFilter;\n   private final Optional<Predicate<PartitionCopy>> fastPartitionSkip;\n   private final Optional<Predicate<HiveCopyEntityHelper>> fastTableSkip;\n-  \n+\n   private final DeregisterFileDeleteMethod deleteMethod;\n \n   private final Optional<CommitStep> tableRegistrationStep;\n@@ -226,6 +226,7 @@\n       this.configuration = configuration;\n       this.targetFs = targetFs;\n \n+      this.targetPathHelper = new HiveTargetPathHelper(this.dataset);\n       this.hiveRegProps = new HiveRegProps(new State(this.dataset.getProperties()));\n       this.targetURI = Optional.fromNullable(this.dataset.getProperties().getProperty(TARGET_METASTORE_URI_KEY));\n       this.targetClientPool = HiveMetastoreClientPool.get(this.dataset.getProperties(), this.targetURI);\n@@ -305,15 +306,16 @@\n         if (HiveUtils.isPartitioned(this.dataset.table)) {\n           this.sourcePartitions = HiveUtils.getPartitionsMap(multiClient.getClient(source_client), this.dataset.table,\n               this.partitionFilter);\n+          // Note: this must be mutable, so we copy the map\n           this.targetPartitions =\n-              this.existingTargetTable.isPresent() ? HiveUtils.getPartitionsMap(multiClient.getClient(target_client),\n-                  this.existingTargetTable.get(), this.partitionFilter) : Maps.<List<String>, Partition> newHashMap();\n+              this.existingTargetTable.isPresent() ? Maps.newHashMap(\n+                  HiveUtils.getPartitionsMap(multiClient.getClient(target_client),\n+                      this.existingTargetTable.get(), this.partitionFilter)) : Maps.<List<String>, Partition> newHashMap();\n         } else {\n           this.sourcePartitions = Maps.newHashMap();\n           this.targetPartitions = Maps.newHashMap();\n         }\n \n-        this.targetPathHelper = new HiveTargetPathHelper(this.dataset);\n       } catch (TException te) {\n         closer.close();\n         throw new IOException(\"Failed to generate work units for table \" + dataset.table.getCompleteName(), te);\n@@ -653,7 +655,7 @@ private int addSharedSteps(List<CopyEntity> copyEntities, String fileSet, int in\n         HiveLocationDescriptor.forTable(this.dataset.table, this.dataset.fs, this.dataset.getProperties());\n     HiveLocationDescriptor desiredTargetLocation =\n         HiveLocationDescriptor.forTable(this.targetTable, this.targetFs, this.dataset.getProperties());\n-    \n+\n     Optional<HiveLocationDescriptor> existingTargetLocation = this.existingTargetTable.isPresent() ? Optional.of(\n         HiveLocationDescriptor.forTable(this.existingTargetTable.get(), this.targetFs, this.dataset.getProperties()))\n         : Optional.<HiveLocationDescriptor> absent();\n@@ -663,12 +665,12 @@ private int addSharedSteps(List<CopyEntity> copyEntities, String fileSet, int in\n       multiTimer.close();\n       return Lists.newArrayList();\n     }\n-    \n+\n     DiffPathSet diffPathSet = fullPathDiff(sourceLocation, desiredTargetLocation, existingTargetLocation,\n         Optional.<Partition> absent(), multiTimer, this);\n \n     multiTimer.nextStage(Stages.FULL_PATH_DIFF);\n-    \n+\n     // Could used to delete files for the existing snapshot\n     DeleteFileCommitStep deleteStep =\n         DeleteFileCommitStep.fromPaths(this.targetFs, diffPathSet.pathsToDelete, this.dataset.getProperties());",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-data-management/src/main/java/gobblin/data/management/copy/hive/HiveCopyEntityHelper.java",
                "sha": "4547faa382944345152ff8f70ae51c7d262d9a03",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-gobblin/blob/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-gobblin/contents/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java?ref=fe7dc7c35eebc3a4faee9987ecccaae3511118c5",
                "deletions": 1,
                "filename": "gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "patch": "@@ -182,8 +182,10 @@ public String toString() {\n     return Hex.encodeHexString(this.sha);\n   }\n \n+  // DigestUtils.sha is deprecated for sha1, but sha1 is not available in old versions of commons codec\n+  @SuppressWarnings(\"deprecation\")\n   private static byte[] computeGuid(byte[] bytes) {\n-    return DigestUtils.sha1(bytes);\n+    return DigestUtils.sha(bytes);\n   }\n \n   static class SimpleHasGuid implements HasGuid {",
                "raw_url": "https://github.com/apache/incubator-gobblin/raw/fe7dc7c35eebc3a4faee9987ecccaae3511118c5/gobblin-utility/src/main/java/gobblin/util/guid/Guid.java",
                "sha": "8b279933c4fdf89bc5338baec60a3dc481ed8d62",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1019 from ibuenros/target-path-fix\n\nFix NPE and mutability of a map in hive target path helper and copy h\u2026",
        "parent": "https://github.com/apache/incubator-gobblin/commit/25b8059436d0bf07921976a62dc50970c88c5b6e",
        "patched_files": [
            "HiveCopyEntityHelper.java",
            "Guid.java"
        ],
        "repo": "incubator-gobblin",
        "unit_tests": [
            "GuidTest.java",
            "HiveCopyEntityHelperTest.java"
        ]
    }
}