{
    "kafka_006630f": {
        "bug_id": "kafka_006630f",
        "commit": "https://github.com/apache/kafka/commit/006630fd93d8efb823e5b5f7d61584138df984a6",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java?ref=006630fd93d8efb823e5b5f7d61584138df984a6",
                "deletions": 0,
                "filename": "clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java",
                "patch": "@@ -389,6 +389,10 @@ synchronized void registerMetric(KafkaMetric metric) {\n         return this.metrics;\n     }\n \n+    public KafkaMetric metric(MetricName metricName) {\n+        return this.metrics.get(metricName);\n+    }\n+\n     /**\n      * This iterates over every Sensor and triggers a removeSensor if it has expired\n      * Package private for testing",
                "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java",
                "sha": "78dad18577310a644d062873a1accadd8ad9746a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/main/scala/kafka/network/SocketServer.scala",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/network/SocketServer.scala?ref=006630fd93d8efb823e5b5f7d61584138df984a6",
                "deletions": 3,
                "filename": "core/src/main/scala/kafka/network/SocketServer.scala",
                "patch": "@@ -105,8 +105,9 @@ class SocketServer(val config: KafkaConfig, val metrics: Metrics, val time: Time\n \n     newGauge(\"NetworkProcessorAvgIdlePercent\",\n       new Gauge[Double] {\n-        def value = allMetricNames.map( metricName =>\n-          metrics.metrics().get(metricName).value()).sum / totalProcessorThreads\n+        def value = allMetricNames.map { metricName =>\n+          Option(metrics.metric(metricName)).fold(0.0)(_.value)\n+        }.sum / totalProcessorThreads\n       }\n     )\n \n@@ -389,7 +390,7 @@ private[kafka] class Processor(val id: Int,\n   newGauge(\"IdlePercent\",\n     new Gauge[Double] {\n       def value = {\n-        metrics.metrics().get(metrics.metricName(\"io-wait-ratio\", \"socket-server-metrics\", metricTags)).value()\n+        Option(metrics.metric(metrics.metricName(\"io-wait-ratio\", \"socket-server-metrics\", metricTags))).fold(0.0)(_.value)\n       }\n     },\n     metricTags.asScala",
                "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/main/scala/kafka/network/SocketServer.scala",
                "sha": "55061edb68f2da698879a794608deea1309bb787",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/test/scala/unit/kafka/network/SocketServerTest.scala",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/network/SocketServerTest.scala?ref=006630fd93d8efb823e5b5f7d61584138df984a6",
                "deletions": 9,
                "filename": "core/src/test/scala/unit/kafka/network/SocketServerTest.scala",
                "patch": "@@ -17,27 +17,29 @@\n \n package kafka.network\n \n-import java.net._\n-import javax.net.ssl._\n import java.io._\n-import java.util.HashMap\n-import java.util.Random\n+import java.net._\n import java.nio.ByteBuffer\n+import java.util.{HashMap, Random}\n+import javax.net.ssl._\n \n+import com.yammer.metrics.core.Gauge\n+import com.yammer.metrics.{Metrics => YammerMetrics}\n+import kafka.server.KafkaConfig\n+import kafka.utils.TestUtils\n+import org.apache.kafka.common.TopicPartition\n import org.apache.kafka.common.metrics.Metrics\n import org.apache.kafka.common.network.NetworkSend\n import org.apache.kafka.common.protocol.{ApiKeys, SecurityProtocol}\n-import org.apache.kafka.common.security.auth.KafkaPrincipal\n-import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.record.MemoryRecords\n import org.apache.kafka.common.requests.{ProduceRequest, RequestHeader}\n+import org.apache.kafka.common.security.auth.KafkaPrincipal\n import org.apache.kafka.common.utils.Time\n-import kafka.server.KafkaConfig\n-import kafka.utils.TestUtils\n-import org.apache.kafka.common.record.MemoryRecords\n import org.junit.Assert._\n import org.junit._\n import org.scalatest.junit.JUnitSuite\n \n+import scala.collection.JavaConverters.mapAsScalaMapConverter\n import scala.collection.mutable.ArrayBuffer\n \n class SocketServerTest extends JUnitSuite {\n@@ -395,4 +397,18 @@ class SocketServerTest extends JUnitSuite {\n \n   }\n \n+  @Test\n+  def testMetricCollectionAfterShutdown(): Unit = {\n+    server.shutdown()\n+\n+    val sum = YammerMetrics\n+      .defaultRegistry\n+      .allMetrics.asScala\n+      .filterKeys(k => k.getName.endsWith(\"IdlePercent\") || k.getName.endsWith(\"NetworkProcessorAvgIdlePercent\"))\n+      .collect { case (_, metric: Gauge[_]) => metric.value.asInstanceOf[Double] }\n+      .sum\n+\n+    assertEquals(0, sum, 0)\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/test/scala/unit/kafka/network/SocketServerTest.scala",
                "sha": "c6f90ff6f30cff7e89bdbf265729c467fbaa55c9",
                "status": "modified"
            }
        ],
        "message": "MINOR: Fix metric collection NPE during shutdown\n\nCollecting socket server metrics during shutdown may throw NullPointerException\n\nAuthor: Xavier L\u00e9aut\u00e9 <xavier@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #2221 from xvrl/fix-metrics-npe-on-shutdown",
        "parent": "https://github.com/apache/kafka/commit/1949a76bc4189534b853e21c476bb11172fa3fc9",
        "repo": "kafka",
        "unit_tests": [
            "MetricsTest.java"
        ]
    },
    "kafka_0a7f2bf": {
        "bug_id": "kafka_0a7f2bf",
        "commit": "https://github.com/apache/kafka/commit/0a7f2bf335e426b12f717ec7f6d51779d56fe59b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/0a7f2bf335e426b12f717ec7f6d51779d56fe59b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java?ref=0a7f2bf335e426b12f717ec7f6d51779d56fe59b",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java",
                "patch": "@@ -145,7 +145,7 @@ public int sizeInBytes() {\n         if (writable) {\n             return compressor.buffer().position();\n         } else {\n-            return compressor.buffer().limit();\n+            return buffer.limit();\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/0a7f2bf335e426b12f717ec7f6d51779d56fe59b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java",
                "sha": "971f0a2131259babdfde7947149a50102f60ccc5",
                "status": "modified"
            }
        ],
        "message": "MINOR: MemoryRecords.sizeInBytes throws NPE when non-writable.\n\nI just noticed that `MemoryRecords.sizeInBytes` throws NPE when MemoryRecords is non-writable. `compressor` is explicitly set to null when `writable` is false (L56) at the construction time, for instance when `MemoryRecords.readableRecords` is used.\n\nguozhangwang Could you take a look when you have time?\n\nAuthor: David Jacot <david.jacot@gmail.com>\n\nReviewers: Guozhang Wang\n\nCloses #786 from dajac/kafka-npe",
        "parent": "https://github.com/apache/kafka/commit/b1d325b3c09cd95d69a66fac4a3760f57d3062c9",
        "repo": "kafka",
        "unit_tests": [
            "MemoryRecordsTest.java"
        ]
    },
    "kafka_0c25c73": {
        "bug_id": "kafka_0c25c73",
        "commit": "https://github.com/apache/kafka/commit/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java?ref=0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "patch": "@@ -234,7 +234,7 @@ private SourceTopicsInfo getSourceTopicsInfo(final String storeName) {\n     }\n \n     private boolean isInitialized() {\n-        return !clusterMetadata.topics().isEmpty();\n+        return clusterMetadata != null && !clusterMetadata.topics().isEmpty();\n     }\n \n     private class SourceTopicsInfo {",
                "raw_url": "https://github.com/apache/kafka/raw/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "sha": "ccb2cdafeeb733ab8eb4908b79f4648718a3858a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java?ref=0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "patch": "@@ -114,6 +114,11 @@ public Object apply(final Object value) {\n         discovery.onChange(hostToPartitions, cluster);\n     }\n \n+    @Test\n+    public void shouldNotThrowNPEWhenOnChangeNotCalled() throws Exception {\n+        new StreamsMetadataState(builder).getAllMetadataForStore(\"store\");\n+    }\n+\n     @Test\n     public void shouldGetAllStreamInstances() throws Exception {\n         final StreamsMetadata one = new StreamsMetadata(hostOne, Utils.mkSet(\"table-one\", \"table-two\", \"merged-table\"),",
                "raw_url": "https://github.com/apache/kafka/raw/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "sha": "411e02db9b9e1cc0e653af8d3ed6bd3e911dbc4b",
                "status": "modified"
            }
        ],
        "message": "HOTFIX: fix npe in StreamsMetadataState when onChange has not been called\n\nIf some StreamsMetadataState methods are called before the onChange method is called a NullPointerException was being thrown. Added null check for cluster in isInitialized method\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1920 from dguy/fix-npe-streamsmetadata",
        "parent": "https://github.com/apache/kafka/commit/61d3378bc84914a521a65cdfffb7299928fa8671",
        "repo": "kafka",
        "unit_tests": [
            "StreamsMetadataStateTest.java"
        ]
    },
    "kafka_105ab47": {
        "bug_id": "kafka_105ab47",
        "commit": "https://github.com/apache/kafka/commit/105ab47ed90c8a0e83c159c97a8f2294c5582657",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657",
                "deletions": 2,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
                "patch": "@@ -538,8 +538,9 @@ public boolean hasUndrained() {\n                                                 // on the client after being sent to the broker at least once.\n                                                 break;\n \n-                                            if (first.hasSequence()\n-                                                    && first.baseSequence() != transactionManager.nextBatchBySequence(first.topicPartition).baseSequence())\n+                                            int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);\n+                                            if (firstInFlightSequence != RecordBatch.NO_SEQUENCE && first.hasSequence()\n+                                                    && first.baseSequence() != firstInFlightSequence)\n                                                 // If the queued batch already has an assigned sequence, then it is being\n                                                 // retried. In this case, we wait until the next immediate batch is ready\n                                                 // and drain that. We only move on when the next in line batch is complete (either successfully",
                "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
                "sha": "ba8c28ece27f65c0fdcaf4ba3448746e59dfa242",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657",
                "deletions": 2,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "patch": "@@ -528,8 +528,8 @@ private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionRespons\n                 } else if (transactionManager.hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {\n                     // If idempotence is enabled only retry the request if the current producer id is the same as\n                     // the producer id of the batch.\n-                    log.debug(\"Retrying batch to topic-partition {}. Sequence number : {}\", batch.topicPartition,\n-                            batch.baseSequence());\n+                    log.debug(\"Retrying batch to topic-partition {}. ProducerId: {}; Sequence number : {}\",\n+                            batch.topicPartition, batch.producerId(), batch.baseSequence());\n                     reenqueueBatch(batch, now);\n                 } else {\n                     failBatch(batch, response, new OutOfOrderSequenceException(\"Attempted to retry sending a \" +",
                "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "sha": "7eea4992b33151ee99b82db823c7c0b510a70a60",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657",
                "deletions": 0,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.kafka.common.errors.GroupAuthorizationException;\n import org.apache.kafka.common.errors.TopicAuthorizationException;\n import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.RecordBatch;\n import org.apache.kafka.common.requests.AbstractRequest;\n import org.apache.kafka.common.requests.AbstractResponse;\n import org.apache.kafka.common.requests.AddOffsetsToTxnRequest;\n@@ -435,6 +436,24 @@ public int compare(ProducerBatch o1, ProducerBatch o2) {\n         inflightBatchesBySequence.get(batch.topicPartition).offer(batch);\n     }\n \n+    /**\n+     * Returns the first inflight sequence for a given partition. This is the base sequence of an inflight batch with\n+     * the lowest sequence number.\n+     * @return the lowest inflight sequence if the transaction manager is tracking inflight requests for this partition.\n+     *         If there are no inflight requests being tracked for this partition, this method will return\n+     *         RecordBatch.NO_SEQUENCE.\n+     */\n+    synchronized int firstInFlightSequence(TopicPartition topicPartition) {\n+        PriorityQueue<ProducerBatch> inFlightBatches = inflightBatchesBySequence.get(topicPartition);\n+        if (inFlightBatches == null)\n+            return RecordBatch.NO_SEQUENCE;\n+\n+        ProducerBatch firstInFlightBatch = inFlightBatches.peek();\n+        if (firstInFlightBatch == null)\n+            return RecordBatch.NO_SEQUENCE;\n+\n+        return firstInFlightBatch.baseSequence();\n+    }\n \n     synchronized ProducerBatch nextBatchBySequence(TopicPartition topicPartition) {\n         PriorityQueue<ProducerBatch> queue = inflightBatchesBySequence.get(topicPartition);",
                "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java",
                "sha": "006a12b1bfd4ca73035e7d69ab894a32b87fb27e",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "patch": "@@ -1104,6 +1104,103 @@ public void testExpiryOfAllSentBatchesShouldCauseUnresolvedSequences() throws Ex\n         assertTrue(transactionManager.hasProducerId(producerId + 1));\n     }\n \n+    @Test\n+    public void testResetOfProducerStateShouldAllowQueuedBatchesToDrain() throws Exception {\n+        final long producerId = 343434L;\n+        TransactionManager transactionManager = new TransactionManager();\n+        transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));\n+        setupWithTransactionState(transactionManager);\n+        client.setNode(new Node(1, \"localhost\", 33343));\n+\n+        int maxRetries = 10;\n+        Metrics m = new Metrics();\n+        SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);\n+\n+        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries,\n+                senderMetrics, time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);\n+\n+        Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds());  // connect.\n+        sender.run(time.milliseconds());  // send.\n+\n+        assertEquals(1, client.inFlightRequestCount());\n+\n+        Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();\n+        responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));\n+        responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));\n+        client.respond(produceResponse(responses));\n+        sender.run(time.milliseconds());\n+        assertTrue(failedResponse.isDone());\n+        assertFalse(\"Expected transaction state to be reset upon receiving an OutOfOrderSequenceException\", transactionManager.hasProducerId());\n+        prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);\n+        assertEquals(producerId + 1, transactionManager.producerIdAndEpoch().producerId);\n+        sender.run(time.milliseconds());  // send request to tp1\n+\n+        assertFalse(successfulResponse.isDone());\n+        client.respond(produceResponse(tp1, 10, Errors.NONE, -1));\n+        sender.run(time.milliseconds());\n+\n+        assertTrue(successfulResponse.isDone());\n+        assertEquals(10, successfulResponse.get().offset());\n+\n+        // Since the response came back for the old producer id, we shouldn't update the next sequence.\n+        assertEquals(0, transactionManager.sequenceNumber(tp1).longValue());\n+    }\n+\n+    @Test\n+    public void testBatchesDrainedWithOldProducerIdShouldFailWithOutOfOrderSequenceOnSubsequentRetry() throws Exception {\n+        final long producerId = 343434L;\n+        TransactionManager transactionManager = new TransactionManager();\n+        transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));\n+        setupWithTransactionState(transactionManager);\n+        client.setNode(new Node(1, \"localhost\", 33343));\n+\n+        int maxRetries = 10;\n+        Metrics m = new Metrics();\n+        SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);\n+\n+        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries,\n+                senderMetrics, time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);\n+\n+        Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds());  // connect.\n+        sender.run(time.milliseconds());  // send.\n+\n+        assertEquals(1, client.inFlightRequestCount());\n+\n+        Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();\n+        responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));\n+        responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));\n+        client.respond(produceResponse(responses));\n+        sender.run(time.milliseconds());\n+        assertTrue(failedResponse.isDone());\n+        assertFalse(\"Expected transaction state to be reset upon receiving an OutOfOrderSequenceException\", transactionManager.hasProducerId());\n+        prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);\n+        assertEquals(producerId + 1, transactionManager.producerIdAndEpoch().producerId);\n+        sender.run(time.milliseconds());  // send request to tp1 with the old producerId\n+\n+        assertFalse(successfulResponse.isDone());\n+        // The response comes back with a retriable error.\n+        client.respond(produceResponse(tp1, 0, Errors.NOT_LEADER_FOR_PARTITION, -1));\n+        sender.run(time.milliseconds());\n+\n+        assertTrue(successfulResponse.isDone());\n+        // Since the batch has an old producerId, it will not be retried yet again, but will be failed with a Fatal\n+        // exception.\n+        try {\n+            successfulResponse.get();\n+            fail(\"Should have raised an OutOfOrderSequenceException\");\n+        } catch (Exception e) {\n+            assertTrue(e.getCause() instanceof OutOfOrderSequenceException);\n+        }\n+    }\n+\n     @Test\n     public void testCorrectHandlingOfDuplicateSequenceError() throws Exception {\n         final long producerId = 343434L;\n@@ -1799,12 +1896,31 @@ public boolean matches(AbstractRequest body) {\n         };\n     }\n \n+    class OffsetAndError {\n+        long offset;\n+        Errors error;\n+        OffsetAndError(long offset, Errors error) {\n+            this.offset = offset;\n+            this.error = error;\n+        }\n+    }\n+\n     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs, long logStartOffset) {\n         ProduceResponse.PartitionResponse resp = new ProduceResponse.PartitionResponse(error, offset, RecordBatch.NO_TIMESTAMP, logStartOffset);\n         Map<TopicPartition, ProduceResponse.PartitionResponse> partResp = Collections.singletonMap(tp, resp);\n         return new ProduceResponse(partResp, throttleTimeMs);\n     }\n \n+    private ProduceResponse produceResponse(Map<TopicPartition, OffsetAndError> responses) {\n+        Map<TopicPartition, ProduceResponse.PartitionResponse> partResponses = new LinkedHashMap<>();\n+        for (Map.Entry<TopicPartition, OffsetAndError> entry : responses.entrySet()) {\n+            ProduceResponse.PartitionResponse response = new ProduceResponse.PartitionResponse(entry.getValue().error,\n+                    entry.getValue().offset, RecordBatch.NO_TIMESTAMP, -1);\n+            partResponses.put(entry.getKey(), response);\n+        }\n+        return new ProduceResponse(partResponses);\n+\n+    }\n     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs) {\n         return produceResponse(tp, offset, error, throttleTimeMs, -1L);\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "sha": "1ce8e5a4bbdb48024aa23c2b8ce390999fb0bae2",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/core/src/main/scala/kafka/log/ProducerStateManager.scala",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/ProducerStateManager.scala?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657",
                "deletions": 3,
                "filename": "core/src/main/scala/kafka/log/ProducerStateManager.scala",
                "patch": "@@ -40,8 +40,22 @@ class CorruptSnapshotException(msg: String) extends KafkaException(msg)\n // ValidationType and its subtypes define the extent of the validation to perform on a given ProducerAppendInfo instance\n private[log] sealed trait ValidationType\n private[log] object ValidationType {\n+\n+  /**\n+    * This indicates no validation should be performed on the incoming append. This is the case for all appends on\n+    * a replica, as well as appends when the producer state is being built from the log.\n+    */\n   case object None extends ValidationType\n+\n+  /**\n+    * We only validate the epoch (and not the sequence numbers) for offset commit requests coming from the transactional\n+    * producer. These appends will not have sequence numbers, so we can't validate them.\n+    */\n   case object EpochOnly extends ValidationType\n+\n+  /**\n+    * Perform the full validation. This should be used fo regular produce requests coming to the leader.\n+    */\n   case object Full extends ValidationType\n }\n \n@@ -148,9 +162,9 @@ private[log] class ProducerIdEntry(val producerId: Long, val batchMetadata: muta\n  *                      be made against the lastest append in the current entry. New appends will replace older appends\n  *                      in the current entry so that the space overhead is constant.\n  * @param validationType Indicates the extent of validation to perform on the appends on this instance. Offset commits\n- *                       coming from the producer should have EpochOnlyValidation. Appends which aren't from a client\n- *                       will not be validated at all, and should be set to NoValidation. All other appends should\n- *                       have FullValidation.\n+ *                       coming from the producer should have ValidationType.EpochOnly. Appends which aren't from a client\n+ *                       should have ValidationType.None. Appends coming from a client for produce requests should have\n+ *                       ValidationType.Full.\n  */\n private[log] class ProducerAppendInfo(val producerId: Long,\n                                       currentEntry: ProducerIdEntry,",
                "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/core/src/main/scala/kafka/log/ProducerStateManager.scala",
                "sha": "7c0a3da6d6adc2ed67f150e0dc0d33fbfb3614fd",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6015; Fix NPE in RecordAccumulator after ProducerId reset\n\nIt is possible for batches with sequence numbers to be in the `deque` while at the same time the in flight batches in the `TransactionManager` are removed due to a producerId reset.\n\nIn this case, when the batches in the `deque` are drained, we will get a `NullPointerException` in the background thread due to this line:\n\n```java\nif (first.hasSequence() && first.baseSequence() != transactionManager.nextBatchBySequence(first.topicPartition).baseSequence())\n```\n\nParticularly, `transactionManager.nextBatchBySequence` will return null, because there no inflight batches being tracked.\n\nIn this patch, we simply allow the batches in the `deque` to be drained if there are no in flight batches being tracked in the TransactionManager. If they succeed, well and good. If the responses come back with an error, the batces will be ultimately failed in the producer with an `OutOfOrderSequenceException` when the response comes back.\n\nAuthor: Apurva Mehta <apurva@confluent.io>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #4022 from apurvam/KAFKA-6015-npe-in-record-accumulator",
        "parent": "https://github.com/apache/kafka/commit/10cd98cc894b88c5d1e24fc54c66361ad9914df2",
        "repo": "kafka",
        "unit_tests": [
            "RecordAccumulatorTest.java",
            "SenderTest.java",
            "TransactionManagerTest.java"
        ]
    },
    "kafka_14314e3": {
        "bug_id": "kafka_14314e3",
        "commit": "https://github.com/apache/kafka/commit/14314e3d687b4c7b77750d27a085b803c934e3e9",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.NoSuchElementException;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentNavigableMap;\n@@ -134,6 +135,8 @@ public void remove(final Windowed<Bytes> sessionKey) {\n     public byte[] fetchSession(final Bytes key, final long startTime, final long endTime) {\n         removeExpiredSegments();\n \n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         // Only need to search if the record hasn't expired yet\n         if (endTime > observedStreamTime - retentionPeriod) {\n             final ConcurrentNavigableMap<Bytes, ConcurrentNavigableMap<Long, byte[]>> keyMap = endTimeMap.get(endTime);\n@@ -152,6 +155,8 @@ public void remove(final Windowed<Bytes> sessionKey) {\n     public KeyValueIterator<Windowed<Bytes>, byte[]> findSessions(final Bytes key,\n                                                                   final long earliestSessionEndTime,\n                                                                   final long latestSessionStartTime) {\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         return registerNewIterator(key,\n@@ -166,6 +171,9 @@ public void remove(final Windowed<Bytes> sessionKey) {\n                                                                   final Bytes keyTo,\n                                                                   final long earliestSessionEndTime,\n                                                                   final long latestSessionStartTime) {\n+        Objects.requireNonNull(keyFrom, \"from key cannot be null\");\n+        Objects.requireNonNull(keyTo, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (keyFrom.compareTo(keyTo) > 0) {\n@@ -183,15 +191,23 @@ public void remove(final Windowed<Bytes> sessionKey) {\n \n     @Override\n     public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes key) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         return registerNewIterator(key, key, Long.MAX_VALUE, endTimeMap.entrySet().iterator());\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes from, final Bytes to) {\n+\n+        Objects.requireNonNull(from, \"from key cannot be null\");\n+        Objects.requireNonNull(to, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n+\n         return registerNewIterator(from, to, Long.MAX_VALUE, endTimeMap.entrySet().iterator());\n     }\n \n@@ -212,6 +228,13 @@ public void flush() {\n \n     @Override\n     public void close() {\n+        if (openIterators.size() != 0) {\n+            LOG.warn(\"Closing {} open iterators for store {}\", openIterators.size(), name);\n+            for (final InMemorySessionStoreIterator it : openIterators) {\n+                it.close();\n+            }\n+        }\n+\n         endTimeMap.clear();\n         openIterators.clear();\n         open = false;\n@@ -303,6 +326,8 @@ public boolean hasNext() {\n \n         @Override\n         public void close() {\n+            next = null;\n+            recordIterator = null;\n             callback.deregisterIterator(this);\n         }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java",
                "sha": "f3b85657278dc19df103049bf5143681bfb63f50",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9",
                "deletions": 15,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import java.nio.ByteBuffer;\n import java.util.Iterator;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentNavigableMap;\n@@ -135,6 +136,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n             } else {\n                 segmentMap.computeIfPresent(windowStartTimestamp, (t, kvMap) -> {\n                     kvMap.remove(keyBytes);\n+                    if (kvMap.isEmpty()) {\n+                        segmentMap.remove(windowStartTimestamp);\n+                    }\n                     return kvMap;\n                 });\n             }\n@@ -143,6 +147,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n \n     @Override\n     public byte[] fetch(final Bytes key, final long windowStartTimestamp) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {\n@@ -160,6 +167,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n     @Deprecated\n     @Override\n     public WindowStoreIterator<byte[]> fetch(final Bytes key, final long timeFrom, final long timeTo) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         // add one b/c records expire exactly retentionPeriod ms after created\n@@ -179,6 +189,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n                                                            final Bytes to,\n                                                            final long timeFrom,\n                                                            final long timeTo) {\n+        Objects.requireNonNull(from, \"from key cannot be null\");\n+        Objects.requireNonNull(to, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (from.compareTo(to) > 0) {\n@@ -242,6 +255,13 @@ public void flush() {\n \n     @Override\n     public void close() {\n+        if (openIterators.size() != 0) {\n+            LOG.warn(\"Closing {} open iterators for store {}\", openIterators.size(), name);\n+            for (final InMemoryWindowStoreIteratorWrapper it : openIterators) {\n+                it.close();\n+            }\n+        }\n+        \n         segmentMap.clear();\n         open = false;\n     }\n@@ -281,7 +301,7 @@ private WrappedInMemoryWindowStoreIterator registerNewWindowStoreIterator(final\n         final Bytes keyTo = retainDuplicates ? wrapForDups(key, Integer.MAX_VALUE) : key;\n \n         final WrappedInMemoryWindowStoreIterator iterator =\n-            new WrappedInMemoryWindowStoreIterator(keyFrom, keyTo, segmentIterator, openIterators::remove);\n+            new WrappedInMemoryWindowStoreIterator(keyFrom, keyTo, segmentIterator, openIterators::remove, retainDuplicates);\n \n         openIterators.add(iterator);\n         return iterator;\n@@ -319,15 +339,18 @@ private WrappedWindowedKeyValueIterator registerNewWindowedKeyValueIterator(fina\n         private final boolean allKeys;\n         private final Bytes keyFrom;\n         private final Bytes keyTo;\n+        private final boolean retainDuplicates;\n         private final ClosingCallback callback;\n \n         InMemoryWindowStoreIteratorWrapper(final Bytes keyFrom,\n                                            final Bytes keyTo,\n                                            final Iterator<Map.Entry<Long, ConcurrentNavigableMap<Bytes, byte[]>>> segmentIterator,\n-                                           final ClosingCallback callback) {\n+                                           final ClosingCallback callback,\n+                                           final boolean retainDuplicates) {\n             this.keyFrom = keyFrom;\n             this.keyTo = keyTo;\n             allKeys = (keyFrom == null) && (keyTo == null);\n+            this.retainDuplicates = retainDuplicates;\n \n             this.segmentIterator = segmentIterator;\n             this.callback = callback;\n@@ -343,15 +366,26 @@ public boolean hasNext() {\n             }\n \n             next = getNext();\n-            return next != null;\n-        }\n+            if (next == null) {\n+                return false;\n+            }\n \n-        public void remove() {\n-            throw new UnsupportedOperationException(\n-                \"remove() is not supported in \" + getClass().getName());\n+            if (allKeys || !retainDuplicates) {\n+                return true;\n+            }\n+\n+            final Bytes key = getKey(next.key);\n+            if (key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0) {\n+                return true;\n+            } else {\n+                next = null;\n+                return hasNext();\n+            }\n         }\n \n         public void close() {\n+            next = null;\n+            recordIterator = null;\n             callback.deregisterIterator(this);\n         }\n \n@@ -395,8 +429,9 @@ Long minTime() {\n         WrappedInMemoryWindowStoreIterator(final Bytes keyFrom,\n                                            final Bytes keyTo,\n                                            final Iterator<Map.Entry<Long, ConcurrentNavigableMap<Bytes, byte[]>>> segmentIterator,\n-                                           final ClosingCallback callback)  {\n-            super(keyFrom, keyTo, segmentIterator, callback);\n+                                           final ClosingCallback callback,\n+                                           final boolean retainDuplicates)  {\n+            super(keyFrom, keyTo, segmentIterator, callback, retainDuplicates);\n         }\n \n         @Override\n@@ -419,13 +454,12 @@ public Long peekNextKey() {\n         }\n \n         public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n-            return new WrappedInMemoryWindowStoreIterator(null, null, null, it -> { });\n+            return new WrappedInMemoryWindowStoreIterator(null, null, null, it -> { }, false);\n         }\n     }\n \n     private static class WrappedWindowedKeyValueIterator extends InMemoryWindowStoreIteratorWrapper implements KeyValueIterator<Windowed<Bytes>, byte[]> {\n \n-        private final boolean retainDuplicates;\n         private final long windowSize;\n \n         WrappedWindowedKeyValueIterator(final Bytes keyFrom,\n@@ -434,8 +468,7 @@ public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n                                         final ClosingCallback callback,\n                                         final boolean retainDuplicates,\n                                         final long windowSize) {\n-            super(keyFrom, keyTo, segmentIterator, callback);\n-            this.retainDuplicates = retainDuplicates;\n+            super(keyFrom, keyTo, segmentIterator, callback, retainDuplicates);\n             this.windowSize = windowSize;\n         }\n \n@@ -457,8 +490,15 @@ public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n         }\n \n         private Windowed<Bytes> getWindowedKey() {\n-            final Bytes key = retainDuplicates ? getKey(super.next.key) : super.next.key;\n-            final TimeWindow timeWindow = new TimeWindow(super.currentTime, super.currentTime + windowSize);\n+            final Bytes key = super.retainDuplicates ? getKey(super.next.key) : super.next.key;\n+            long endTime = super.currentTime + windowSize;\n+\n+            if (endTime < 0) {\n+                LOG.warn(\"Warning: window end time was truncated to Long.MAX\");\n+                endTime = Long.MAX_VALUE;\n+            }\n+\n+            final TimeWindow timeWindow = new TimeWindow(super.currentTime, endTime);\n             return new Windowed<>(key, timeWindow);\n         }\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java",
                "sha": "8063410212eb679e8557e5f1a04a37a1efc5add8",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java",
                "patch": "@@ -26,9 +26,13 @@\n \n import java.nio.ByteBuffer;\n import java.util.List;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class WindowKeySchema implements RocksDBSegmentedBytesStore.KeySchema {\n \n+    private static final Logger LOG = LoggerFactory.getLogger(WindowKeySchema.class);\n+\n     private static final int SEQNUM_SIZE = 4;\n     private static final int TIMESTAMP_SIZE = 8;\n     private static final int SUFFIX_SIZE = TIMESTAMP_SIZE + SEQNUM_SIZE;\n@@ -99,8 +103,13 @@ public HasNextCondition hasNextCondition(final Bytes binaryKeyFrom,\n      */\n     static TimeWindow timeWindowForSize(final long startMs,\n                                         final long windowSize) {\n-        final long endMs = startMs + windowSize;\n-        return new TimeWindow(startMs, endMs < 0 ? Long.MAX_VALUE : endMs);\n+        long endMs = startMs + windowSize;\n+\n+        if (endMs < 0) {\n+            LOG.warn(\"Warning: window end time was truncated to Long.MAX\");\n+            endMs = Long.MAX_VALUE;\n+        }\n+        return new TimeWindow(startMs, endMs);\n     }\n \n     // for pipe serdes",
                "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java",
                "sha": "9218ccf0a7752d6084c1e80fc9cb3971b3a9357b",
                "status": "modified"
            }
        ],
        "message": "[HOT FIX] in-memory store behavior should match rocksDB (#6657)\n\nWhile working on consolidating the various store unit tests I uncovered some minor \"bugs\" in the in-memory stores (inconsistencies with the behavior as established by the RocksDB stores).\r\n\r\nopen iterators should be properly closed in the case the store is closed\r\nfetch/findSessions should always throw NPE if key is null\r\nwindow end time should be truncated at Long.MAX_VALUE rather than throw exception\r\n(Verified in-memory stores pass all applicable rocksDB tests now, unified unit tests coming in another PR)\r\n\r\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/a1b1e088b98763818e933dce335b580d02916640",
        "repo": "kafka",
        "unit_tests": [
            "InMemorySessionStoreTest.java",
            "InMemoryWindowStoreTest.java",
            "WindowKeySchemaTest.java"
        ]
    },
    "kafka_1f52881": {
        "bug_id": "kafka_1f52881",
        "commit": "https://github.com/apache/kafka/commit/1f528815de8d0e094ee5446794ab7325629ca7ed",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed",
                "deletions": 5,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java",
                "patch": "@@ -268,11 +268,7 @@ public void updateEndOffsets(Map<TopicPartition, Long> newOffsets) {\n     @Override\n     public List<PartitionInfo> partitionsFor(String topic) {\n         ensureNotClosed();\n-        List<PartitionInfo> parts = this.partitions.get(topic);\n-        if (parts == null)\n-            return Collections.emptyList();\n-        else\n-            return parts;\n+        return this.partitions.get(topic);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java",
                "sha": "9ab4c29493da28d05cf745906537f7d33c780475",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "patch": "@@ -186,7 +186,11 @@ public void register(StateStore store, boolean loggingEnabled, StateRestoreCallb\n                 // ignore\n             }\n \n-            for (PartitionInfo partitionInfo : restoreConsumer.partitionsFor(topic)) {\n+            List<PartitionInfo> partitionInfos = restoreConsumer.partitionsFor(topic);\n+            if (partitionInfos == null) {\n+                throw new StreamsException(\"Could not find partition info for topic: \" + topic);\n+            }\n+            for (PartitionInfo partitionInfo : partitionInfos) {\n                 if (partitionInfo.partition() == partition) {\n                     partitionNotFound = false;\n                     break;",
                "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "sha": "1d97384a9bf571be2cbd0f841fa13651c66b2a11",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed",
                "deletions": 9,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java",
                "patch": "@@ -152,15 +152,13 @@ public Subscription subscription(Set<String> topics) {\n      * @param topicToTaskIds Map that contains the topic names to be created\n      * @param compactTopic If true, the topic should be a compacted topic. This is used for\n      *                     change log topics usually.\n-     * @param outPartitionInfo If true, compute and return all partitions created\n      * @param postPartitionPhase If true, the computation for calculating the number of partitions\n      *                           is slightly different. Set to true after the initial topic-to-partition\n      *                           assignment.\n      * @return\n      */\n     private Map<TopicPartition, PartitionInfo> prepareTopic(Map<String, Set<TaskId>> topicToTaskIds,\n                                                             boolean compactTopic,\n-                                                            boolean outPartitionInfo,\n                                                             boolean postPartitionPhase) {\n         Map<TopicPartition, PartitionInfo> partitionInfos = new HashMap<>();\n         // if ZK is specified, prepare the internal source topic before calling partition grouper\n@@ -192,13 +190,24 @@ public Subscription subscription(Set<String> topics) {\n                     partitions = streamThread.restoreConsumer.partitionsFor(topic);\n                 } while (partitions == null || partitions.size() != numPartitions);\n \n-                if (outPartitionInfo) {\n-                    for (PartitionInfo partition : partitions)\n-                        partitionInfos.put(new TopicPartition(partition.topic(), partition.partition()), partition);\n-                }\n+                for (PartitionInfo partition : partitions)\n+                    partitionInfos.put(new TopicPartition(partition.topic(), partition.partition()), partition);\n             }\n \n             log.info(\"Completed validating internal topics in partition assignor.\");\n+        } else {\n+            List<String> missingTopics = new ArrayList<>();\n+            for (String topic : topicToTaskIds.keySet()) {\n+                List<PartitionInfo> partitions = streamThread.restoreConsumer.partitionsFor(topic);\n+                if (partitions == null) {\n+                    missingTopics.add(topic);\n+                }\n+            }\n+            if (!missingTopics.isEmpty()) {\n+                log.warn(\"Topic {} do not exists but couldn't created as the config '{}' isn't supplied\",\n+                         missingTopics, StreamsConfig.ZOOKEEPER_CONNECT_CONFIG);\n+\n+            }\n         }\n \n         return partitionInfos;\n@@ -284,7 +293,7 @@ public Subscription subscription(Set<String> topics) {\n             }\n         }\n \n-        Map<TopicPartition, PartitionInfo> internalPartitionInfos = prepareTopic(internalSourceTopicToTaskIds, false, true, false);\n+        Map<TopicPartition, PartitionInfo> internalPartitionInfos = prepareTopic(internalSourceTopicToTaskIds, false, false);\n         internalSourceTopicToTaskIds.clear();\n \n         Cluster metadataWithInternalTopics = metadata;\n@@ -380,9 +389,9 @@ public Subscription subscription(Set<String> topics) {\n         }\n \n         // if ZK is specified, validate the internal topics again\n-        prepareTopic(internalSourceTopicToTaskIds, false /* compactTopic */, false, true);\n+        prepareTopic(internalSourceTopicToTaskIds, false /* compactTopic */, true);\n         // change log topics should be compacted\n-        prepareTopic(stateChangelogTopicToTaskIds, true /* compactTopic */, false, true);\n+        prepareTopic(stateChangelogTopicToTaskIds, true /* compactTopic */, true);\n \n         return assignment;\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java",
                "sha": "f2eea36c11c1da3df58d1b67e3c26cb98f2f90e3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed",
                "deletions": 2,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "patch": "@@ -21,14 +21,14 @@\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.MockConsumer;\n import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n-import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.common.record.TimestampType;\n import org.apache.kafka.common.serialization.IntegerSerializer;\n import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.errors.StreamsException;\n import org.apache.kafka.streams.state.internals.OffsetCheckpoint;\n import org.apache.kafka.test.MockStateStoreSupplier;\n import org.junit.Test;\n@@ -223,7 +223,7 @@ public void testLockStateDirectory() throws IOException {\n         }\n     }\n \n-    @Test(expected = KafkaException.class)\n+    @Test(expected = StreamsException.class)\n     public void testNoTopic() throws IOException {\n         File baseDir = Files.createTempDirectory(stateDir).toFile();\n         try {",
                "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "sha": "890af0fc1e864487fa005d7692f8ef706afd7a06",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3642: Fix NPE from ProcessorStateManager when the changelog topic not exists\n\nIssue: https://issues.apache.org/jira/browse/KAFKA-3642\n\nAuthor: Yuto Kawamura <kawamuray.dadada@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1289 from kawamuray/KAFKA-3642-streams-NPE",
        "parent": "https://github.com/apache/kafka/commit/62253539d87e1ccb353673ed96adc98fb2d854ae",
        "repo": "kafka",
        "unit_tests": [
            "MockConsumerTest.java",
            "ProcessorStateManagerTest.java"
        ]
    },
    "kafka_1f692bd": {
        "bug_id": "kafka_1f692bd",
        "commit": "https://github.com/apache/kafka/commit/1f692bdf53af4a80b7fd256de4e94ff1d17fc861",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861",
                "deletions": 5,
                "filename": "clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java",
                "patch": "@@ -16,16 +16,17 @@\n  */\n package org.apache.kafka.common.header.internals;\n \n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.record.Record;\n+import org.apache.kafka.common.utils.AbstractIterator;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n-\n-import org.apache.kafka.common.header.Header;\n-import org.apache.kafka.common.header.Headers;\n-import org.apache.kafka.common.record.Record;\n-import org.apache.kafka.common.utils.AbstractIterator;\n+import java.util.Objects;\n \n public class RecordHeaders implements Headers {\n \n@@ -61,6 +62,7 @@ public RecordHeaders(Iterable<Header> headers) {\n \n     @Override\n     public Headers add(Header header) throws IllegalStateException {\n+        Objects.requireNonNull(header, \"Header cannot be null.\");\n         canWrite();\n         headers.add(header);\n         return this;",
                "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java",
                "sha": "5801bed99cd8c712b33c94031a2ed55ce4353791",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861",
                "deletions": 8,
                "filename": "clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java",
                "patch": "@@ -16,19 +16,19 @@\n  */\n package org.apache.kafka.common.header.internals;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.Headers;\n+import org.junit.Test;\n \n import java.io.IOException;\n import java.util.Arrays;\n import java.util.Iterator;\n \n-import org.apache.kafka.common.header.Header;\n-import org.apache.kafka.common.header.Headers;\n-import org.junit.Test;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n public class RecordHeadersTest {\n \n@@ -206,6 +206,11 @@ public void testNew() throws IOException {\n         assertEquals(2, getCount(newHeaders));\n     }\n \n+    @Test(expected = NullPointerException.class)\n+    public void shouldThrowNpeWhenAddingNullHeader() {\n+        new RecordHeaders().add(null);\n+    }\n+\n     private int getCount(Headers headers) {\n         int count = 0;\n         Iterator<Header> headerIterator = headers.iterator();",
                "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java",
                "sha": "5b9f95ea91f182a75551f96f9c074ced41aee3c0",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java",
                "patch": "@@ -90,7 +90,10 @@ public long sizeBytes() {\n         if (headers != null) {\n             for (final Header header : headers) {\n                 size += header.key().toCharArray().length;\n-                size += header.value().length;\n+                final byte[] value = header.value();\n+                if (value != null) {\n+                    size += value.length;\n+                }\n             }\n         }\n         return size;",
                "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java",
                "sha": "00012746bd02dddcc5c07929912d3a3192dc147b",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java",
                "patch": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class ProcessorRecordContextTest {\n+    // timestamp + offset + partition: 8 + 8 + 4\n+    private final static long MIN_SIZE = 20L;\n+\n+    @Test\n+    public void shouldEstimateNullTopicAndNullHeadersAsZeroLength() {\n+        final Headers headers = new RecordHeaders();\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            null\n+        );\n+\n+        assertEquals(MIN_SIZE, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateEmptyHeaderAsZeroLength() {\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            new RecordHeaders()\n+        );\n+\n+        assertEquals(MIN_SIZE, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateTopicLength() {\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            \"topic\",\n+            null\n+        );\n+\n+        assertEquals(MIN_SIZE + 5L, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateHeadersLength() {\n+        final Headers headers = new RecordHeaders();\n+        headers.add(\"header-key\", \"header-value\".getBytes());\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            headers\n+        );\n+\n+        assertEquals(MIN_SIZE + 10L + 12L, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateNullValueInHeaderAsZero() {\n+        final Headers headers = new RecordHeaders();\n+        headers.add(\"header-key\", null);\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            headers\n+        );\n+\n+        assertEquals(MIN_SIZE + 10L, context.sizeBytes());\n+    }\n+}",
                "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java",
                "sha": "1ea646fce2f3e8860d844c6a3939a4b6958f5b21",
                "status": "added"
            }
        ],
        "message": "KAFKA-8142: Fix NPE for nulls in Headers (#6484)\n\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/3124b070666d3b8cdc0e96067e69b8e7a6245742",
        "repo": "kafka",
        "unit_tests": [
            "RecordHeadersTest.java",
            "ProcessorRecordContextTest.java"
        ]
    },
    "kafka_1f8527b": {
        "bug_id": "kafka_1f8527b",
        "commit": "https://github.com/apache/kafka/commit/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/checkstyle/suppressions.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5",
                "deletions": 1,
                "filename": "checkstyle/suppressions.xml",
                "patch": "@@ -73,7 +73,7 @@\n               files=\"RequestResponseTest.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"MemoryRecordsTest.java\"/>\n+              files=\"MemoryRecordsTest|MetricsTest\"/>\n \n     <!-- Connect -->\n     <suppress checks=\"ClassFanOutComplexity\"",
                "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/checkstyle/suppressions.xml",
                "sha": "e80d5bf24c1cfa2dbe1869393142a040a9c2e36a",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5",
                "deletions": 9,
                "filename": "clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java",
                "patch": "@@ -48,6 +48,7 @@\n     private final Time time;\n     private volatile long lastRecordTime;\n     private final long inactiveSensorExpirationTimeMs;\n+    private final Object metricLock;\n \n     public enum RecordingLevel {\n         INFO(0, \"INFO\"), DEBUG(1, \"DEBUG\");\n@@ -113,6 +114,7 @@ public boolean shouldRecord(final int configId) {\n         this.inactiveSensorExpirationTimeMs = TimeUnit.MILLISECONDS.convert(inactiveSensorExpirationTimeSeconds, TimeUnit.SECONDS);\n         this.lastRecordTime = time.milliseconds();\n         this.recordingLevel = recordingLevel;\n+        this.metricLock = new Object();\n         checkForest(new HashSet<Sensor>());\n     }\n \n@@ -174,9 +176,11 @@ public void record(double value, long timeMs, boolean checkQuotas) {\n         if (shouldRecord()) {\n             this.lastRecordTime = timeMs;\n             synchronized (this) {\n-                // increment all the stats\n-                for (Stat stat : this.stats)\n-                    stat.record(config, value, timeMs);\n+                synchronized (metricLock()) {\n+                    // increment all the stats\n+                    for (Stat stat : this.stats)\n+                        stat.record(config, value, timeMs);\n+                }\n                 if (checkQuotas)\n                     checkQuotas(timeMs);\n             }\n@@ -229,7 +233,7 @@ public synchronized boolean add(CompoundStat stat, MetricConfig config) {\n             return false;\n \n         this.stats.add(Utils.notNull(stat));\n-        Object lock = metricLock(stat);\n+        Object lock = metricLock();\n         for (NamedMeasurable m : stat.stats()) {\n             final KafkaMetric metric = new KafkaMetric(lock, m.name(), m.stat(), config == null ? this.config : config, time);\n             if (!metrics.containsKey(metric.metricName())) {\n@@ -265,7 +269,7 @@ public synchronized boolean add(final MetricName metricName, final MeasurableSta\n             return true;\n         } else {\n             final KafkaMetric metric = new KafkaMetric(\n-                metricLock(stat),\n+                metricLock(),\n                 Utils.notNull(metricName),\n                 Utils.notNull(stat),\n                 config == null ? this.config : config,\n@@ -291,10 +295,26 @@ public boolean hasExpired() {\n     }\n \n     /**\n-     * KafkaMetrics of sensors which use SampledStat should be synchronized on the Sensor object\n-     * to allow concurrent reads and updates. For simplicity, all sensors are synchronized on Sensor.\n+     * KafkaMetrics of sensors which use SampledStat should be synchronized on the same lock\n+     * for sensor record and metric value read to allow concurrent reads and updates. For simplicity,\n+     * all sensors are synchronized on this object.\n+     * <p>\n+     * Sensor object is not used as a lock for reading metric value since metrics reporter is\n+     * invoked while holding Sensor and Metrics locks to report addition and removal of metrics\n+     * and synchronized reporters may deadlock if Sensor lock is used for reading metrics values.\n+     * Note that Sensor object itself is used as a lock to protect the access to stats and metrics\n+     * while recording metric values, adding and deleting sensors.\n+     * </p><p>\n+     * Locking order (assume all MetricsReporter methods may be synchronized):\n+     * <ul>\n+     *   <li>Sensor#add: Sensor -> Metrics -> MetricsReporter</li>\n+     *   <li>Metrics#removeSensor: Sensor -> Metrics -> MetricsReporter</li>\n+     *   <li>KafkaMetric#metricValue: MetricsReporter -> Sensor#metricLock</li>\n+     *   <li>Sensor#record: Sensor -> Sensor#metricLock</li>\n+     * </ul>\n+     * </p>\n      */\n-    private Object metricLock(Stat stat) {\n-        return this;\n+    private Object metricLock() {\n+        return metricLock;\n     }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java",
                "sha": "ccbe8aad9cde51ee9c5bc7cdc4a20f913b6a0ade",
                "status": "modified"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java",
                "changes": 117,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5",
                "deletions": 12,
                "filename": "clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java",
                "patch": "@@ -26,13 +26,16 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Deque;\n+import java.util.List;\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.ConcurrentLinkedDeque;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n+\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.kafka.common.Metric;\n@@ -54,9 +57,12 @@\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n @SuppressWarnings(\"deprecation\")\n public class MetricsTest {\n+    private static final Logger log = LoggerFactory.getLogger(MetricsTest.class);\n \n     private static final double EPS = 0.000001;\n     private MockTime time = new MockTime();\n@@ -604,25 +610,21 @@ public void testMetricInstances() {\n         }\n     }\n \n+    /**\n+     * Verifies that concurrent sensor add, remove, updates and read don't result\n+     * in errors or deadlock.\n+     */\n     @Test\n-    public void testConcurrentAccess() throws Exception {\n+    public void testConcurrentReadUpdate() throws Exception {\n         final Random random = new Random();\n         final Deque<Sensor> sensors = new ConcurrentLinkedDeque<>();\n         metrics = new Metrics(new MockTime(10));\n         SensorCreator sensorCreator = new SensorCreator(metrics);\n \n         final AtomicBoolean alive = new AtomicBoolean(true);\n         executorService = Executors.newSingleThreadExecutor();\n-        executorService.submit(new Runnable() {\n-            @Override\n-            public void run() {\n-                while (alive.get()) {\n-                    for (Sensor sensor : sensors) {\n-                        sensor.record(random.nextInt(10000));\n-                    }\n-                }\n-            }\n-        });\n+        executorService.submit(new ConcurrentMetricOperation(alive, \"record\",\n+            () -> sensors.forEach(sensor -> sensor.record(random.nextInt(10000)))));\n \n         for (int i = 0; i < 10000; i++) {\n             if (sensors.size() > 5) {\n@@ -640,6 +642,97 @@ public void run() {\n         alive.set(false);\n     }\n \n+    /**\n+     * Verifies that concurrent sensor add, remove, updates and read with a metrics reporter\n+     * that synchronizes on every reporter method doesn't result in errors or deadlock.\n+     */\n+    @Test\n+    public void testConcurrentReadUpdateReport() throws Exception {\n+\n+        class LockingReporter implements MetricsReporter {\n+            Map<MetricName, KafkaMetric> activeMetrics = new HashMap<>();\n+            @Override\n+            public synchronized void init(List<KafkaMetric> metrics) {\n+            }\n+\n+            @Override\n+            public synchronized void metricChange(KafkaMetric metric) {\n+                activeMetrics.put(metric.metricName(), metric);\n+            }\n+\n+            @Override\n+            public synchronized void metricRemoval(KafkaMetric metric) {\n+                activeMetrics.remove(metric.metricName(), metric);\n+            }\n+\n+            @Override\n+            public synchronized void close() {\n+            }\n+\n+            @Override\n+            public void configure(Map<String, ?> configs) {\n+            }\n+\n+            synchronized void processMetrics() {\n+                for (KafkaMetric metric : activeMetrics.values()) {\n+                    assertNotNull(\"Invalid metric value\", metric.metricValue());\n+                }\n+            }\n+        }\n+\n+        final LockingReporter reporter = new LockingReporter();\n+        this.metrics.close();\n+        this.metrics = new Metrics(config, Arrays.asList((MetricsReporter) reporter), new MockTime(10), true);\n+        final Deque<Sensor> sensors = new ConcurrentLinkedDeque<>();\n+        SensorCreator sensorCreator = new SensorCreator(metrics);\n+\n+        final Random random = new Random();\n+        final AtomicBoolean alive = new AtomicBoolean(true);\n+        executorService = Executors.newFixedThreadPool(3);\n+\n+        Future<?> writeFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"record\",\n+            () -> sensors.forEach(sensor -> sensor.record(random.nextInt(10000)))));\n+        Future<?> readFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"read\",\n+            () -> sensors.forEach(sensor -> sensor.metrics().forEach(metric ->\n+                assertNotNull(\"Invalid metric value\", metric.metricValue())))));\n+        Future<?> reportFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"report\",\n+            () -> reporter.processMetrics()));\n+\n+        for (int i = 0; i < 10000; i++) {\n+            if (sensors.size() > 10) {\n+                Sensor sensor = random.nextBoolean() ? sensors.removeFirst() : sensors.removeLast();\n+                metrics.removeSensor(sensor.name());\n+            }\n+            StatType statType = StatType.forId(random.nextInt(StatType.values().length));\n+            sensors.add(sensorCreator.createSensor(statType, i));\n+        }\n+        assertFalse(\"Read failed\", readFuture.isDone());\n+        assertFalse(\"Write failed\", writeFuture.isDone());\n+        assertFalse(\"Report failed\", reportFuture.isDone());\n+\n+        alive.set(false);\n+    }\n+\n+    private class ConcurrentMetricOperation implements Runnable {\n+        private final AtomicBoolean alive;\n+        private final String opName;\n+        private final Runnable op;\n+        ConcurrentMetricOperation(AtomicBoolean alive, String opName, Runnable op) {\n+            this.alive = alive;\n+            this.opName = opName;\n+            this.op = op;\n+        }\n+        public void run() {\n+            try {\n+                while (alive.get()) {\n+                    op.run();\n+                }\n+            } catch (Throwable t) {\n+                log.error(\"Metric {} failed with exception\", opName, t);\n+            }\n+        }\n+    }\n+\n     enum StatType {\n         AVG(0),\n         TOTAL(1),\n@@ -676,7 +769,7 @@ static StatType forId(int id) {\n         }\n \n         private Sensor createSensor(StatType statType, int index) {\n-            Sensor sensor = metrics.sensor(\"kafka.requests\");\n+            Sensor sensor = metrics.sensor(\"kafka.requests.\" + index);\n             Map<String, String> tags = Collections.singletonMap(\"tag\", \"tag\" + index);\n             switch (statType) {\n                 case AVG:",
                "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java",
                "sha": "59bc84e40decf49ea22902f572e84f32af21ee29",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7136: Avoid deadlocks in synchronized metrics reporters (#5341)\n\nWe need to use the same lock for metric update and read to avoid NPE and concurrent modification exceptions. Sensor add/remove/update are synchronized on Sensor since they access lists and maps that are not thread-safe. Reporters are notified of metrics add/remove while holding (Sensor, Metrics) locks and reporters may synchronize on the reporter lock. Metric read may be invoked by metrics reporters while holding a reporter lock. So read/update cannot be synchronized using Sensor since that could lead to deadlock. This PR introduces a new lock in Sensor for update/read.\r\nLocking order:\r\n\r\n- Sensor#add: Sensor -> Metrics -> MetricsReporter\r\n- Metrics#removeSensor: Sensor -> Metrics -> MetricsReporter\r\n- KafkaMetric#metricValue: MetricsReporter -> Sensor#metricLock\r\n- Sensor#record: Sensor -> Sensor#metricLock\r\n\r\n\r\nReviewers: Jun Rao <junrao@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/1aea0834d346f0afa16f946c47e51fefac37612b",
        "repo": "kafka",
        "unit_tests": [
            "SensorTest.java"
        ]
    },
    "kafka_289ac09": {
        "bug_id": "kafka_289ac09",
        "commit": "https://github.com/apache/kafka/commit/289ac092923a32e5a839750fcaef0ae0856d05b3",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java?ref=289ac092923a32e5a839750fcaef0ae0856d05b3",
                "deletions": 1,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "patch": "@@ -21,6 +21,7 @@\n import org.apache.kafka.common.config.ConfigTransformer;\n import org.apache.kafka.common.config.ConfigTransformerResult;\n import org.apache.kafka.connect.runtime.Herder.ConfigReloadAction;\n+import org.apache.kafka.connect.util.Callback;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -86,7 +87,15 @@ private void scheduleReload(String connectorName, String path, long ttl) {\n             }\n         }\n         log.info(\"Scheduling a restart of connector {} in {} ms\", connectorName, ttl);\n-        HerderRequest request = worker.herder().restartConnector(ttl, connectorName, null);\n+        Callback<Void> cb = new Callback<Void>() {\n+            @Override\n+            public void onCompletion(Throwable error, Void result) {\n+                if (error != null) {\n+                    log.error(\"Unexpected error during connector restart: \", error);\n+                }\n+            }\n+        };\n+        HerderRequest request = worker.herder().restartConnector(ttl, connectorName, cb);\n         connectorRequests.put(path, request);\n     }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "sha": "1a799bb37297699106c4ac8df3098af73898b165",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java?ref=289ac092923a32e5a839750fcaef0ae0856d05b3",
                "deletions": 4,
                "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.common.config.ConfigData;\n import org.apache.kafka.common.config.provider.ConfigProvider;\n import org.easymock.EasyMock;\n+import static org.easymock.EasyMock.eq;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -34,6 +35,7 @@\n \n import static org.apache.kafka.connect.runtime.ConnectorConfig.CONFIG_RELOAD_ACTION_CONFIG;\n import static org.apache.kafka.connect.runtime.ConnectorConfig.CONFIG_RELOAD_ACTION_NONE;\n+import static org.easymock.EasyMock.notNull;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.powermock.api.easymock.PowerMock.replayAll;\n@@ -84,8 +86,7 @@ public void testReplaceVariableWithTTL() {\n     @Test\n     public void testReplaceVariableWithTTLAndScheduleRestart() {\n         EasyMock.expect(worker.herder()).andReturn(herder);\n-        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);\n-\n+        EasyMock.expect(herder.restartConnector(eq(1L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n         replayAll();\n \n         Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, \"${test:testPath:testKeyWithTTL}\"));\n@@ -95,13 +96,13 @@ public void testReplaceVariableWithTTLAndScheduleRestart() {\n     @Test\n     public void testReplaceVariableWithTTLFirstCancelThenScheduleRestart() {\n         EasyMock.expect(worker.herder()).andReturn(herder);\n-        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);\n+        EasyMock.expect(herder.restartConnector(eq(1L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n \n         EasyMock.expect(worker.herder()).andReturn(herder);\n         EasyMock.expectLastCall();\n         requestId.cancel();\n         EasyMock.expectLastCall();\n-        EasyMock.expect(herder.restartConnector(10L, MY_CONNECTOR, null)).andReturn(requestId);\n+        EasyMock.expect(herder.restartConnector(eq(10L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n \n         replayAll();\n ",
                "raw_url": "https://github.com/apache/kafka/raw/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "sha": "6f4bda66904d7fc25e903ede7d35a440b082b3c0",
                "status": "modified"
            }
        ],
        "message": "KAFKA-8591; WorkerConfigTransformer NPE on connector configuration reloading (#6991)\n\nA bug in `WorkerConfigTransformer` prevents the connector configuration reload when the ConfigData TTL expires. \r\n\r\nThe issue boils down to the fact that `worker.herder().restartConnector` is receiving a null callback. \r\n\r\n```\r\n[2019-06-17 14:34:12,320] INFO Scheduling a restart of connector workshop-incremental in 60000 ms (org.apache.kafka.connect.runtime.WorkerConfigTransformer:88)\r\n[2019-06-17 14:34:12,321] ERROR Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:227)\r\njava.lang.NullPointerException\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1187)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1183)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:273)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:219)\r\n```\r\nThis patch adds a callback which just logs the error.\r\n\r\nReviewers: Robert Yokota <rayokota@gmail.com>, Jason Gustafson <jason@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/05cba28ca7aafd3974e9e818be08f239b6162855",
        "repo": "kafka",
        "unit_tests": [
            "WorkerConfigTransformerTest.java"
        ]
    },
    "kafka_3000fda": {
        "bug_id": "kafka_3000fda",
        "commit": "https://github.com/apache/kafka/commit/3000fda8b9800f4cddfc9675dfae11f762aabccc",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java?ref=3000fda8b9800f4cddfc9675dfae11f762aabccc",
                "deletions": 5,
                "filename": "connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java",
                "patch": "@@ -274,7 +274,7 @@ public Header lastWithName(String key) {\n     @Override\n     public Headers remove(String key) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             Iterator<Header> iterator = iterator();\n             while (iterator.hasNext()) {\n                 if (iterator.next().key().equals(key)) {\n@@ -287,7 +287,7 @@ public Headers remove(String key) {\n \n     @Override\n     public Headers retainLatest() {\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             Set<String> keys = new HashSet<>();\n             ListIterator<Header> iter = headers.listIterator(headers.size());\n             while (iter.hasPrevious()) {\n@@ -304,7 +304,7 @@ public Headers retainLatest() {\n     @Override\n     public Headers retainLatest(String key) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             boolean found = false;\n             ListIterator<Header> iter = headers.listIterator(headers.size());\n             while (iter.hasPrevious()) {\n@@ -322,7 +322,7 @@ public Headers retainLatest(String key) {\n     @Override\n     public Headers apply(String key, HeaderTransform transform) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             ListIterator<Header> iter = headers.listIterator();\n             while (iter.hasNext()) {\n                 Header orig = iter.next();\n@@ -341,7 +341,7 @@ public Headers apply(String key, HeaderTransform transform) {\n \n     @Override\n     public Headers apply(HeaderTransform transform) {\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             ListIterator<Header> iter = headers.listIterator();\n             while (iter.hasNext()) {\n                 Header orig = iter.next();",
                "raw_url": "https://github.com/apache/kafka/raw/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java",
                "sha": "0b5c484b3533e124a76cec330f2c3ec8f6361e8d",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/kafka/blob/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java?ref=3000fda8b9800f4cddfc9675dfae11f762aabccc",
                "deletions": 1,
                "filename": "connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java",
                "patch": "@@ -118,6 +118,14 @@ public void shouldHaveToString() {\n         assertNotNull(headers.toString());\n     }\n \n+    @Test\n+    public void shouldRetainLatestWhenEmpty() {\n+        headers.retainLatest(other);\n+        headers.retainLatest(key);\n+        headers.retainLatest();\n+        assertTrue(headers.isEmpty());\n+    }\n+\n     @Test\n     public void shouldAddMultipleHeadersWithSameKeyAndRetainLatest() {\n         populate(headers);\n@@ -179,6 +187,12 @@ public void shouldNotAddHeadersWithObjectValuesAndMismatchedSchema() {\n         attemptAndFailToAddHeader(\"k2\", Schema.OPTIONAL_STRING_SCHEMA, 0L);\n     }\n \n+    @Test\n+    public void shouldRemoveAllHeadersWithSameKeyWhenEmpty() {\n+        headers.remove(key);\n+        assertNoHeaderWithKey(key);\n+    }\n+\n     @Test\n     public void shouldRemoveAllHeadersWithSameKey() {\n         populate(headers);\n@@ -211,6 +225,13 @@ public void shouldRemoveAllHeaders() {\n         assertTrue(headers.isEmpty());\n     }\n \n+    @Test\n+    public void shouldTransformHeadersWhenEmpty() {\n+        headers.apply(appendToKey(\"-suffix\"));\n+        headers.apply(key, appendToKey(\"-suffix\"));\n+        assertTrue(headers.isEmpty());\n+    }\n+\n     @Test\n     public void shouldTransformHeaders() {\n         populate(headers);\n@@ -544,4 +565,4 @@ protected void assertHeader(Header header, String key, Schema schema, Object val\n         assertSame(schema, header.schema());\n         assertSame(value, header.value());\n     }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/kafka/raw/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java",
                "sha": "72418ba47ffa7ba84f6fe07b22ca159a07aa7dcc",
                "status": "modified"
            }
        ],
        "message": "KAFKA-8277: Fix NPEs in several methods of ConnectHeaders (#6550)\n\nReplace `headers.isEmpty()` by calls to `isEmpty()` as the latter does a null check on heathers (that is lazily created).\r\n\r\nAuthor: Sebasti\u00e1n Ortega <sebastian.ortega@letgo.com>\r\nReviewers: Konstantine Karantasis <konstantine@confluent.io>, Arjun Satish <arjunconfluent.io>, Randall Hauch <rhauch@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/c2bee988faa3c338fdc275aac9f638bb652b38a0",
        "repo": "kafka",
        "unit_tests": [
            "ConnectHeadersTest.java"
        ]
    },
    "kafka_3b8b7a4": {
        "bug_id": "kafka_3b8b7a4",
        "commit": "https://github.com/apache/kafka/commit/3b8b7a4be39cb4a3f7257f8a17f74a887572e627",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java?ref=3b8b7a4be39cb4a3f7257f8a17f74a887572e627",
                "deletions": 6,
                "filename": "clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java",
                "patch": "@@ -31,7 +31,6 @@\n  * An internal class which represents the API versions supported by a particular node.\n  */\n public class NodeApiVersions {\n-    private static final Short API_NOT_ON_NODE = null;\n     private static final short NODE_TOO_OLD = (short) -1;\n     private static final short NODE_TOO_NEW = (short) -2;\n     private final Collection<ApiVersion> nodeApiVersions;\n@@ -47,7 +46,7 @@\n      * @return A new NodeApiVersions object.\n      */\n     public static NodeApiVersions create() {\n-        return create(Collections.EMPTY_LIST);\n+        return create(Collections.<ApiVersion>emptyList());\n     }\n \n     /**\n@@ -98,7 +97,7 @@ public NodeApiVersions(Collection<ApiVersion> nodeApiVersions) {\n      */\n     public short usableVersion(ApiKeys apiKey) {\n         Short usableVersion = usableVersions.get(apiKey);\n-        if (usableVersion == API_NOT_ON_NODE)\n+        if (usableVersion == null)\n             throw new UnsupportedVersionException(\"The broker does not support \" + apiKey);\n         else if (usableVersion == NODE_TOO_OLD)\n             throw new UnsupportedVersionException(\"The broker is too old to support \" + apiKey +\n@@ -160,17 +159,17 @@ private String apiVersionToText(ApiVersion apiVersion) {\n         ApiKeys apiKey = null;\n         if (ApiKeys.hasId(apiVersion.apiKey)) {\n             apiKey = ApiKeys.forId(apiVersion.apiKey);\n-        }\n-        if (apiKey != null) {\n             bld.append(apiKey.name).append(\"(\").append(apiKey.id).append(\"): \");\n         } else {\n-            bld.append(\"UNKNOWN(\").append(apiKey.id).append(\"): \");\n+            bld.append(\"UNKNOWN(\").append(apiVersion.apiKey).append(\"): \");\n         }\n+\n         if (apiVersion.minVersion == apiVersion.maxVersion) {\n             bld.append(apiVersion.minVersion);\n         } else {\n             bld.append(apiVersion.minVersion).append(\" to \").append(apiVersion.maxVersion);\n         }\n+\n         if (apiKey != null) {\n             Short usableVersion = usableVersions.get(apiKey);\n             if (usableVersion == NODE_TOO_OLD)",
                "raw_url": "https://github.com/apache/kafka/raw/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java",
                "sha": "906c2264c55ceb319f4767124f0f2f0eb370126f",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/kafka/blob/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java?ref=3b8b7a4be39cb4a3f7257f8a17f74a887572e627",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import java.util.List;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n public class NodeApiVersionsTest {\n \n@@ -47,6 +48,13 @@ public void testUnsupportedVersionsToString() {\n         assertEquals(bld.toString(), versions.toString());\n     }\n \n+    @Test\n+    public void testUnknownApiVersionsToString() {\n+        ApiVersion unknownApiVersion = new ApiVersion((short) 337, (short) 0, (short) 1);\n+        NodeApiVersions versions = new NodeApiVersions(Collections.singleton(unknownApiVersion));\n+        assertTrue(versions.toString().endsWith(\"UNKNOWN(337): 0 to 1)\"));\n+    }\n+\n     @Test\n     public void testVersionsToString() {\n         List<ApiVersion> versionList = new ArrayList<>();",
                "raw_url": "https://github.com/apache/kafka/raw/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java",
                "sha": "53c47c833bfb7b91c68f761e8acfeae7225072c3",
                "status": "modified"
            }
        ],
        "message": "MINOR: Fix NPE handling unknown APIs in NodeApiVersions.toString\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Colin P. Mccabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #2561 from hachikuji/fix-npe-api-version-tostring",
        "parent": "https://github.com/apache/kafka/commit/e41e782006d807a1ca8098dbfb95b8ab2295d6af",
        "repo": "kafka",
        "unit_tests": [
            "NodeApiVersionsTest.java"
        ]
    },
    "kafka_3d74196": {
        "bug_id": "kafka_3d74196",
        "commit": "https://github.com/apache/kafka/commit/3d74196f205c53946b6fc3dd0501aa2095f0031a",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "patch": "@@ -329,7 +329,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param key               Key to use to for partition\n      * @param keySerializer     Serializer for the key\n      * @param <K>               key type\n-     * @return  The {@link StreamsMetadata} for the storeName and key\n+     * @return  The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public <K> StreamsMetadata metadataForKey(final String storeName,\n                                               final K key,\n@@ -350,7 +351,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param key               Key to use to for partition\n      * @param partitioner       Partitioner for the store\n      * @param <K>               key type\n-     * @return  The {@link StreamsMetadata} for the storeName and key\n+     * @return  The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public <K> StreamsMetadata metadataForKey(final String storeName,\n                                               final K key,\n@@ -368,6 +370,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param queryableStoreType    accept only stores that are accepted by {@link QueryableStoreType#accepts(StateStore)}\n      * @param <T>                   return type\n      * @return  A facade wrapping the {@link org.apache.kafka.streams.processor.StateStore} instances\n+     * @throws org.apache.kafka.streams.errors.InvalidStateStoreException if the streams are (re-)initializing or\n+     * a store with storeName and queryableStoreType doesnt' exist.\n      */\n     public <T> T store(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n         validateIsRunning();",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "sha": "d88d09ef1d90d690bb749bf0dd453650e24696bb",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "patch": "@@ -70,6 +70,10 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n     public synchronized Collection<StreamsMetadata> getAllMetadataForStore(final String storeName) {\n         Objects.requireNonNull(storeName, \"storeName cannot be null\");\n \n+        if (!isInitialized()) {\n+            return Collections.emptyList();\n+        }\n+\n         final Set<String> sourceTopics = builder.stateStoreNameToSourceTopics().get(storeName);\n         if (sourceTopics == null) {\n             return Collections.emptyList();\n@@ -96,7 +100,8 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n      * @param key           Key to use\n      * @param keySerializer Serializer for the key\n      * @param <K>           key type\n-     * @return The {@link StreamsMetadata} for the storeName and key\n+     * @return The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public synchronized <K> StreamsMetadata getMetadataWithKey(final String storeName,\n                                                                final K key,\n@@ -105,10 +110,15 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n         Objects.requireNonNull(storeName, \"storeName can't be null\");\n         Objects.requireNonNull(key, \"key can't be null\");\n \n+        if (!isInitialized()) {\n+            return StreamsMetadata.NOT_AVAILABLE;\n+        }\n+\n         final SourceTopicsInfo sourceTopicsInfo = getSourceTopicsInfo(storeName);\n         if (sourceTopicsInfo == null) {\n             return null;\n         }\n+\n         return getStreamsMetadataForKey(storeName,\n                                         key,\n                                         new DefaultStreamPartitioner<>(keySerializer,\n@@ -131,7 +141,8 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n      * @param key         Key to use\n      * @param partitioner partitioner to use to find correct partition for key\n      * @param <K>         key type\n-     * @return The {@link StreamsMetadata} for the storeName and key\n+     * @return The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public synchronized <K> StreamsMetadata getMetadataWithKey(final String storeName,\n                                                                final K key,\n@@ -140,6 +151,10 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n         Objects.requireNonNull(key, \"key can't be null\");\n         Objects.requireNonNull(partitioner, \"partitioner can't be null\");\n \n+        if (!isInitialized()) {\n+            return StreamsMetadata.NOT_AVAILABLE;\n+        }\n+\n         SourceTopicsInfo sourceTopicsInfo = getSourceTopicsInfo(storeName);\n         if (sourceTopicsInfo == null) {\n             return null;\n@@ -218,6 +233,10 @@ private SourceTopicsInfo getSourceTopicsInfo(final String storeName) {\n         return new SourceTopicsInfo(sourceTopics);\n     }\n \n+    private boolean isInitialized() {\n+        return !clusterMetadata.topics().isEmpty();\n+    }\n+\n     private class SourceTopicsInfo {\n         private final Set<String> sourceTopics;\n         private int maxPartitions;",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java",
                "sha": "6f9bea69af2356ffafc38cdb5abe45161f932b82",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java",
                "patch": "@@ -19,6 +19,7 @@\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.streams.KafkaStreams;\n \n+import java.util.Collections;\n import java.util.Set;\n \n /**\n@@ -29,6 +30,14 @@\n  * NOTE: This is a point in time view. It may change when rebalances happen.\n  */\n public class StreamsMetadata {\n+    /**\n+     * Sentinel to indicate that the StreamsMetadata is currently unavailable. This can occur during rebalance\n+     * operations.\n+     */\n+    public final static StreamsMetadata NOT_AVAILABLE = new StreamsMetadata(new HostInfo(\"unavailable\", -1),\n+                                                                            Collections.<String>emptySet(),\n+                                                                            Collections.<TopicPartition>emptySet());\n+\n     private final HostInfo hostInfo;\n     private final Set<String> stateStoreNames;\n     private final Set<TopicPartition> topicPartitions;",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java",
                "sha": "9602bfe10a5ab18064712f3c04894ca4139063fb",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 5,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java",
                "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.KeyValueIterator;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n@@ -48,10 +49,15 @@ public CompositeReadOnlyKeyValueStore(final StateStoreProvider storeProvider,\n     public V get(final K key) {\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);\n         for (ReadOnlyKeyValueStore<K, V> store : stores) {\n-            V result = store.get(key);\n-            if (result != null) {\n-                return result;\n+            try {\n+                final V result = store.get(key);\n+                if (result != null) {\n+                    return result;\n+                }\n+            } catch (InvalidStateStoreException e) {\n+                throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n             }\n+\n         }\n         return null;\n     }\n@@ -61,7 +67,11 @@ public V get(final K key) {\n         final NextIteratorFunction<K, V> nextIteratorFunction = new NextIteratorFunction<K, V>() {\n             @Override\n             public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {\n-                return store.range(from, to);\n+                try {\n+                    return store.range(from, to);\n+                } catch (InvalidStateStoreException e) {\n+                    throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n+                }\n             }\n         };\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);\n@@ -73,7 +83,11 @@ public V get(final K key) {\n         final NextIteratorFunction<K, V> nextIteratorFunction = new NextIteratorFunction<K, V>() {\n             @Override\n             public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {\n-                return store.all();\n+                try {\n+                    return store.all();\n+                } catch (InvalidStateStoreException e) {\n+                    throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n+                }\n             }\n         };\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java",
                "sha": "5c47419a1bc2539831260ed92bd819cff67e5e77",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 5,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java",
                "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n import org.apache.kafka.streams.state.WindowStoreIterator;\n@@ -44,11 +45,15 @@ public CompositeReadOnlyWindowStore(final StateStoreProvider provider,\n     public WindowStoreIterator<V> fetch(final K key, final long timeFrom, final long timeTo) {\n         final List<ReadOnlyWindowStore<K, V>> stores = provider.stores(storeName, windowStoreType);\n         for (ReadOnlyWindowStore<K, V> windowStore : stores) {\n-            final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);\n-            if (!result.hasNext()) {\n-                result.close();\n-            } else {\n-                return result;\n+            try {\n+                final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);\n+                if (!result.hasNext()) {\n+                    result.close();\n+                } else {\n+                    return result;\n+                }\n+            } catch (InvalidStateStoreException e) {\n+                throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n             }\n         }\n         return new WindowStoreIterator<V>() {",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java",
                "sha": "b33c0f0ceab3dc4d1b6854381bde22a14c068a6c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java",
                "patch": "@@ -46,7 +46,7 @@ public QueryableStoreProvider(final List<StateStoreProvider> storeProviders) {\n             allStores.addAll(storeProvider.stores(storeName, queryableStoreType));\n         }\n         if (allStores.isEmpty()) {\n-            throw new InvalidStateStoreException(\"Store: \" + storeName + \" is currently not available\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         return queryableStoreType.create(\n                 new WrappingStoreProvider(storeProviders),",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java",
                "sha": "64dac1f24b124f82aa407652114078fd6da867af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java",
                "patch": "@@ -39,14 +39,14 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n     @Override\n     public <T> List<T> stores(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n         if (!streamThread.isInitialized()) {\n-            throw new InvalidStateStoreException(\"Store: \" + storeName + \" is currently not available as the stream thread has not (re-)initialized yet\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         final List<T> stores = new ArrayList<>();\n         for (StreamTask streamTask : streamThread.tasks().values()) {\n             final StateStore store = streamTask.getStore(storeName);\n             if (store != null && queryableStoreType.accepts(store)) {\n                 if (!store.isOpen()) {\n-                    throw new InvalidStateStoreException(\"Store: \" + storeName + \" isn't isOpen\");\n+                    throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n                 }\n                 stores.add((T) store);\n             }",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java",
                "sha": "3a50a68a7d09bbab9e42b29bd882ff4b4ddce3b1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java",
                "patch": "@@ -48,8 +48,7 @@ public WrappingStoreProvider(final List<StateStoreProvider> storeProviders) {\n             allStores.addAll(stores);\n         }\n         if (allStores.isEmpty()) {\n-            throw new InvalidStateStoreException(\"Store \" + storeName + \" is currently \"\n-                                                 + \"unavailable\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         return allStores;\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java",
                "sha": "eb1bc6473870a00cd1b68df1de82673fe4c0c72e",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 24,
                "filename": "streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java",
                "patch": "@@ -74,8 +74,6 @@\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.core.IsEqual.equalTo;\n \n-\n-\n @RunWith(Parameterized.class)\n public class QueryableStateIntegrationTest {\n     private static final int NUM_BROKERS = 1;\n@@ -265,24 +263,23 @@ private void verifyAllKVKeys(final StreamRunnable[] streamRunnables, final Kafka\n             TestUtils.waitForCondition(new TestCondition() {\n                 @Override\n                 public boolean conditionMet() {\n-                    final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n-                    if (metadata == null) {\n-                        return false;\n-                    }\n-                    final int index = metadata.hostInfo().port();\n-                    final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n-                    final ReadOnlyKeyValueStore<String, Long> store;\n                     try {\n-                        store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>keyValueStore());\n-                    } catch (final InvalidStateStoreException e) {\n-                        // rebalance\n-                        return false;\n+                        final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n+                        if (metadata == null) {\n+                            return false;\n+                        }\n+                        final int index = metadata.hostInfo().port();\n+                        final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n+                        final ReadOnlyKeyValueStore<String, Long> store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>keyValueStore());\n+                        return store != null && store.get(key) != null;\n                     } catch (final IllegalStateException e) {\n                         // Kafka Streams instance may have closed but rebalance hasn't happened\n                         return false;\n-                    } \n+                    } catch (final InvalidStateStoreException e) {\n+                        // rebalance\n+                        return false;\n+                    }\n \n-                    return store != null && store.get(key) != null;\n                 }\n             }, 30000, \"waiting for metadata, store and value to be non null\");\n         }\n@@ -296,23 +293,23 @@ private void verifyAllWindowedKeys(final StreamRunnable[] streamRunnables, final\n             TestUtils.waitForCondition(new TestCondition() {\n                 @Override\n                 public boolean conditionMet() {\n-                    final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n-                    if (metadata == null) {\n-                        return false;\n-                    }\n-                    final int index = metadata.hostInfo().port();\n-                    final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n-                    final ReadOnlyWindowStore<String, Long> store;\n                     try {\n-                        store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>windowStore());\n+                        final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n+                        if (metadata == null) {\n+                            return false;\n+                        }\n+                        final int index = metadata.hostInfo().port();\n+                        final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n+                        final ReadOnlyWindowStore<String, Long> store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>windowStore());\n+                        return store != null && store.fetch(key, from, to) != null;\n                     } catch (final IllegalStateException e) {\n                         // Kafka Streams instance may have closed but rebalance hasn't happened\n                         return false;\n                     } catch (InvalidStateStoreException e) {\n                         // rebalance\n                         return false;\n                     }\n-                    return store != null && store.fetch(key, from, to) != null;\n+\n                 }\n             }, 30000, \"waiting for metadata, store and value to be non null\");\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java",
                "sha": "e6d7be8129ec3b5f8f08ff4fd576c094c2ca817a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "patch": "@@ -209,6 +209,13 @@ public Integer partition(final String key, final Object value, final int numPart\n         assertEquals(expected, actual);\n     }\n \n+    @Test\n+    public void shouldReturnNotAvailableWhenClusterIsEmpty() throws Exception {\n+        discovery.onChange(Collections.<HostInfo, Set<TopicPartition>>emptyMap(), Cluster.empty());\n+        final StreamsMetadata result = discovery.getMetadataWithKey(\"table-one\", \"a\", Serdes.String().serializer());\n+        assertEquals(StreamsMetadata.NOT_AVAILABLE, result);\n+    }\n+\n     @Test\n     public void shouldGetInstanceWithKeyWithMergedStreams() throws Exception {\n         final TopicPartition topic2P2 = new TopicPartition(\"topic-two\", 2);",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java",
                "sha": "03280a831a58ea98eacceb8c70ff1672b4338005",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 10,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java",
                "patch": "@@ -18,6 +18,7 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.KeyValueStore;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -42,8 +43,8 @@\n     @SuppressWarnings(\"unchecked\")\n     @Before\n     public void before() {\n-        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub();\n-        stubProviderTwo = new StateStoreProviderStub();\n+        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub(false);\n+        stubProviderTwo = new StateStoreProviderStub(false);\n \n         stubOneUnderlying = newStoreInstance();\n         stubProviderOne.addStore(storeName, stubOneUnderlying);\n@@ -148,19 +149,19 @@ public void shouldSupportAllAcrossMultipleStores() throws Exception {\n     }\n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnGet() throws Exception {\n-        noStores().get(\"anything\");\n+    public void shouldThrowInvalidStoreExceptionDuringRebalance() throws Exception {\n+        rebalancing().get(\"anything\");\n     }\n \n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnRange() throws Exception {\n-        noStores().range(\"anything\", \"something\");\n+    public void shouldThrowInvalidStoreExceptionOnRangeDuringRebalance() throws Exception {\n+        rebalancing().range(\"anything\", \"something\");\n     }\n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnAll() throws Exception {\n-        noStores().all();\n+    public void shouldThrowInvalidStoreExceptionOnAllDuringRebalance() throws Exception {\n+        rebalancing().all();\n     }\n \n     @Test\n@@ -192,8 +193,8 @@ public long approximateNumEntries() {\n         assertEquals(Long.MAX_VALUE, theStore.approximateNumEntries());\n     }\n \n-    private CompositeReadOnlyKeyValueStore<Object, Object> noStores() {\n-        return new CompositeReadOnlyKeyValueStore<>(new WrappingStoreProvider(Collections.<StateStoreProvider>emptyList()),\n+    private CompositeReadOnlyKeyValueStore<Object, Object> rebalancing() {\n+        return new CompositeReadOnlyKeyValueStore<>(new WrappingStoreProvider(Collections.<StateStoreProvider>singletonList(new StateStoreProviderStub(true))),\n                 QueryableStoreTypes.keyValueStore(), storeName);\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java",
                "sha": "05c32f02d7d8c840567f8f3eb8a5cf515e076365",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java",
                "patch": "@@ -15,8 +15,10 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.WindowStoreIterator;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -42,8 +44,8 @@\n \n     @Before\n     public void before() {\n-        stubProviderOne = new StateStoreProviderStub();\n-        stubProviderTwo = new StateStoreProviderStub();\n+        stubProviderOne = new StateStoreProviderStub(false);\n+        stubProviderTwo = new StateStoreProviderStub(false);\n         underlyingWindowStore = new ReadOnlyWindowStoreStub<>();\n         stubProviderOne.addStore(storeName, underlyingWindowStore);\n \n@@ -103,6 +105,19 @@ public void shouldNotGetValuesFromOtherStores() throws Exception {\n         assertEquals(Collections.singletonList(new KeyValue<>(1L, \"my-value\")), results);\n     }\n \n+\n+    @Test(expected = InvalidStateStoreException.class)\n+    public void shouldThrowInvalidStateStoreExceptionOnRebalance() throws Exception {\n+        final CompositeReadOnlyWindowStore<Object, Object> store = new CompositeReadOnlyWindowStore<>(new StateStoreProviderStub(true), QueryableStoreTypes.windowStore(), \"foo\");\n+        store.fetch(\"key\", 1, 10);\n+    }\n+\n+    @Test(expected = InvalidStateStoreException.class)\n+    public void shouldThrowInvalidStateStoreExceptionIfFetchThrows() throws Exception {\n+        underlyingWindowStore.setOpen(false);\n+        underlyingWindowStore.fetch(\"key\", 1, 10);\n+    }\n+\n     static <K, V> List<KeyValue<K, V>> toList(final Iterator<KeyValue<K, V>> iterator) {\n         final List<KeyValue<K, V>> results = new ArrayList<>();\n ",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java",
                "sha": "d098429e28a833c1d2a4dbf48b3beb293cde94f1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 1,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java",
                "patch": "@@ -18,6 +18,7 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.NoOpWindowStore;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -33,7 +34,7 @@\n \n     @Before\n     public void before() {\n-        final StateStoreProviderStub theStoreProvider = new StateStoreProviderStub();\n+        final StateStoreProviderStub theStoreProvider = new StateStoreProviderStub(false);\n         theStoreProvider.addStore(keyValueStore, new StateStoreTestUtils.NoOpReadOnlyStore<>());\n         theStoreProvider.addStore(windowStore, new NoOpWindowStore());\n         storeProvider =",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java",
                "sha": "3660e8eb2a0b6af8ca107546aac13803313fd979",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 1,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java",
                "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.ProcessorContext;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n@@ -32,9 +33,13 @@\n public class ReadOnlyWindowStoreStub<K, V> implements ReadOnlyWindowStore<K, V>, StateStore {\n \n     private final Map<Long, Map<K, V>> data = new HashMap<>();\n+    private boolean open  = true;\n \n     @Override\n     public WindowStoreIterator<V> fetch(final K key, final long timeFrom, final long timeTo) {\n+        if (!open) {\n+            throw new InvalidStateStoreException(\"Store is not open\");\n+        }\n         final List<KeyValue<Long, V>> results = new ArrayList<>();\n         for (long now = timeFrom; now <= timeTo; now++) {\n             final Map<K, V> kvMap = data.get(now);\n@@ -79,7 +84,11 @@ public boolean persistent() {\n \n     @Override\n     public boolean isOpen() {\n-        return false;\n+        return open;\n+    }\n+\n+    public void setOpen(final boolean open) {\n+        this.open = open;\n     }\n \n     private class TheWindowStoreIterator<E> implements WindowStoreIterator<E> {",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java",
                "sha": "2082e00cba0bc9ce63e066562b04cc32a2b5c780",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 2,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -37,8 +38,8 @@\n \n     @Before\n     public void before() {\n-        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub();\n-        final StateStoreProviderStub stubProviderTwo = new StateStoreProviderStub();\n+        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub(false);\n+        final StateStoreProviderStub stubProviderTwo = new StateStoreProviderStub(false);\n \n \n         stubProviderOne.addStore(\"kv\", StateStoreTestUtils.newKeyValueStore(\"kv\", String.class, String.class));",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java",
                "sha": "708e1534dcc1e22e9ddf5a8f206ac395eafdf05d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a",
                "deletions": 1,
                "filename": "streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java",
                "patch": "@@ -12,10 +12,12 @@\n  * or implied. See the License for the specific language governing permissions and limitations under\n  * the License.\n  */\n-package org.apache.kafka.streams.state.internals;\n+package org.apache.kafka.test;\n \n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.state.QueryableStoreType;\n+import org.apache.kafka.streams.state.internals.StateStoreProvider;\n \n import java.util.Collections;\n import java.util.HashMap;\n@@ -25,10 +27,19 @@\n public class StateStoreProviderStub implements StateStoreProvider {\n \n     private final Map<String, StateStore> stores = new HashMap<>();\n+    private final boolean throwException;\n+\n+    public StateStoreProviderStub(final boolean throwException) {\n+\n+        this.throwException = throwException;\n+    }\n \n     @SuppressWarnings(\"unchecked\")\n     @Override\n     public <T> List<T> stores(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n+        if (throwException) {\n+            throw new InvalidStateStoreException(\"store is unavailable\");\n+        }\n         if (stores.containsKey(storeName) && queryableStoreType.accepts(stores.get(storeName))) {\n             return (List<T>) Collections.singletonList(stores.get(storeName));\n         }",
                "previous_filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/StateStoreProviderStub.java",
                "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java",
                "sha": "f17777fab3e5b359b10d28c6687063dcf86ea49d",
                "status": "renamed"
            }
        ],
        "message": "KAFKA-4163: NPE in StreamsMetadataState during re-balance operations\n\nDuring rebalance operations the Cluster object gets set to Cluster.empty(). This can result in NPEs when doing certain operation on StreamsMetadataState. This should throw a StreamsException if the Cluster is empty as it is not yet (re-)initialized\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Eno Thereska, Guozhang Wang\n\nCloses #1845 from dguy/streams-meta-hotfix",
        "parent": "https://github.com/apache/kafka/commit/70afd5f9dd2eddc784a24fa2518992ef3371f0a4",
        "repo": "kafka",
        "unit_tests": [
            "KafkaStreamsTest.java",
            "StreamsMetadataStateTest.java",
            "CompositeReadOnlyKeyValueStoreTest.java",
            "CompositeReadOnlyWindowStoreTest.java",
            "QueryableStoreProviderTest.java",
            "StreamThreadStateStoreProviderTest.java",
            "WrappingStoreProviderTest.java"
        ]
    },
    "kafka_3e48bdb": {
        "bug_id": "kafka_3e48bdb",
        "commit": "https://github.com/apache/kafka/commit/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece",
        "file": [
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/kafka/blob/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java?ref=3e48bdbc333602a042b6b0fb7fb9e14625ab4ece",
                "deletions": 7,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "patch": "@@ -190,14 +190,20 @@ State setState(final State newState) {\n             oldState = state;\n \n             if (state == State.PENDING_SHUTDOWN && newState != State.DEAD) {\n+                log.debug(\"Ignoring request to transit from PENDING_SHUTDOWN to {}: \" +\n+                              \"only DEAD state is a valid next state\", newState);\n                 // when the state is already in PENDING_SHUTDOWN, all other transitions will be\n                 // refused but we do not throw exception here\n                 return null;\n             } else if (state == State.DEAD) {\n+                log.debug(\"Ignoring request to transit from DEAD to {}: \" +\n+                              \"no valid next state after DEAD\", newState);\n                 // when the state is already in NOT_RUNNING, all its transitions\n                 // will be refused but we do not throw exception here\n                 return null;\n             } else if (state == State.PARTITIONS_REVOKED && newState == State.PARTITIONS_REVOKED) {\n+                log.debug(\"Ignoring request to transit from PARTITIONS_REVOKED to PARTITIONS_REVOKED: \" +\n+                              \"self transition is not allowed\");\n                 // when the state is already in PARTITIONS_REVOKED, its transition to itself will be\n                 // refused but we do not throw exception here\n                 return null;\n@@ -268,17 +274,23 @@ public void onPartitionsAssigned(final Collection<TopicPartition> assignment) {\n             final long start = time.milliseconds();\n             try {\n                 if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) {\n-                    return;\n-                }\n-                if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.NONE.code()) {\n+                    log.debug(\n+                        \"Skipping task creation in rebalance because we are already in {} state.\",\n+                        streamThread.state()\n+                    );\n+                } else if (streamThread.assignmentErrorCode.get() != StreamsPartitionAssignor.Error.NONE.code()) {\n+                    log.debug(\n+                        \"Encountered assignment error during partition assignment: {}. Skipping task initialization\",\n+                        streamThread.assignmentErrorCode\n+                    );\n+                } else {\n+                    log.debug(\"Creating tasks based on assignment.\");\n                     taskManager.createTasks(assignment);\n                 }\n             } catch (final Throwable t) {\n                 log.error(\n                     \"Error caught during partition assignment, \" +\n-                        \"will abort the current process and re-throw at the end of rebalance: {}\",\n-                    t\n-                );\n+                        \"will abort the current process and re-throw at the end of rebalance\", t);\n                 streamThread.setRebalanceException(t);\n             } finally {\n                 log.info(\"partition assignment took {} ms.\\n\" +\n@@ -809,7 +821,6 @@ private void enforceRebalance() {\n     // Visible for testing\n     void runOnce() {\n         final ConsumerRecords<byte[], byte[]> records;\n-\n         now = time.milliseconds();\n \n         if (state == State.PARTITIONS_ASSIGNED) {\n@@ -830,6 +841,15 @@ void runOnce() {\n             throw new StreamsException(logPrefix + \"Unexpected state \" + state + \" during normal iteration\");\n         }\n \n+        // Shutdown hook could potentially be triggered and transit the thread state to PENDING_SHUTDOWN during #pollRequests().\n+        // The task manager internal states could be uninitialized if the state transition happens during #onPartitionsAssigned().\n+        // Should only proceed when the thread is still running after #pollRequests(), because no external state mutation\n+        // could affect the task manager state beyond this point within #runOnce().\n+        if (!isRunning()) {\n+            log.debug(\"State already transits to {}, skipping the run once call after poll request\", state);\n+            return;\n+        }\n+\n         final long pollLatency = advanceNowAndComputeLatency();\n \n         if (records != null && !records.isEmpty()) {",
                "raw_url": "https://github.com/apache/kafka/raw/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "sha": "ea1e29d0bcba6bf84e5717ef769a64e621516b8b",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/kafka/blob/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java?ref=3e48bdbc333602a042b6b0fb7fb9e14625ab4ece",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.InvalidOffsetException;\n import org.apache.kafka.clients.consumer.MockConsumer;\n import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n@@ -78,6 +79,7 @@\n import java.io.File;\n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -701,6 +703,85 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {\n         EasyMock.verify(taskManager);\n     }\n \n+    @Test\n+    public void shouldNotThrowWhenPendingShutdownInRunOnce() {\n+        mockRunOnce(true);\n+    }\n+\n+    @Test\n+    public void shouldNotThrowWithoutPendingShutdownInRunOnce() {\n+        // A reference test to verify that without intermediate shutdown the runOnce should pass\n+        // without any exception.\n+        mockRunOnce(false);\n+    }\n+\n+    private void mockRunOnce(final boolean shutdownOnPoll) {\n+        final Collection<TopicPartition> assignedPartitions = Collections.singletonList(t1p1);\n+        class MockStreamThreadConsumer<K, V> extends MockConsumer<K, V> {\n+\n+            private StreamThread streamThread;\n+\n+            private MockStreamThreadConsumer(final OffsetResetStrategy offsetResetStrategy) {\n+                super(offsetResetStrategy);\n+            }\n+\n+            @Override\n+            public synchronized ConsumerRecords<K, V> poll(final Duration timeout) {\n+                assertNotNull(streamThread);\n+                if (shutdownOnPoll) {\n+                    streamThread.shutdown();\n+                }\n+                streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);\n+                return super.poll(timeout);\n+            }\n+\n+            private void setStreamThread(final StreamThread streamThread) {\n+                this.streamThread = streamThread;\n+            }\n+        }\n+\n+        final MockStreamThreadConsumer<byte[], byte[]> mockStreamThreadConsumer =\n+            new MockStreamThreadConsumer<>(OffsetResetStrategy.EARLIEST);\n+\n+        final TaskManager taskManager = new TaskManager(new MockChangelogReader(),\n+                                                        processId,\n+                                                        \"log-prefix\",\n+                                                        mockStreamThreadConsumer,\n+                                                        streamsMetadataState,\n+                                                        null,\n+                                                        null,\n+                                                        null,\n+                                                        new AssignedStreamsTasks(new LogContext()),\n+                                                        new AssignedStandbyTasks(new LogContext()));\n+        taskManager.setConsumer(mockStreamThreadConsumer);\n+        taskManager.setAssignmentMetadata(Collections.emptyMap(), Collections.emptyMap());\n+\n+        final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId);\n+        final StreamThread thread = new StreamThread(\n+            mockTime,\n+            config,\n+            null,\n+            mockStreamThreadConsumer,\n+            mockStreamThreadConsumer,\n+            null,\n+            taskManager,\n+            streamsMetrics,\n+            internalTopologyBuilder,\n+            clientId,\n+            new LogContext(\"\"),\n+            new AtomicInteger()\n+        ).updateThreadMetadata(getSharedAdminClientId(clientId));\n+\n+        mockStreamThreadConsumer.setStreamThread(thread);\n+        mockStreamThreadConsumer.assign(assignedPartitions);\n+        mockStreamThreadConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n+\n+        addRecord(mockStreamThreadConsumer, 1L, 0L);\n+        thread.setState(StreamThread.State.STARTING);\n+        thread.setState(StreamThread.State.PARTITIONS_REVOKED);\n+        thread.runOnce();\n+    }\n+\n     @Test\n     public void shouldOnlyShutdownOnce() {\n         final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);",
                "raw_url": "https://github.com/apache/kafka/raw/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java",
                "sha": "aff5d6c74861c16a04eec1c7c190bc4cb4f71c1f",
                "status": "modified"
            }
        ],
        "message": "KAFKA-8620: fix NPE due to race condition during shutdown while rebalancing (#7021)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <bruno@confluent.io>, John Roesler <john@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/528e5c0f57aa5014cd13baef2b683a1a328f459a",
        "repo": "kafka",
        "unit_tests": [
            "StreamThreadTest.java"
        ]
    },
    "kafka_40432e3": {
        "bug_id": "kafka_40432e3",
        "commit": "https://github.com/apache/kafka/commit/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java?ref=40432e31f7c5b0e0992a51621dc8b1c0e3d3103f",
                "deletions": 3,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java",
                "patch": "@@ -112,7 +112,7 @@ void createTasks(final Collection<TopicPartition> assignment) {\n     }\n \n     private void addStreamTasks(final Collection<TopicPartition> assignment) {\n-        if (assignedActiveTasks.isEmpty()) {\n+        if (assignedActiveTasks == null || assignedActiveTasks.isEmpty()) {\n             return;\n         }\n         final Map<TaskId, Set<TopicPartition>> newTasks = new HashMap<>();\n@@ -151,8 +151,7 @@ private void addStreamTasks(final Collection<TopicPartition> assignment) {\n     }\n \n     private void addStandbyTasks() {\n-        final Map<TaskId, Set<TopicPartition>> assignedStandbyTasks = this.assignedStandbyTasks;\n-        if (assignedStandbyTasks.isEmpty()) {\n+        if (assignedStandbyTasks == null || assignedStandbyTasks.isEmpty()) {\n             return;\n         }\n         log.debug(\"Adding assigned standby tasks {}\", assignedStandbyTasks);",
                "raw_url": "https://github.com/apache/kafka/raw/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java",
                "sha": "a7fa1fa96706c2486777dc1f8f0463b80d643b8b",
                "status": "modified"
            }
        ],
        "message": "MONIR: Check for NULL in case of version probing (#7275)\n\nIn case of version probing we would skip the logic for setting cluster / assigned tasks; since these values are initialized as null they are vulnerable to NPE when code changes.\r\n\r\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bill@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/d18d6b033e09515adff19225f8ec6845ca34c23b",
        "repo": "kafka",
        "unit_tests": [
            "TaskManagerTest.java"
        ]
    },
    "kafka_4c76b5f": {
        "bug_id": "kafka_4c76b5f",
        "commit": "https://github.com/apache/kafka/commit/4c76b5fa6a72412efa5936c284800148c2c69c24",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java?ref=4c76b5fa6a72412efa5936c284800148c2c69c24",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "patch": "@@ -298,7 +298,7 @@ public void to(Serde<K> keySerde, Serde<V> valSerde, StreamPartitioner<K, V> par\n         String name = topology.newName(SINK_NAME);\n \n         Serializer<K> keySerializer = keySerde == null ? null : keySerde.serializer();\n-        Serializer<V> valSerializer = keySerde == null ? null : valSerde.serializer();\n+        Serializer<V> valSerializer = valSerde == null ? null : valSerde.serializer();\n         \n         if (partitioner == null && keySerializer != null && keySerializer instanceof WindowedSerializer) {\n             WindowedSerializer<Object> windowedSerializer = (WindowedSerializer<Object>) keySerializer;",
                "raw_url": "https://github.com/apache/kafka/raw/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "sha": "91bcef94eb35d0175ae5a82f07cf2526d90b0a09",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java?ref=4c76b5fa6a72412efa5936c284800148c2c69c24",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "patch": "@@ -133,4 +133,11 @@ public Integer apply(Integer value1, Integer value2) {\n             1, // process\n             builder.build(\"X\", null).processors().size());\n     }\n+\n+    @Test\n+    public void testToWithNullValueSerdeDoesntNPE() {\n+        final KStreamBuilder builder = new KStreamBuilder();\n+        final KStream<String, String> inputStream = builder.stream(stringSerde, stringSerde, \"input\");\n+        inputStream.to(stringSerde, null, \"output\");\n+    }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "sha": "3d45d1dcc8a290892b7bbb59b9e716243c6bc7c2",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3629; KStreamImpl.to(...) throws NPE when the value SerDe is null\n\nguozhangwang\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Matthias J. Sax, Guozhang Wang\n\nCloses #1272 from dguy/kstreamimpl-to-npe and squashes the following commits:\n\n49d48fb [Damian Guy] actually commit the fix\n07ce589 [Damian Guy] fix npe in KStreamImpl.to(..)\n74d396d [Damian Guy] fix npe in KStreamImpl.to(..)",
        "parent": "https://github.com/apache/kafka/commit/2885bc33daaf75477bf39a92d1d1da02c0e03eaa",
        "repo": "kafka",
        "unit_tests": [
            "KStreamImplTest.java"
        ]
    },
    "kafka_5b6de9f": {
        "bug_id": "kafka_5b6de9f",
        "commit": "https://github.com/apache/kafka/commit/5b6de9f2d022cf25df73067b9de77ec267b4af8c",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 16,
                "filename": "clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java",
                "patch": "@@ -18,13 +18,13 @@\n \n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.RequestHeader;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * A simple implementation of `MetadataUpdater` that returns the cluster nodes set via the constructor or via\n@@ -36,9 +36,6 @@\n  * This class is not thread-safe!\n  */\n public class ManualMetadataUpdater implements MetadataUpdater {\n-\n-    private static final Logger log = LoggerFactory.getLogger(ManualMetadataUpdater.class);\n-\n     private List<Node> nodes;\n \n     public ManualMetadataUpdater() {\n@@ -69,24 +66,18 @@ public long maybeUpdate(long now) {\n     }\n \n     @Override\n-    public void handleDisconnection(String destination) {\n-        // Do nothing\n-    }\n-\n-    @Override\n-    public void handleFatalException(KafkaException exception) {\n-        // We don't fail the broker on failures, but there should be sufficient information in the logs indicating the reason\n-        // for failure.\n-        log.debug(\"An error occurred in broker-to-broker communication.\", exception);\n+    public void handleServerDisconnect(long now, String nodeId, Optional<AuthenticationException> maybeAuthException) {\n+        // We don't fail the broker on failures. There should be sufficient information from\n+        // the NetworkClient logs to indicate the reason for the failure.\n     }\n \n     @Override\n-    public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n+    public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n         // Do nothing\n     }\n \n     @Override\n-    public void requestUpdate() {\n+    public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n         // Do nothing\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java",
                "sha": "c1c1fba4a56d058ee47a84301bfcb09cf8dda1f5",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/Metadata.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 5,
                "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "patch": "@@ -209,10 +209,8 @@ public synchronized boolean updateRequested() {\n         }\n     }\n \n-    public synchronized void bootstrap(List<InetSocketAddress> addresses, long now) {\n+    public synchronized void bootstrap(List<InetSocketAddress> addresses) {\n         this.needUpdate = true;\n-        this.lastRefreshMs = now;\n-        this.lastSuccessfulRefreshMs = now;\n         this.updateVersion += 1;\n         this.cache = MetadataCache.bootstrap(addresses);\n     }\n@@ -419,9 +417,18 @@ private void clearRecoverableErrors() {\n      * Record an attempt to update the metadata that failed. We need to keep track of this\n      * to avoid retrying immediately.\n      */\n-    public synchronized void failedUpdate(long now, KafkaException fatalException) {\n+    public synchronized void failedUpdate(long now) {\n         this.lastRefreshMs = now;\n-        this.fatalException = fatalException;\n+    }\n+\n+    /**\n+     * Propagate a fatal error which affects the ability to fetch metadata for the cluster.\n+     * Two examples are authentication and unsupported version exceptions.\n+     *\n+     * @param exception The fatal exception\n+     */\n+    public synchronized void fatalError(KafkaException exception) {\n+        this.fatalException = exception;\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "sha": "a9e68b861fedfcad76d627f4c8cf425318beb000",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 14,
                "filename": "clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java",
                "patch": "@@ -18,11 +18,14 @@\n \n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.errors.AuthenticationException;\n+import org.apache.kafka.common.errors.UnsupportedVersionException;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.RequestHeader;\n \n import java.io.Closeable;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * The interface used by `NetworkClient` to request cluster metadata info to be updated and to retrieve the cluster nodes\n@@ -46,7 +49,7 @@\n      * Starts a cluster metadata update if needed and possible. Returns the time until the metadata update (which would\n      * be 0 if an update has been started as a result of this call).\n      *\n-     * If the implementation relies on `NetworkClient` to send requests, `handleCompletedMetadataResponse` will be\n+     * If the implementation relies on `NetworkClient` to send requests, `handleSuccessfulResponse` will be\n      * invoked after the metadata response is received.\n      *\n      * The semantics of `needed` and `possible` are implementation-dependent and may take into account a number of\n@@ -55,34 +58,32 @@\n     long maybeUpdate(long now);\n \n     /**\n-     * Handle disconnections for metadata requests.\n+     * Handle a server disconnect.\n      *\n      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own\n      * requests with special handling for disconnections of such requests.\n-     * @param destination\n+     *\n+     * @param now Current time in milliseconds\n+     * @param nodeId The id of the node that disconnected\n+     * @param maybeAuthException Optional authentication error\n      */\n-    void handleDisconnection(String destination);\n+    void handleServerDisconnect(long now, String nodeId, Optional<AuthenticationException> maybeAuthException);\n \n     /**\n-     * Handle failure. Propagate the exception if awaiting metadata.\n+     * Handle a metadata request failure.\n      *\n-     * @param fatalException exception corresponding to the failure\n+     * @param now Current time in milliseconds\n+     * @param maybeFatalException Optional fatal error (e.g. {@link UnsupportedVersionException})\n      */\n-    void handleFatalException(KafkaException fatalException);\n+    void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException);\n \n     /**\n      * Handle responses for metadata requests.\n      *\n      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own\n      * requests with special handling for completed receives of such requests.\n      */\n-    void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse);\n-\n-    /**\n-     * Schedules an update of the current cluster metadata info. A subsequent call to `maybeUpdate` would trigger the\n-     * start of the update if possible (see `maybeUpdate` for more information).\n-     */\n-    void requestUpdate();\n+    void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse);\n \n     /**\n      * Close this updater.",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java",
                "sha": "77f3efadce3a0f50270f7c41536b46cee6da1d18",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 58,
                "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "patch": "@@ -30,8 +30,8 @@\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.network.Send;\n import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.protocol.CommonFields;\n+import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.protocol.types.Struct;\n import org.apache.kafka.common.requests.AbstractRequest;\n import org.apache.kafka.common.requests.AbstractResponse;\n@@ -51,11 +51,13 @@\n import java.net.InetSocketAddress;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.HashMap;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.Random;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.stream.Collectors;\n@@ -308,24 +310,29 @@ public void disconnect(String nodeId) {\n             return;\n \n         selector.close(nodeId);\n-        List<ApiKeys> requestTypes = new ArrayList<>();\n         long now = time.milliseconds();\n-        for (InFlightRequest request : inFlightRequests.clearAll(nodeId)) {\n-            if (request.isInternalRequest) {\n-                if (request.header.apiKey() == ApiKeys.METADATA) {\n-                    metadataUpdater.handleDisconnection(request.destination);\n-                }\n-            } else {\n-                requestTypes.add(request.header.apiKey());\n-                abortedSends.add(new ClientResponse(request.header,\n-                        request.callback, request.destination, request.createdTimeMs, now,\n-                        true, null, null, null));\n-            }\n-        }\n+\n+        cancelInFlightRequests(nodeId, now, abortedSends);\n+\n         connectionStates.disconnected(nodeId, now);\n-        if (log.isDebugEnabled()) {\n-            log.debug(\"Manually disconnected from {}. Removed requests: {}.\", nodeId,\n-                Utils.join(requestTypes, \", \"));\n+\n+        if (log.isTraceEnabled()) {\n+            log.trace(\"Manually disconnected from {}. Aborted in-flight requests: {}.\", nodeId, inFlightRequests);\n+        }\n+    }\n+\n+    private void cancelInFlightRequests(String nodeId, long now, Collection<ClientResponse> responses) {\n+        Iterable<InFlightRequest> inFlightRequests = this.inFlightRequests.clearAll(nodeId);\n+        for (InFlightRequest request : inFlightRequests) {\n+            log.trace(\"Cancelled request {} {} with correlation id {} due to node {} being disconnected\",\n+                    request.header.apiKey(), request.request, request.header.correlationId(), nodeId);\n+\n+            if (!request.isInternalRequest) {\n+                if (responses != null)\n+                    responses.add(request.disconnected(now, null));\n+            } else if (request.header.apiKey() == ApiKeys.METADATA) {\n+                metadataUpdater.handleFailedRequest(now, Optional.empty());\n+            }\n         }\n     }\n \n@@ -339,9 +346,8 @@ public void disconnect(String nodeId) {\n     @Override\n     public void close(String nodeId) {\n         selector.close(nodeId);\n-        for (InFlightRequest request : inFlightRequests.clearAll(nodeId))\n-            if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleDisconnection(request.destination);\n+        long now = time.milliseconds();\n+        cancelInFlightRequests(nodeId, now, null);\n         connectionStates.remove(nodeId);\n     }\n \n@@ -481,10 +487,11 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n             ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()),\n                     clientRequest.callback(), clientRequest.destination(), now, now,\n                     false, unsupportedVersionException, null, null);\n-            abortedSends.add(clientResponse);\n \n-            if (isInternalRequest && clientRequest.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleFatalException(unsupportedVersionException);\n+            if (!isInternalRequest)\n+                abortedSends.add(clientResponse);\n+            else if (clientRequest.apiKey() == ApiKeys.METADATA)\n+                metadataUpdater.handleFailedRequest(now, Optional.of(unsupportedVersionException));\n         }\n     }\n \n@@ -735,7 +742,6 @@ private void processDisconnection(List<ClientResponse> responses,\n             case AUTHENTICATION_FAILED:\n                 AuthenticationException exception = disconnectState.exception();\n                 connectionStates.authenticationFailed(nodeId, now, exception);\n-                metadataUpdater.handleFatalException(exception);\n                 log.error(\"Connection to node {} ({}) failed authentication due to: {}\", nodeId,\n                     disconnectState.remoteAddress(), exception.getMessage());\n                 break;\n@@ -752,14 +758,9 @@ private void processDisconnection(List<ClientResponse> responses,\n             default:\n                 break; // Disconnections in other states are logged at debug level in Selector\n         }\n-        for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n-            log.trace(\"Cancelled request {} {} with correlation id {} due to node {} being disconnected\",\n-                    request.header.apiKey(), request.request, request.header.correlationId(), nodeId);\n-            if (!request.isInternalRequest)\n-                responses.add(request.disconnected(now, disconnectState.exception()));\n-            else if (request.header.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleDisconnection(request.destination);\n-        }\n+\n+        cancelInFlightRequests(nodeId, now, responses);\n+        metadataUpdater.handleServerDisconnect(now, nodeId, Optional.ofNullable(disconnectState.exception()));\n     }\n \n     /**\n@@ -777,10 +778,6 @@ private void handleTimedOutRequests(List<ClientResponse> responses, long now) {\n             log.debug(\"Disconnecting from node {} due to request timeout.\", nodeId);\n             processDisconnection(responses, nodeId, now, ChannelState.LOCAL_CLOSE);\n         }\n-\n-        // we disconnected, so we should probably refresh our metadata\n-        if (!nodeIds.isEmpty())\n-            metadataUpdater.requestUpdate();\n     }\n \n     private void handleAbortedSends(List<ClientResponse> responses) {\n@@ -844,7 +841,7 @@ private void handleCompletedReceives(List<ClientResponse> responses, long now) {\n                     parseResponse(req.header.apiKey(), responseStruct, req.header.apiVersion());\n             maybeThrottle(body, req.header.apiVersion(), req.destination, now);\n             if (req.isInternalRequest && body instanceof MetadataResponse)\n-                metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);\n+                metadataUpdater.handleSuccessfulResponse(req.header, now, (MetadataResponse) body);\n             else if (req.isInternalRequest && body instanceof ApiVersionsResponse)\n                 handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body);\n             else\n@@ -894,9 +891,6 @@ private void handleDisconnections(List<ClientResponse> responses, long now) {\n             log.debug(\"Node {} disconnected.\", node);\n             processDisconnection(responses, node, now, entry.getValue());\n         }\n-        // we got a disconnect so we should probably refresh our metadata and see if that broker is dead\n-        if (this.selector.disconnected().size() > 0)\n-            metadataUpdater.requestUpdate();\n     }\n \n     /**\n@@ -960,10 +954,10 @@ private void initiateConnect(Node node, long now) {\n                     this.socketReceiveBuffer);\n         } catch (IOException e) {\n             log.warn(\"Error connecting to node {}\", node, e);\n-            /* attempt failed, we'll try again after the backoff */\n+            // Attempt failed, we'll try again after the backoff\n             connectionStates.disconnected(nodeConnectionId, now);\n-            /* maybe the problem is our metadata, update it */\n-            metadataUpdater.requestUpdate();\n+            // Notify metadata updater of the connection failure\n+            metadataUpdater.handleServerDisconnect(now, nodeConnectionId, Optional.empty());\n         }\n     }\n \n@@ -1001,7 +995,6 @@ public long maybeUpdate(long now) {\n             long waitForMetadataFetch = hasFetchInProgress() ? defaultRequestTimeoutMs : 0;\n \n             long metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);\n-\n             if (metadataTimeout > 0) {\n                 return metadataTimeout;\n             }\n@@ -1018,31 +1011,39 @@ public long maybeUpdate(long now) {\n         }\n \n         @Override\n-        public void handleDisconnection(String destination) {\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeFatalException) {\n             Cluster cluster = metadata.fetch();\n             // 'processDisconnection' generates warnings for misconfigured bootstrap server configuration\n             // resulting in 'Connection Refused' and misconfigured security resulting in authentication failures.\n             // The warning below handles the case where a connection to a broker was established, but was disconnected\n             // before metadata could be obtained.\n             if (cluster.isBootstrapConfigured()) {\n-                int nodeId = Integer.parseInt(destination);\n+                int nodeId = Integer.parseInt(destinationId);\n                 Node node = cluster.nodeById(nodeId);\n                 if (node != null)\n                     log.warn(\"Bootstrap broker {} disconnected\", node);\n             }\n \n-            inProgressRequestVersion = null;\n+            // If we have a disconnect while an update is due, we treat it as a failed update\n+            // so that we can backoff properly\n+            if (isUpdateDue(now))\n+                handleFailedRequest(now, Optional.empty());\n+\n+            maybeFatalException.ifPresent(metadata::fatalError);\n+\n+            // The disconnect may be the result of stale metadata, so request an update\n+            metadata.requestUpdate();\n         }\n \n         @Override\n-        public void handleFatalException(KafkaException fatalException) {\n-            if (metadata.updateRequested())\n-                metadata.failedUpdate(time.milliseconds(), fatalException);\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n+            maybeFatalException.ifPresent(metadata::fatalError);\n+            metadata.failedUpdate(now);\n             inProgressRequestVersion = null;\n         }\n \n         @Override\n-        public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n+        public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n             // If any partition has leader with missing listeners, log up to ten of these partitions\n             // for diagnosing broker configuration issues.\n             // This could be a transient issue if listeners were added dynamically to brokers.\n@@ -1066,19 +1067,14 @@ public void handleCompletedMetadataResponse(RequestHeader requestHeader, long no\n             // created which means we will get errors and no nodes until it exists\n             if (response.brokers().isEmpty()) {\n                 log.trace(\"Ignoring empty metadata response with correlation id {}.\", requestHeader.correlationId());\n-                this.metadata.failedUpdate(now, null);\n+                this.metadata.failedUpdate(now);\n             } else {\n                 this.metadata.update(inProgressRequestVersion, response, now);\n             }\n \n             inProgressRequestVersion = null;\n         }\n \n-        @Override\n-        public void requestUpdate() {\n-            this.metadata.requestUpdate();\n-        }\n-\n         @Override\n         public void close() {\n             this.metadata.close();\n@@ -1104,10 +1100,10 @@ private long maybeUpdate(long now, Node node) {\n \n             if (canSendRequest(nodeConnectionId, now)) {\n                 Metadata.MetadataRequestAndVersion requestAndVersion = metadata.newMetadataRequestAndVersion();\n-                this.inProgressRequestVersion = requestAndVersion.requestVersion;\n                 MetadataRequest.Builder metadataRequest = requestAndVersion.requestBuilder;\n                 log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node);\n                 sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);\n+                this.inProgressRequestVersion = requestAndVersion.requestVersion;\n                 return defaultRequestTimeoutMs;\n             }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "sha": "3431b83c9dacd8d97031ddb3c0ef3221443e883d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 10,
                "filename": "clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java",
                "patch": "@@ -29,6 +29,7 @@\n \n import java.util.Collections;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * Manages the metadata for KafkaAdminClient.\n@@ -100,23 +101,19 @@ public long maybeUpdate(long now) {\n         }\n \n         @Override\n-        public void handleDisconnection(String destination) {\n-            // Do nothing\n-        }\n-\n-        @Override\n-        public void handleFatalException(KafkaException e) {\n-            updateFailed(e);\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeFatalException) {\n+            maybeFatalException.ifPresent(AdminMetadataManager.this::updateFailed);\n+            AdminMetadataManager.this.requestUpdate();\n         }\n \n         @Override\n-        public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse) {\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n             // Do nothing\n         }\n \n         @Override\n-        public void requestUpdate() {\n-            AdminMetadataManager.this.requestUpdate();\n+        public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse) {\n+            // Do nothing\n         }\n \n         @Override",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java",
                "sha": "6e834520c46f7b0dab82d8fe0a055f03dbd4db14",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "patch": "@@ -734,7 +734,7 @@ else if (enableAutoCommit)\n                     subscriptions, logContext, clusterResourceListeners);\n             List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(\n                     config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));\n-            this.metadata.bootstrap(addresses, time.milliseconds());\n+            this.metadata.bootstrap(addresses);\n             String metricGrpPrefix = \"consumer\";\n \n             FetcherMetricsRegistry metricsRegistry = new FetcherMetricsRegistry(Collections.singleton(CLIENT_ID_METRIC_TAG), metricGrpPrefix);",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "sha": "b5a1047f0efd3e3caaf1d673c4684a3bd56ebf81",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "patch": "@@ -415,7 +415,7 @@ public KafkaProducer(Properties properties, Serializer<K> keySerializer, Seriali\n                         logContext,\n                         clusterResourceListeners,\n                         Time.SYSTEM);\n-                this.metadata.bootstrap(addresses, time.milliseconds());\n+                this.metadata.bootstrap(addresses);\n             }\n             this.errors = this.metrics.sensor(\"errors\");\n             this.sender = newSender(logContext, kafkaClient, this.metadata);",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "sha": "dd3a94acbe40f0e13a901e0acc83600387deccce",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 4,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java",
                "patch": "@@ -107,10 +107,9 @@ public synchronized void update(int requestVersion, MetadataResponse response, l\n     }\n \n     @Override\n-    public synchronized void failedUpdate(long now, KafkaException fatalException) {\n-        super.failedUpdate(now, fatalException);\n-        if (fatalException != null)\n-            notifyAll();\n+    public synchronized void fatalError(KafkaException fatalException) {\n+        super.fatalError(fatalException);\n+        notifyAll();\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java",
                "sha": "fea086a3a93023049eaae855cf1d12631fe11b0f",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 4,
                "filename": "clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "patch": "@@ -105,6 +105,18 @@ private static void checkTimeToNextUpdate(long refreshBackoffMs, long metadataEx\n         assertEquals(0, metadata.timeToNextUpdate(now + 1));\n     }\n \n+    @Test\n+    public void testUpdateMetadataAllowedImmediatelyAfterBootstrap() {\n+        MockTime time = new MockTime();\n+\n+        Metadata metadata = new Metadata(refreshBackoffMs, metadataExpireMs, new LogContext(),\n+                new ClusterResourceListeners());\n+        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(\"localhost\", 9002)));\n+\n+        assertEquals(0, metadata.timeToAllowUpdate(time.milliseconds()));\n+        assertEquals(0, metadata.timeToNextUpdate(time.milliseconds()));\n+    }\n+\n     @Test\n     public void testTimeToNextUpdate() {\n         checkTimeToNextUpdate(100, 1000);\n@@ -119,7 +131,7 @@ public void testTimeToNextUpdate_RetryBackoff() {\n         long now = 10000;\n \n         // lastRefreshMs updated to now.\n-        metadata.failedUpdate(now, null);\n+        metadata.failedUpdate(now);\n \n         // Backing off. Remaining time until next try should be returned.\n         assertEquals(refreshBackoffMs, metadata.timeToNextUpdate(now));\n@@ -141,7 +153,7 @@ public void testFailedUpdate() {\n         metadata.update(emptyMetadataResponse(), time);\n \n         assertEquals(100, metadata.timeToNextUpdate(1000));\n-        metadata.failedUpdate(1100, null);\n+        metadata.failedUpdate(1100);\n \n         assertEquals(100, metadata.timeToNextUpdate(1100));\n         assertEquals(100, metadata.lastSuccessfulUpdate());\n@@ -152,14 +164,13 @@ public void testFailedUpdate() {\n \n     @Test\n     public void testClusterListenerGetsNotifiedOfUpdate() {\n-        long time = 0;\n         MockClusterResourceListener mockClusterListener = new MockClusterResourceListener();\n         ClusterResourceListeners listeners = new ClusterResourceListeners();\n         listeners.maybeAdd(mockClusterListener);\n         metadata = new Metadata(refreshBackoffMs, metadataExpireMs, new LogContext(), listeners);\n \n         String hostName = \"www.example.com\";\n-        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(hostName, 9002)), time);\n+        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(hostName, 9002)));\n         assertFalse(\"ClusterResourceListener should not called when metadata is updated with bootstrap Cluster\",\n                 MockClusterResourceListener.IS_ON_UPDATE_CALLED.get());\n ",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "sha": "4517f76664274f13c6600b343aad1969be308273",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 5,
                "filename": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java",
                "patch": "@@ -16,10 +16,13 @@\n  */\n package org.apache.kafka.clients;\n \n+import org.apache.kafka.common.Cluster;\n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.errors.UnsupportedVersionException;\n+import org.apache.kafka.common.internals.ClusterResourceListeners;\n import org.apache.kafka.common.message.ApiVersionsResponseData;\n import org.apache.kafka.common.message.ApiVersionsResponseData.ApiVersionsResponseKey;\n import org.apache.kafka.common.message.ApiVersionsResponseData.ApiVersionsResponseKeyCollection;\n@@ -31,6 +34,7 @@\n import org.apache.kafka.common.record.MemoryRecords;\n import org.apache.kafka.common.requests.ApiVersionsResponse;\n import org.apache.kafka.common.requests.MetadataRequest;\n+import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.ProduceRequest;\n import org.apache.kafka.common.requests.RequestHeader;\n import org.apache.kafka.common.requests.ResponseHeader;\n@@ -48,11 +52,13 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Optional;\n \n import static org.apache.kafka.common.protocol.ApiKeys.PRODUCE;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n@@ -83,6 +89,12 @@ private NetworkClient createNetworkClientWithStaticNodes() {\n                 ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), new LogContext());\n     }\n \n+    private NetworkClient createNetworkClientWithNoVersionDiscovery(Metadata metadata) {\n+        return new NetworkClient(selector, metadata, \"mock\", Integer.MAX_VALUE,\n+                reconnectBackoffMsTest, 0, 64 * 1024, 64 * 1024,\n+                defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, false, new ApiVersions(), new LogContext());\n+    }\n+\n     private NetworkClient createNetworkClientWithNoVersionDiscovery() {\n         return new NetworkClient(selector, metadataUpdater, \"mock\", Integer.MAX_VALUE,\n                 reconnectBackoffMsTest, reconnectBackoffMaxMsTest,\n@@ -532,16 +544,19 @@ public void testThrottlingNotEnabledForConnectionToOlderBroker() {\n     }\n \n     private int sendEmptyProduceRequest() {\n+        return sendEmptyProduceRequest(node.idString());\n+    }\n+\n+    private int sendEmptyProduceRequest(String nodeId) {\n         ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000,\n                 Collections.emptyMap());\n         TestCallbackHandler handler = new TestCallbackHandler();\n-        ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true,\n+        ClientRequest request = client.newClientRequest(nodeId, builder, time.milliseconds(), true,\n                 defaultRequestTimeoutMs, handler);\n         client.send(request, time.milliseconds());\n         return request.correlationId();\n     }\n \n-\n     private void sendResponse(ResponseHeader respHeader, Struct response) {\n         Struct responseHeaderStruct = respHeader.toStruct();\n         int size = responseHeaderStruct.sizeOf() + response.sizeOf();\n@@ -587,6 +602,49 @@ public void testLeastLoadedNode() {\n         assertNull(\"There should be NO leastloadednode\", leastNode);\n     }\n \n+    @Test\n+    public void testAuthenticationFailureWithInFlightMetadataRequest() {\n+        int refreshBackoffMs = 50;\n+\n+        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith(2, Collections.emptyMap());\n+        Metadata metadata = new Metadata(refreshBackoffMs, 5000, new LogContext(), new ClusterResourceListeners());\n+        metadata.update(metadataResponse, time.milliseconds());\n+\n+        Cluster cluster = metadata.fetch();\n+        Node node1 = cluster.nodes().get(0);\n+        Node node2 = cluster.nodes().get(1);\n+\n+        NetworkClient client = createNetworkClientWithNoVersionDiscovery(metadata);\n+\n+        awaitReady(client, node1);\n+\n+        metadata.requestUpdate();\n+        time.sleep(refreshBackoffMs);\n+\n+        client.poll(0, time.milliseconds());\n+\n+        Optional<Node> nodeWithPendingMetadataOpt = cluster.nodes().stream()\n+                .filter(node -> client.hasInFlightRequests(node.idString()))\n+                .findFirst();\n+        assertEquals(Optional.of(node1), nodeWithPendingMetadataOpt);\n+\n+        assertFalse(client.ready(node2, time.milliseconds()));\n+        selector.serverAuthenticationFailed(node2.idString());\n+        client.poll(0, time.milliseconds());\n+        assertNotNull(client.authenticationException(node2));\n+\n+        ByteBuffer requestBuffer = selector.completedSendBuffers().get(0).buffer();\n+        RequestHeader header = parseHeader(requestBuffer);\n+        assertEquals(ApiKeys.METADATA, header.apiKey());\n+\n+        ByteBuffer responseBuffer = metadataResponse.serialize(ApiKeys.METADATA, header.apiVersion(), header.correlationId());\n+        selector.delayedReceive(new DelayedReceive(node1.idString(), new NetworkReceive(node1.idString(), responseBuffer)));\n+\n+        int initialUpdateVersion = metadata.updateVersion();\n+        client.poll(0, time.milliseconds());\n+        assertEquals(initialUpdateVersion + 1, metadata.updateVersion());\n+    }\n+\n     @Test\n     public void testLeastLoadedNodeConsidersThrottledConnections() {\n         client.ready(node, time.milliseconds());\n@@ -840,9 +898,18 @@ public TestMetadataUpdater(List<Node> nodes) {\n         }\n \n         @Override\n-        public void handleFatalException(KafkaException exception) {\n-            failure = exception;\n-            super.handleFatalException(exception);\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeAuthException) {\n+            maybeAuthException.ifPresent(exception -> {\n+                failure = exception;\n+            });\n+            super.handleServerDisconnect(now, destinationId, maybeAuthException);\n+        }\n+\n+        @Override\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n+            maybeFatalException.ifPresent(exception -> {\n+                failure = exception;\n+            });\n         }\n \n         public KafkaException getAndClearFailure() {",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java",
                "sha": "a4145d1bc0c15a6ae22cfcf0ca99650a82fb8f4b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 2,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java",
                "patch": "@@ -235,7 +235,7 @@ public void run() {\n \n     @Test\n     public void testAuthenticationExceptionPropagatedFromMetadata() {\n-        metadata.failedUpdate(time.milliseconds(), new AuthenticationException(\"Authentication failed\"));\n+        metadata.fatalError(new AuthenticationException(\"Authentication failed\"));\n         try {\n             consumerClient.poll(time.timer(Duration.ZERO));\n             fail(\"Expected authentication error thrown\");\n@@ -264,7 +264,7 @@ public void testTopicAuthorizationExceptionPropagatedFromMetadata() {\n     @Test\n     public void testMetadataFailurePropagated() {\n         KafkaException metadataException = new KafkaException();\n-        metadata.failedUpdate(time.milliseconds(), metadataException);\n+        metadata.fatalError(metadataException);\n         try {\n             consumerClient.poll(time.timer(Duration.ZERO));\n             fail(\"Expected poll to throw exception\");",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java",
                "sha": "c50bce45dcfe0f1d8ee7c79f807fe66dfdf4e446",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 1,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "patch": "@@ -3338,7 +3338,7 @@ private void testGetOffsetsForTimesWithError(Errors errorForP0,\n         TopicPartition t2p0 = new TopicPartition(topicName2, 0);\n         // Expect a metadata refresh.\n         metadata.bootstrap(ClientUtils.parseAndValidateAddresses(Collections.singletonList(\"1.1.1.1:1111\"),\n-                ClientDnsLookup.DEFAULT), time.milliseconds());\n+                ClientDnsLookup.DEFAULT));\n \n         Map<String, Integer> partitionNumByTopic = new HashMap<>();\n         partitionNumByTopic.put(topicName, 2);",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "sha": "6440c42db53c203f7769cba2b85e9f9377a2b8b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 5,
                "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java",
                "patch": "@@ -22,7 +22,6 @@\n import org.apache.kafka.common.internals.ClusterResourceListeners;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.utils.LogContext;\n-import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Time;\n import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n@@ -37,9 +36,9 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n-import static org.junit.Assert.assertThrows;\n \n public class ProducerMetadataTest {\n \n@@ -178,9 +177,8 @@ public void testTopicExpiry() {\n     }\n \n     @Test\n-    public void testMetadataWaitAbortedOnFatalException() throws Exception {\n-        Time time = new MockTime();\n-        metadata.failedUpdate(time.milliseconds(), new AuthenticationException(\"Fatal exception from test\"));\n+    public void testMetadataWaitAbortedOnFatalException() {\n+        metadata.fatalError(new AuthenticationException(\"Fatal exception from test\"));\n         assertThrows(AuthenticationException.class, () -> metadata.awaitUpdate(0, 1000));\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java",
                "sha": "51168474fa4af81f7c9789aba3815f10d33a1d0e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 25,
                "filename": "clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java",
                "patch": "@@ -18,12 +18,14 @@\n \n import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.TopicDescription;\n import org.apache.kafka.clients.consumer.ConsumerConfig;\n import org.apache.kafka.clients.consumer.KafkaConsumer;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerConfig;\n import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.SaslConfigs;\n import org.apache.kafka.common.config.internals.BrokerSecurityConfigs;\n import org.apache.kafka.common.errors.SaslAuthenticationException;\n@@ -35,6 +37,7 @@\n import org.apache.kafka.common.serialization.StringDeserializer;\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -44,9 +47,9 @@\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.Future;\n \n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n+import static org.junit.Assert.assertThrows;\n \n public class ClientAuthenticationFailureTest {\n     private static MockTime time = new MockTime(50);\n@@ -88,13 +91,10 @@ public void testConsumerWithInvalidCredentials() {\n         StringDeserializer deserializer = new StringDeserializer();\n \n         try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props, deserializer, deserializer)) {\n-            consumer.subscribe(Arrays.asList(topic));\n-            consumer.poll(Duration.ofSeconds(10));\n-            fail(\"Expected an authentication error!\");\n-        } catch (SaslAuthenticationException e) {\n-            // OK\n-        } catch (Exception e) {\n-            throw new AssertionError(\"Expected only an authentication error, but another error occurred.\", e);\n+            assertThrows(SaslAuthenticationException.class, () -> {\n+                consumer.subscribe(Collections.singleton(topic));\n+                consumer.poll(Duration.ofSeconds(10));\n+            });\n         }\n     }\n \n@@ -106,11 +106,8 @@ public void testProducerWithInvalidCredentials() {\n \n         try (KafkaProducer<String, String> producer = new KafkaProducer<>(props, serializer, serializer)) {\n             ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"message\");\n-            producer.send(record).get();\n-            fail(\"Expected an authentication error!\");\n-        } catch (Exception e) {\n-            assertTrue(\"Expected SaslAuthenticationException, got \" + e.getCause().getClass(),\n-                    e.getCause() instanceof SaslAuthenticationException);\n+            Future<RecordMetadata> future = producer.send(record);\n+            TestUtils.assertFutureThrows(future, SaslAuthenticationException.class);\n         }\n     }\n \n@@ -119,12 +116,8 @@ public void testAdminClientWithInvalidCredentials() {\n         Map<String, Object> props = new HashMap<>(saslClientConfigs);\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:\" + server.port());\n         try (Admin client = Admin.create(props)) {\n-            DescribeTopicsResult result = client.describeTopics(Collections.singleton(\"test\"));\n-            result.all().get();\n-            fail(\"Expected an authentication error!\");\n-        } catch (Exception e) {\n-            assertTrue(\"Expected SaslAuthenticationException, got \" + e.getCause().getClass(),\n-                    e.getCause() instanceof SaslAuthenticationException);\n+            KafkaFuture<Map<String, TopicDescription>> future = client.describeTopics(Collections.singleton(\"test\")).all();\n+            TestUtils.assertFutureThrows(future, SaslAuthenticationException.class);\n         }\n     }\n \n@@ -137,10 +130,7 @@ public void testTransactionalProducerWithInvalidCredentials() {\n         StringSerializer serializer = new StringSerializer();\n \n         try (KafkaProducer<String, String> producer = new KafkaProducer<>(props, serializer, serializer)) {\n-            producer.initTransactions();\n-            fail(\"Expected an authentication error!\");\n-        } catch (SaslAuthenticationException e) {\n-            // expected exception\n+            assertThrows(SaslAuthenticationException.class, producer::initTransactions);\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java",
                "sha": "0a93c6afebc9fe9a6a5c31a85ee2acbe18b27d23",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/test/MockSelector.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/test/MockSelector.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 11,
                "filename": "clients/src/test/java/org/apache/kafka/test/MockSelector.java",
                "patch": "@@ -16,9 +16,9 @@\n  */\n package org.apache.kafka.test;\n \n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.network.ChannelState;\n import org.apache.kafka.common.network.NetworkReceive;\n-import org.apache.kafka.common.network.NetworkSend;\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.network.Send;\n import org.apache.kafka.common.requests.ByteBufferChannel;\n@@ -88,13 +88,15 @@ public void serverDisconnect(String id) {\n         close(id);\n     }\n \n+    public void serverAuthenticationFailed(String id) {\n+        ChannelState authFailed = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED,\n+                new AuthenticationException(\"Authentication failed\"), null);\n+        this.disconnected.put(id, authFailed);\n+        close(id);\n+    }\n+\n     private void removeSendsForNode(String id, Collection<Send> sends) {\n-        Iterator<Send> iter = sends.iterator();\n-        while (iter.hasNext()) {\n-            Send send = iter.next();\n-            if (id.equals(send.destination()))\n-                iter.remove();\n-        }\n+        sends.removeIf(send -> id.equals(send.destination()));\n     }\n \n     public void clear() {\n@@ -153,10 +155,6 @@ private void completeDelayedReceives() {\n         return completedSends;\n     }\n \n-    public void completeSend(NetworkSend send) {\n-        this.completedSends.add(send);\n-    }\n-\n     public List<ByteBufferChannel> completedSendBuffers() {\n         return completedSendBuffers;\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/test/MockSelector.java",
                "sha": "90a83b0adc7cb656afe18982de58dd8bcbd4f18d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 1,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java",
                "patch": "@@ -96,7 +96,7 @@ public WorkerGroupMember(DistributedConfig config,\n             List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(\n                     config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG),\n                     config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG));\n-            this.metadata.bootstrap(addresses, time.milliseconds());\n+            this.metadata.bootstrap(addresses);\n             String metricGrpPrefix = \"connect\";\n             ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time);\n             NetworkClient netClient = new NetworkClient(",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java",
                "sha": "dfe54867fa9ac77e6e7f3caef00c9bd671156d31",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c",
                "deletions": 1,
                "filename": "core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala",
                "patch": "@@ -278,7 +278,7 @@ object BrokerApiVersionsCommand {\n       val brokerUrls = config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG)\n       val clientDnsLookup = config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)\n       val brokerAddresses = ClientUtils.parseAndValidateAddresses(brokerUrls, clientDnsLookup)\n-      metadata.bootstrap(brokerAddresses, time.milliseconds())\n+      metadata.bootstrap(brokerAddresses)\n \n       val selector = new Selector(\n         DefaultConnectionMaxIdleMs,",
                "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala",
                "sha": "92cdb9ee023966bb6f98b7dcafc476aa4571b019",
                "status": "modified"
            }
        ],
        "message": "KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure (#7682)\n\nThis patch fixes an NPE in `DefaultMetadataUpdater` due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.\r\n\r\nReviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",
        "parent": "https://github.com/apache/kafka/commit/95581f33f3f0a3dfb8769a2aa782cc11ffe1dccd",
        "repo": "kafka",
        "unit_tests": [
            "MetadataTest.java",
            "NetworkClientTest.java",
            "AdminMetadataManagerTest.java",
            "KafkaConsumerTest.java",
            "KafkaProducerTest.java",
            "ProducerMetadataTest.java"
        ]
    },
    "kafka_5ecac84": {
        "bug_id": "kafka_5ecac84",
        "commit": "https://github.com/apache/kafka/commit/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java?ref=5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java",
                "patch": "@@ -49,7 +49,7 @@ public TableProcessorNode(final String nodeName,\n     public String toString() {\n         return \"TableProcessorNode{\" +\n             \", processorParameters=\" + processorParameters +\n-            \", storeBuilder=\" + storeBuilder.name() +\n+            \", storeBuilder=\" + (storeBuilder == null ? \"null\" : storeBuilder.name()) +\n             \", storeNames=\" + Arrays.toString(storeNames) +\n             \"} \" + super.toString();\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java",
                "sha": "6fc5e25862b93acdc548971187d4c485762c742a",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/kafka/blob/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java?ref=5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java",
                "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals.graph;\n+\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+public class TableProcessorNodeTest {\n+    private static class TestProcessor extends AbstractProcessor<String, String> {\n+        @Override\n+        public void init(final ProcessorContext context) {\n+        }\n+\n+        @Override\n+        public void process(final String key, final String value) {\n+        }\n+\n+        @Override\n+        public void close() {\n+        }\n+    }\n+\n+    @Test\n+    public void shouldConvertToStringWithNullStoreBuilder() {\n+        final TableProcessorNode<String, String> node = new TableProcessorNode<>(\n+            \"name\",\n+            new ProcessorParameters<>(TestProcessor::new, \"processor\"),\n+            null,\n+            new String[]{\"store1\", \"store2\"}\n+        );\n+\n+        final String asString = node.toString();\n+        final String expected = \"storeBuilder=null\";\n+        assertTrue(\n+            String.format(\n+                \"Expected toString to return string with \\\"%s\\\", received: %s\",\n+                expected,\n+                asString),\n+            asString.contains(expected)\n+        );\n+    }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/kafka/raw/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java",
                "sha": "a2c4938336376e4be2bc3eb7a464dd136cd93745",
                "status": "added"
            }
        ],
        "message": "MINOR: Fix `toString` NPE in tableProcessorNode (#6807)\n\nThis gets hit when debug logging is enabled.\r\n\r\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/46a02f3231cd6d340c622636159b9f59b4b3cb6e",
        "repo": "kafka",
        "unit_tests": [
            "TableProcessorNodeTest.java"
        ]
    },
    "kafka_6352a30": {
        "bug_id": "kafka_6352a30",
        "commit": "https://github.com/apache/kafka/commit/6352a30f46f2da11a8dc3e58912d0a2db8284c35",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/6352a30f46f2da11a8dc3e58912d0a2db8284c35/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java?ref=6352a30f46f2da11a8dc3e58912d0a2db8284c35",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "patch": "@@ -682,6 +682,7 @@ private void removeStandbyTasks() {\n \n         standbyTasks.clear();\n         standbyTasksByPartition.clear();\n+        standbyRecords.clear();\n     }\n \n     private void ensureCopartitioning(Collection<Set<String>> copartitionGroups) {",
                "raw_url": "https://github.com/apache/kafka/raw/6352a30f46f2da11a8dc3e58912d0a2db8284c35/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
                "sha": "d51974a51f15924007620545c5dadf4149ff4b96",
                "status": "modified"
            }
        ],
        "message": "HOTFIX: Fix NPE after standby task reassignment\n\nBuffered records of change logs must be cleared upon reassignment of standby tasks.\n\nAuthor: Yasuhiro Matsuda <yasuhiro@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #889 from ymatsuda/hotfix",
        "parent": "https://github.com/apache/kafka/commit/9f5a1f87667c23db557a712d51c45541372f3c5d",
        "repo": "kafka",
        "unit_tests": [
            "StreamThreadTest.java"
        ]
    },
    "kafka_65f6a79": {
        "bug_id": "kafka_65f6a79",
        "commit": "https://github.com/apache/kafka/commit/65f6a7964f5e59e789eae3cdd4d301bb6a649064",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java?ref=65f6a7964f5e59e789eae3cdd4d301bb6a649064",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java",
                "patch": "@@ -190,6 +190,9 @@ private void putHead(LRUNode node) {\n     }\n \n     synchronized void evict() {\n+        if (tail == null) {\n+            return;\n+        }\n         final LRUNode eldest = tail;\n         currentSizeBytes -= eldest.size();\n         if (eldest.entry.isDirty()) {",
                "raw_url": "https://github.com/apache/kafka/raw/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java",
                "sha": "65a836eb9d9d4eff1b65a908631961b677575d84",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java?ref=65f6a7964f5e59e789eae3cdd4d301bb6a649064",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java",
                "patch": "@@ -186,4 +186,9 @@ public void shouldGetIteratorOverAllKeys() throws Exception {\n         assertEquals(Bytes.wrap(new byte[]{2}), iterator.next());\n         assertFalse(iterator.hasNext());\n     }\n+\n+    @Test\n+    public void shouldNotThrowNullPointerWhenCacheIsEmptyAndEvictionCalled() throws Exception {\n+        cache.evict();\n+    }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/kafka/raw/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java",
                "sha": "3067256b5a994b8f50ce20b338328da66a21f142",
                "status": "modified"
            }
        ],
        "message": "KAFKA-4300: NamedCache throws an NPE when evict is called and the cache is empty\n\nIf evict is called on a NamedCache and the cache is empty an NPE is thrown. This was reported on the user list from a developer running 0.10.1.\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Eno Thereska, Matthias J. Sax, Guozhang Wang\n\nCloses #2024 from dguy/cache-bug",
        "parent": "https://github.com/apache/kafka/commit/83116c733d09f5c85ced56701cdb614ce6efc847",
        "repo": "kafka",
        "unit_tests": [
            "NamedCacheTest.java"
        ]
    },
    "kafka_6810617": {
        "bug_id": "kafka_6810617",
        "commit": "https://github.com/apache/kafka/commit/6810617179222ae659343efb02ef7e6cefb15662",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java?ref=6810617179222ae659343efb02ef7e6cefb15662",
                "deletions": 0,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "patch": "@@ -39,6 +39,7 @@ public WorkerConfigTransformer(Worker worker, Map<String, ConfigProvider> config\n     }\n \n     public Map<String, String> transform(String connectorName, Map<String, String> configs) {\n+        if (configs == null) return null;\n         ConfigTransformerResult result = configTransformer.transform(configs);\n         scheduleReload(connectorName, result.ttls());\n         return result.data();",
                "raw_url": "https://github.com/apache/kafka/raw/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java",
                "sha": "7efb481ac75fb9225d33a5dd242639d86713828d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/kafka/blob/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java?ref=6810617179222ae659343efb02ef7e6cefb15662",
                "deletions": 0,
                "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "patch": "@@ -32,6 +32,7 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.powermock.api.easymock.PowerMock.replayAll;\n \n @RunWith(PowerMockRunner.class)\n@@ -110,6 +111,11 @@ public void testReplaceVariableWithTTLFirstCancelThenScheduleRestart() throws Ex\n         assertEquals(TEST_RESULT_WITH_LONGER_TTL, result.get(MY_KEY));\n     }\n \n+    @Test\n+    public void testTransformNullConfiguration() {\n+        assertNull(configTransformer.transform(MY_CONNECTOR, null));\n+    }\n+\n     public static class TestConfigProvider implements ConfigProvider {\n \n         public void configure(Map<String, ?> configs) {",
                "raw_url": "https://github.com/apache/kafka/raw/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java",
                "sha": "300022de76ebd5300b56a730456cf9601b78f3d4",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7048 NPE when creating connector (#5202)\n\nReviewers: Robert Yokota <rayokota@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34",
        "repo": "kafka",
        "unit_tests": [
            "WorkerConfigTransformerTest.java"
        ]
    },
    "kafka_6d3ff13": {
        "bug_id": "kafka_6d3ff13",
        "commit": "https://github.com/apache/kafka/commit/6d3ff132b57835fc879d678e9addc5e7c3804205",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/kafka/blob/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java?ref=6d3ff132b57835fc879d678e9addc5e7c3804205",
                "deletions": 3,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java",
                "patch": "@@ -282,7 +282,7 @@ private boolean isMatch(final String topic) {\n \n         @Override\n         Source describe() {\n-            return new Source(name, new HashSet<>(topics), pattern);\n+            return new Source(name, topics.size() == 0 ? null : new HashSet<>(topics), pattern);\n         }\n     }\n \n@@ -1281,6 +1281,9 @@ private boolean nodeGroupContainsGlobalSourceNode(final Set<String> allNodesOfGr\n         @Override\n         public int compare(final TopologyDescription.Node node1,\n                            final TopologyDescription.Node node2) {\n+            if (node1.equals(node2)) {\n+                return 0;\n+            }\n             final int size1 = ((AbstractNode) node1).size;\n             final int size2 = ((AbstractNode) node2).size;\n \n@@ -1399,6 +1402,7 @@ public int hashCode() {\n         int size;\n \n         AbstractNode(final String name) {\n+            Objects.requireNonNull(name, \"name cannot be null\");\n             this.name = name;\n             this.size = 1;\n         }\n@@ -1435,6 +1439,13 @@ public Source(final String name,\n                       final Set<String> topics,\n                       final Pattern pattern) {\n             super(name);\n+            if (topics == null && pattern == null) {\n+                throw new IllegalArgumentException(\"Either topics or pattern must be not-null, but both are null.\");\n+            }\n+            if (topics != null && pattern != null) {\n+                throw new IllegalArgumentException(\"Either topics or pattern must be null, but both are not null.\");\n+            }\n+\n             this.topics = topics;\n             this.topicPattern = pattern;\n         }\n@@ -1479,8 +1490,10 @@ public boolean equals(final Object o) {\n             final Source source = (Source) o;\n             // omit successor to avoid infinite loops\n             return name.equals(source.name)\n-                && topics.equals(source.topics)\n-                && topicPattern.equals(source.topicPattern);\n+                && (topics == null && source.topics == null\n+                    || topics != null && topics.equals(source.topics))\n+                && (topicPattern == null && source.topicPattern == null\n+                    || topicPattern != null && topicPattern.pattern().equals(source.topicPattern.pattern()));\n         }\n \n         @Override\n@@ -1709,6 +1722,9 @@ public String toString() {\n         @Override\n         public int compare(final TopologyDescription.GlobalStore globalStore1,\n                            final TopologyDescription.GlobalStore globalStore2) {\n+            if (globalStore1.equals(globalStore2)) {\n+                return 0;\n+            }\n             return globalStore1.id() - globalStore2.id();\n         }\n     }\n@@ -1719,6 +1735,9 @@ public int compare(final TopologyDescription.GlobalStore globalStore1,\n         @Override\n         public int compare(final TopologyDescription.Subtopology subtopology1,\n                            final TopologyDescription.Subtopology subtopology2) {\n+            if (subtopology1.equals(subtopology2)) {\n+                return 0;\n+            }\n             return subtopology1.id() - subtopology2.id();\n         }\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java",
                "sha": "2d527e51f6b650be09ee8ac78598c57b1555c4e0",
                "status": "modified"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/kafka/blob/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java?ref=6d3ff132b57835fc879d678e9addc5e7c3804205",
                "deletions": 34,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java",
                "patch": "@@ -23,15 +23,13 @@\n import org.apache.kafka.streams.Topology;\n import org.apache.kafka.streams.TopologyDescription;\n import org.apache.kafka.streams.errors.TopologyException;\n-import org.apache.kafka.streams.processor.Processor;\n-import org.apache.kafka.streams.processor.ProcessorSupplier;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.processor.TopicNameExtractor;\n import org.apache.kafka.streams.state.KeyValueStore;\n import org.apache.kafka.streams.state.StoreBuilder;\n import org.apache.kafka.streams.state.Stores;\n-import org.apache.kafka.test.MockProcessorSupplier;\n import org.apache.kafka.test.MockKeyValueStoreBuilder;\n+import org.apache.kafka.test.MockProcessorSupplier;\n import org.apache.kafka.test.MockTimestampExtractor;\n import org.apache.kafka.test.StreamsTestUtils;\n import org.junit.Test;\n@@ -50,12 +48,14 @@\n import static java.time.Duration.ofSeconds;\n import static java.util.Arrays.asList;\n import static org.apache.kafka.common.utils.Utils.mkSet;\n-import static org.hamcrest.core.IsInstanceOf.instanceOf;\n+import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-\n+import static org.hamcrest.Matchers.not;\n+import static org.hamcrest.core.IsInstanceOf.instanceOf;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertThrows;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -353,9 +353,9 @@ public void testTopicGroups() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n \n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> expectedTopicGroups = new HashMap<>();\n-        expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-1\", \"X-topic-1x\", \"topic-2\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n-        expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-3\", \"topic-4\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n-        expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-5\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n+        expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-1\", \"X-topic-1x\", \"topic-2\"), Collections.emptyMap(), Collections.emptyMap()));\n+        expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-3\", \"topic-4\"), Collections.emptyMap(), Collections.emptyMap()));\n+        expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-5\"), Collections.emptyMap(), Collections.emptyMap()));\n \n         assertEquals(3, topicGroups.size());\n         assertEquals(expectedTopicGroups, topicGroups);\n@@ -393,17 +393,17 @@ public void testTopicGroupsByStateStore() {\n         final String store2 = ProcessorStateManager.storeChangelogTopic(\"X\", \"store-2\");\n         final String store3 = ProcessorStateManager.storeChangelogTopic(\"X\", \"store-3\");\n         expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-1\", \"topic-1x\", \"topic-2\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store1, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store1, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-1\", \"topic-1x\", \"topic-2\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store1, new UnwindowedChangelogTopicConfig(store1, Collections.emptyMap()))));\n         expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-3\", \"topic-4\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store2, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store2, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-3\", \"topic-4\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store2, new UnwindowedChangelogTopicConfig(store2, Collections.emptyMap()))));\n         expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-5\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store3, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store3, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-5\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store3, new UnwindowedChangelogTopicConfig(store3, Collections.emptyMap()))));\n \n         assertEquals(3, topicGroups.size());\n         assertEquals(expectedTopicGroups, topicGroups);\n@@ -499,12 +499,7 @@ public void shouldNotAllowNullTopicChooserWhenAddingSink() {\n \n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullNameWhenAddingProcessor() {\n-        builder.addProcessor(null, new ProcessorSupplier() {\n-            @Override\n-            public Processor get() {\n-                return null;\n-            }\n-        });\n+        builder.addProcessor(null, () -> null);\n     }\n \n     @Test(expected = NullPointerException.class)\n@@ -604,14 +599,14 @@ public void shouldAddInternalTopicConfigForWindowStores() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();\n         final InternalTopicConfig topicConfig1 = topicsInfo.stateChangelogTopics.get(\"appId-store1-changelog\");\n-        final Map<String, String> properties1 = topicConfig1.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties1 = topicConfig1.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(2, properties1.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + \",\" + TopicConfig.CLEANUP_POLICY_DELETE, properties1.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"40000\", properties1.get(TopicConfig.RETENTION_MS_CONFIG));\n         assertEquals(\"appId-store1-changelog\", topicConfig1.name());\n         assertTrue(topicConfig1 instanceof WindowedChangelogTopicConfig);\n         final InternalTopicConfig topicConfig2 = topicsInfo.stateChangelogTopics.get(\"appId-store2-changelog\");\n-        final Map<String, String> properties2 = topicConfig2.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties2 = topicConfig2.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(2, properties2.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + \",\" + TopicConfig.CLEANUP_POLICY_DELETE, properties2.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"40000\", properties2.get(TopicConfig.RETENTION_MS_CONFIG));\n@@ -628,7 +623,7 @@ public void shouldAddInternalTopicConfigForNonWindowStores() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();\n         final InternalTopicConfig topicConfig = topicsInfo.stateChangelogTopics.get(\"appId-store-changelog\");\n-        final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties = topicConfig.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(1, properties.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT, properties.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"appId-store-changelog\", topicConfig.name());\n@@ -642,7 +637,7 @@ public void shouldAddInternalTopicConfigForRepartitionTopics() {\n         builder.addSource(null, \"source\", null, null, null, \"foo\");\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = builder.topicGroups().values().iterator().next();\n         final InternalTopicConfig topicConfig = topicsInfo.repartitionSourceTopics.get(\"appId-foo\");\n-        final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties = topicConfig.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(3, properties.size());\n         assertEquals(String.valueOf(-1), properties.get(TopicConfig.RETENTION_MS_CONFIG));\n         assertEquals(TopicConfig.CLEANUP_POLICY_DELETE, properties.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n@@ -708,32 +703,32 @@ public void shouldSortProcessorNodesCorrectly() {\n \n         assertTrue(iterator.hasNext());\n         InternalTopologyBuilder.AbstractNode node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"source1\"));\n+        assertEquals(\"source1\", node.name);\n         assertEquals(6, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"source2\"));\n+        assertEquals(\"source2\", node.name);\n         assertEquals(4, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor2\"));\n+        assertEquals(\"processor2\", node.name);\n         assertEquals(3, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor1\"));\n+        assertEquals(\"processor1\", node.name);\n         assertEquals(2, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor3\"));\n+        assertEquals(\"processor3\", node.name);\n         assertEquals(2, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"sink1\"));\n+        assertEquals(\"sink1\", node.name);\n         assertEquals(1, node.size);\n     }\n \n@@ -760,7 +755,7 @@ public void shouldConnectRegexMatchedTopicsToStateStore() throws Exception {\n         final Map<String, List<String>> stateStoreAndTopics = builder.stateStoreNameToSourceTopics();\n         final List<String> topics = stateStoreAndTopics.get(storeBuilder.name());\n \n-        assertTrue(\"Expected to contain two topics\", topics.size() == 2);\n+        assertEquals(\"Expected to contain two topics\", 2, topics.size());\n \n         assertTrue(topics.contains(\"topic-2\"));\n         assertTrue(topics.contains(\"topic-3\"));\n@@ -781,4 +776,74 @@ public void shouldNotAllowToAddGlobalStoreWithSourceNameEqualsProcessorName() {\n             sameNameForSourceAndProcessor,\n             new MockProcessorSupplier());\n     }\n+\n+    @Test\n+    public void shouldThrowIfNameIsNull() {\n+        final Exception e = assertThrows(NullPointerException.class, () -> new InternalTopologyBuilder.Source(null, Collections.emptySet(), null));\n+        assertEquals(\"name cannot be null\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void shouldThrowIfTopicAndPatternAreNull() {\n+        final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source(\"name\", null, null));\n+        assertEquals(\"Either topics or pattern must be not-null, but both are null.\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void shouldThrowIfBothTopicAndPatternAreNotNull() {\n+        final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source(\"name\", Collections.emptySet(), Pattern.compile(\"\")));\n+        assertEquals(\"Either topics or pattern must be null, but both are not null.\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void sourceShouldBeEqualIfNameAndTopicListAreTheSame() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source sameAsBase = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+\n+        assertThat(base, equalTo(sameAsBase));\n+    }\n+\n+    @Test\n+    public void sourceShouldBeEqualIfNameAndPatternAreTheSame() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source sameAsBase = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+\n+        assertThat(base, equalTo(sameAsBase));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentNamesWithSameTopicList() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source differentName = new InternalTopologyBuilder.Source(\"name2\", Collections.singleton(\"topic\"), null);\n+\n+        assertThat(base, not(equalTo(differentName)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentNamesWithSamePattern() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source differentName = new InternalTopologyBuilder.Source(\"name2\", null, Pattern.compile(\"topic\"));\n+\n+        assertThat(base, not(equalTo(differentName)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentTopicList() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source differentTopicList = new InternalTopologyBuilder.Source(\"name\", Collections.emptySet(), null);\n+        final InternalTopologyBuilder.Source differentTopic = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic2\"), null);\n+\n+        assertThat(base, not(equalTo(differentTopicList)));\n+        assertThat(base, not(equalTo(differentTopic)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentPattern() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source differentPattern = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic2\"));\n+        final InternalTopologyBuilder.Source overlappingPattern = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"top*\"));\n+\n+        assertThat(base, not(equalTo(differentPattern)));\n+        assertThat(base, not(equalTo(overlappingPattern)));\n+    }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java",
                "sha": "b86211fbcd238ef9858438295f1628b3a6edb26f",
                "status": "modified"
            }
        ],
        "message": "KAFKA-8240: Fix NPE in Source.equals() (#6589)\n\nReviewers: John Roesler <john@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/3ba4686d4d650f0f9155b2e22dddb192a5a56a6c",
        "repo": "kafka",
        "unit_tests": [
            "InternalTopologyBuilderTest.java"
        ]
    },
    "kafka_70d8828": {
        "bug_id": "kafka_70d8828",
        "commit": "https://github.com/apache/kafka/commit/70d882861e1bf3eb503c84a31834e8b628de2df9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 5,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "patch": "@@ -523,14 +523,13 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n     private List<ErrorReporter> sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                   ErrorHandlingMetrics errorHandlingMetrics) {\n         ArrayList<ErrorReporter> reporters = new ArrayList<>();\n-        LogReporter logReporter = new LogReporter(id, connConfig);\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);\n         reporters.add(logReporter);\n \n         // check if topic for dead letter queue exists\n         String topic = connConfig.dlqTopicName();\n         if (topic != null && !topic.isEmpty()) {\n-            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(config, id, connConfig, producerProps);\n+            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(config, id, connConfig, producerProps, errorHandlingMetrics);\n             reporters.add(reporter);\n         }\n \n@@ -540,8 +539,7 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n     private List<ErrorReporter> sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics) {\n         List<ErrorReporter> reporters = new ArrayList<>();\n-        LogReporter logReporter = new LogReporter(id, connConfig);\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);\n         reporters.add(logReporter);\n \n         return reporters;",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "sha": "df73a434d31b2859ba83a05348bf5b835d9211e0",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 8,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java",
                "patch": "@@ -36,6 +36,7 @@\n import java.io.PrintStream;\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ExecutionException;\n \n import static java.util.Collections.singleton;\n@@ -66,13 +67,14 @@\n \n     private final SinkConnectorConfig connConfig;\n     private final ConnectorTaskId connectorTaskId;\n+    private final ErrorHandlingMetrics errorHandlingMetrics;\n \n     private KafkaProducer<byte[], byte[]> kafkaProducer;\n-    private ErrorHandlingMetrics errorHandlingMetrics;\n \n     public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n                                                          ConnectorTaskId id,\n-                                                         SinkConnectorConfig sinkConfig, Map<String, Object> producerProps) {\n+                                                         SinkConnectorConfig sinkConfig, Map<String, Object> producerProps,\n+                                                         ErrorHandlingMetrics errorHandlingMetrics) {\n         String topic = sinkConfig.dlqTopicName();\n \n         try (AdminClient admin = AdminClient.create(workerConfig.originals())) {\n@@ -90,7 +92,7 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n         }\n \n         KafkaProducer<byte[], byte[]> dlqProducer = new KafkaProducer<>(producerProps);\n-        return new DeadLetterQueueReporter(dlqProducer, sinkConfig, id);\n+        return new DeadLetterQueueReporter(dlqProducer, sinkConfig, id, errorHandlingMetrics);\n     }\n \n     /**\n@@ -99,14 +101,16 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n      * @param kafkaProducer a Kafka Producer to produce the original consumed records.\n      */\n     // Visible for testing\n-    DeadLetterQueueReporter(KafkaProducer<byte[], byte[]> kafkaProducer, SinkConnectorConfig connConfig, ConnectorTaskId id) {\n+    DeadLetterQueueReporter(KafkaProducer<byte[], byte[]> kafkaProducer, SinkConnectorConfig connConfig,\n+                            ConnectorTaskId id, ErrorHandlingMetrics errorHandlingMetrics) {\n+        Objects.requireNonNull(kafkaProducer);\n+        Objects.requireNonNull(connConfig);\n+        Objects.requireNonNull(id);\n+        Objects.requireNonNull(errorHandlingMetrics);\n+\n         this.kafkaProducer = kafkaProducer;\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n-    }\n-\n-    @Override\n-    public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {\n         this.errorHandlingMetrics = errorHandlingMetrics;\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java",
                "sha": "c059dcff793a0ec18e3287d35fd17c7266407f63",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 8,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java",
                "patch": "@@ -28,12 +28,4 @@\n      */\n     void report(ProcessingContext context);\n \n-    /**\n-     * Provides the container for error handling metrics to implementations. This method will be called once the error\n-     * reporter object is instantiated.\n-     *\n-     * @param errorHandlingMetrics metrics for error handling (cannot be null).\n-     */\n-    void metrics(ErrorHandlingMetrics errorHandlingMetrics);\n-\n }",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java",
                "sha": "58336163fbf4a84176a77b5d2716143640dc992b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 7,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.Objects;\n+\n /**\n  * Writes errors and their context to application logs.\n  */\n@@ -30,12 +32,16 @@\n \n     private final ConnectorTaskId id;\n     private final ConnectorConfig connConfig;\n+    private final ErrorHandlingMetrics errorHandlingMetrics;\n \n-    private ErrorHandlingMetrics errorHandlingMetrics;\n+    public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics) {\n+        Objects.requireNonNull(id);\n+        Objects.requireNonNull(connConfig);\n+        Objects.requireNonNull(errorHandlingMetrics);\n \n-    public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig) {\n         this.id = id;\n         this.connConfig = connConfig;\n+        this.errorHandlingMetrics = errorHandlingMetrics;\n     }\n \n     /**\n@@ -57,11 +63,6 @@ public void report(ProcessingContext context) {\n         errorHandlingMetrics.recordErrorLogged();\n     }\n \n-    @Override\n-    public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {\n-        this.errorHandlingMetrics = errorHandlingMetrics;\n-    }\n-\n     // Visible for testing\n     String message(ProcessingContext context) {\n         return String.format(\"Error encountered in task %s. %s\", String.valueOf(id),",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java",
                "sha": "8b07adf8e499288c76041f515a7d5897006ee65f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 6,
                "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
                "patch": "@@ -166,8 +166,7 @@ public void testErrorHandlingInSinkTasks() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);\n@@ -218,8 +217,7 @@ public void testErrorHandlingInSourceTasks() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);\n@@ -283,8 +281,7 @@ public void testErrorHandlingInSourceTasksWthBadConverter() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
                "sha": "1bf9c717068e3098da95d231e665f74c782fc0fc",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9",
                "deletions": 16,
                "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java",
                "patch": "@@ -94,10 +94,15 @@ public void tearDown() {\n         }\n     }\n \n+    @Test(expected = NullPointerException.class)\n+    public void initializeDLQWithNullMetrics() {\n+        new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID, null);\n+    }\n+\n     @Test\n     public void testDLQConfigWithEmptyTopicName() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(emptyMap()), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -111,8 +116,8 @@ public void testDLQConfigWithEmptyTopicName() {\n \n     @Test\n     public void testDLQConfigWithValidTopicName() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -126,8 +131,8 @@ public void testDLQConfigWithValidTopicName() {\n \n     @Test\n     public void testReportDLQTwice() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -142,8 +147,7 @@ public void testReportDLQTwice() {\n \n     @Test\n     public void testLogOnDisabledLogReporter() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n         context.error(new RuntimeException());\n@@ -155,8 +159,7 @@ public void testLogOnDisabledLogReporter() {\n \n     @Test\n     public void testLogOnEnabledLogReporter() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n         context.error(new RuntimeException());\n@@ -168,8 +171,7 @@ public void testLogOnEnabledLogReporter() {\n \n     @Test\n     public void testLogMessageWithNoRecords() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -184,8 +186,7 @@ public void testLogMessageWithSinkRecords() {\n         props.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         props.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n \n-        LogReporter logReporter = new LogReporter(TASK_ID, config(props));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(props), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -208,7 +209,7 @@ public void testDlqHeaderConsumerRecord() {\n         Map<String, String> props = new HashMap<>();\n         props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, \"true\");\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = new ProcessingContext();\n         context.consumerRecord(new ConsumerRecord<>(\"source-topic\", 7, 10, \"source-key\".getBytes(), \"source-value\".getBytes()));\n@@ -236,7 +237,7 @@ public void testDlqHeaderIsAppended() {\n         Map<String, String> props = new HashMap<>();\n         props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, \"true\");\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = new ProcessingContext();\n         context.consumerRecord(new ConsumerRecord<>(\"source-topic\", 7, 10, \"source-key\".getBytes(), \"source-value\".getBytes()));",
                "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java",
                "sha": "fa628b0984080b2f61b1cafecacf0f57a982d891",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7228: Set errorHandlingMetrics for dead letter queue\n\nDLQ reporter does not get a `errorHandlingMetrics` object when created by the worker. This results in an NPE.\n\nSigned-off-by: Arjun Satish <arjunconfluent.io>\n\n*More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.*\n\n*Summary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.*\n\nAuthor: Arjun Satish <arjun@confluent.io>\n\nReviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #5440 from wicknicks/KAFKA-7228",
        "parent": "https://github.com/apache/kafka/commit/596c6c0c0b27c13f2017c770ea37cd39e27e5dcf",
        "repo": "kafka",
        "unit_tests": [
            "WorkerTest.java",
            "ErrorReporterTest.java"
        ]
    },
    "kafka_783900c": {
        "bug_id": "kafka_783900c",
        "commit": "https://github.com/apache/kafka/commit/783900c259511f86f5af03cbd96f2e74833447b9",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java?ref=783900c259511f86f5af03cbd96f2e74833447b9",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java",
                "patch": "@@ -77,14 +77,16 @@ public void process(final K key, final V value) {\n         try {\n             collector.send(topic, key, value, null, timestamp, keySerializer, valSerializer, partitioner);\n         } catch (ClassCastException e) {\n+            final String keyClass = key == null ? \"unknown because key is null\" : key.getClass().getName();\n+            final String valueClass = value == null ? \"unknown because value is null\" : value.getClass().getName();\n             throw new StreamsException(\n                     String.format(\"A serializer (key: %s / value: %s) is not compatible to the actual key or value type \" +\n                                     \"(key type: %s / value type: %s). Change the default Serdes in StreamConfig or \" +\n                                     \"provide correct Serdes via method parameters.\",\n                                     keySerializer.getClass().getName(),\n                                     valSerializer.getClass().getName(),\n-                                    key.getClass().getName(),\n-                                    value.getClass().getName()),\n+                                    keyClass,\n+                                    valueClass),\n                     e);\n         }\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java",
                "sha": "3d4f2829cfeb1f46f905ff4ce80c011ba572c222",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/kafka/blob/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java?ref=783900c259511f86f5af03cbd96f2e74833447b9",
                "deletions": 20,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java",
                "patch": "@@ -20,52 +20,106 @@\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.streams.StreamsConfig;\n import org.apache.kafka.streams.errors.StreamsException;\n import org.apache.kafka.streams.state.StateSerdes;\n import org.apache.kafka.test.MockProcessorContext;\n import org.junit.Test;\n \n-import java.util.Properties;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n \n public class SinkNodeTest {\n \n-    @Test(expected = StreamsException.class)\n+    @Test\n     @SuppressWarnings(\"unchecked\")\n-    public void invalidInputRecordTimestampTest() {\n+    public void shouldThrowStreamsExceptionOnInputRecordWithInvalidTimestamp() {\n+        // Given\n         final Serializer anySerializer = Serdes.Bytes().serializer();\n         final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n+        sink.init(context);\n+        final Bytes anyKey = new Bytes(\"any key\".getBytes());\n+        final Bytes anyValue = new Bytes(\"any value\".getBytes());\n \n-        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,  new RecordCollectorImpl(null, null));\n-        context.setTime(-1);\n+        // When/Then\n+        context.setTime(-1); // ensures a negative timestamp is set for the record we send next\n+        try {\n+            sink.process(anyKey, anyValue);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException ignored) {\n+        }\n+    }\n \n-        final SinkNode sink = new SinkNode<>(\"name\", \"output-topic\", anySerializer, anySerializer, null);\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n+        final Serializer anySerializer = Serdes.Bytes().serializer();\n+        final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(0);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n         sink.init(context);\n+        final String keyOfDifferentTypeThanSerializer = \"key with different type\";\n+        final String valueOfDifferentTypeThanSerializer = \"value with different type\";\n \n-        sink.process(null, null);\n+        // When/Then\n+        try {\n+            sink.process(keyOfDifferentTypeThanSerializer, valueOfDifferentTypeThanSerializer);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException e) {\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+        }\n     }\n \n-    @Test(expected = StreamsException.class)\n+    @Test\n     @SuppressWarnings(\"unchecked\")\n-    public void shouldThrowStreamsExceptionOnKeyValyeTypeSerializerMissmatch() {\n+    public void shouldHandleNullKeysWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n         final Serializer anySerializer = Serdes.Bytes().serializer();\n         final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(1);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n+        sink.init(context);\n+        final String invalidValueToTriggerSerializerMismatch = \"\";\n \n-        Properties config = new Properties();\n-        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n-        final MockProcessorContext context = new MockProcessorContext(anyStateSerde, new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n-        context.setTime(0);\n+        // When/Then\n+        try {\n+            sink.process(null, invalidValueToTriggerSerializerMismatch);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException e) {\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+            assertThat(e.getMessage(), containsString(\"unknown because key is null\"));\n+        }\n+    }\n \n-        final SinkNode sink = new SinkNode<>(\"name\", \"output-topic\", anySerializer, anySerializer, null);\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldHandleNullValuesWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n+        final Serializer anySerializer = Serdes.Bytes().serializer();\n+        final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(1);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n         sink.init(context);\n+        final String invalidKeyToTriggerSerializerMismatch = \"\";\n \n+        // When/Then\n         try {\n-            sink.process(\"\", \"\");\n+            sink.process(invalidKeyToTriggerSerializerMismatch, null);\n+            fail(\"Should have thrown StreamsException\");\n         } catch (final StreamsException e) {\n-            if (e.getCause() instanceof ClassCastException) {\n-                throw e;\n-            }\n-            throw new RuntimeException(e);\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+            assertThat(e.getMessage(), containsString(\"unknown because value is null\"));\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java",
                "sha": "dc9129ab5ade262e6fe732a9f5229fc006bd83ae",
                "status": "modified"
            }
        ],
        "message": "MINOR: Guard against NPE when throwing StreamsException on serializer mismatch\n\nAuthor: Michael G. Noll <michael@confluent.io>\n\nReviewers: Damian Guy, Guozhang Wang\n\nCloses #2696 from miguno/trunk-sinknode-NPE",
        "parent": "https://github.com/apache/kafka/commit/9e787716b013595851b4a6c1ddf8b8af1ec0f42e",
        "repo": "kafka",
        "unit_tests": [
            "SinkNodeTest.java"
        ]
    },
    "kafka_7b16b47": {
        "bug_id": "kafka_7b16b47",
        "commit": "https://github.com/apache/kafka/commit/7b16b4731666ff321fbe46828d526872ff5f56d7",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/kafka/blob/7b16b4731666ff321fbe46828d526872ff5f56d7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java?ref=7b16b4731666ff321fbe46828d526872ff5f56d7",
                "deletions": 17,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java",
                "patch": "@@ -206,24 +206,16 @@ public synchronized void ensureCoordinatorReady() {\n         }\n     }\n \n-    protected RequestFuture<Void> lookupCoordinator() {\n-        if (findCoordinatorFuture == null) {\n+    protected synchronized RequestFuture<Void> lookupCoordinator() {\n+        if (findCoordinatorFuture == null)\n             findCoordinatorFuture = sendGroupCoordinatorRequest();\n-            findCoordinatorFuture.addListener(new RequestFutureListener<Void>() {\n-                @Override\n-                public void onSuccess(Void value) {\n-                    findCoordinatorFuture = null;\n-                }\n-\n-                @Override\n-                public void onFailure(RuntimeException e) {\n-                    findCoordinatorFuture = null;\n-                }\n-            });\n-        }\n         return findCoordinatorFuture;\n     }\n \n+    private synchronized void clearFindCoordinatorFuture() {\n+        findCoordinatorFuture = null;\n+    }\n+\n     /**\n      * Check whether the group should be rejoined (e.g. if metadata changes)\n      * @return true if it should, false otherwise\n@@ -532,6 +524,7 @@ public void onSuccess(ClientResponse resp, RequestFuture<Void> future) {\n             // for the coordinator in the underlying network client layer\n             // TODO: this needs to be better handled in KAFKA-1935\n             Errors error = Errors.forCode(groupCoordinatorResponse.errorCode());\n+            clearFindCoordinatorFuture();\n             if (error == Errors.NONE) {\n                 synchronized (AbstractCoordinator.this) {\n                     AbstractCoordinator.this.coordinator = new Node(\n@@ -550,6 +543,12 @@ public void onSuccess(ClientResponse resp, RequestFuture<Void> future) {\n                 future.raise(error);\n             }\n         }\n+\n+        @Override\n+        public void onFailure(RuntimeException e, RequestFuture<Void> future) {\n+            clearFindCoordinatorFuture();\n+            super.onFailure(e, future);\n+        }\n     }\n \n     /**\n@@ -820,7 +819,6 @@ private RuntimeException failureCause() {\n         @Override\n         public void run() {\n             try {\n-                RequestFuture findCoordinatorFuture = null;\n \n                 while (true) {\n                     synchronized (AbstractCoordinator.this) {\n@@ -843,8 +841,8 @@ public void run() {\n                         long now = time.milliseconds();\n \n                         if (coordinatorUnknown()) {\n-                            if (findCoordinatorFuture == null || findCoordinatorFuture.isDone())\n-                                findCoordinatorFuture = lookupCoordinator();\n+                            if (findCoordinatorFuture == null)\n+                                lookupCoordinator();\n                             else\n                                 AbstractCoordinator.this.wait(retryBackoffMs);\n                         } else if (heartbeat.sessionTimeoutExpired(now)) {",
                "raw_url": "https://github.com/apache/kafka/raw/7b16b4731666ff321fbe46828d526872ff5f56d7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java",
                "sha": "f2e15ca6f943dfca017b4dce72ee221a85f55ee4",
                "status": "modified"
            }
        ],
        "message": "KAFKA-4066; Fix NPE in consumer due to multi-threaded updates\n\nAuthor: Rajini Sivaram <rajinisivaram@googlemail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1763 from rajinisivaram/KAFKA-4066",
        "parent": "https://github.com/apache/kafka/commit/6ed3e6b1cb8a73b1f5f78926ccb247a8953a554c",
        "repo": "kafka",
        "unit_tests": [
            "AbstractCoordinatorTest.java"
        ]
    },
    "kafka_7cef37c": {
        "bug_id": "kafka_7cef37c",
        "commit": "https://github.com/apache/kafka/commit/7cef37cf55353f542db3562157547d26c992e782",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java?ref=7cef37cf55353f542db3562157547d26c992e782",
                "deletions": 0,
                "filename": "clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java",
                "patch": "@@ -24,6 +24,10 @@\n  * A simple immutable value object class holding customizable SASL extensions\n  */\n public class SaslExtensions {\n+    /**\n+     * An \"empty\" instance indicating no SASL extensions\n+     */\n+    public static final SaslExtensions NO_SASL_EXTENSIONS = new SaslExtensions(Collections.emptyMap());\n     private final Map<String, String> extensionsMap;\n \n     public SaslExtensions(Map<String, String> extensionsMap) {",
                "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java",
                "sha": "c129f1ec400f7e1c7637c5a9ce30310c2d0f380d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java?ref=7cef37cf55353f542db3562157547d26c992e782",
                "deletions": 4,
                "filename": "clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java",
                "patch": "@@ -17,27 +17,35 @@\n \n package org.apache.kafka.common.security.auth;\n \n+import java.util.Objects;\n+\n import javax.security.auth.callback.Callback;\n \n /**\n  * Optional callback used for SASL mechanisms if any extensions need to be set\n  * in the SASL exchange.\n  */\n public class SaslExtensionsCallback implements Callback {\n-    private SaslExtensions extensions;\n+    private SaslExtensions extensions = SaslExtensions.NO_SASL_EXTENSIONS;\n \n     /**\n-     * Returns a {@link SaslExtensions} consisting of the extension names and values that are sent by the client to\n-     * the server in the initial client SASL authentication message.\n+     * Returns always non-null {@link SaslExtensions} consisting of the extension\n+     * names and values that are sent by the client to the server in the initial\n+     * client SASL authentication message. The default value is\n+     * {@link SaslExtensions#NO_SASL_EXTENSIONS} so that if this callback is\n+     * unhandled the client will see a non-null value.\n      */\n     public SaslExtensions extensions() {\n         return extensions;\n     }\n \n     /**\n      * Sets the SASL extensions on this callback.\n+     * \n+     * @param extensions\n+     *            the mandatory extensions to set\n      */\n     public void extensions(SaslExtensions extensions) {\n-        this.extensions = extensions;\n+        this.extensions = Objects.requireNonNull(extensions, \"extensions must not be null\");\n     }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java",
                "sha": "c5bd449e0cc0804e9dea54813c02c8fc66829b9a",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java?ref=7cef37cf55353f542db3562157547d26c992e782",
                "deletions": 5,
                "filename": "clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java",
                "patch": "@@ -22,6 +22,7 @@\n import javax.security.sasl.SaslException;\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n@@ -58,7 +59,9 @@ public OAuthBearerClientInitialResponse(byte[] response) throws SaslException {\n         if (auth == null)\n             throw new SaslException(\"Invalid OAUTHBEARER client first message: 'auth' not specified\");\n         properties.remove(AUTH_KEY);\n-        this.saslExtensions = validateExtensions(new SaslExtensions(properties));\n+        SaslExtensions extensions = new SaslExtensions(properties);\n+        validateExtensions(extensions);\n+        this.saslExtensions = extensions;\n \n         Matcher authMatcher = AUTH_PATTERN.matcher(auth);\n         if (!authMatcher.matches())\n@@ -71,16 +74,48 @@ public OAuthBearerClientInitialResponse(byte[] response) throws SaslException {\n         this.tokenValue = authMatcher.group(\"token\");\n     }\n \n+    /**\n+     * Constructor\n+     * \n+     * @param tokenValue\n+     *            the mandatory token value\n+     * @param extensions\n+     *            the optional extensions\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     */\n     public OAuthBearerClientInitialResponse(String tokenValue, SaslExtensions extensions) throws SaslException {\n         this(tokenValue, \"\", extensions);\n     }\n \n+    /**\n+     * Constructor\n+     * \n+     * @param tokenValue\n+     *            the mandatory token value\n+     * @param authorizationId\n+     *            the optional authorization ID\n+     * @param extensions\n+     *            the optional extensions\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     */\n     public OAuthBearerClientInitialResponse(String tokenValue, String authorizationId, SaslExtensions extensions) throws SaslException {\n-        this.tokenValue = tokenValue;\n+        this.tokenValue = Objects.requireNonNull(tokenValue, \"token value must not be null\");\n         this.authorizationId = authorizationId == null ? \"\" : authorizationId;\n-        this.saslExtensions = validateExtensions(extensions);\n+        validateExtensions(extensions);\n+        this.saslExtensions = extensions != null ? extensions : SaslExtensions.NO_SASL_EXTENSIONS;\n     }\n \n+    /**\n+     * Return the always non-null extensions\n+     * \n+     * @return the always non-null extensions\n+     */\n     public SaslExtensions extensions() {\n         return saslExtensions;\n     }\n@@ -97,21 +132,40 @@ public SaslExtensions extensions() {\n         return message.getBytes(StandardCharsets.UTF_8);\n     }\n \n+    /**\n+     * Return the always non-null token value\n+     * \n+     * @return the always non-null toklen value\n+     */\n     public String tokenValue() {\n         return tokenValue;\n     }\n \n+    /**\n+     * Return the always non-null authorization ID\n+     * \n+     * @return the always non-null authorization ID\n+     */\n     public String authorizationId() {\n         return authorizationId;\n     }\n \n     /**\n      * Validates that the given extensions conform to the standard. They should also not contain the reserve key name {@link OAuthBearerClientInitialResponse#AUTH_KEY}\n      *\n+     * @param extensions\n+     *            optional extensions to validate\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     *\n      * @see <a href=\"https://tools.ietf.org/html/rfc7628#section-3.1\">RFC 7628,\n      *  Section 3.1</a>\n      */\n-    public static SaslExtensions validateExtensions(SaslExtensions extensions) throws SaslException {\n+    public static void validateExtensions(SaslExtensions extensions) throws SaslException {\n+        if (extensions == null)\n+            return;\n         if (extensions.map().containsKey(OAuthBearerClientInitialResponse.AUTH_KEY))\n             throw new SaslException(\"Extension name \" + OAuthBearerClientInitialResponse.AUTH_KEY + \" is invalid\");\n \n@@ -124,7 +178,6 @@ public static SaslExtensions validateExtensions(SaslExtensions extensions) throw\n             if (!EXTENSION_VALUE_PATTERN.matcher(extensionValue).matches())\n                 throw new SaslException(\"Extension value (\" + extensionValue + \") for extension \" + extensionName + \" is invalid\");\n         }\n-        return extensions;\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java",
                "sha": "a356f0da3ddb941f6473bffaaf781f51318046d4",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java?ref=7cef37cf55353f542db3562157547d26c992e782",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java",
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.kafka.common.security.oauthbearer.internals;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n import org.apache.kafka.common.security.auth.SaslExtensions;\n import org.junit.Test;\n@@ -99,4 +100,25 @@ public void testRfc7688Example() throws Exception {\n         assertEquals(\"server.example.com\", response.extensions().map().get(\"host\"));\n         assertEquals(\"143\", response.extensions().map().get(\"port\"));\n     }\n+\n+    @Test\n+    public void testNoExtensionsFromByteArray() throws Exception {\n+        String message = \"n,a=user@example.com,\\u0001\" +\n+                \"auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\\u0001\\u0001\";\n+        OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));\n+        assertEquals(\"vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\", response.tokenValue());\n+        assertEquals(\"user@example.com\", response.authorizationId());\n+        assertTrue(response.extensions().map().isEmpty());\n+    }\n+\n+    @Test\n+    public void testNoExtensionsFromTokenAndNullExtensions() throws Exception {\n+        OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(\"token\", null);\n+        assertTrue(response.extensions().map().isEmpty());\n+    }\n+\n+    @Test\n+    public void testValidateNullExtensions() throws Exception {\n+        OAuthBearerClientInitialResponse.validateExtensions(null);\n+    }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java",
                "sha": "0ba956561dfd52d59e3685fdce94b359c99d8e94",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java?ref=7cef37cf55353f542db3562157547d26c992e782",
                "deletions": 4,
                "filename": "clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java",
                "patch": "@@ -20,8 +20,8 @@\n import org.apache.kafka.common.security.auth.SaslExtensionsCallback;\n import org.apache.kafka.common.security.auth.AuthenticateCallbackHandler;\n import org.apache.kafka.common.security.auth.SaslExtensions;\n+import org.apache.kafka.common.security.oauthbearer.OAuthBearerToken;\n import org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback;\n-import org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredJws;\n import org.easymock.EasyMockSupport;\n import org.junit.Test;\n \n@@ -30,9 +30,11 @@\n import javax.security.auth.login.AppConfigurationEntry;\n import javax.security.sasl.SaslException;\n import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.fail;\n@@ -70,7 +72,32 @@ public void configure(Map<String, ?> configs, String saslMechanism, List<AppConf\n         public void handle(Callback[] callbacks) throws UnsupportedCallbackException {\n             for (Callback callback : callbacks) {\n                 if (callback instanceof OAuthBearerTokenCallback)\n-                    ((OAuthBearerTokenCallback) callback).token(createMock(OAuthBearerUnsecuredJws.class));\n+                    ((OAuthBearerTokenCallback) callback).token(new OAuthBearerToken() {\n+                        @Override\n+                        public String value() {\n+                            return \"\";\n+                        }\n+\n+                        @Override\n+                        public Set<String> scope() {\n+                            return Collections.emptySet();\n+                        }\n+\n+                        @Override\n+                        public long lifetimeMs() {\n+                            return 100;\n+                        }\n+\n+                        @Override\n+                        public String principalName() {\n+                            return \"principalName\";\n+                        }\n+\n+                        @Override\n+                        public Long startTimeMs() {\n+                            return null;\n+                        }\n+                    });\n                 else if (callback instanceof SaslExtensionsCallback) {\n                     if (toThrow)\n                         throw new ConfigException(errorMessage);\n@@ -88,7 +115,7 @@ public void close() {\n \n     @Test\n     public void testAttachesExtensionsToFirstClientMessage() throws Exception {\n-        String expectedToken = new String(new OAuthBearerClientInitialResponse(null, testExtensions).toBytes(), StandardCharsets.UTF_8);\n+        String expectedToken = new String(new OAuthBearerClientInitialResponse(\"\", testExtensions).toBytes(), StandardCharsets.UTF_8);\n \n         OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));\n \n@@ -101,7 +128,7 @@ public void testAttachesExtensionsToFirstClientMessage() throws Exception {\n     public void testNoExtensionsDoesNotAttachAnythingToFirstClientMessage() throws Exception {\n         TEST_PROPERTIES.clear();\n         testExtensions = new SaslExtensions(TEST_PROPERTIES);\n-        String expectedToken = new String(new OAuthBearerClientInitialResponse(null, new SaslExtensions(TEST_PROPERTIES)).toBytes(), StandardCharsets.UTF_8);\n+        String expectedToken = new String(new OAuthBearerClientInitialResponse(\"\", new SaslExtensions(TEST_PROPERTIES)).toBytes(), StandardCharsets.UTF_8);\n         OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));\n \n         String message = new String(client.evaluateChallenge(\"\".getBytes()), StandardCharsets.UTF_8);",
                "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java",
                "sha": "fad743136f33b556014a75d38514e33ea3e3f08e",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)\n\nSet empty extensions if null is passed in.\r\n\r\nReviewers: Satish Duggana <sduggana@hortonworks.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",
        "parent": "https://github.com/apache/kafka/commit/9e23af3115ddfbf311f6b37c273327b3fa8e9a14",
        "repo": "kafka",
        "unit_tests": [
            "SaslExtensionsTest.java",
            "OAuthBearerClientInitialResponseTest.java"
        ]
    },
    "kafka_80d78f8": {
        "bug_id": "kafka_80d78f8",
        "commit": "https://github.com/apache/kafka/commit/80d78f81470f109dc6d221f755b039c7332bb93b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/80d78f81470f109dc6d221f755b039c7332bb93b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java?ref=80d78f81470f109dc6d221f755b039c7332bb93b",
                "deletions": 2,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java",
                "patch": "@@ -245,7 +245,7 @@ public void close() {\n     public void put(K key, V value) {\n         byte[] rawKey = putAndReturnInternalKey(key, value, USE_CURRENT_TIMESTAMP);\n \n-        if (loggingEnabled) {\n+        if (rawKey != null && loggingEnabled) {\n             changeLogger.add(rawKey);\n             changeLogger.maybeLogChange(this.getter);\n         }\n@@ -255,7 +255,7 @@ public void put(K key, V value) {\n     public void put(K key, V value, long timestamp) {\n         byte[] rawKey = putAndReturnInternalKey(key, value, timestamp);\n \n-        if (loggingEnabled) {\n+        if (rawKey != null && loggingEnabled) {\n             changeLogger.add(rawKey);\n             changeLogger.maybeLogChange(this.getter);\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/80d78f81470f109dc6d221f755b039c7332bb93b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java",
                "sha": "9851c0489b8864932d975e81656758ce4a2dd985",
                "status": "modified"
            }
        ],
        "message": "HOTFIX: fix NPE in changelogger\n\nFix NPE in StoreChangeLogger caused by a record out of window retention period.\nguozhangwang\n\nAuthor: Yasuhiro Matsuda <yasuhiro@confluent.io>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1124 from ymatsuda/logger_npe",
        "parent": "https://github.com/apache/kafka/commit/d4d5920ed40736d21f056188efa8a86c93e22506",
        "repo": "kafka",
        "unit_tests": [
            "RocksDBWindowStoreTest.java"
        ]
    },
    "kafka_8429db9": {
        "bug_id": "kafka_8429db9",
        "commit": "https://github.com/apache/kafka/commit/8429db937e2134d9935d9dccd2ed0febc474fd66",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66",
                "deletions": 9,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java",
                "patch": "@@ -45,14 +45,6 @@ public String name() {\n         return \"range\";\n     }\n \n-    private List<TopicPartition> partitions(String topic,\n-                                            int numPartitions) {\n-        List<TopicPartition> partitions = new ArrayList<>();\n-        for (int i = 0; i < numPartitions; i++)\n-            partitions.add(new TopicPartition(topic, i));\n-        return partitions;\n-    }\n-\n     private Map<String, List<String>> consumersPerTopic(Map<String, List<String>> consumerMetadata) {\n         Map<String, List<String>> res = new HashMap<>();\n         for (Map.Entry<String, List<String>> subscriptionEntry : consumerMetadata.entrySet()) {\n@@ -84,7 +76,7 @@ public String name() {\n             int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();\n             int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();\n \n-            List<TopicPartition> partitions = partitions(topic, numPartitionsForTopic);\n+            List<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);\n             for (int i = 0, n = consumersForTopic.size(); i < n; i++) {\n                 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);\n                 int length = numPartitionsPerConsumer + (i + 1 > consumersWithExtraPartition ? 0 : 1);",
                "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java",
                "sha": "16c1d77c429a7147456d0bb572414fd85f7b8390",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66",
                "deletions": 4,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java",
                "patch": "@@ -65,10 +65,9 @@\n \n         List<TopicPartition> allPartitions = new ArrayList<>();\n         for (String topic : topics) {\n-            Integer partitions = partitionsPerTopic.get(topic);\n-            for (int partition = 0; partition < partitions; partition++) {\n-                allPartitions.add(new TopicPartition(topic, partition));\n-            }\n+            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);\n+            if (numPartitionsForTopic != null)\n+                allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));\n         }\n         return allPartitions;\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java",
                "sha": "a5de595cd361c5b6a48f7dc1f817b4d288d487d3",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66",
                "deletions": 3,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java",
                "patch": "@@ -33,9 +33,10 @@\n \n     /**\n      * Perform the group assignment given the partition counts and member subscriptions\n-     * @param partitionsPerTopic The number of partitions for each subscribed topic (may be empty for some topics)\n+     * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded\n+     *                           from this map.\n      * @param subscriptions Map from the memberId to their respective topic subscription\n-     * @return Map from each member to the\n+     * @return Map from each member to the list of partitions assigned to them.\n      */\n     public abstract Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                              Map<String, List<String>> subscriptions);\n@@ -58,7 +59,7 @@ public Subscription subscription(Set<String> topics) {\n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n         for (String topic : allSubscribedTopics) {\n             Integer numPartitions = metadata.partitionCountForTopic(topic);\n-            if (numPartitions != null)\n+            if (numPartitions != null && numPartitions > 0)\n                 partitionsPerTopic.put(topic, numPartitions);\n             else\n                 log.debug(\"Skipping assignment for topic {} since no metadata is available\", topic);\n@@ -87,4 +88,10 @@ public void onAssignment(Assignment assignment) {\n         list.add(value);\n     }\n \n+    protected static List<TopicPartition> partitions(String topic, int numPartitions) {\n+        List<TopicPartition> partitions = new ArrayList<>(numPartitions);\n+        for (int i = 0; i < numPartitions; i++)\n+            partitions.add(new TopicPartition(topic, i));\n+        return partitions;\n+    }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java",
                "sha": "4f90e66f2794ca9e7ab84e9a1dc523789ead4f40",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66",
                "deletions": 2,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java",
                "patch": "@@ -53,8 +53,6 @@ public void testOneConsumerNonexistentTopic() {\n         String consumerId = \"consumer\";\n \n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n-        partitionsPerTopic.put(topic, 0);\n-\n         Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic,\n                 Collections.singletonMap(consumerId, Arrays.asList(topic)));\n         assertEquals(Collections.singleton(consumerId), assignment.keySet());",
                "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java",
                "sha": "72febb02ca6efad00d7fe5fc9d8d49638af540d1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66",
                "deletions": 2,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java",
                "patch": "@@ -47,8 +47,6 @@ public void testOneConsumerNonexistentTopic() {\n         String consumerId = \"consumer\";\n \n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n-        partitionsPerTopic.put(topic, 0);\n-\n         Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic,\n                 Collections.singletonMap(consumerId, Arrays.asList(topic)));\n ",
                "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java",
                "sha": "1d62700e5cbe1d43f4c8dcb4070ca42a8f8f8bc5",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3661; fix NPE in o.a.k.c.c.RoundRobinAssignor when topic metadata not found\n\nAbstractPartitionAssignor.assign has an ambiguous line in its documentation:\n> param partitionsPerTopic The number of partitions for each subscribed topic (may be empty for some topics)\n\nDoes empty mean the topic has an entry with value zero, or that the entry is excluded from the map altogether? The current implementation in AbstractPartitionAssignor excludes the entry from partitionsPerTopic if the topic isn't in the metadata.\n\nRoundRobinAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value.\nRangeAssignor interprets emptiness as excluding the entry from the map.\nRangeAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value.\n\nThis implementation chooses to solve the NPE by deciding to exclude topics from partitionsPerTopic when the topic is not in the metadata.\n\nAuthor: Onur Karaman <okaraman@linkedin.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1326 from onurkaraman/KAFKA-3661",
        "parent": "https://github.com/apache/kafka/commit/7f4e3ccde820eedd962b4cfd3abaecd8a49b83a8",
        "repo": "kafka",
        "unit_tests": [
            "RangeAssignorTest.java",
            "RoundRobinAssignorTest.java",
            "AbstractPartitionAssignorTest.java"
        ]
    },
    "kafka_87d493f": {
        "bug_id": "kafka_87d493f",
        "commit": "https://github.com/apache/kafka/commit/87d493f07219a215816783557a5981a74f192f06",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java?ref=87d493f07219a215816783557a5981a74f192f06",
                "deletions": 5,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java",
                "patch": "@@ -27,6 +27,11 @@\n     private static final RecordConverter RAW_TO_TIMESTAMED_INSTANCE = record -> {\n         final byte[] rawValue = record.value();\n         final long timestamp = record.timestamp();\n+        final byte[] recordValue = rawValue == null ? null :\n+            ByteBuffer.allocate(8 + rawValue.length)\n+                .putLong(timestamp)\n+                .put(rawValue)\n+                .array();\n         return new ConsumerRecord<>(\n             record.topic(),\n             record.partition(),\n@@ -37,11 +42,7 @@\n             record.serializedKeySize(),\n             record.serializedValueSize(),\n             record.key(),\n-            ByteBuffer\n-                .allocate(8 + rawValue.length)\n-                .putLong(timestamp)\n-                .put(rawValue)\n-                .array(),\n+            recordValue,\n             record.headers(),\n             record.leaderEpoch()\n         );",
                "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java",
                "sha": "8305d5216afc386d5d4f16854c23fee5250c4831",
                "status": "modified"
            },
            {
                "additions": 122,
                "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java?ref=87d493f07219a215816783557a5981a74f192f06",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java",
                "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import kafka.utils.MockTime;\n+import org.apache.kafka.common.serialization.BytesDeserializer;\n+import org.apache.kafka.common.serialization.BytesSerializer;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+@Category({IntegrationTest.class})\n+public class StateRestorationIntegrationTest {\n+    private StreamsBuilder builder = new StreamsBuilder();\n+\n+    private static final String APPLICATION_ID = \"restoration-test-app\";\n+    private static final String STATE_STORE_NAME = \"stateStore\";\n+    private static final String INPUT_TOPIC = \"input\";\n+    private static final String OUTPUT_TOPIC = \"output\";\n+\n+    private Properties streamsConfiguration;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+    private final MockTime mockTime = CLUSTER.time;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        final Properties props = new Properties();\n+\n+        streamsConfiguration = StreamsTestUtils.getStreamsConfig(\n+                APPLICATION_ID,\n+                CLUSTER.bootstrapServers(),\n+                Serdes.Integer().getClass().getName(),\n+                Serdes.ByteArray().getClass().getName(),\n+                props);\n+\n+        CLUSTER.createTopics(INPUT_TOPIC);\n+        CLUSTER.createTopics(OUTPUT_TOPIC);\n+\n+        IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n+    }\n+\n+    @Test\n+    public void shouldRestoreNullRecord() throws InterruptedException, ExecutionException {\n+        builder.table(INPUT_TOPIC, Materialized.<Integer, Bytes>as(\n+                Stores.persistentTimestampedKeyValueStore(STATE_STORE_NAME))\n+                .withKeySerde(Serdes.Integer())\n+                .withValueSerde(Serdes.Bytes())\n+                .withCachingDisabled()).toStream().to(OUTPUT_TOPIC);\n+\n+        final Properties producerConfig = TestUtils.producerConfig(\n+                CLUSTER.bootstrapServers(), IntegerSerializer.class, BytesSerializer.class);\n+\n+        final List<KeyValue<Integer, Bytes>> initialKeyValues = Arrays.asList(\n+                KeyValue.pair(3, new Bytes(new byte[]{3})),\n+                KeyValue.pair(3, null),\n+                KeyValue.pair(1, new Bytes(new byte[]{1})));\n+\n+        IntegrationTestUtils.produceKeyValuesSynchronously(\n+                INPUT_TOPIC, initialKeyValues, producerConfig, mockTime);\n+\n+        KafkaStreams streams = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration);\n+        streams.start();\n+\n+        final Properties consumerConfig = TestUtils.consumerConfig(\n+                CLUSTER.bootstrapServers(), IntegerDeserializer.class, BytesDeserializer.class);\n+\n+        IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(\n+                consumerConfig, OUTPUT_TOPIC, initialKeyValues);\n+\n+        // wipe out state store to trigger restore process on restart\n+        streams.close();\n+        streams.cleanUp();\n+\n+        // Restart the stream instance. There should not be exception handling the null value within changelog topic.\n+        final List<KeyValue<Integer, Bytes>> newKeyValues =\n+                Collections.singletonList(KeyValue.pair(2, new Bytes(new byte[3])));\n+        IntegrationTestUtils.produceKeyValuesSynchronously(\n+                INPUT_TOPIC, newKeyValues, producerConfig, mockTime);\n+        streams = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration);\n+        streams.start();\n+        IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(\n+                consumerConfig, OUTPUT_TOPIC, newKeyValues);\n+        streams.close();\n+    }\n+}",
                "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java",
                "sha": "e22ff4f76597d1f410ab75a312f92f7c315877d6",
                "status": "added"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java?ref=87d493f07219a215816783557a5981a74f192f06",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java",
                "patch": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.junit.Test;\n+\n+import java.nio.ByteBuffer;\n+\n+import static org.apache.kafka.streams.state.internals.RecordConverters.rawValueToTimestampedValue;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertNull;\n+\n+public class RecordConvertersTest {\n+\n+    private final RecordConverter timestampedValueConverter = rawValueToTimestampedValue();\n+\n+    @Test\n+    public void shouldPreserveNullValueOnConversion() {\n+        final ConsumerRecord<byte[], byte[]> nullValueRecord = new ConsumerRecord<>(\"\", 0, 0L, new byte[0], null);\n+        assertNull(timestampedValueConverter.convert(nullValueRecord).value());\n+    }\n+\n+    @Test\n+    public void shouldAddTimestampToValueOnConversionWhenValueIsNotNull() {\n+        final long timestamp = 10L;\n+        final byte[] value = new byte[1];\n+        final ConsumerRecord<byte[], byte[]> inputRecord = new ConsumerRecord<>(\n+                \"topic\", 1, 0, timestamp, TimestampType.CREATE_TIME, 0L, 0, 0, new byte[0], value);\n+        final byte[] expectedValue = ByteBuffer.allocate(9).putLong(timestamp).put(value).array();\n+        final byte[] actualValue = timestampedValueConverter.convert(inputRecord).value();\n+        assertArrayEquals(expectedValue, actualValue);\n+    }\n+}",
                "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java",
                "sha": "bacbacd2fed01a49fd2f3b9f140872121edecc95",
                "status": "added"
            }
        ],
        "message": "KAFKA-8446: Kafka Streams restoration crashes with NPE when the record value is null (#6842)\n\nWhen the restored record value is null, we are in danger of NPE during restoration phase.\r\n\r\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/529a5a502e5fc4d05536d23df0bd75973affff01",
        "repo": "kafka",
        "unit_tests": [
            "RecordConvertersTest.java"
        ]
    },
    "kafka_8f90fd6": {
        "bug_id": "kafka_8f90fd6",
        "commit": "https://github.com/apache/kafka/commit/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java?ref=8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "patch": "@@ -636,7 +636,8 @@ private void failBatch(ProducerBatch batch, long baseOffset, long logAppendTime,\n      */\n     private boolean canRetry(ProducerBatch batch, ProduceResponse.PartitionResponse response) {\n         return batch.attempts() < this.retries &&\n-                ((response.error.exception() instanceof RetriableException) || transactionManager.canRetry(response, batch));\n+                ((response.error.exception() instanceof RetriableException) ||\n+                        (transactionManager != null && transactionManager.canRetry(response, batch)));\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/kafka/raw/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
                "sha": "45a29197b4c7ec3b801e1855ea98a25d5d55147b",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/kafka/blob/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java?ref=8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.kafka.common.errors.OutOfOrderSequenceException;\n import org.apache.kafka.common.errors.RecordTooLargeException;\n import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n import org.apache.kafka.common.errors.UnsupportedForMessageFormatException;\n import org.apache.kafka.common.errors.UnsupportedVersionException;\n import org.apache.kafka.common.internals.ClusterResourceListeners;\n@@ -500,6 +501,35 @@ public void testClusterAuthorizationExceptionInInitProducerIdRequest() throws Ex\n         assertSendFailure(ClusterAuthorizationException.class);\n     }\n \n+    @Test\n+    public void testCanRetryWithoutIdempotence() throws Exception {\n+        // do a successful retry\n+        Future<RecordMetadata> future = accumulator.append(tp0, 0L, \"key\".getBytes(), \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds()); // connect\n+        sender.run(time.milliseconds()); // send produce request\n+        String id = client.requests().peek().destination();\n+        Node node = new Node(Integer.parseInt(id), \"localhost\", 0);\n+        assertEquals(1, client.inFlightRequestCount());\n+        assertTrue(client.hasInFlightRequests());\n+        assertTrue(\"Client ready status should be true\", client.isReady(node, 0L));\n+        assertFalse(future.isDone());\n+\n+        client.respond(new MockClient.RequestMatcher() {\n+            @Override\n+            public boolean matches(AbstractRequest body) {\n+                ProduceRequest request = (ProduceRequest) body;\n+                assertFalse(request.isIdempotent());\n+                return true;\n+            }\n+        }, produceResponse(tp0, -1L, Errors.TOPIC_AUTHORIZATION_FAILED, 0));\n+        sender.run(time.milliseconds());\n+        assertTrue(future.isDone());\n+        try {\n+            future.get();\n+        } catch (Exception e) {\n+            assertTrue(e.getCause() instanceof TopicAuthorizationException);\n+        }\n+    }\n \n     @Test\n     public void testIdempotenceWithMultipleInflights() throws Exception {",
                "raw_url": "https://github.com/apache/kafka/raw/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java",
                "sha": "ecf77aa794052a35608c31897c262416636dd5c4",
                "status": "modified"
            }
        ],
        "message": "KAFKA-5959; Fix NPE in Sender.canRetry when idempotence is not enabled\n\nAuthor: Apurva Mehta <apurva@confluent.io>\n\nReviewers: tedyu <yuzhihong@gmail.com>, Jason Gustafson <jason@confluent.io>\n\nCloses #3947 from apurvam/KAFKA-5959-npe-in-sender",
        "parent": "https://github.com/apache/kafka/commit/d60f011d77ce80a44b02d43bf0889a50a8797dcd",
        "repo": "kafka",
        "unit_tests": [
            "SenderTest.java"
        ]
    },
    "kafka_9f94a77": {
        "bug_id": "kafka_9f94a77",
        "commit": "https://github.com/apache/kafka/commit/9f94a7752a590c36186a7eb1eee16e992ec68c97",
        "file": [
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/kafka/blob/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java?ref=9f94a7752a590c36186a7eb1eee16e992ec68c97",
                "deletions": 7,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "patch": "@@ -790,15 +790,22 @@ private KafkaConsumer(ConsumerConfig config,\n      * @param topics The list of topics to subscribe to\n      * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the\n      *                 subscribed topics\n+     * @throws IllegalArgumentException If topics is null or contains null or empty elements\n      */\n     @Override\n     public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener) {\n         acquire();\n         try {\n-            if (topics.isEmpty()) {\n+            if (topics == null) {\n+                throw new IllegalArgumentException(\"Topic collection to subscribe to cannot be null\");\n+            } else if (topics.isEmpty()) {\n                 // treat subscribing to empty topic list as the same as unsubscribing\n                 this.unsubscribe();\n             } else {\n+                for (String topic : topics) {\n+                    if (topic == null || topic.trim().isEmpty())\n+                        throw new IllegalArgumentException(\"Topic collection to subscribe to cannot contain null or empty topic\");\n+                }\n                 log.debug(\"Subscribed to topic(s): {}\", Utils.join(topics, \", \"));\n                 this.subscriptions.subscribe(topics, listener);\n                 metadata.setTopics(subscriptions.groupSubscription());\n@@ -824,6 +831,7 @@ public void subscribe(Collection<String> topics, ConsumerRebalanceListener liste\n      * management since the listener gives you an opportunity to commit offsets before a rebalance finishes.\n      *\n      * @param topics The list of topics to subscribe to\n+     * @throws IllegalArgumentException If topics is null or contains null or empty elements\n      */\n     @Override\n     public void subscribe(Collection<String> topics) {\n@@ -833,6 +841,7 @@ public void subscribe(Collection<String> topics) {\n     /**\n      * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. The pattern matching will be done periodically against topics\n      * existing at the time of check.\n+     *\n      * <p>\n      * As part of group management, the consumer will keep track of the list of consumers that\n      * belong to a particular group and will trigger a rebalance operation if one of the\n@@ -845,11 +854,14 @@ public void subscribe(Collection<String> topics) {\n      * </ul>\n      *\n      * @param pattern Pattern to subscribe to\n+     * @throws IllegalArgumentException If pattern is null\n      */\n     @Override\n     public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) {\n         acquire();\n         try {\n+            if (pattern == null)\n+                throw new IllegalArgumentException(\"Topic pattern to subscribe to cannot be null\");\n             log.debug(\"Subscribed to pattern: {}\", pattern);\n             this.subscriptions.subscribe(pattern, listener);\n             this.metadata.needMetadataForAllTopics(true);\n@@ -878,24 +890,39 @@ public void unsubscribe() {\n     /**\n      * Manually assign a list of partition to this consumer. This interface does not allow for incremental assignment\n      * and will replace the previous assignment (if there is one).\n+     *\n+     * If the given list of topic partition is empty, it is treated the same as {@link #unsubscribe()}.\n+     *\n      * <p>\n      * Manual topic assignment through this method does not use the consumer's group management\n      * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic\n      * metadata change. Note that it is not possible to use both manual partition assignment with {@link #assign(Collection)}\n      * and group assignment with {@link #subscribe(Collection, ConsumerRebalanceListener)}.\n      *\n      * @param partitions The list of partitions to assign this consumer\n+     * @throws IllegalArgumentException If partitions is null or contains null or empty topics\n      */\n     @Override\n     public void assign(Collection<TopicPartition> partitions) {\n         acquire();\n         try {\n-            log.debug(\"Subscribed to partition(s): {}\", Utils.join(partitions, \", \"));\n-            this.subscriptions.assignFromUser(partitions);\n-            Set<String> topics = new HashSet<>();\n-            for (TopicPartition tp : partitions)\n-                topics.add(tp.topic());\n-            metadata.setTopics(topics);\n+            if (partitions == null) {\n+                throw new IllegalArgumentException(\"Topic partition collection to assign to cannot be null\");\n+            } else if (partitions.isEmpty()) {\n+                this.unsubscribe();\n+            } else {\n+                Set<String> topics = new HashSet<>();\n+                for (TopicPartition tp : partitions) {\n+                    String topic = (tp != null) ? tp.topic() : null;\n+                    if (topic == null || topic.trim().isEmpty())\n+                        throw new IllegalArgumentException(\"Topic partitions to assign to cannot have null or empty topic\");\n+                    topics.add(topic);\n+                }\n+\n+                log.debug(\"Subscribed to partition(s): {}\", Utils.join(partitions, \", \"));\n+                this.subscriptions.assignFromUser(partitions);\n+                metadata.setTopics(topics);\n+            }\n         } finally {\n             release();\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
                "sha": "ff94dc81e80ff60b6e9adfaf1502883d9ca05e99",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/kafka/blob/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
                "changes": 102,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java?ref=9f94a7752a590c36186a7eb1eee16e992ec68c97",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient;\n import org.apache.kafka.clients.consumer.internals.ConsumerProtocol;\n import org.apache.kafka.clients.consumer.internals.Fetcher;\n+import org.apache.kafka.clients.consumer.internals.NoOpConsumerRebalanceListener;\n import org.apache.kafka.clients.consumer.internals.PartitionAssignor;\n import org.apache.kafka.clients.consumer.internals.SubscriptionState;\n import org.apache.kafka.common.Cluster;\n@@ -65,6 +66,7 @@\n import java.util.Map;\n import java.util.Properties;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n \n import static java.util.Collections.singleton;\n import static org.junit.Assert.assertEquals;\n@@ -147,6 +149,53 @@ public void testSubscription() {\n         consumer.close();\n     }\n \n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullTopicCollection() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+\n+        try {\n+            consumer.subscribe(null);\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullTopic() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        String nullTopic = null;\n+\n+        try {\n+            consumer.subscribe(Collections.singletonList(nullTopic));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnEmptyTopic() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        String emptyTopic = \"  \";\n+\n+        try {\n+            consumer.subscribe(Collections.singletonList(emptyTopic));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullPattern() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        Pattern pattern = null;\n+\n+        try {\n+            consumer.subscribe(pattern, new NoOpConsumerRebalanceListener());\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n     @Test(expected = IllegalArgumentException.class)\n     public void testSeekNegative() {\n         Properties props = new Properties();\n@@ -162,6 +211,59 @@ public void testSeekNegative() {\n         }\n     }\n \n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnNullTopicPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnNullTopicPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(null);\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test\n+    public void testAssignOnEmptyTopicPartition() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+\n+        consumer.assign(Collections.<TopicPartition>emptyList());\n+        assertTrue(consumer.subscription().isEmpty());\n+        assertTrue(consumer.assignment().isEmpty());\n+\n+        consumer.close();\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnNullTopicInPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnNullTopicInPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(Arrays.asList(new TopicPartition(null, 0)));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnEmptyTopicInPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnEmptyTopicInPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(Arrays.asList(new TopicPartition(\"  \", 0)));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n     @Test\n     public void testInterceptorConstructorClose() throws Exception {\n         try {",
                "raw_url": "https://github.com/apache/kafka/raw/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
                "sha": "b5a5fcab6aa26c40fecbb80c65258134a1cf1771",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3905: Handling null/empty topics and collections, patterns when subscription with list of topics or with patterns, and with assignments.\n\n- Added validity checks for input parameters on subscribe, assign to avoid NPE, and provide an argument exception instead\n- Updated behavior for subscription with null collection to be same as when subscription with emptyList.i.e., unsubscribes.\n- Added tests on subscription, assign\n\nAuthor: Rekha Joshi <rekhajoshm@gmail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1601 from rekhajoshm/KAFKA-3905-1",
        "parent": "https://github.com/apache/kafka/commit/f17790cd9952aecef67fbec58122c55c72e5c2b2",
        "repo": "kafka",
        "unit_tests": [
            "KafkaConsumerTest.java"
        ]
    },
    "kafka_a1f7925": {
        "bug_id": "kafka_a1f7925",
        "commit": "https://github.com/apache/kafka/commit/a1f7925d23be0b81cb77561d2113443df52c6f74",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java?ref=a1f7925d23be0b81cb77561d2113443df52c6f74",
                "deletions": 2,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java",
                "patch": "@@ -219,13 +219,13 @@\n         for (Entry<String, Subscription> entry: subscriptions.entrySet()) {\n             String consumer = entry.getKey();\n             consumer2AllPotentialPartitions.put(consumer, new ArrayList<TopicPartition>());\n-            for (String topic: entry.getValue().topics()) {\n+            entry.getValue().topics().stream().filter(topic -> partitionsPerTopic.get(topic) != null).forEach(topic -> {\n                 for (int i = 0; i < partitionsPerTopic.get(topic); ++i) {\n                     TopicPartition topicPartition = new TopicPartition(topic, i);\n                     consumer2AllPotentialPartitions.get(consumer).add(topicPartition);\n                     partition2AllPotentialConsumers.get(topicPartition).add(consumer);\n                 }\n-            }\n+            });\n \n             // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist\n             if (!currentAssignment.containsKey(consumer))\n@@ -705,6 +705,8 @@ static ByteBuffer serializeTopicPartitionAssignment(List<TopicPartition> partiti\n      */\n     private <T> boolean hasIdenticalListElements(Collection<List<T>> col) {\n         Iterator<List<T>> it = col.iterator();\n+        if (!it.hasNext())\n+            return true;\n         List<T> cur = it.next();\n         while (it.hasNext()) {\n             List<T> next = it.next();",
                "raw_url": "https://github.com/apache/kafka/raw/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java",
                "sha": "ee537eba78802f3c9c1ea4304d5a1dea557ff9b2",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/kafka/blob/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java?ref=a1f7925d23be0b81cb77561d2113443df52c6f74",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java",
                "patch": "@@ -606,6 +606,38 @@ public void testStickiness() {\n         }\n     }\n \n+    @Test\n+    public void testAssignmentUpdatedForDeletedTopic() {\n+        String consumerId = \"consumer\";\n+\n+        Map<String, Integer> partitionsPerTopic = new HashMap<>();\n+        partitionsPerTopic.put(\"topic01\", 1);\n+        partitionsPerTopic.put(\"topic03\", 100);\n+        Map<String, Subscription> subscriptions =\n+                Collections.singletonMap(consumerId, new Subscription(topics(\"topic01\", \"topic02\", \"topic03\")));\n+\n+        Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);\n+        assertEquals(assignment.values().stream().mapToInt(topicPartitions -> topicPartitions.size()).sum(), 1 + 100);\n+        assertEquals(Collections.singleton(consumerId), assignment.keySet());\n+        assertTrue(isFullyBalanced(assignment));\n+    }\n+\n+    @Test\n+    public void testNoExceptionThrownWhenOnlySubscribedTopicDeleted() {\n+        String topic = \"topic01\";\n+        String consumer = \"consumer01\";\n+        Map<String, Integer> partitionsPerTopic = new HashMap<>();\n+        partitionsPerTopic.put(topic, 3);\n+        Map<String, Subscription> subscriptions = new HashMap<>();\n+        subscriptions.put(consumer, new Subscription(topics(topic)));\n+        Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);\n+        subscriptions.put(consumer, new Subscription(topics(topic), StickyAssignor.serializeTopicPartitionAssignment(assignment.get(consumer))));\n+\n+        assignment = assignor.assign(Collections.emptyMap(), subscriptions);\n+        assertEquals(assignment.size(), 1);\n+        assertTrue(assignment.get(consumer).isEmpty());\n+    }\n+\n     private String getTopicName(int i, int maxNum) {\n         return getCanonicalName(\"t\", i, maxNum);\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java",
                "sha": "32ba16a482023cafc66a8fbf917b3bb268163300",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7962: Avoid NPE for StickyAssignor (#6308)\n\n* KAFKA-7962: StickyAssignor: throws NullPointerException during assignments if topic is deleted\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-7962\r\n\r\nConsumer using StickyAssignor throws NullPointerException if a subscribed topic was removed.\r\n\r\n* addressed vahidhashemian's comments\r\n\r\n* lower NPath Complexity\r\n\r\n* added a unit test",
        "parent": "https://github.com/apache/kafka/commit/f667f573ff12321f2913ccf17cdf9e9bcc97b550",
        "repo": "kafka",
        "unit_tests": [
            "StickyAssignorTest.java"
        ]
    },
    "kafka_a5c47db": {
        "bug_id": "kafka_a5c47db",
        "commit": "https://github.com/apache/kafka/commit/a5c47db1382c14720106ae1da20d2b332f89c22c",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c",
                "deletions": 21,
                "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "patch": "@@ -251,7 +251,7 @@ public void disconnect(String nodeId) {\n         }\n         connectionStates.disconnected(nodeId, now);\n         if (log.isDebugEnabled()) {\n-            log.debug(\"Manually disconnected from {}.  Removed requests: {}.\", nodeId,\n+            log.debug(\"Manually disconnected from {}. Removed requests: {}.\", nodeId,\n                 Utils.join(requestTypes, \", \"));\n         }\n     }\n@@ -360,8 +360,8 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n             if (versionInfo == null) {\n                 version = builder.desiredOrLatestVersion();\n                 if (discoverBrokerVersions && log.isTraceEnabled())\n-                    log.trace(\"No version information found when sending message of type {} to node {}. \" +\n-                            \"Assuming version {}.\", clientRequest.apiKey(), nodeId, version);\n+                    log.trace(\"No version information found when sending {} with correlation id {} to node {}. \" +\n+                            \"Assuming version {}.\", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);\n             } else {\n                 version = versionInfo.usableVersion(clientRequest.apiKey(), builder.desiredVersion());\n             }\n@@ -371,8 +371,8 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n         } catch (UnsupportedVersionException e) {\n             // If the version is not supported, skip sending the request over the wire.\n             // Instead, simply add it to the local queue of aborted requests.\n-            log.debug(\"Version mismatch when attempting to send {} to {}\",\n-                    clientRequest.toString(), clientRequest.destination(), e);\n+            log.debug(\"Version mismatch when attempting to send {} with correlation id {} to {}\", builder,\n+                    clientRequest.correlationId(), clientRequest.destination(), e);\n             ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.desiredOrLatestVersion()),\n                     clientRequest.callback(), clientRequest.destination(), now, now,\n                     false, e, null);\n@@ -386,10 +386,11 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n         if (log.isDebugEnabled()) {\n             int latestClientVersion = clientRequest.apiKey().latestVersion();\n             if (header.apiVersion() == latestClientVersion) {\n-                log.trace(\"Sending {} {} to node {}.\", clientRequest.apiKey(), request, nodeId);\n+                log.trace(\"Sending {} {} with correlation id {} to node {}\", clientRequest.apiKey(), request,\n+                        clientRequest.correlationId(), nodeId);\n             } else {\n-                log.debug(\"Using older server API v{} to send {} {} to node {}.\",\n-                        header.apiVersion(), clientRequest.apiKey(), request, nodeId);\n+                log.debug(\"Using older server API v{} to send {} {} with correlation id {} to node {}\",\n+                        header.apiVersion(), clientRequest.apiKey(), request, clientRequest.correlationId(), nodeId);\n             }\n         }\n         Send send = request.toSend(nodeId, header);\n@@ -554,8 +555,8 @@ public static AbstractResponse parseResponse(ByteBuffer responseBuffer, RequestH\n     private static Struct parseStructMaybeUpdateThrottleTimeMetrics(ByteBuffer responseBuffer, RequestHeader requestHeader,\n                                                                     Sensor throttleTimeSensor, long now) {\n         ResponseHeader responseHeader = ResponseHeader.parse(responseBuffer);\n-        // Always expect the response version id to be the same as the request version id\n         ApiKeys apiKey = ApiKeys.forId(requestHeader.apiKey());\n+        // Always expect the response version id to be the same as the request version id\n         Struct responseBody = apiKey.parseResponse(requestHeader.apiVersion(), responseBuffer);\n         correlate(requestHeader, responseHeader);\n         if (throttleTimeSensor != null && responseBody.hasField(AbstractResponse.THROTTLE_TIME_KEY_NAME))\n@@ -591,7 +592,8 @@ private void processDisconnection(List<ClientResponse> responses, String nodeId,\n                 break; // Disconnections in other states are logged at debug level in Selector\n         }\n         for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n-            log.trace(\"Cancelled request {} due to node {} being disconnected\", request.request, nodeId);\n+            log.trace(\"Cancelled request {} with correlation id {} due to node {} being disconnected\", request.request,\n+                    request.header.correlationId(), nodeId);\n             if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA.id)\n                 metadataUpdater.handleDisconnection(request.destination);\n             else\n@@ -655,8 +657,8 @@ private void handleCompletedReceives(List<ClientResponse> responses, long now) {\n             Struct responseStruct = parseStructMaybeUpdateThrottleTimeMetrics(receive.payload(), req.header,\n                 throttleTimeSensor, now);\n             if (log.isTraceEnabled()) {\n-                log.trace(\"Completed receive from node {}, for key {}, received {}\", req.destination,\n-                    req.header.apiKey(), responseStruct.toString());\n+                log.trace(\"Completed receive from node {} for {} with correlation id {}, received {}\", req.destination,\n+                    ApiKeys.forId(req.header.apiKey()), req.header.correlationId(), responseStruct);\n             }\n             AbstractResponse body = createResponse(responseStruct, req.header);\n             if (req.isInternalRequest && body instanceof MetadataResponse)\n@@ -673,8 +675,8 @@ private void handleApiVersionsResponse(List<ClientResponse> responses,\n         final String node = req.destination;\n         if (apiVersionsResponse.error() != Errors.NONE) {\n             if (req.request.version() == 0 || apiVersionsResponse.error() != Errors.UNSUPPORTED_VERSION) {\n-                log.warn(\"Node {} got error {} when making an ApiVersionsRequest.  Disconnecting.\",\n-                        node, apiVersionsResponse.error());\n+                log.warn(\"Received error {} from node {} when making an ApiVersionsRequest with correlation id {}. Disconnecting.\",\n+                        apiVersionsResponse.error(), node, req.header.correlationId());\n                 this.selector.close(node);\n                 processDisconnection(responses, node, now, ChannelState.LOCAL_CLOSE);\n             } else {\n@@ -719,10 +721,10 @@ private void handleConnections() {\n             if (discoverBrokerVersions) {\n                 this.connectionStates.checkingApiVersions(node);\n                 nodesNeedingApiVersionsFetch.put(node, new ApiVersionsRequest.Builder());\n-                log.debug(\"Completed connection to node {}.  Fetching API versions.\", node);\n+                log.debug(\"Completed connection to node {}. Fetching API versions.\", node);\n             } else {\n                 this.connectionStates.ready(node);\n-                log.debug(\"Completed connection to node {}.  Ready.\", node);\n+                log.debug(\"Completed connection to node {}. Ready.\", node);\n             }\n         }\n     }\n@@ -757,7 +759,7 @@ private static void correlate(RequestHeader requestHeader, ResponseHeader respon\n     private void initiateConnect(Node node, long now) {\n         String nodeConnectionId = node.idString();\n         try {\n-            log.debug(\"Initiating connection to node {} at {}:{}.\", node.id(), node.host(), node.port());\n+            log.debug(\"Initiating connection to node {}\", node);\n             this.connectionStates.connecting(nodeConnectionId, now);\n             selector.connect(nodeConnectionId,\n                              new InetSocketAddress(node.host(), node.port()),\n@@ -768,7 +770,7 @@ private void initiateConnect(Node node, long now) {\n             connectionStates.disconnected(nodeConnectionId, now);\n             /* maybe the problem is our metadata, update it */\n             metadataUpdater.requestUpdate();\n-            log.debug(\"Error connecting to node {} at {}:{}:\", node.id(), node.host(), node.port(), e);\n+            log.debug(\"Error connecting to node {}\", node, e);\n         }\n     }\n \n@@ -828,7 +830,7 @@ public void handleDisconnection(String destination) {\n                 int nodeId = Integer.parseInt(destination);\n                 Node node = cluster.nodeById(nodeId);\n                 if (node != null)\n-                    log.warn(\"Bootstrap broker {}:{} disconnected\", node.host(), node.port());\n+                    log.warn(\"Bootstrap broker {} disconnected\", node);\n             }\n \n             metadataFetchInProgress = false;\n@@ -886,7 +888,7 @@ private long maybeUpdate(long now, Node node) {\n                             metadata.allowAutoTopicCreation());\n \n \n-                log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node.id());\n+                log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node);\n                 sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);\n                 return requestTimeoutMs;\n             }\n@@ -902,7 +904,7 @@ private long maybeUpdate(long now, Node node) {\n \n             if (connectionStates.canConnect(nodeConnectionId, now)) {\n                 // we don't have a connection to this node right now, make one\n-                log.debug(\"Initialize connection to node {} for sending metadata request\", node.id());\n+                log.debug(\"Initialize connection to node {} for sending metadata request\", node);\n                 initiateConnect(node, now);\n                 return reconnectBackoffMs;\n             }",
                "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
                "sha": "59c606f8fabd77000896ae2e57ee5176e746e8f0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java",
                "patch": "@@ -71,9 +71,10 @@ public OffsetFetchRequest build(short version) {\n         @Override\n         public String toString() {\n             StringBuilder bld = new StringBuilder();\n+            String partitionsString = partitions == null ? \"<ALL>\" : Utils.join(partitions, \",\");\n             bld.append(\"(type=OffsetFetchRequest, \").\n                     append(\"groupId=\").append(groupId).\n-                    append(\", partitions=\").append(Utils.join(partitions, \",\")).\n+                    append(\", partitions=\").append(partitionsString).\n                     append(\")\");\n             return bld.toString();\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java",
                "sha": "15fdf57dcf5c64278f67875d9ad011bc55a34f4d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
                "patch": "@@ -554,6 +554,16 @@ public void testJoinGroupRequestVersion0RebalanceTimeout() throws Exception {\n         assertEquals(jgr2.rebalanceTimeout(), jgr.rebalanceTimeout());\n     }\n \n+    @Test\n+    public void testOffsetFetchRequestBuilderToString() {\n+        String allTopicPartitionsString = OffsetFetchRequest.Builder.allTopicPartitions(\"someGroup\").toString();\n+        assertTrue(allTopicPartitionsString.contains(\"<ALL>\"));\n+        String string = new OffsetFetchRequest.Builder(\"group1\",\n+                singletonList(new TopicPartition(\"test11\", 1))).toString();\n+        assertTrue(string.contains(\"test11\"));\n+        assertTrue(string.contains(\"group1\"));\n+    }\n+\n     private RequestHeader createRequestHeader() {\n         return new RequestHeader((short) 10, (short) 1, \"\", 10);\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
                "sha": "a3c277f247c3ea535be450e0c0f9a7339bf948d0",
                "status": "modified"
            }
        ],
        "message": "KAFKA-5506; Fix NPE in OffsetFetchRequest.toString and logging improvements\n\nNetworkClient's logging improvements:\n- Include correlation id in a number of log statements\n- Avoid eager toString call in parameter passed to log.debug\n- Use node.toString instead of passing a subset of fields to the\nlogger\n- Use requestBuilder instead of clientRequest in one of the log\nstatements\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Damian Guy <damian.guy@gmail.com>, Jason Gustafson <jason@confluent.io>\n\nCloses #3420 from ijuma/kafka-5506-offset-fetch-request-to-string-npe",
        "parent": "https://github.com/apache/kafka/commit/ee5eac715d58e6b16a115692ede93ae481ae7785",
        "repo": "kafka",
        "unit_tests": [
            "NetworkClientTest.java",
            "OffsetFetchRequestTest.java"
        ]
    },
    "kafka_b46cb3b": {
        "bug_id": "kafka_b46cb3b",
        "commit": "https://github.com/apache/kafka/commit/b46cb3b2975afd8a3e82a0265c57760d8b9910da",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/kafka/blob/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/Metadata.java?ref=b46cb3b2975afd8a3e82a0265c57760d8b9910da",
                "deletions": 1,
                "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "patch": "@@ -14,10 +14,12 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Set;\n import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.errors.TimeoutException;\n import org.slf4j.Logger;\n@@ -237,11 +239,13 @@ public void removeListener(Listener listener) {\n \n     private Cluster getClusterForCurrentTopics(Cluster cluster) {\n         Collection<PartitionInfo> partitionInfos = new ArrayList<>();\n+        List<Node> nodes = Collections.emptyList();\n         if (cluster != null) {\n             for (String topic : this.topics) {\n                 partitionInfos.addAll(cluster.partitionsForTopic(topic));\n             }\n+            nodes = cluster.nodes();\n         }\n-        return new Cluster(cluster.nodes(), partitionInfos);\n+        return new Cluster(nodes, partitionInfos);\n     }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/main/java/org/apache/kafka/clients/Metadata.java",
                "sha": "f2fca12e09d527fd63aa4700e60a6e1aa56dfa03",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java?ref=b46cb3b2975afd8a3e82a0265c57760d8b9910da",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "patch": "@@ -111,6 +111,9 @@ public void testFailedUpdate() {\n         assertEquals(100, metadata.timeToNextUpdate(1100));\n         assertEquals(100, metadata.lastSuccessfulUpdate());\n \n+        metadata.needMetadataForAllTopics(true);\n+        metadata.update(null, time);\n+        assertEquals(100, metadata.timeToNextUpdate(1000));\n     }\n \n     @Test",
                "raw_url": "https://github.com/apache/kafka/raw/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java",
                "sha": "b7160a1996e8e7ed37dce0b6028f089751d7c234",
                "status": "modified"
            }
        ],
        "message": "KAFKA-2599: Fix Metadata.getClusterForCurrentTopics throws NPE\n\n\u2026h null checking\n\nAuthor: Edward Ribeiro <edward.ribeiro@gmail.com>\n\nReviewers: Ismael Juma, Guozhang Wang\n\nCloses #262 from eribeiro/KAFKA-2599",
        "parent": "https://github.com/apache/kafka/commit/726e23ef491637e03b0ba90c21992a639dcd158d",
        "repo": "kafka",
        "unit_tests": [
            "MetadataTest.java"
        ]
    },
    "kafka_b51002c": {
        "bug_id": "kafka_b51002c",
        "commit": "https://github.com/apache/kafka/commit/b51002c576ea9758132d75a8a0fe454e1bc270a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "patch": "@@ -207,6 +207,9 @@ public void writeAsText(String filePath, Serde<K> keySerde, Serde<V> valSerde) {\n     @Override\n     public void writeAsText(String filePath, String streamName, Serde<K> keySerde, Serde<V> valSerde) {\n         Objects.requireNonNull(filePath, \"filePath can't be null\");\n+        if (filePath.trim().isEmpty()) {\n+            throw new TopologyBuilderException(\"filePath can't be an empty string\");\n+        }\n         String name = topology.newName(PRINTING_NAME);\n         streamName = (streamName == null) ? this.name : streamName;\n         try {",
                "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java",
                "sha": "bb77e963655e35db9456b7941bbec84bc99b07ee",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java",
                "patch": "@@ -189,6 +189,10 @@ public void writeAsText(String filePath, Serde<K> keySerde, Serde<V> valSerde) {\n      */\n     @Override\n     public void writeAsText(String filePath, String streamName, Serde<K> keySerde, Serde<V> valSerde) {\n+        Objects.requireNonNull(filePath, \"filePath can't be null\");\n+        if (filePath.trim().isEmpty()) {\n+            throw new TopologyBuilderException(\"filePath can't be an empty string\");\n+        }\n         String name = topology.newName(PRINTING_NAME);\n         streamName = (streamName == null) ? this.name : streamName;\n         try {",
                "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java",
                "sha": "fc1c07674c8b5425ade4df71c4f6d92f56af235d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.errors.TopologyBuilderException;\n import org.apache.kafka.streams.kstream.JoinWindows;\n import org.apache.kafka.streams.kstream.KStream;\n import org.apache.kafka.streams.kstream.KStreamBuilder;\n@@ -183,6 +184,11 @@ public void shouldNotAllowNullFilePathOnWriteAsText() throws Exception {\n         testStream.writeAsText(null);\n     }\n \n+    @Test(expected = TopologyBuilderException.class)\n+    public void shouldNotAllowEmptyFilePathOnWriteAsText() throws Exception {\n+        testStream.writeAsText(\"\\t    \\t\");\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullMapperOnFlatMap() throws Exception {\n         testStream.flatMap(null);",
                "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java",
                "sha": "e5e334c48b49f456eac4b0b6ada772321a2b6415",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.errors.TopologyBuilderException;\n import org.apache.kafka.streams.kstream.KStreamBuilder;\n import org.apache.kafka.streams.kstream.KTable;\n import org.apache.kafka.streams.kstream.Predicate;\n@@ -402,6 +403,11 @@ public void shouldNotAllowNullFilePathOnWriteAsText() throws Exception {\n         table.writeAsText(null);\n     }\n \n+    @Test(expected = TopologyBuilderException.class)\n+    public void shouldNotAllowEmptyFilePathOnWriteAsText() throws Exception {\n+        table.writeAsText(\"\\t  \\t\");\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullActionOnForEach() throws Exception {\n         table.foreach(null);",
                "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java",
                "sha": "afa1033d732a0d85520435c845a2ada7b883bf55",
                "status": "modified"
            }
        ],
        "message": "KAFKA-4312: If filePath is empty string writeAsText should have more meaningful error message\n\n\u2026eaningful error message\n\nAuthor: bbejeck <bbejeck@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2042 from bbejeck/KAFKA-4312_write_as_text_throws_NPE_empty_string",
        "parent": "https://github.com/apache/kafka/commit/c2a8b86117ede2ffda4cc4a8800b46f65ef9922d",
        "repo": "kafka",
        "unit_tests": [
            "KStreamImplTest.java",
            "KTableImplTest.java"
        ]
    },
    "kafka_bf2c46a": {
        "bug_id": "kafka_bf2c46a",
        "commit": "https://github.com/apache/kafka/commit/bf2c46a0b8266562bf4697dc8e3e9443059f9abb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java?ref=bf2c46a0b8266562bf4697dc8e3e9443059f9abb",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java",
                "patch": "@@ -292,7 +292,7 @@ public void buildAndOptimizeTopology(final Properties props) {\n \n     private void maybePerformOptimizations(final Properties props) {\n \n-        if (props != null && props.getProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION).equals(StreamsConfig.OPTIMIZE)) {\n+        if (props != null && StreamsConfig.OPTIMIZE.equals(props.getProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION))) {\n             LOG.debug(\"Optimizing the Kafka Streams graph for repartition nodes\");\n             maybeOptimizeRepartitionOperations();\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java",
                "sha": "c1532af97f2748eb08d177a583565f9eb9f465a2",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/kafka/blob/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java?ref=bf2c46a0b8266562bf4697dc8e3e9443059f9abb",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java",
                "patch": "@@ -55,6 +55,14 @@\n     private final StreamsBuilder builder = new StreamsBuilder();\n     private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n \n+    @Test\n+    public void shouldNotThrowNullPointerIfOptimizationsNotSpecified() {\n+        final Properties properties = new Properties();\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.build(properties);\n+    }\n+\n     @Test\n     public void shouldAllowJoinUnmaterializedFilteredKTable() {\n         final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(\"table-topic\").filter(MockPredicate.<Bytes, String>allGoodPredicate());",
                "raw_url": "https://github.com/apache/kafka/raw/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java",
                "sha": "2531591869daaad968f321dd984abee8f4dfef0c",
                "status": "modified"
            }
        ],
        "message": "MINOR: Change order of Property Check To Avoid NPE (#5528)\n\nReviewer: Matthias J. Sax <matthias@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/7c4e6724699bb6fc65112b5513848c733a03019e",
        "repo": "kafka",
        "unit_tests": [
            "InternalStreamsBuilderTest.java"
        ]
    },
    "kafka_c439268": {
        "bug_id": "kafka_c439268",
        "commit": "https://github.com/apache/kafka/commit/c439268224e3178002bfa28bc048722870f992e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/build.gradle",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/build.gradle?ref=c439268224e3178002bfa28bc048722870f992e3",
                "deletions": 0,
                "filename": "build.gradle",
                "patch": "@@ -576,6 +576,9 @@ project(':clients') {\n \n     testCompile libs.bcpkix\n     testCompile libs.junit\n+    testCompile libs.easymock\n+    testCompile libs.powermock\n+    testCompile libs.powermockEasymock\n \n     testRuntime libs.slf4jlog4j\n   }",
                "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/build.gradle",
                "sha": "b3be020cbd7117b163ab920cdf6a5002d534fafe",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java?ref=c439268224e3178002bfa28bc048722870f992e3",
                "deletions": 13,
                "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "patch": "@@ -437,8 +437,9 @@ private static int parseAcks(String acksString) {\n         TopicPartition tp = null;\n         try {\n             // first make sure the metadata for the topic is available\n-            long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs);\n-            long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs);\n+            ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), this.maxBlockTimeMs);\n+            long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\n+            Cluster cluster = clusterAndWaitTime.cluster;\n             byte[] serializedKey;\n             try {\n                 serializedKey = keySerializer.serialize(record.topic(), record.key());\n@@ -455,7 +456,8 @@ private static int parseAcks(String acksString) {\n                         \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +\n                         \" specified in value.serializer\");\n             }\n-            int partition = partition(record, serializedKey, serializedValue, metadata.fetch());\n+\n+            int partition = partition(record, serializedKey, serializedValue, cluster);\n             int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);\n             ensureValidRecordSize(serializedSize);\n             tp = new TopicPartition(record.topic(), partition);\n@@ -508,17 +510,19 @@ private static int parseAcks(String acksString) {\n      * Wait for cluster metadata including partitions for the given topic to be available.\n      * @param topic The topic we want metadata for\n      * @param maxWaitMs The maximum time in ms for waiting on the metadata\n-     * @return The amount of time we waited in ms\n+     * @return The cluster containing topic metadata and the amount of time we waited in ms\n      */\n-    private long waitOnMetadata(String topic, long maxWaitMs) throws InterruptedException {\n+    private ClusterAndWaitTime waitOnMetadata(String topic, long maxWaitMs) throws InterruptedException {\n         // add topic to metadata topic list if it is not there already and reset expiry\n         this.metadata.add(topic);\n-        if (metadata.fetch().partitionsForTopic(topic) != null)\n-            return 0;\n+        Cluster cluster = metadata.fetch();\n+        if (cluster.partitionsForTopic(topic) != null)\n+            return new ClusterAndWaitTime(cluster, 0);\n \n         long begin = time.milliseconds();\n         long remainingWaitMs = maxWaitMs;\n-        while (metadata.fetch().partitionsForTopic(topic) == null) {\n+        long elapsed = 0;\n+        while (cluster.partitionsForTopic(topic) == null) {\n             log.trace(\"Requesting metadata update for topic {}.\", topic);\n             int version = metadata.requestUpdate();\n             sender.wakeup();\n@@ -528,14 +532,15 @@ private long waitOnMetadata(String topic, long maxWaitMs) throws InterruptedExce\n                 // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs\n                 throw new TimeoutException(\"Failed to update metadata after \" + maxWaitMs + \" ms.\");\n             }\n-            long elapsed = time.milliseconds() - begin;\n+            cluster = metadata.fetch();\n+            elapsed = time.milliseconds() - begin;\n             if (elapsed >= maxWaitMs)\n                 throw new TimeoutException(\"Failed to update metadata after \" + maxWaitMs + \" ms.\");\n-            if (metadata.fetch().unauthorizedTopics().contains(topic))\n+            if (cluster.unauthorizedTopics().contains(topic))\n                 throw new TopicAuthorizationException(topic);\n             remainingWaitMs = maxWaitMs - elapsed;\n         }\n-        return time.milliseconds() - begin;\n+        return new ClusterAndWaitTime(cluster, elapsed);\n     }\n \n     /**\n@@ -600,12 +605,13 @@ public void flush() {\n      */\n     @Override\n     public List<PartitionInfo> partitionsFor(String topic) {\n+        Cluster cluster;\n         try {\n-            waitOnMetadata(topic, this.maxBlockTimeMs);\n+            cluster = waitOnMetadata(topic, this.maxBlockTimeMs).cluster;\n         } catch (InterruptedException e) {\n             throw new InterruptException(e);\n         }\n-        return this.metadata.fetch().partitionsForTopic(topic);\n+        return cluster.partitionsForTopic(topic);\n     }\n \n     /**\n@@ -724,6 +730,15 @@ private int partition(ProducerRecord<K, V> record, byte[] serializedKey , byte[]\n             cluster);\n     }\n \n+    private static class ClusterAndWaitTime {\n+        final Cluster cluster;\n+        final long waitedOnMetadataMs;\n+        ClusterAndWaitTime(Cluster cluster, long waitedOnMetadataMs) {\n+            this.cluster = cluster;\n+            this.waitedOnMetadataMs = waitedOnMetadataMs;\n+        }\n+    }\n+\n     private static class FutureFailure implements Future<RecordMetadata> {\n \n         private final ExecutionException exception;",
                "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
                "sha": "05d937754aad8f150809e4db69c71b291aff2b45",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java?ref=c439268224e3178002bfa28bc048722870f992e3",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java",
                "patch": "@@ -16,21 +16,37 @@\n  */\n package org.apache.kafka.clients.producer;\n \n+import org.apache.kafka.common.Cluster;\n import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.serialization.ByteArraySerializer;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.Metadata;\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.test.MockMetricsReporter;\n import org.apache.kafka.test.MockProducerInterceptor;\n import org.apache.kafka.test.MockSerializer;\n+import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.api.support.membermodification.MemberModifier;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareOnlyThisForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import java.util.Properties;\n import java.util.Map;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashMap;\n \n+@RunWith(PowerMockRunner.class)\n+@PowerMockIgnore(\"javax.management.*\")\n public class KafkaProducerTest {\n \n     @Test\n@@ -123,4 +139,50 @@ public void testInvalidSocketReceiveBufferSize() throws Exception {\n         config.put(ProducerConfig.RECEIVE_BUFFER_CONFIG, -2);\n         new KafkaProducer<>(config, new ByteArraySerializer(), new ByteArraySerializer());\n     }\n+\n+    @PrepareOnlyThisForTest(Metadata.class)\n+    @Test\n+    public void testMetadataFetch() throws Exception {\n+        Properties props = new Properties();\n+        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\n+        Metadata metadata = PowerMock.createNiceMock(Metadata.class);\n+        MemberModifier.field(KafkaProducer.class, \"metadata\").set(producer, metadata);\n+\n+        String topic = \"topic\";\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"value\");\n+        Collection<Node> nodes = Collections.singletonList(new Node(0, \"host1\", 1000));\n+        final Cluster emptyCluster = new Cluster(nodes,\n+                Collections.<PartitionInfo>emptySet(),\n+                Collections.<String>emptySet());\n+        final Cluster cluster = new Cluster(\n+                Collections.singletonList(new Node(0, \"host1\", 1000)),\n+                Arrays.asList(new PartitionInfo(topic, 0, null, null, null)),\n+                Collections.<String>emptySet());\n+\n+        // Expect exactly one fetch for each attempt to refresh while topic metadata is not available\n+        final int refreshAttempts = 5;\n+        EasyMock.expect(metadata.fetch()).andReturn(emptyCluster).times(refreshAttempts - 1);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.send(record);\n+        PowerMock.verify(metadata);\n+\n+        // Expect exactly one fetch if topic metadata is available\n+        PowerMock.reset(metadata);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.send(record, null);\n+        PowerMock.verify(metadata);\n+\n+        // Expect exactly one fetch if topic metadata is available\n+        PowerMock.reset(metadata);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.partitionsFor(topic);\n+        PowerMock.verify(metadata);\n+    }\n }",
                "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java",
                "sha": "1780e2f823fef915b85f0f89d3a88334472095e9",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3562; Handle topic deletion during a send\n\nFix timing window in producer by holding onto cluster object while processing send requests so that changes to cluster during metadata refresh don't cause NPE if a topic is deleted.\n\nAuthor: Rajini Sivaram <rajinisivaram@googlemail.com>\n\nReviewers: Sriharsha Chintalapani <harsha@hortonworks.com>, Ewen Cheslack-Postava <ewen@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1478 from rajinisivaram/KAFKA-3562",
        "parent": "https://github.com/apache/kafka/commit/383cec9cf38607cdfbda0256d3447d253dcdde7d",
        "repo": "kafka",
        "unit_tests": [
            "KafkaProducerTest.java"
        ]
    },
    "kafka_c783630": {
        "bug_id": "kafka_c783630",
        "commit": "https://github.com/apache/kafka/commit/c7836307c3588ed5d267f32eabcd7dfc0dbeec80",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80",
                "deletions": 6,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java",
                "patch": "@@ -151,30 +151,41 @@ public void remove(final Windowed<K> sessionKey) {\n     @Override\n     public V fetchSession(final K key, final long startTime, final long endTime) {\n         Objects.requireNonNull(key, \"key cannot be null\");\n-        final V value;\n         final Bytes bytesKey = keyBytes(key);\n         final long startNs = time.nanoseconds();\n         try {\n-            value = serdes.valueFrom(wrapped().fetchSession(bytesKey, startTime, endTime));\n+            final byte[] result = wrapped().fetchSession(bytesKey, startTime, endTime);\n+            if (result == null) {\n+                return null;\n+            }\n+            return serdes.valueFrom(result);\n         } finally {\n             metrics.recordLatency(flushTime, startNs, time.nanoseconds());\n         }\n-\n-        return value;\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<K>, V> fetch(final K key) {\n         Objects.requireNonNull(key, \"key cannot be null\");\n-        return findSessions(key, 0, Long.MAX_VALUE);\n+        return new MeteredWindowedKeyValueIterator<>(\n+            wrapped().fetch(keyBytes(key)),\n+            fetchTime,\n+            metrics,\n+            serdes,\n+            time);\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<K>, V> fetch(final K from,\n                                                   final K to) {\n         Objects.requireNonNull(from, \"from cannot be null\");\n         Objects.requireNonNull(to, \"to cannot be null\");\n-        return findSessions(from, to, 0, Long.MAX_VALUE);\n+        return new MeteredWindowedKeyValueIterator<>(\n+            wrapped().fetch(keyBytes(from), keyBytes(to)),\n+            fetchTime,\n+            metrics,\n+            serdes,\n+            time);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java",
                "sha": "94b004e330e75d310eaa3d77851b85ef47426797",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80",
                "deletions": 0,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java",
                "patch": "@@ -55,6 +55,7 @@\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(EasyMockRunner.class)\n@@ -243,6 +244,14 @@ public void shouldSetFlushListenerOnWrappedCachingStore() {\n         verify(cachedKeyValueStore);\n     }\n \n+    @Test\n+    public void shouldNotThrowNullPointerExceptionIfGetReturnsNull() {\n+        expect(inner.get(Bytes.wrap(\"a\".getBytes()))).andReturn(null);\n+\n+        init();\n+        assertNull(metered.get(\"a\"));\n+    }\n+\n     @Test\n     public void shouldNotSetFlushListenerOnWrappedNoneCachingStore() {\n         assertFalse(metered.setFlushListener(null, false));",
                "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java",
                "sha": "5cbe95cea2d37c0da2f0ec7cc1b599f199e36cd0",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80",
                "deletions": 2,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java",
                "patch": "@@ -57,6 +57,7 @@\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(EasyMockRunner.class)\n@@ -172,7 +173,7 @@ public void shouldRemoveFromStoreAndRecordRemoveMetric() {\n \n     @Test\n     public void shouldFetchForKeyAndRecordFetchMetric() {\n-        expect(inner.findSessions(Bytes.wrap(keyBytes), 0, Long.MAX_VALUE))\n+        expect(inner.fetch(Bytes.wrap(keyBytes)))\n                 .andReturn(new KeyValueIteratorStub<>(\n                         Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));\n         init();\n@@ -189,7 +190,7 @@ public void shouldFetchForKeyAndRecordFetchMetric() {\n \n     @Test\n     public void shouldFetchRangeFromStoreAndRecordFetchMetric() {\n-        expect(inner.findSessions(Bytes.wrap(keyBytes), Bytes.wrap(keyBytes), 0, Long.MAX_VALUE))\n+        expect(inner.fetch(Bytes.wrap(keyBytes), Bytes.wrap(keyBytes)))\n                 .andReturn(new KeyValueIteratorStub<>(\n                         Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));\n         init();\n@@ -211,6 +212,14 @@ public void shouldRecordRestoreTimeOnInit() {\n         assertTrue((Double) metric.metricValue() > 0);\n     }\n \n+    @Test\n+    public void shouldNotThrowNullPointerExceptionIfFetchSessionReturnsNull() {\n+        expect(inner.fetchSession(Bytes.wrap(\"a\".getBytes()), 0, Long.MAX_VALUE)).andReturn(null);\n+\n+        init();\n+        assertNull(metered.fetchSession(\"a\", 0, Long.MAX_VALUE));\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldThrowNullPointerOnPutIfKeyIsNull() {\n         metered.put(null, \"a\");",
                "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java",
                "sha": "30c382b19c1c9b5fc259bd1e54503584a955eb95",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80",
                "deletions": 1,
                "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java",
                "patch": "@@ -178,7 +178,7 @@ public void shouldCloseUnderlyingStore() {\n     }\n \n     @Test\n-    public void shouldNotExceptionIfFetchReturnsNull() {\n+    public void shouldNotThrowNullPointerExceptionIfFetchReturnsNull() {\n         expect(innerStoreMock.fetch(Bytes.wrap(\"a\".getBytes()), 0)).andReturn(null);\n         replay(innerStoreMock);\n ",
                "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java",
                "sha": "c0ed7f634017effccd61044325ff7eb26c9504fe",
                "status": "modified"
            }
        ],
        "message": "[HOT FIX] Check for null before deserializing in MeteredSessionStore  (#6575)\n\nThe fetchSession() method of SessionStore searches for a (single) specific session and returns null if none are found. This is analogous to fetch(key, time) in WindowStore or get(key) in KeyValueStore. MeteredWindowStore and MeteredKeyValueStore both check for a null result before attempting to deserialize, however MeteredSessionStore just blindly deserializes and as a result NPE is thrown when we search for a record that does not exist.\r\n\r\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Bruno Cadonna <bruno@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/555d05971e16123a497ad423ef4786e0831b2101",
        "repo": "kafka",
        "unit_tests": [
            "MeteredSessionStoreTest.java"
        ]
    },
    "kafka_d01f01e": {
        "bug_id": "kafka_d01f01e",
        "commit": "https://github.com/apache/kafka/commit/d01f01ec63f61bd4742f02abff2ab6cf339e2897",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 0,
                "filename": "clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java",
                "patch": "@@ -117,6 +117,10 @@ public String id() {\n         return id;\n     }\n \n+    public SelectionKey selectionKey() {\n+        return transportLayer.selectionKey();\n+    }\n+\n     /**\n      * externally muting a channel should be done via selector to ensure proper state handling\n      */",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java",
                "sha": "e125bbca10e82876c35b60e8c236d65ebcad8e96",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 14,
                "filename": "clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java",
                "patch": "@@ -63,6 +63,11 @@ public SocketChannel socketChannel() {\n         return socketChannel;\n     }\n \n+    @Override\n+    public SelectionKey selectionKey() {\n+        return key;\n+    }\n+\n     @Override\n     public boolean isOpen() {\n         return socketChannel.isOpen();\n@@ -73,20 +78,10 @@ public boolean isConnected() {\n         return socketChannel.isConnected();\n     }\n \n-    /**\n-     * Closes this channel\n-     *\n-     * @throws IOException If I/O error occurs\n-     */\n     @Override\n     public void close() throws IOException {\n-        try {\n-            socketChannel.socket().close();\n-            socketChannel.close();\n-        } finally {\n-            key.attach(null);\n-            key.cancel();\n-        }\n+        socketChannel.socket().close();\n+        socketChannel.close();\n     }\n \n     /**\n@@ -191,7 +186,6 @@ public Principal peerPrincipal() throws IOException {\n \n     /**\n      * Adds the interestOps to selectionKey.\n-     * @param ops\n      */\n     @Override\n     public void addInterestOps(int ops) {\n@@ -201,7 +195,6 @@ public void addInterestOps(int ops) {\n \n     /**\n      * Removes the interestOps from selectionKey.\n-     * @param ops\n      */\n     @Override\n     public void removeInterestOps(int ops) {",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java",
                "sha": "ccb9c606185faf8bfe37b2e8938292b8a049ed67",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/Selector.java",
                "changes": 162,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/Selector.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 74,
                "filename": "clients/src/main/java/org/apache/kafka/common/network/Selector.java",
                "patch": "@@ -16,6 +16,23 @@\n  */\n package org.apache.kafka.common.network;\n \n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.errors.AuthenticationException;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.metrics.Measurable;\n+import org.apache.kafka.common.metrics.MetricConfig;\n+import org.apache.kafka.common.metrics.Metrics;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.metrics.stats.Avg;\n+import org.apache.kafka.common.metrics.stats.Count;\n+import org.apache.kafka.common.metrics.stats.Max;\n+import org.apache.kafka.common.metrics.stats.Meter;\n+import org.apache.kafka.common.metrics.stats.SampledStat;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.slf4j.Logger;\n+\n import java.io.IOException;\n import java.net.InetSocketAddress;\n import java.net.Socket;\n@@ -37,23 +54,6 @@\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n \n-import org.apache.kafka.common.KafkaException;\n-import org.apache.kafka.common.memory.MemoryPool;\n-import org.apache.kafka.common.metrics.Measurable;\n-import org.apache.kafka.common.metrics.MetricConfig;\n-import org.apache.kafka.common.MetricName;\n-import org.apache.kafka.common.errors.AuthenticationException;\n-import org.apache.kafka.common.metrics.Metrics;\n-import org.apache.kafka.common.metrics.Sensor;\n-import org.apache.kafka.common.metrics.stats.Avg;\n-import org.apache.kafka.common.metrics.stats.Meter;\n-import org.apache.kafka.common.metrics.stats.SampledStat;\n-import org.apache.kafka.common.metrics.stats.Count;\n-import org.apache.kafka.common.metrics.stats.Max;\n-import org.apache.kafka.common.utils.LogContext;\n-import org.apache.kafka.common.utils.Time;\n-import org.slf4j.Logger;\n-\n /**\n  * A nioSelector interface for doing non-blocking multi-connection network I/O.\n  * <p>\n@@ -195,12 +195,37 @@ public Selector(long connectionMaxIdleMS, Metrics metrics, Time time, String met\n      */\n     @Override\n     public void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException {\n-        if (this.channels.containsKey(id))\n-            throw new IllegalStateException(\"There is already a connection for id \" + id);\n-        if (this.closingChannels.containsKey(id))\n-            throw new IllegalStateException(\"There is already a connection for id \" + id + \" that is still being closed\");\n-\n+        ensureNotRegistered(id);\n         SocketChannel socketChannel = SocketChannel.open();\n+        try {\n+            configureSocketChannel(socketChannel, sendBufferSize, receiveBufferSize);\n+            boolean connected = doConnect(socketChannel, address);\n+            SelectionKey key = registerChannel(id, socketChannel, SelectionKey.OP_CONNECT);\n+\n+            if (connected) {\n+                // OP_CONNECT won't trigger for immediately connected channels\n+                log.debug(\"Immediately connected to node {}\", id);\n+                immediatelyConnectedKeys.add(key);\n+                key.interestOps(0);\n+            }\n+        } catch (IOException | RuntimeException e) {\n+            socketChannel.close();\n+            throw e;\n+        }\n+    }\n+\n+    // Visible to allow test cases to override. In particular, we use this to implement a blocking connect\n+    // in order to simulate \"immediately connected\" sockets.\n+    protected boolean doConnect(SocketChannel channel, InetSocketAddress address) throws IOException {\n+        try {\n+            return channel.connect(address);\n+        } catch (UnresolvedAddressException e) {\n+            throw new IOException(\"Can't resolve address: \" + address, e);\n+        }\n+    }\n+\n+    private void configureSocketChannel(SocketChannel socketChannel, int sendBufferSize, int receiveBufferSize)\n+            throws IOException {\n         socketChannel.configureBlocking(false);\n         Socket socket = socketChannel.socket();\n         socket.setKeepAlive(true);\n@@ -209,25 +234,6 @@ public void connect(String id, InetSocketAddress address, int sendBufferSize, in\n         if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)\n             socket.setReceiveBufferSize(receiveBufferSize);\n         socket.setTcpNoDelay(true);\n-        boolean connected;\n-        try {\n-            connected = socketChannel.connect(address);\n-        } catch (UnresolvedAddressException e) {\n-            socketChannel.close();\n-            throw new IOException(\"Can't resolve address: \" + address, e);\n-        } catch (IOException e) {\n-            socketChannel.close();\n-            throw e;\n-        }\n-        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);\n-        KafkaChannel channel = buildChannel(socketChannel, id, key);\n-\n-        if (connected) {\n-            // OP_CONNECT won't trigger for immediately connected channels\n-            log.debug(\"Immediately connected to node {}\", channel.id());\n-            immediatelyConnectedKeys.add(key);\n-            key.interestOps(0);\n-        }\n     }\n \n     /**\n@@ -245,19 +251,29 @@ public void connect(String id, InetSocketAddress address, int sendBufferSize, in\n      * </p>\n      */\n     public void register(String id, SocketChannel socketChannel) throws IOException {\n+        ensureNotRegistered(id);\n+        registerChannel(id, socketChannel, SelectionKey.OP_READ);\n+    }\n+\n+    private void ensureNotRegistered(String id) {\n         if (this.channels.containsKey(id))\n             throw new IllegalStateException(\"There is already a connection for id \" + id);\n         if (this.closingChannels.containsKey(id))\n             throw new IllegalStateException(\"There is already a connection for id \" + id + \" that is still being closed\");\n+    }\n \n-        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_READ);\n-        buildChannel(socketChannel, id, key);\n+    private SelectionKey registerChannel(String id, SocketChannel socketChannel, int interestedOps) throws IOException {\n+        SelectionKey key = socketChannel.register(nioSelector, interestedOps);\n+        KafkaChannel channel = buildAndAttachKafkaChannel(socketChannel, id, key);\n+        this.channels.put(id, channel);\n+        return key;\n     }\n \n-    private KafkaChannel buildChannel(SocketChannel socketChannel, String id, SelectionKey key) throws IOException {\n-        KafkaChannel channel;\n+    private KafkaChannel buildAndAttachKafkaChannel(SocketChannel socketChannel, String id, SelectionKey key) throws IOException {\n         try {\n-            channel = channelBuilder.buildChannel(id, key, maxReceiveSize, memoryPool);\n+            KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize, memoryPool);\n+            key.attach(channel);\n+            return channel;\n         } catch (Exception e) {\n             try {\n                 socketChannel.close();\n@@ -266,9 +282,6 @@ private KafkaChannel buildChannel(SocketChannel socketChannel, String id, Select\n             }\n             throw new IOException(\"Channel could not be created for socket \" + socketChannel, e);\n         }\n-        key.attach(channel);\n-        this.channels.put(id, channel);\n-        return channel;\n     }\n \n     /**\n@@ -386,20 +399,22 @@ public void poll(long timeout) throws IOException {\n \n         if (numReadyKeys > 0 || !immediatelyConnectedKeys.isEmpty() || dataInBuffers) {\n             Set<SelectionKey> readyKeys = this.nioSelector.selectedKeys();\n-            keysWithBufferedRead.removeAll(readyKeys); //so no channel gets polled twice\n \n-            //poll from channels that have buffered data (but nothing more from the underlying socket)\n-            if (!keysWithBufferedRead.isEmpty()) {\n+            // Poll from channels that have buffered data (but nothing more from the underlying socket)\n+            if (dataInBuffers) {\n+                keysWithBufferedRead.removeAll(readyKeys); //so no channel gets polled twice\n                 Set<SelectionKey> toPoll = keysWithBufferedRead;\n                 keysWithBufferedRead = new HashSet<>(); //poll() calls will repopulate if needed\n                 pollSelectionKeys(toPoll, false, endSelect);\n             }\n-            //poll from channels where the underlying socket has more data\n-            pollSelectionKeys(readyKeys, false, endSelect);\n-            pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n \n+            // Poll from channels where the underlying socket has more data\n+            pollSelectionKeys(readyKeys, false, endSelect);\n             // Clear all selected keys so that they are included in the ready count for the next select\n             readyKeys.clear();\n+\n+            pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n+            immediatelyConnectedKeys.clear();\n         } else {\n             madeReadProgressLastPoll = true; //no work is also \"progress\"\n         }\n@@ -424,11 +439,9 @@ public void poll(long timeout) throws IOException {\n      */\n     // package-private for testing\n     void pollSelectionKeys(Set<SelectionKey> selectionKeys,\n-                                   boolean isImmediatelyConnected,\n-                                   long currentTimeNanos) {\n-        Iterator<SelectionKey> iterator = determineHandlingOrder(selectionKeys).iterator();\n-        while (iterator.hasNext()) {\n-            SelectionKey key = iterator.next();\n+                           boolean isImmediatelyConnected,\n+                           long currentTimeNanos) {\n+        for (SelectionKey key : determineHandlingOrder(selectionKeys)) {\n             KafkaChannel channel = channel(key);\n             long channelStartTimeNanos = recordTimePerConnection ? time.nanoseconds() : 0;\n \n@@ -507,16 +520,13 @@ else if (e instanceof AuthenticationException) // will be logged later as error\n     private Collection<SelectionKey> determineHandlingOrder(Set<SelectionKey> selectionKeys) {\n         //it is possible that the iteration order over selectionKeys is the same every invocation.\n         //this may cause starvation of reads when memory is low. to address this we shuffle the keys if memory is low.\n-        Collection<SelectionKey> inHandlingOrder;\n-\n         if (!outOfMemory && memoryPool.availableMemory() < lowMemThreshold) {\n-            List<SelectionKey> temp = new ArrayList<>(selectionKeys);\n-            Collections.shuffle(temp);\n-            inHandlingOrder = temp;\n+            List<SelectionKey> shuffledKeys = new ArrayList<>(selectionKeys);\n+            Collections.shuffle(shuffledKeys);\n+            return shuffledKeys;\n         } else {\n-            inHandlingOrder = selectionKeys;\n+            return selectionKeys;\n         }\n-        return inHandlingOrder;\n     }\n \n     private void attemptRead(SelectionKey key, KafkaChannel channel) throws IOException {\n@@ -642,19 +652,17 @@ private void clear() {\n     /**\n      * Check for data, waiting up to the given timeout.\n      *\n-     * @param ms Length of time to wait, in milliseconds, which must be non-negative\n+     * @param timeoutMs Length of time to wait, in milliseconds, which must be non-negative\n      * @return The number of keys ready\n-     * @throws IllegalArgumentException\n-     * @throws IOException\n      */\n-    private int select(long ms) throws IOException {\n-        if (ms < 0L)\n+    private int select(long timeoutMs) throws IOException {\n+        if (timeoutMs < 0L)\n             throw new IllegalArgumentException(\"timeout should be >= 0\");\n \n-        if (ms == 0L)\n+        if (timeoutMs == 0L)\n             return this.nioSelector.selectNow();\n         else\n-            return this.nioSelector.select(ms);\n+            return this.nioSelector.select(timeoutMs);\n     }\n \n     /**\n@@ -713,10 +721,16 @@ private void close(KafkaChannel channel, boolean processOutstanding) {\n     }\n \n     private void doClose(KafkaChannel channel, boolean notifyDisconnect) {\n+        SelectionKey key = channel.selectionKey();\n         try {\n+            immediatelyConnectedKeys.remove(key);\n+            keysWithBufferedRead.remove(key);\n             channel.close();\n         } catch (IOException e) {\n             log.error(\"Exception closing connection to node {}:\", channel.id(), e);\n+        } finally {\n+            key.cancel();\n+            key.attach(null);\n         }\n         this.sensors.connectionClosed.record();\n         this.stagedReceives.remove(channel);",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/Selector.java",
                "sha": "6bfcfd21a90e89a90b3eecdd7acce205d5a8308d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 7,
                "filename": "clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java",
                "patch": "@@ -130,6 +130,11 @@ public SocketChannel socketChannel() {\n         return socketChannel;\n     }\n \n+    @Override\n+    public SelectionKey selectionKey() {\n+        return key;\n+    }\n+\n     @Override\n     public boolean isOpen() {\n         return socketChannel.isOpen();\n@@ -169,13 +174,8 @@ public void close() throws IOException {\n         } catch (IOException ie) {\n             log.warn(\"Failed to send SSL Close message \", ie);\n         } finally {\n-            try {\n-                socketChannel.socket().close();\n-                socketChannel.close();\n-            } finally {\n-                key.attach(null);\n-                key.cancel();\n-            }\n+            socketChannel.socket().close();\n+            socketChannel.close();\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java",
                "sha": "69ca037266d9beb7917db6e79061586da2980f4a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 3,
                "filename": "clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java",
                "patch": "@@ -25,6 +25,7 @@\n  */\n import java.io.IOException;\n import java.nio.channels.FileChannel;\n+import java.nio.channels.SelectionKey;\n import java.nio.channels.SocketChannel;\n import java.nio.channels.ScatteringByteChannel;\n import java.nio.channels.GatheringByteChannel;\n@@ -60,13 +61,16 @@\n      */\n     SocketChannel socketChannel();\n \n+    /**\n+     * Get the underlying selection key\n+     */\n+    SelectionKey selectionKey();\n \n     /**\n      * This a no-op for the non-secure PLAINTEXT implementation. For SSL, this performs\n      * SSL handshake. The SSL handshake includes client authentication if configured using\n-     * {@link org.apache.kafka.common.config.SslConfigsSslConfigs#SSL_CLIENT_AUTH_CONFIG}.\n-     * @throws AuthenticationException if handshake fails due to an\n-     *         {@link javax.net.ssl.SSLExceptionSSLException}.\n+     * {@link org.apache.kafka.common.config.SslConfigs#SSL_CLIENT_AUTH_CONFIG}.\n+     * @throws AuthenticationException if handshake fails due to an {@link javax.net.ssl.SSLException}.\n      * @throws IOException if read or write fails with an I/O error.\n     */\n     void handshake() throws AuthenticationException, IOException;",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java",
                "sha": "3673d21dae6b57b173074341aae2cc9d6f4cf5f9",
                "status": "modified"
            },
            {
                "additions": 92,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java",
                "changes": 115,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 23,
                "filename": "clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Time;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.test.TestCondition;\n import org.apache.kafka.test.TestUtils;\n import org.easymock.IMocksControl;\n import org.junit.After;\n@@ -41,6 +42,7 @@\n import java.nio.channels.SelectionKey;\n import java.nio.channels.ServerSocketChannel;\n import java.nio.channels.SocketChannel;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n@@ -104,16 +106,29 @@ public SecurityProtocol securityProtocol() {\n      */\n     @Test\n     public void testServerDisconnect() throws Exception {\n-        String node = \"0\";\n+        final String node = \"0\";\n \n         // connect and do a simple request\n         blockingConnect(node);\n         assertEquals(\"hello\", blockingRequest(node, \"hello\"));\n \n+        KafkaChannel channel = selector.channel(node);\n+\n         // disconnect\n         this.server.closeConnections();\n-        while (!selector.disconnected().containsKey(node))\n-            selector.poll(1000L);\n+        TestUtils.waitForCondition(new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                try {\n+                    selector.poll(1000L);\n+                    return selector.disconnected().containsKey(node);\n+                } catch (IOException e) {\n+                    throw new RuntimeException(e);\n+                }\n+            }\n+        }, 5000, \"Failed to observe disconnected node in disconnected set\");\n+\n+        assertNull(channel.selectionKey().attachment());\n \n         // reconnect and do another request\n         blockingConnect(node);\n@@ -186,8 +201,8 @@ public void testNormalOperation() throws Exception {\n         for (int i = 0; i < conns; i++)\n             connect(Integer.toString(i), addr);\n         // send echo requests and receive responses\n-        Map<String, Integer> requests = new HashMap<String, Integer>();\n-        Map<String, Integer> responses = new HashMap<String, Integer>();\n+        Map<String, Integer> requests = new HashMap<>();\n+        Map<String, Integer> responses = new HashMap<>();\n         int responseCount = 0;\n         for (int i = 0; i < conns; i++) {\n             String node = Integer.toString(i);\n@@ -252,11 +267,6 @@ public void testLargeMessageSequence() throws Exception {\n         sendAndReceive(node, requestPrefix, 0, reqs);\n     }\n \n-\n-\n-    /**\n-     * Test sending an empty string\n-     */\n     @Test\n     public void testEmptyRequest() throws Exception {\n         String node = \"0\";\n@@ -333,6 +343,7 @@ public void testCloseConnectionInClosingState() throws Exception {\n         assertNull(\"Channel not removed from closingChannels\", selector.closingChannel(id));\n         assertTrue(\"Unexpected disconnect notification\", selector.disconnected().isEmpty());\n         assertEquals(ChannelState.EXPIRED, channel.state());\n+        assertNull(channel.selectionKey().attachment());\n         selector.poll(0);\n         assertTrue(\"Unexpected disconnect notification\", selector.disconnected().isEmpty());\n     }\n@@ -349,6 +360,41 @@ public void testCloseOldestConnection() throws Exception {\n         assertEquals(ChannelState.EXPIRED, selector.disconnected().get(id));\n     }\n \n+    @Test\n+    public void testImmediatelyConnectedCleaned() throws Exception {\n+        Metrics metrics = new Metrics(); // new metrics object to avoid metric registration conflicts\n+        Selector selector = new Selector(5000, metrics, time, \"MetricGroup\", channelBuilder, new LogContext()) {\n+            @Override\n+            protected boolean doConnect(SocketChannel channel, InetSocketAddress address) throws IOException {\n+                // Use a blocking connect to trigger the immediately connected path\n+                channel.configureBlocking(true);\n+                boolean connected = super.doConnect(channel, address);\n+                channel.configureBlocking(false);\n+                return connected;\n+            }\n+        };\n+\n+        try {\n+            testImmediatelyConnectedCleaned(selector, true);\n+            testImmediatelyConnectedCleaned(selector, false);\n+        } finally {\n+            selector.close();\n+            metrics.close();\n+        }\n+    }\n+\n+    private void testImmediatelyConnectedCleaned(Selector selector, boolean closeAfterFirstPoll) throws Exception {\n+        String id = \"0\";\n+        selector.connect(id, new InetSocketAddress(\"localhost\", server.port), BUFFER_SIZE, BUFFER_SIZE);\n+        verifyNonEmptyImmediatelyConnectedKeys(selector);\n+        if (closeAfterFirstPoll) {\n+            selector.poll(0);\n+            verifyEmptyImmediatelyConnectedKeys(selector);\n+        }\n+        selector.close(id);\n+        verifySelectorEmpty(selector);\n+    }\n+\n     @Test\n     public void testCloseOldestConnectionWithOneStagedReceive() throws Exception {\n         verifyCloseOldestConnectionWithStagedReceives(1);\n@@ -410,8 +456,6 @@ private void verifyCloseOldestConnectionWithStagedReceives(int maxStagedReceives\n         assertTrue(\"Unexpected receive\", selector.completedReceives().isEmpty());\n     }\n \n-\n-\n     @Test\n     public void testMuteOnOOM() throws Exception {\n         //clean up default selector, replace it with one that uses a finite mem pool\n@@ -517,8 +561,11 @@ public void testConnectDisconnectDuringInSinglePoll() throws Exception {\n         expectLastCall().andThrow(new IOException());\n \n         SelectionKey selectionKey = control.createMock(SelectionKey.class);\n+        expect(kafkaChannel.selectionKey()).andStubReturn(selectionKey);\n         expect(selectionKey.channel()).andReturn(SocketChannel.open());\n         expect(selectionKey.readyOps()).andStubReturn(SelectionKey.OP_CONNECT);\n+        selectionKey.cancel();\n+        expectLastCall();\n \n         control.replay();\n \n@@ -528,6 +575,7 @@ public void testConnectDisconnectDuringInSinglePoll() throws Exception {\n \n         assertFalse(selector.connected().contains(kafkaChannel.id()));\n         assertTrue(selector.disconnected().containsKey(kafkaChannel.id()));\n+        assertNull(selectionKey.attachment());\n \n         control.verify();\n     }\n@@ -551,6 +599,7 @@ protected void connect(String node, InetSocketAddress serverAddr) throws IOExcep\n     private void blockingConnect(String node) throws IOException {\n         blockingConnect(node, new InetSocketAddress(\"localhost\", server.port));\n     }\n+\n     protected void blockingConnect(String node, InetSocketAddress serverAddr) throws IOException {\n         selector.connect(node, serverAddr, BUFFER_SIZE, BUFFER_SIZE);\n         while (!selector.connected().contains(node))\n@@ -589,21 +638,41 @@ private void sendAndReceive(String node, String requestPrefix, int startIndex, i\n         }\n     }\n \n-    private void verifySelectorEmpty() throws Exception {\n-        for (KafkaChannel channel : selector.channels())\n+    private void verifyNonEmptyImmediatelyConnectedKeys(Selector selector) throws Exception {\n+        Field field = Selector.class.getDeclaredField(\"immediatelyConnectedKeys\");\n+        field.setAccessible(true);\n+        Collection<?> immediatelyConnectedKeys = (Collection<?>) field.get(selector);\n+        assertFalse(immediatelyConnectedKeys.isEmpty());\n+    }\n+\n+    private void verifyEmptyImmediatelyConnectedKeys(Selector selector) throws Exception {\n+        Field field = Selector.class.getDeclaredField(\"immediatelyConnectedKeys\");\n+        ensureEmptySelectorField(selector, field);\n+    }\n+\n+    protected void verifySelectorEmpty() throws Exception {\n+        verifySelectorEmpty(this.selector);\n+    }\n+\n+    private void verifySelectorEmpty(Selector selector) throws Exception {\n+        for (KafkaChannel channel : selector.channels()) {\n             selector.close(channel.id());\n+            assertNull(channel.selectionKey().attachment());\n+        }\n         selector.poll(0);\n         selector.poll(0); // Poll a second time to clear everything\n-        for (Field field : selector.getClass().getDeclaredFields()) {\n-            field.setAccessible(true);\n-            Object obj = field.get(selector);\n-            if (obj instanceof Set)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Set<?>) obj).isEmpty());\n-            else if (obj instanceof Map)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Map<?, ?>) obj).isEmpty());\n-            else if (obj instanceof List)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((List<?>) obj).isEmpty());\n+        for (Field field : Selector.class.getDeclaredFields()) {\n+            ensureEmptySelectorField(selector, field);\n         }\n     }\n \n+    private void ensureEmptySelectorField(Selector selector, Field field) throws Exception {\n+        field.setAccessible(true);\n+        Object obj = field.get(selector);\n+        if (obj instanceof Collection)\n+            assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Collection<?>) obj).isEmpty());\n+        else if (obj instanceof Map)\n+            assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Map<?, ?>) obj).isEmpty());\n+    }\n+\n }",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java",
                "sha": "f1c6a5af1420befb23877c1a2cfcdb8b321b09d2",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java",
                "patch": "@@ -22,7 +22,9 @@\n import org.apache.kafka.common.security.auth.SecurityProtocol;\n import org.apache.kafka.common.utils.LogContext;\n import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.test.TestCondition;\n import org.apache.kafka.test.TestSslUtils;\n+import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -77,6 +79,30 @@ public SecurityProtocol securityProtocol() {\n         return SecurityProtocol.PLAINTEXT;\n     }\n \n+    @Test\n+    public void testDisconnectWithIntermediateBufferedBytes() throws Exception {\n+        int requestSize = 100 * 1024;\n+        final String node = \"0\";\n+        connect(node, new InetSocketAddress(\"localhost\", server.port));\n+        String request = TestUtils.randomString(requestSize);\n+        selector.send(createSend(node, request));\n+\n+        TestUtils.waitForCondition(new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                try {\n+                    selector.poll(0L);\n+                    return selector.channel(node).hasBytesBuffered();\n+                } catch (IOException e) {\n+                    throw new RuntimeException(e);\n+                }\n+            }\n+        }, 2000L, \"Failed to reach socket state with bytes buffered\");\n+\n+        selector.close(node);\n+        verifySelectorEmpty();\n+    }\n+\n     /**\n      * Renegotiation is not supported since it is potentially unsafe and it has been removed in TLS 1.3\n      */\n@@ -197,4 +223,5 @@ protected void connect(String node, InetSocketAddress serverAddr) throws IOExcep\n     private SslSender createSender(InetSocketAddress serverAddress, byte[] payload) {\n         return new SslSender(serverAddress, payload);\n     }\n+\n }",
                "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java",
                "sha": "dc062ea3b6857494b90fe3726be962b603018be9",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6260; Ensure selection keys are removed from all collections on socket close\n\nWhen a socket is closed, we must remove corresponding selection keys from internal collections. This fixes an NPE which is caused by attempting to access the selection key's attached channel after it had been cleared after disconnecting.\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>\n\nCloses #4276 from hachikuji/KAFKA-6260",
        "parent": "https://github.com/apache/kafka/commit/18e34482e685098ebef90233d78ae803f906633a",
        "repo": "kafka",
        "unit_tests": [
            "KafkaChannelTest.java",
            "SelectorTest.java",
            "SslTransportLayerTest.java"
        ]
    },
    "kafka_d06da1b": {
        "bug_id": "kafka_d06da1b",
        "commit": "https://github.com/apache/kafka/commit/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java?ref=d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34",
                "deletions": 5,
                "filename": "clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java",
                "patch": "@@ -80,11 +80,13 @@ public ConfigTransformerResult transform(Map<String, String> configs) {\n \n         // Collect the variables from the given configs that need transformation\n         for (Map.Entry<String, String> config : configs.entrySet()) {\n-            List<ConfigVariable> vars = getVars(config.getKey(), config.getValue(), DEFAULT_PATTERN);\n-            for (ConfigVariable var : vars) {\n-                Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(var.providerName, k -> new HashMap<>());\n-                Set<String> keys = keysByPath.computeIfAbsent(var.path, k -> new HashSet<>());\n-                keys.add(var.variable);\n+            if (config.getValue() != null) {\n+                List<ConfigVariable> vars = getVars(config.getKey(), config.getValue(), DEFAULT_PATTERN);\n+                for (ConfigVariable var : vars) {\n+                    Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(var.providerName, k -> new HashMap<>());\n+                    Set<String> keys = keysByPath.computeIfAbsent(var.path, k -> new HashSet<>());\n+                    keys.add(var.variable);\n+                }\n             }\n         }\n \n@@ -131,6 +133,9 @@ public ConfigTransformerResult transform(Map<String, String> configs) {\n     private static String replace(Map<String, Map<String, Map<String, String>>> lookupsByProvider,\n                                   String value,\n                                   Pattern pattern) {\n+        if (value == null) {\n+            return null;\n+        }\n         Matcher matcher = pattern.matcher(value);\n         StringBuilder builder = new StringBuilder();\n         int i = 0;",
                "raw_url": "https://github.com/apache/kafka/raw/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java",
                "sha": "f5a3737d3347552f582b0d1e025e0cf3470ec53a",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/kafka/blob/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java?ref=d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34",
                "deletions": 1,
                "filename": "clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n public class ConfigTransformerTest {\n@@ -37,6 +38,7 @@\n     public static final String TEST_PATH = \"testPath\";\n     public static final String TEST_RESULT = \"testResult\";\n     public static final String TEST_RESULT_WITH_TTL = \"testResultWithTTL\";\n+    public static final String TEST_RESULT_NO_PATH = \"testResultNoPath\";\n \n     private ConfigTransformer configTransformer;\n \n@@ -84,6 +86,24 @@ public void testSingleLevelOfIndirection() throws Exception {\n         assertEquals(\"${test:testPath:testResult}\", data.get(MY_KEY));\n     }\n \n+    @Test\n+    public void testReplaceVariableNoPath() throws Exception {\n+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, \"${test:testKey}\"));\n+        Map<String, String> data = result.data();\n+        Map<String, Long> ttls = result.ttls();\n+        assertEquals(TEST_RESULT_NO_PATH, data.get(MY_KEY));\n+        assertTrue(ttls.isEmpty());\n+    }\n+\n+    @Test\n+    public void testNullConfigValue() throws Exception {\n+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, null));\n+        Map<String, String> data = result.data();\n+        Map<String, Long> ttls = result.ttls();\n+        assertNull(data.get(MY_KEY));\n+        assertTrue(ttls.isEmpty());\n+    }\n+\n     public static class TestConfigProvider implements ConfigProvider {\n \n         public void configure(Map<String, ?> configs) {\n@@ -96,7 +116,7 @@ public ConfigData get(String path) {\n         public ConfigData get(String path, Set<String> keys) {\n             Map<String, String> data = new HashMap<>();\n             Long ttl = null;\n-            if (path.equals(TEST_PATH)) {\n+            if (TEST_PATH.equals(path)) {\n                 if (keys.contains(TEST_KEY)) {\n                     data.put(TEST_KEY, TEST_RESULT);\n                 }\n@@ -107,6 +127,10 @@ public ConfigData get(String path, Set<String> keys) {\n                 if (keys.contains(TEST_INDIRECTION)) {\n                     data.put(TEST_INDIRECTION, \"${test:testPath:testResult}\");\n                 }\n+            } else {\n+                if (keys.contains(TEST_KEY)) {\n+                    data.put(TEST_KEY, TEST_RESULT_NO_PATH);\n+                }\n             }\n             return new ConfigData(data, ttl);\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java",
                "sha": "e2b9f6b001cee3c2f1f9f2b5f576a98d5c9e6954",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7068: Handle null config values during transform (KIP-297)\n\nFix NPE when processing null config values during transform.\n\nAuthor: Robert Yokota <rayokota@gmail.com>\n\nReviewers: Magesh Nandakumar <magesh.n.kumar@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #5241 from rayokota/KIP-297-null-config-values",
        "parent": "https://github.com/apache/kafka/commit/be846d833caade74f1d0536ecf9d540855cde758",
        "repo": "kafka",
        "unit_tests": [
            "ConfigTransformerTest.java"
        ]
    },
    "kafka_d9369de": {
        "bug_id": "kafka_d9369de",
        "commit": "https://github.com/apache/kafka/commit/d9369de8f2c6435843fb7577d313bb24e3b09cba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba",
                "deletions": 1,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                "patch": "@@ -76,7 +76,9 @@\n     public static final String HEADER_CONVERTER_CLASS_CONFIG = WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG;\n     public static final String HEADER_CONVERTER_CLASS_DOC = WorkerConfig.HEADER_CONVERTER_CLASS_DOC;\n     public static final String HEADER_CONVERTER_CLASS_DISPLAY = \"Header converter class\";\n-    public static final String HEADER_CONVERTER_CLASS_DEFAULT = WorkerConfig.HEADER_CONVERTER_CLASS_DEFAULT;\n+    // The Connector config should not have a default for the header converter, since the absence of a config property means that\n+    // the worker config settings should be used. Thus, we set the default to null here.\n+    public static final String HEADER_CONVERTER_CLASS_DEFAULT = null;\n \n     public static final String TASKS_MAX_CONFIG = \"tasks.max\";\n     private static final String TASKS_MAX_DOC = \"Maximum number of tasks to use for this connector.\";",
                "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                "sha": "fd05af57a648c2682879896d91a27a4f60e8b523",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba",
                "deletions": 0,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "patch": "@@ -397,12 +397,21 @@ public boolean startTask(\n             );\n             if (keyConverter == null) {\n                 keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the key converter {} for task {} using the worker config\", keyConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the key converter {} for task {} using the connector config\", keyConverter.getClass(), id);\n             }\n             if (valueConverter == null) {\n                 valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the value converter {} for task {} using the worker config\", valueConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the value converter {} for task {} using the connector config\", valueConverter.getClass(), id);\n             }\n             if (headerConverter == null) {\n                 headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the header converter {} for task {} using the worker config\", headerConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the header converter {} for task {} using the connector config\", headerConverter.getClass(), id);\n             }\n \n             workerTask = buildWorkerTask(connConfig, id, task, statusListener, initialState, keyConverter, valueConverter, headerConverter, connectorLoader);",
                "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "sha": "1c6465855ff7bd3392a7911eaffa3017850fdac5",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba",
                "deletions": 5,
                "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java",
                "patch": "@@ -234,6 +234,8 @@ public Converter newConverter(AbstractConfig config, String classPropertyName, C\n         // Configure the Converter using only the old configuration mechanism ...\n         String configPrefix = classPropertyName + \".\";\n         Map<String, Object> converterConfig = config.originalsWithPrefix(configPrefix);\n+        log.debug(\"Configuring the {} converter with configuration:{}{}\",\n+                  isKeyConverter ? \"key\" : \"value\", System.lineSeparator(), converterConfig);\n         plugin.configure(converterConfig, isKeyConverter);\n         return plugin;\n     }\n@@ -249,20 +251,21 @@ public Converter newConverter(AbstractConfig config, String classPropertyName, C\n      * @throws ConnectException if the {@link HeaderConverter} implementation class could not be found\n      */\n     public HeaderConverter newHeaderConverter(AbstractConfig config, String classPropertyName, ClassLoaderUsage classLoaderUsage) {\n-        if (!config.originals().containsKey(classPropertyName)) {\n-            // This configuration does not define the header converter via the specified property name\n-            return null;\n-        }\n         HeaderConverter plugin = null;\n         switch (classLoaderUsage) {\n             case CURRENT_CLASSLOADER:\n+                if (!config.originals().containsKey(classPropertyName)) {\n+                    // This connector configuration does not define the header converter via the specified property name\n+                    return null;\n+                }\n                 // Attempt to load first with the current classloader, and plugins as a fallback.\n                 // Note: we can't use config.getConfiguredInstance because we have to remove the property prefixes\n                 // before calling config(...)\n                 plugin = getInstance(config, classPropertyName, HeaderConverter.class);\n                 break;\n             case PLUGINS:\n-                // Attempt to load with the plugin class loader, which uses the current classloader as a fallback\n+                // Attempt to load with the plugin class loader, which uses the current classloader as a fallback.\n+                // Note that there will always be at least a default header converter for the worker\n                 String converterClassOrAlias = config.getClass(classPropertyName).getName();\n                 Class<? extends HeaderConverter> klass;\n                 try {\n@@ -288,6 +291,7 @@ public HeaderConverter newHeaderConverter(AbstractConfig config, String classPro\n         String configPrefix = classPropertyName + \".\";\n         Map<String, Object> converterConfig = config.originalsWithPrefix(configPrefix);\n         converterConfig.put(ConverterConfig.TYPE_CONFIG, ConverterType.HEADER.getName());\n+        log.debug(\"Configuring the header converter with configuration:{}{}\", System.lineSeparator(), converterConfig);\n         plugin.configure(converterConfig);\n         return plugin;\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java",
                "sha": "f4cd2ba14b0de6d95efeb33c2d023e692951a73a",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba",
                "deletions": 17,
                "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.kafka.connect.storage.ConverterConfig;\n import org.apache.kafka.connect.storage.ConverterType;\n import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.apache.kafka.connect.storage.SimpleHeaderConverter;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -39,18 +40,31 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n \n public class PluginsTest {\n \n-    private static Map<String, String> props;\n+    private static Map<String, String> pluginProps;\n     private static Plugins plugins;\n+    private Map<String, String> props;\n     private AbstractConfig config;\n     private TestConverter converter;\n     private TestHeaderConverter headerConverter;\n \n     @BeforeClass\n     public static void beforeAll() {\n-        props = new HashMap<>();\n+        pluginProps = new HashMap<>();\n+\n+        // Set up the plugins to have no additional plugin directories.\n+        // This won't allow us to test classpath isolation, but it will allow us to test some of the utility methods.\n+        pluginProps.put(WorkerConfig.PLUGIN_PATH_CONFIG, \"\");\n+        plugins = new Plugins(pluginProps);\n+    }\n+\n+    @Before\n+    public void setup() {\n+        props = new HashMap<>(pluginProps);\n         props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestConverter.class.getName());\n         props.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, TestConverter.class.getName());\n         props.put(\"key.converter.\" + JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"true\");\n@@ -66,14 +80,10 @@ public static void beforeAll() {\n         props.put(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, TestHeaderConverter.class.getName());\n         props.put(\"header.converter.extra.config\", \"baz\");\n \n-        // Set up the plugins to have no additional plugin directories.\n-        // This won't allow us to test classpath isolation, but it will allow us to test some of the utility methods.\n-        props.put(WorkerConfig.PLUGIN_PATH_CONFIG, \"\");\n-        plugins = new Plugins(props);\n+        createConfig();\n     }\n \n-    @Before\n-    public void setup() {\n+    protected void createConfig() {\n         this.config = new TestableWorkerConfig(props);\n     }\n \n@@ -104,23 +114,55 @@ public void shouldInstantiateAndConfigureInternalConverters() {\n     }\n \n     @Test\n-    public void shouldInstantiateAndConfigureHeaderConverter() {\n-        instantiateAndConfigureHeaderConverter(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);\n+    public void shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader() {\n+        assertNotNull(props.get(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG));\n+        HeaderConverter headerConverter = plugins.newHeaderConverter(config,\n+                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof TestHeaderConverter);\n+        this.headerConverter = (TestHeaderConverter) headerConverter;\n+\n+        // Validate extra configs got passed through to overridden converters\n+        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);\n+        assertEquals(\"baz\", this.headerConverter.configs.get(\"extra.config\"));\n+\n+        headerConverter = plugins.newHeaderConverter(config,\n+                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                     ClassLoaderUsage.PLUGINS);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof TestHeaderConverter);\n+        this.headerConverter = (TestHeaderConverter) headerConverter;\n+\n         // Validate extra configs got passed through to overridden converters\n-        assertConverterType(ConverterType.HEADER, headerConverter.configs);\n-        assertEquals(\"baz\", headerConverter.configs.get(\"extra.config\"));\n+        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);\n+        assertEquals(\"baz\", this.headerConverter.configs.get(\"extra.config\"));\n+    }\n+\n+    @Test\n+    public void shouldInstantiateAndConfigureDefaultHeaderConverter() {\n+        props.remove(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);\n+        createConfig();\n+\n+        // Because it's not explicitly set on the supplied configuration, the logic to use the current classloader for the connector\n+        // will exit immediately, and so this method always returns null\n+        HeaderConverter headerConverter = plugins.newHeaderConverter(config,\n+                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);\n+        assertNull(headerConverter);\n+        // But we should always find it (or the worker's default) when using the plugins classloader ...\n+        headerConverter = plugins.newHeaderConverter(config,\n+                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                     ClassLoaderUsage.PLUGINS);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof SimpleHeaderConverter);\n     }\n \n     protected void instantiateAndConfigureConverter(String configPropName, ClassLoaderUsage classLoaderUsage) {\n         converter = (TestConverter) plugins.newConverter(config, configPropName, classLoaderUsage);\n         assertNotNull(converter);\n     }\n \n-    protected void instantiateAndConfigureHeaderConverter(String configPropName) {\n-        headerConverter = (TestHeaderConverter) plugins.newHeaderConverter(config, configPropName, ClassLoaderUsage.CURRENT_CLASSLOADER);\n-        assertNotNull(headerConverter);\n-    }\n-\n     protected void assertConverterType(ConverterType type, Map<String, ?> props) {\n         assertEquals(type.getName(), props.get(ConverterConfig.TYPE_CONFIG));\n     }",
                "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java",
                "sha": "a9a944fa3604cdb34033b05a50bb60a4264f80c8",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6728: Corrected the worker\u2019s instantiation of the HeaderConverter\n\n## Summary of the problem\nWhen the `header.converter` is not specified in the worker config or the connector config, a bug in the `Plugins` test causes it to never instantiate the `HeaderConverter` instance, even though there is a default value.\n\nThis is a problem as soon as the connector deals with headers, either in records created by a source connector or in messages on the Kafka topics consumed by a sink connector. As soon as that happens, a NPE occurs.\n\nA workaround is to explicitly set the `header.converter` configuration property, but this was added in AK 1.1 and thus means that upgrading to AK 1.1 will not be backward compatible and will require this configuration change.\n\n## The Changes\n\nThe `Plugins.newHeaderConverter` methods were always returning null if the `header.converter` configuration value was not specified in the supplied connector or worker configuration. Thus, even though the `header.converter` property has a default, it was never being used.\n\nThe fix was to only check whether a `header.converter` property was specified when the connector configuration was being used, and if no such property exists in the connector configuration to return null. Then, when the worker configuration is being used, the method simply gets the `header.converter` value (or the default if no value was explicitly set).\n\nAlso, the ConnectorConfig had the same default value for the `header.converter` property as the WorkerConfig, but this resulted in very confusing log messages that implied the default header converter should be used even when the worker config specified the `header.converter` value. By removing the default, the log messages now make sense, and the Worker still properly instantiates the correct header converter.\n\nFinally, updated comments and added log messages to make it more clear which converters are being used and how they are being converted.\n\n## Testing\n\nSeveral new unit tests for `Plugins.newHeaderConverter` were added to check the various behavior. Additionally, the runtime JAR with these changes was built and inserted into an AK 1.1 installation, and a source connector was manually tested with 8 different combinations of settings for the `header.converter` configuration:\n\n1. default value\n1. worker configuration has `header.converter` explicitly set to the default\n1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin\n1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n1. connector configuration has `header.converter` explicitly set to the default\n1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin\n1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n1. worker configuration has `header.converter` explicitly set to the default, and the connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n\nThe worker created the correct `HeaderConverter` implementation with the correct configuration in all of these tests.\n\nFinally, the default configuration was used with the aforementioned custom source connector that generated records with headers, and an S3 connector that consumes the records with headers (but didn't do anything with them). This test also passed.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #4815 from rhauch/kafka-6728",
        "parent": "https://github.com/apache/kafka/commit/67077ebbcf24448e48d6223dc7090d5d94ea7780",
        "repo": "kafka",
        "unit_tests": [
            "ConnectorConfigTest.java",
            "WorkerTest.java",
            "PluginsTest.java",
            "TestPlugins.java"
        ]
    },
    "kafka_e2ec2d7": {
        "bug_id": "kafka_e2ec2d7",
        "commit": "https://github.com/apache/kafka/commit/e2ec2d79c8d5adefc0c764583cec47144dbc5705",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/checkstyle/suppressions.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705",
                "deletions": 1,
                "filename": "checkstyle/suppressions.xml",
                "patch": "@@ -70,7 +70,7 @@\n               files=\"MockAdminClient.java\"/>\n \n     <suppress checks=\"JavaNCSS\"\n-              files=\"RequestResponseTest.java\"/>\n+              files=\"RequestResponseTest.java|FetcherTest.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n               files=\"MemoryRecordsTest|MetricsTest\"/>",
                "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/checkstyle/suppressions.xml",
                "sha": "2fa499c21655e85a971cf4fd27c0cb76bd84a820",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705",
                "deletions": 7,
                "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java",
                "patch": "@@ -414,7 +414,7 @@ private ListOffsetResult fetchOffsetsByTimes(Map<TopicPartition, Long> timestamp\n                 if (value.partitionsToRetry.isEmpty())\n                     return result;\n \n-                remainingToSearch.keySet().removeAll(result.fetchedOffsets.keySet());\n+                remainingToSearch.keySet().retainAll(value.partitionsToRetry);\n             } else if (!future.isRetriable()) {\n                 throw future.exception();\n             }\n@@ -575,7 +575,7 @@ private void resetOffsetsAsync(Map<TopicPartition, Long> partitionResetTimestamp\n             metadata.add(tp.topic());\n \n         Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode =\n-                groupListOffsetRequests(partitionResetTimestamps);\n+                groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());\n         for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {\n             Node node = entry.getKey();\n             final Map<TopicPartition, ListOffsetRequest.PartitionData> resetTimestamps = entry.getValue();\n@@ -624,19 +624,19 @@ public void onFailure(RuntimeException e) {\n         for (TopicPartition tp : timestampsToSearch.keySet())\n             metadata.add(tp.topic());\n \n+        final Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode =\n-                groupListOffsetRequests(timestampsToSearch);\n+                groupListOffsetRequests(timestampsToSearch, partitionsToRetry);\n         if (timestampsToSearchByNode.isEmpty())\n             return RequestFuture.failure(new StaleMetadataException());\n \n         final RequestFuture<ListOffsetResult> listOffsetRequestsFuture = new RequestFuture<>();\n         final Map<TopicPartition, OffsetData> fetchedTimestampOffsets = new HashMap<>();\n-        final Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         final AtomicInteger remainingResponses = new AtomicInteger(timestampsToSearchByNode.size());\n \n         for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {\n             RequestFuture<ListOffsetResult> future =\n-                    sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);\n+                sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);\n             future.addListener(new RequestFutureListener<ListOffsetResult>() {\n                 @Override\n                 public void onSuccess(ListOffsetResult partialResult) {\n@@ -663,8 +663,16 @@ public void onFailure(RuntimeException e) {\n         return listOffsetRequestsFuture;\n     }\n \n+    /**\n+     * Groups timestamps to search by node for topic partitions in `timestampsToSearch` that have\n+     * leaders available. Topic partitions from `timestampsToSearch` that do not have their leader\n+     * available are added to `partitionsToRetry`\n+     * @param timestampsToSearch The mapping from partitions ot the target timestamps\n+     * @param partitionsToRetry A set of topic partitions that will be extended with partitions\n+     *                          that need metadata update or re-connect to the leader.\n+     */\n     private Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> groupListOffsetRequests(\n-            Map<TopicPartition, Long> timestampsToSearch) {\n+            Map<TopicPartition, Long> timestampsToSearch, Set<TopicPartition> partitionsToRetry) {\n         final Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode = new HashMap<>();\n         for (Map.Entry<TopicPartition, Long> entry: timestampsToSearch.entrySet()) {\n             TopicPartition tp  = entry.getKey();\n@@ -673,17 +681,20 @@ public void onFailure(RuntimeException e) {\n                 metadata.add(tp.topic());\n                 log.debug(\"Leader for partition {} is unknown for fetching offset\", tp);\n                 metadata.requestUpdate();\n+                partitionsToRetry.add(tp);\n             } else if (info.leader() == null) {\n                 log.debug(\"Leader for partition {} is unavailable for fetching offset\", tp);\n                 metadata.requestUpdate();\n+                partitionsToRetry.add(tp);\n             } else if (client.isUnavailable(info.leader())) {\n                 client.maybeThrowAuthFailure(info.leader());\n \n                 // The connection has failed and we need to await the blackout period before we can\n                 // try again. No need to request a metadata update since the disconnect will have\n                 // done so already.\n                 log.debug(\"Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires\",\n-                        info.leader(), tp);\n+                          info.leader(), tp);\n+                partitionsToRetry.add(tp);\n             } else {\n                 Node node = info.leader();\n                 Map<TopicPartition, ListOffsetRequest.PartitionData> topicData =",
                "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java",
                "sha": "c84dd6f9b7b5adbadcb6c3fe4358e46421c48b64",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "patch": "@@ -108,6 +108,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -1938,6 +1939,48 @@ public void testGetOffsetsForTimes() {\n         testGetOffsetsForTimesWithError(Errors.BROKER_NOT_AVAILABLE, Errors.NONE, 10L, 100L, 10L, 100L);\n     }\n \n+    @Test\n+    public void testGetOffsetsForTimesWhenSomeTopicPartitionLeadersNotKnownInitially() {\n+        final String anotherTopic = \"another-topic\";\n+        final TopicPartition t2p0 = new TopicPartition(anotherTopic, 0);\n+\n+        client.reset();\n+\n+        // Metadata initially has one topic\n+        Cluster cluster = TestUtils.clusterWith(3, topicName, 2);\n+        metadata.update(cluster, Collections.<String>emptySet(), time.milliseconds());\n+\n+        // The first metadata refresh should contain one topic\n+        client.prepareMetadataUpdate(cluster, Collections.<String>emptySet(), false);\n+        client.prepareResponseFrom(listOffsetResponse(tp0, Errors.NONE, 1000L, 11L), cluster.leaderFor(tp0));\n+        client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, 1000L, 32L), cluster.leaderFor(tp1));\n+\n+        // Second metadata refresh should contain two topics\n+        Map<String, Integer> partitionNumByTopic = new HashMap<>();\n+        partitionNumByTopic.put(topicName, 2);\n+        partitionNumByTopic.put(anotherTopic, 1);\n+        Cluster updatedCluster = TestUtils.clusterWith(3, partitionNumByTopic);\n+        client.prepareMetadataUpdate(updatedCluster, Collections.<String>emptySet(), false);\n+        client.prepareResponseFrom(listOffsetResponse(t2p0, Errors.NONE, 1000L, 54L), cluster.leaderFor(t2p0));\n+\n+        Map<TopicPartition, Long> timestampToSearch = new HashMap<>();\n+        timestampToSearch.put(tp0, ListOffsetRequest.LATEST_TIMESTAMP);\n+        timestampToSearch.put(tp1, ListOffsetRequest.LATEST_TIMESTAMP);\n+        timestampToSearch.put(t2p0, ListOffsetRequest.LATEST_TIMESTAMP);\n+        Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+            fetcher.offsetsByTimes(timestampToSearch, time.timer(Long.MAX_VALUE));\n+\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + tp0,\n+                      offsetAndTimestampMap.get(tp0));\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + tp1,\n+                      offsetAndTimestampMap.get(tp1));\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + t2p0,\n+                      offsetAndTimestampMap.get(t2p0));\n+        assertEquals(11L, offsetAndTimestampMap.get(tp0).offset());\n+        assertEquals(32L, offsetAndTimestampMap.get(tp1).offset());\n+        assertEquals(54L, offsetAndTimestampMap.get(t2p0).offset());\n+    }\n+\n     @Test(expected = TimeoutException.class)\n     public void testBatchedListOffsetsMetadataErrors() {\n         Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();",
                "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
                "sha": "3bf3deba1aa6c69227854ce9603e7759a3ccb317",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705",
                "deletions": 10,
                "filename": "core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala",
                "patch": "@@ -292,12 +292,8 @@ object ConsumerGroupCommand extends Logging {\n       }\n \n       getLogEndOffsets(topicPartitions).map {\n-        logEndOffsetResult =>\n-          logEndOffsetResult._2 match {\n-            case LogOffsetResult.LogOffset(logEndOffset) => getDescribePartitionResult(logEndOffsetResult._1, Some(logEndOffset))\n-            case LogOffsetResult.Unknown => getDescribePartitionResult(logEndOffsetResult._1, None)\n-            case LogOffsetResult.Ignore => null\n-          }\n+        case (topicPartition, LogOffsetResult.LogOffset(offset)) => getDescribePartitionResult(topicPartition, Some(offset))\n+        case (topicPartition, _) => getDescribePartitionResult(topicPartition, None)\n       }.toArray\n     }\n \n@@ -399,16 +395,20 @@ object ConsumerGroupCommand extends Logging {\n     private def getLogEndOffsets(topicPartitions: Seq[TopicPartition]): Map[TopicPartition, LogOffsetResult] = {\n       val offsets = getConsumer.endOffsets(topicPartitions.asJava)\n       topicPartitions.map { topicPartition =>\n-        val logEndOffset = offsets.get(topicPartition)\n-        topicPartition -> LogOffsetResult.LogOffset(logEndOffset)\n+        Option(offsets.get(topicPartition)) match {\n+          case Some(logEndOffset) => topicPartition -> LogOffsetResult.LogOffset(logEndOffset)\n+          case _ => topicPartition -> LogOffsetResult.Unknown\n+        }\n       }.toMap\n     }\n \n     private def getLogStartOffsets(topicPartitions: Seq[TopicPartition]): Map[TopicPartition, LogOffsetResult] = {\n       val offsets = getConsumer.beginningOffsets(topicPartitions.asJava)\n       topicPartitions.map { topicPartition =>\n-        val logStartOffset = offsets.get(topicPartition)\n-        topicPartition -> LogOffsetResult.LogOffset(logStartOffset)\n+        Option(offsets.get(topicPartition)) match {\n+          case Some(logStartOffset) => topicPartition -> LogOffsetResult.LogOffset(logStartOffset)\n+          case _ => topicPartition -> LogOffsetResult.Unknown\n+        }\n       }.toMap\n     }\n ",
                "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala",
                "sha": "c0f6797eddd073a029d0042f94fb0158ff1622b2",
                "status": "modified"
            }
        ],
        "message": "KAFKA-7044; Fix Fetcher.fetchOffsetsByTimes and NPE in describe consumer group (#5627)\n\nA call to `kafka-consumer-groups --describe --group ...` can result in NullPointerException for two reasons:\r\n1)  `Fetcher.fetchOffsetsByTimes()` may return too early, without sending list offsets request for topic partitions that are not in cached metadata.\r\n2) `ConsumerGroupCommand.getLogEndOffsets()` and `getLogStartOffsets()` assumed that endOffsets()/beginningOffsets() which eventually call Fetcher.fetchOffsetsByTimes(), would return a map with all the topic partitions passed to endOffsets()/beginningOffsets() and that values are not null. Because of (1), null values were possible if some of the topic partitions were already known (in metadata cache) and some not (metadata cache did not have entries for some of the topic partitions). However, even with fixing (1), endOffsets()/beginningOffsets() may return a map with some topic partitions missing, when list offset request returns a non-retriable error. This happens in corner cases such as message format on broker is before 0.10, or maybe in cases of some other errors. \r\n\r\nTesting:\r\n-- added unit test to verify fix in Fetcher.fetchOffsetsByTimes() \r\n-- did some manual testing with `kafka-consumer-groups --describe`, causing NPE. Was not able to reproduce any NPE cases with DescribeConsumerGroupTest.scala,\r\n\r\nReviewers: Jason Gustafson <jason@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/79ad9026a667469a2013ce82961c0c90f3bb0877",
        "repo": "kafka",
        "unit_tests": [
            "FetcherTest.java"
        ]
    },
    "kafka_e32dcb9": {
        "bug_id": "kafka_e32dcb9",
        "commit": "https://github.com/apache/kafka/commit/e32dcb9a669bc354cc97cb45f14f0dbad9657693",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java?ref=e32dcb9a669bc354cc97cb45f14f0dbad9657693",
                "deletions": 1,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java",
                "patch": "@@ -163,7 +163,10 @@ public boolean isOpen() {\n     }\n \n     private byte[] getInternal(final Bytes key) {\n-        final LRUCacheEntry entry = cache.get(cacheName, key);\n+        LRUCacheEntry entry = null;\n+        if (cache != null) {\n+            entry = cache.get(cacheName, key);\n+        }\n         if (entry == null) {\n             final byte[] rawValue = underlying.get(key);\n             if (rawValue == null) {",
                "raw_url": "https://github.com/apache/kafka/raw/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java",
                "sha": "16684e39c1ac2e83bbe126678688c43478093986",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/kafka/blob/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java?ref=e32dcb9a669bc354cc97cb45f14f0dbad9657693",
                "deletions": 0,
                "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java",
                "patch": "@@ -160,6 +160,9 @@ public synchronized void put(final Bytes key, final byte[] value, final long tim\n         validateStoreOpen();\n         final Bytes bytesKey = WindowKeySchema.toStoreKeyBinary(key, timestamp, 0);\n         final Bytes cacheKey = cacheFunction.cacheKey(bytesKey);\n+        if (cache == null) {\n+            return underlying.fetch(key, timestamp);\n+        }\n         final LRUCacheEntry entry = cache.get(name, cacheKey);\n         if (entry == null) {\n             return underlying.fetch(key, timestamp);",
                "raw_url": "https://github.com/apache/kafka/raw/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java",
                "sha": "1d0455b2b6f164f7c2f7739e90b2fa269b8694ff",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6878: NPE when querying global state store not in READY state (#4978)\n\nCheck whether cache is null before retrieving from cache.\r\n\r\nReviewers: Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/5ca9ed5ede4f03cdef54cdbce70be3fdf052157d",
        "repo": "kafka",
        "unit_tests": [
            "CachingKeyValueStoreTest.java",
            "CachingWindowStoreTest.java"
        ]
    },
    "kafka_fc4fea6": {
        "bug_id": "kafka_fc4fea6",
        "commit": "https://github.com/apache/kafka/commit/fc4fea6761986749f0ac640868d9b4d2a552eb62",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/kafka/blob/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java?ref=fc4fea6761986749f0ac640868d9b4d2a552eb62",
                "deletions": 11,
                "filename": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java",
                "patch": "@@ -35,7 +35,7 @@\n import java.util.Map;\n \n import static org.apache.kafka.connect.transforms.util.Requirements.requireMap;\n-import static org.apache.kafka.connect.transforms.util.Requirements.requireStruct;\n+import static org.apache.kafka.connect.transforms.util.Requirements.requireStructOrNull;\n \n public abstract class Flatten<R extends ConnectRecord<R>> implements Transformation<R> {\n \n@@ -136,20 +136,24 @@ private void applySchemaless(Map<String, Object> originalRecord, String fieldNam\n     }\n \n     private R applyWithSchema(R record) {\n-        final Struct value = requireStruct(operatingValue(record), PURPOSE);\n+        final Struct value = requireStructOrNull(operatingValue(record), PURPOSE);\n \n-        Schema updatedSchema = schemaUpdateCache.get(value.schema());\n+        Schema schema = operatingSchema(record);\n+        Schema updatedSchema = schemaUpdateCache.get(schema);\n         if (updatedSchema == null) {\n-            final SchemaBuilder builder = SchemaUtil.copySchemaBasics(value.schema(), SchemaBuilder.struct());\n-            Struct defaultValue = (Struct) value.schema().defaultValue();\n-            buildUpdatedSchema(value.schema(), \"\", builder, value.schema().isOptional(), defaultValue);\n+            final SchemaBuilder builder = SchemaUtil.copySchemaBasics(schema, SchemaBuilder.struct());\n+            Struct defaultValue = (Struct) schema.defaultValue();\n+            buildUpdatedSchema(schema, \"\", builder, schema.isOptional(), defaultValue);\n             updatedSchema = builder.build();\n-            schemaUpdateCache.put(value.schema(), updatedSchema);\n+            schemaUpdateCache.put(schema, updatedSchema);\n+        }\n+        if (value == null) {\n+            return newRecord(record, updatedSchema, null);\n+        } else {\n+            final Struct updatedValue = new Struct(updatedSchema);\n+            buildWithSchema(value, \"\", updatedValue);\n+            return newRecord(record, updatedSchema, updatedValue);\n         }\n-\n-        final Struct updatedValue = new Struct(updatedSchema);\n-        buildWithSchema(value, \"\", updatedValue);\n-        return newRecord(record, updatedSchema, updatedValue);\n     }\n \n     /**\n@@ -216,6 +220,9 @@ private Schema convertFieldSchema(Schema orig, boolean optional, Object defaultF\n     }\n \n     private void buildWithSchema(Struct record, String fieldNamePrefix, Struct newRecord) {\n+        if (record == null) {\n+            return;\n+        }\n         for (Field field : record.schema().fields()) {\n             final String fieldName = fieldName(fieldNamePrefix, field.name());\n             switch (field.schema().type()) {",
                "raw_url": "https://github.com/apache/kafka/raw/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java",
                "sha": "d7d21445d73d818b3ebd56c8d3b7e2691b5a55e6",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/kafka/blob/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java?ref=fc4fea6761986749f0ac640868d9b4d2a552eb62",
                "deletions": 0,
                "filename": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java",
                "patch": "@@ -182,6 +182,46 @@ public void testOptionalFieldStruct() {\n         assertNull(transformedStruct.get(\"B.opt_int32\"));\n     }\n \n+    @Test\n+    public void testOptionalStruct() {\n+        xformValue.configure(Collections.<String, String>emptyMap());\n+\n+        SchemaBuilder builder = SchemaBuilder.struct().optional();\n+        builder.field(\"opt_int32\", Schema.OPTIONAL_INT32_SCHEMA);\n+        Schema schema = builder.build();\n+\n+        SourceRecord transformed = xformValue.apply(new SourceRecord(null, null,\n+            \"topic\", 0,\n+            schema, null));\n+\n+        assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());\n+        assertNull(transformed.value());\n+    }\n+\n+    @Test\n+    public void testOptionalNestedStruct() {\n+        xformValue.configure(Collections.<String, String>emptyMap());\n+\n+        SchemaBuilder builder = SchemaBuilder.struct().optional();\n+        builder.field(\"opt_int32\", Schema.OPTIONAL_INT32_SCHEMA);\n+        Schema supportedTypesSchema = builder.build();\n+\n+        builder = SchemaBuilder.struct();\n+        builder.field(\"B\", supportedTypesSchema);\n+        Schema oneLevelNestedSchema = builder.build();\n+\n+        Struct oneLevelNestedStruct = new Struct(oneLevelNestedSchema);\n+        oneLevelNestedStruct.put(\"B\", null);\n+\n+        SourceRecord transformed = xformValue.apply(new SourceRecord(null, null,\n+            \"topic\", 0,\n+            oneLevelNestedSchema, oneLevelNestedStruct));\n+\n+        assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());\n+        Struct transformedStruct = (Struct) transformed.value();\n+        assertNull(transformedStruct.get(\"B.opt_int32\"));\n+    }\n+\n     @Test\n     public void testOptionalFieldMap() {\n         xformValue.configure(Collections.<String, String>emptyMap());",
                "raw_url": "https://github.com/apache/kafka/raw/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java",
                "sha": "2e7be9562cb26111e5344ce4ede8bfc68a244a9c",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6605: Fix NPE in Flatten when optional Struct is null (#5705)\n\nCorrect the Flatten SMT to properly handle null key or value `Struct` instances.\r\n\r\nAuthor: Michal Borowiecki <michal.borowiecki@openbet.com>\r\nReviewers: Arjun Satish <arjun@confluent.io>, Robert Yokota <rayokota@gmail.com>, Randall Hauch <rhauch@gmail.com>",
        "parent": "https://github.com/apache/kafka/commit/8cabd44e1d35d3cf3f865d8a4eee8c04228e0d75",
        "repo": "kafka",
        "unit_tests": [
            "FlattenTest.java"
        ]
    },
    "kafka_feab5a3": {
        "bug_id": "kafka_feab5a3",
        "commit": "https://github.com/apache/kafka/commit/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/kafka/blob/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java?ref=feab5a374a33a7b7b8e96c6a88b872c4db33dcf1",
                "deletions": 2,
                "filename": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java",
                "patch": "@@ -170,10 +170,10 @@ public ApiException exception() {\n     }\n \n     /**\n-     * Returns the class name of the exception\n+     * Returns the class name of the exception or null if this is {@code Errors.NONE}.\n      */\n     public String exceptionName() {\n-        return exception.getClass().getName();\n+        return exception == null ? null : exception.getClass().getName();\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/kafka/raw/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java",
                "sha": "bd7310ba4551f1e9d2ce3ebfcdc76ec3931adf1f",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/kafka/blob/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java?ref=feab5a374a33a7b7b8e96c6a88b872c4db33dcf1",
                "deletions": 0,
                "filename": "clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java",
                "patch": "@@ -77,4 +77,14 @@ public void testForExceptionDefault() {\n         assertEquals(\"forException should default to unknown\", Errors.UNKNOWN, error);\n     }\n \n+    @Test\n+    public void testExceptionName() {\n+        String exceptionName = Errors.UNKNOWN.exceptionName();\n+        assertEquals(\"org.apache.kafka.common.errors.UnknownServerException\", exceptionName);\n+        exceptionName = Errors.NONE.exceptionName();\n+        assertNull(exceptionName);\n+        exceptionName = Errors.INVALID_TOPIC_EXCEPTION.exceptionName();\n+        assertEquals(\"org.apache.kafka.common.errors.InvalidTopicException\", exceptionName);\n+    }\n+\n }",
                "raw_url": "https://github.com/apache/kafka/raw/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java",
                "sha": "e198e739ac96daca7923a0fb4a64e4c98429c1aa",
                "status": "modified"
            }
        ],
        "message": "KAFKA-3781; Errors.exceptionName() can throw NPE\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Grant Henke <granthenke@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #1476 from ijuma/kafka-3781-exception-name-npe",
        "parent": "https://github.com/apache/kafka/commit/f643d1b75d17bb27a378c7e66fcc49607454e445",
        "repo": "kafka",
        "unit_tests": [
            "ErrorsTest.java"
        ]
    },
    "kafka_ff96d57": {
        "bug_id": "kafka_ff96d57",
        "commit": "https://github.com/apache/kafka/commit/ff96d574371811c75f4f454847f67508d1de98c0",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java?ref=ff96d574371811c75f4f454847f67508d1de98c0",
                "deletions": 9,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java",
                "patch": "@@ -36,17 +36,18 @@\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n \n     final File baseDir;\n-    final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>();\n-\n+    private final boolean eosEnabled;\n     OffsetCheckpoint checkpoint;\n \n+    final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>();\n     final Map<String, StateStore> stores = new LinkedHashMap<>();\n     final Map<String, StateStore> globalStores = new LinkedHashMap<>();\n \n-    AbstractStateManager(final File baseDir) {\n+    AbstractStateManager(final File baseDir,\n+                         final boolean eosEnabled) {\n         this.baseDir = baseDir;\n+        this.eosEnabled = eosEnabled;\n         this.checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-\n     }\n \n     public void reinitializeStateStoresForPartitions(final Logger log,\n@@ -62,11 +63,14 @@ public void reinitializeStateStoresForPartitions(final Logger log,\n             checkpointableOffsets.remove(topicPartition);\n             storeToBeReinitialized.add(changelogTopicToStore.get(topicPartition.topic()));\n         }\n-        try {\n-            checkpoint.write(checkpointableOffsets);\n-        } catch (final IOException fatalException) {\n-            log.error(\"Failed to write offset checkpoint file to {} while re-initializing {}: {}\", checkpoint, stateStores, fatalException);\n-            throw new StreamsException(\"Failed to reinitialize global store.\", fatalException);\n+\n+        if (!eosEnabled) {\n+            try {\n+                checkpoint.write(checkpointableOffsets);\n+            } catch (final IOException fatalException) {\n+                log.error(\"Failed to write offset checkpoint file to {} while re-initializing {}: {}\", checkpoint, stateStores, fatalException);\n+                throw new StreamsException(\"Failed to reinitialize global store.\", fatalException);\n+            }\n         }\n \n         for (final Map.Entry<String, StateStore> entry : storesCopy.entrySet()) {",
                "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java",
                "sha": "66ddec950c8d9809f8d87ef93dcf79feba06ed6a",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java?ref=ff96d574371811c75f4f454847f67508d1de98c0",
                "deletions": 12,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java",
                "patch": "@@ -69,7 +69,7 @@ public GlobalStateManagerImpl(final LogContext logContext,\n                                   final StateDirectory stateDirectory,\n                                   final StateRestoreListener stateRestoreListener,\n                                   final StreamsConfig config) {\n-        super(stateDirectory.globalStateDir());\n+        super(stateDirectory.globalStateDir(), StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)));\n \n         this.log = logContext.logger(GlobalStateManagerImpl.class);\n         this.topology = topology;\n@@ -92,16 +92,16 @@ public void setGlobalProcessorContext(final InternalProcessorContext processorCo\n             if (!stateDirectory.lockGlobalState()) {\n                 throw new LockException(String.format(\"Failed to lock the global state directory: %s\", baseDir));\n             }\n-        } catch (IOException e) {\n+        } catch (final IOException e) {\n             throw new LockException(String.format(\"Failed to lock the global state directory: %s\", baseDir));\n         }\n \n         try {\n             this.checkpointableOffsets.putAll(checkpoint.read());\n-        } catch (IOException e) {\n+        } catch (final IOException e) {\n             try {\n                 stateDirectory.unlockGlobalState();\n-            } catch (IOException e1) {\n+            } catch (final IOException e1) {\n                 log.error(\"Failed to unlock the global state directory\", e);\n             }\n             throw new StreamsException(\"Failed to read checkpoints for global state globalStores\", e);\n@@ -232,7 +232,7 @@ public void register(final StateStore store,\n         }\n \n         final List<TopicPartition> topicPartitions = new ArrayList<>();\n-        for (PartitionInfo partition : partitionInfos) {\n+        for (final PartitionInfo partition : partitionInfos) {\n             topicPartitions.add(new TopicPartition(partition.topic(), partition.partition()));\n         }\n         return topicPartitions;\n@@ -253,8 +253,7 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n \n             long offset = globalConsumer.position(topicPartition);\n             final Long highWatermark = highWatermarks.get(topicPartition);\n-            BatchingStateRestoreCallback\n-                stateRestoreAdapter =\n+            final BatchingStateRestoreCallback stateRestoreAdapter =\n                 (BatchingStateRestoreCallback) ((stateRestoreCallback instanceof\n                                                      BatchingStateRestoreCallback)\n                                                 ? stateRestoreCallback\n@@ -267,7 +266,7 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n                 try {\n                     final ConsumerRecords<byte[], byte[]> records = globalConsumer.poll(pollTime);\n                     final List<KeyValue<byte[], byte[]>> restoreRecords = new ArrayList<>();\n-                    for (ConsumerRecord<byte[], byte[]> record : records) {\n+                    for (final ConsumerRecord<byte[], byte[]> record : records) {\n                         if (record.key() != null) {\n                             restoreRecords.add(KeyValue.pair(record.key(), record.value()));\n                         }\n@@ -294,11 +293,11 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n     @Override\n     public void flush() {\n         log.debug(\"Flushing all global globalStores registered in the state manager\");\n-        for (StateStore store : this.globalStores.values()) {\n+        for (final StateStore store : this.globalStores.values()) {\n             try {\n                 log.trace(\"Flushing global store={}\", store.name());\n                 store.flush();\n-            } catch (Exception e) {\n+            } catch (final Exception e) {\n                 throw new ProcessorStateException(String.format(\"Failed to flush global state store %s\", store.name()), e);\n             }\n         }\n@@ -316,7 +315,7 @@ public void close(final Map<TopicPartition, Long> offsets) throws IOException {\n                 log.debug(\"Closing global storage engine {}\", entry.getKey());\n                 try {\n                     entry.getValue().close();\n-                } catch (Exception e) {\n+                } catch (final Exception e) {\n                     log.error(\"Failed to close global state store {}\", entry.getKey(), e);\n                     closeFailed.append(\"Failed to close global state store:\")\n                             .append(entry.getKey())\n@@ -341,7 +340,7 @@ public void checkpoint(final Map<TopicPartition, Long> offsets) {\n         if (!checkpointableOffsets.isEmpty()) {\n             try {\n                 checkpoint.write(checkpointableOffsets);\n-            } catch (IOException e) {\n+            } catch (final IOException e) {\n                 log.warn(\"Failed to write offset checkpoint file to {} for global stores: {}\", checkpoint, e);\n             }\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java",
                "sha": "78c4a363f29382041f7dce6229395f3e6a433e9a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java?ref=ff96d574371811c75f4f454847f67508d1de98c0",
                "deletions": 9,
                "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "patch": "@@ -67,7 +67,7 @@ public ProcessorStateManager(final TaskId taskId,\n                                  final ChangelogReader changelogReader,\n                                  final boolean eosEnabled,\n                                  final LogContext logContext) throws IOException {\n-        super(stateDirectory.directoryForTask(taskId));\n+        super(stateDirectory.directoryForTask(taskId), eosEnabled);\n \n         this.log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n@@ -81,12 +81,11 @@ public ProcessorStateManager(final TaskId taskId,\n         offsetLimits = new HashMap<>();\n         standbyRestoredOffsets = new HashMap<>();\n         this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new HashMap<String, StateRestoreCallback>() : null;\n+        restoreCallbacks = isStandby ? new HashMap<>() : null;\n         this.storeToChangelogTopic = storeToChangelogTopic;\n \n         // load the checkpoint information\n         checkpointableOffsets.putAll(checkpoint.read());\n-\n         if (eosEnabled) {\n             // delete the checkpoint file after finish loading its stored offsets\n             checkpoint.delete();\n@@ -169,11 +168,7 @@ public void reinitializeStateStoresForPartitions(final Collection<TopicPartition\n             final int partition = getPartition(topicName);\n             final TopicPartition storePartition = new TopicPartition(topicName, partition);\n \n-            if (checkpointableOffsets.containsKey(storePartition)) {\n-                partitionsAndOffsets.put(storePartition, checkpointableOffsets.get(storePartition));\n-            } else {\n-                partitionsAndOffsets.put(storePartition, -1L);\n-            }\n+            partitionsAndOffsets.put(storePartition, checkpointableOffsets.getOrDefault(storePartition, -1L));\n         }\n         return partitionsAndOffsets;\n     }\n@@ -340,7 +335,7 @@ public StateStore getGlobalStore(final String name) {\n         return globalStores.get(name);\n     }\n \n-    private BatchingStateRestoreCallback getBatchingRestoreCallback(StateRestoreCallback callback) {\n+    private BatchingStateRestoreCallback getBatchingRestoreCallback(final StateRestoreCallback callback) {\n         if (callback instanceof BatchingStateRestoreCallback) {\n             return (BatchingStateRestoreCallback) callback;\n         }",
                "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
                "sha": "afb56c1ac1b19015ede570d5a928488823e2b258",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java?ref=ff96d574371811c75f4f454847f67508d1de98c0",
                "deletions": 32,
                "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "patch": "@@ -123,7 +123,7 @@ public void shouldRestoreStoreWithBatchingRestoreSpecification() throws Exceptio\n             assertThat(batchingRestoreCallback.getRestoredRecords().size(), is(1));\n             assertTrue(batchingRestoreCallback.getRestoredRecords().contains(expectedKeyValue));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -141,7 +141,7 @@ public void shouldRestoreStoreWithSinglePutRestoreSpecification() throws Excepti\n             assertThat(persistentStore.keys.size(), is(1));\n             assertTrue(persistentStore.keys.contains(intKey));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -169,7 +169,7 @@ public void testRegisterPersistentStore() throws IOException {\n             stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);\n             assertTrue(changelogReader.wasRegistered(new TopicPartition(persistentStoreTopicName, 2)));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -196,7 +196,7 @@ public void testRegisterNonPersistentStore() throws IOException {\n             stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);\n             assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -257,7 +257,7 @@ public void testChangeLogOffsets() throws IOException {\n             assertEquals(-1L, (long) changeLogOffsets.get(partition3));\n \n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -269,7 +269,7 @@ public void testGetStore() throws IOException {\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -280,13 +280,13 @@ public void testGetStore() throws IOException {\n             assertEquals(mockStateStore, stateMgr.getStore(nonPersistentStoreName));\n \n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n     @Test\n     public void testFlushAndClose() throws IOException {\n-        checkpoint.write(Collections.<TopicPartition, Long>emptyMap());\n+        checkpoint.write(Collections.emptyMap());\n \n         // set up ack'ed offsets\n         final HashMap<TopicPartition, Long> ackedOffsets = new HashMap<>();\n@@ -339,7 +339,7 @@ public void shouldRegisterStoreWithoutLoggingEnabledAndNotBackedByATopic() throw\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -358,7 +358,7 @@ public void shouldNotChangeOffsetsIfAckedOffsetsIsNull() throws IOException {\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -408,7 +408,7 @@ public void shouldWriteCheckpointForStandbyReplica() throws IOException {\n                                                                   bytes,\n                                                                   bytes)));\n \n-        stateMgr.checkpoint(Collections.<TopicPartition, Long>emptyMap());\n+        stateMgr.checkpoint(Collections.emptyMap());\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n         assertThat(read, equalTo(Collections.singletonMap(persistentStorePartition, 889L)));\n@@ -433,7 +433,7 @@ public void shouldNotWriteCheckpointForNonPersistent() throws IOException {\n         stateMgr.checkpoint(Collections.singletonMap(topicPartition, 876L));\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n-        assertThat(read, equalTo(Collections.<TopicPartition, Long>emptyMap()));\n+        assertThat(read, equalTo(Collections.emptyMap()));\n     }\n \n     @Test\n@@ -443,7 +443,7 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx\n             noPartitions,\n             true, // standby\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -453,18 +453,17 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx\n         stateMgr.checkpoint(Collections.singletonMap(persistentStorePartition, 987L));\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n-        assertThat(read, equalTo(Collections.<TopicPartition, Long>emptyMap()));\n+        assertThat(read, equalTo(Collections.emptyMap()));\n     }\n \n-\n     @Test\n     public void shouldThrowIllegalArgumentExceptionIfStoreNameIsSameAsCheckpointFileName() throws IOException {\n         final ProcessorStateManager stateManager = new ProcessorStateManager(\n             taskId,\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -484,7 +483,7 @@ public void shouldThrowIllegalArgumentExceptionOnRegisterWhenStoreHasAlreadyBeen\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -551,7 +550,7 @@ public void close() {\n         stateManager.register(stateStore, stateStore.stateRestoreCallback);\n \n         try {\n-            stateManager.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateManager.close(Collections.emptyMap());\n             fail(\"Should throw ProcessorStateException if store close throws exception\");\n         } catch (final ProcessorStateException e) {\n             // pass\n@@ -623,7 +622,7 @@ public void close() {\n         stateManager.register(stateStore2, stateStore2.stateRestoreCallback);\n \n         try {\n-            stateManager.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateManager.close(Collections.emptyMap());\n         } catch (final ProcessorStateException expected) { /* ignode */ }\n         Assert.assertTrue(closedStore.get());\n     }\n@@ -640,7 +639,7 @@ public void shouldDeleteCheckpointFileOnCreationIfEosEnabled() throws IOExceptio\n                 noPartitions,\n                 false,\n                 stateDirectory,\n-                Collections.<String, String>emptyMap(),\n+                Collections.emptyMap(),\n                 changelogReader,\n                 true,\n                 logContext);\n@@ -653,28 +652,36 @@ public void shouldDeleteCheckpointFileOnCreationIfEosEnabled() throws IOExceptio\n         }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n-    public void shouldSuccessfullyReInitializeStateStores() throws IOException {\n+    public void shouldSuccessfullyReInitializeStateStoresWithEosDisable() throws Exception {\n+        shouldSuccessfullyReInitializeStateStores(false);\n+    }\n+\n+    @Test\n+    public void shouldSuccessfullyReInitializeStateStoresWithEosEnable() throws Exception {\n+        shouldSuccessfullyReInitializeStateStores(true);\n+    }\n+\n+    private void shouldSuccessfullyReInitializeStateStores(final boolean eosEnabled) throws Exception {\n         final String store2Name = \"store2\";\n         final String store2Changelog = \"store2-changelog\";\n         final TopicPartition store2Partition = new TopicPartition(store2Changelog, 0);\n         final List<TopicPartition> changelogPartitions = Arrays.asList(changelogTopicPartition, store2Partition);\n-        Map<String, String> storeToChangelog = new HashMap() {\n+        final Map<String, String> storeToChangelog = new HashMap<String, String>() {\n             {\n                 put(storeName, changelogTopic);\n                 put(store2Name, store2Changelog);\n             }\n         };\n         final ProcessorStateManager stateManager = new ProcessorStateManager(\n-                taskId,\n-                changelogPartitions,\n-                false,\n-                stateDirectory,\n-                storeToChangelog,\n-                changelogReader,\n-                false,\n-                logContext);\n+            taskId,\n+            changelogPartitions,\n+            false,\n+            stateDirectory,\n+            storeToChangelog,\n+            changelogReader,\n+            eosEnabled,\n+            logContext);\n \n         final MockStateStore stateStore = new MockStateStore(storeName, true);\n         final MockStateStore stateStore2 = new MockStateStore(store2Name, true);\n@@ -696,7 +703,7 @@ public void register(final StateStore store, final StateRestoreCallback stateRes\n         assertTrue(stateStore2.initialized);\n     }\n \n-    private ProcessorStateManager getStandByStateManager(TaskId taskId) throws IOException {\n+    private ProcessorStateManager getStandByStateManager(final TaskId taskId) throws IOException {\n         return new ProcessorStateManager(\n             taskId,\n             noPartitions,",
                "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java",
                "sha": "1b03cd4f2949aa600b52fe7267291b9b689ebf0a",
                "status": "modified"
            }
        ],
        "message": "KAFKA-6860: Fix NPE in Kafka Streams with EOS enabled (#5187)\n\nReviewers: John Roesler <john@confluent.io>, Ko Byoung Kwon, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",
        "parent": "https://github.com/apache/kafka/commit/ce7fe8fe5f8333e1e6936e2198713211bfb0dfe5",
        "repo": "kafka",
        "unit_tests": [
            "GlobalStateManagerImplTest.java",
            "ProcessorStateManagerTest.java"
        ]
    }
}