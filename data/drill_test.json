{
    "drill_06e1522": {
        "bug_id": "drill_06e1522",
        "commit": "https://github.com/apache/drill/commit/06e1522b5ddf7e15d49921be1d9323f1e09273b0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0",
                "deletions": 2,
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "patch": "@@ -93,8 +93,9 @@ protected DrillConnectionImpl(DriverImpl driver, AvaticaFactory factory,\n     super(driver, factory, url, info);\n \n     // Initialize transaction-related settings per Drill behavior.\n-    super.setTransactionIsolation( TRANSACTION_NONE );\n-    super.setAutoCommit( true );\n+    super.setTransactionIsolation(TRANSACTION_NONE);\n+    super.setAutoCommit(true);\n+    super.setReadOnly(false);\n \n     this.config = new DrillConnectionConfig(info);\n ",
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillConnectionImpl.java",
                "sha": "0e4726d994b4a2864c9d0945d0bd1b1f5d049b47",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0",
                "deletions": 2,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -17,9 +17,9 @@\n  */\n package org.apache.drill.jdbc;\n \n-import org.apache.drill.jdbc.Driver;\n \n import static org.hamcrest.CoreMatchers.*;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n@@ -42,6 +42,7 @@\n /**\n  * Test for Drill's implementation of Connection's methods (other than\n  * main transaction-related methods in {@link ConnectionTransactionMethodsTest}).\n+ * TODO: When here will be more tests, they should be sorted according to the {@link Connection} methods order\n  */\n public class ConnectionTest extends JdbcTestBase {\n \n@@ -332,4 +333,9 @@ public void testSetNetworkTimeoutRejectsBadExecutorValue() throws SQLException {\n     }\n   }\n \n+  @Test\n+  public void testIsReadOnly() throws Exception {\n+    assertFalse(connection.isReadOnly());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/exec/jdbc/src/test/java/org/apache/drill/jdbc/ConnectionTest.java",
                "sha": "09b75a66c856b064c991b9cdca88d70b3da9f38e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/06e1522b5ddf7e15d49921be1d9323f1e09273b0/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/pom.xml?ref=06e1522b5ddf7e15d49921be1d9323f1e09273b0",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -35,7 +35,7 @@\n     <dep.guava.version>18.0</dep.guava.version>\n     <forkCount>2</forkCount>\n     <parquet.version>1.8.1-drill-r0</parquet.version>\n-    <calcite.version>1.4.0-drill-r20</calcite.version>\n+    <calcite.version>1.4.0-drill-r21</calcite.version>\n     <janino.version>2.7.6</janino.version>\n     <sqlline.version>1.1.9-drill-r7</sqlline.version>\n     <jackson.version>2.7.1</jackson.version>",
                "raw_url": "https://github.com/apache/drill/raw/06e1522b5ddf7e15d49921be1d9323f1e09273b0/pom.xml",
                "sha": "5bde19b8b5becf372e4351f0ccf0b4f870b87d1e",
                "status": "modified"
            }
        ],
        "message": "DRILL-5413: DrillConnectionImpl.isReadOnly() throws NullPointerException\n\nchange is in CALCITE-843.\nupdate drill's calcite version to 1.4.0-drill-r21\n\nclose #806",
        "parent": "https://github.com/apache/drill/commit/d3718a62e2315a601615db8803cdfdcc3cedab82",
        "patched_files": [
            "pom.xml",
            "DrillConnectionImpl.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "ConnectionTest.java"
        ]
    },
    "drill_125a927": {
        "bug_id": "drill_125a927",
        "commit": "https://github.com/apache/drill/commit/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/README.md?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/README.md",
                "patch": "@@ -1,2 +1,8 @@\n drill-mapr-plugin\n =================\n+By default all the tests in contrib/format-maprdb are disabled.\n+To enable and run these tests please use -Pmapr profile to\n+compile and execute the tests.\n+\n+Here is an example of the mvn command to use to run these tests.\n+mvn install -Dtests=cluster -Pmapr",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "sha": "a94a7cb012bddb432f97bd276621adcf4b292642",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "patch": "@@ -32,6 +32,12 @@\n   public boolean ignoreSchemaChange = false;\n   public boolean readAllNumbersAsDouble = false;\n   public boolean disableCountOptimization = false;\n+  /* This flag is a switch to do special handling in case of\n+   * no columns in the query exists in the maprdb table. This flag\n+   * can get deprecated once it is observed that this special handling\n+   * is not regressing performance of reading maprdb table.\n+   */\n+  public boolean nonExistentFieldSupport = true;\n \n   @Override\n   public int hashCode() {\n@@ -40,6 +46,7 @@ public int hashCode() {\n     result = 31 * result + (ignoreSchemaChange ? 1231 : 1237);\n     result = 31 * result + (readAllNumbersAsDouble ? 1231 : 1237);\n     result = 31 * result + (disableCountOptimization ? 1231 : 1237);\n+    result = 31 * result + (nonExistentFieldSupport ? 1231 : 1237);\n     return result;\n   }\n \n@@ -56,6 +63,8 @@ protected boolean impEquals(Object obj) {\n       return false;\n     } else if (disableCountOptimization != other.disableCountOptimization) {\n       return false;\n+    } else if (nonExistentFieldSupport != other.nonExistentFieldSupport) {\n+      return false;\n     }\n     return true;\n   }\n@@ -76,6 +85,8 @@ public boolean isEnablePushdown() {\n     return enablePushdown;\n   }\n \n+  public boolean isNonExistentFieldSupport() { return nonExistentFieldSupport; }\n+\n   public boolean isIgnoreSchemaChange() {\n     return ignoreSchemaChange;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "sha": "ad153fe7fa5e7f44628b084483f50b1258ad4048",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.List;\n import java.util.Set;\n import java.util.Stack;\n+import java.util.Collections;\n import java.util.concurrent.TimeUnit;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n@@ -44,6 +45,7 @@\n import org.apache.drill.exec.util.Utilities;\n import org.apache.drill.exec.vector.BaseValueVector;\n import org.apache.drill.exec.vector.complex.impl.MapOrListWriterImpl;\n+import org.apache.drill.exec.vector.complex.fn.JsonReaderUtils;\n import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;\n import org.ojai.DocumentReader;\n import org.ojai.DocumentReader.EventType;\n@@ -95,6 +97,7 @@\n   private final boolean allTextMode;\n   private final boolean ignoreSchemaChange;\n   private final boolean disableCountOptimization;\n+  private final boolean nonExistentColumnsProjection;\n \n   public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n       MapRDBFormatPluginConfig formatPluginConfig,\n@@ -119,6 +122,7 @@ public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n     allTextMode = formatPluginConfig.isAllTextMode();\n     ignoreSchemaChange = formatPluginConfig.isIgnoreSchemaChange();\n     disablePushdown = !formatPluginConfig.isEnablePushdown();\n+    nonExistentColumnsProjection = formatPluginConfig.isNonExistentFieldSupport();\n   }\n \n   @Override\n@@ -230,6 +234,9 @@ public int next() {\n       }\n     }\n \n+    if (nonExistentColumnsProjection && recordCount > 0) {\n+      JsonReaderUtils.ensureAtLeastOneField(vectorWriter, getColumns(), allTextMode, Collections.EMPTY_LIST);\n+    }\n     vectorWriter.setValueCount(recordCount);\n     logger.debug(\"Took {} ms to get {} records\", watch.elapsed(TimeUnit.MILLISECONDS), recordCount);\n     return recordCount;",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "sha": "113b3adb7d98b56762feffa397cfadbbf2c8b0f6",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "patch": "@@ -57,6 +57,16 @@ public void testSelectId() throws Exception {\n     runSQLAndVerifyCount(sql, 10);\n   }\n \n+  @Test\n+  public void testSelectNonExistentColumns() throws Exception {\n+    setColumnWidths(new int[] {23});\n+    final String sql = \"SELECT\\n\"\n+            + \"  something\\n\"\n+            + \"FROM\\n\"\n+            + \"  hbase.business business limit 5\";\n+    runSQLAndVerifyCount(sql, 5);\n+  }\n+\n   @Test\n   public void testKVGen() throws Exception {\n     setColumnWidths(new int[] {21, 10, 6});",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "sha": "26f54b837a1ff1e0ba1a4412aa9ee2567976e08c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 65,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "patch": "@@ -104,71 +104,7 @@ public JsonReader(DrillBuf managedBuf, List<SchemaPath> columns,\n   @SuppressWarnings(\"resource\")\n   @Override\n   public void ensureAtLeastOneField(ComplexWriter writer) {\n-    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n-    List<PathSegment> fieldPathList = Lists.newArrayList();\n-    BitSet emptyStatus = new BitSet(columns.size());\n-\n-    // first pass: collect which fields are empty\n-    for (int i = 0; i < columns.size(); i++) {\n-      SchemaPath sp = columns.get(i);\n-      PathSegment fieldPath = sp.getRootSegment();\n-      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n-      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n-        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n-        fieldPath = fieldPath.getChild();\n-      }\n-      writerList.add(fieldWriter);\n-      fieldPathList.add(fieldPath);\n-      if (fieldWriter.isEmptyMap()) {\n-        emptyStatus.set(i, true);\n-      }\n-      if (i == 0 && !allTextMode) {\n-        // when allTextMode is false, there is not much benefit to producing all\n-        // the empty\n-        // fields; just produce 1 field. The reason is that the type of the\n-        // fields is\n-        // unknown, so if we produce multiple Integer fields by default, a\n-        // subsequent batch\n-        // that contains non-integer fields will error out in any case. Whereas,\n-        // with\n-        // allTextMode true, we are sure that all fields are going to be treated\n-        // as varchar,\n-        // so it makes sense to produce all the fields, and in fact is necessary\n-        // in order to\n-        // avoid schema change exceptions by downstream operators.\n-        break;\n-      }\n-\n-    }\n-\n-    // second pass: create default typed vectors corresponding to empty fields\n-    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n-    // may be\n-    // shared by multiple fields whereas we want to keep track of all fields\n-    // independently,\n-    // so we rely on the emptyStatus.\n-    for (int j = 0; j < fieldPathList.size(); j++) {\n-      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n-      PathSegment fieldPath = fieldPathList.get(j);\n-      if (emptyStatus.get(j)) {\n-        if (allTextMode) {\n-          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n-        } else {\n-          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n-        }\n-      }\n-    }\n-\n-    for (ListWriter field : emptyArrayWriters) {\n-      // checks that array has not been initialized\n-      if (field.getValueCapacity() == 0) {\n-        if (allTextMode) {\n-          field.varChar();\n-        } else {\n-          field.integer();\n-        }\n-      }\n-    }\n+    JsonReaderUtils.ensureAtLeastOneField(writer, columns, allTextMode, emptyArrayWriters);\n   }\n \n   public void setSource(int start, int end, DrillBuf buf) throws IOException {",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "sha": "4ffbb26e01eb31852334203ef61bca5af592f18f",
                "status": "modified"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.vector.complex.fn;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.drill.common.expression.PathSegment;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.vector.complex.writer.BaseWriter;\n+\n+import java.util.BitSet;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public class JsonReaderUtils {\n+\n+  public static void ensureAtLeastOneField(BaseWriter.ComplexWriter writer,\n+                                    Collection<SchemaPath> columns,\n+                                    boolean allTextMode,\n+                                    List<BaseWriter.ListWriter> emptyArrayWriters) {\n+\n+    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n+    List<PathSegment> fieldPathList = Lists.newArrayList();\n+    BitSet emptyStatus = new BitSet(columns.size());\n+    int i = 0;\n+\n+    // first pass: collect which fields are empty\n+    for (SchemaPath sp : columns) {\n+      PathSegment fieldPath = sp.getRootSegment();\n+      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n+      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n+        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n+        fieldPath = fieldPath.getChild();\n+      }\n+      writerList.add(fieldWriter);\n+      fieldPathList.add(fieldPath);\n+      if (fieldWriter.isEmptyMap()) {\n+        emptyStatus.set(i, true);\n+      }\n+      if (i == 0 && !allTextMode) {\n+        // when allTextMode is false, there is not much benefit to producing all\n+        // the empty fields; just produce 1 field. The reason is that the type of the\n+        // fields is unknown, so if we produce multiple Integer fields by default, a\n+        // subsequent batch that contains non-integer fields will error out in any case.\n+        // Whereas, with allTextMode true, we are sure that all fields are going to be\n+        // treated as varchar, so it makes sense to produce all the fields, and in fact\n+        // is necessary in order to avoid schema change exceptions by downstream operators.\n+        break;\n+      }\n+      i++;\n+    }\n+\n+    // second pass: create default typed vectors corresponding to empty fields\n+    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n+    // may be shared by multiple fields whereas we want to keep track of all fields\n+    // independently, so we rely on the emptyStatus.\n+    for (int j = 0; j < fieldPathList.size(); j++) {\n+      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n+      PathSegment fieldPath = fieldPathList.get(j);\n+      if (emptyStatus.get(j)) {\n+        if (allTextMode) {\n+          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n+        } else {\n+          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n+        }\n+      }\n+    }\n+\n+    for (BaseWriter.ListWriter field : emptyArrayWriters) {\n+      // checks that array has not been initialized\n+      if (field.getValueCapacity() == 0) {\n+        if (allTextMode) {\n+          field.varChar();\n+        } else {\n+          field.integer();\n+        }\n+      }\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "sha": "775be023783401a8367debd885eda277fe3fa78e",
                "status": "added"
            }
        ],
        "message": "DRILL-5864: Selecting a non-existing field from a MapR-DB JSON table fails with NPE.",
        "parent": "https://github.com/apache/drill/commit/4a718a0bd728ae02b502ac93620d132f0f6e1b6c",
        "patched_files": [
            "MapRDBFormatPluginConfig.java",
            "JsonReader.java",
            "MaprDBJsonRecordReader.java",
            "JsonReaderUtils.java",
            "README.md"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestSimpleJson.java",
            "TestJsonReader.java"
        ]
    },
    "drill_1355bfd": {
        "bug_id": "drill_1355bfd",
        "commit": "https://github.com/apache/drill/commit/1355bfddb1c76462366e28ff7f98fdb6823b4b2a",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "patch": "@@ -95,8 +95,14 @@ public IterOutcome innerNext() {\n             return upstream;\n \n           case NOT_YET:\n-          case NONE:\n             break;\n+          case NONE:\n+            if (schema != null) {\n+              // Schema is for the output batch schema which is setup in setupNewSchema(). Since the output\n+              // schema is fixed ((Fragment(VARCHAR), Number of records written (BIGINT)) we should set it\n+              // up even with 0 records for it to be reported back to the client.\n+              break;\n+            }\n \n           case OK_NEW_SCHEMA:\n             setupNewSchema();",
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "sha": "65d0c54d1c020da199f0a85ee1a035b5fa78f210",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "patch": "@@ -230,7 +230,8 @@ private void newSchema() throws IOException {\n     // Its value is likely below Integer.MAX_VALUE (2GB), although rowGroupSize is a long type.\n     // Therefore this size is cast to int, since allocating byte array in under layer needs to\n     // limit the array size in an int scope.\n-    int initialBlockBufferSize = max(MINIMUM_BUFFER_SIZE, blockSize / this.schema.getColumns().size() / 5);\n+    int initialBlockBufferSize = this.schema.getColumns().size() > 0 ?\n+        max(MINIMUM_BUFFER_SIZE, blockSize / this.schema.getColumns().size() / 5) : MINIMUM_BUFFER_SIZE;\n     // We don't want this number to be too small either. Ideally, slightly bigger than the page size,\n     // but not bigger than the block buffer\n     int initialPageBufferSize = max(MINIMUM_BUFFER_SIZE, min(pageSize + pageSize / 10, initialBlockBufferSize));",
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordWriter.java",
                "sha": "0e40c9e360b3f183bf9ba7c35e822e3f495f8d8a",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java?ref=1355bfddb1c76462366e28ff7f98fdb6823b4b2a",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "patch": "@@ -315,6 +315,16 @@ public void testCreateTableIfNotExistsWhenTableWithSameNameDoesNotExist() throws\n     }\n   }\n \n+  @Test\n+  public void testCTASWithEmptyJson() throws Exception {\n+    final String newTblName = \"tbl4444\";\n+    try {\n+      test(String.format(\"CREATE TABLE %s.%s AS SELECT * FROM cp.`project/pushdown/empty.json`\", DFS_TMP_SCHEMA, newTblName));\n+    } finally {\n+      test(\"DROP TABLE IF EXISTS %s.%s\", DFS_TMP_SCHEMA, newTblName);\n+    }\n+  }\n+\n   private static void ctasErrorTestHelper(final String ctasSql, final String expErrorMsg) throws Exception {\n     final String createTableSql = String.format(ctasSql, \"testTableName\");\n     errorMsgTestHelper(createTableSql, expErrorMsg);",
                "raw_url": "https://github.com/apache/drill/raw/1355bfddb1c76462366e28ff7f98fdb6823b4b2a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "sha": "2e7c052fce7ca3840c897eda232e8ef48147a3e0",
                "status": "modified"
            }
        ],
        "message": "DRILL-3964 : Fix NPE in WriterRecordBatch when 0 rows\n\ncloses #1290",
        "parent": "https://github.com/apache/drill/commit/18766950c640c6963ffd1c94224c4a984bedd3c1",
        "patched_files": [
            "WriterRecordBatch.java",
            "ParquetRecordWriter.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestCTAS.java"
        ]
    },
    "drill_163219c": {
        "bug_id": "drill_163219c",
        "commit": "https://github.com/apache/drill/commit/163219c2a802481cbd90171912540250d4059ea8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "patch": "@@ -110,6 +110,7 @@\n   public boolean next() throws IOException {\n \n     currentPage = null;\n+    valuesRead = 0;\n \n     // TODO - the metatdata for total size appears to be incorrect for impala generated files, need to find cause\n     // and submit a bug report\n@@ -162,7 +163,6 @@ public boolean next() throws IOException {\n     pageDataByteArray = currentPage.getBytes().toByteArray();\n \n     readPosInBytes = 0;\n-    valuesRead = 0;\n     if (parentColumnReader.columnDescriptor.getMaxDefinitionLevel() != 0){\n       parentColumnReader.currDefLevel = -1;\n       if (!currentPage.getValueEncoding().usesDictionary()) {",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "sha": "3ad1d6c793378cb98a90e0d7f2eb937d931d29a4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "patch": "@@ -228,7 +228,7 @@ public void setup(OutputMutator output) throws ExecutionSetupException {\n \n     // none of the columns in the parquet file matched the request columns from the query\n     if (columnsToScan == 0){\n-      return;\n+      throw new ExecutionSetupException(\"Error reading from parquet file. No columns requested were found in the file.\");\n     }\n     if (allFieldsFixedLength) {\n       recordsPerBatch = (int) Math.min(Math.min(batchSize / bitWidthAllFixedFields,",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "sha": "4c5f4bba70b1aed62d4f8085fa7fe271f2274731",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "patch": "@@ -352,6 +352,15 @@ public void testMultipleRowGroupsAndReads() throws Exception {\n         \"/tmp/test.parquet\", i, props);\n   }\n \n+  @Test\n+  public void testReadError_Drill_901() throws Exception {\n+    // select cast( L_COMMENT as varchar) from  dfs.`/tmp/drilltest/employee_parquet`\n+    HashMap<String, FieldInfo> fields = new HashMap<>();\n+    ParquetTestProperties props = new ParquetTestProperties(1, 120350, DEFAULT_BYTES_PER_PAGE, fields);\n+    testParquetFullEngineEventBased(false, false, \"/parquet/par_writer_test.json\", null,\n+        \"unused, no file is generated\", 1, props, false);\n+  }\n+\n \n   @Ignore\n   @Test",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "sha": "ad63dc96bf4f19d6df502d9807b4094fe71cf8ec",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/par_writer_test.json?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "patch": "@@ -0,0 +1,26 @@\n+  {\n+    head : {\n+      version : 1,\n+          generator : {\n+        type : \"manual\",\n+            info : \"na\"\n+      },\n+      type : \"APACHE_DRILL_PHYSICAL\"\n+    },\n+    graph : [ {\n+    pop : \"parquet-scan\",\n+    @id : 1,\n+        entries : [ {\n+      path : \"/tpch/lineitem.parquet\"\n+    } ],\n+    storage : {\n+      type : \"file\",\n+      connection : \"classpath:///\"\n+    },\n+    columns: [ \"L_COMMENT\"]\n+  }, {\n+    pop : \"screen\",\n+    @id : 2,\n+        child : 1\n+  } ]\n+  }",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "sha": "34f2ba6e7b147b578b3e7742ffd8f90588c6d58b",
                "status": "added"
            }
        ],
        "message": "DRILL-901: Fix Parquet read bug with VarBinary.\n\nAlso now throw an exception if parquet reader is not passed any columns found in the file. Previously a NPE was thrown as the setup method exited early, skipping an object initialization that manifested in the first call to the next method.",
        "parent": "https://github.com/apache/drill/commit/393adee7e441cb5b03b4489e1497282c68ffbf52",
        "patched_files": [
            "par_writer_test.json",
            "PageReadStatus.java",
            "ParquetRecordReader.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "ParquetRecordReaderTest.java"
        ]
    },
    "drill_1b96174": {
        "bug_id": "drill_1b96174",
        "commit": "https://github.com/apache/drill/commit/1b96174b1e5bafb13a873dd79f03467802d7c929",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java?ref=1b96174b1e5bafb13a873dd79f03467802d7c929",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "patch": "@@ -98,7 +98,8 @@ public Reader getReader(int rowOffset) throws InvalidAccessException{\n \n   @Override\n   public String getString(int rowOffset) throws InvalidAccessException{\n-    return getObject(rowOffset).toString();\n+    Object o = getObject(rowOffset);\n+    return o != null ? o.toString() : null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/java-exec/src/main/java/org/apache/drill/exec/vector/accessor/AbstractSqlAccessor.java",
                "sha": "e24f39c3d4a2b466201a37dffafe7ef6f9fe45d2",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/drill/blob/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java?ref=1b96174b1e5bafb13a873dd79f03467802d7c929",
                "deletions": 0,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "patch": "@@ -62,6 +62,7 @@\n \n   private static Connection connection;\n   private static ResultSet testDataRow;\n+  private static ResultSet testDataRowWithNulls;\n \n   @BeforeClass\n   public static void setUpConnectionAndMetadataToCheck() throws SQLException {\n@@ -99,6 +100,33 @@ public static void setUpConnectionAndMetadataToCheck() throws SQLException {\n         + \"\\nLIMIT 1 \" );\n     // Note: Assertions must be enabled (as they have been so far in tests).\n     assertTrue( testDataRow.next() );\n+\n+    final Statement stmtForNulls = connection.createStatement();\n+    testDataRowWithNulls = stmtForNulls.executeQuery(\n+        \"\"\n+            +   \"SELECT  \"\n+            + \"\\n\"\n+            + \"\\n  CAST(null as boolean)                  AS  C_BOOLEAN_TRUE, \"\n+            // TODO(DRILL-2470): Uncomment when TINYINT is implemented:\n+            //+ \"\\n  CAST(  null AS TINYINT            ) AS  C_TINYINT_1, \"\n+            // TODO(DRILL-2470): Uncomment when SMALLINT is implemented:\n+            //+ \"\\n  CAST(  null AS SMALLINT           ) AS  C_SMALLINT_2, \"\n+            + \"\\n  CAST(  null AS INTEGER            ) AS  C_INTEGER_3, \"\n+            + \"\\n  CAST(  null AS BIGINT             ) AS  C_BIGINT_4, \"\n+            // TODO(DRILL-2683): Uncomment when REAL is implemented:\n+            //+ \"\\n  CAST(  null AS REAL             ) AS `C_REAL_5.5`, \"\n+            + \"\\n  CAST(  null AS DOUBLE PRECISION ) AS `C_DOUBLE_PREC._6.6`, \"\n+            + \"\\n  CAST(  null AS FLOAT            ) AS `C_FLOAT_7.7`, \"\n+            + \"\\n  CAST( null AS DECIMAL         ) AS `C_DECIMAL_10.10`, \"\n+            + \"\\n  CAST( null  AS DECIMAL         ) AS `C_DECIMAL_10.5`, \"\n+            + \"\\n  CAST( null AS DECIMAL(9,2)    ) AS `C_DECIMAL(9,2)_11.11`, \"\n+            + \"\\n  CAST( null AS DECIMAL(18,2)   ) AS `C_DECIMAL(18,2)_12.12`, \"\n+            + \"\\n  CAST( null AS DECIMAL(28,2)   ) AS `C_DECIMAL(28,2)_13.13`, \"\n+            + \"\\n  CAST( null AS DECIMAL(38,2)   ) AS `C_DECIMAL(38,2)_14.14`, \"\n+            + \"\\n  '' \"\n+            + \"\\nFROM (VALUES(1))\" );\n+    // Note: Assertions must be enabled (as they have been so far in tests).\n+    assertTrue( testDataRowWithNulls.next() );\n   }\n \n   @AfterClass\n@@ -492,43 +520,51 @@ public void test_getBigDecimal_handles_DECIMAL_2() throws SQLException {\n   @Test\n   public void test_getString_handles_TINYINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_TINYINT_1\" ), equalTo( \"1\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_TINYINT_1\" ), equalTo( null ) );\n   }\n \n   @Ignore( \"TODO(DRILL-2470): unignore when SMALLINT is implemented\" )\n   @Test\n   public void test_getString_handles_SMALLINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_SMALLINT_2\" ), equalTo( \"2\" ) );\n+    assertThat( testDataRowWithNulls.getString(\"C_SMALLINT_2\"), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_INTEGER() throws SQLException {\n     assertThat( testDataRow.getString( \"C_INTEGER_3\" ), equalTo( \"3\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_INTEGER_3\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_BIGINT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_BIGINT_4\" ), equalTo( \"4\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_BIGINT_4\" ), equalTo( null ) );\n   }\n \n   @Ignore( \"TODO(DRILL-2683): unignore when REAL is implemented\" )\n   @Test\n   public void test_getString_handles_REAL() throws SQLException {\n     assertThat( testDataRow.getString( \"C_REAL_5.5\" ), equalTo( \"5.5????\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_REAL_5.5\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_DOUBLE() throws SQLException {\n     assertThat( testDataRow.getString( \"C_DOUBLE_PREC._6.6\" ), equalTo( \"6.6\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_DOUBLE_PREC._6.6\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_FLOAT() throws SQLException {\n     assertThat( testDataRow.getString( \"C_FLOAT_7.7\" ), equalTo( \"7.7\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_FLOAT_7.7\" ), equalTo( null ) );\n   }\n \n   @Test\n   public void test_getString_handles_DECIMAL() throws SQLException {\n     assertThat( testDataRow.getString( \"C_DECIMAL_10.10\" ), equalTo( \"10.1\" ) );\n+    assertThat( testDataRowWithNulls.getString( \"C_DECIMAL_10.10\" ), equalTo( null ) );\n   }\n \n ",
                "raw_url": "https://github.com/apache/drill/raw/1b96174b1e5bafb13a873dd79f03467802d7c929/exec/jdbc/src/test/java/org/apache/drill/jdbc/ResultSetGetMethodConversionsTest.java",
                "sha": "e0e5232ee8df01d28d77c3e4f07d93aae2879d9d",
                "status": "modified"
            }
        ],
        "message": "DRILL-4128: Fix NPE when calling getString on a JDBC ResultSet when the type is not varchar",
        "parent": "https://github.com/apache/drill/commit/9ff947288f3214fe8e525e001d89a4f91b8b0728",
        "patched_files": [
            "AbstractSqlAccessor.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "ResultSetGetMethodConversionsTest.java"
        ]
    },
    "drill_1c08723": {
        "bug_id": "drill_1c08723",
        "commit": "https://github.com/apache/drill/commit/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/pom.xml",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/pom.xml?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
                "deletions": 6,
                "filename": "contrib/storage-jdbc/pom.xml",
                "patch": "@@ -31,7 +31,7 @@\n   <name>contrib/jdbc-storage-plugin</name>\n \n   <properties>\n-    <mysql.connector.version>5.1.36</mysql.connector.version>\n+    <mysql.connector.version>8.0.13</mysql.connector.version>\n     <derby.database.name>drill_derby_test</derby.database.name>\n     <mysql.database.name>drill_mysql_test</mysql.database.name>\n     <skipTests>false</skipTests>\n@@ -62,13 +62,13 @@\n     <dependency>\n       <groupId>org.apache.derby</groupId>\n       <artifactId>derbyclient</artifactId>\n-      <version>10.11.1.1</version>\n+      <version>10.14.2.0</version>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.derby</groupId>\n       <artifactId>derbynet</artifactId>\n-      <version>10.11.1.1</version>\n+      <version>10.14.2.0</version>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n@@ -104,7 +104,7 @@\n         <!-- Because the JDBC tests are somewhat heavyweight, we only run them in the 'verify' phase -->\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-failsafe-plugin</artifactId>\n-        <version>2.22.0</version>\n+        <version>2.22.1</version>\n         <configuration>\n           <forkCount combine.self=\"override\">1</forkCount>\n           <systemPropertyVariables>\n@@ -128,7 +128,7 @@\n         <!-- Allows us to reserve ports for external servers that we will launch  -->\n         <groupId>org.codehaus.mojo</groupId>\n         <artifactId>build-helper-maven-plugin</artifactId>\n-        <version>1.9.1</version>\n+        <version>3.0.0</version>\n         <executions>\n           <execution>\n             <id>reserve-network-port</id>\n@@ -147,7 +147,7 @@\n       </plugin>\n       <plugin>\n         <artifactId>maven-dependency-plugin</artifactId>\n-        <version>2.8</version>\n+        <version>3.1.1</version>\n         <executions>\n           <execution>\n             <goals>",
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/pom.xml",
                "sha": "5bad6b111b1736dbf0c6fa635f64a7827db5ada5",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
                "deletions": 50,
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "patch": "@@ -22,9 +22,14 @@\n import org.apache.drill.exec.expr.fn.impl.DateUtility;\n import org.apache.drill.exec.proto.UserBitShared;\n \n+import org.apache.drill.exec.util.StoragePluginTestUtils;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n+import java.math.BigDecimal;\n+import java.nio.file.Paths;\n+\n import static org.junit.Assert.assertEquals;\n \n /**\n@@ -33,107 +38,114 @@\n @Category(JdbcStorageTest.class)\n public class TestJdbcPluginWithDerbyIT extends PlanTestBase {\n \n+  private static final String TABLE_PATH = \"jdbcmulti/\";\n+  private static final String TABLE_NAME = String.format(\"%s.`%s`\", StoragePluginTestUtils.DFS_PLUGIN_NAME, TABLE_PATH);\n+\n+  @BeforeClass\n+  public static void copyData() throws Exception {\n+    dirTestWatcher.copyResourceToRoot(Paths.get(TABLE_PATH));\n+  }\n+\n   @Test\n   public void testCrossSourceMultiFragmentJoin() throws Exception {\n-    testNoResult(\"USE derby\");\n     testNoResult(\"SET `planner.slice_target` = 1\");\n-    String query = \"select x.person_id, y.salary from DRILL_DERBY_TEST.PERSON x \"\n-        + \"join dfs.`${WORKING_PATH}/src/test/resources/jdbcmulti/` y on x.person_id = y.person_id \";\n-    test(query);\n+    test(\"select x.person_id, y.salary from derby.drill_derby_test.person x \"\n+        + \"join %s y on x.person_id = y.person_id \", TABLE_NAME);\n   }\n \n   @Test\n   public void validateResult() throws Exception {\n-\n     // Skip date, time, and timestamp types since derby mangles these due to improper timezone support.\n     testBuilder()\n-            .sqlQuery(\n-                    \"select PERSON_ID, FIRST_NAME, LAST_NAME, ADDRESS, CITY, STATE, ZIP, JSON, BIGINT_FIELD, SMALLINT_FIELD, \" +\n-                            \"NUMERIC_FIELD, BOOLEAN_FIELD, DOUBLE_FIELD, FLOAT_FIELD, REAL_FIELD, TIME_FIELD, TIMESTAMP_FIELD, \" +\n-                            \"DATE_FIELD, CLOB_FIELD from derby.DRILL_DERBY_TEST.PERSON\")\n-            .ordered()\n-            .baselineColumns(\"PERSON_ID\", \"FIRST_NAME\", \"LAST_NAME\", \"ADDRESS\", \"CITY\", \"STATE\", \"ZIP\", \"JSON\",\n-                    \"BIGINT_FIELD\", \"SMALLINT_FIELD\", \"NUMERIC_FIELD\", \"BOOLEAN_FIELD\", \"DOUBLE_FIELD\",\n-                    \"FLOAT_FIELD\", \"REAL_FIELD\", \"TIME_FIELD\", \"TIMESTAMP_FIELD\", \"DATE_FIELD\", \"CLOB_FIELD\")\n-            .baselineValues(1, \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\",   \"Philadelphia\",     \"PA\",\n-                            19107, \"{ a : 5, b : 6 }\",            123456L,         1, 10.01, false, 1.0, 1.1, 111.00,\n-                            DateUtility.parseLocalTime(\"13:00:01.0\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2012-02-29\"), \"some clob data 1\")\n-            .baselineValues(2, \"first_name_2\", \"last_name_2\", \"One Ferry Building\",         \"San Francisco\",    \"CA\",\n-                            94111, \"{ foo : \\\"abc\\\" }\",            95949L,         2, 20.02, true, 2.0, 2.1, 222.00,\n-                            DateUtility.parseLocalTime(\"23:59:59.0\"),  DateUtility.parseLocalDateTime(\"1999-09-09 23:59:59.0\"), DateUtility.parseLocalDate(\"1999-09-09\"), \"some more clob data\")\n-            .baselineValues(3, \"first_name_3\", \"last_name_3\", \"176 Bowery\",                 \"New York\",         \"NY\",\n-                            10012, \"{ z : [ 1, 2, 3 ] }\",           45456L,        3, 30.04, true, 3.0, 3.1, 333.00,\n-                            DateUtility.parseLocalTime(\"11:34:21.0\"),  DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2011-10-30\"), \"clobber\")\n-            .baselineValues(4, null, null, \"2 15th St NW\", \"Washington\", \"DC\", 20007, \"{ z : { a : 1, b : 2, c : 3 } \" +\n-                    \"}\", -67L, 4, 40.04, false, 4.0, 4.1, 444.00, DateUtility.parseLocalTime(\"16:00:01.0\"), DateUtility.parseLocalDateTime(\"2015-06-01 16:00:01.0\"),  DateUtility.parseLocalDate(\"2015-06-01\"), \"xxx\")\n-            .baselineValues(5, null, null, null, null, null, null, null, null, null, null, null, null, null, null,\n-                            null, null, null, null)\n-            .build().run();\n+        .sqlQuery(\n+            \"select person_id, first_name, last_name, address, city, state, zip, json, bigint_field, smallint_field, \" +\n+                \"numeric_field, boolean_field, double_field, float_field, real_field, time_field, timestamp_field, \" +\n+                \"date_field, clob_field from derby.`drill_derby_test`.person\")\n+        .ordered()\n+        .baselineColumns(\"person_id\", \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\", \"json\",\n+            \"bigint_field\", \"smallint_field\", \"numeric_field\", \"boolean_field\", \"double_field\", \"float_field\",\n+            \"real_field\", \"time_field\", \"timestamp_field\", \"date_field\", \"clob_field\")\n+        .baselineValues(1, \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\",   \"Philadelphia\",     \"PA\", 19107,\n+            \"{ a : 5, b : 6 }\", 123456L, 1, new BigDecimal(\"10.01\"), false, 1.0, 1.1, 111.00,\n+            DateUtility.parseLocalTime(\"13:00:01.0\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"),\n+            DateUtility.parseLocalDate(\"2012-02-29\"), \"some clob data 1\")\n+        .baselineValues(2, \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n+            \"{ foo : \\\"abc\\\" }\", 95949L, 2, new BigDecimal(\"20.02\"), true, 2.0, 2.1, 222.00,\n+            DateUtility.parseLocalTime(\"23:59:59.0\"),  DateUtility.parseLocalDateTime(\"1999-09-09 23:59:59.0\"),\n+            DateUtility.parseLocalDate(\"1999-09-09\"), \"some more clob data\")\n+        .baselineValues(3, \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012, \"{ z : [ 1, 2, 3 ] }\",\n+            45456L, 3, new BigDecimal(\"30.04\"), true, 3.0, 3.1, 333.00, DateUtility.parseLocalTime(\"11:34:21.0\"),\n+            DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2011-10-30\"), \"clobber\")\n+        .baselineValues(4, null, null, \"2 15th St NW\", \"Washington\", \"DC\", 20007, \"{ z : { a : 1, b : 2, c : 3 } }\",\n+            -67L, 4, new BigDecimal(\"40.04\"), false, 4.0, 4.1, 444.00, DateUtility.parseLocalTime(\"16:00:01.0\"),\n+            DateUtility.parseLocalDateTime(\"2015-06-01 16:00:01.0\"),  DateUtility.parseLocalDate(\"2015-06-01\"), \"xxx\")\n+        .baselineValues(5, null, null, null, null, null, null, null, null, null, null, null, null, null, null,\n+            null, null, null, null)\n+        .go();\n   }\n \n   @Test\n   public void pushdownJoin() throws Exception {\n     testNoResult(\"use derby\");\n-    String query = \"select x.person_id from (select person_id from DRILL_DERBY_TEST.PERSON) x \"\n-            + \"join (select person_id from DRILL_DERBY_TEST.PERSON) y on x.person_id = y.person_id \";\n+    String query = \"select x.person_id from (select person_id from derby.drill_derby_test.person) x \"\n+            + \"join (select person_id from derby.drill_derby_test.person) y on x.person_id = y.person_id \";\n     testPlanMatchingPatterns(query, new String[]{}, new String[]{\"Join\"});\n   }\n \n   @Test\n   public void pushdownJoinAndFilterPushDown() throws Exception {\n     final String query = \"select * from \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON e\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON s\\n\" +\n-            \"ON e.FIRST_NAME = s.FIRST_NAME\\n\" +\n-            \"WHERE e.LAST_NAME > 'hello'\";\n+        \"derby.drill_derby_test.person e\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person s\\n\" +\n+        \"ON e.FIRST_NAME = s.FIRST_NAME\\n\" +\n+        \"WHERE e.LAST_NAME > 'hello'\";\n \n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n   @Test\n   public void pushdownAggregation() throws Exception {\n-    final String query = \"select count(*) from derby.DRILL_DERBY_TEST.PERSON\";\n+    final String query = \"select count(*) from derby.drill_derby_test.person\";\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Aggregate\" });\n   }\n \n   @Test\n   public void pushdownDoubleJoinAndFilter() throws Exception {\n     final String query = \"select * from \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON e\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON s\\n\" +\n-            \"ON e.PERSON_ID = s.PERSON_ID\\n\" +\n-            \"INNER JOIN \\n\" +\n-            \"derby.DRILL_DERBY_TEST.PERSON ed\\n\" +\n-            \"ON e.PERSON_ID = ed.PERSON_ID\\n\" +\n-            \"WHERE s.FIRST_NAME > 'abc' and ed.FIRST_NAME > 'efg'\";\n+        \"derby.drill_derby_test.person e\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person s\\n\" +\n+        \"ON e.person_ID = s.person_ID\\n\" +\n+        \"INNER JOIN \\n\" +\n+        \"derby.drill_derby_test.person ed\\n\" +\n+        \"ON e.person_ID = ed.person_ID\\n\" +\n+        \"WHERE s.first_name > 'abc' and ed.first_name > 'efg'\";\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n   @Test\n   public void showTablesDefaultSchema() throws Exception {\n-    testNoResult(\"use derby\");\n-    assertEquals(1, testRunAndPrint(UserBitShared.QueryType.SQL, \"show tables like 'PERSON'\"));\n+    test(\"use derby\");\n+    assertEquals(1, testRunAndPrint(UserBitShared.QueryType.SQL, \"show tables like 'person'\"));\n   }\n \n   @Test\n   public void describe() throws Exception {\n-    testNoResult(\"use derby\");\n-    assertEquals(19, testRunAndPrint(UserBitShared.QueryType.SQL, \"describe PERSON\"));\n+    test(\"use derby\");\n+    assertEquals(19, testRunAndPrint(UserBitShared.QueryType.SQL, \"describe drill_derby_test.person\"));\n   }\n \n   @Test\n   public void ensureDrillFunctionsAreNotPushedDown() throws Exception {\n     // This should verify that we're not trying to push CONVERT_FROM into the JDBC storage plugin. If were pushing\n     // this function down, the SQL query would fail.\n-    testNoResult(\"select CONVERT_FROM(JSON, 'JSON') from derby.DRILL_DERBY_TEST.PERSON where PERSON_ID = 4\");\n+    testNoResult(\"select CONVERT_FROM(JSON, 'JSON') from derby.drill_derby_test.person where person_ID = 4\");\n   }\n \n   @Test\n   public void pushdownFilter() throws Exception {\n-    testNoResult(\"use derby\");\n-    String query = \"select * from DRILL_DERBY_TEST.PERSON where PERSON_ID = 1\";\n+    String query = \"select * from derby.drill_derby_test.person where person_ID = 1\";\n     testPlanMatchingPatterns(query, new String[]{}, new String[]{\"Filter\"});\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithDerbyIT.java",
                "sha": "65a1ea564eec5f90ef0a17383fff948e26e235b1",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
                "deletions": 71,
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "patch": "@@ -24,6 +24,8 @@\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n+import java.math.BigDecimal;\n+\n /**\n  * JDBC storage plugin tests against MySQL.\n  * Note: it requires libaio.so library in the system\n@@ -35,77 +37,77 @@\n   public void validateResult() throws Exception {\n \n     testBuilder()\n-            .sqlQuery(\n-                    \"select person_id, \" +\n-                            \"first_name, last_name, address, city, state, zip, \" +\n-                            \"bigint_field, smallint_field, numeric_field, \" +\n-                            \"boolean_field, double_field, float_field, real_field, \" +\n-                            \"date_field, datetime_field, year_field, time_field, \" +\n-                            \"json, text_field, tiny_text_field, medium_text_field, long_text_field, \" +\n-                            \"blob_field, bit_field, enum_field \" +\n-                    \"from mysql.`drill_mysql_test`.person\")\n-            .ordered()\n-            .baselineColumns(\"person_id\",\n-                    \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\",\n-                    \"bigint_field\", \"smallint_field\", \"numeric_field\",\n-                    \"boolean_field\",\n-                    \"double_field\", \"float_field\", \"real_field\",\n-                    \"date_field\", \"datetime_field\", \"year_field\", \"time_field\",\n-                    \"json\", \"text_field\", \"tiny_text_field\", \"medium_text_field\", \"long_text_field\",\n-                    \"blob_field\", \"bit_field\", \"enum_field\")\n-            .baselineValues(1,\n-                    \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\", \"Philadelphia\", \"PA\", 19107,\n-                    123456789L, 1, 10.01,\n-                    false,\n-                    1.0, 1.1, 1.2,\n-                    DateUtility.parseLocalDate(\"2012-02-29\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"13:00:01.0\"),\n-                    \"{ a : 5, b : 6 }\",\n-                    \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n-                    \"xxx\",\n-                    \"a medium piece of text\",\n-                    \"a longer piece of text this is going on.....\",\n-                    \"this is a test\".getBytes(),\n-                    true, \"XXX\")\n-            .baselineValues(2,\n-                    \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n-                    45456767L, 3, 30.04,\n-                    true,\n-                    3.0, 3.1, 3.2,\n-                    DateUtility.parseLocalDate(\"2011-10-30\"), DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"11:34:21.0\"),\n-                    \"{ z : [ 1, 2, 3 ] }\",\n-                    \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n-                    \"abc\",\n-                    \"a medium piece of text 2\",\n-                    \"somewhat more text\",\n-                    \"this is a test 2\".getBytes(),\n-                    false, \"YYY\")\n-            .baselineValues(3,\n-                    \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012,\n-                    123090L, -3, 55.12,\n-                    false,\n-                    5.0, 5.1, 5.55,\n-                    DateUtility.parseLocalDate(\"2015-06-01\"), DateUtility.parseLocalDateTime(\"2015-09-22 15:46:10.0\"), DateUtility.parseLocalDate(\"1901-01-01\"), DateUtility.parseLocalTime(\"16:00:01.0\"),\n-                    \"{ [ a, b, c ] }\",\n-                    \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\",\n-                    \"abc\",\n-                    \"a medium piece of text 3\",\n-                    \"somewhat more text\",\n-                    \"this is a test 3\".getBytes(),\n-                    true, \"ZZZ\")\n-            .baselineValues(5,\n-                    null, null, null, null, null, null,\n-                    null, null, null,\n-                    null,\n-                    null, null, null,\n-                    null, null, null, null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null,\n-                    null, \"XXX\")\n-                  .build().run();\n+        .sqlQuery(\n+            \"select person_id, \" +\n+                \"first_name, last_name, address, city, state, zip, \" +\n+                \"bigint_field, smallint_field, numeric_field, \" +\n+                \"boolean_field, double_field, float_field, real_field, \" +\n+                \"date_field, datetime_field, year_field, time_field, \" +\n+                \"json, text_field, tiny_text_field, medium_text_field, long_text_field, \" +\n+                \"blob_field, bit_field, enum_field \" +\n+            \"from mysql.`drill_mysql_test`.person\")\n+        .ordered()\n+        .baselineColumns(\"person_id\",\n+            \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \"zip\",\n+            \"bigint_field\", \"smallint_field\", \"numeric_field\",\n+            \"boolean_field\",\n+            \"double_field\", \"float_field\", \"real_field\",\n+            \"date_field\", \"datetime_field\", \"year_field\", \"time_field\",\n+            \"json\", \"text_field\", \"tiny_text_field\", \"medium_text_field\", \"long_text_field\",\n+            \"blob_field\", \"bit_field\", \"enum_field\")\n+        .baselineValues(1,\n+            \"first_name_1\", \"last_name_1\", \"1401 John F Kennedy Blvd\", \"Philadelphia\", \"PA\", 19107,\n+            123456789L, 1, new BigDecimal(\"10.01\"),\n+            false,\n+            1.0, 1.1, 1.2,\n+            DateUtility.parseLocalDate(\"2012-02-29\"), DateUtility.parseLocalDateTime(\"2012-02-29 13:00:01.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"13:00:01.0\"),\n+            \"{ a : 5, b : 6 }\",\n+            \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n+            \"xxx\",\n+            \"a medium piece of text\",\n+            \"a longer piece of text this is going on.....\",\n+            \"this is a test\".getBytes(),\n+            true, \"XXX\")\n+        .baselineValues(2,\n+            \"first_name_2\", \"last_name_2\", \"One Ferry Building\", \"San Francisco\", \"CA\", 94111,\n+            45456767L, 3, new BigDecimal(\"30.04\"),\n+            true,\n+            3.0, 3.1, 3.2,\n+            DateUtility.parseLocalDate(\"2011-10-30\"), DateUtility.parseLocalDateTime(\"2011-10-30 11:34:21.0\"), DateUtility.parseLocalDate(\"2015-01-01\"), DateUtility.parseLocalTime(\"11:34:21.0\"),\n+            \"{ z : [ 1, 2, 3 ] }\",\n+            \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\",\n+            \"abc\",\n+            \"a medium piece of text 2\",\n+            \"somewhat more text\",\n+            \"this is a test 2\".getBytes(),\n+            false, \"YYY\")\n+        .baselineValues(3,\n+            \"first_name_3\", \"last_name_3\", \"176 Bowery\", \"New York\", \"NY\", 10012,\n+            123090L, -3, new BigDecimal(\"55.12\"),\n+            false,\n+            5.0, 5.1, 5.55,\n+            DateUtility.parseLocalDate(\"2015-06-01\"), DateUtility.parseLocalDateTime(\"2015-09-22 15:46:10.0\"), DateUtility.parseLocalDate(\"1901-01-01\"), DateUtility.parseLocalTime(\"16:00:01.0\"),\n+            \"{ [ a, b, c ] }\",\n+            \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\",\n+            \"abc\",\n+            \"a medium piece of text 3\",\n+            \"somewhat more text\",\n+            \"this is a test 3\".getBytes(),\n+            true, \"ZZZ\")\n+        .baselineValues(5,\n+            null, null, null, null, null, null,\n+            null, null, null,\n+            null,\n+            null, null, null,\n+            null, null, null, null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null, \"XXX\")\n+            .go();\n   }\n \n   @Test\n@@ -132,4 +134,12 @@ public void testPhysicalPlanSubmission() throws Exception {\n     testPhysicalPlanExecutionBasedOnQuery(\"select * from mysql.`drill_mysql_test`.person\");\n   }\n \n+  @Test\n+  public void emptyOutput() throws Exception {\n+    String query = \"select * from mysql.`drill_mysql_test`.person e limit 0\";\n+\n+    test(query);\n+  }\n+\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "sha": "c8396b5097501351efadbbfa220ffa3340fd55c1",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java?ref=1c087237a5a23e9c97cf5318e34f0c8c58ec7af6",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "patch": "@@ -52,7 +52,12 @@ public RelNode visit(TableScan scan) {\n     DrillTable unwrap;\n     unwrap = scan.getTable().unwrap(DrillTable.class);\n     if (unwrap == null) {\n-      unwrap = scan.getTable().unwrap(DrillTranslatableTable.class).getDrillTable();\n+      DrillTranslatableTable drillTranslatableTable = scan.getTable().unwrap(DrillTranslatableTable.class);\n+      if (drillTranslatableTable == null) {\n+        contains = true; // it rejects single mode.\n+        return scan;\n+      }\n+      unwrap = drillTranslatableTable.getDrillTable();\n     }\n \n     try {",
                "raw_url": "https://github.com/apache/drill/raw/1c087237a5a23e9c97cf5318e34f0c8c58ec7af6/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/FindHardDistributionScans.java",
                "sha": "90cc178fea551f0f8a6a7969a24cb0073575f600",
                "status": "modified"
            }
        ],
        "message": "DRILL-6850: JDBC integration tests failures\n\n- Fix RDBMS integration tests (expected decimal output and testCrossSourceMultiFragmentJoin)\n- Update libraries versions\n- Resolve NPE for empty result",
        "parent": "https://github.com/apache/drill/commit/6a990c7eb928b00311935e107427c8420258fd6c",
        "patched_files": [
            "pom.xml",
            "FindHardDistributionScans.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestJdbcPluginWithMySQLIT.java",
            "TestJdbcPluginWithDerbyIT.java"
        ]
    },
    "drill_1c09c2f": {
        "bug_id": "drill_1c09c2f",
        "commit": "https://github.com/apache/drill/commit/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "patch": "@@ -29,6 +29,32 @@\n \n   private final static Pattern IGNORE= Pattern.compile(\"^(sun|com\\\\.sun|java).*\");\n \n+  /**\n+   * Constructs the root error message in the form [root exception class name]: [root exception message]\n+   *\n+   * @param cause exception we want the root message for\n+   * @return root error message or empty string if none found\n+   */\n+  static String getRootMessage(final Throwable cause) {\n+    String message = \"\";\n+\n+    Throwable ex = cause;\n+    while (ex != null) {\n+      if (ex.getMessage() != null) {\n+        message = ex.getClass().getName() + \": \" + ex.getMessage();\n+      }\n+\n+      if (ex.getCause() != null && ex.getCause() != ex) {\n+        ex = ex.getCause();\n+      } else {\n+        break;\n+      }\n+    }\n+\n+    return message;\n+  }\n+\n+\n   static String buildCausesMessage(final Throwable t) {\n \n     StringBuilder sb = new StringBuilder();",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/ErrorHelper.java",
                "sha": "5dd9b67a863860d44fc6ff88858df2903591cc5e",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "changes": 117,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/exceptions/UserException.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 44,
                "filename": "common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "patch": "@@ -21,14 +21,14 @@\n import org.apache.drill.exec.proto.UserBitShared.DrillPBError;\n \n /**\n- * Base class for all user exception. The goal is to separate out common error condititions where we can give users\n+ * Base class for all user exception. The goal is to separate out common error conditions where we can give users\n  * useful feedback.\n  * <p>Throwing a user exception will guarantee it's message will be displayed to the user, along with any context\n  * information added to the exception at various levels while being sent to the client.\n  * <p>A specific class of user exceptions are system exception. They represent system level errors that don't display\n  * any specific error message to the user apart from \"A system error has occurend\" along with informations to retrieve\n  * the details of the exception from the logs.\n- * <p>although system exception should only display a generic message to the user, for now they will display the root\n+ * <p>Although system exception should only display a generic message to the user, for now they will display the root\n  * error message, until all user errors are properly sent from the server side.\n  * <p>Any thrown exception that is not wrapped inside a user exception will automatically be converted to a system\n  * exception before being sent to the client.\n@@ -37,10 +37,32 @@\n  */\n public class UserException extends DrillRuntimeException {\n \n+  public static final String MEMORY_ERROR_MSG = \"One or more nodes ran out of memory while executing the query.\";\n+\n+  /**\n+   * Creates a RESOURCE error with a prebuilt message for out of memory exceptions\n+   *\n+   * @param cause exception that will be wrapped inside a memory error\n+   * @return resource error builder\n+   */\n+  public static Builder memoryError(final Throwable cause) {\n+    return UserException.resourceError(cause)\n+      .message(MEMORY_ERROR_MSG);\n+  }\n+\n+  /**\n+   * Creates a RESOURCE error with a prebuilt message for out of memory exceptions\n+   *\n+   * @return resource error builder\n+   */\n+  public static Builder memoryError() {\n+    return memoryError(null);\n+  }\n+\n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#SYSTEM\n@@ -49,7 +71,7 @@\n    *              returned by the builder instead of creating a new user exception\n    * @return user exception builder\n    *\n-   * @deprecated this method should never need to be used explicitely, unless you are passing the exception to the\n+   * @deprecated This method should never need to be used explicitly, unless you are passing the exception to the\n    *             Rpc layer or UserResultListener.submitFailed()\n    */\n   @Deprecated\n@@ -58,7 +80,7 @@ public static Builder systemError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#CONNECTION\n    * @return user exception builder\n@@ -68,9 +90,9 @@ public static Builder connectionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a connection error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a connection error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#CONNECTION\n@@ -84,7 +106,7 @@ public static Builder connectionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_READ\n    * @return user exception builder\n@@ -94,9 +116,9 @@ public static Builder dataReadError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a data read error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a data read error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_READ\n@@ -110,7 +132,7 @@ public static Builder dataReadError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_WRITE\n    * @return user exception builder\n@@ -120,9 +142,9 @@ public static Builder dataWriteError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a data write error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a data write error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#DATA_WRITE\n@@ -136,7 +158,7 @@ public static Builder dataWriteError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#FUNCTION\n    * @return user exception builder\n@@ -146,9 +168,9 @@ public static Builder functionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a function error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a function error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#FUNCTION\n@@ -162,7 +184,7 @@ public static Builder functionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PARSE\n    * @return user exception builder\n@@ -172,9 +194,9 @@ public static Builder parseError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PARSE\n@@ -188,7 +210,7 @@ public static Builder parseError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PERMISSION\n    * @return user exception builder\n@@ -198,9 +220,9 @@ public static Builder permissionError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PERMISSION\n@@ -214,7 +236,7 @@ public static Builder permissionError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PLAN\n    * @return user exception builder\n@@ -224,9 +246,9 @@ public static Builder planError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#PLAN\n@@ -240,7 +262,7 @@ public static Builder planError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#RESOURCE\n    * @return user exception builder\n@@ -250,9 +272,9 @@ public static Builder resourceError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#RESOURCE\n@@ -266,7 +288,7 @@ public static Builder resourceError(final Throwable cause) {\n   }\n \n   /**\n-   * creates a new user exception builder .\n+   * Creates a new user exception builder .\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#UNSUPPORTED_OPERATION\n    * @return user exception builder\n@@ -276,9 +298,9 @@ public static Builder unsupportedError() {\n   }\n \n   /**\n-   * wraps the passed exception inside a system error.\n-   * <p>the cause message will be used unless {@link Builder#message(String, Object...)} is called.\n-   * <p>if the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n+   * Wraps the passed exception inside a system error.\n+   * <p>The cause message will be used unless {@link Builder#message(String, Object...)} is called.\n+   * <p>If the wrapped exception is, or wraps, a user exception it will be returned by {@link Builder#build()} instead\n    * of creating a new exception. Any added context will be added to the user exception as well.\n    *\n    * @see org.apache.drill.exec.proto.UserBitShared.DrillPBError.ErrorType#UNSUPPORTED_OPERATION\n@@ -307,7 +329,7 @@ public static Builder unsupportedError(final Throwable cause) {\n     private String message;\n \n     /**\n-     * wraps an existing exception inside a user exception.\n+     * Wraps an existing exception inside a user exception.\n      *\n      * @param errorType user exception type that should be created if the passed exception isn't,\n      *                  or doesn't wrap a user exception\n@@ -462,13 +484,20 @@ public UserException build() {\n         return uex;\n       }\n \n+      boolean isSystemError = errorType == DrillPBError.ErrorType.SYSTEM;\n+\n+      // make sure system errors use the root error message and display the root cause class name\n+      if (isSystemError) {\n+        message = ErrorHelper.getRootMessage(cause);\n+      }\n+\n       final UserException newException = new UserException(this);\n \n       // since we just created a new exception, we should log it for later reference. If this is a system error, this is\n       // an issue that the Drill admin should pay attention to and we should log as ERROR. However, if this is a user\n       // mistake or data read issue, the system admin should not be concerned about these and thus we'll log this\n       // as an INFO message.\n-      if (errorType == DrillPBError.ErrorType.SYSTEM) {\n+      if (isSystemError) {\n         logger.error(newException.getMessage(), newException);\n       } else {\n         logger.info(\"User Error Occurred\", newException);",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/main/java/org/apache/drill/common/exceptions/UserException.java",
                "sha": "d90ace14662d5ef6039dc46aafb783a696b4d197",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 2,
                "filename": "common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "patch": "@@ -40,9 +40,10 @@ private Exception wrap(UserException uex, int numWraps) {\n   @Test\n   public void testBuildSystemException() {\n     String message = \"This is an exception\";\n-    UserException uex = UserException.systemError(new RuntimeException(message)).build();\n+    UserException uex = UserException.systemError(new Exception(new RuntimeException(message))).build();\n \n-    Assert.assertEquals(message, uex.getOriginalMessage());\n+    Assert.assertTrue(uex.getOriginalMessage().contains(message));\n+    Assert.assertTrue(uex.getOriginalMessage().contains(\"RuntimeException\"));\n \n     DrillPBError error = uex.getOrCreatePBError(true);\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/common/src/test/java/org/apache/drill/common/exceptions/TestUserException.java",
                "sha": "151b762b6ac9d96422947421a0325e693ffd12e9",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "patch": "@@ -38,7 +38,7 @@\n  *   The width of each element is ${type.width} byte(s)\n  *   The equivalent Java primitive is '${minor.javaType!type.javaType}'\n  *\n- * NB: this class is automatically generated from ValueVectorTypes.tdd using FreeMarker.\n+ * Source code generated using FreeMarker template ${.template_name}\n  */\n @SuppressWarnings(\"unused\")\n public final class ${minor.class}Vector extends BaseDataValueVector implements FixedWidthVector{\n@@ -92,30 +92,50 @@ public boolean allocateNewSafe() {\n       allocationValueCount = (int) (allocationValueCount * 2);\n       allocationMonitor = 0;\n     }\n-    this.data = allocator.buffer(allocationValueCount * ${type.width});\n-    if(data == null) return false;\n+\n+    DrillBuf newBuf = allocator.buffer(allocationValueCount * ${type.width});\n+    if(newBuf == null) {\n+      return false;\n+    }\n+\n+    this.data = newBuf;\n     this.data.readerIndex(0);\n     return true;\n   }\n \n   /**\n    * Allocate a new buffer that supports setting at least the provided number of values.  May actually be sized bigger depending on underlying buffer rounding size. Must be called prior to using the ValueVector.\n    * @param valueCount\n+   * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the new buffer\n    */\n   public void allocateNew(int valueCount) {\n     clear();\n-    this.data = allocator.buffer(valueCount * ${type.width});\n+\n+    DrillBuf newBuf = allocator.buffer(valueCount * ${type.width});\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(\n+        String.format(\"Failure while allocating buffer of %d bytes\",valueCount * ${type.width}));\n+    }\n+\n+    this.data = newBuf;\n     this.data.readerIndex(0);\n     this.allocationValueCount = valueCount;\n   }\n \n /**\n  * Allocate new buffer with double capacity, and copy data into the new buffer. Replace vector's buffer with new buffer, and release old one\n+ *\n+ * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the new buffer\n  */\n   public void reAlloc() {\n     logger.info(\"Realloc vector {}. [{}] -> [{}]\", field, allocationValueCount * ${type.width}, allocationValueCount * 2 * ${type.width});\n     allocationValueCount *= 2;\n     DrillBuf newBuf = allocator.buffer(allocationValueCount * ${type.width});\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(\n+      String.format(\"Failure while reallocating buffer to %d bytes\",allocationValueCount * ${type.width}));\n+    }\n+\n     newBuf.setBytes(0, data, 0, data.capacity());\n     newBuf.setZero(newBuf.capacity() / 2, newBuf.capacity() / 2);\n     newBuf.writerIndex(data.writerIndex());",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/FixedValueVectors.java",
                "sha": "a805b8e239d9db2f2d203f5a653919aaaa4a1149",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 5,
                "filename": "exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "patch": "@@ -46,7 +46,7 @@\n  *   The width of each element is ${type.width} byte(s)\n  *   The equivalent Java primitive is '${minor.javaType!type.javaType}'\n  *\n- * NB: this class is automatically generated from ValueVectorTypes.tdd using FreeMarker.\n+ * Source code generated using FreeMarker template ${.template_name}\n  */\n @SuppressWarnings(\"unused\")\n public final class ${minor.class}Vector extends BaseDataValueVector implements VariableWidthVector{\n@@ -281,12 +281,14 @@ public boolean allocateNewSafe() {\n       allocationMonitor = 0;\n     }\n \n-    data = allocator.buffer(allocationTotalByteCount);\n-    if(data == null){\n+    DrillBuf newBuf = allocator.buffer(allocationTotalByteCount);\n+    if(newBuf == null){\n       return false;\n     }\n-    \n+\n+    this.data = newBuf;\n     data.readerIndex(0);\n+\n     if(!offsetVector.allocateNewSafe()){\n       return false;\n     }\n@@ -297,7 +299,12 @@ public boolean allocateNewSafe() {\n   public void allocateNew(int totalBytes, int valueCount) {\n     clear();\n     assert totalBytes >= 0;\n-    data = allocator.buffer(totalBytes);\n+    DrillBuf newBuf = allocator.buffer(totalBytes);\n+    if(newBuf == null){\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of %d bytes\", totalBytes));\n+    }\n+\n+    this.data = newBuf;\n     data.readerIndex(0);\n     allocationTotalByteCount = totalBytes;\n     offsetVector.allocateNew(valueCount+1);\n@@ -307,6 +314,11 @@ public void allocateNew(int totalBytes, int valueCount) {\n     public void reAlloc() {\n       allocationTotalByteCount *= 2;\n       DrillBuf newBuf = allocator.buffer(allocationTotalByteCount);\n+      if(newBuf == null){\n+        throw new OutOfMemoryRuntimeException(\n+          String.format(\"Failure while reallocating buffer of %d bytes\", allocationTotalByteCount));\n+      }\n+\n       newBuf.setBytes(0, data, 0, data.capacity());\n       data.release();\n       data = newBuf;",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/codegen/templates/VariableLengthVectors.java",
                "sha": "659d99be35d6e1226400a4e1b35fb460bb21a6dd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "patch": "@@ -27,6 +27,7 @@\n \n import org.apache.drill.common.config.DrillConfig;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.exception.ClassTransformationException;\n import org.apache.drill.exec.expr.ClassGenerator;\n@@ -35,6 +36,7 @@\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n import org.apache.drill.exec.proto.BitControl.PlanFragment;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n@@ -149,6 +151,10 @@ public FragmentContext(final DrillbitContext dbContext, final PlanFragment fragm\n     try {\n       allocator = context.getAllocator().getChildAllocator(this, fragment.getMemInitial(), fragment.getMemMax(), true);\n       Preconditions.checkNotNull(allocator, \"Unable to acuqire allocator\");\n+    } catch(final OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+      throw UserException.memoryError(e)\n+        .addContext(\"Fragment\", getHandle().getMajorFragmentId() + \":\" + getHandle().getMinorFragmentId())\n+        .build();\n     } catch(final Throwable e) {\n       throw new ExecutionSetupException(\"Failure while getting memory allocator for fragment.\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/ops/FragmentContext.java",
                "sha": "cf4e9bbc517e880d18e831dd8b77eec6ef2108c9",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 11,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "patch": "@@ -25,13 +25,15 @@\n import java.util.Map;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.types.TypeProtos.MinorType;\n import org.apache.drill.common.types.Types;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.exception.SchemaChangeException;\n import org.apache.drill.exec.expr.TypeHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.OperatorContext;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n@@ -47,6 +49,7 @@\n import org.apache.drill.exec.record.selection.SelectionVector4;\n import org.apache.drill.exec.server.options.OptionValue;\n import org.apache.drill.exec.store.RecordReader;\n+import org.apache.drill.exec.testing.ExecutionControlsInjector;\n import org.apache.drill.exec.vector.AllocationHelper;\n import org.apache.drill.exec.vector.NullableVarCharVector;\n import org.apache.drill.exec.vector.SchemaChangeCallBack;\n@@ -60,13 +63,11 @@\n  */\n public class ScanBatch implements CloseableRecordBatch {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ScanBatch.class);\n-\n-  private static final int MAX_RECORD_CNT = Character.MAX_VALUE;\n+  private final static ExecutionControlsInjector injector = ExecutionControlsInjector.getInjector(ScanBatch.class);\n \n   private final Map<MaterializedField.Key, ValueVector> fieldVectorMap = Maps.newHashMap();\n \n   private final VectorContainer container = new VectorContainer();\n-  private VectorContainer tempContainer;\n   private int recordCount;\n   private final FragmentContext context;\n   private final OperatorContext oContext;\n@@ -79,7 +80,6 @@\n   private List<ValueVector> partitionVectors;\n   private List<Integer> selectedPartitionColumns;\n   private String partitionColumnDesignator;\n-  private boolean first = true;\n   private boolean done = false;\n   private SchemaChangeCallBack callBack = new SchemaChangeCallBack();\n \n@@ -159,13 +159,14 @@ public IterOutcome next() {\n     if (done) {\n       return IterOutcome.NONE;\n     }\n-    long t1 = System.nanoTime();\n     oContext.getStats().startProcessing();\n     try {\n       try {\n+        injector.injectChecked(context.getExecutionControls(), \"next-allocate\", OutOfMemoryException.class);\n+\n         currentReader.allocate(fieldVectorMap);\n-      } catch (OutOfMemoryException e) {\n-        logger.debug(\"Caught OutOfMemoryException\");\n+      } catch (OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+        logger.debug(\"Caught Out of Memory Exception\", e);\n         for (ValueVector v : fieldVectorMap.values()) {\n           v.clear();\n         }\n@@ -219,6 +220,9 @@ public IterOutcome next() {\n       } else {\n         return IterOutcome.OK;\n       }\n+    } catch (OutOfMemoryRuntimeException ex) {\n+      context.fail(UserException.memoryError(ex).build());\n+      return IterOutcome.STOP;\n     } catch (Exception ex) {\n       logger.debug(\"Failed to read the batch. Stopping...\", ex);\n       context.fail(ex);\n@@ -328,7 +332,7 @@ public void allocate(int recordCount) {\n     @Override\n     public boolean isNewSchema() {\n       // Check if top level schema has changed, second condition checks if one of the deeper map schema has changed\n-      if (schemaChange == true || callBack.getSchemaChange()) {\n+      if (schemaChange || callBack.getSchemaChange()) {\n         schemaChange = false;\n         return true;\n       }\n@@ -353,9 +357,6 @@ public WritableBatch getWritableBatch() {\n \n   public void close() {\n     container.clear();\n-    if (tempContainer != null) {\n-      tempContainer.clear();\n-    }\n     for (ValueVector v : partitionVectors) {\n       v.clear();\n     }",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "sha": "6176f779bb69a4fdeafcd71d844cf2f50c2c05e1",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingUserConnection;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -80,6 +81,8 @@ public boolean innerNext() {\n       IterOutcome outcome = next(incoming);\n       logger.trace(\"Screen Outcome {}\", outcome);\n       switch (outcome) {\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n         return false;\n       case NONE:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScreenCreator.java",
                "sha": "c31de6616f4f0876bc1903a2652062b79353fe60",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -94,6 +95,8 @@ public boolean innerNext() {\n       }\n //      logger.debug(\"Outcome of sender next {}\", out);\n       switch (out) {\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n       case NONE:\n         // if we didn't do anything yet, send an empty schema.",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/SingleSenderCreator.java",
                "sha": "fe6239e9fb04e701212dc45c35727d2c367d6a20",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "patch": "@@ -48,9 +48,7 @@\n import org.apache.drill.exec.record.BatchSchema;\n import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;\n import org.apache.drill.exec.record.ExpandableHyperContainer;\n-import org.apache.drill.exec.record.MaterializedField;\n import org.apache.drill.exec.record.RecordBatch;\n-import org.apache.drill.exec.record.TransferPair;\n import org.apache.drill.exec.record.TypedFieldId;\n import org.apache.drill.exec.record.VectorAccessible;\n import org.apache.drill.exec.record.VectorContainer;\n@@ -61,19 +59,15 @@\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.drill.exec.vector.complex.AbstractContainerVector;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import com.google.common.base.Stopwatch;\n-import com.google.common.collect.Lists;\n import com.sun.codemodel.JConditional;\n import com.sun.codemodel.JExpr;\n \n public class TopNBatch extends AbstractRecordBatch<TopN> {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TopNBatch.class);\n \n   private static final long MAX_SORT_BYTES = 1L * 1024 * 1024 * 1024;\n-  public static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n-  public static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n   private  final int batchPurgeThreshold;\n \n   public final MappingSet MAIN_MAPPING = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_SCALAR_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n@@ -89,10 +83,8 @@\n   private long countSincePurge;\n   private int batchCount;\n   private Copier copier;\n-  private boolean schemaBuilt = false;\n   private boolean first = true;\n   private int recordCount = 0;\n-  private boolean stop;\n \n   public TopNBatch(TopN popConfig, FragmentContext context, RecordBatch incoming) throws OutOfMemoryException {\n     super(popConfig, context);\n@@ -153,7 +145,11 @@ public void buildSchema() throws SchemaChangeException {\n         container.setRecordCount(0);\n         return;\n       case STOP:\n-        stop = true;\n+        state = BatchState.STOP;\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n       case NONE:\n         state = BatchState.DONE;\n       default:\n@@ -198,6 +194,7 @@ public IterOutcome innerNext() {\n           break outer;\n         case NOT_YET:\n           throw new UnsupportedOperationException();\n+        case OUT_OF_MEMORY:\n         case STOP:\n           return upstream;\n         case OK_NEW_SCHEMA:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/TopN/TopNBatch.java",
                "sha": "c3e70f5edfa650bc7f0ac28013b73f2facabd258",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "patch": "@@ -42,7 +42,7 @@\n \n /* Write the RecordBatch to the given RecordWriter. */\n public class WriterRecordBatch extends AbstractRecordBatch<Writer> {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WriterRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WriterRecordBatch.class);\n \n   private EventBasedRecordWriter eventBasedRecordWriter;\n   private RecordWriter recordWriter;\n@@ -78,11 +78,6 @@ public BatchSchema getSchema() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-//    try {\n-//      setupNewSchema();\n-//    } catch (Exception e) {\n-//      throw new SchemaChangeException(e);\n-//    }\n   }\n \n   @Override\n@@ -101,8 +96,9 @@ public IterOutcome innerNext() {\n         upstream = next(incoming);\n \n         switch(upstream) {\n+          case OUT_OF_MEMORY:\n           case STOP:\n-            return IterOutcome.STOP;\n+            return upstream;\n \n           case NOT_YET:\n           case NONE:\n@@ -124,7 +120,7 @@ public IterOutcome innerNext() {\n             throw new UnsupportedOperationException();\n         }\n       } while(upstream != IterOutcome.NONE);\n-    }catch(Exception ex){\n+    } catch(IOException ex) {\n       logger.error(\"Failure during query\", ex);\n       kill(false);\n       context.fail(ex);\n@@ -154,7 +150,7 @@ private void addOutputContainerData(){\n     container.setRecordCount(1);\n   }\n \n-  protected void setupNewSchema() throws Exception {\n+  protected void setupNewSchema() throws IOException {\n     try {\n       // update the schema in RecordWriter\n       stats.startSetup();",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "sha": "28a99d9fd9bb74361aeb5dd9f8d45874855a2861",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 27,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "patch": "@@ -93,11 +93,20 @@ public int getRecordCount() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(SelectionVectorMode.NONE);\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n     }\n+\n     if (!createAggregator()) {\n       state = BatchState.DONE;\n     }\n@@ -115,33 +124,29 @@ public IterOutcome innerNext() {\n \n     if (aggregator.buildComplete() && !aggregator.allFlushed()) {\n       // aggregation is complete and not all records have been output yet\n-      IterOutcome outcome = aggregator.outputCurrentBatch();\n-      return outcome;\n+      return aggregator.outputCurrentBatch();\n     }\n \n     logger.debug(\"Starting aggregator doWork; incoming record count = {} \", incoming.getRecordCount());\n \n-    while (true) {\n-      AggOutcome out = aggregator.doWork();\n-      logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n-      switch (out) {\n-      case CLEANUP_AND_RETURN:\n-        container.zeroVectors();\n-        aggregator.cleanup();\n-        state = BatchState.DONE;\n-        // fall through\n-      case RETURN_OUTCOME:\n-        IterOutcome outcome = aggregator.getOutcome();\n-        return aggregator.getOutcome();\n-      case UPDATE_AGGREGATOR:\n-        context.fail(UserException.unsupportedError()\n-          .message(\"Hash aggregate does not support schema changes\").build());\n-        close();\n-        killIncoming(false);\n-        return IterOutcome.STOP;\n-      default:\n-        throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n-      }\n+    AggOutcome out = aggregator.doWork();\n+    logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n+    switch (out) {\n+    case CLEANUP_AND_RETURN:\n+      container.zeroVectors();\n+      aggregator.cleanup();\n+      state = BatchState.DONE;\n+      // fall through\n+    case RETURN_OUTCOME:\n+      return aggregator.getOutcome();\n+    case UPDATE_AGGREGATOR:\n+      context.fail(UserException.unsupportedError()\n+        .message(\"Hash aggregate does not support schema changes\").build());\n+      close();\n+      killIncoming(false);\n+      return IterOutcome.STOP;\n+    default:\n+      throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "sha": "e1b59096a25d72c2d1175b9146553ebae7c09337",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "patch": "@@ -309,6 +309,7 @@ public AggOutcome doWork() {\n               logger.debug(\"Received IterOutcome of {}\", out);\n             }\n             switch (out) {\n+              case OUT_OF_MEMORY:\n               case NOT_YET:\n                 this.outcome = out;\n                 return AggOutcome.RETURN_OUTCOME;",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggTemplate.java",
                "sha": "e92de4021c12b7be39ec2c44ccd9eac5619eb0ed",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 33,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "patch": "@@ -96,11 +96,20 @@ public int getRecordCount() {\n \n   @Override\n   public void buildSchema() throws SchemaChangeException {\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(SelectionVectorMode.NONE);\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n     }\n+\n     if (!createAggregator()) {\n       state = BatchState.DONE;\n     }\n@@ -137,6 +146,7 @@ public IterOutcome innerNext() {\n           specialBatchSent = true;\n           return IterOutcome.OK;\n         }\n+      case OUT_OF_MEMORY:\n       case NOT_YET:\n       case STOP:\n         return outcome;\n@@ -153,38 +163,37 @@ public IterOutcome innerNext() {\n       }\n     }\n \n-    while (true) {\n-      AggOutcome out = aggregator.doWork();\n-      recordCount = aggregator.getOutputCount();\n-      logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n-      switch (out) {\n-      case CLEANUP_AND_RETURN:\n-        if (!first) {\n-          container.zeroVectors();\n-        }\n+    AggOutcome out = aggregator.doWork();\n+    recordCount = aggregator.getOutputCount();\n+    logger.debug(\"Aggregator response {}, records {}\", out, aggregator.getOutputCount());\n+    switch (out) {\n+    case CLEANUP_AND_RETURN:\n+      if (!first) {\n+        container.zeroVectors();\n+      }\n+      done = true;\n+      // fall through\n+    case RETURN_OUTCOME:\n+      IterOutcome outcome = aggregator.getOutcome();\n+      if (outcome == IterOutcome.NONE && first) {\n+        first = false;\n         done = true;\n-        // fall through\n-      case RETURN_OUTCOME:\n-        IterOutcome outcome = aggregator.getOutcome();\n-        if (outcome == IterOutcome.NONE && first) {\n-          first = false;\n-          done = true;\n-          return IterOutcome.OK_NEW_SCHEMA;\n-        } else if (outcome == IterOutcome.OK && first) {\n-          outcome = IterOutcome.OK_NEW_SCHEMA;\n-        }\n+        return IterOutcome.OK_NEW_SCHEMA;\n+      } else if (outcome == IterOutcome.OK && first) {\n+        outcome = IterOutcome.OK_NEW_SCHEMA;\n+      } else if (outcome != IterOutcome.OUT_OF_MEMORY) {\n         first = false;\n-        return outcome;\n-      case UPDATE_AGGREGATOR:\n-        context.fail(UserException.unsupportedError()\n-          .message(\"Streaming aggregate does not support schema changes\")\n-          .build());\n-        close();\n-        killIncoming(false);\n-        return IterOutcome.STOP;\n-      default:\n-        throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n       }\n+      return outcome;\n+    case UPDATE_AGGREGATOR:\n+      context.fail(UserException.unsupportedError()\n+        .message(\"Streaming aggregate does not support schema changes\")\n+        .build());\n+      close();\n+      killIncoming(false);\n+      return IterOutcome.STOP;\n+    default:\n+      throw new IllegalStateException(String.format(\"Unknown state %s.\", out));\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "sha": "b252971111c00548bac2b9f796ab986f92995412",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "patch": "@@ -97,6 +97,9 @@ public AggOutcome doWork() {\n               } else {\n                 break outer;\n               }\n+            case OUT_OF_MEMORY:\n+              outcome = out;\n+              return AggOutcome.RETURN_OUTCOME;\n             case NONE:\n               out = IterOutcome.OK_NEW_SCHEMA;\n             case STOP:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggTemplate.java",
                "sha": "0bbfd18ca484c671ffea30e46c72309d32e58112",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.util.List;\n \n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -97,6 +98,8 @@ public boolean innerNext() {\n     RecordBatch.IterOutcome out = next(incoming);\n     logger.debug(\"Outcome of sender next {}\", out);\n     switch(out){\n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n       case STOP:\n       case NONE:\n         for (int i = 0; i < tunnels.length; ++i) {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/broadcastsender/BroadcastSenderRootExec.java",
                "sha": "c6a07f8aa37547d4ad32d244704ab0465db8d33c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "patch": "@@ -20,6 +20,7 @@\n import javax.inject.Named;\n \n import org.apache.drill.exec.exception.SchemaChangeException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;\n import org.apache.drill.exec.record.RecordBatch;\n@@ -64,7 +65,7 @@ public void filterBatch(int recordCount){\n       return;\n     }\n     if (! outgoingSelectionVector.allocateNew(recordCount)) {\n-      throw new UnsupportedOperationException(\"Unable to allocate filter batch\");\n+      throw new OutOfMemoryRuntimeException(\"Unable to allocate filter batch\");\n     }\n     switch(svMode){\n     case NONE:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "sha": "cd2fbe93ec25315e044020943c4e845af16484eb",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "patch": "@@ -171,6 +171,12 @@ public int getRecordCount() {\n   protected void buildSchema() throws SchemaChangeException {\n     leftUpstream = next(left);\n     rightUpstream = next(right);\n+\n+    if (leftUpstream == IterOutcome.OUT_OF_MEMORY || rightUpstream == IterOutcome.OUT_OF_MEMORY) {\n+      state = BatchState.OUT_OF_MEMORY;\n+      return;\n+    }\n+\n     // Initialize the hash join helper context\n     hjHelper = new HashJoinHelper(context, oContext.getAllocator());\n     try {\n@@ -328,6 +334,7 @@ public void executeBuildPhase() throws SchemaChangeException, ClassTransformatio\n \n       switch (rightUpstream) {\n \n+      case OUT_OF_MEMORY:\n       case NONE:\n       case NOT_YET:\n       case STOP:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "sha": "56ce0ee9f43e3e627f60bac26b7999a25df020ff",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "patch": "@@ -69,7 +69,7 @@\n  */\n public class MergeJoinBatch extends AbstractRecordBatch<MergeJoinPOP> {\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergeJoinBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergeJoinBatch.class);\n \n   public static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n   public static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n@@ -148,6 +148,12 @@ public int getRecordCount() {\n \n   public void buildSchema() throws SchemaChangeException {\n     status.ensureInitial();\n+\n+    if (status.getLastLeft() == IterOutcome.OUT_OF_MEMORY || status.getLastRight() == IterOutcome.OUT_OF_MEMORY) {\n+      state = BatchState.OUT_OF_MEMORY;\n+      return;\n+    }\n+\n     allocateBatch(true);\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "sha": "0430f1bde54de7672551ed013726f861e42269f4",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "patch": "@@ -147,7 +147,7 @@ public IterOutcome innerNext() {\n       }\n \n       boolean drainRight = true;\n-      while (drainRight == true) {\n+      while (drainRight) {\n         rightUpstream = next(RIGHT_INPUT, right);\n         switch (rightUpstream) {\n           case OK_NEW_SCHEMA:\n@@ -159,8 +159,11 @@ public IterOutcome innerNext() {\n           case OK:\n             addBatchToHyperContainer(right);\n             break;\n+          case OUT_OF_MEMORY:\n+            return IterOutcome.OUT_OF_MEMORY;\n           case NONE:\n           case STOP:\n+            //TODO we got a STOP, shouldn't we stop immediately ?\n           case NOT_YET:\n             drainRight = false;\n             break;\n@@ -274,6 +277,11 @@ protected void buildSchema() throws SchemaChangeException {\n       leftUpstream = next(LEFT_INPUT, left);\n       rightUpstream = next(RIGHT_INPUT, right);\n \n+      if (leftUpstream == IterOutcome.OUT_OF_MEMORY || rightUpstream == IterOutcome.OUT_OF_MEMORY) {\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n+      }\n+\n       if (leftUpstream != IterOutcome.NONE) {\n         leftSchema = left.getSchema();\n         for (VectorWrapper vw : left) {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/NestedLoopJoinBatch.java",
                "sha": "4c86f5ca99f6487376348e950a8407db2fae74da",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "patch": "@@ -34,15 +34,14 @@\n \n public class LimitRecordBatch extends AbstractSingleRecordBatch<Limit> {\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LimitRecordBatch.class);\n+//  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LimitRecordBatch.class);\n \n   private SelectionVector2 outgoingSv;\n   private SelectionVector2 incomingSv;\n   private int recordsToSkip;\n   private int recordsLeft;\n   private boolean noEndLimit;\n   private boolean skipBatch;\n-  private boolean done = false;\n   private boolean first = true;\n   List<TransferPair> transfers = Lists.newArrayList();\n \n@@ -90,14 +89,13 @@ protected boolean setupNewSchema() throws SchemaChangeException {\n \n   @Override\n   public IterOutcome innerNext() {\n-    if (done) {\n-      return IterOutcome.NONE;\n-    }\n-\n     if(!first && !noEndLimit && recordsLeft <= 0) {\n       incoming.kill(true);\n \n       IterOutcome upStream = next(incoming);\n+      if (upStream == IterOutcome.OUT_OF_MEMORY) {\n+        return upStream;\n+      }\n \n       while (upStream == IterOutcome.OK || upStream == IterOutcome.OK_NEW_SCHEMA) {\n \n@@ -106,10 +104,14 @@ public IterOutcome innerNext() {\n           wrapper.getValueVector().clear();\n         }\n         upStream = next(incoming);\n+        if (upStream == IterOutcome.OUT_OF_MEMORY) {\n+          return upStream;\n+        }\n       }\n \n       return IterOutcome.NONE;\n     }\n+\n     return super.innerNext();\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/limit/LimitRecordBatch.java",
                "sha": "d9330ea753afcfee49abec37cbee835cba437389",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 5,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "patch": "@@ -75,7 +75,6 @@\n import org.apache.drill.exec.vector.FixedWidthVector;\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import parquet.Preconditions;\n \n@@ -88,10 +87,8 @@\n  * The MergingRecordBatch merges pre-sorted record batches from remote senders.\n  */\n public class MergingRecordBatch extends AbstractRecordBatch<MergingReceiverPOP> implements RecordBatch {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergingRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MergingRecordBatch.class);\n \n-  private static final long ALLOCATOR_INITIAL_RESERVATION = 1*1024*1024;\n-  private static final long ALLOCATOR_MAX_RESERVATION = 20L*1000*1000*1000;\n   private static final int OUTGOING_BATCH_SIZE = 32 * 1024;\n \n   private RecordBatchLoader[] batchLoaders;\n@@ -170,7 +167,7 @@ public IterOutcome innerNext() {\n       prevBatchWasFull = false;\n     }\n \n-    if (hasMoreIncoming == false) {\n+    if (!hasMoreIncoming) {\n       logger.debug(\"next() was called after all values have been processed\");\n       outgoingPosition = 0;\n       return IterOutcome.NONE;",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/mergereceiver/MergingRecordBatch.java",
                "sha": "f19f3718f8c8ca2f90f1b030fe9ab14f7ec72e5f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.drill.exec.expr.CodeGenerator;\n import org.apache.drill.exec.expr.ExpressionTreeMaterializer;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.AccountingDataTunnel;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.MetricDef;\n@@ -169,6 +170,9 @@ public boolean innerNext() {\n         }\n         return false;\n \n+      case OUT_OF_MEMORY:\n+        throw new OutOfMemoryRuntimeException();\n+\n       case STOP:\n         if (partitioner != null) {\n           partitioner.clear();",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/partitionsender/PartitionSenderRootExec.java",
                "sha": "1872a515878fe3918fa1d8ec7f65360afc6fcff9",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.drill.common.types.TypeProtos.MajorType;\n import org.apache.drill.exec.expr.TypeHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -38,7 +39,7 @@\n import org.apache.drill.exec.vector.ValueVector;\n \n public class ProducerConsumerBatch extends AbstractRecordBatch {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProducerConsumerBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ProducerConsumerBatch.class);\n \n   private final RecordBatch incoming;\n   private final Thread producer = new Thread(new Producer(), Thread.currentThread().getName() + \" - Producer Thread\");\n@@ -67,7 +68,7 @@ public IterOutcome innerNext() {\n       wrapper = queue.take();\n       logger.debug(\"Got batch from queue\");\n     } catch (final InterruptedException e) {\n-      if (!context.shouldContinue()) {\n+      if (context.shouldContinue()) {\n         context.fail(e);\n       }\n       return IterOutcome.STOP;\n@@ -79,6 +80,8 @@ public IterOutcome innerNext() {\n       return IterOutcome.NONE;\n     } else if (wrapper.failed) {\n       return IterOutcome.STOP;\n+    } else if (wrapper.outOfMemory) {\n+      throw new OutOfMemoryRuntimeException();\n     }\n \n     recordCount = wrapper.batch.getRecordCount();\n@@ -131,27 +134,37 @@ public void run() {\n             case NONE:\n               stop = true;\n               break outer;\n+            case OUT_OF_MEMORY:\n+              queue.putFirst(RecordBatchDataWrapper.outOfMemory());\n+              return;\n             case STOP:\n-              queue.putFirst(new RecordBatchDataWrapper(null, false, true));\n+              queue.putFirst(RecordBatchDataWrapper.failed());\n               return;\n             case OK_NEW_SCHEMA:\n             case OK:\n-              wrapper = new RecordBatchDataWrapper(new RecordBatchData(incoming), false, false);\n+              wrapper = RecordBatchDataWrapper.batch(new RecordBatchData(incoming));\n               queue.put(wrapper);\n               wrapper = null;\n               break;\n             default:\n               throw new UnsupportedOperationException();\n           }\n         }\n+      } catch (final OutOfMemoryRuntimeException e) {\n+        try {\n+          queue.putFirst(RecordBatchDataWrapper.outOfMemory());\n+        } catch (final InterruptedException ex) {\n+          logger.error(\"Unable to enqueue the last batch indicator. Something is broken.\", ex);\n+          // TODO InterruptedException\n+        }\n       } catch (final InterruptedException e) {\n         logger.warn(\"Producer thread is interrupted.\", e);\n         // TODO InterruptedException\n       } finally {\n         if (stop) {\n           try {\n             clearQueue();\n-            queue.put(new RecordBatchDataWrapper(null, true, false));\n+            queue.put(RecordBatchDataWrapper.finished());\n           } catch (final InterruptedException e) {\n             logger.error(\"Unable to enqueue the last batch indicator. Something is broken.\", e);\n             // TODO InterruptedException\n@@ -206,14 +219,32 @@ public int getRecordCount() {\n   }\n \n   private static class RecordBatchDataWrapper {\n-    RecordBatchData batch;\n-    boolean finished;\n-    boolean failed;\n+    final RecordBatchData batch;\n+    final boolean finished;\n+    final boolean failed;\n+    final boolean outOfMemory;\n \n-    RecordBatchDataWrapper(final RecordBatchData batch, final boolean finished, final boolean failed) {\n+    RecordBatchDataWrapper(final RecordBatchData batch, final boolean finished, final boolean failed, final boolean outOfMemory) {\n       this.batch = batch;\n       this.finished = finished;\n       this.failed = failed;\n+      this.outOfMemory = outOfMemory;\n+    }\n+\n+    public static RecordBatchDataWrapper batch(final RecordBatchData batch) {\n+      return new RecordBatchDataWrapper(batch, false, false, false);\n+    }\n+\n+    public static RecordBatchDataWrapper finished() {\n+      return new RecordBatchDataWrapper(null, true, false, false);\n+    }\n+\n+    public static RecordBatchDataWrapper failed() {\n+      return new RecordBatchDataWrapper(null, false, true, false);\n+    }\n+\n+    public static RecordBatchDataWrapper outOfMemory() {\n+      return new RecordBatchDataWrapper(null, false, false, true);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/producer/ProducerConsumerBatch.java",
                "sha": "bca9622b2239fa763aeb844b6503b00990f0fcf6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 11,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.drill.common.expression.FunctionCall;\n import org.apache.drill.common.expression.FunctionCallFactory;\n import org.apache.drill.common.expression.LogicalExpression;\n-import org.apache.drill.common.expression.PathSegment;\n import org.apache.drill.common.expression.PathSegment.NameSegment;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.expression.ValueExpressions;\n@@ -80,7 +79,6 @@\n   private boolean hasRemainder = false;\n   private int remainderIndex = 0;\n   private int recordCount;\n-  private final boolean buildingSchema = true;\n \n   private static final String EMPTY_STRING = \"\";\n   private boolean first = true;\n@@ -144,7 +142,10 @@ protected IterOutcome doWork() {\n         IterOutcome next = null;\n         while (incomingRecordCount == 0) {\n           next = next(incoming);\n-          if (next != IterOutcome.OK && next != IterOutcome.OK_NEW_SCHEMA) {\n+          if (next == IterOutcome.OUT_OF_MEMORY) {\n+            outOfMemory = true;\n+            return next;\n+          } else if (next != IterOutcome.OK && next != IterOutcome.OK_NEW_SCHEMA) {\n             return next;\n           }\n           incomingRecordCount = incoming.getRecordCount();\n@@ -255,13 +256,7 @@ private void setValueCount(final int count) {\n \n   /** hack to make ref and full work together... need to figure out if this is still necessary. **/\n   private FieldReference getRef(final NamedExpression e) {\n-    final FieldReference ref = e.getRef();\n-    final PathSegment seg = ref.getRootSegment();\n-\n-//    if (seg.isNamed() && \"output\".contentEquals(seg.getNameSegment().getPath())) {\n-//      return new FieldReference(ref.getPath().toString().subSequence(7, ref.getPath().length()), ref.getPosition());\n-//    }\n-    return ref;\n+    return e.getRef();\n   }\n \n   private boolean isAnyWildcard(final List<NamedExpression> exprs) {\n@@ -321,7 +316,6 @@ protected boolean setupNewSchema() throws SchemaChangeException {\n             int k = 0;\n             for (final VectorWrapper<?> wrapper : incoming) {\n               final ValueVector vvIn = wrapper.getValueVector();\n-              final SchemaPath originalPath = vvIn.getField().getPath();\n               if (k > result.outputNames.size()-1) {\n                 assert false;\n               }",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/project/ProjectRecordBatch.java",
                "sha": "32ffb6f0b6e29ea3eb095906cdf5e705158924dc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "patch": "@@ -43,13 +43,12 @@\n import org.apache.drill.exec.record.selection.SelectionVector2;\n import org.apache.drill.exec.record.selection.SelectionVector4;\n import org.apache.calcite.rel.RelFieldCollation.Direction;\n-import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n \n import com.sun.codemodel.JConditional;\n import com.sun.codemodel.JExpr;\n \n public class SortBatch extends AbstractRecordBatch<Sort> {\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SortBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SortBatch.class);\n \n   public final MappingSet mainMapping = new MappingSet( (String) null, null, ClassGenerator.DEFAULT_CONSTANT_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n   public final MappingSet leftMapping = new MappingSet(\"leftIndex\", null, ClassGenerator.DEFAULT_CONSTANT_MAP, ClassGenerator.DEFAULT_SCALAR_MAP);\n@@ -107,6 +106,7 @@ public IterOutcome innerNext() {\n           break outer;\n         case NOT_YET:\n           throw new UnsupportedOperationException();\n+        case OUT_OF_MEMORY:\n         case STOP:\n           return upstream;\n         case OK_NEW_SCHEMA:",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortBatch.java",
                "sha": "8748aaf77ba9fc4c3d06058b0637214f215b55d0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "patch": "@@ -17,9 +17,7 @@\n  */\n package org.apache.drill.exec.physical.impl.svremover;\n \n-import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n-import java.io.PrintStream;\n import java.util.List;\n \n import org.apache.drill.exec.exception.ClassTransformationException;\n@@ -36,21 +34,19 @@\n import org.apache.drill.exec.record.VectorContainer;\n import org.apache.drill.exec.record.VectorWrapper;\n import org.apache.drill.exec.record.WritableBatch;\n-import org.apache.drill.exec.util.BatchPrinter;\n import org.apache.drill.exec.vector.CopyUtil;\n import org.apache.drill.exec.vector.ValueVector;\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.Lists;\n \n public class RemovingRecordBatch extends AbstractSingleRecordBatch<SelectionVectorRemover>{\n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RemovingRecordBatch.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RemovingRecordBatch.class);\n \n   private Copier copier;\n   private int recordCount;\n   private boolean hasRemainder;\n   private int remainderIndex;\n-  private boolean first;\n \n   public RemovingRecordBatch(SelectionVectorRemover popConfig, FragmentContext context, RecordBatch incoming) throws OutOfMemoryException {\n     super(popConfig, context, incoming);\n@@ -248,7 +244,7 @@ public static Copier getGenerated4Copier(RecordBatch batch, FragmentContext cont\n \n     for(VectorWrapper<?> vv : batch){\n       ValueVector v = vv.getValueVectors()[0];\n-      TransferPair tp = v.makeTransferPair(container.addOrGet(v.getField()));\n+      v.makeTransferPair(container.addOrGet(v.getField()));\n     }\n \n     try {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/svremover/RemovingRecordBatch.java",
                "sha": "57e7b55d8026e82f6b5ab99f5628b71e0437ca39",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "patch": "@@ -108,8 +108,7 @@ protected IterOutcome doWork() {\n     } else {\n       sv = null;\n     }\n-    WritableBatch batch = WritableBatch.getBatchNoHVWrap(incoming.getRecordCount(), incoming, incomingHasSv2 ? true\n-        : false);\n+    WritableBatch batch = WritableBatch.getBatchNoHVWrap(incoming.getRecordCount(), incoming, incomingHasSv2);\n     VectorAccessibleSerializable wrap = new VectorAccessibleSerializable(batch, sv, oContext.getAllocator());\n \n     try {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/trace/TraceRecordBatch.java",
                "sha": "78e83d63e17f0ba4967cb136f66e1d01d747257b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.drill.exec.expr.ValueVectorReadExpression;\n import org.apache.drill.exec.expr.ValueVectorWriteExpression;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.UnionAll;\n import org.apache.drill.exec.record.AbstractRecordBatch;\n@@ -143,7 +144,9 @@ private void setValueCount(int count) {\n \n   private boolean doAlloc() {\n     for (ValueVector v : allocationVectors) {\n-      if(!AllocationHelper.allocateNew(v, current.getRecordCount())) {\n+      try {\n+        AllocationHelper.allocateNew(v, current.getRecordCount());\n+      } catch (OutOfMemoryRuntimeException ex) {\n         return false;\n       }\n     }",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/union/UnionAllRecordBatch.java",
                "sha": "445568b8620b21d548e5d31455ebdb4f326677e3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "patch": "@@ -37,7 +37,6 @@\n import org.apache.drill.exec.expr.ValueVectorWriteExpression;\n import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n-import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.WindowPOP;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -128,6 +127,7 @@ public IterOutcome innerNext() {\n         case NONE:\n           noMoreBatches = true;\n           break;\n+        case OUT_OF_MEMORY:\n         case NOT_YET:\n         case STOP:\n           return upstream;\n@@ -160,7 +160,7 @@ public IterOutcome innerNext() {\n     // process a saved batch\n     try {\n       framer.doWork();\n-    } catch (DrillException | OutOfMemoryRuntimeException e) {\n+    } catch (DrillException e) {\n       context.fail(e);\n       if (framer != null) {\n         framer.cleanup();\n@@ -179,10 +179,18 @@ public IterOutcome innerNext() {\n   @Override\n   protected void buildSchema() throws SchemaChangeException {\n     logger.trace(\"buildSchema()\");\n-    if (next(incoming) == IterOutcome.NONE) {\n-      state = BatchState.DONE;\n-      container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n-      return;\n+    IterOutcome outcome = next(incoming);\n+    switch (outcome) {\n+      case NONE:\n+        state = BatchState.DONE;\n+        container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n+        return;\n+      case STOP:\n+        state = BatchState.STOP;\n+        return;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        return;\n     }\n \n     try {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/window/WindowFrameRecordBatch.java",
                "sha": "428632fe43270482c2e42461d4038fd18f2d32d5",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 5,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.drill.exec.expr.fn.FunctionGenerationHelper;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.physical.config.ExternalSort;\n import org.apache.drill.exec.physical.impl.sort.RecordBatchData;\n@@ -185,12 +186,16 @@ public void buildSchema() throws SchemaChangeException {\n         }\n         container.buildSchema(SelectionVectorMode.NONE);\n         container.setRecordCount(0);\n-        return;\n+        break;\n       case STOP:\n+        state = BatchState.STOP;\n+        break;\n+      case OUT_OF_MEMORY:\n+        state = BatchState.OUT_OF_MEMORY;\n+        break;\n       case NONE:\n         state = BatchState.DONE;\n-      default:\n-        return;\n+        break;\n     }\n   }\n \n@@ -273,7 +278,7 @@ public IterOutcome innerNext() {\n             try {\n               sv2 = newSV2();\n             } catch (OutOfMemoryException e) {\n-              throw new RuntimeException(e);\n+              throw new OutOfMemoryRuntimeException(e);\n             }\n           }\n           int count = sv2.getCount();\n@@ -316,11 +321,15 @@ public IterOutcome innerNext() {\n //          logger.debug(\"Took {} us to sort {} records\", t, count);\n           break;\n         case OUT_OF_MEMORY:\n+          logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n           highWaterMark = totalSizeInMemory;\n           if (batchesSinceLastSpill > 2) {\n             spilledBatchGroups.add(mergeAndSpill(batchGroups));\n+            batchesSinceLastSpill = 0;\n+          } else {\n+            logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n+            return IterOutcome.OUT_OF_MEMORY;\n           }\n-          batchesSinceLastSpill = 0;\n           break;\n         default:\n           throw new UnsupportedOperationException();",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/ExternalSortBatch.java",
                "sha": "aab339167eae1627895a9045651ad31b54d7de8f",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 15,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Iterator;\n \n import org.apache.drill.common.exceptions.DrillRuntimeException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.exec.exception.SchemaChangeException;\n import org.apache.drill.exec.memory.OutOfMemoryException;\n@@ -67,7 +68,9 @@ protected AbstractRecordBatch(final T popConfig, final FragmentContext context,\n   protected static enum BatchState {\n     BUILD_SCHEMA, // Need to build schema and return\n     FIRST, // This is still the first data batch\n-    NOT_FIRST, // The first data batch has alread been returned\n+    NOT_FIRST, // The first data batch has already been returned\n+    STOP, // The query most likely failed, we need to propagate STOP to the root\n+    OUT_OF_MEMORY, // Out of Memory while building the Schema...Ouch!\n     DONE // All work is done, no more data to be sent\n   }\n \n@@ -119,23 +122,21 @@ public final IterOutcome next(final int inputIndex, final RecordBatch b){\n   public final IterOutcome next() {\n     try {\n       stats.startProcessing();\n-//      if (state == BatchState.BUILD_SCHEMA) {\n-//        buildSchema();\n-//        if (state == BatchState.BUILD_SCHEMA.DONE) {\n-//          return IterOutcome.NONE;\n-//        } else {\n-//          state = BatchState.FIRST;\n-//          return IterOutcome.OK_NEW_SCHEMA;\n-//        }\n-//      }\n       switch (state) {\n         case BUILD_SCHEMA: {\n           buildSchema();\n-          if (state == BatchState.DONE) {\n-            return IterOutcome.NONE;\n-          } else {\n-            state = BatchState.FIRST;\n-            return IterOutcome.OK_NEW_SCHEMA;\n+          switch (state) {\n+            case DONE:\n+              return IterOutcome.NONE;\n+            case OUT_OF_MEMORY:\n+              // because we don't support schema changes, it is safe to fail the query right away\n+              context.fail(UserException.memoryError().build());\n+              // FALL-THROUGH\n+            case STOP:\n+              return IterOutcome.STOP;\n+            default:\n+              state = BatchState.FIRST;\n+              return IterOutcome.OK_NEW_SCHEMA;\n           }\n         }\n         case DONE: {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "sha": "330ec79c3d2e082e88417533c60ed79bd6308d23",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.drill.exec.vector;\n \n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n+\n public class AllocationHelper {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AllocationHelper.class);\n \n@@ -44,16 +46,15 @@ public static void allocate(ValueVector v, int valueCount, int bytesPerValue, in\n \n   /**\n    * Allocates the exact amount if v is fixed width, otherwise falls back to dynamic allocation\n-   * @param v\n-   * @param valueCount\n-   * @return\n+   * @param v value vector we are trying to allocate\n+   * @param valueCount  size we are trying to allocate\n+   * @throws org.apache.drill.exec.memory.OutOfMemoryRuntimeException if it can't allocate the memory\n    */\n-  public static boolean allocateNew(ValueVector v, int valueCount){\n+  public static void allocateNew(ValueVector v, int valueCount) {\n     if (v instanceof  FixedWidthVector) {\n       ((FixedWidthVector) v).allocateNew(valueCount);\n-      return true;\n     } else {\n-      return v.allocateNewSafe();\n+      v.allocateNew();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/AllocationHelper.java",
                "sha": "bf465c7994bba8e225ed5ed1fb4402c636e8cf46",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "patch": "@@ -96,10 +96,12 @@ public boolean allocateNewSafe() {\n \n     clear();\n     int valueSize = getSizeFromCount(allocationValueCount);\n-    data = allocator.buffer(valueSize);\n-    if (data == null) {\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n       return false;\n     }\n+\n+    data = newBuf;\n     zeroVector();\n     return true;\n   }\n@@ -113,7 +115,12 @@ public boolean allocateNewSafe() {\n   public void allocateNew(int valueCount) {\n     clear();\n     int valueSize = getSizeFromCount(valueCount);\n-    data = allocator.buffer(valueSize);\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of d% bytes.\", valueSize));\n+    }\n+\n+    data = newBuf;\n     zeroVector();\n   }\n \n@@ -122,7 +129,12 @@ public void allocateNew(int valueCount) {\n    */\n   public void reAlloc() {\n     allocationValueCount *= 2;\n-    DrillBuf newBuf = allocator.buffer(getSizeFromCount(allocationValueCount));\n+    int valueSize = getSizeFromCount(allocationValueCount);\n+    DrillBuf newBuf = allocator.buffer(valueSize);\n+    if (newBuf == null) {\n+      throw new OutOfMemoryRuntimeException(String.format(\"Failure while allocating buffer of %d bytes.\", valueSize));\n+    }\n+\n     newBuf.setZero(0, newBuf.capacity());\n     newBuf.setBytes(0, data, 0, data.capacity());\n     data.release();",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "sha": "ae5fad535a103a0ea41207e97181f7289f6e2880",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "patch": "@@ -42,6 +42,8 @@\n import org.apache.drill.exec.coord.DistributedSemaphore;\n import org.apache.drill.exec.coord.DistributedSemaphore.DistributedLease;\n import org.apache.drill.exec.exception.OptimizerException;\n+import org.apache.drill.exec.memory.OutOfMemoryException;\n+import org.apache.drill.exec.memory.OutOfMemoryRuntimeException;\n import org.apache.drill.exec.ops.FragmentContext;\n import org.apache.drill.exec.ops.QueryContext;\n import org.apache.drill.exec.opt.BasicOptimizer;\n@@ -222,6 +224,8 @@ public void run() {\n         throw new IllegalStateException();\n       }\n       injector.injectChecked(queryContext.getExecutionControls(), \"run-try-end\", ForemanException.class);\n+    } catch (final OutOfMemoryException | OutOfMemoryRuntimeException e) {\n+      moveToState(QueryState.FAILED, UserException.memoryError(e).build());\n     } catch (final ForemanException e) {\n       moveToState(QueryState.FAILED, e);\n     } catch (AssertionError | Exception ex) {",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/foreman/Foreman.java",
                "sha": "7b36e21d4f14ecb0d5070802e668d4880b28d4ab",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "patch": "@@ -207,9 +207,7 @@ public Void run() throws Exception {\n       updateState(FragmentState.FINISHED);\n     } catch (OutOfMemoryError | OutOfMemoryRuntimeException e) {\n       if (!(e instanceof OutOfMemoryError) || \"Direct buffer memory\".equals(e.getMessage())) {\n-        fail(UserException.resourceError(e)\n-            .message(\"One or more nodes ran out of memory while executing the query.\")\n-            .build());\n+        fail(UserException.memoryError(e).build());\n       } else {\n         // we have a heap out of memory error. The JVM in unstable, exit.\n         System.err.println(\"Node ran out of Heap memory, exiting.\");",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/main/java/org/apache/drill/exec/work/fragment/FragmentExecutor.java",
                "sha": "dc83cc644b639798548d748c4137269b8f32ef1a",
                "status": "modified"
            },
            {
                "additions": 132,
                "blob_url": "https://github.com/apache/drill/blob/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "changes": 132,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java?ref=1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "patch": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill;\n+\n+import org.apache.drill.common.exceptions.UserException;\n+import org.apache.drill.exec.proto.CoordinationProtos;\n+import org.apache.drill.exec.proto.UserBitShared.DrillPBError;\n+import org.apache.drill.exec.testing.ControlsInjectionUtil;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Run several tpch queries and inject an OutOfMemoryException in ScanBatch that will cause an OUT_OF_MEMORY outcome to\n+ * be propagated downstream. Make sure the proper \"memory error\" message is sent to the client.\n+ */\n+public class TestOutOfMemoryOutcome extends BaseTestQuery{\n+\n+  private static final String SINGLE_MODE = \"ALTER SESSION SET `planner.disable_exchanges` = true\";\n+\n+  private void testSingleMode(String fileName) throws Exception{\n+    test(SINGLE_MODE);\n+\n+    CoordinationProtos.DrillbitEndpoint endpoint = bits[0].getContext().getEndpoint();\n+    String controlsString = \"{\\\"injections\\\":[{\"\n+      + \"\\\"address\\\":\\\"\" + endpoint.getAddress() + \"\\\",\"\n+      + \"\\\"port\\\":\\\"\" + endpoint.getUserPort() + \"\\\",\"\n+      + \"\\\"type\\\":\\\"exception\\\",\"\n+      + \"\\\"siteClass\\\":\\\"\" + \"org.apache.drill.exec.physical.impl.ScanBatch\" + \"\\\",\"\n+      + \"\\\"desc\\\":\\\"\" + \"next-allocate\" + \"\\\",\"\n+      + \"\\\"nSkip\\\":0,\"\n+      + \"\\\"nFire\\\":1,\"\n+      + \"\\\"exceptionClass\\\":\\\"\" + \"org.apache.drill.exec.memory.OutOfMemoryException\" + \"\\\"\"\n+      + \"}]}\";\n+    ControlsInjectionUtil.setControls(client, controlsString);\n+\n+    String query = getFile(fileName);\n+\n+    try {\n+      test(query);\n+    } catch(UserException uex) {\n+      DrillPBError error = uex.getOrCreatePBError(false);\n+      Assert.assertEquals(DrillPBError.ErrorType.RESOURCE, error.getErrorType());\n+      Assert.assertTrue(\"Error message isn't related to memory error\",\n+        uex.getMessage().contains(UserException.MEMORY_ERROR_MSG));\n+    }\n+  }\n+\n+  @Test\n+  public void tpch01() throws Exception{\n+    testSingleMode(\"queries/tpch/01.sql\");\n+  }\n+\n+  @Test\n+  public void tpch03() throws Exception{\n+    testSingleMode(\"queries/tpch/03.sql\");\n+  }\n+\n+  @Test\n+  public void tpch04() throws Exception{\n+    testSingleMode(\"queries/tpch/04.sql\");\n+  }\n+\n+  @Test\n+  public void tpch05() throws Exception{\n+    testSingleMode(\"queries/tpch/05.sql\");\n+  }\n+\n+  @Test\n+  public void tpch06() throws Exception{\n+    testSingleMode(\"queries/tpch/06.sql\");\n+  }\n+\n+  @Test\n+  public void tpch07() throws Exception{\n+    testSingleMode(\"queries/tpch/07.sql\");\n+  }\n+\n+  @Test\n+  public void tpch08() throws Exception{\n+    testSingleMode(\"queries/tpch/08.sql\");\n+  }\n+\n+  @Test\n+  public void tpch09() throws Exception{\n+    testSingleMode(\"queries/tpch/09.sql\");\n+  }\n+\n+  @Test\n+  public void tpch10() throws Exception{\n+    testSingleMode(\"queries/tpch/10.sql\");\n+  }\n+\n+  @Test\n+  public void tpch12() throws Exception{\n+    testSingleMode(\"queries/tpch/12.sql\");\n+  }\n+\n+  @Test\n+  public void tpch13() throws Exception{\n+    testSingleMode(\"queries/tpch/13.sql\");\n+  }\n+\n+  @Test\n+  public void tpch14() throws Exception{\n+    testSingleMode(\"queries/tpch/14.sql\");\n+  }\n+\n+  @Test\n+  public void tpch18() throws Exception{\n+    testSingleMode(\"queries/tpch/18.sql\");\n+  }\n+\n+  @Test\n+  public void tpch20() throws Exception{\n+    testSingleMode(\"queries/tpch/20.sql\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/1c09c2f13bd0f50ca40c17dc0bfa7aae5826b8c3/exec/java-exec/src/test/java/org/apache/drill/TestOutOfMemoryOutcome.java",
                "sha": "b270a8bfa08a0b3237be77da47a582e07af8198e",
                "status": "added"
            }
        ],
        "message": "DRILL-2757: Verify operators correctly handle low memory conditions and cancellations\n\nincludes:\nDRILL-2816: system error does not display the original Exception message\nDRILL-2893: ScanBatch throws a NullPointerException instead of returning OUT_OF_MEMORY\nDRILL-2894: FixedValueVectors shouldn't set it's data buffer to null when it fails to allocate it\nDRILL-2895: AbstractRecordBatch.buildSchema() should properly handle OUT_OF_MEMORY outcome\nDRILL-2905: RootExec implementations should properly handle IterOutcome.OUT_OF_MEMORY\nDRILL-2920: properly handle OutOfMemoryException\nDRILL-2947: AllocationHelper.allocateNew() doesn't have a consistent behavior when it can't allocate\n\nalso:\n- added UserException.memoryError() with a pre assigned error message\n- injection site in ScanBatch and unit test that runs various tpch queries and injects\n  an exception in the ScanBatch that will cause an OUT_OF_MEMORY outcome to be sent",
        "parent": "https://github.com/apache/drill/commit/a296383632946a1f45a9b66d4638dab00a026d30",
        "patched_files": [
            "HashAggBatch.java",
            "StreamingAggBatch.java",
            "MergingRecordBatch.java",
            "BroadcastSenderRootExec.java",
            "TraceRecordBatch.java",
            "TopNBatch.java",
            "FragmentContext.java",
            "UnionAllRecordBatch.java",
            "ExternalSortBatch.java",
            "LimitRecordBatch.java",
            "ErrorHelper.java",
            "AllocationHelper.java",
            "Foreman.java",
            "AbstractRecordBatch.java",
            "FragmentExecutor.java",
            "ProducerConsumerBatch.java",
            "SortBatch.java",
            "HashJoinBatch.java",
            "StreamingAggTemplate.java",
            "NestedLoopJoinBatch.java",
            "WriterRecordBatch.java",
            "ScreenCreator.java",
            "ScanBatch.java",
            "MergeJoinBatch.java",
            "RemovingRecordBatch.java",
            "HashAggTemplate.java",
            "UserException.java",
            "BitVector.java",
            "WindowFrameRecordBatch.java",
            "FixedValueVectors.java",
            "VariableLengthVectors.java",
            "FilterTemplate2.java",
            "PartitionSenderRootExec.java",
            "SingleSenderCreator.java",
            "ProjectRecordBatch.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestUserException.java",
            "TestOutOfMemoryOutcome.java"
        ]
    },
    "drill_2f13c08": {
        "bug_id": "drill_2f13c08",
        "commit": "https://github.com/apache/drill/commit/2f13c08f35152639e4619d2898b2ca8fe7115259",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "patch": "@@ -144,6 +144,7 @@\n   String UDF_DIRECTORY_STAGING = \"drill.exec.udf.directory.staging\";\n   String UDF_DIRECTORY_REGISTRY = \"drill.exec.udf.directory.registry\";\n   String UDF_DIRECTORY_TMP = \"drill.exec.udf.directory.tmp\";\n+  String UDF_DISABLE_DYNAMIC = \"drill.exec.udf.disable_dynamic\";\n \n   /**\n    * Local temporary directory is used as base for temporary storage of Dynamic UDF jars.\n@@ -264,7 +265,7 @@\n       SLICE_TARGET_DEFAULT);\n \n   String CAST_TO_NULLABLE_NUMERIC = \"drill.exec.functions.cast_empty_string_to_null\";\n-  OptionValidator CAST_TO_NULLABLE_NUMERIC_OPTION = new BooleanValidator(CAST_TO_NULLABLE_NUMERIC, false);\n+  BooleanValidator CAST_TO_NULLABLE_NUMERIC_OPTION = new BooleanValidator(CAST_TO_NULLABLE_NUMERIC, false);\n \n   /**\n    * HashTable runtime settings",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "sha": "91498fced7e9a00a9451f57fcde915726fc1b0a6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "patch": "@@ -27,6 +27,6 @@\n    *\n    * @param event  event details\n    */\n-  void onChange(TransientStoreEvent event);\n+  void onChange(TransientStoreEvent<?> event);\n \n }",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/coord/store/TransientStoreListener.java",
                "sha": "3cd86f9fc46f188ef03afd7eb288689a2c23bcaf",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 30,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "patch": "@@ -83,17 +83,30 @@\n   private boolean deleteTmpDir = false;\n   private File tmpDir;\n   private List<PluggableFunctionRegistry> pluggableFuncRegistries = Lists.newArrayList();\n-  private OptionManager optionManager = null;\n+  private final OptionManager optionManager;\n+  private final boolean useDynamicUdfs;\n \n-  @Deprecated @VisibleForTesting\n-  public FunctionImplementationRegistry(DrillConfig config){\n-    this(config, ClassPathScanner.fromPrescan(config));\n+  @VisibleForTesting\n+  public FunctionImplementationRegistry(DrillConfig config) {\n+    this(config, ClassPathScanner.fromPrescan(config), null);\n   }\n \n-  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan){\n+  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan) {\n+    this(config, classpathScan, null);\n+  }\n+\n+  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan, OptionManager optionManager) {\n     Stopwatch w = Stopwatch.createStarted();\n \n     logger.debug(\"Generating function registry.\");\n+    this.optionManager = optionManager;\n+\n+    // Unit tests fail if dynamic UDFs are turned on AND the test happens\n+    // to access an undefined function. Since we want a reasonable failure\n+    // rather than a crash, we provide a boot-time option, set only by\n+    // tests, to disable DUDF lookup.\n+\n+    useDynamicUdfs = ! config.getBoolean(ExecConstants.UDF_DISABLE_DYNAMIC);\n     localFunctionRegistry = new LocalFunctionRegistry(classpathScan);\n \n     Set<Class<? extends PluggableFunctionRegistry>> registryClasses =\n@@ -123,11 +136,6 @@ public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathSc\n     this.localUdfDir = getLocalUdfDir(config);\n   }\n \n-  public FunctionImplementationRegistry(DrillConfig config, ScanResult classpathScan, OptionManager optionManager) {\n-    this(config, classpathScan);\n-    this.optionManager = optionManager;\n-  }\n-\n   /**\n    * Register functions in given operator table.\n    * @param operatorTable operator table\n@@ -142,7 +150,7 @@ public void register(DrillOperatorTable operatorTable) {\n   }\n \n   /**\n-   * First attempts to finds the Drill function implementation that matches the name, arg types and return type.\n+   * First attempts to find the Drill function implementation that matches the name, arg types and return type.\n    * If exact function implementation was not found,\n    * syncs local function registry with remote function registry if needed\n    * and tries to find function implementation one more time\n@@ -156,17 +164,25 @@ public void register(DrillOperatorTable operatorTable) {\n   public DrillFuncHolder findDrillFunction(FunctionResolver functionResolver, FunctionCall functionCall) {\n     AtomicLong version = new AtomicLong();\n     String newFunctionName = functionReplacement(functionCall);\n-    List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n-    FunctionResolver exactResolver = FunctionResolverFactory.getExactResolver(functionCall);\n-    DrillFuncHolder holder = exactResolver.getBestMatch(functions, functionCall);\n \n-    if (holder == null) {\n+    // Dynamic UDFS: First try with exact match. If not found, we may need to\n+    // update the registry, so sync with remote.\n+\n+    if (useDynamicUdfs) {\n+      List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n+      FunctionResolver exactResolver = FunctionResolverFactory.getExactResolver(functionCall);\n+      DrillFuncHolder holder = exactResolver.getBestMatch(functions, functionCall);\n+      if (holder != null) {\n+        return holder;\n+      }\n       syncWithRemoteRegistry(version.get());\n-      List<DrillFuncHolder> updatedFunctions = localFunctionRegistry.getMethods(newFunctionName, version);\n-      holder = functionResolver.getBestMatch(updatedFunctions, functionCall);\n     }\n \n-    return holder;\n+    // Whether Dynamic UDFs or not: look in the registry for\n+    // an inexact match.\n+\n+    List<DrillFuncHolder> functions = localFunctionRegistry.getMethods(newFunctionName, version);\n+    return functionResolver.getBestMatch(functions, functionCall);\n   }\n \n   /**\n@@ -177,16 +193,20 @@ public DrillFuncHolder findDrillFunction(FunctionResolver functionResolver, Func\n    */\n   private String functionReplacement(FunctionCall functionCall) {\n     String funcName = functionCall.getName();\n-      if (functionCall.args.size() > 0) {\n-          MajorType majorType =  functionCall.args.get(0).getMajorType();\n-          DataMode dataMode = majorType.getMode();\n-          MinorType minorType = majorType.getMinorType();\n-          if (optionManager != null\n-              && optionManager.getOption(ExecConstants.CAST_TO_NULLABLE_NUMERIC).bool_val\n-              && CastFunctions.isReplacementNeeded(funcName, minorType)) {\n-              funcName = CastFunctions.getReplacingCastFunction(funcName, dataMode, minorType);\n-          }\n-      }\n+    if (functionCall.args.size() == 0) {\n+      return funcName;\n+    }\n+    boolean castToNullableNumeric = optionManager != null &&\n+                  optionManager.getOption(ExecConstants.CAST_TO_NULLABLE_NUMERIC_OPTION);\n+    if (! castToNullableNumeric) {\n+      return funcName;\n+    }\n+    MajorType majorType =  functionCall.args.get(0).getMajorType();\n+    DataMode dataMode = majorType.getMode();\n+    MinorType minorType = majorType.getMinorType();\n+    if (CastFunctions.isReplacementNeeded(funcName, minorType)) {\n+        funcName = CastFunctions.getReplacingCastFunction(funcName, dataMode, minorType);\n+    }\n \n     return funcName;\n   }\n@@ -200,7 +220,7 @@ private String functionReplacement(FunctionCall functionCall) {\n    * @return exactly matching function holder\n    */\n   public DrillFuncHolder findExactMatchingDrillFunction(String name, List<MajorType> argTypes, MajorType returnType) {\n-    return findExactMatchingDrillFunction(name, argTypes, returnType, true);\n+    return findExactMatchingDrillFunction(name, argTypes, returnType, useDynamicUdfs);\n   }\n \n   /**\n@@ -315,6 +335,7 @@ public RemoteFunctionRegistry getRemoteFunctionRegistry() {\n    * @param version remote function registry local function registry was based on\n    * @return true if remote and local function registries were synchronized after given version\n    */\n+  @SuppressWarnings(\"resource\")\n   public boolean syncWithRemoteRegistry(long version) {\n     if (isRegistrySyncNeeded(remoteFunctionRegistry.getRegistryVersion(), localFunctionRegistry.getVersion())) {\n       synchronized (this) {\n@@ -495,6 +516,7 @@ private File getTmpDir(DrillConfig config) {\n    * @return local path to jar that was copied\n    * @throws IOException in case of problems during jar coping process\n    */\n+  @SuppressWarnings(\"resource\")\n   private Path copyJarToLocal(String jarName, RemoteFunctionRegistry remoteFunctionRegistry) throws IOException {\n     Path registryArea = remoteFunctionRegistry.getRegistryArea();\n     FileSystem fs = remoteFunctionRegistry.getFs();\n@@ -549,7 +571,7 @@ public void close() {\n   private class UnregistrationListener implements TransientStoreListener {\n \n     @Override\n-    public void onChange(TransientStoreEvent event) {\n+    public void onChange(TransientStoreEvent<?> event) {\n       String jarName = (String) event.getValue();\n       localFunctionRegistry.unregister(jarName);\n       String localDir = localUdfDir.toUri().getPath();",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/FunctionImplementationRegistry.java",
                "sha": "c1ba2d8e96e6bbf9a95601ee936baa7cbf8871f3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/resources/drill-module.conf",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/drill-module.conf?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/resources/drill-module.conf",
                "patch": "@@ -245,6 +245,9 @@ drill.exec: {\n   },\n   udf: {\n     retry-attempts: 5,\n+    // Disables (parts of) the dynamic UDF functionality.\n+    // Primarily for testing.\n+    disable_dynamic: false,\n     directory: {\n       // Base directory for remote and local udf directories, unique among clusters.\n       base: ${drill.exec.zk.root}\"/udf\",",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/main/resources/drill-module.conf",
                "sha": "3d66d19f2029edfa72714691bcdcf8004cb6edf6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "patch": "@@ -52,7 +52,7 @@\n     GuavaPatcher.patch();\n   }\n \n-  private static final DrillConfig c = DrillConfig.create();\n+  protected static final DrillConfig c = DrillConfig.create();\n \n   @After\n   public void clear(){",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/ExecTest.java",
                "sha": "dead858fd56fd0c8768ae95e811299cda6499497",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java?ref=2f13c08f35152639e4619d2898b2ca8fe7115259",
                "deletions": 3,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "patch": "@@ -61,11 +61,10 @@\n import mockit.Injectable;\n \n public class TestSimpleFunctions extends ExecTest {\n-  //private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TestSimpleFunctions.class);\n-  private final DrillConfig c = DrillConfig.create();\n \n   @Test\n   public void testHashFunctionResolution() throws JClassAlreadyExistsException, IOException {\n+    @SuppressWarnings(\"resource\")\n     final FunctionImplementationRegistry registry = new FunctionImplementationRegistry(c);\n     // test required vs nullable Int input\n     resolveHash(c,\n@@ -133,7 +132,6 @@ public void resolveHash(DrillConfig config, LogicalExpression arg, TypeProtos.Ma\n                                     FunctionImplementationRegistry registry) throws JClassAlreadyExistsException, IOException {\n     final List<LogicalExpression> args = new ArrayList<>();\n     args.add(arg);\n-    final String[] registeredNames = { \"hash\" };\n     FunctionCall call = new FunctionCall(\n         \"hash\",\n         args,",
                "raw_url": "https://github.com/apache/drill/raw/2f13c08f35152639e4619d2898b2ca8fe7115259/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/TestSimpleFunctions.java",
                "sha": "6c48651a42d08c9e152cc4a5ab04db6d0f640e9b",
                "status": "modified"
            }
        ],
        "message": "DRILL-5330: NPE in FunctionImplementationRegistry\n\nFixes:\n\n* DRILL-5330: NPE in\nFunctionImplementationRegistry.functionReplacement()\n* DRILL-5331:\nNPE in FunctionImplementationRegistry.findDrillFunction() if dynamic\nUDFs disabled\n\nWhen running in a unit test, the dynamic UDF (DUDF) mechanism is not\navailable. When running in production, the DUDF mechanism is available,\nbut may be disabled.\n\nOne confusing aspect of this code is that the function registry\nis given the option manager, but the option manager is not yet valid\n(not yet initialized) in the function registry constructor. So, we\ncannot access the option manager in the function registry constructor.\n\nIn any event, the existing system options cannot be used to disable DUDF\nsupport. For obscure reasons, DUDF support is always enabled, even when\ndisabled by the user.\n\nInstead, for DRILL-5331, we added a config option to \"really\" disable DUDFS.\nThe property is set only for tests, disables DUDF support.\nNote that, in the future, this option could be generalized to\n\"off, read-only, on\" to capture the full set of DUDF modes.\nBut, for now, just turning this off is sufficient.\n\nFor DRILL-5330, we use an existing option validator rather than\naccessing the raw option directly.\n\nAlso includes a bit of code cleanup in the class in question.\n\nThe result is that the code now works when used in a sub-operator unit\ntest.\n\nclose apache/drill#777",
        "parent": "https://github.com/apache/drill/commit/90bc80052f66874cf9eaf8b8bc93f28f703b5506",
        "patched_files": [
            "TransientStoreListener.java",
            "FunctionImplementationRegistry.java",
            "ExecConstants.java",
            "drill-module.conf"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestSimpleFunctions.java",
            "ExecTest.java"
        ]
    },
    "drill_3896a58": {
        "bug_id": "drill_3896a58",
        "commit": "https://github.com/apache/drill/commit/3896a58243f310c5d9466a98edc205b61f9dd2e7",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 1,
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "patch": "@@ -95,7 +95,7 @@ public void testGetJsonObject() throws Exception {\n   @Test // DRILL-3272\n   public void testIf() throws Exception {\n     testBuilder()\n-        .sqlQuery(\"select `if`(1999 > 2000, 'latest', 'old') Period from hive.kv limit 1\")\n+        .sqlQuery(\"select `if`(1999 > 2000, 'latest', 'old') `Period` from hive.kv limit 1\")\n         .ordered()\n         .baselineColumns(\"Period\")\n         .baselineValues(\"old\")",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/fn/hive/TestInbuiltHiveUDFs.java",
                "sha": "d4e0b5cb9c29d3ddcfb4a034067c97e1bfdf95eb",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 3,
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -469,7 +469,7 @@ public void selectUser0_db_u1g1_only() throws Exception {\n \n     errorMsgTestHelper(\n         String.format(\"SELECT * FROM hive.%s.%s ORDER BY gpa DESC LIMIT 2\", db_u1g1_only, u1g1_student_all_755),\n-        String.format(\"Table 'hive.%s.%s' not found\", db_u1g1_only, u1g1_student_all_755));\n+        String.format(\"Object '%s' not found within 'hive.%s'\", u1g1_student_all_755, db_u1g1_only));\n   }\n \n   // Try to read the tables \"user1\" has access to read in db_general.\n@@ -489,7 +489,7 @@ public void selectUser1_db_u0_only() throws Exception {\n \n     errorMsgTestHelper(\n         String.format(\"SELECT * FROM hive.%s.%s ORDER BY gpa DESC LIMIT 2\", db_u0_only, u0_student_all_755),\n-        String.format(\"Table 'hive.%s.%s' not found\", db_u0_only, u0_student_all_755));\n+        String.format(\"Object '%s' not found within 'hive.%s'\", u0_student_all_755, db_u0_only));\n   }\n \n   private static void queryViewHelper(final String queryUser, final String query) throws Exception {",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/impersonation/hive/TestStorageBasedHiveAuthorization.java",
                "sha": "685d3bf6320bc06efd76528521d11bd9c91abccd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -88,7 +88,7 @@ private SqlNode expandAvg(\n       final SqlNode arg) {\n     final SqlParserPos pos = SqlParserPos.ZERO;\n     final SqlNode sum =\n-        SqlStdOperatorTable.SUM.createCall(pos, arg);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, arg);\n     final SqlNode count =\n         SqlStdOperatorTable.COUNT.createCall(pos, arg);\n     final SqlNode sumAsDouble =\n@@ -128,9 +128,9 @@ private SqlNode expandVariance(\n     final SqlNode argSquared =\n         SqlStdOperatorTable.MULTIPLY.createCall(pos, castHighArg, castHighArg);\n     final SqlNode sumArgSquared =\n-        SqlStdOperatorTable.SUM.createCall(pos, argSquared);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, argSquared);\n     final SqlNode sum =\n-        SqlStdOperatorTable.SUM.createCall(pos, castHighArg);\n+        DrillCalciteSqlAggFunctionWrapper.SUM.createCall(pos, castHighArg);\n     final SqlNode sumSquared =\n         SqlStdOperatorTable.MULTIPLY.createCall(pos, sum, sum);\n     final SqlNode count =",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillAvgVarianceConvertlet.java",
                "sha": "bfb4c05c6c73e87c2fecdb9db1939135cebaf2f7",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.calcite.sql.SqlOperatorBinding;\n import org.apache.calcite.sql.SqlSyntax;\n import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.type.SqlReturnTypeInference;\n import org.apache.calcite.sql.validate.SqlMonotonicity;\n import org.apache.calcite.sql.validate.SqlValidator;\n@@ -42,6 +43,11 @@\n  * simply forwards the method calls to the wrapped SqlAggFunction.\n  */\n public class DrillCalciteSqlAggFunctionWrapper extends SqlAggFunction implements DrillCalciteSqlWrapper {\n+\n+  public final static DrillCalciteSqlAggFunctionWrapper SUM =\n+      new DrillCalciteSqlAggFunctionWrapper(SqlStdOperatorTable.SUM,\n+          SqlStdOperatorTable.SUM.getReturnTypeInference());\n+\n   private final SqlAggFunction operator;\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/DrillCalciteSqlAggFunctionWrapper.java",
                "sha": "bd46d2d45a8594c55be5cb897575f8548252b65e",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.util.Set;\n \n import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Sets;\n import org.apache.calcite.adapter.java.JavaTypeFactory;\n@@ -43,7 +44,10 @@\n import org.apache.calcite.rel.type.RelDataTypeFactory;\n import org.apache.calcite.rel.type.RelDataTypeSystemImpl;\n import org.apache.calcite.rex.RexBuilder;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexLiteral;\n import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.runtime.Hook;\n import org.apache.calcite.schema.SchemaPlus;\n import org.apache.calcite.sql.SqlNode;\n import org.apache.calcite.sql.SqlOperatorTable;\n@@ -73,7 +77,6 @@\n import org.apache.drill.exec.planner.logical.DrillConstExecutor;\n import org.apache.drill.exec.planner.physical.DrillDistributionTraitDef;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n-import org.apache.drill.exec.planner.sql.parser.impl.DrillParserWithCompoundIdConverter;\n import org.apache.drill.exec.rpc.user.UserSession;\n \n import com.google.common.base.Joiner;\n@@ -106,6 +109,7 @@\n \n   private String sql;\n   private VolcanoPlanner planner;\n+  private boolean useRootSchema = false;\n \n \n   public SqlConverter(QueryContext context) {\n@@ -217,6 +221,15 @@ public void disallowTemporaryTables() {\n     catalog.disallowTemporaryTables();\n   }\n \n+  /**\n+   * Is root schema path should be used as default schema path.\n+   *\n+   * @param useRoot flag\n+   */\n+  public void useRootSchemaAsDefault(boolean useRoot) {\n+    useRootSchema = useRoot;\n+  }\n+\n   private class DrillValidator extends SqlValidatorImpl {\n     private final Set<SqlValidatorScope> identitySet = Sets.newIdentityHashSet();\n \n@@ -273,6 +286,14 @@ public RelRoot toRel(final SqlNode validatedNode) {\n     final SqlToRelConverter sqlToRelConverter =\n         new SqlToRelConverter(new Expander(), validator, catalog, cluster, DrillConvertletTable.INSTANCE,\n             sqlToRelConverterConfig);\n+\n+    /*\n+     * Sets value to false to avoid simplifying project expressions\n+     * during creating new projects since it may cause changing data mode\n+     * which causes to assertion errors during type validation\n+     */\n+    Hook.REL_BUILDER_SIMPLIFY.add(Hook.property(false));\n+\n     //To avoid unexpected column errors set a value of top to false\n     final RelRoot rel = sqlToRelConverter.convertQuery(validatedNode, false, false);\n     final RelRoot rel2 = rel.withRel(sqlToRelConverter.flattenTypes(rel.rel, true));\n@@ -430,6 +451,28 @@ public RexNode ensureType(\n         boolean matchNullability) {\n       return node;\n     }\n+\n+    /**\n+     * Creates a call to the CAST operator, expanding if possible, and optionally\n+     * also preserving nullability.\n+     *\n+     * <p>Tries to expand the cast, and therefore the result may be something\n+     * other than a {@link RexCall} to the CAST operator, such as a\n+     * {@link RexLiteral} if {@code matchNullability} is false.\n+     *\n+     * @param type             Type to cast to\n+     * @param exp              Expression being cast\n+     * @param matchNullability Whether to ensure the result has the same\n+     *                         nullability as {@code type}\n+     * @return Call to CAST operator\n+     */\n+    @Override\n+    public RexNode makeCast(RelDataType type, RexNode exp, boolean matchNullability) {\n+      if (matchNullability) {\n+        return makeAbstractCast(type, exp);\n+      }\n+      return super.makeCast(type, exp, false);\n+    }\n   }\n \n   /**\n@@ -506,6 +549,14 @@ public void disallowTemporaryTables() {\n       return table;\n     }\n \n+    @Override\n+    public List<List<String>> getSchemaPaths() {\n+      if (useRootSchema) {\n+        return ImmutableList.<List<String>>of(ImmutableList.<String>of());\n+      }\n+      return super.getSchemaPaths();\n+    }\n+\n     /**\n      * check if the schema provided is a valid schema:\n      * <li>schema is not indicated (only one element in the names list)<li/>",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/SqlConverter.java",
                "sha": "f9005878d2bda142d9aa91b740ef7c80cc4ade88",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "patch": "@@ -41,8 +41,6 @@\n import org.apache.calcite.rel.core.TableFunctionScan;\n import org.apache.calcite.rel.core.TableScan;\n import org.apache.calcite.rel.logical.LogicalValues;\n-import org.apache.calcite.rel.metadata.CachingRelMetadataProvider;\n-import org.apache.calcite.rel.metadata.ChainedRelMetadataProvider;\n import org.apache.calcite.rel.metadata.JaninoRelMetadataProvider;\n import org.apache.calcite.rel.metadata.RelMetadataProvider;\n import org.apache.calcite.rel.metadata.RelMetadataQuery;\n@@ -622,7 +620,7 @@ public Void visitOp(PhysicalOperator op, Collection<PhysicalOperator> collection\n \n   }\n \n-  private Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException, RelConversionException, ForemanSetupException {\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException, RelConversionException, ForemanSetupException {\n     final SqlNode sqlNodeValidated = config.getConverter().validate(sqlNode);\n     final Pair<SqlNode, RelDataType> typedSqlNode = new Pair<>(sqlNodeValidated, config.getConverter().getOutputType(\n         sqlNodeValidated));",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DefaultSqlHandler.java",
                "sha": "5c3432309f4ee87de20a390d43fd8bc0c719e191",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "patch": "@@ -28,8 +28,8 @@\n \n import java.util.List;\n \n+import org.apache.calcite.rel.type.RelDataType;\n import org.apache.calcite.schema.SchemaPlus;\n-import org.apache.calcite.sql.SqlDescribeTable;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n import org.apache.calcite.sql.SqlNode;\n@@ -38,9 +38,12 @@\n import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.parser.SqlParserPos;\n import org.apache.calcite.tools.RelConversionException;\n+import org.apache.calcite.tools.ValidationException;\n+import org.apache.calcite.util.Pair;\n import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.planner.sql.SqlConverter;\n import org.apache.drill.exec.planner.sql.parser.DrillParserUtil;\n import org.apache.drill.exec.planner.sql.parser.DrillSqlDescribeTable;\n import org.apache.drill.exec.work.foreman.ForemanSetupException;\n@@ -134,4 +137,15 @@ public SqlNode rewrite(SqlNode sqlNode) throws RelConversionException, ForemanSe\n           .build(logger);\n     }\n   }\n+\n+  @Override\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException,\n+      RelConversionException, ForemanSetupException {\n+    SqlConverter converter = config.getConverter();\n+    // set this to true since INFORMATION_SCHEMA in the root schema, not in the default\n+    converter.useRootSchemaAsDefault(true);\n+    Pair<SqlNode, RelDataType> sqlNodeRelDataTypePair = super.validateNode(sqlNode);\n+    converter.useRootSchemaAsDefault(false);\n+    return sqlNodeRelDataTypePair;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeTableHandler.java",
                "sha": "4d01424153afd39ee21311a104a2aefc3424b370",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "patch": "@@ -25,6 +25,7 @@\n \n import java.util.List;\n \n+import org.apache.calcite.rel.type.RelDataType;\n import org.apache.calcite.schema.SchemaPlus;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n@@ -34,9 +35,12 @@\n import org.apache.calcite.sql.fun.SqlStdOperatorTable;\n import org.apache.calcite.sql.parser.SqlParserPos;\n import org.apache.calcite.tools.RelConversionException;\n+import org.apache.calcite.tools.ValidationException;\n+import org.apache.calcite.util.Pair;\n import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.planner.sql.SqlConverter;\n import org.apache.drill.exec.planner.sql.parser.DrillParserUtil;\n import org.apache.drill.exec.planner.sql.parser.SqlShowTables;\n import org.apache.drill.exec.store.AbstractSchema;\n@@ -105,4 +109,15 @@ public SqlNode rewrite(SqlNode sqlNode) throws RelConversionException, ForemanSe\n     return new SqlSelect(SqlParserPos.ZERO, null, new SqlNodeList(selectList, SqlParserPos.ZERO),\n         fromClause, where, null, null, null, null, null, null);\n   }\n+\n+  @Override\n+  protected Pair<SqlNode, RelDataType> validateNode(SqlNode sqlNode) throws ValidationException,\n+      RelConversionException, ForemanSetupException {\n+    SqlConverter converter = config.getConverter();\n+    // set this to true since INFORMATION_SCHEMA in the root schema, not in the default\n+    converter.useRootSchemaAsDefault(true);\n+    Pair<SqlNode, RelDataType> sqlNodeRelDataTypePair = super.validateNode(sqlNode);\n+    converter.useRootSchemaAsDefault(false);\n+    return sqlNodeRelDataTypePair;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/ShowTablesHandler.java",
                "sha": "7084877b426271fd8b7e4de9d7d1faaf2ec9b008",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "patch": "@@ -86,9 +86,9 @@ public static RelNode resolveNewTableRel(boolean isNewTableView, List<String> ta\n             .build(logger);\n       }\n \n-      // CTAS's query field list shouldn't have \"*\" when table's field list is specified.\n+      // CTAS's query field list shouldn't have \"**\" when table's field list is specified.\n       for (String field : validatedRowtype.getFieldNames()) {\n-        if (field.equals(\"*\")) {\n+        if (field.equals(\"**\")) {\n           final String tblType = isNewTableView ? \"view\" : \"table\";\n           throw UserException.validationError()\n               .message(\"%s's query field list has a '*', which is invalid when %s's field list is specified.\",",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/SqlHandlerUtil.java",
                "sha": "72d269999b41f6ecf3d012c092a6eee448b0f8af",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 35,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -20,6 +20,7 @@\n import java.util.Collections;\n import java.util.List;\n \n+import com.google.common.base.Function;\n import org.apache.calcite.sql.SqlBasicCall;\n import org.apache.calcite.sql.SqlIdentifier;\n import org.apache.calcite.sql.SqlLiteral;\n@@ -30,13 +31,19 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n \n-public class DrillCompoundIdentifier extends SqlIdentifier{\n+public class DrillCompoundIdentifier extends SqlIdentifier {\n \n-  List<IdentifierHolder> ids;\n+  private static final Function<String, String> STAR_TO_EMPTY = new Function<String, String>() {\n+    public String apply(String s) {\n+      return s.equals(\"*\") ? \"\" : s;\n+    }\n+  };\n+\n+  private final List<IdentifierHolder> ids;\n \n-  private static List<String> getNames(List<IdentifierHolder> identifiers){\n+  private static List<String> getNames(List<IdentifierHolder> identifiers) {\n     List<String> names = Lists.newArrayListWithCapacity(identifiers.size());\n-    for(IdentifierHolder h : identifiers){\n+    for (IdentifierHolder h : identifiers) {\n       names.add(h.value);\n     }\n     return names;\n@@ -47,74 +54,69 @@ public DrillCompoundIdentifier(List<IdentifierHolder> identifiers) {\n     this.ids = identifiers;\n   }\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DrillCompoundIdentifier.class);\n-\n-  public static Builder newBuilder(){\n+  public static Builder newBuilder() {\n     return new Builder();\n   }\n \n   public static class Builder {\n     private List<IdentifierHolder> identifiers = Lists.newArrayList();\n \n-    public DrillCompoundIdentifier build(){\n+    public DrillCompoundIdentifier build() {\n       return new DrillCompoundIdentifier(identifiers);\n     }\n \n-    public void addString(String name, SqlParserPos pos){\n+    public void addString(String name, SqlParserPos pos) {\n       identifiers.add(new IdentifierHolder(name, pos, false));\n     }\n \n-    public void addIndex(int index, SqlParserPos pos){\n+    public void addIndex(int index, SqlParserPos pos) {\n       identifiers.add(new IdentifierHolder(Integer.toString(index), pos, true));\n     }\n   }\n \n-  public SqlNode getAsSqlNode(){\n-    if(ids.size() == 1){\n+  public SqlNode getAsSqlNode() {\n+    if (ids.size() == 1) {\n       return new SqlIdentifier(Collections.singletonList(ids.get(0).value), ids.get(0).parserPos);\n     }\n \n     int startIndex;\n     SqlNode node;\n \n-    if(ids.get(1).isArray()){\n+    if (ids.get(1).isArray()) {\n       // handle everything post zero index as item operator.\n       startIndex = 1;\n-      node = new SqlIdentifier( //\n-          ImmutableList.of(ids.get(0).value), //\n-          null, //\n-          ids.get(0).parserPos, //\n+      node = new SqlIdentifier(\n+          ImmutableList.of(ids.get(0).value),\n+          null,\n+          ids.get(0).parserPos,\n           ImmutableList.of(ids.get(0).parserPos));\n-    }else{\n+    } else {\n       // handle everything post two index as item operator.\n       startIndex = 2;\n-      node = new SqlIdentifier( //\n-          ImmutableList.of(ids.get(0).value, ids.get(1).value), //\n-          null, //\n-          ids.get(0).parserPos, //\n+      node = new SqlIdentifier(\n+          // Replaces star by empty string. See SqlIdentifier#isStar()\n+          ImmutableList.of(ids.get(0).value, STAR_TO_EMPTY.apply(ids.get(1).value)), null,\n+          ids.get(0).parserPos,\n           ImmutableList.of(ids.get(0).parserPos, ids.get(1).parserPos));\n-\n     }\n-    for(int i = startIndex ; i < ids.size(); i++){\n+    for (int i = startIndex; i < ids.size(); i++) {\n       node = ids.get(i).getNode(node);\n     }\n \n     return node;\n   }\n \n-\n-  public SqlNode getAsCompoundIdentifier(){\n+  public SqlNode getAsCompoundIdentifier() {\n     List<String> names = Lists.newArrayListWithCapacity(ids.size());\n     List<SqlParserPos> pos = Lists.newArrayListWithCapacity(ids.size());\n-    for(int i =0; i < ids.size(); i++){\n-      IdentifierHolder holder = ids.get(i);\n+    for (IdentifierHolder holder : ids) {\n       names.add(holder.value);\n       pos.add(holder.parserPos);\n     }\n     return new SqlIdentifier(names, null, pos.get(0), pos);\n   }\n \n-  private static class IdentifierHolder{\n+  private static class IdentifierHolder {\n     String value;\n     SqlParserPos parserPos;\n     boolean isArray;\n@@ -126,18 +128,18 @@ public IdentifierHolder(String value, SqlParserPos parserPos, boolean isArray) {\n       this.parserPos = parserPos;\n     }\n \n-    public boolean isArray(){\n+    public boolean isArray() {\n       return isArray;\n     }\n \n-    public SqlNode getNode(SqlNode node){\n+    public SqlNode getNode(SqlNode node) {\n       SqlLiteral literal;\n-      if(isArray){\n+      if (isArray) {\n         literal = SqlLiteral.createExactNumeric(value, parserPos);\n-      }else{\n+      } else {\n         literal = SqlLiteral.createCharString(value, parserPos);\n       }\n-      return new SqlBasicCall(SqlStdOperatorTable.ITEM, new SqlNode[]{ node, literal }, parserPos);\n+      return new SqlBasicCall(SqlStdOperatorTable.ITEM, new SqlNode[]{node, literal}, parserPos);\n     }\n \n   }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/parser/DrillCompoundIdentifier.java",
                "sha": "a6c75c13fec37c8a64ee3250aa6f1c86302c80b8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.drill.exec.store.ischema;\n \n+import static org.apache.drill.exec.planner.types.DrillRelDataTypeSystem.DRILL_REL_DATATYPE_SYSTEM;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.CATS_COL_CATALOG_NAME;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.COLS_COL_COLUMN_NAME;\n import static org.apache.drill.exec.store.ischema.InfoSchemaConstants.IS_CATALOG_CONNECT;\n@@ -231,7 +232,7 @@ public void visitTables(String schemaPath, SchemaPlus schema) {\n       // Visit the table, and if requested ...\n       if(shouldVisitTable(schemaPath, tableName, tableType) && visitTable(schemaPath, tableName, table)) {\n         // ... do for each of the table's fields.\n-        final RelDataType tableRow = table.getRowType(new JavaTypeFactoryImpl());\n+        final RelDataType tableRow = table.getRowType(new JavaTypeFactoryImpl(DRILL_REL_DATATYPE_SYSTEM));\n         for (RelDataTypeField field: tableRow.getFieldList()) {\n           if (shouldVisitColumn(schemaPath, tableName, field.getName())) {\n             visitField(schemaPath, tableName, field);",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/main/java/org/apache/drill/exec/store/ischema/InfoSchemaRecordGenerator.java",
                "sha": "d2c8c6f32f3dfb5c99e0f0d881442e104c6190e3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 2,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "patch": "@@ -102,14 +102,14 @@ public void testFromInfoSchema() throws Exception {\n \n   @Test\n   public void testTPCH1() throws Exception {\n-    String expectedColNames = \" \\\"columns\\\" : [ \\\"`l_returnflag`\\\", \\\"`l_linestatus`\\\", \\\"`l_shipdate`\\\", \\\"`l_quantity`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\", \\\"`l_tax`\\\" ]\";\n+    String expectedColNames = \" \\\"columns\\\" : [ \\\"`l_shipdate`\\\", \\\"`l_returnflag`\\\", \\\"`l_linestatus`\\\", \\\"`l_quantity`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\", \\\"`l_tax`\\\" ]\";\n     testPhysicalPlanFromFile(\"queries/tpch/01.sql\", expectedColNames);\n   }\n \n   @Test\n   public void testTPCH3() throws Exception {\n     String expectedColNames1 = \"\\\"columns\\\" : [ \\\"`c_mktsegment`\\\", \\\"`c_custkey`\\\" ]\";\n-    String expectedColNames2 = \" \\\"columns\\\" : [ \\\"`o_orderdate`\\\", \\\"`o_shippriority`\\\", \\\"`o_custkey`\\\", \\\"`o_orderkey`\\\" \";\n+    String expectedColNames2 = \" \\\"columns\\\" : [ \\\"`o_custkey`\\\", \\\"`o_orderkey`\\\", \\\"`o_orderdate`\\\", \\\"`o_shippriority`\\\" ]\";\n     String expectedColNames3 = \"\\\"columns\\\" : [ \\\"`l_orderkey`\\\", \\\"`l_shipdate`\\\", \\\"`l_extendedprice`\\\", \\\"`l_discount`\\\" ]\";\n     testPhysicalPlanFromFile(\"queries/tpch/03.sql\", expectedColNames1, expectedColNames2, expectedColNames3);\n   }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestProjectPushDown.java",
                "sha": "ad55a0dd19d07d4d3f18b7379c58f4310ce961b5",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 15,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "patch": "@@ -16,17 +16,16 @@\n  */\n package org.apache.drill;\n \n-import mockit.Deencapsulation;\n import mockit.Mock;\n import mockit.MockUp;\n import mockit.integration.junit4.JMockit;\n-import org.apache.calcite.util.SaffronProperties;\n+import org.apache.calcite.util.Util;\n import org.apache.drill.common.exceptions.UserRemoteException;\n import org.apache.drill.test.BaseTestQuery;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n \n-import java.util.Properties;\n+import java.nio.charset.Charset;\n \n import static org.hamcrest.CoreMatchers.containsString;\n import static org.junit.Assert.assertThat;\n@@ -47,19 +46,16 @@ public void testUtf8SupportInQueryStringByDefault() throws Exception {\n \n   @Test(expected = UserRemoteException.class)\n   public void testDisableUtf8SupportInQueryString() throws Exception {\n-    Deencapsulation.setField(SaffronProperties.class, \"properties\", null);\n-    final Properties properties = System.getProperties();\n     final String charset = \"ISO-8859-1\";\n-    new MockUp<System>()\n+\n+    // Mocked Util.getDefaultCharset() since it uses static field Util.DEFAULT_CHARSET\n+    // which is initialized when declared using SaffronProperties.INSTANCE field which also is initialized\n+    // when declared.\n+    new MockUp<Util>()\n     {\n       @Mock\n-      Properties getProperties() {\n-        Properties newProperties = new Properties();\n-        newProperties.putAll(properties);\n-        newProperties.put(\"saffron.default.charset\", charset);\n-        newProperties.put(\"saffron.default.nationalcharset\", charset);\n-        newProperties.put(\"saffron.default.collation.name\", charset + \"$en_US\");\n-        return newProperties;\n+      Charset getDefaultCharset() {\n+        return Charset.forName(charset);\n       }\n     };\n \n@@ -70,8 +66,6 @@ Properties getProperties() {\n       assertThat(e.getMessage(), containsString(\n           String.format(\"Failed to encode '%s' in character set '%s'\", hello, charset)));\n       throw e;\n-    } finally {\n-      Deencapsulation.setField(SaffronProperties.class, \"properties\", null);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/TestUtf8SupportInQueryString.java",
                "sha": "a515763d89df201d8b1a66f22c70fb03371a9345",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "patch": "@@ -120,6 +120,9 @@ public void invalidQueryParserError() throws Exception {\n    */\n   @Test\n   public void invalidQueryValidationError() throws Exception {\n-    createPrepareStmt(\"SELECT * sdflkgdh\", true, ErrorType.PARSE /** Drill returns incorrect error for parse error*/);\n+    // CALCITE-1120 allows SELECT without from syntax.\n+    // So with this change the query fails with VALIDATION error.\n+    createPrepareStmt(\"SELECT * sdflkgdh\", true,\n+        ErrorType.VALIDATION /* Drill returns incorrect error for parse error*/);\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/java-exec/src/test/java/org/apache/drill/exec/work/prepare/TestPreparedStatementProvider.java",
                "sha": "97452970a9129a18b0450fb0006c8ec4d0b321c5",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 5,
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "patch": "@@ -1113,7 +1113,7 @@ public ExecuteResult prepareAndExecute(StatementHandle h, String sql, long maxRo\n \n   @Override\n   public ExecuteResult prepareAndExecute(final StatementHandle handle, final String sql, final long maxRowCount,\n-                                         int maxRowsInFirstFrame, final PrepareCallback callback) throws NoSuchStatementException {\n+        int maxRowsInFirstFrame, final PrepareCallback callback) throws NoSuchStatementException {\n     return prepareAndExecute(handle, sql, maxRowCount, callback);\n   }\n \n@@ -1133,13 +1133,17 @@ public Frame fetch(StatementHandle statementHandle, long l, int i) throws NoSuch\n   }\n \n   @Override\n-  public ExecuteResult execute(StatementHandle statementHandle, List<TypedValue> list, long l) throws NoSuchStatementException {\n-    throw new UnsupportedOperationException(this.getClass().getSimpleName());\n+  public ExecuteResult execute(StatementHandle statementHandle,\n+        List<TypedValue> list, long l) throws NoSuchStatementException {\n+    return new ExecuteResult(Collections.singletonList(\n+        MetaResultSet.create(statementHandle.connectionId, statementHandle.id,\n+            true, statementHandle.signature, null)));\n   }\n \n   @Override\n-  public ExecuteResult execute(StatementHandle statementHandle, List<TypedValue> list, int i) throws NoSuchStatementException {\n-    return null;\n+  public ExecuteResult execute(StatementHandle statementHandle,\n+      List<TypedValue> list, int i) throws NoSuchStatementException {\n+    return execute(statementHandle, list, (long) i);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillMetaImpl.java",
                "sha": "0b33167fb180c3b886bb5c4ae888bb70bfcb3f39",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 3,
                "filename": "exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "patch": "@@ -38,7 +38,7 @@\n  * <p>\n  * This class has sub-classes which implement JDBC 3.0 and JDBC 4.0 APIs; it is\n  * instantiated using\n- * {@link net.hydromatic.avatica.AvaticaFactory#newPreparedStatement}.\n+ * {@link org.apache.calcite.avatica.AvaticaFactory#newPreparedStatement}.\n  * </p>\n  */\n abstract class DrillPreparedStatementImpl extends AvaticaPreparedStatement\n@@ -58,7 +58,9 @@ protected DrillPreparedStatementImpl(DrillConnectionImpl connection,\n           resultSetType, resultSetConcurrency, resultSetHoldability);\n     connection.openStatementsRegistry.addStatement(this);\n     this.preparedStatementHandle = preparedStatementHandle;\n-    ((DrillColumnMetaDataList) signature.columns).updateColumnMetaData(preparedStatementHandle.getColumnsList());\n+    if (preparedStatementHandle != null) {\n+      ((DrillColumnMetaDataList) signature.columns).updateColumnMetaData(preparedStatementHandle.getColumnsList());\n+    }\n   }\n \n   /**\n@@ -329,7 +331,7 @@ public void addBatch(String sql) throws SQLException {\n   }\n \n   @Override\n-  public void clearBatch() throws RuntimeException {\n+  public void clearBatch() {\n     try {\n       throwIfClosed();\n     } catch (AlreadyClosedSqlException e) {",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/main/java/org/apache/drill/jdbc/impl/DrillPreparedStatementImpl.java",
                "sha": "a13f936670d69b45cadb54423b70cbba62af0776",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 29,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -421,19 +421,15 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n       }\n       else if (SQLClientInfoException.class == cause.getClass()\n                 && normalClosedExceptionText.equals(cause.getMessage())\n-                && (false\n-                    || method.getName().equals(\"setClientInfo\")\n-                    || method.getName().equals(\"getClientInfo\")\n-                    )) {\n+                && (method.getName().equals(\"setClientInfo\")\n+                    || method.getName().equals(\"getClientInfo\"))) {\n         // Special good case--we had to use SQLClientInfoException from those.\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n                && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getCatalog\")\n-                   || method.getName().equals(\"getSchema\")\n-                   )) {\n+               && (method.getName().equals(\"getCatalog\")\n+                  || method.getName().equals(\"getSchema\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n       }\n@@ -481,20 +477,18 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n       if (super.isOkaySpecialCaseException(method, cause)) {\n         result = true;\n       }\n-      else if (   method.getName().equals(\"executeLargeBatch\")\n+      else if (method.getName().equals(\"executeLargeBatch\")\n                || method.getName().equals(\"executeLargeUpdate\")) {\n         // TODO: New Java 8 methods not implemented in Avatica.\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n                && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getConnection\")\n+               && (method.getName().equals(\"getConnection\")\n                    || method.getName().equals(\"getFetchDirection\")\n                    || method.getName().equals(\"getFetchSize\")\n                    || method.getName().equals(\"getMaxRows\")\n-                   || method.getName().equals(\"getLargeMaxRows\") // TODO: Java 8\n-                   )) {\n+                   || method.getName().equals(\"getLargeMaxRows\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n       }\n@@ -544,27 +538,20 @@ protected boolean isOkaySpecialCaseException(Method method, Throwable cause) {\n         result = true;\n       }\n       else if (RuntimeException.class == cause.getClass()\n-               && normalClosedExceptionText.equals(cause.getMessage())\n-               && (false\n-                   || method.getName().equals(\"getConnection\")\n+               && cause.getMessage().contains(normalClosedExceptionText)\n+               && (method.getName().equals(\"getConnection\")\n                    || method.getName().equals(\"getFetchDirection\")\n                    || method.getName().equals(\"getFetchSize\")\n                    || method.getName().equals(\"getMaxRows\")\n                    || method.getName().equals(\"getMetaData\")\n-                   )) {\n+                   || method.getName().equals(\"clearBatch\"))) {\n         // Special good-enough case--we had to use RuntimeException for now.\n         result = true;\n-      }\n-      else if (  method.getName().equals(\"setObject\")\n-              || method.getName().equals(\"executeLargeUpdate\")\n-              || method.getName().equals(\"executeLargeBatch\")\n-              || method.getName().equals(\"getLargeMaxRows\")\n-              ) {\n-        // TODO: Java 8 methods not yet supported by Avatica.\n-        result = true;\n-      }\n-      else {\n-        result = false;\n+      } else {\n+        result = method.getName().equals(\"setObject\")\n+          || method.getName().equals(\"executeLargeUpdate\")\n+          || method.getName().equals(\"executeLargeBatch\")\n+          || method.getName().equals(\"getLargeMaxRows\");\n       }\n       return result;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2489CallsAfterCloseThrowExceptionsTest.java",
                "sha": "8521586eed7024c56710e97e64613eabaef983da",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java?ref=3896a58243f310c5d9466a98edc205b61f9dd2e7",
                "deletions": 9,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -96,13 +96,8 @@ public static void setUpObjects() throws Exception {\n     catch (SQLException | UnsupportedOperationException e) {\n       // Expected.\n     }\n-    try {\n-      connection.createArrayOf(\"INTEGER\", new Object[0]);\n-      fail(\"Test seems to be out of date.  Were arrays implemented?\");\n-    }\n-    catch (SQLException | UnsupportedOperationException e) {\n-      // Expected.\n-    }\n+\n+    connection.createArrayOf(\"INTEGER\", new Object[0]);\n \n     resultSet = plainStatement.executeQuery(\"VALUES 'plain Statement query'\");\n     resultSet.next();\n@@ -161,7 +156,9 @@ protected INTF getJdbcObject() throws SQLException {\n      */\n     private static Object getDummyValueForType(Class<?> type) {\n       final Object result;\n-      if (! type.isPrimitive()) {\n+      if (type.equals(String.class)) {\n+        result = \"\";\n+      } else if (! type.isPrimitive()) {\n         result = null;\n       }\n       else {",
                "raw_url": "https://github.com/apache/drill/raw/3896a58243f310c5d9466a98edc205b61f9dd2e7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/Drill2769UnsupportedReportsUseSqlExceptionTest.java",
                "sha": "9e2399bd3732dd6848baea66ce2ba1d1746061cf",
                "status": "modified"
            }
        ],
        "message": "DRILL-3993: Fix unit test failures connected with support Calcite 1.13\n\n- Use root schema as default for describe table statement.\nFix TestOpenTSDBPlugin.testDescribe() and TestInfoSchemaOnHiveStorage.varCharMaxLengthAndDecimalPrecisionInInfoSchema() unit tests.\n- Modify expected results for tests:\nTestPreparedStatementProvider.invalidQueryValidationError();\nTestProjectPushDown.testTPCH1();\nTestProjectPushDown.testTPCH3();\nTestStorageBasedHiveAuthorization.selectUser1_db_u0_only();\nTestStorageBasedHiveAuthorization.selectUser0_db_u1g1_only()\n- Fix TestCTAS.whenTableQueryColumnHasStarAndTableFiledListIsSpecified(), TestViewSupport.createViewWhenViewQueryColumnHasStarAndViewFiledListIsSpecified(), TestInbuiltHiveUDFs.testIf(), testDisableUtf8SupportInQueryString unit tests.\n- Fix UnsupportedOperationException and NPE for jdbc tests.\n- Fix AssertionError: Conversion to relational algebra failed to preserve datatypes\n\n*DrillCompoundIdentifier:\nAccording to the changes, made in [CALCITE-546], star Identifier is replaced by empty string during parsing the query. Since Drill uses its own DrillCompoundIdentifier, it should also replace star by empty string before creating SqlIdentifier instance to avoid further errors connected with star column. see SqlIdentifier.isStar() method.\n\n*SqlConverter:\nIn [CALCITE-1417] added simplification of expressions which should be projected every time when a new project rel node is created using RelBuilder. It causes assertion errors connected with types nullability. This hook was set to false to avoid project expressions simplification. See usage of this hook and RelBuilder.project() method.\n\nIn Drill the type nullability of the function depends on only the nullability of its arguments. In some cases, a function may return null value even if it had non-nullable arguments. When Calice simplifies expressions, it checks that the type of the result is the same as the type of the expression. Otherwise, makeCast() method is called. But when a function returns null literal, this cast does nothing, even when the function has a non-nullable type. So to avoid this issue, method makeCast() was overridden.\n\n*DrillAvgVarianceConvertlet:\nProblem with sum0 and specific changes in old Calcite (it is CALCITE-777). (see HistogramShuttle.visitCall method) Changes were made to avoid changes in Calcite.\n\n*SqlConverter, DescribeTableHandler, ShowTablesHandler:\nNew Calcite tries to combine both default and specified workspaces during the query validation. In some cases, for example, when describe table statement is used, Calcite tries to find INFORMATION_SCHEMA in the schema used as default. When it does not find the schema, it tries to find a table with such name. For some storage plugins, such as opentsdb and hbase, when a table was not found, the error is thrown, and the query fails. To avoid this issue, default schema was changed to root schema for validation stage for describe table and show tables queries.",
        "parent": "https://github.com/apache/drill/commit/9fabb612f16f6f541b3bde68ad7d734cad26df33",
        "patched_files": [
            "InfoSchemaRecordGenerator.java",
            "DrillPreparedStatementImpl.java",
            "SqlConverter.java",
            "DescribeTableHandler.java",
            "ShowTablesHandler.java",
            "DefaultSqlHandler.java",
            "PreparedStatementProvider.java",
            "DrillCalciteSqlAggFunctionWrapper.java",
            "DrillCompoundIdentifier.java",
            "SqlHandlerUtil.java",
            "DrillMetaImpl.java",
            "DrillAvgVarianceConvertlet.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestInbuiltHiveUDFs.java",
            "Drill2769UnsupportedReportsUseSqlExceptionTest.java",
            "Drill2489CallsAfterCloseThrowExceptionsTest.java",
            "TestProjectPushDown.java",
            "TestPreparedStatementProvider.java",
            "TestUtf8SupportInQueryString.java",
            "TestStorageBasedHiveAuthorization.java"
        ]
    },
    "drill_3d2a3e1": {
        "bug_id": "drill_3d2a3e1",
        "commit": "https://github.com/apache/drill/commit/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "patch": "@@ -60,6 +60,9 @@ private void doTransfers(){\n   }\n \n   public void filterBatch(int recordCount){\n+    if (recordCount == 0) {\n+      return;\n+    }\n     if (! outgoingSelectionVector.allocateNew(recordCount)) {\n       throw new UnsupportedOperationException(\"Unable to allocate filter batch\");\n     }",
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate2.java",
                "sha": "26f2657826dc2fdc2d36530c0c04709a06cab752",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "patch": "@@ -43,6 +43,9 @@ public void setup(FragmentContext context, RecordBatch incoming, RecordBatch out\n \n   @Override\n   public void filterBatch(int recordCount){\n+    if (recordCount == 0) {\n+      return;\n+    }\n     int outPos = 0;\n     for (int i = 0; i < incomingSelectionVector.getCount(); i++) {\n       int index = incomingSelectionVector.get(i);",
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/filter/FilterTemplate4.java",
                "sha": "fd1f9e68c04a121cf5d6f3f385ea00e90093387a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/drill/blob/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java?ref=3d2a3e1c8b27d44c5f28a24f83959a1608c9317c",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "patch": "@@ -504,4 +504,18 @@ public void testFilterInSubqueryAndOutside() throws Exception {\n     assertEquals(expectedRecordCount, actualRecordCount);\n   }\n \n+  @Test // DRILL-1973\n+  public void testLimit0SubqueryWithFilter() throws Exception {\n+    String query1 = \"select * from (select sum(1) as x from  cp.`tpch/region.parquet` limit 0) WHERE x < 10\";\n+    String query2 = \"select * from (select sum(1) as x from  cp.`tpch/region.parquet` limit 0) WHERE (0 = 1)\";\n+    int actualRecordCount = 0;\n+    int expectedRecordCount = 0;\n+\n+    actualRecordCount = testSql(query1);\n+    assertEquals(expectedRecordCount, actualRecordCount);\n+\n+    actualRecordCount = testSql(query2);\n+    assertEquals(expectedRecordCount, actualRecordCount);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/3d2a3e1c8b27d44c5f28a24f83959a1608c9317c/exec/java-exec/src/test/java/org/apache/drill/TestExampleQueries.java",
                "sha": "a3b5ff19b00cdf9fd24f8ce6dedac3d129171515",
                "status": "modified"
            }
        ],
        "message": "DRILL-1973: Filter should check for record count before doing evaluation. Fixes NPE.",
        "parent": "https://github.com/apache/drill/commit/952114fcdb9b819b546bd0283d30e56027c3f831",
        "patched_files": [
            "FilterTemplate2.java",
            "FilterTemplate4.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestExampleQueries.java"
        ]
    },
    "drill_4d4e0c2": {
        "bug_id": "drill_4d4e0c2",
        "commit": "https://github.com/apache/drill/commit/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
        "file": [
            {
                "additions": 64,
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "patch": "@@ -17,6 +17,12 @@\n  */\n <@pp.dropOutputFile />\n \n+<#macro reassignHolder>\n+        previous.buffer = buf.reallocIfNeeded(length);\n+        previous.buffer.setBytes(0, in.buffer, in.start, length);\n+        previous.end = length;\n+</#macro>\n+\n \n <@pp.changeOutputFile name=\"/org/apache/drill/exec/expr/fn/impl/GNewValueFunctions.java\" />\n <#include \"/@includes/license.ftl\" />\n@@ -39,28 +45,51 @@\n  */\n public class GNewValueFunctions {\n <#list vv.types as type>\n-<#if type.major == \"Fixed\" || type.major = \"Bit\">\n-\n <#list type.minor as minor>\n <#list vv.modes as mode>\n   <#if mode.name != \"Repeated\">\n \n <#if !minor.class.startsWith(\"Decimal28\") && !minor.class.startsWith(\"Decimal38\") && !minor.class.startsWith(\"Interval\")>\n @SuppressWarnings(\"unused\")\n-@FunctionTemplate(name = \"newPartitionValue\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls=NullHandling.INTERNAL)\n-public static class NewValue${minor.class}${mode.prefix} implements DrillSimpleFunc{\n+@FunctionTemplate(name = \"newPartitionValue\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = NullHandling.INTERNAL)\n+public static class NewValue${minor.class}${mode.prefix} implements DrillSimpleFunc {\n \n   @Param ${mode.prefix}${minor.class}Holder in;\n   @Workspace ${mode.prefix}${minor.class}Holder previous;\n   @Workspace Boolean initialized;\n   @Output BitHolder out;\n+  <#if type.major == \"VarLen\">\n+  @Inject DrillBuf buf;\n+  </#if>\n \n   public void setup() {\n     initialized = false;\n+    <#if type.major == \"VarLen\">\n+    previous.buffer = buf;\n+    previous.start = 0;\n+    </#if>\n   }\n \n-  <#if mode.name == \"Required\">\n   public void eval() {\n+  <#if mode.name == \"Required\">\n+  <#if type.major == \"VarLen\">\n+    int length = in.end - in.start;\n+\n+    if (initialized) {\n+      if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n+          previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n+        out.value = 0;\n+      } else {\n+        <@reassignHolder/>\n+        out.value = 1;\n+      }\n+    } else {\n+      <@reassignHolder/>\n+      out.value = 1;\n+      initialized = true;\n+    }\n+  </#if>\n+  <#if type.major == \"Fixed\" || type.major == \"Bit\">\n     if (initialized) {\n       if (in.value == previous.value) {\n         out.value = 0;\n@@ -73,10 +102,36 @@ public void eval() {\n       out.value = 1;\n       initialized = true;\n     }\n-  }\n   </#if>\n+  </#if> <#-- mode.name == \"Required\" -->\n+\n   <#if mode.name == \"Optional\">\n-  public void eval() {\n+  <#if type.major == \"VarLen\">\n+    int length = in.isSet == 0 ? 0 : in.end - in.start;\n+\n+    if (initialized) {\n+      if (previous.isSet == 0 && in.isSet == 0) {\n+        out.value = 0;\n+      } else if (previous.isSet != 0 && in.isSet != 0 && org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n+          previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n+        out.value = 0;\n+      } else {\n+        if (in.isSet == 1) {\n+          <@reassignHolder/>\n+        }\n+        previous.isSet = in.isSet;\n+        out.value = 1;\n+      }\n+    } else {\n+      if (in.isSet == 1) {\n+        <@reassignHolder/>\n+      }\n+      previous.isSet = in.isSet;\n+      out.value = 1;\n+      initialized = true;\n+    }\n+  </#if>\n+  <#if type.major == \"Fixed\" || type.major == \"Bit\">\n     if (initialized) {\n       if (in.isSet == 0 && previous.isSet == 0) {\n         out.value = 0;\n@@ -93,14 +148,14 @@ public void eval() {\n       out.value = 1;\n       initialized = true;\n     }\n-  }\n   </#if>\n+  </#if> <#-- mode.name == \"Optional\" -->\n+  }\n }\n </#if> <#-- minor.class.startWith -->\n \n </#if> <#-- mode.name -->\n </#list>\n </#list>\n-</#if> <#-- type.major -->\n </#list>\n }",
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/main/codegen/templates/NewValueFunctions.java",
                "sha": "5591d669c4bedbb053d5acbf9385d255b62ca7f6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/15b021f1f4c73c08c0443fd9cae0221ae43877ba/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "changes": 209,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java?ref=15b021f1f4c73c08c0443fd9cae0221ae43877ba",
                "deletions": 209,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "patch": "@@ -1,209 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.drill.exec.store;\n-\n-import io.netty.buffer.DrillBuf;\n-import org.apache.drill.exec.expr.DrillSimpleFunc;\n-import org.apache.drill.exec.expr.annotations.FunctionTemplate;\n-import org.apache.drill.exec.expr.annotations.FunctionTemplate.NullHandling;\n-import org.apache.drill.exec.expr.annotations.Output;\n-import org.apache.drill.exec.expr.annotations.Param;\n-import org.apache.drill.exec.expr.annotations.Workspace;\n-import org.apache.drill.exec.expr.holders.BitHolder;\n-import org.apache.drill.exec.expr.holders.IntHolder;\n-import org.apache.drill.exec.expr.holders.NullableVarBinaryHolder;\n-import org.apache.drill.exec.expr.holders.NullableVarCharHolder;\n-import org.apache.drill.exec.expr.holders.VarBinaryHolder;\n-import org.apache.drill.exec.expr.holders.VarCharHolder;\n-\n-import javax.inject.Inject;\n-\n-/**\n- *  The functions are similar to those created through FreeMarker template for fixed types. There is not much benefit to\n- *  using code generation for generating the functions for variable length types, so simply doing them by hand.\n- */\n-public class NewValueFunction {\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarChar implements DrillSimpleFunc {\n-\n-    @Param VarCharHolder in;\n-    @Workspace VarCharHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.end - in.start;\n-\n-      if (initialized) {\n-        if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n-          out.value = 0;\n-        } else {\n-          previous.buffer = buf.reallocIfNeeded(length);\n-          previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-          previous.end = in.end - in.start;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarCharNullable implements DrillSimpleFunc {\n-\n-    @Param NullableVarCharHolder in;\n-    @Workspace NullableVarCharHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.isSet == 0 ? 0 : in.end - in.start;\n-\n-      if (initialized) {\n-        if (previous.isSet == 0 && in.isSet == 0 ||\n-            (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n-                previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0)) {\n-          out.value = 0;\n-        } else {\n-          if (in.isSet == 1) {\n-            previous.buffer = buf.reallocIfNeeded(length);\n-            previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-            previous.end = in.end - in.start;\n-          }\n-          previous.isSet = in.isSet;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        previous.isSet = 1;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarBinary implements DrillSimpleFunc {\n-\n-    @Param VarBinaryHolder in;\n-    @Workspace VarBinaryHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.end - in.start;\n-\n-      if (initialized) {\n-        if (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0) {\n-          out.value = 0;\n-        } else {\n-          previous.buffer = buf.reallocIfNeeded(length);\n-          previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-          previous.end = in.end - in.start;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-\n-  @FunctionTemplate(name = \"newPartitionValue\",\n-      scope = FunctionTemplate.FunctionScope.SIMPLE,\n-      nulls = NullHandling.INTERNAL)\n-  public static class NewValueVarBinaryNullable implements DrillSimpleFunc {\n-\n-    @Param NullableVarBinaryHolder in;\n-    @Workspace NullableVarBinaryHolder previous;\n-    @Workspace Boolean initialized;\n-    @Output BitHolder out;\n-    @Inject DrillBuf buf;\n-\n-    public void setup() {\n-      initialized = false;\n-      previous.buffer = buf;\n-      previous.start = 0;\n-    }\n-\n-    public void eval() {\n-      int length = in.isSet == 0 ? 0 : in.end - in.start;\n-\n-      if (initialized) {\n-        if (previous.isSet == 0 && in.isSet == 0 ||\n-            (org.apache.drill.exec.expr.fn.impl.ByteFunctionHelpers.compare(\n-                previous.buffer, 0, previous.end, in.buffer, in.start, in.end) == 0)) {\n-          out.value = 0;\n-        } else {\n-          if (in.isSet == 1) {\n-            previous.buffer = buf.reallocIfNeeded(length);\n-            previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-            previous.end = in.end - in.start;\n-          }\n-          previous.isSet = in.isSet;\n-          out.value = 1;\n-        }\n-      } else {\n-        previous.buffer = buf.reallocIfNeeded(length);\n-        previous.buffer.setBytes(0, in.buffer, in.start, in.end - in.start);\n-        previous.end = in.end - in.start;\n-        previous.isSet = 1;\n-        out.value = 1;\n-        initialized = true;\n-      }\n-    }\n-  }\n-}",
                "raw_url": "https://github.com/apache/drill/raw/15b021f1f4c73c08c0443fd9cae0221ae43877ba/exec/java-exec/src/main/java/org/apache/drill/exec/store/NewValueFunction.java",
                "sha": "fedb4733a5fb0d62e0121c1d2a23f54085395e27",
                "status": "removed"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
                "deletions": 30,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.types.TypeProtos;\n import org.apache.drill.common.util.TestTools;\n+import org.apache.drill.exec.proto.UserBitShared;\n+import org.apache.drill.exec.rpc.user.QueryDataBatch;\n import org.junit.Ignore;\n import org.junit.Test;\n \n@@ -285,41 +287,51 @@ public void stddevEmptyNonexistentNullableInput() throws Exception {\n \n   }\n   @Test\n-  public void minEmptyNonnullableInput() throws Exception {\n-    // test min function on required type\n-    String query = \"select \" +\n-        \"min(bool_col) col1, min(int_col) col2, min(bigint_col) col3, min(float4_col) col4, min(float8_col) col5, \" +\n-        \"min(date_col) col6, min(time_col) col7, min(timestamp_col) col8, min(interval_year_col) col9, \" +\n-        \"min(varhcar_col) col10 \" +\n-        \"from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n-\n-    testBuilder()\n-        .sqlQuery(query)\n-        .unOrdered()\n-        .baselineColumns(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\")\n-        .baselineValues(null, null, null, null, null, null, null, null, null, null)\n-        .go();\n-  }\n+  public void minMaxEmptyNonNullableInput() throws Exception {\n+    // test min and max functions on required type\n+\n+    final QueryDataBatch result = testSqlWithResults(\"select * from cp.`parquet/alltypes_required.parquet` limit 0\")\n+        .get(0);\n+\n+    final Map<String, StringBuilder> functions = Maps.newHashMap();\n+    functions.put(\"min\", new StringBuilder());\n+    functions.put(\"max\", new StringBuilder());\n+\n+    final Map<String, Object> resultingValues = Maps.newHashMap();\n+    for (UserBitShared.SerializedField field : result.getHeader().getDef().getFieldList()) {\n+      final String fieldName = field.getNamePart().getName();\n+      // Only COUNT aggregate function supported for Boolean type\n+      if (fieldName.equals(\"col_bln\")) {\n+        continue;\n+      }\n+      resultingValues.put(String.format(\"`%s`\", fieldName), null);\n+      for (Map.Entry<String, StringBuilder> function : functions.entrySet()) {\n+        function.getValue()\n+            .append(function.getKey())\n+            .append(\"(\")\n+            .append(fieldName)\n+            .append(\") \")\n+            .append(fieldName)\n+            .append(\",\");\n+      }\n+    }\n+    result.release();\n \n-  @Test\n-  public void maxEmptyNonnullableInput() throws Exception {\n+    final String query = \"select %s from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n+    final List<Map<String, Object>> baselineRecords = Lists.newArrayList();\n+    baselineRecords.add(resultingValues);\n \n-    // test max function\n-    final String query = \"select \" +\n-        \"max(int_col) col1, max(bigint_col) col2, max(float4_col) col3, max(float8_col) col4, \" +\n-        \"max(date_col) col5, max(time_col) col6, max(timestamp_col) col7, max(interval_year_col) col8, \" +\n-        \"max(varhcar_col) col9 \" +\n-        \"from cp.`parquet/alltypes_required.parquet` where 1 = 0\";\n+    for (StringBuilder selectBody : functions.values()) {\n+      selectBody.setLength(selectBody.length() - 1);\n \n-    testBuilder()\n-        .sqlQuery(query)\n-        .unOrdered()\n-        .baselineColumns(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\")\n-        .baselineValues(null, null, null, null, null, null, null, null, null)\n-        .go();\n+      testBuilder()\n+          .sqlQuery(query, selectBody.toString())\n+          .unOrdered()\n+          .baselineRecords(baselineRecords)\n+          .go();\n+    }\n   }\n \n-\n   /*\n    * Streaming agg on top of a filter produces wrong results if the first two batches are filtered out.\n    * In the below test we have three files in the input directory and since the ordering of reading",
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "sha": "36ee1b91659ed517b239c777c65e874ec111582b",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
                "deletions": 2,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "patch": "@@ -17,11 +17,15 @@\n  */\n package org.apache.drill.exec.sql;\n \n+import com.google.common.collect.Maps;\n import org.apache.commons.io.FileUtils;\n import org.apache.drill.BaseTestQuery;\n+import org.apache.drill.exec.proto.UserBitShared;\n+import org.apache.drill.exec.rpc.user.QueryDataBatch;\n import org.junit.Test;\n \n import java.io.File;\n+import java.util.Map;\n \n public class TestCTAS extends BaseTestQuery {\n   @Test // DRILL-2589\n@@ -125,8 +129,7 @@ public void ctasPartitionWithEmptyList() throws Exception {\n     try {\n       final String ctasQuery = String.format(\"CREATE TABLE %s.%s PARTITION BY AS SELECT * from cp.`region.json`\", TEMP_SCHEMA, newTblName);\n \n-      errorMsgTestHelper(ctasQuery,\n-          String.format(\"PARSE ERROR: Encountered \\\"AS\\\"\"));\n+      errorMsgTestHelper(ctasQuery,\"PARSE ERROR: Encountered \\\"AS\\\"\");\n     } finally {\n       FileUtils.deleteQuietly(new File(getDfsTestTmpSchemaLocation(), newTblName));\n     }\n@@ -238,6 +241,41 @@ public void ctasWithPartition() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testPartitionByForAllTypes() throws Exception {\n+    final String location = \"partitioned_tables_with_nulls\";\n+    final String ctasQuery = \"create table %s partition by (%s) as %s\";\n+    final String tablePath = \"%s.`%s/%s_%s`\";\n+\n+    // key - new table suffix, value - data query\n+    final Map<String, String> variations = Maps.newHashMap();\n+    variations.put(\"required\", \"select * from cp.`parquet/alltypes_required.parquet`\");\n+    variations.put(\"optional\", \"select * from cp.`parquet/alltypes_optional.parquet`\");\n+    variations.put(\"nulls_only\", \"select * from cp.`parquet/alltypes_optional.parquet` where %s is null\");\n+\n+    try {\n+      final QueryDataBatch result = testSqlWithResults(\"select * from cp.`parquet/alltypes_required.parquet` limit 0\").get(0);\n+      for (UserBitShared.SerializedField field : result.getHeader().getDef().getFieldList()) {\n+        final String fieldName = field.getNamePart().getName();\n+\n+        for (Map.Entry<String, String> variation : variations.entrySet()) {\n+          final String table = String.format(tablePath, TEMP_SCHEMA, location, fieldName, variation.getKey());\n+          final String dataQuery = String.format(variation.getValue(), fieldName);\n+          test(ctasQuery, table, fieldName, dataQuery, fieldName);\n+          testBuilder()\n+              .sqlQuery(\"select * from %s\", table)\n+              .unOrdered()\n+              .sqlBaselineQuery(dataQuery)\n+              .build()\n+              .run();\n+        }\n+      }\n+      result.release();\n+    } finally {\n+      FileUtils.deleteQuietly(new File(getDfsTestTmpSchemaLocation(), location));\n+    }\n+  }\n+\n   private static void ctasErrorTestHelper(final String ctasSql, final String expErrorMsg) throws Exception {\n     final String createTableSql = String.format(ctasSql, TEMP_SCHEMA, \"testTableName\");\n     errorMsgTestHelper(createTableSql, expErrorMsg);",
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestCTAS.java",
                "sha": "9d9b403ede37975644385638d80e1562e326adaf",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_optional.parquet",
                "sha": "53f5fa19d2f1761b93878424301d242466eba272",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet?ref=4d4e0c2b23caead69dd4c6c02c07a9800b3c7611",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "raw_url": "https://github.com/apache/drill/raw/4d4e0c2b23caead69dd4c6c02c07a9800b3c7611/exec/java-exec/src/test/resources/parquet/alltypes_required.parquet",
                "sha": "efc6add0cb583674bb2310aeca75c41d179a653d",
                "status": "modified"
            }
        ],
        "message": "DRILL-5039: NPE - CTAS PARTITION BY (<char-type-column>)\n\nclose apache/drill#706",
        "parent": "https://github.com/apache/drill/commit/15b021f1f4c73c08c0443fd9cae0221ae43877ba",
        "patched_files": [
            "alltypes_required.parquet",
            "NewValueFunctions.java",
            "NewValueFunction.java",
            "alltypes_optional.parquet"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestCTAS.java",
            "TestAggregateFunctions.java"
        ]
    },
    "drill_579ebca": {
        "bug_id": "drill_579ebca",
        "commit": "https://github.com/apache/drill/commit/579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 5,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "patch": "@@ -60,8 +60,9 @@\n import org.apache.drill.exec.planner.logical.DrillWindowRule;\n import org.apache.drill.exec.planner.logical.partition.ParquetPruneScanRule;\n import org.apache.drill.exec.planner.logical.partition.PruneScanRule;\n+import org.apache.drill.exec.planner.logical.ConvertCountToDirectScanRule;\n import org.apache.drill.exec.planner.physical.AnalyzePrule;\n-import org.apache.drill.exec.planner.physical.ConvertCountToDirectScan;\n+import org.apache.drill.exec.planner.physical.ConvertCountToDirectScanPrule;\n import org.apache.drill.exec.planner.physical.LateralJoinPrule;\n import org.apache.drill.exec.planner.physical.DirectScanPrule;\n import org.apache.drill.exec.planner.physical.FilterPrule;\n@@ -474,8 +475,10 @@ static RuleSet getDirPruneScanRules(OptimizerRulesContext optimizerRulesContext)\n         .add(\n             PruneScanRule.getDirFilterOnProject(optimizerRulesContext),\n             PruneScanRule.getDirFilterOnScan(optimizerRulesContext),\n-            PruneScanRule.getConvertAggScanToValuesRule(optimizerRulesContext)\n-        )\n+            PruneScanRule.getConvertAggScanToValuesRule(optimizerRulesContext),\n+            ConvertCountToDirectScanRule.AGG_ON_PROJ_ON_SCAN,\n+            ConvertCountToDirectScanRule.AGG_ON_SCAN\n+          )\n         .build();\n \n     return RuleSets.ofList(pruneRules);\n@@ -501,8 +504,8 @@ static RuleSet getJoinPermRules(OptimizerRulesContext optimizerRulesContext) {\n   static RuleSet getPhysicalRules(OptimizerRulesContext optimizerRulesContext) {\n     final List<RelOptRule> ruleList = new ArrayList<>();\n     final PlannerSettings ps = optimizerRulesContext.getPlannerSettings();\n-    ruleList.add(ConvertCountToDirectScan.AGG_ON_PROJ_ON_SCAN);\n-    ruleList.add(ConvertCountToDirectScan.AGG_ON_SCAN);\n+    ruleList.add(ConvertCountToDirectScanPrule.AGG_ON_PROJ_ON_SCAN);\n+    ruleList.add(ConvertCountToDirectScanPrule.AGG_ON_SCAN);\n     ruleList.add(SortConvertPrule.INSTANCE);\n     ruleList.add(SortPrule.INSTANCE);\n     ruleList.add(ProjectPrule.INSTANCE);",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/PlannerPhase.java",
                "sha": "0c89c1bf2737fb9c56797db2f7aef96335297a19",
                "status": "modified"
            },
            {
                "additions": 108,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "patch": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.common;\n+\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rel.type.RelDataTypeFieldImpl;\n+import org.apache.calcite.rel.type.RelRecordType;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * A utility class that contains helper functions used by rules that convert COUNT(*) and COUNT(col)\n+ * aggregates (no group-by) to DirectScan\n+ */\n+public class CountToDirectScanUtils {\n+\n+  /**\n+   * Checks if aggregate call contains star or non-null expression:\n+   * <pre>\n+   * count(*)  == >  empty arg  ==>  rowCount\n+   * count(Not-null-input) ==> rowCount\n+   * </pre>\n+   *\n+   * @param aggregateCall aggregate call\n+   * @param aggregate aggregate relation expression\n+   * @return true of aggregate call contains star or non-null expression\n+   */\n+  public static boolean containsStarOrNotNullInput(AggregateCall aggregateCall, Aggregate aggregate) {\n+    return aggregateCall.getArgList().isEmpty() ||\n+        (aggregateCall.getArgList().size() == 1 &&\n+            !aggregate.getInput().getRowType().getFieldList().get(aggregateCall.getArgList().get(0)).getType().isNullable());\n+  }\n+\n+  /**\n+   * For each aggregate call creates field based on its name with bigint type.\n+   * Constructs record type for created fields.\n+   *\n+   * @param aggregateRel aggregate relation expression\n+   * @param fieldNames field names\n+   * @return record type\n+   */\n+  public static RelDataType constructDataType(Aggregate aggregateRel, Collection<String> fieldNames) {\n+    List<RelDataTypeField> fields = new ArrayList<>();\n+    int fieldIndex = 0;\n+    for (String name : fieldNames) {\n+      RelDataTypeField field = new RelDataTypeFieldImpl(\n+          name,\n+          fieldIndex++,\n+          aggregateRel.getCluster().getTypeFactory().createSqlType(SqlTypeName.BIGINT));\n+      fields.add(field);\n+    }\n+    return new RelRecordType(fields);\n+  }\n+\n+  /**\n+   * Builds schema based on given field names.\n+   * Type for each schema is set to long.class.\n+   *\n+   * @param fieldNames field names\n+   * @return schema\n+   */\n+  public static LinkedHashMap<String, Class<?>> buildSchema(List<String> fieldNames) {\n+    LinkedHashMap<String, Class<?>> schema = new LinkedHashMap<>();\n+    for (String fieldName: fieldNames) {\n+      schema.put(fieldName, long.class);\n+    }\n+    return schema;\n+  }\n+\n+  /**\n+   * For each field creates row expression.\n+   *\n+   * @param rowType row type\n+   * @return list of row expressions\n+   */\n+  public static List<RexNode> prepareFieldExpressions(RelDataType rowType) {\n+    List<RexNode> expressions = new ArrayList<>();\n+    for (int i = 0; i < rowType.getFieldCount(); i++) {\n+      expressions.add(RexInputRef.of(i, rowType));\n+    }\n+    return expressions;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/CountToDirectScanUtils.java",
                "sha": "2b5755f5c8f652a8ffad49c24866d5c3c1cbd8d6",
                "status": "added"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.drill.exec.planner.logical.DrillRelFactories;\n import org.apache.drill.exec.planner.logical.DrillTable;\n import org.apache.drill.exec.planner.logical.FieldsReWriterUtil;\n+import org.apache.drill.exec.planner.logical.DrillTranslatableTable;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n import org.apache.drill.exec.resolver.TypeCastRules;\n import org.apache.drill.exec.util.Utilities;\n@@ -647,4 +648,16 @@ public static boolean analyzeSimpleEquiJoin(Join join, int[] joinFieldOrdinals)\n       }\n     }\n   }\n+\n+  public static DrillTable getDrillTable(final TableScan scan) {\n+    DrillTable drillTable = null;\n+    drillTable = scan.getTable().unwrap(DrillTable.class);\n+    if (drillTable == null) {\n+      DrillTranslatableTable transTable = scan.getTable().unwrap(DrillTranslatableTable.class);\n+      if (transTable != null) {\n+        drillTable = transTable.getDrillTable();\n+      }\n+    }\n+    return drillTable;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "sha": "531c2b85d635758e9b2f59729b13ecbb52953de9",
                "status": "modified"
            },
            {
                "additions": 305,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "changes": 305,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "patch": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.logical;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptRuleOperand;\n+import org.apache.calcite.rel.core.Aggregate;\n+import org.apache.calcite.rel.core.AggregateCall;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.core.TableScan;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.common.logical.FormatPluginConfig;\n+\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.ScanStats;\n+import org.apache.drill.exec.planner.common.CountToDirectScanUtils;\n+import org.apache.drill.exec.planner.common.DrillRelOptUtil;\n+\n+import org.apache.drill.exec.planner.physical.PlannerSettings;\n+import org.apache.drill.exec.store.ColumnExplorer;\n+import org.apache.drill.exec.store.dfs.DrillFileSystem;\n+import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n+import org.apache.drill.exec.store.dfs.FormatSelection;\n+import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n+import org.apache.drill.exec.store.direct.MetadataDirectGroupScan;\n+import org.apache.drill.exec.store.parquet.ParquetFormatConfig;\n+import org.apache.drill.exec.store.parquet.ParquetReaderConfig;\n+import org.apache.drill.exec.store.parquet.metadata.Metadata;\n+import org.apache.drill.exec.store.parquet.metadata.Metadata_V4;\n+import org.apache.drill.exec.store.pojo.DynamicPojoRecordReader;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.LinkedHashMap;\n+import java.util.Set;\n+\n+/**\n+ * <p> This rule is a logical planning counterpart to a corresponding <b>ConvertCountToDirectScanPrule</b>\n+ * physical rule\n+ * </p>\n+ * <p>\n+ * This rule will convert <b>\" select count(*)  as mycount from table \"</b>\n+ * or <b>\" select count(not-nullable-expr) as mycount from table \"</b> into\n+ * <pre>\n+ *    Project(mycount)\n+ *         \\\n+ *    DirectGroupScan ( PojoRecordReader ( rowCount ))\n+ *</pre>\n+ * or <b>\" select count(column) as mycount from table \"</b> into\n+ * <pre>\n+ *      Project(mycount)\n+ *           \\\n+ *            DirectGroupScan (PojoRecordReader (columnValueCount))\n+ *</pre>\n+ * Rule can be applied if query contains multiple count expressions.\n+ * <b>\" select count(column1), count(column2), count(*) from table \"</b>\n+ * </p>\n+ *\n+ * <p>\n+ * The rule utilizes the Parquet Metadata Cache's summary information to retrieve the total row count\n+ * and the per-column null count.  As such, the rule is only applicable for Parquet tables and only if the\n+ * metadata cache has been created with the summary information.\n+ * </p>\n+ */\n+public class ConvertCountToDirectScanRule extends RelOptRule {\n+\n+  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScanRule(\n+      RelOptHelper.some(Aggregate.class,\n+                        RelOptHelper.some(Project.class,\n+                            RelOptHelper.any(TableScan.class))), \"Agg_on_proj_on_scan\");\n+\n+  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScanRule(\n+      RelOptHelper.some(Aggregate.class,\n+                            RelOptHelper.any(TableScan.class)), \"Agg_on_scan\");\n+\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScanRule.class);\n+\n+  protected ConvertCountToDirectScanRule(RelOptRuleOperand rule, String id) {\n+    super(rule, \"ConvertCountToDirectScanRule:\" + id);\n+  }\n+\n+  @Override\n+  public void onMatch(RelOptRuleCall call) {\n+    final Aggregate agg = (Aggregate) call.rel(0);\n+    final TableScan scan = (TableScan) call.rel(call.rels.length - 1);\n+    final Project project = call.rels.length == 3 ? (Project) call.rel(1) : null;\n+\n+    // Qualifying conditions for rule:\n+    //    1) There's no GroupBY key,\n+    //    2) Agg is not a DISTINCT agg\n+    //    3) Additional checks are done further below ..\n+    if (agg.getGroupCount() > 0 ||\n+        agg.containsDistinctCall()) {\n+      return;\n+    }\n+\n+    DrillTable drillTable = DrillRelOptUtil.getDrillTable(scan);\n+\n+    if (drillTable == null) {\n+      logger.debug(\"Rule does not apply since an eligible drill table instance was not found.\");\n+      return;\n+    }\n+\n+    Object selection = drillTable.getSelection();\n+\n+    if (!(selection instanceof FormatSelection)) {\n+      logger.debug(\"Rule does not apply since only Parquet file format is eligible.\");\n+      return;\n+    }\n+\n+    PlannerSettings settings = call.getPlanner().getContext().unwrap(PlannerSettings.class);\n+\n+    //  Rule is applicable only if the statistics for row count and null count are available from the metadata,\n+    FormatSelection formatSelection = (FormatSelection) selection;\n+    Pair<Boolean, Metadata_V4.MetadataSummary> status = checkMetadataForScanStats(settings, drillTable, formatSelection);\n+\n+    if (!status.getLeft()) {\n+      logger.debug(\"Rule does not apply since MetadataSummary metadata was not found.\");\n+      return;\n+    }\n+\n+    Metadata_V4.MetadataSummary metadataSummary = status.getRight();\n+    Map<String, Long> result = collectCounts(settings, metadataSummary, agg, scan, project);\n+    logger.trace(\"Calculated the following aggregate counts: \", result);\n+\n+    // if counts could not be determined, rule won't be applied\n+    if (result.isEmpty()) {\n+      logger.debug(\"Rule does not apply since one or more COUNTs could not be determined from metadata.\");\n+      return;\n+    }\n+\n+    List<Path> fileList =\n+            ImmutableList.of(Metadata.getSummaryFileName(formatSelection.getSelection().getSelectionRoot()));\n+\n+    final RelDataType scanRowType = CountToDirectScanUtils.constructDataType(agg, result.keySet());\n+\n+    final DynamicPojoRecordReader<Long> reader = new DynamicPojoRecordReader<>(\n+        CountToDirectScanUtils.buildSchema(scanRowType.getFieldNames()),\n+        Collections.singletonList((List<Long>) new ArrayList<>(result.values())));\n+\n+    final ScanStats scanStats = new ScanStats(ScanStats.GroupScanProperty.EXACT_ROW_COUNT, 1, 1, scanRowType.getFieldCount());\n+    final MetadataDirectGroupScan directScan = new MetadataDirectGroupScan(reader, fileList, scanStats, true);\n+\n+    final DrillDirectScanRel newScan = new DrillDirectScanRel(scan.getCluster(), scan.getTraitSet().plus(DrillRel.DRILL_LOGICAL),\n+      directScan, scanRowType);\n+\n+    final DrillProjectRel newProject = new DrillProjectRel(agg.getCluster(), agg.getTraitSet().plus(DrillRel.DRILL_LOGICAL),\n+      newScan, CountToDirectScanUtils.prepareFieldExpressions(scanRowType), agg.getRowType());\n+\n+    call.transformTo(newProject);\n+  }\n+\n+  private Pair<Boolean, Metadata_V4.MetadataSummary> checkMetadataForScanStats(PlannerSettings settings, DrillTable drillTable,\n+                                                                               FormatSelection formatSelection) {\n+\n+    // Currently only support metadata rowcount stats for Parquet tables\n+    FormatPluginConfig formatConfig = formatSelection.getFormat();\n+    if (!((formatConfig instanceof ParquetFormatConfig)\n+      || ((formatConfig instanceof NamedFormatPluginConfig)\n+      && ((NamedFormatPluginConfig) formatConfig).name.equals(\"parquet\")))) {\n+      return new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+    }\n+\n+    FileSystemPlugin plugin = (FileSystemPlugin) drillTable.getPlugin();\n+    DrillFileSystem fs = null;\n+    try {\n+       fs = new DrillFileSystem(plugin.getFormatPlugin(formatSelection.getFormat()).getFsConf());\n+    } catch (IOException e) {\n+      logger.warn(\"Unable to create the file system object for retrieving statistics from metadata cache file \", e);\n+      return new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+    }\n+\n+    // check if the cacheFileRoot has been set: this is needed because after directory pruning, the\n+    // cacheFileRoot could have been changed and not be the same as the original selectionRoot\n+    Path selectionRoot = formatSelection.getSelection().getCacheFileRoot() != null ?\n+            formatSelection.getSelection().getCacheFileRoot() :\n+            formatSelection.getSelection().getSelectionRoot();\n+\n+    ParquetReaderConfig parquetReaderConfig= ParquetReaderConfig.builder()\n+            .withFormatConfig((ParquetFormatConfig) formatConfig)\n+            .withOptions(settings.getOptions())\n+            .build();\n+\n+    Metadata_V4.MetadataSummary metadataSummary = Metadata.getSummary(fs, selectionRoot, false, parquetReaderConfig);\n+\n+    return metadataSummary != null ? new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(true, metadataSummary) :\n+      new ImmutablePair<Boolean, Metadata_V4.MetadataSummary>(false, null);\n+  }\n+\n+  /**\n+   * Collects counts for each aggregation call by using the metadata summary information\n+   * Will return empty result map if was not able to determine count for at least one aggregation call.\n+   *\n+   * For each aggregate call will determine if count can be calculated. Collects counts only for COUNT function.\n+   *   1. First, we get the total row count from the metadata summary.\n+   *   2. For COUNT(*) and COUNT(<non null column>) and COUNT(<implicit column>), the count = total row count\n+   *   3. For COUNT(nullable column), count = (total row count - column's null count)\n+   *   4. Also count can not be calculated for parition columns.\n+   *\n+   * @param settings planner options\n+   * @param metadataSummary metadata summary containing row counts and column counts\n+   * @param agg aggregate relational expression\n+   * @param scan scan relational expression\n+   * @param project project relational expression\n+   * @return result map where key is count column name, value is count value\n+   */\n+  private Map<String, Long> collectCounts(PlannerSettings settings, Metadata_V4.MetadataSummary metadataSummary,\n+                                          Aggregate agg, TableScan scan, Project project) {\n+    final Set<String> implicitColumnsNames = ColumnExplorer.initImplicitFileColumns(settings.getOptions()).keySet();\n+    final long totalRecordCount = metadataSummary.getTotalRowCount();\n+    final LinkedHashMap<String, Long> result = new LinkedHashMap<>();\n+\n+    for (int i = 0; i < agg.getAggCallList().size(); i++) {\n+      AggregateCall aggCall = agg.getAggCallList().get(i);\n+      long cnt;\n+\n+      // rule can be applied only for count function, return empty counts\n+      if (!\"count\".equalsIgnoreCase(aggCall.getAggregation().getName()) ) {\n+        return ImmutableMap.of();\n+      }\n+\n+      if (CountToDirectScanUtils.containsStarOrNotNullInput(aggCall, agg)) {\n+        cnt = totalRecordCount;\n+\n+      } else if (aggCall.getArgList().size() == 1) {\n+        // count(columnName) ==> Agg ( Scan )) ==> columnValueCount\n+        int index = aggCall.getArgList().get(0);\n+\n+        if (project != null) {\n+          // project in the middle of Agg and Scan : Only when input of AggCall is a RexInputRef in Project, we find the index of Scan's field.\n+          // For instance,\n+          // Agg - count($0)\n+          //  \\\n+          //  Proj - Exp={$1}\n+          //    \\\n+          //   Scan (col1, col2).\n+          // return count of \"col2\" in Scan's metadata, if found.\n+          if (!(project.getProjects().get(index) instanceof RexInputRef)) {\n+            return ImmutableMap.of(); // do not apply for all other cases.\n+          }\n+\n+          index = ((RexInputRef) project.getProjects().get(index)).getIndex();\n+        }\n+\n+        String columnName = scan.getRowType().getFieldNames().get(index).toLowerCase();\n+\n+        // for implicit column count will be the same as total record count\n+        if (implicitColumnsNames.contains(columnName)) {\n+          cnt = totalRecordCount;\n+        } else {\n+          SchemaPath simplePath = SchemaPath.getSimplePath(columnName);\n+\n+          if (ColumnExplorer.isPartitionColumn(settings.getOptions(), simplePath)) {\n+            return ImmutableMap.of();\n+          }\n+\n+          Metadata_V4.ColumnTypeMetadata_v4 columnMetadata = metadataSummary.getColumnTypeInfo(new Metadata_V4.ColumnTypeMetadata_v4.Key(simplePath));\n+\n+         if (columnMetadata == null || columnMetadata.totalNullCount == GroupScan.NO_COLUMN_STATS) {\n+            // if column stats is not available don't apply this rule, return empty counts\n+            return ImmutableMap.of();\n+          } else {\n+           // count of a nullable column = (total row count - column's null count)\n+           cnt = totalRecordCount - columnMetadata.totalNullCount;\n+         }\n+\n+        }\n+      } else {\n+        return ImmutableMap.of();\n+      }\n+\n+      String name = \"count\" + i + \"$\" + (aggCall.getName() == null ? aggCall.toString() : aggCall.getName());\n+      result.put(name, cnt);\n+    }\n+\n+    return ImmutableMap.copyOf(result);\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/ConvertCountToDirectScanRule.java",
                "sha": "63206c52473c7876cff714663d723b69e1458e4d",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 15,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.concurrent.TimeUnit;\n import java.util.regex.Pattern;\n \n+import org.apache.drill.exec.planner.common.DrillRelOptUtil;\n import org.apache.drill.exec.util.DrillFileSystemUtil;\n import org.apache.drill.shaded.guava.com.google.common.base.Stopwatch;\n import org.apache.calcite.adapter.enumerable.EnumerableTableScan;\n@@ -65,8 +66,6 @@\n import org.apache.drill.exec.planner.logical.DrillRel;\n import org.apache.drill.exec.planner.logical.DrillRelFactories;\n import org.apache.drill.exec.planner.logical.DrillScanRel;\n-import org.apache.drill.exec.planner.logical.DrillTable;\n-import org.apache.drill.exec.planner.logical.DrillTranslatableTable;\n import org.apache.drill.exec.planner.logical.DrillValuesRel;\n import org.apache.drill.exec.planner.logical.RelOptHelper;\n import org.apache.drill.exec.planner.physical.PlannerSettings;\n@@ -172,7 +171,7 @@ protected void doOnMatch(RelOptRuleCall call, Filter filterRel, Project projectR\n     PartitionDescriptor descriptor = getPartitionDescriptor(settings, scanRel);\n     final BufferAllocator allocator = optimizerContext.getAllocator();\n \n-    final Object selection = getDrillTable(scanRel).getSelection();\n+    final Object selection = DrillRelOptUtil.getDrillTable(scanRel).getSelection();\n     MetadataContext metaContext = null;\n     if (selection instanceof FormatSelection) {\n          metaContext = ((FormatSelection)selection).getSelection().getMetaContext();\n@@ -541,18 +540,9 @@ protected OptimizerRulesContext getOptimizerRulesContext() {\n \n   public abstract PartitionDescriptor getPartitionDescriptor(PlannerSettings settings, TableScan scanRel);\n \n-  private static DrillTable getDrillTable(final TableScan scan) {\n-    DrillTable drillTable;\n-    drillTable = scan.getTable().unwrap(DrillTable.class);\n-    if (drillTable == null) {\n-      drillTable = scan.getTable().unwrap(DrillTranslatableTable.class).getDrillTable();\n-    }\n-    return drillTable;\n-  }\n-\n   private static boolean isQualifiedDirPruning(final TableScan scan) {\n     if (scan instanceof EnumerableTableScan) {\n-      final Object selection = getDrillTable(scan).getSelection();\n+      final Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n       if (selection instanceof FormatSelection\n           && ((FormatSelection)selection).supportDirPruning()) {\n         return true;  // Do directory-based pruning in Calcite logical\n@@ -657,7 +647,7 @@ public void onMatch(RelOptRuleCall call) {\n       logger.debug(\"Beginning file partition pruning, pruning class: {}\", pruningClassName);\n       Stopwatch totalPruningTime = logger.isDebugEnabled() ? Stopwatch.createStarted() : null;\n \n-      Object selection = getDrillTable(scan).getSelection();\n+      Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n       MetadataContext metaContext = null;\n       FileSelection fileSelection = null;\n       if (selection instanceof FormatSelection) {\n@@ -779,7 +769,7 @@ public void onMatch(RelOptRuleCall call) {\n \n     private static boolean isQualifiedFilePruning(final TableScan scan) {\n       if (scan instanceof EnumerableTableScan) {\n-        Object selection = getDrillTable(scan).getSelection();\n+        Object selection = DrillRelOptUtil.getDrillTable(scan).getSelection();\n         return selection instanceof FormatSelection;\n       } else if (scan instanceof DrillScanRel) {\n         GroupScan groupScan = ((DrillScanRel) scan).getGroupScan();",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/partition/PruneScanRule.java",
                "sha": "723cd084f4e5cf7d47d81bb2ded7e9fd0833af9a",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "changes": 117,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 92,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "patch": "@@ -17,38 +17,32 @@\n  */\n package org.apache.drill.exec.planner.physical;\n \n+import java.util.List;\n import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.Iterator;\n import java.util.LinkedHashMap;\n-import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.Collections;\n \n-import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n import org.apache.calcite.plan.RelOptRule;\n import org.apache.calcite.plan.RelOptRuleCall;\n import org.apache.calcite.plan.RelOptRuleOperand;\n-import org.apache.calcite.rel.core.AggregateCall;\n import org.apache.calcite.rel.type.RelDataType;\n-import org.apache.calcite.rel.type.RelDataTypeField;\n-import org.apache.calcite.rel.type.RelDataTypeFieldImpl;\n-import org.apache.calcite.rel.type.RelRecordType;\n import org.apache.calcite.rex.RexInputRef;\n-import org.apache.calcite.rex.RexNode;\n-import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.calcite.rel.core.AggregateCall;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.exec.physical.base.GroupScan;\n import org.apache.drill.exec.physical.base.ScanStats;\n import org.apache.drill.exec.planner.logical.DrillAggregateRel;\n import org.apache.drill.exec.planner.logical.DrillProjectRel;\n import org.apache.drill.exec.planner.logical.DrillScanRel;\n import org.apache.drill.exec.planner.logical.RelOptHelper;\n-import org.apache.drill.exec.store.ColumnExplorer;\n+import org.apache.drill.exec.planner.common.CountToDirectScanUtils;\n \n+import org.apache.drill.exec.store.ColumnExplorer;\n import org.apache.drill.exec.store.direct.MetadataDirectGroupScan;\n import org.apache.drill.exec.store.pojo.DynamicPojoRecordReader;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n \n /**\n  * <p>\n@@ -74,23 +68,30 @@\n  * obtained from parquet row group info. This will save the cost to\n  * scan the whole parquet files.\n  * </p>\n+ *\n+ * <p>\n+ *     NOTE: This rule is a physical planning counterpart to a similar ConvertCountToDirectScanRule\n+ *     logical rule. However, while the logical rule relies on the Parquet metadata cache's Summary\n+ *     aggregates, this rule is applicable if the exact row count is available from the GroupScan\n+ *     regardless of where that stat came from. Hence, it is more general, with the trade-off that the\n+ *     GroupScan relies on the fully expanded list of row groups to compute the aggregate row count.\n+ * </p>\n  */\n-public class ConvertCountToDirectScan extends Prule {\n+public class ConvertCountToDirectScanPrule extends Prule {\n \n-  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScan(\n+  public static final RelOptRule AGG_ON_PROJ_ON_SCAN = new ConvertCountToDirectScanPrule(\n       RelOptHelper.some(DrillAggregateRel.class,\n                         RelOptHelper.some(DrillProjectRel.class,\n                             RelOptHelper.any(DrillScanRel.class))), \"Agg_on_proj_on_scan\");\n \n-  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScan(\n+  public static final RelOptRule AGG_ON_SCAN = new ConvertCountToDirectScanPrule(\n       RelOptHelper.some(DrillAggregateRel.class,\n                             RelOptHelper.any(DrillScanRel.class)), \"Agg_on_scan\");\n \n-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScan.class);\n+  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ConvertCountToDirectScanPrule.class);\n \n-  /** Creates a SplunkPushDownRule. */\n-  protected ConvertCountToDirectScan(RelOptRuleOperand rule, String id) {\n-    super(rule, \"ConvertCountToDirectScan:\" + id);\n+  protected ConvertCountToDirectScanPrule(RelOptRuleOperand rule, String id) {\n+    super(rule, \"ConvertCountToDirectScanPrule:\" + id);\n   }\n \n   @Override\n@@ -119,20 +120,20 @@ public void onMatch(RelOptRuleCall call) {\n       return;\n     }\n \n-    final RelDataType scanRowType = constructDataType(agg, result.keySet());\n+    final RelDataType scanRowType = CountToDirectScanUtils.constructDataType(agg, result.keySet());\n \n     final DynamicPojoRecordReader<Long> reader = new DynamicPojoRecordReader<>(\n-        buildSchema(scanRowType.getFieldNames()),\n+        CountToDirectScanUtils.buildSchema(scanRowType.getFieldNames()),\n         Collections.singletonList((List<Long>) new ArrayList<>(result.values())));\n \n     final ScanStats scanStats = new ScanStats(ScanStats.GroupScanProperty.EXACT_ROW_COUNT, 1, 1, scanRowType.getFieldCount());\n-    final GroupScan directScan = new MetadataDirectGroupScan(reader, oldGrpScan.getFiles(), scanStats);\n+    final GroupScan directScan = new MetadataDirectGroupScan(reader, oldGrpScan.getFiles(), scanStats, false);\n \n     final DirectScanPrel newScan = DirectScanPrel.create(scan, scan.getTraitSet().plus(Prel.DRILL_PHYSICAL)\n         .plus(DrillDistributionTrait.SINGLETON), directScan, scanRowType);\n \n     final ProjectPrel newProject = new ProjectPrel(agg.getCluster(), agg.getTraitSet().plus(Prel.DRILL_PHYSICAL)\n-        .plus(DrillDistributionTrait.SINGLETON), newScan, prepareFieldExpressions(scanRowType), agg.getRowType());\n+        .plus(DrillDistributionTrait.SINGLETON), newScan, CountToDirectScanUtils.prepareFieldExpressions(scanRowType), agg.getRowType());\n \n     call.transformTo(newProject);\n   }\n@@ -165,7 +166,7 @@ public void onMatch(RelOptRuleCall call) {\n         return ImmutableMap.of();\n       }\n \n-      if (containsStarOrNotNullInput(aggCall, agg)) {\n+      if (CountToDirectScanUtils.containsStarOrNotNullInput(aggCall, agg)) {\n         cnt = totalRecordCount;\n \n       } else if (aggCall.getArgList().size() == 1) {\n@@ -217,72 +218,4 @@ public void onMatch(RelOptRuleCall call) {\n     return ImmutableMap.copyOf(result);\n   }\n \n-  /**\n-   * Checks if aggregate call contains star or non-null expression:\n-   * <pre>\n-   * count(*)  == >  empty arg  ==>  rowCount\n-   * count(Not-null-input) ==> rowCount\n-   * </pre>\n-   *\n-   * @param aggregateCall aggregate call\n-   * @param aggregate aggregate relation expression\n-   * @return true of aggregate call contains star or non-null expression\n-   */\n-  private boolean containsStarOrNotNullInput(AggregateCall aggregateCall, DrillAggregateRel aggregate) {\n-    return aggregateCall.getArgList().isEmpty() ||\n-        (aggregateCall.getArgList().size() == 1 &&\n-            !aggregate.getInput().getRowType().getFieldList().get(aggregateCall.getArgList().get(0)).getType().isNullable());\n-  }\n-\n-  /**\n-   * For each aggregate call creates field based on its name with bigint type.\n-   * Constructs record type for created fields.\n-   *\n-   * @param aggregateRel aggregate relation expression\n-   * @param fieldNames field names\n-   * @return record type\n-   */\n-  private RelDataType constructDataType(DrillAggregateRel aggregateRel, Collection<String> fieldNames) {\n-    List<RelDataTypeField> fields = new ArrayList<>();\n-    Iterator<String> filedNamesIterator = fieldNames.iterator();\n-    int fieldIndex = 0;\n-    while (filedNamesIterator.hasNext()) {\n-      RelDataTypeField field = new RelDataTypeFieldImpl(\n-          filedNamesIterator.next(),\n-          fieldIndex++,\n-          aggregateRel.getCluster().getTypeFactory().createSqlType(SqlTypeName.BIGINT));\n-      fields.add(field);\n-    }\n-    return new RelRecordType(fields);\n-  }\n-\n-  /**\n-   * Builds schema based on given field names.\n-   * Type for each schema is set to long.class.\n-   *\n-   * @param fieldNames field names\n-   * @return schema\n-   */\n-  private LinkedHashMap<String, Class<?>> buildSchema(List<String> fieldNames) {\n-    LinkedHashMap<String, Class<?>> schema = new LinkedHashMap<>();\n-    for (String fieldName: fieldNames) {\n-      schema.put(fieldName, long.class);\n-    }\n-    return schema;\n-  }\n-\n-  /**\n-   * For each field creates row expression.\n-   *\n-   * @param rowType row type\n-   * @return list of row expressions\n-   */\n-  private List<RexNode> prepareFieldExpressions(RelDataType rowType) {\n-    List<RexNode> expressions = new ArrayList<>();\n-    for (int i = 0; i < rowType.getFieldCount(); i++) {\n-      expressions.add(RexInputRef.of(i, rowType));\n-    }\n-    return expressions;\n-  }\n-\n }",
                "previous_filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScan.java",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/ConvertCountToDirectScanPrule.java",
                "sha": "4176950608484b695edcb34f653389d1886ed924",
                "status": "renamed"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 7,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "patch": "@@ -38,21 +38,19 @@\n public class MetadataDirectGroupScan extends DirectGroupScan {\n \n   private final Collection<Path> files;\n+  private boolean usedMetadataSummaryFile = false;\n \n-  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files) {\n-    super(reader);\n-    this.files = files;\n-  }\n-\n-  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files, ScanStats stats) {\n+  public MetadataDirectGroupScan(RecordReader reader, Collection<Path> files, ScanStats stats,\n+                                 boolean usedMetadataSummaryFile) {\n     super(reader, stats);\n     this.files = files;\n+    this.usedMetadataSummaryFile = usedMetadataSummaryFile;\n   }\n \n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     assert children == null || children.isEmpty();\n-    return new MetadataDirectGroupScan(reader, files, stats);\n+    return new MetadataDirectGroupScan(reader, files, stats, usedMetadataSummaryFile);\n   }\n \n   @Override\n@@ -78,6 +76,7 @@ public String getDigest() {\n       StringBuilder builder = new StringBuilder();\n       builder.append(\"files = \").append(files).append(\", \");\n       builder.append(\"numFiles = \").append(files.size()).append(\", \");\n+      builder.append(\"usedMetadataSummaryFile = \").append(usedMetadataSummaryFile).append(\", \");\n       return builder.append(super.getDigest()).toString();\n     }\n     return super.getDigest();",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/direct/MetadataDirectGroupScan.java",
                "sha": "4fea4564c481e01a5adfe81a65ecd83fb69930a7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "patch": "@@ -796,12 +796,12 @@ private boolean getallColumnsInteresting(FileSystem fs, Path metadataParentDir,\n     return metadataSummary.isAllColumnsInteresting();\n   }\n \n-  private static Path getSummaryFileName(Path metadataParentDir) {\n+  public static Path getSummaryFileName(Path metadataParentDir) {\n     Path summaryFile = new Path(metadataParentDir, METADATA_SUMMARY_FILENAME);\n     return summaryFile;\n   }\n \n-  private static Path getDirFileName(Path metadataParentDir) {\n+  public static Path getDirFileName(Path metadataParentDir) {\n     Path metadataDirFile = new Path(metadataParentDir, METADATA_DIRECTORIES_FILENAME);\n     return metadataDirFile;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java",
                "sha": "2b0581c39516cc22c2915f10d291ab29b3b16799",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/drill/blob/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java?ref=579ebcac17eefd01b90e5d4fac8ea87b64c499ad",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "patch": "@@ -163,4 +163,79 @@ public void ensureCorrectCountWithMissingStatistics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testCountsWithMetadataCacheSummary() throws Exception {\n+    test(\"use dfs.tmp\");\n+    String tableName = \"parquet_table_counts\";\n+\n+    try {\n+      test(String.format(\"create table `%s/1` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/2` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/3` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/4` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+\n+      test(\"refresh table metadata %s\", tableName);\n+\n+      String sql = String.format(\"select\\n\" +\n+              \"count(*) as star_count,\\n\" +\n+              \"count(col_int) as int_column_count,\\n\" +\n+              \"count(col_vrchr) as vrchr_column_count\\n\" +\n+              \"from %s\", tableName);\n+\n+      int expectedNumFiles = 1;\n+      String numFilesPattern = \"numFiles = \" + expectedNumFiles;\n+      String usedMetaSummaryPattern = \"usedMetadataSummaryFile = true\";\n+      String recordReaderPattern = \"DynamicPojoRecordReader\";\n+\n+      testPlanMatchingPatterns(sql, new String[]{numFilesPattern, usedMetaSummaryPattern, recordReaderPattern});\n+\n+      testBuilder()\n+              .sqlQuery(sql)\n+              .unOrdered()\n+              .baselineColumns(\"star_count\", \"int_column_count\", \"vrchr_column_count\")\n+              .baselineValues(24L, 8L, 12L)\n+              .go();\n+\n+    } finally {\n+      test(\"drop table if exists %s\", tableName);\n+    }\n+  }\n+\n+  @Test\n+  public void testCountsWithMetadataCacheSummaryAndDirPruning() throws Exception {\n+    test(\"use dfs.tmp\");\n+    String tableName = \"parquet_table_counts\";\n+\n+    try {\n+      test(String.format(\"create table `%s/1` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/2` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/3` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+      test(String.format(\"create table `%s/4` as select * from cp.`parquet/alltypes_optional.parquet`\", tableName));\n+\n+      test(\"refresh table metadata %s\", tableName);\n+\n+      String sql = String.format(\"select\\n\" +\n+              \"count(*) as star_count,\\n\" +\n+              \"count(col_int) as int_column_count,\\n\" +\n+              \"count(col_vrchr) as vrchr_column_count\\n\" +\n+              \"from %s where dir0 = 1 \", tableName);\n+\n+      int expectedNumFiles = 1;\n+      String numFilesPattern = \"numFiles = \" + expectedNumFiles;\n+      String usedMetaSummaryPattern = \"usedMetadataSummaryFile = false\";\n+      String recordReaderPattern = \"DynamicPojoRecordReader\";\n+\n+      testPlanMatchingPatterns(sql, new String[]{numFilesPattern, usedMetaSummaryPattern, recordReaderPattern});\n+\n+      testBuilder()\n+              .sqlQuery(sql)\n+              .unOrdered()\n+              .baselineColumns(\"star_count\", \"int_column_count\", \"vrchr_column_count\")\n+              .baselineValues(6L, 2L, 3L)\n+              .go();\n+\n+    } finally {\n+      test(\"drop table if exists %s\", tableName);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/579ebcac17eefd01b90e5d4fac8ea87b64c499ad/exec/java-exec/src/test/java/org/apache/drill/exec/planner/logical/TestConvertCountToDirectScan.java",
                "sha": "d18ed45277aa7f75328394d23d9bb6fd71629261",
                "status": "modified"
            }
        ],
        "message": "DRILL-7064: Leverage the summary metadata for plain COUNT aggregates.\n\nAdd unit test\n\nModify MetadataDirectGroupScan to track summary file information and use in unit test.\n\nConflicts:\n\texec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/metadata/Metadata_V4.java\n\nFix NPE for DrillTable to account for non-eligible tables.\n\nFix bug with direct scan after directory pruning.  Add unit test.\n\nAddress review comments.\n\ncloses #1736",
        "parent": "https://github.com/apache/drill/commit/f3022eacebe63fe4d7a4a9e22a8c398e88e42588",
        "patched_files": [
            "Metadata.java",
            "PruneScanRule.java",
            "CountToDirectScanUtils.java",
            "ConvertCountToDirectScanPrule.java",
            "ConvertCountToDirectScanRule.java",
            "DrillRelOptUtil.java",
            "PlannerPhase.java",
            "MetadataDirectGroupScan.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestConvertCountToDirectScan.java"
        ]
    },
    "drill_58e4cec": {
        "bug_id": "drill_58e4cec",
        "commit": "https://github.com/apache/drill/commit/58e4cec9a913e381ef0e96b072c31e34085277b3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 3,
                "filename": "contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "patch": "@@ -160,7 +160,7 @@ private void init() {\n       statsCalculator = new TableStatsCalculator(conn, hbaseScanSpec, storagePlugin.getContext().getConfig(), storagePluginConfig);\n \n       boolean foundStartRegion = false;\n-      regionsToScan = new TreeMap<HRegionInfo, ServerName>();\n+      regionsToScan = new TreeMap<>();\n       for (HRegionLocation regionLocation : regionLocations) {\n         HRegionInfo regionInfo = regionLocation.getRegionInfo();\n         if (!foundStartRegion && hbaseScanSpec.getStartRow() != null && hbaseScanSpec.getStartRow().length != 0 && !regionInfo.containsRow(hbaseScanSpec.getStartRow())) {\n@@ -338,8 +338,7 @@ public HBaseSubScan getSpecificScan(int minorFragmentId) {\n     assert minorFragmentId < endpointFragmentMapping.size() : String.format(\n         \"Mappings length [%d] should be greater than minor fragment id [%d] but it isn't.\", endpointFragmentMapping.size(),\n         minorFragmentId);\n-    return new HBaseSubScan(getUserName(), storagePlugin, storagePluginConfig,\n-        endpointFragmentMapping.get(minorFragmentId), columns);\n+    return new HBaseSubScan(getUserName(), storagePlugin, endpointFragmentMapping.get(minorFragmentId), columns);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseGroupScan.java",
                "sha": "11782987ac4deeb9c245b346dcc276f68c379556",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 20,
                "filename": "contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "patch": "@@ -24,7 +24,6 @@\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractBase;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n import org.apache.drill.exec.physical.base.PhysicalVisitor;\n@@ -49,44 +48,43 @@\n public class HBaseSubScan extends AbstractBase implements SubScan {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(HBaseSubScan.class);\n \n-  @JsonProperty\n-  public final HBaseStoragePluginConfig storage;\n-  @JsonIgnore\n   private final HBaseStoragePlugin hbaseStoragePlugin;\n   private final List<HBaseSubScanSpec> regionScanSpecList;\n   private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public HBaseSubScan(@JacksonInject StoragePluginRegistry registry,\n                       @JsonProperty(\"userName\") String userName,\n-                      @JsonProperty(\"storage\") StoragePluginConfig storage,\n+                      @JsonProperty(\"hbaseStoragePluginConfig\") HBaseStoragePluginConfig hbaseStoragePluginConfig,\n                       @JsonProperty(\"regionScanSpecList\") LinkedList<HBaseSubScanSpec> regionScanSpecList,\n                       @JsonProperty(\"columns\") List<SchemaPath> columns) throws ExecutionSetupException {\n-    super(userName);\n-    hbaseStoragePlugin = (HBaseStoragePlugin) registry.getPlugin(storage);\n-    this.regionScanSpecList = regionScanSpecList;\n-    this.storage = (HBaseStoragePluginConfig) storage;\n-    this.columns = columns;\n+    this(userName,\n+        (HBaseStoragePlugin) registry.getPlugin(hbaseStoragePluginConfig),\n+        regionScanSpecList,\n+        columns);\n   }\n \n-  public HBaseSubScan(String userName, HBaseStoragePlugin plugin, HBaseStoragePluginConfig config,\n-      List<HBaseSubScanSpec> regionInfoList, List<SchemaPath> columns) {\n+  public HBaseSubScan(String userName,\n+                      HBaseStoragePlugin hbaseStoragePlugin,\n+                      List<HBaseSubScanSpec> regionInfoList,\n+                      List<SchemaPath> columns) {\n     super(userName);\n-    hbaseStoragePlugin = plugin;\n-    storage = config;\n+    this.hbaseStoragePlugin = hbaseStoragePlugin;\n     this.regionScanSpecList = regionInfoList;\n     this.columns = columns;\n   }\n \n-  public List<HBaseSubScanSpec> getRegionScanSpecList() {\n-    return regionScanSpecList;\n+  @JsonProperty\n+  public HBaseStoragePluginConfig getHbaseStoragePluginConfig() {\n+    return hbaseStoragePlugin.getConfig();\n   }\n \n-  @JsonIgnore\n-  public HBaseStoragePluginConfig getStorageConfig() {\n-    return storage;\n+  @JsonProperty\n+  public List<HBaseSubScanSpec> getRegionScanSpecList() {\n+    return regionScanSpecList;\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n@@ -109,7 +107,7 @@ public HBaseStoragePlugin getStorageEngine(){\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new HBaseSubScan(getUserName(), hbaseStoragePlugin, storage, regionScanSpecList, columns);\n+    return new HBaseSubScan(getUserName(), hbaseStoragePlugin, regionScanSpecList, columns);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/main/java/org/apache/drill/exec/store/hbase/HBaseSubScan.java",
                "sha": "bd179fbac35cd649a3187aa127f73b22f02dbd03",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 0,
                "filename": "contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.categories.HbaseStorageTest;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.exec.rpc.user.QueryDataBatch;\n@@ -109,4 +110,9 @@ public void testSelectFromSchema() throws Exception {\n     runHBaseSQLVerifyCount(\"SELECT row_key\\n\"\n         + \" FROM hbase.TestTableNullStr t WHERE row_key='a1'\", 1);\n   }\n+\n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from hbase.TestTableNullStr\");\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hbase/src/test/java/org/apache/drill/hbase/TestHBaseQueries.java",
                "sha": "0d5349943d7c25754a34970b06630e006615b5c2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 7,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -64,7 +64,7 @@ public HivePartitionDescriptor(@SuppressWarnings(\"unused\") final PlannerSettings\n     this.scanRel = scanRel;\n     this.managedBuffer = managedBuffer.reallocIfNeeded(256);\n     this.defaultPartitionValue = defaultPartitionValue;\n-    for (HiveTableWrapper.FieldSchemaWrapper wrapper : ((HiveScan) scanRel.getGroupScan()).hiveReadEntry.table.partitionKeys) {\n+    for (HiveTableWrapper.FieldSchemaWrapper wrapper : ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry().table.partitionKeys) {\n       partitionMap.put(wrapper.name, i);\n       i++;\n     }\n@@ -88,7 +88,7 @@ public int getMaxHierarchyLevel() {\n \n   @Override\n   public String getBaseTableLocation() {\n-    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).hiveReadEntry;\n+    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry();\n     return origEntry.table.getTable().getSd().getLocation();\n   }\n \n@@ -97,7 +97,7 @@ public void populatePartitionVectors(ValueVector[] vectors, List<PartitionLocati\n                                        BitSet partitionColumnBitSet, Map<Integer, String> fieldNameMap) {\n     int record = 0;\n     final HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n-    final Map<String, String> partitionNameTypeMap = hiveScan.hiveReadEntry.table.getPartitionNameTypeMap();\n+    final Map<String, String> partitionNameTypeMap = hiveScan.getHiveReadEntry().table.getPartitionNameTypeMap();\n     for(PartitionLocation partitionLocation: partitions) {\n       for(int partitionColumnIndex : BitSets.toIter(partitionColumnBitSet)){\n         final String hiveType = partitionNameTypeMap.get(fieldNameMap.get(partitionColumnIndex));\n@@ -126,7 +126,7 @@ public void populatePartitionVectors(ValueVector[] vectors, List<PartitionLocati\n   public TypeProtos.MajorType getVectorType(SchemaPath column, PlannerSettings plannerSettings) {\n     HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n     String partitionName = column.getAsNamePart().getName();\n-    Map<String, String> partitionNameTypeMap = hiveScan.hiveReadEntry.table.getPartitionNameTypeMap();\n+    Map<String, String> partitionNameTypeMap = hiveScan.getHiveReadEntry().table.getPartitionNameTypeMap();\n     String hiveType = partitionNameTypeMap.get(partitionName);\n     PrimitiveTypeInfo primitiveTypeInfo = (PrimitiveTypeInfo) TypeInfoUtils.getTypeInfoFromTypeString(hiveType);\n \n@@ -143,7 +143,7 @@ public Integer getIdIfValid(String name) {\n   @Override\n   protected void createPartitionSublists() {\n     List<PartitionLocation> locations = new LinkedList<>();\n-    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).hiveReadEntry;\n+    HiveReadEntry origEntry = ((HiveScan) scanRel.getGroupScan()).getHiveReadEntry();\n     for (Partition partition: origEntry.getPartitions()) {\n       locations.add(new HivePartitionLocation(partition.getValues(), partition.getSd().getLocation()));\n     }\n@@ -165,7 +165,7 @@ public TableScan createTableScan(List<PartitionLocation> newPartitions, boolean\n \n   private GroupScan createNewGroupScan(List<PartitionLocation> newPartitionLocations) throws ExecutionSetupException {\n     HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n-    HiveReadEntry origReadEntry = hiveScan.hiveReadEntry;\n+    HiveReadEntry origReadEntry = hiveScan.getHiveReadEntry();\n     List<HiveTableWrapper.HivePartitionWrapper> oldPartitions = origReadEntry.partitions;\n     List<HiveTableWrapper.HivePartitionWrapper> newPartitions = Lists.newLinkedList();\n ",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/HivePartitionDescriptor.java",
                "sha": "2d2bb6c6984b617de482dba38127771aad088816",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 7,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "patch": "@@ -97,16 +97,16 @@ public boolean matches(RelOptRuleCall call) {\n \n     final HiveScan hiveScan = (HiveScan) scanRel.getGroupScan();\n     final HiveConf hiveConf = hiveScan.getHiveConf();\n-    final HiveTableWithColumnCache hiveTable = hiveScan.hiveReadEntry.getTable();\n+    final HiveTableWithColumnCache hiveTable = hiveScan.getHiveReadEntry().getTable();\n \n     final Class<? extends InputFormat<?,?>> tableInputFormat =\n-        getInputFormatFromSD(HiveUtilities.getTableMetadata(hiveTable), hiveScan.hiveReadEntry, hiveTable.getSd(),\n+        getInputFormatFromSD(HiveUtilities.getTableMetadata(hiveTable), hiveScan.getHiveReadEntry(), hiveTable.getSd(),\n             hiveConf);\n     if (tableInputFormat == null || !tableInputFormat.equals(MapredParquetInputFormat.class)) {\n       return false;\n     }\n \n-    final List<HivePartitionWrapper> partitions = hiveScan.hiveReadEntry.getHivePartitionWrappers();\n+    final List<HivePartitionWrapper> partitions = hiveScan.getHiveReadEntry().getHivePartitionWrappers();\n     if (partitions == null) {\n       return true;\n     }\n@@ -116,7 +116,7 @@ public boolean matches(RelOptRuleCall call) {\n     for (HivePartitionWrapper partition : partitions) {\n       final StorageDescriptor partitionSD = partition.getPartition().getSd();\n       Class<? extends InputFormat<?, ?>> inputFormat = getInputFormatFromSD(\n-          HiveUtilities.getPartitionMetadata(partition.getPartition(), hiveTable), hiveScan.hiveReadEntry, partitionSD,\n+          HiveUtilities.getPartitionMetadata(partition.getPartition(), hiveTable), hiveScan.getHiveReadEntry(), partitionSD,\n           hiveConf);\n       if (inputFormat == null || !inputFormat.equals(tableInputFormat)) {\n         return false;\n@@ -172,7 +172,7 @@ public void onMatch(RelOptRuleCall call) {\n       final PlannerSettings settings = PrelUtil.getPlannerSettings(call.getPlanner());\n       final String partitionColumnLabel = settings.getFsPartitionColumnLabel();\n \n-      final Table hiveTable = hiveScan.hiveReadEntry.getTable();\n+      final Table hiveTable = hiveScan.getHiveReadEntry().getTable();\n       checkForUnsupportedDataTypes(hiveTable);\n \n       final Map<String, String> partitionColMapping =\n@@ -245,8 +245,8 @@ private DrillScanRel createNativeScanRel(final Map<String, String> partitionColM\n     final HiveDrillNativeParquetScan nativeHiveScan =\n         new HiveDrillNativeParquetScan(\n             hiveScan.getUserName(),\n-            hiveScan.hiveReadEntry,\n-            hiveScan.storagePlugin,\n+            hiveScan.getHiveReadEntry(),\n+            hiveScan.getStoragePlugin(),\n             nativeScanCols,\n             null);\n ",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/planner/sql/logical/ConvertHiveParquetScanToDrillParquetScan.java",
                "sha": "a7322454bc1b55ee80778fd062ec7df091711f7a",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 11,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -41,16 +41,16 @@\n \n   @JsonCreator\n   public HiveDrillNativeParquetScan(@JsonProperty(\"userName\") String userName,\n-                                    @JsonProperty(\"hive-table\") HiveReadEntry hiveReadEntry,\n-                                    @JsonProperty(\"storage-plugin\") String storagePluginName,\n+                                    @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n+                                    @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig,\n                                     @JsonProperty(\"columns\") List<SchemaPath> columns,\n                                     @JacksonInject StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n-    super(userName, hiveReadEntry, storagePluginName, columns, pluginRegistry);\n+    super(userName, hiveReadEntry, hiveStoragePluginConfig, columns, pluginRegistry);\n   }\n \n-  public HiveDrillNativeParquetScan(String userName, HiveReadEntry hiveReadEntry, HiveStoragePlugin storagePlugin,\n+  public HiveDrillNativeParquetScan(String userName, HiveReadEntry hiveReadEntry, HiveStoragePlugin hiveStoragePlugin,\n       List<SchemaPath> columns, HiveMetadataProvider metadataProvider) throws ExecutionSetupException {\n-    super(userName, hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    super(userName, hiveReadEntry, hiveStoragePlugin, columns, metadataProvider);\n   }\n \n   public HiveDrillNativeParquetScan(final HiveScan hiveScan) {\n@@ -91,7 +91,7 @@ public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) thro\n \n   @Override\n   public HiveScan clone(HiveReadEntry hiveReadEntry) throws ExecutionSetupException {\n-    return new HiveDrillNativeParquetScan(getUserName(), hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    return new HiveDrillNativeParquetScan(getUserName(), hiveReadEntry, getStoragePlugin(), getColumns(), getMetadataProvider());\n   }\n \n   @Override\n@@ -103,12 +103,12 @@ public GroupScan clone(List<SchemaPath> columns) {\n \n   @Override\n   public String toString() {\n-    final List<HivePartitionWrapper> partitions = hiveReadEntry.getHivePartitionWrappers();\n+    final List<HivePartitionWrapper> partitions = getHiveReadEntry().getHivePartitionWrappers();\n     int numPartitions = partitions == null ? 0 : partitions.size();\n-    return \"HiveDrillNativeParquetScan [table=\" + hiveReadEntry.getHiveTableWrapper()\n-        + \", columns=\" + columns\n+    return \"HiveDrillNativeParquetScan [table=\" + getHiveReadEntry().getHiveTableWrapper()\n+        + \", columns=\" + getColumns()\n         + \", numPartitions=\" + numPartitions\n         + \", partitions= \" + partitions\n-        + \", inputDirectories=\" + metadataProvider.getInputDirectories(hiveReadEntry) + \"]\";\n+        + \", inputDirectories=\" + getMetadataProvider().getInputDirectories(getHiveReadEntry()) + \"]\";\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetScan.java",
                "sha": "202bd435e1c31515853542e6170f91fa573aa855",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 2,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "patch": "@@ -42,9 +42,9 @@ public HiveDrillNativeParquetSubScan(@JacksonInject StoragePluginRegistry regist\n                                        @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n                                        @JsonProperty(\"splitClasses\") List<String> splitClasses,\n                                        @JsonProperty(\"columns\") List<SchemaPath> columns,\n-                                       @JsonProperty(\"storagePluginName\") String pluginName)\n+                                       @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig)\n       throws IOException, ExecutionSetupException, ReflectiveOperationException {\n-    super(registry, userName, splits, hiveReadEntry, splitClasses, columns, pluginName);\n+    super(registry, userName, splits, hiveReadEntry, splitClasses, columns, hiveStoragePluginConfig);\n   }\n \n   public HiveDrillNativeParquetSubScan(final HiveSubScan subScan)",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveDrillNativeParquetSubScan.java",
                "sha": "2129ed454234dec2123e4f86df7221eed0d66d39",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 26,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "patch": "@@ -61,41 +61,36 @@\n \n   private static int HIVE_SERDE_SCAN_OVERHEAD_FACTOR_PER_COLUMN = 20;\n \n-  @JsonProperty(\"hive-table\")\n-  public HiveReadEntry hiveReadEntry;\n+  private final HiveStoragePlugin hiveStoragePlugin;\n+  private final HiveReadEntry hiveReadEntry;\n+  private final HiveMetadataProvider metadataProvider;\n \n-  @JsonIgnore\n-  public HiveStoragePlugin storagePlugin;\n-\n-  @JsonProperty(\"columns\")\n-  public List<SchemaPath> columns;\n-\n-  @JsonIgnore\n-  protected final HiveMetadataProvider metadataProvider;\n-\n-  @JsonIgnore\n   private List<List<LogicalInputSplit>> mappings;\n+  private List<LogicalInputSplit> inputSplits;\n \n-  @JsonIgnore\n-  protected List<LogicalInputSplit> inputSplits;\n+  protected List<SchemaPath> columns;\n \n   @JsonCreator\n   public HiveScan(@JsonProperty(\"userName\") final String userName,\n-                  @JsonProperty(\"hive-table\") final HiveReadEntry hiveReadEntry,\n-                  @JsonProperty(\"storage-plugin\") final String storagePluginName,\n+                  @JsonProperty(\"hiveReadEntry\") final HiveReadEntry hiveReadEntry,\n+                  @JsonProperty(\"hiveStoragePluginConfig\") final HiveStoragePluginConfig hiveStoragePluginConfig,\n                   @JsonProperty(\"columns\") final List<SchemaPath> columns,\n                   @JacksonInject final StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n-    this(userName, hiveReadEntry, (HiveStoragePlugin) pluginRegistry.getPlugin(storagePluginName), columns, null);\n+    this(userName,\n+        hiveReadEntry,\n+        (HiveStoragePlugin) pluginRegistry.getPlugin(hiveStoragePluginConfig),\n+        columns,\n+        null);\n   }\n \n-  public HiveScan(final String userName, final HiveReadEntry hiveReadEntry, final HiveStoragePlugin storagePlugin,\n+  public HiveScan(final String userName, final HiveReadEntry hiveReadEntry, final HiveStoragePlugin hiveStoragePlugin,\n       final List<SchemaPath> columns, final HiveMetadataProvider metadataProvider) throws ExecutionSetupException {\n     super(userName);\n     this.hiveReadEntry = hiveReadEntry;\n     this.columns = columns;\n-    this.storagePlugin = storagePlugin;\n+    this.hiveStoragePlugin = hiveStoragePlugin;\n     if (metadataProvider == null) {\n-      this.metadataProvider = new HiveMetadataProvider(userName, hiveReadEntry, storagePlugin.getHiveConf());\n+      this.metadataProvider = new HiveMetadataProvider(userName, hiveReadEntry, hiveStoragePlugin.getHiveConf());\n     } else {\n       this.metadataProvider = metadataProvider;\n     }\n@@ -105,26 +100,47 @@ public HiveScan(final HiveScan that) {\n     super(that);\n     this.columns = that.columns;\n     this.hiveReadEntry = that.hiveReadEntry;\n-    this.storagePlugin = that.storagePlugin;\n+    this.hiveStoragePlugin = that.hiveStoragePlugin;\n     this.metadataProvider = that.metadataProvider;\n   }\n \n   public HiveScan clone(final HiveReadEntry hiveReadEntry) throws ExecutionSetupException {\n-    return new HiveScan(getUserName(), hiveReadEntry, storagePlugin, columns, metadataProvider);\n+    return new HiveScan(getUserName(), hiveReadEntry, hiveStoragePlugin, columns, metadataProvider);\n+  }\n+\n+  @JsonProperty\n+  public HiveReadEntry getHiveReadEntry() {\n+    return hiveReadEntry;\n   }\n \n+  @JsonProperty\n+  public HiveStoragePluginConfig getHiveStoragePluginConfig() {\n+    return hiveStoragePlugin.getConfig();\n+  }\n+\n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n-  protected List<LogicalInputSplit> getInputSplits() {\n+  @JsonIgnore\n+  public HiveStoragePlugin getStoragePlugin() {\n+    return hiveStoragePlugin;\n+  }\n+\n+  protected HiveMetadataProvider getMetadataProvider() {\n+    return metadataProvider;\n+  }\n+\n+  private List<LogicalInputSplit> getInputSplits() {\n     if (inputSplits == null) {\n       inputSplits = metadataProvider.getInputSplits(hiveReadEntry);\n     }\n \n     return inputSplits;\n   }\n \n+\n   @Override\n   public void applyAssignments(final List<CoordinationProtos.DrillbitEndpoint> endpoints) {\n     mappings = new ArrayList<>();\n@@ -160,7 +176,7 @@ public SubScan getSpecificScan(final int minorFragmentId) throws ExecutionSetupE\n       }\n \n       final HiveReadEntry subEntry = new HiveReadEntry(hiveReadEntry.getTableWrapper(), parts);\n-      return new HiveSubScan(getUserName(), encodedInputSplits, subEntry, splitTypes, columns, storagePlugin);\n+      return new HiveSubScan(getUserName(), encodedInputSplits, subEntry, splitTypes, columns, hiveStoragePlugin);\n     } catch (IOException | ReflectiveOperationException e) {\n       throw new ExecutionSetupException(e);\n     }\n@@ -174,7 +190,7 @@ public int getMaxParallelizationWidth() {\n   @Override\n   public List<EndpointAffinity> getOperatorAffinity() {\n     final Map<String, DrillbitEndpoint> endpointMap = new HashMap<>();\n-    for (final DrillbitEndpoint endpoint : storagePlugin.getContext().getBits()) {\n+    for (final DrillbitEndpoint endpoint : hiveStoragePlugin.getContext().getBits()) {\n       endpointMap.put(endpoint.getAddress(), endpoint);\n       logger.debug(\"endpoing address: {}\", endpoint.getAddress());\n     }\n@@ -285,7 +301,7 @@ public boolean supportsPartitionFilterPushdown() {\n \n   @JsonIgnore\n   public HiveConf getHiveConf() {\n-    return storagePlugin.getHiveConf();\n+    return hiveStoragePlugin.getHiveConf();\n   }\n \n   @JsonIgnore",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveScan.java",
                "sha": "11d47f304cbb150fc5769ae2b93c6310ece75121",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "changes": 130,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 59,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "patch": "@@ -24,6 +24,7 @@\n import java.util.List;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.google.common.collect.ImmutableSet;\n import org.apache.commons.codec.binary.Base64;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n@@ -40,26 +41,20 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n-import com.google.common.collect.Iterators;\n import com.google.common.io.ByteArrayDataInput;\n import com.google.common.io.ByteStreams;\n \n @JsonTypeName(\"hive-sub-scan\")\n public class HiveSubScan extends AbstractBase implements SubScan {\n-  protected HiveReadEntry hiveReadEntry;\n \n-  @JsonIgnore\n-  protected List<List<InputSplit>> inputSplits = new ArrayList<>();\n-  @JsonIgnore\n-  protected HiveTableWithColumnCache table;\n-  @JsonIgnore\n-  protected List<HivePartition> partitions;\n-  @JsonIgnore\n-  protected HiveStoragePlugin storagePlugin;\n-\n-  private List<List<String>> splits;\n-  private List<String> splitClasses;\n-  protected List<SchemaPath> columns;\n+  private final HiveReadEntry hiveReadEntry;\n+  private final List<List<InputSplit>> inputSplits = new ArrayList<>();\n+  private final HiveStoragePlugin hiveStoragePlugin;\n+  private final List<List<String>> splits;\n+  private final List<String> splitClasses;\n+  private final HiveTableWithColumnCache table;\n+  private final List<HivePartition> partitions;\n+  private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public HiveSubScan(@JacksonInject StoragePluginRegistry registry,\n@@ -68,13 +63,22 @@ public HiveSubScan(@JacksonInject StoragePluginRegistry registry,\n                      @JsonProperty(\"hiveReadEntry\") HiveReadEntry hiveReadEntry,\n                      @JsonProperty(\"splitClasses\") List<String> splitClasses,\n                      @JsonProperty(\"columns\") List<SchemaPath> columns,\n-                     @JsonProperty(\"storagePluginName\") String pluginName)\n+                     @JsonProperty(\"hiveStoragePluginConfig\") HiveStoragePluginConfig hiveStoragePluginConfig)\n       throws IOException, ExecutionSetupException, ReflectiveOperationException {\n-    this(userName, splits, hiveReadEntry, splitClasses, columns, (HiveStoragePlugin)registry.getPlugin(pluginName));\n-  }\n-\n-  public HiveSubScan(final String userName, final List<List<String>> splits, final HiveReadEntry hiveReadEntry,\n-      final List<String> splitClasses, final List<SchemaPath> columns, final HiveStoragePlugin plugin)\n+    this(userName,\n+        splits,\n+        hiveReadEntry,\n+        splitClasses,\n+        columns,\n+        (HiveStoragePlugin) registry.getPlugin(hiveStoragePluginConfig));\n+  }\n+\n+  public HiveSubScan(final String userName,\n+                     final List<List<String>> splits,\n+                     final HiveReadEntry hiveReadEntry,\n+                      final List<String> splitClasses,\n+                     final List<SchemaPath> columns,\n+                     final HiveStoragePlugin hiveStoragePlugin)\n     throws IOException, ReflectiveOperationException {\n     super(userName);\n     this.hiveReadEntry = hiveReadEntry;\n@@ -83,66 +87,61 @@ public HiveSubScan(final String userName, final List<List<String>> splits, final\n     this.splits = splits;\n     this.splitClasses = splitClasses;\n     this.columns = columns;\n-    this.storagePlugin = plugin;\n+    this.hiveStoragePlugin = hiveStoragePlugin;\n \n     for (int i = 0; i < splits.size(); i++) {\n       inputSplits.add(deserializeInputSplit(splits.get(i), splitClasses.get(i)));\n     }\n   }\n \n-  @JsonProperty(\"storagePluginName\")\n-  @SuppressWarnings(\"unused\")\n-  public String getStoragePluginName() {\n-    return storagePlugin.getName();\n-  }\n-\n-  @JsonIgnore\n-  public HiveStoragePlugin getStoragePlugin() {\n-    return storagePlugin;\n-  }\n-\n+  @JsonProperty\n   public List<List<String>> getSplits() {\n     return splits;\n   }\n \n-  public HiveTableWithColumnCache getTable() {\n-    return table;\n-  }\n-\n-  public List<HivePartition> getPartitions() {\n-    return partitions;\n+  @JsonProperty\n+  public HiveReadEntry getHiveReadEntry() {\n+    return hiveReadEntry;\n   }\n \n+  @JsonProperty\n   public List<String> getSplitClasses() {\n     return splitClasses;\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n+  @JsonProperty\n+  public HiveStoragePluginConfig getHiveStoragePluginConfig() {\n+    return hiveStoragePlugin.getConfig();\n+  }\n+\n+  @JsonIgnore\n+  public HiveTableWithColumnCache getTable() {\n+    return table;\n+  }\n+\n+  @JsonIgnore\n+  public List<HivePartition> getPartitions() {\n+    return partitions;\n+  }\n+\n+  @JsonIgnore\n   public List<List<InputSplit>> getInputSplits() {\n     return inputSplits;\n   }\n \n-  public HiveReadEntry getHiveReadEntry() {\n-    return hiveReadEntry;\n+  @JsonIgnore\n+  public HiveStoragePlugin getStoragePlugin() {\n+    return hiveStoragePlugin;\n   }\n \n-  public static List<InputSplit> deserializeInputSplit(List<String> base64, String className) throws IOException, ReflectiveOperationException{\n-    Constructor<?> constructor = Class.forName(className).getDeclaredConstructor();\n-    if (constructor == null) {\n-      throw new ReflectiveOperationException(\"Class \" + className + \" does not implement a default constructor.\");\n-    }\n-    constructor.setAccessible(true);\n-    List<InputSplit> splits = new ArrayList<>();\n-    for (String str : base64) {\n-      InputSplit split = (InputSplit) constructor.newInstance();\n-      ByteArrayDataInput byteArrayDataInput = ByteStreams.newDataInput(Base64.decodeBase64(str));\n-      split.readFields(byteArrayDataInput);\n-      splits.add(split);\n-    }\n-    return splits;\n+  @JsonIgnore\n+  public HiveConf getHiveConf() {\n+    return hiveStoragePlugin.getHiveConf();\n   }\n \n   @Override\n@@ -153,24 +152,37 @@ public HiveReadEntry getHiveReadEntry() {\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {\n     try {\n-      return new HiveSubScan(getUserName(), splits, hiveReadEntry, splitClasses, columns, storagePlugin);\n+      return new HiveSubScan(getUserName(), splits, hiveReadEntry, splitClasses, columns, hiveStoragePlugin);\n     } catch (IOException | ReflectiveOperationException e) {\n       throw new ExecutionSetupException(e);\n     }\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n-    return Iterators.emptyIterator();\n+    return ImmutableSet.<PhysicalOperator>of().iterator();\n   }\n \n   @Override\n   public int getOperatorType() {\n     return CoreOperatorType.HIVE_SUB_SCAN_VALUE;\n   }\n \n-  @JsonIgnore\n-  public HiveConf getHiveConf() {\n-    return storagePlugin.getHiveConf();\n+  private static List<InputSplit> deserializeInputSplit(List<String> base64, String className)\n+      throws IOException, ReflectiveOperationException{\n+    Constructor<?> constructor = Class.forName(className).getDeclaredConstructor();\n+    if (constructor == null) {\n+      throw new ReflectiveOperationException(\"Class \" + className + \" does not implement a default constructor.\");\n+    }\n+    constructor.setAccessible(true);\n+    List<InputSplit> splits = new ArrayList<>();\n+    for (String str : base64) {\n+      InputSplit split = (InputSplit) constructor.newInstance();\n+      ByteArrayDataInput byteArrayDataInput = ByteStreams.newDataInput(Base64.decodeBase64(str));\n+      split.readFields(byteArrayDataInput);\n+      splits.add(split);\n+    }\n+    return splits;\n   }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveSubScan.java",
                "sha": "8ca8647cb1d7950f559b7b84f75e0a9a62ffd484",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 1,
                "filename": "contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.categories.HiveStorageTest;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.common.exceptions.UserRemoteException;\n@@ -100,7 +101,6 @@ public void convertFromOnHiveBinaryType() throws Exception {\n   /**\n    * Test to ensure Drill reads the all supported types correctly both normal fields (converted to Nullable types) and\n    * partition fields (converted to Required types).\n-   * @throws Exception\n    */\n   @Test\n   public void readAllSupportedHiveDataTypes() throws Exception {\n@@ -558,6 +558,11 @@ public void testNonAsciiStringLiterals() throws Exception {\n         .go();\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from hive.kv\");\n+  }\n+\n   private void verifyColumnsMetadata(List<UserProtos.ResultColumnMetadata> columnsList, Map<String, Integer> expectedResult) {\n     for (UserProtos.ResultColumnMetadata columnMetadata : columnsList) {\n       assertTrue(\"Column should be present in result set\", expectedResult.containsKey(columnMetadata.getColumnName()));",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-hive/core/src/test/java/org/apache/drill/exec/hive/TestHiveStorage.java",
                "sha": "c2412ad7e7f4fc18d7e10f9fad42760c24227970",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 0,
                "filename": "contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "patch": "@@ -127,4 +127,9 @@ public void pushdownJoinAndFilterPushDown() throws Exception {\n     testPlanMatchingPatterns(query, new String[] {}, new String[] { \"Join\", \"Filter\" });\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    testPhysicalPlanExecutionBasedOnQuery(\"select * from mysql.`drill_mysql_test`.person\");\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-jdbc/src/test/java/org/apache/drill/exec/store/jdbc/TestJdbcPluginWithMySQLIT.java",
                "sha": "7b8c21acc0dc2f783ddccc03deed2361099841a4",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 19,
                "filename": "contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "patch": "@@ -69,47 +69,49 @@\n   private static final long MSG_SIZE = 1024;\n \n   private final KafkaStoragePlugin kafkaStoragePlugin;\n-  private final KafkaStoragePluginConfig kafkaStoragePluginConfig;\n-  private List<SchemaPath> columns;\n   private final KafkaScanSpec kafkaScanSpec;\n \n+  private List<SchemaPath> columns;\n   private List<PartitionScanWork> partitionWorkList;\n   private ListMultimap<Integer, PartitionScanWork> assignments;\n   private List<EndpointAffinity> affinities;\n \n   @JsonCreator\n   public KafkaGroupScan(@JsonProperty(\"userName\") String userName,\n-      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n-      @JsonProperty(\"columns\") List<SchemaPath> columns, @JsonProperty(\"scanSpec\") KafkaScanSpec scanSpec,\n-      @JacksonInject StoragePluginRegistry pluginRegistry) {\n-    this(userName, kafkaStoragePluginConfig, columns, scanSpec, (KafkaStoragePlugin) pluginRegistry);\n+                        @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n+                        @JsonProperty(\"columns\") List<SchemaPath> columns,\n+                        @JsonProperty(\"kafkaScanSpec\") KafkaScanSpec scanSpec,\n+                        @JacksonInject StoragePluginRegistry pluginRegistry) throws ExecutionSetupException {\n+    this(userName,\n+        (KafkaStoragePlugin) pluginRegistry.getPlugin(kafkaStoragePluginConfig),\n+        columns,\n+        scanSpec);\n   }\n \n   public KafkaGroupScan(KafkaStoragePlugin kafkaStoragePlugin, KafkaScanSpec kafkaScanSpec, List<SchemaPath> columns) {\n     super(StringUtils.EMPTY);\n     this.kafkaStoragePlugin = kafkaStoragePlugin;\n-    this.kafkaStoragePluginConfig = (KafkaStoragePluginConfig) kafkaStoragePlugin.getConfig();\n     this.columns = columns;\n     this.kafkaScanSpec = kafkaScanSpec;\n     init();\n   }\n \n-  public KafkaGroupScan(String userName, KafkaStoragePluginConfig kafkaStoragePluginConfig, List<SchemaPath> columns,\n-      KafkaScanSpec kafkaScanSpec, KafkaStoragePlugin pluginRegistry) {\n+  public KafkaGroupScan(String userName,\n+                        KafkaStoragePlugin kafkaStoragePlugin,\n+                        List<SchemaPath> columns,\n+                        KafkaScanSpec kafkaScanSpec) {\n     super(userName);\n-    this.kafkaStoragePluginConfig = kafkaStoragePluginConfig;\n+    this.kafkaStoragePlugin = kafkaStoragePlugin;\n     this.columns = columns;\n     this.kafkaScanSpec = kafkaScanSpec;\n-    this.kafkaStoragePlugin = pluginRegistry;\n     init();\n   }\n \n   public KafkaGroupScan(KafkaGroupScan that) {\n     super(that);\n-    this.kafkaStoragePluginConfig = that.kafkaStoragePluginConfig;\n+    this.kafkaStoragePlugin = that.kafkaStoragePlugin;\n     this.columns = that.columns;\n     this.kafkaScanSpec = that.kafkaScanSpec;\n-    this.kafkaStoragePlugin = that.kafkaStoragePlugin;\n     this.partitionWorkList = that.partitionWorkList;\n     this.assignments = that.assignments;\n   }\n@@ -242,7 +244,7 @@ public KafkaSubScan getSpecificScan(int minorFragmentId) {\n           work.getBeginOffset(), work.getLatestOffset()));\n     }\n \n-    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, kafkaStoragePluginConfig, columns, scanSpecList);\n+    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, columns, scanSpecList);\n   }\n \n   @Override\n@@ -291,18 +293,18 @@ public GroupScan clone(List<SchemaPath> columns) {\n     return clone;\n   }\n \n-  @JsonProperty(\"kafkaStoragePluginConfig\")\n-  public KafkaStoragePluginConfig getStorageConfig() {\n-    return this.kafkaStoragePluginConfig;\n+  @JsonProperty\n+  public KafkaStoragePluginConfig getKafkaStoragePluginConfig() {\n+    return kafkaStoragePlugin.getConfig();\n   }\n \n   @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n-  @JsonProperty(\"kafkaScanSpec\")\n-  public KafkaScanSpec getScanSpec() {\n+  @JsonProperty\n+  public KafkaScanSpec getKafkaScanSpec() {\n     return kafkaScanSpec;\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaGroupScan.java",
                "sha": "9cf575b3451bac5e0f75409e2c27368ded82adbf",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 29,
                "filename": "contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "patch": "@@ -41,34 +41,31 @@\n @JsonTypeName(\"kafka-partition-scan\")\n public class KafkaSubScan extends AbstractBase implements SubScan {\n \n-  @JsonProperty\n-  private final KafkaStoragePluginConfig KafkaStoragePluginConfig;\n-\n-  @JsonIgnore\n   private final KafkaStoragePlugin kafkaStoragePlugin;\n   private final List<SchemaPath> columns;\n-  private final List<KafkaSubScanSpec> partitions;\n+  private final List<KafkaSubScanSpec> partitionSubScanSpecList;\n \n   @JsonCreator\n-  public KafkaSubScan(@JacksonInject StoragePluginRegistry registry, @JsonProperty(\"userName\") String userName,\n-      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n-      @JsonProperty(\"columns\") List<SchemaPath> columns,\n-      @JsonProperty(\"partitionSubScanSpecList\") LinkedList<KafkaSubScanSpec> partitions)\n+  public KafkaSubScan(@JacksonInject StoragePluginRegistry registry,\n+                      @JsonProperty(\"userName\") String userName,\n+                      @JsonProperty(\"kafkaStoragePluginConfig\") KafkaStoragePluginConfig kafkaStoragePluginConfig,\n+                      @JsonProperty(\"columns\") List<SchemaPath> columns,\n+                      @JsonProperty(\"partitionSubScanSpecList\") LinkedList<KafkaSubScanSpec> partitionSubScanSpecList)\n       throws ExecutionSetupException {\n-    super(userName);\n-    this.KafkaStoragePluginConfig = kafkaStoragePluginConfig;\n-    this.columns = columns;\n-    this.partitions = partitions;\n-    this.kafkaStoragePlugin = (KafkaStoragePlugin) registry.getPlugin(kafkaStoragePluginConfig);\n+    this(userName,\n+        (KafkaStoragePlugin) registry.getPlugin(kafkaStoragePluginConfig),\n+        columns,\n+        partitionSubScanSpecList);\n   }\n \n-  public KafkaSubScan(String userName, KafkaStoragePlugin plugin, KafkaStoragePluginConfig kafkStoragePluginConfig,\n-      List<SchemaPath> columns, List<KafkaSubScanSpec> partitionSubScanSpecList) {\n+  public KafkaSubScan(String userName,\n+                      KafkaStoragePlugin kafkaStoragePlugin,\n+                      List<SchemaPath> columns,\n+                      List<KafkaSubScanSpec> partitionSubScanSpecList) {\n     super(userName);\n+    this.kafkaStoragePlugin = kafkaStoragePlugin;\n     this.columns = columns;\n-    this.KafkaStoragePluginConfig = kafkStoragePluginConfig;\n-    this.kafkaStoragePlugin = plugin;\n-    this.partitions = partitionSubScanSpecList;\n+    this.partitionSubScanSpecList = partitionSubScanSpecList;\n   }\n \n   @Override\n@@ -79,31 +76,32 @@ public KafkaSubScan(String userName, KafkaStoragePlugin plugin, KafkaStoragePlug\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) throws ExecutionSetupException {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, KafkaStoragePluginConfig, columns,\n-        partitions);\n+    return new KafkaSubScan(getUserName(), kafkaStoragePlugin, columns, partitionSubScanSpecList);\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n     return Collections.emptyIterator();\n   }\n \n-  @JsonIgnore\n+  @JsonProperty\n   public KafkaStoragePluginConfig getKafkaStoragePluginConfig() {\n-    return KafkaStoragePluginConfig;\n-  }\n-\n-  @JsonIgnore\n-  public KafkaStoragePlugin getKafkaStoragePlugin() {\n-    return kafkaStoragePlugin;\n+    return kafkaStoragePlugin.getConfig();\n   }\n \n+  @JsonProperty\n   public List<SchemaPath> getColumns() {\n     return columns;\n   }\n \n+  @JsonProperty\n   public List<KafkaSubScanSpec> getPartitionSubScanSpecList() {\n-    return partitions;\n+    return partitionSubScanSpecList;\n+  }\n+\n+  @JsonIgnore\n+  public KafkaStoragePlugin getKafkaStoragePlugin() {\n+    return kafkaStoragePlugin;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/main/java/org/apache/drill/exec/store/kafka/KafkaSubScan.java",
                "sha": "468f766a9685cdd010c68ea449b2d437357aeccc",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 8,
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.drill.exec.store.kafka;\n \n-import java.util.Arrays;\n import java.util.Collections;\n import java.util.Map;\n import java.util.Set;\n@@ -39,7 +38,7 @@\n \n   @Test\n   public void testSqlQueryOnInvalidTopic() throws Exception {\n-    String queryString = String.format(QueryConstants.MSG_SELECT_QUERY, QueryConstants.INVALID_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.INVALID_TOPIC);\n     try {\n       testBuilder().sqlQuery(queryString).unOrdered().baselineRecords(Collections.<Map<String, Object>> emptyList())\n           .build().run();\n@@ -51,7 +50,7 @@ public void testSqlQueryOnInvalidTopic() throws Exception {\n \n   @Test\n   public void testResultCount() throws Exception {\n-    String queryString = String.format(QueryConstants.MSG_SELECT_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.JSON_TOPIC);\n     runKafkaSQLVerifyCount(queryString, TestKafkaSuit.NUM_JSON_MSG);\n   }\n \n@@ -60,27 +59,27 @@ public void testPartitionMinOffset() throws Exception {\n     // following kafka.tools.GetOffsetShell for earliest as -2\n     Map<TopicPartition, Long> startOffsetsMap = fetchOffsets(-2);\n \n-    String queryString = String.format(QueryConstants.MIN_OFFSET_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MIN_OFFSET_QUERY, TestQueryConstants.JSON_TOPIC);\n     testBuilder().sqlQuery(queryString).unOrdered().baselineColumns(\"minOffset\")\n-        .baselineValues(startOffsetsMap.get(new TopicPartition(QueryConstants.JSON_TOPIC, 0))).go();\n+        .baselineValues(startOffsetsMap.get(new TopicPartition(TestQueryConstants.JSON_TOPIC, 0))).go();\n   }\n \n   @Test\n   public void testPartitionMaxOffset() throws Exception {\n     // following kafka.tools.GetOffsetShell for latest as -1\n     Map<TopicPartition, Long> endOffsetsMap = fetchOffsets(-1);\n \n-    String queryString = String.format(QueryConstants.MAX_OFFSET_QUERY, QueryConstants.JSON_TOPIC);\n+    String queryString = String.format(TestQueryConstants.MAX_OFFSET_QUERY, TestQueryConstants.JSON_TOPIC);\n     testBuilder().sqlQuery(queryString).unOrdered().baselineColumns(\"maxOffset\")\n-        .baselineValues(endOffsetsMap.get(new TopicPartition(QueryConstants.JSON_TOPIC, 0))-1).go();\n+        .baselineValues(endOffsetsMap.get(new TopicPartition(TestQueryConstants.JSON_TOPIC, 0))-1).go();\n   }\n \n   private Map<TopicPartition, Long> fetchOffsets(int flag) {\n     KafkaConsumer<byte[], byte[]> kafkaConsumer = new KafkaConsumer<>(storagePluginConfig.getKafkaConsumerProps(),\n         new ByteArrayDeserializer(), new ByteArrayDeserializer());\n \n     Map<TopicPartition, Long> offsetsMap = Maps.newHashMap();\n-    kafkaConsumer.subscribe(Arrays.asList(QueryConstants.JSON_TOPIC));\n+    kafkaConsumer.subscribe(Collections.singletonList(TestQueryConstants.JSON_TOPIC));\n     // based on KafkaConsumer JavaDoc, seekToBeginning/seekToEnd functions\n     // evaluates lazily, seeking to the\n     // first/last offset in all partitions only when poll(long) or\n@@ -110,4 +109,10 @@ public void testPartitionMaxOffset() throws Exception {\n     return offsetsMap;\n   }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = String.format(TestQueryConstants.MSG_SELECT_QUERY, TestQueryConstants.JSON_TOPIC);\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/KafkaQueriesTest.java",
                "sha": "ce9eb9984e3ad136a2fde21909121bcae05c2c0c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 1,
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "patch": "@@ -49,7 +49,7 @@ public void setUp() {\n     consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n     consumerProps.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"4\");\n     kafkaConsumer = new KafkaConsumer<>(consumerProps);\n-    subScanSpec = new KafkaSubScanSpec(QueryConstants.JSON_TOPIC, 0, 0, TestKafkaSuit.NUM_JSON_MSG);\n+    subScanSpec = new KafkaSubScanSpec(TestQueryConstants.JSON_TOPIC, 0, 0, TestKafkaSuit.NUM_JSON_MSG);\n   }\n \n   @After",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/MessageIteratorTest.java",
                "sha": "4a155963fa3f1548739b91203afbed69df54c1b8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 3,
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "patch": "@@ -72,15 +72,15 @@ public static void initKafka() throws Exception {\n         Properties topicProps = new Properties();\n         zkClient = new ZkClient(embeddedKafkaCluster.getZkServer().getConnectionString(), SESSION_TIMEOUT, CONN_TIMEOUT, ZKStringSerializer$.MODULE$);\n         ZkUtils zkUtils = new ZkUtils(zkClient, new ZkConnection(embeddedKafkaCluster.getZkServer().getConnectionString()), false);\n-        AdminUtils.createTopic(zkUtils, QueryConstants.JSON_TOPIC, 1, 1, topicProps, RackAwareMode.Disabled$.MODULE$);\n+        AdminUtils.createTopic(zkUtils, TestQueryConstants.JSON_TOPIC, 1, 1, topicProps, RackAwareMode.Disabled$.MODULE$);\n \n         org.apache.kafka.common.requests.MetadataResponse.TopicMetadata fetchTopicMetadataFromZk = AdminUtils\n-            .fetchTopicMetadataFromZk(QueryConstants.JSON_TOPIC, zkUtils);\n+            .fetchTopicMetadataFromZk(TestQueryConstants.JSON_TOPIC, zkUtils);\n         logger.info(\"Topic Metadata: \" + fetchTopicMetadataFromZk);\n \n         KafkaMessageGenerator generator = new KafkaMessageGenerator(embeddedKafkaCluster.getKafkaBrokerList(),\n             StringSerializer.class);\n-        generator.populateJsonMsgIntoKafka(QueryConstants.JSON_TOPIC, NUM_JSON_MSG);\n+        generator.populateJsonMsgIntoKafka(TestQueryConstants.JSON_TOPIC, NUM_JSON_MSG);\n       }\n       initCount.incrementAndGet();\n       runningSuite = true;",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestKafkaSuit.java",
                "sha": "ed0174755ad7833cbcb0bcf21990f481c5af189a",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 13,
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "patch": "@@ -17,24 +17,24 @@\n  */\n package org.apache.drill.exec.store.kafka;\n \n-public interface QueryConstants {\n+public interface TestQueryConstants {\n \n   // Kafka Server Prop Constants\n-  public static final String BROKER_DELIM = \",\";\n-  public final String LOCAL_HOST = \"127.0.0.1\";\n+  String BROKER_DELIM = \",\";\n+  String LOCAL_HOST = \"127.0.0.1\";\n \n   // ZK\n-  public final static String ZK_TMP = \"zk_tmp\";\n-  public final static int TICK_TIME = 500;\n-  public final static int MAX_CLIENT_CONNECTIONS = 100;\n+  String ZK_TMP = \"zk_tmp\";\n+  int TICK_TIME = 500;\n+  int MAX_CLIENT_CONNECTIONS = 100;\n \n-  public static final String JSON_TOPIC = \"drill-json-topic\";\n-  public static final String AVRO_TOPIC = \"drill-avro-topic\";\n-  public static final String INVALID_TOPIC = \"invalid-topic\";\n+  String JSON_TOPIC = \"drill-json-topic\";\n+  String AVRO_TOPIC = \"drill-avro-topic\";\n+  String INVALID_TOPIC = \"invalid-topic\";\n \n   // Queries\n-  public static final String MSG_COUNT_QUERY = \"select count(*) from kafka.`%s`\";\n-  public static final String MSG_SELECT_QUERY = \"select * from kafka.`%s`\";\n-  public static final String MIN_OFFSET_QUERY = \"select MIN(kafkaMsgOffset) as minOffset from kafka.`%s`\";\n-  public static final String MAX_OFFSET_QUERY = \"select MAX(kafkaMsgOffset) as maxOffset from kafka.`%s`\";\n+  String MSG_COUNT_QUERY = \"select count(*) from kafka.`%s`\";\n+  String MSG_SELECT_QUERY = \"select * from kafka.`%s`\";\n+  String MIN_OFFSET_QUERY = \"select MIN(kafkaMsgOffset) as minOffset from kafka.`%s`\";\n+  String MAX_OFFSET_QUERY = \"select MAX(kafkaMsgOffset) as maxOffset from kafka.`%s`\";\n }",
                "previous_filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/QueryConstants.java",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/TestQueryConstants.java",
                "sha": "057af7eb9bcbf971afa909461c1495aff3922a1b",
                "status": "renamed"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 2,
                "filename": "contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "patch": "@@ -26,7 +26,7 @@\n \n import org.apache.drill.exec.ZookeeperHelper;\n import org.apache.drill.exec.store.kafka.KafkaStoragePluginConfig;\n-import org.apache.drill.exec.store.kafka.QueryConstants;\n+import org.apache.drill.exec.store.kafka.TestQueryConstants;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.slf4j.Logger;\n@@ -35,7 +35,7 @@\n import kafka.server.KafkaConfig;\n import kafka.server.KafkaServerStartable;\n \n-public class EmbeddedKafkaCluster implements QueryConstants {\n+public class EmbeddedKafkaCluster implements TestQueryConstants {\n   private static final Logger logger = LoggerFactory.getLogger(EmbeddedKafkaCluster.class);\n   private List<KafkaServerStartable> brokers;\n   private final ZookeeperHelper zkHelper;",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kafka/src/test/java/org/apache/drill/exec/store/kafka/cluster/EmbeddedKafkaCluster.java",
                "sha": "663e0e47a86aa513e8ece2aafcd915b7533bdd86",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 21,
                "filename": "contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -19,7 +19,6 @@\n \n import java.io.IOException;\n import java.util.Collection;\n-import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n \n@@ -45,7 +44,6 @@\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.ImmutableList;\n import org.apache.drill.exec.store.schedule.AffinityCreator;\n import org.apache.drill.exec.store.schedule.AssignmentCreator;\n import org.apache.drill.exec.store.schedule.CompleteWork;\n@@ -59,10 +57,10 @@\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(KuduGroupScan.class);\n   private static final long DEFAULT_TABLET_SIZE = 1000;\n \n-  private KuduStoragePluginConfig storagePluginConfig;\n+  private KuduStoragePlugin kuduStoragePlugin;\n   private List<SchemaPath> columns;\n   private KuduScanSpec kuduScanSpec;\n-  private KuduStoragePlugin storagePlugin;\n+\n   private boolean filterPushedDown = false;\n   private List<KuduWork> kuduWorkList = Lists.newArrayList();\n   private ListMultimap<Integer,KuduWork> assignments;\n@@ -71,31 +69,31 @@\n \n   @JsonCreator\n   public KuduGroupScan(@JsonProperty(\"kuduScanSpec\") KuduScanSpec kuduScanSpec,\n-                        @JsonProperty(\"storage\") KuduStoragePluginConfig storagePluginConfig,\n+                        @JsonProperty(\"kuduStoragePluginConfig\") KuduStoragePluginConfig kuduStoragePluginConfig,\n                         @JsonProperty(\"columns\") List<SchemaPath> columns,\n                         @JacksonInject StoragePluginRegistry pluginRegistry) throws IOException, ExecutionSetupException {\n-    this((KuduStoragePlugin) pluginRegistry.getPlugin(storagePluginConfig), kuduScanSpec, columns);\n+    this((KuduStoragePlugin) pluginRegistry.getPlugin(kuduStoragePluginConfig), kuduScanSpec, columns);\n   }\n \n-  public KuduGroupScan(KuduStoragePlugin storagePlugin, KuduScanSpec scanSpec,\n-      List<SchemaPath> columns) {\n+  public KuduGroupScan(KuduStoragePlugin kuduStoragePlugin,\n+                       KuduScanSpec kuduScanSpec,\n+                       List<SchemaPath> columns) {\n     super((String) null);\n-    this.storagePlugin = storagePlugin;\n-    this.storagePluginConfig = storagePlugin.getConfig();\n-    this.kuduScanSpec = scanSpec;\n+    this.kuduStoragePlugin = kuduStoragePlugin;\n+    this.kuduScanSpec = kuduScanSpec;\n     this.columns = columns == null || columns.size() == 0? ALL_COLUMNS : columns;\n     init();\n   }\n \n   private void init() {\n     String tableName = kuduScanSpec.getTableName();\n-    Collection<DrillbitEndpoint> endpoints = storagePlugin.getContext().getBits();\n+    Collection<DrillbitEndpoint> endpoints = kuduStoragePlugin.getContext().getBits();\n     Map<String,DrillbitEndpoint> endpointMap = Maps.newHashMap();\n     for (DrillbitEndpoint endpoint : endpoints) {\n       endpointMap.put(endpoint.getAddress(), endpoint);\n     }\n     try {\n-      List<LocatedTablet> locations = storagePlugin.getClient().openTable(tableName).getTabletsLocations(10000);\n+      List<LocatedTablet> locations = kuduStoragePlugin.getClient().openTable(tableName).getTabletsLocations(10000);\n       for (LocatedTablet tablet : locations) {\n         KuduWork work = new KuduWork(tablet.getPartition().getPartitionKeyStart(), tablet.getPartition().getPartitionKeyEnd());\n         for (Replica replica : tablet.getReplicas()) {\n@@ -153,10 +151,9 @@ public int compareTo(CompleteWork o) {\n    */\n   private KuduGroupScan(KuduGroupScan that) {\n     super(that);\n+    this.kuduStoragePlugin = that.kuduStoragePlugin;\n     this.columns = that.columns;\n     this.kuduScanSpec = that.kuduScanSpec;\n-    this.storagePlugin = that.storagePlugin;\n-    this.storagePluginConfig = that.storagePluginConfig;\n     this.filterPushedDown = that.filterPushedDown;\n     this.kuduWorkList = that.kuduWorkList;\n     this.assignments = that.assignments;\n@@ -204,7 +201,7 @@ public KuduSubScan getSpecificScan(int minorFragmentId) {\n       scanSpecList.add(new KuduSubScanSpec(getTableName(), work.getPartitionKeyStart(), work.getPartitionKeyEnd()));\n     }\n \n-    return new KuduSubScan(storagePlugin, storagePluginConfig, scanSpecList, this.columns);\n+    return new KuduSubScan(kuduStoragePlugin, scanSpecList, this.columns);\n   }\n \n   // KuduStoragePlugin plugin, KuduStoragePluginConfig config,\n@@ -224,7 +221,7 @@ public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n \n   @JsonIgnore\n   public KuduStoragePlugin getStoragePlugin() {\n-    return storagePlugin;\n+    return kuduStoragePlugin;\n   }\n \n   @JsonIgnore\n@@ -244,9 +241,9 @@ public String toString() {\n         + columns + \"]\";\n   }\n \n-  @JsonProperty(\"storage\")\n-  public KuduStoragePluginConfig getStorageConfig() {\n-    return this.storagePluginConfig;\n+  @JsonProperty\n+  public KuduStoragePluginConfig getKuduStoragePluginConfig() {\n+    return kuduStoragePlugin.getConfig();\n   }\n \n   @JsonProperty",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduGroupScan.java",
                "sha": "7bddf18617e8360fd468033d7ec61ba967c09041",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 24,
                "filename": "contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -21,9 +21,9 @@\n import java.util.LinkedList;\n import java.util.List;\n \n+import com.google.common.collect.ImmutableSet;\n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractBase;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n import org.apache.drill.exec.physical.base.PhysicalVisitor;\n@@ -37,49 +37,40 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.Iterators;\n \n // Class containing information for reading a single Kudu tablet\n-@JsonTypeName(\"kudu-tablet-scan\")\n+@JsonTypeName(\"kudu-sub-scan\")\n public class KuduSubScan extends AbstractBase implements SubScan {\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(KuduSubScan.class);\n \n-  @JsonProperty\n-  public final KuduStoragePluginConfig storage;\n-\n-\n   private final KuduStoragePlugin kuduStoragePlugin;\n   private final List<KuduSubScanSpec> tabletScanSpecList;\n   private final List<SchemaPath> columns;\n \n   @JsonCreator\n   public KuduSubScan(@JacksonInject StoragePluginRegistry registry,\n-                      @JsonProperty(\"storage\") StoragePluginConfig storage,\n-      @JsonProperty(\"tabletScanSpecList\") LinkedList<KuduSubScanSpec> tabletScanSpecList,\n+                      @JsonProperty(\"kuduStoragePluginConfig\") KuduStoragePluginConfig kuduStoragePluginConfig,\n+                      @JsonProperty(\"tabletScanSpecList\") LinkedList<KuduSubScanSpec> tabletScanSpecList,\n                       @JsonProperty(\"columns\") List<SchemaPath> columns) throws ExecutionSetupException {\n     super((String) null);\n-    kuduStoragePlugin = (KuduStoragePlugin) registry.getPlugin(storage);\n+    kuduStoragePlugin = (KuduStoragePlugin) registry.getPlugin(kuduStoragePluginConfig);\n     this.tabletScanSpecList = tabletScanSpecList;\n-    this.storage = (KuduStoragePluginConfig) storage;\n     this.columns = columns;\n   }\n \n-  public KuduSubScan(KuduStoragePlugin plugin, KuduStoragePluginConfig config,\n-      List<KuduSubScanSpec> tabletInfoList, List<SchemaPath> columns) {\n+  public KuduSubScan(KuduStoragePlugin plugin, List<KuduSubScanSpec> tabletInfoList, List<SchemaPath> columns) {\n     super((String) null);\n-    kuduStoragePlugin = plugin;\n-    storage = config;\n+    this.kuduStoragePlugin = plugin;\n     this.tabletScanSpecList = tabletInfoList;\n     this.columns = columns;\n   }\n \n-  public List<KuduSubScanSpec> getTabletScanSpecList() {\n-    return tabletScanSpecList;\n+  public KuduStoragePluginConfig getKuduStoragePluginConfig() {\n+    return kuduStoragePlugin.getConfig();\n   }\n \n-  @JsonIgnore\n-  public KuduStoragePluginConfig getStorageConfig() {\n-    return storage;\n+  public List<KuduSubScanSpec> getTabletScanSpecList() {\n+    return tabletScanSpecList;\n   }\n \n   public List<SchemaPath> getColumns() {\n@@ -104,12 +95,12 @@ public KuduStoragePlugin getStorageEngine(){\n   @Override\n   public PhysicalOperator getNewWithChildren(List<PhysicalOperator> children) {\n     Preconditions.checkArgument(children.isEmpty());\n-    return new KuduSubScan(kuduStoragePlugin, storage, tabletScanSpecList, columns);\n+    return new KuduSubScan(kuduStoragePlugin, tabletScanSpecList, columns);\n   }\n \n   @Override\n   public Iterator<PhysicalOperator> iterator() {\n-    return Iterators.emptyIterator();\n+    return ImmutableSet.<PhysicalOperator>of().iterator();\n   }\n \n   public static class KuduSubScanSpec {\n@@ -143,7 +134,7 @@ public String getTableName() {\n \n   @Override\n   public int getOperatorType() {\n-    return CoreOperatorType.HBASE_SUB_SCAN_VALUE;\n+    return CoreOperatorType.KUDU_SUB_SCAN_VALUE;\n   }\n \n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/main/java/org/apache/drill/exec/store/kudu/KuduSubScan.java",
                "sha": "ca577e753f265fbad1f31eb98ba58b33d7f4ecfb",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 1,
                "filename": "contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -17,6 +17,7 @@\n  */\n package org.apache.drill.store.kudu;\n \n+import org.apache.drill.PlanTestBase;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.categories.KuduStorageTest;\n import org.junit.Ignore;\n@@ -44,6 +45,11 @@ public void testCreate() throws Exception {\n     test(\"create table kudu.regions as select 1, * from sys.options limit 1\");\n     test(\"select * from kudu.regions\");\n     test(\"drop table kudu.regions\");\n+  }\n \n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(\"select * from kudu.demo\");\n   }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-kudu/src/test/java/org/apache/drill/store/kudu/TestKuduPlugin.java",
                "sha": "4e1c7fd5f745b8833e911d39744e1756160c1be7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 0,
                "filename": "contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "patch": "@@ -82,4 +82,11 @@ public void testUnShardedDBInShardedClusterWithGroupByProjectionAndFilter() thro\n         DONUTS_DB, DONUTS_COLLECTION);\n     runMongoSQLVerifyCount(queryString, 5);\n   }\n+\n+  @Test\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = String.format(TEST_BOOLEAN_FILTER_QUERY_TEMPLATE1,\n+        EMPLOYEE_DB, EMPINFO_COLLECTION);\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-mongo/src/test/java/org/apache/drill/exec/store/mongo/TestMongoQueries.java",
                "sha": "8d0064f2f948d1909c01c0e54773c68e5fd10998",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 4,
                "filename": "contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "patch": "@@ -174,10 +174,9 @@ public void testBasicQueryWithNonExistentTableName() throws Exception {\n   }\n \n   @Test\n-  public void testPhysicalPlanExecutionBasedOnQuery() throws Exception {\n-    String query = \"EXPLAIN PLAN for select * from openTSDB.`(metric=warp.speed.test, start=47y-ago, aggregator=sum)`\";\n-    String plan = getPlanInString(query, JSON_FORMAT);\n-    Assert.assertEquals(18, testPhysical(plan));\n+  public void testPhysicalPlanSubmission() throws Exception {\n+    String query = \"select * from openTSDB.`(metric=warp.speed.test, start=47y-ago, aggregator=sum)`\";\n+    testPhysicalPlanExecutionBasedOnQuery(query);\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/contrib/storage-opentsdb/src/test/java/org/apache/drill/store/openTSDB/TestOpenTSDBPlugin.java",
                "sha": "27ca09c61aeaa539c4c84eee2204786a2cd5dd9d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 6,
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "patch": "@@ -521,6 +521,10 @@ private FragmentState(int index, int value) {\n      * <code>KAFKA_SUB_SCAN = 38;</code>\n      */\n     KAFKA_SUB_SCAN(38, 38),\n+    /**\n+     * <code>KUDU_SUB_SCAN = 39;</code>\n+     */\n+    KUDU_SUB_SCAN(39, 39),\n     ;\n \n     /**\n@@ -679,6 +683,10 @@ private FragmentState(int index, int value) {\n      * <code>KAFKA_SUB_SCAN = 38;</code>\n      */\n     public static final int KAFKA_SUB_SCAN_VALUE = 38;\n+    /**\n+     * <code>KUDU_SUB_SCAN = 39;</code>\n+     */\n+    public static final int KUDU_SUB_SCAN_VALUE = 39;\n \n \n     public final int getNumber() { return value; }\n@@ -724,6 +732,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 36: return AVRO_SUB_SCAN;\n         case 37: return PCAP_SUB_SCAN;\n         case 38: return KAFKA_SUB_SCAN;\n+        case 39: return KUDU_SUB_SCAN;\n         default: return null;\n       }\n     }\n@@ -24104,7 +24113,7 @@ public Builder clearStatus() {\n       \"agmentState\\022\\013\\n\\007SENDING\\020\\000\\022\\027\\n\\023AWAITING_ALL\" +\n       \"OCATION\\020\\001\\022\\013\\n\\007RUNNING\\020\\002\\022\\014\\n\\010FINISHED\\020\\003\\022\\r\\n\\t\" +\n       \"CANCELLED\\020\\004\\022\\n\\n\\006FAILED\\020\\005\\022\\032\\n\\026CANCELLATION_\" +\n-      \"REQUESTED\\020\\006*\\204\\006\\n\\020CoreOperatorType\\022\\021\\n\\rSING\" +\n+      \"REQUESTED\\020\\006*\\227\\006\\n\\020CoreOperatorType\\022\\021\\n\\rSING\" +\n       \"LE_SENDER\\020\\000\\022\\024\\n\\020BROADCAST_SENDER\\020\\001\\022\\n\\n\\006FIL\" +\n       \"TER\\020\\002\\022\\022\\n\\016HASH_AGGREGATE\\020\\003\\022\\r\\n\\tHASH_JOIN\\020\\004\" +\n       \"\\022\\016\\n\\nMERGE_JOIN\\020\\005\\022\\031\\n\\025HASH_PARTITION_SENDE\" +\n@@ -24123,11 +24132,11 @@ public Builder clearStatus() {\n       \"X_TO_JSON\\020\\037\\022\\025\\n\\021PRODUCER_CONSUMER\\020 \\022\\022\\n\\016HB\",\n       \"ASE_SUB_SCAN\\020!\\022\\n\\n\\006WINDOW\\020\\\"\\022\\024\\n\\020NESTED_LOO\" +\n       \"P_JOIN\\020#\\022\\021\\n\\rAVRO_SUB_SCAN\\020$\\022\\021\\n\\rPCAP_SUB_\" +\n-      \"SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCAN\\020&*g\\n\\nSaslStatus\" +\n-      \"\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\\nSASL_START\\020\\001\\022\\024\\n\\020SA\" +\n-      \"SL_IN_PROGRESS\\020\\002\\022\\020\\n\\014SASL_SUCCESS\\020\\003\\022\\017\\n\\013SA\" +\n-      \"SL_FAILED\\020\\004B.\\n\\033org.apache.drill.exec.pro\" +\n-      \"toB\\rUserBitSharedH\\001\"\n+      \"SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCAN\\020&\\022\\021\\n\\rKUDU_SUB_S\" +\n+      \"CAN\\020\\'*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\" +\n+      \"\\nSASL_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRESS\\020\\002\\022\\020\\n\\014S\" +\n+      \"ASL_SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B.\\n\\033org.ap\" +\n+      \"ache.drill.exec.protoB\\rUserBitSharedH\\001\"\n     };\n     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =\n       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "sha": "6ae9deb4cc878376071a7ae0570136b788e42c0f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 1,
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "patch": "@@ -60,7 +60,8 @@\n     NESTED_LOOP_JOIN(35),\n     AVRO_SUB_SCAN(36),\n     PCAP_SUB_SCAN(37),\n-    KAFKA_SUB_SCAN(38);\n+    KAFKA_SUB_SCAN(38),\n+    KUDU_SUB_SCAN(39);\n     \n     public final int number;\n     \n@@ -117,6 +118,7 @@ public static CoreOperatorType valueOf(int number)\n             case 36: return AVRO_SUB_SCAN;\n             case 37: return PCAP_SUB_SCAN;\n             case 38: return KAFKA_SUB_SCAN;\n+            case 39: return KUDU_SUB_SCAN;\n             default: return null;\n         }\n     }",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "sha": "71595f72e99c06cc76733c14cb46d77308d832d2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/protobuf/UserBitShared.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/protobuf/UserBitShared.proto?ref=58e4cec9a913e381ef0e96b072c31e34085277b3",
                "deletions": 0,
                "filename": "protocol/src/main/protobuf/UserBitShared.proto",
                "patch": "@@ -326,6 +326,7 @@ enum CoreOperatorType {\n   AVRO_SUB_SCAN = 36;\n   PCAP_SUB_SCAN = 37;\n   KAFKA_SUB_SCAN = 38;\n+  KUDU_SUB_SCAN = 39;\n }\n \n /* Registry that contains list of jars, each jar contains its name and list of function signatures.",
                "raw_url": "https://github.com/apache/drill/raw/58e4cec9a913e381ef0e96b072c31e34085277b3/protocol/src/main/protobuf/UserBitShared.proto",
                "sha": "b13f0591894a9e49a238c2cdbdd9fb7298da9c18",
                "status": "modified"
            }
        ],
        "message": "DRILL-6130: Fix NPE during physical plan submission for various storage plugins\n\n1. Fixed ser / de issues for Hive, Kafka, Hbase plugins.\n2. Added physical plan submission unit test for all storage plugins in contrib module.\n3. Refactoring.\n\ncloses #1108",
        "parent": "https://github.com/apache/drill/commit/58f3b10464f5a3b969ce19705750cb163be9b287",
        "patched_files": [
            "HiveDrillNativeParquetSubScan.java",
            "MessageIterator.java",
            "EmbeddedKafkaCluster.java",
            "HivePartitionDescriptor.java",
            "HiveScan.java",
            "KafkaSubScan.java",
            "KuduSubScan.java",
            "CoreOperatorType.java",
            "HBaseSubScan.java",
            "UserBitShared.proto",
            "ConvertHiveParquetScanToDrillParquetScan.java",
            "HiveDrillNativeParquetScan.java",
            "KafkaGroupScan.java",
            "HiveSubScan.java",
            "UserBitShared.java",
            "KuduGroupScan.java",
            "HBaseGroupScan.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestKafkaSuit.java",
            "TestJdbcPluginWithMySQLIT.java",
            "KafkaQueriesTest.java",
            "TestHBaseQueries.java",
            "TestQueryConstants.java",
            "TestKuduPlugin.java",
            "TestOpenTSDBPlugin.java",
            "MessageIteratorTest.java",
            "TestHiveStorage.java",
            "TestMongoQueries.java"
        ]
    },
    "drill_624634d": {
        "bug_id": "drill_624634d",
        "commit": "https://github.com/apache/drill/commit/624634d88ab870e3edb1648743097242522e58d6",
        "file": [
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/drill/blob/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java?ref=624634d88ab870e3edb1648743097242522e58d6",
                "deletions": 39,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "patch": "@@ -129,21 +129,6 @@\n \n   private File tmpJavaScriptDir;\n \n-  public File getTmpJavaScriptDir() {\n-    if (tmpJavaScriptDir == null) {\n-      tmpJavaScriptDir = org.apache.drill.shaded.guava.com.google.common.io.Files.createTempDir();\n-      tmpJavaScriptDir.deleteOnExit();\n-      //Perform All auto generated files at this point\n-      try {\n-        generateOptionsDescriptionJSFile();\n-        generateFunctionJS();\n-      } catch (IOException e) {\n-        logger.error(\"Unable to create temp dir for JavaScripts. {}\", e);\n-      }\n-    }\n-    return tmpJavaScriptDir;\n-  }\n-\n   /**\n    * Create Jetty based web server.\n    *\n@@ -246,8 +231,8 @@ private ServletContextHandler createServletContextHandler(final boolean authEnab\n     //Add Local path resource (This will allow access to dynamically created files like JavaScript)\n     final ServletHolder dynamicHolder = new ServletHolder(\"dynamic\", DefaultServlet.class);\n     //Skip if unable to get a temp directory (e.g. during Unit tests)\n-    if (getTmpJavaScriptDir() != null) {\n-      dynamicHolder.setInitParameter(\"resourceBase\", getTmpJavaScriptDir().getAbsolutePath());\n+    if (getOrCreateTmpJavaScriptDir() != null) {\n+      dynamicHolder.setInitParameter(\"resourceBase\", getOrCreateTmpJavaScriptDir().getAbsolutePath());\n       dynamicHolder.setInitParameter(\"dirAllowed\", \"true\");\n       dynamicHolder.setInitParameter(\"pathInfoOnly\", \"true\");\n       servletContextHandler.addServlet(dynamicHolder, \"/dynamic/*\");\n@@ -467,43 +452,61 @@ public void close() throws Exception {\n     if (embeddedJetty != null) {\n       embeddedJetty.stop();\n     }\n-    //Deleting temp directory\n-    FileUtils.deleteDirectory(getTmpJavaScriptDir());\n+    // Deleting temp directory\n+    FileUtils.deleteQuietly(tmpJavaScriptDir);\n   }\n \n+  /**\n+   * Creates if not exists, and returns File for temporary Javascript directory\n+   * @return File handle\n+   */\n+  public File getOrCreateTmpJavaScriptDir() {\n+    if (tmpJavaScriptDir == null && this.drillbit.getContext() != null) {\n+      tmpJavaScriptDir = org.apache.drill.shaded.guava.com.google.common.io.Files.createTempDir();\n+      // Perform All auto generated files at this point\n+      try {\n+        generateOptionsDescriptionJSFile();\n+        generateFunctionJS();\n+      } catch (IOException e) {\n+        logger.error(\"Unable to create temp dir for JavaScripts. {}\", e);\n+      }\n+    }\n+    return tmpJavaScriptDir;\n+  }\n+\n+\n   /**\n    * Generate Options Description JavaScript to serve http://drillhost/options ACE library search features\n    * @throws IOException\n    */\n   private void generateOptionsDescriptionJSFile() throws IOException {\n-    //Obtain list of Options & their descriptions\n+    // Obtain list of Options & their descriptions\n     OptionManager optionManager = this.drillbit.getContext().getOptionManager();\n     OptionList publicOptions = optionManager.getPublicOptionList();\n     List<OptionValue> options = new ArrayList<>(publicOptions);\n-    //Add internal options\n+    // Add internal options\n     OptionList internalOptions = optionManager.getInternalOptionList();\n     options.addAll(internalOptions);\n     Collections.sort(options);\n     int numLeftToWrite = options.size();\n \n-    //Template source Javascript file\n+    // Template source Javascript file\n     InputStream optionsDescripTemplateStream = Resource.newClassPathResource(OPTIONS_DESCRIBE_TEMPLATE_JS).getInputStream();\n-    //Generated file\n-    File optionsDescriptionFile = new File(getTmpJavaScriptDir(), OPTIONS_DESCRIBE_JS);\n+    // Generated file\n+    File optionsDescriptionFile = new File(getOrCreateTmpJavaScriptDir(), OPTIONS_DESCRIBE_JS);\n     final String file_content_footer = \"};\";\n-    optionsDescriptionFile.deleteOnExit();\n-    //Create a copy of a template and write with that!\n+    // Create a copy of a template and write with that!\n     java.nio.file.Files.copy(optionsDescripTemplateStream, optionsDescriptionFile.toPath());\n     logger.info(\"Will write {} descriptions to {}\", numLeftToWrite, optionsDescriptionFile.getAbsolutePath());\n \n     try (BufferedWriter writer = new BufferedWriter(new FileWriter(optionsDescriptionFile, true))) {\n-      //Iterate through options\n+      // Iterate through options\n       for (OptionValue option : options) {\n         numLeftToWrite--;\n         String optionName = option.getName();\n         OptionDescription optionDescription = optionManager.getOptionDefinition(optionName).getValidator().getOptionDescription();\n         if (optionDescription != null) {\n-          //Note: We don't need to worry about short descriptions for WebUI, since they will never be explicitly accessed from the map\n+          // Note: We don't need to worry about short descriptions for WebUI, since they will never be explicitly accessed from the map\n           writer.append(\"  \\\"\").append(optionName).append(\"\\\" : \\\"\")\n           .append(StringEscapeUtils.escapeEcmaScript(optionDescription.getDescription()))\n           .append( numLeftToWrite > 0 ? \"\\\",\" : \"\\\"\");\n@@ -521,14 +524,14 @@ private void generateOptionsDescriptionJSFile() throws IOException {\n    * @throws IOException\n    */\n   private void generateFunctionJS() throws IOException {\n-    //Naturally ordered set of function names\n+    // Naturally ordered set of function names\n     TreeSet<String> functionSet = new TreeSet<>();\n-    //Extracting ONLY builtIn functions (i.e those already available)\n+    // Extracting ONLY builtIn functions (i.e those already available)\n     List<FunctionHolder> builtInFuncHolderList = this.drillbit.getContext().getFunctionImplementationRegistry().getLocalFunctionRegistry()\n         .getAllJarsWithFunctionsHolders().get(LocalFunctionRegistry.BUILT_IN);\n \n-    //Build List of 'usable' functions (i.e. functions that start with an alphabet and can be autocompleted by the ACE library)\n-    //Example of 'unusable' functions would be operators like '<', '!'\n+    // Build List of 'usable' functions (i.e. functions that start with an alphabet and can be autocompleted by the ACE library)\n+    // Example of 'unusable' functions would be operators like '<', '!'\n     int skipCount = 0;\n     for (FunctionHolder builtInFunctionHolder : builtInFuncHolderList) {\n       String name = builtInFunctionHolder.getName();\n@@ -541,22 +544,21 @@ private void generateFunctionJS() throws IOException {\n     }\n     logger.debug(\"{} functions will not be available in WebUI\", skipCount);\n \n-    //Generated file\n-    File functionsListFile = new File(getTmpJavaScriptDir(), ACE_MODE_SQL_JS);\n-    functionsListFile.deleteOnExit();\n-    //Template source Javascript file\n+    // Generated file\n+    File functionsListFile = new File(getOrCreateTmpJavaScriptDir(), ACE_MODE_SQL_JS);\n+    // Template source Javascript file\n     try (InputStream aceModeSqlTemplateStream = Resource.newClassPathResource(ACE_MODE_SQL_TEMPLATE_JS).getInputStream()) {\n-      //Create a copy of a template and write with that!\n+      // Create a copy of a template and write with that!\n       java.nio.file.Files.copy(aceModeSqlTemplateStream, functionsListFile.toPath());\n     }\n \n-    //Construct String\n+    // Construct String\n     String funcListString = String.join(\"|\", functionSet);\n \n     Path path = Paths.get(functionsListFile.getPath());\n     try (Stream<String> lines = Files.lines(path)) {\n       List <String> replaced =\n-          lines //Replacing first occurrence\n+          lines // Replacing first occurrence\n             .map(line -> line.replaceFirst(DRILL_FUNCTIONS_PLACEHOLDER, funcListString))\n             .collect(Collectors.toList());\n       Files.write(path, replaced);",
                "raw_url": "https://github.com/apache/drill/raw/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebServer.java",
                "sha": "cdde4aa3ea9dc880d0e5beee78fc0829191741f6",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java?ref=624634d88ab870e3edb1648743097242522e58d6",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "patch": "@@ -17,11 +17,13 @@\n  */\n package org.apache.drill.test;\n \n+import org.apache.commons.lang3.reflect.FieldUtils;\n import org.apache.drill.categories.SlowTest;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n import org.apache.drill.exec.server.Drillbit;\n+import org.apache.drill.exec.server.rest.WebServer;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n@@ -30,17 +32,21 @@\n import org.junit.rules.TestRule;\n \n import java.io.BufferedWriter;\n+import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n import java.io.PrintWriter;\n+import java.lang.reflect.Field;\n import java.net.HttpURLConnection;\n import java.net.URL;\n import java.nio.file.Path;\n import java.util.Collection;\n \n import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n @Category({SlowTest.class})\n@@ -205,6 +211,42 @@ public void testDrillbitWithSamePortContainsShutdownThread() throws Exception {\n     }\n   }\n \n+  @Test // DRILL-7056\n+  public void testDrillbitTempDir() throws Exception {\n+    File originalDrillbitTempDir = null;\n+    ClusterFixtureBuilder fixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher).withLocalZk()\n+        .configProperty(ExecConstants.ALLOW_LOOPBACK_ADDRESS_BINDING, true)\n+        .configProperty(ExecConstants.INITIAL_USER_PORT, QueryTestUtil.getFreePortNumber(31170, 300))\n+        .configProperty(ExecConstants.INITIAL_BIT_PORT, QueryTestUtil.getFreePortNumber(31180, 300));\n+\n+    try (ClusterFixture fixture = fixtureBuilder.build();\n+        Drillbit twinDrillbitOnSamePort = new Drillbit(fixture.config(),\n+            fixtureBuilder.configBuilder().getDefinitions(), fixture.serviceSet())) {\n+      // Assert preconditions :\n+      //      1. First drillbit instance should be started normally\n+      //      2. Second instance startup should fail, because ports are occupied by the first one\n+      Drillbit originalDrillbit = fixture.drillbit();\n+      assertNotNull(\"First drillbit instance should be initialized\", originalDrillbit);\n+      originalDrillbitTempDir = getWebServerTempDirPath(originalDrillbit);\n+      assertTrue(\"First drillbit instance should have a temporary Javascript dir initialized\", originalDrillbitTempDir.exists());\n+      try {\n+        twinDrillbitOnSamePort.run();\n+        fail(\"Invocation of 'twinDrillbitOnSamePort.run()' should throw UserException\");\n+      } catch (UserException userEx) {\n+        assertThat(userEx.getMessage(), containsString(\"RESOURCE ERROR: Drillbit could not bind to port\"));\n+      }\n+    }\n+    // Verify deletion\n+    assertFalse(\"First drillbit instance should have a temporary Javascript dir deleted\", originalDrillbitTempDir.exists());\n+  }\n+\n+  private static File getWebServerTempDirPath(Drillbit drillbit) throws IllegalAccessException {\n+    Field webServerField = FieldUtils.getField(drillbit.getClass(), \"webServer\", true);\n+    WebServer webServerHandle = (WebServer) FieldUtils.readField(webServerField, drillbit, true);\n+    File webServerTempDirPath = webServerHandle.getOrCreateTmpJavaScriptDir();\n+    return webServerTempDirPath;\n+  }\n+\n   private static boolean waitAndAssertDrillbitCount(ClusterFixture cluster, int zkRefresh) throws InterruptedException {\n \n     while (true) {",
                "raw_url": "https://github.com/apache/drill/raw/624634d88ab870e3edb1648743097242522e58d6/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "sha": "d74f1e691f9f3bcb506612c5c5f291a27d23cd58",
                "status": "modified"
            }
        ],
        "message": "DRILL-7056: Drill fails with NPE when starting in distributed mode & 31010 port is used\ncloses #1656",
        "parent": "https://github.com/apache/drill/commit/5abcd88642e224beb8252185f938a5e42387b18e",
        "patched_files": [
            "WebServer.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestGracefulShutdown.java"
        ]
    },
    "drill_71608ca": {
        "bug_id": "drill_71608ca",
        "commit": "https://github.com/apache/drill/commit/71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "patch": "@@ -150,7 +150,11 @@ public boolean apply(@Nullable FileStatus status) {\n     logger.debug(\"FileSelection.minusDirectories() took {} ms, numFiles: {}\",\n         timer.elapsed(TimeUnit.MILLISECONDS), total);\n \n-    fileSel.setExpanded();\n+    // fileSel will be null if we query an empty folder\n+    if (fileSel != null) {\n+      fileSel.setExpanded();\n+    }\n+\n     return fileSel;\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "sha": "5b4813ab6c4e9b852237a5552522de064f93c012",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.Set;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.logical.StoragePluginConfig;\n@@ -568,6 +569,7 @@ public long getRowCount() {\n    * @return file selection read from cache\n    *\n    * @throws IOException\n+   * @throws UserException when the updated selection is empty, this happens if the user selects an empty folder.\n    */\n   private FileSelection\n   initFromMetadataCache(FileSelection selection, Path metaFilePath) throws IOException {\n@@ -580,7 +582,8 @@ public long getRowCount() {\n     List<String> fileNames = Lists.newArrayList();\n     List<FileStatus> fileStatuses = selection.getStatuses(fs);\n \n-    if (fileStatuses.size() == 1 && fileStatuses.get(0).isDirectory()) {\n+    final Path first = fileStatuses.get(0).getPath();\n+    if (fileStatuses.size() == 1 && selection.getSelectionRoot().equals(first.toString())) {\n       // we are selecting all files from selection root. Expand the file list from the cache\n       for (Metadata.ParquetFileMetadata file : parquetTableMetadata.getFiles()) {\n         fileNames.add(file.getPath());\n@@ -590,7 +593,7 @@ public long getRowCount() {\n       // we need to expand the files from fileStatuses\n       for (FileStatus status : fileStatuses) {\n         if (status.isDirectory()) {\n-          //TODO read the metadata cache files in parallel\n+          //TODO [DRILL-4496] read the metadata cache files in parallel\n           final Path metaPath = new Path(status.getPath(), Metadata.METADATA_FILENAME);\n           final Metadata.ParquetTableMetadataBase metadata = Metadata.readBlockMeta(fs, metaPath.toString());\n           for (Metadata.ParquetFileMetadata file : metadata.getFiles()) {\n@@ -606,6 +609,11 @@ public long getRowCount() {\n       fileSet = Sets.newHashSet(fileNames);\n     }\n \n+    if (fileNames.isEmpty()) {\n+      // no files were found, most likely we tried to query some empty sub folders\n+      throw UserException.validationError().message(\"The table you tried to query is empty\").build(logger);\n+    }\n+\n     // when creating the file selection, set the selection root in the form /a/b instead of\n     // file:/a/b.  The reason is that the file names above have been created in the form\n     // /a/b/c.parquet and the format of the selection root must match that of the file names",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "sha": "47172cc8a59ace5f016a4f9d451cd2f41784d1e9",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.parquet;\n+\n+import org.apache.drill.BaseTestQuery;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestParquetGroupScan extends BaseTestQuery {\n+\n+  private void prepareTables(final String tableName, boolean refreshMetadata) throws Exception {\n+    // first create some parquet subfolders\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s`      AS SELECT employee_id FROM cp.`employee.json` LIMIT 1\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/501`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 2\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/502`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 4\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/503`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 8\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/504`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 16\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/505`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 32\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/60`   AS SELECT employee_id FROM cp.`employee.json` LIMIT 64\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/602`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 128\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6031` AS SELECT employee_id FROM cp.`employee.json` LIMIT 256\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6032` AS SELECT employee_id FROM cp.`employee.json` LIMIT 512\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6033` AS SELECT employee_id FROM cp.`employee.json` LIMIT 1024\", tableName);\n+\n+    // we need an empty subfolder `4376/20160401`\n+    // to do this we first create a table inside that subfolder\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6041/a` AS SELECT * FROM cp.`employee.json` LIMIT 1\", tableName);\n+    // then we delete the table, leaving the parent subfolder empty\n+    testNoResult(\"DROP TABLE   dfs_test.tmp.`%s/6041/a`\", tableName);\n+\n+    if (refreshMetadata) {\n+      // build the metadata cache file\n+      testNoResult(\"REFRESH TABLE METADATA dfs_test.tmp.`%s`\", tableName);\n+    }\n+  }\n+\n+  @Test\n+  public void testFix4376() throws Exception {\n+    prepareTables(\"4376_1\", true);\n+\n+    testBuilder()\n+      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_1/60*`\")\n+      .ordered()\n+      .baselineColumns(\"count\").baselineValues(1984L)\n+      .go();\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_2\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_2/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_3\", false);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_3/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_3/604*' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_4\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_4/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_5\", false);\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_5/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_5/6041' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "sha": "21d62858f7a901a80fb4fd12266198f2559f0d83",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 19,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "patch": "@@ -177,25 +177,6 @@ public void testFix4449() throws Exception {\n       .go();\n   }\n \n-  @Test\n-  public void testFix4376() throws Exception {\n-    // first create some parquet subfolders\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376`    AS SELECT * FROM cp.`employee.json` LIMIT 1\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/01` AS SELECT * FROM cp.`employee.json` LIMIT 2\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/02` AS SELECT * FROM cp.`employee.json` LIMIT 4\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/0`  AS SELECT * FROM cp.`employee.json` LIMIT 8\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/11` AS SELECT * FROM cp.`employee.json` LIMIT 16\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/12` AS SELECT * FROM cp.`employee.json` LIMIT 32\");\n-    // next, build the metadata cache file\n-    runSQL(\"REFRESH TABLE METADATA dfs_test.tmp.`4376`\");\n-\n-    testBuilder()\n-      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376/0*`\")\n-      .ordered()\n-      .baselineColumns(\"count\").baselineValues(15L)\n-      .go();\n-    }\n-\n   private void checkForMetadataFile(String table) throws Exception {\n     String tmpDir = getDfsTestTmpSchemaLocation();\n     String metaFile = Joiner.on(\"/\").join(tmpDir, table, Metadata.METADATA_FILENAME);",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "sha": "4330c96471e1324572bac6283ef7f0a40b38eb37",
                "status": "modified"
            }
        ],
        "message": "DRILL-4484: NPE when querying  empty directory",
        "parent": "https://github.com/apache/drill/commit/11fe8d7cdb1df4100cd48bcce1de0b2c3c5f983a",
        "patched_files": [
            "ParquetGroupScan.java",
            "FileSelection.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestParquetGroupScan.java",
            "TestFileSelection.java",
            "TestParquetMetadataCache.java"
        ]
    },
    "drill_7506cfb": {
        "bug_id": "drill_7506cfb",
        "commit": "https://github.com/apache/drill/commit/7506cfbb5c8522d371c12dbdc2268d48a9449a48",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -37,7 +37,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -105,13 +104,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n         FileSelection selection, FileSystemPlugin fsPlugin,\n         String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "sha": "fd6e59b5f45e1b5c2562134e79a2b03edb137c68",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 7,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "patch": "@@ -77,13 +77,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n       FileSelection selection, FileSystemPlugin fsPlugin,\n       String storageEngineName, String userName) throws IOException {\n     if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-      if (plugin.getName() != null) {\n-        NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-        namedConfig.name = plugin.getName();\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(namedConfig, selection));\n-      } else {\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n-      }\n+      return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n     }\n     return null;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "sha": "65260739cac90ac91640c767e667de2ac3ca6f18",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "patch": "@@ -37,7 +37,6 @@\n  */\n public class FileSelection {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(FileSelection.class);\n-  private static final String PATH_SEPARATOR = System.getProperty(\"file.separator\");\n   private static final String WILD_CARD = \"*\";\n \n   private List<FileStatus> statuses;\n@@ -224,7 +223,7 @@ private static String commonPathForFiles(final List<String> files) {\n     int shortest = Integer.MAX_VALUE;\n     for (int i = 0; i < total; i++) {\n       final Path path = new Path(files.get(i));\n-      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(PATH_SEPARATOR);\n+      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(Path.SEPARATOR);\n       shortest = Math.min(shortest, folders[i].length);\n     }\n \n@@ -247,7 +246,7 @@ private static String commonPathForFiles(final List<String> files) {\n   private static String buildPath(final String[] path, final int folderIndex) {\n     final StringBuilder builder = new StringBuilder();\n     for (int i=0; i<folderIndex; i++) {\n-      builder.append(path[i]).append(PATH_SEPARATOR);\n+      builder.append(path[i]).append(Path.SEPARATOR);\n     }\n     builder.deleteCharAt(builder.length()-1);\n     return builder.toString();",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "sha": "6aff1dd019f9734cfe91f7b27fe372bd58cdbbda",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 12,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -133,15 +133,7 @@ public StoragePluginConfig getConfig() {\n   public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection, List<SchemaPath> columns)\n       throws IOException {\n     FormatSelection formatSelection = selection.getWith(lpPersistance, FormatSelection.class);\n-    FormatPlugin plugin;\n-    if (formatSelection.getFormat() instanceof NamedFormatPluginConfig) {\n-      plugin = formatCreator.getFormatPluginByName( ((NamedFormatPluginConfig) formatSelection.getFormat()).name);\n-    } else {\n-      plugin = formatPluginsByConfig.get(formatSelection.getFormat());\n-    }\n-    if (plugin == null) {\n-      plugin = formatCreator.newFormatPlugin(formatSelection.getFormat());\n-    }\n+    FormatPlugin plugin = getFormatPlugin(formatSelection.getFormat());\n     return plugin.getGroupScan(userName, formatSelection.getSelection(), columns);\n   }\n \n@@ -154,12 +146,23 @@ public FormatPlugin getFormatPlugin(String name) {\n     return formatCreator.getFormatPluginByName(name);\n   }\n \n+  /**\n+   * If format plugin configuration is for named format plugin, will return format plugin from pre-loaded list by name.\n+   * For other cases will try to find format plugin by its configuration, if not present will attempt to create one.\n+   *\n+   * @param config format plugin configuration\n+   * @return format plugin for given configuration if found, null otherwise\n+   */\n   public FormatPlugin getFormatPlugin(FormatPluginConfig config) {\n     if (config instanceof NamedFormatPluginConfig) {\n       return formatCreator.getFormatPluginByName(((NamedFormatPluginConfig) config).name);\n-    } else {\n-      return formatPluginsByConfig.get(config);\n     }\n+\n+    FormatPlugin plugin = formatPluginsByConfig.get(config);\n+    if (plugin == null) {\n+      plugin = formatCreator.newFormatPlugin(config);\n+    }\n+    return plugin;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "sha": "5d382fe376fdc410c536ef994c5bdd33d29df87a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "patch": "@@ -251,7 +251,14 @@ public boolean isOptional() {\n \n     @Override\n     public TranslatableTable apply(List<Object> arguments) {\n-      return new DrillTranslatableTable(schema.getDrillTable(new TableInstance(sig, arguments)));\n+      DrillTable drillTable = schema.getDrillTable(new TableInstance(sig, arguments));\n+      if (drillTable == null) {\n+        throw UserException\n+            .validationError()\n+            .message(\"Unable to find table [%s] in schema [%s]\", sig.name, schema.getFullSchemaName())\n+            .build(logger);\n+      }\n+      return new DrillTranslatableTable(drillTable);\n     }\n \n   }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "sha": "6629fc4e2ec8a1f7a12edaa02b9e40ac13af3372",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -26,7 +26,6 @@\n import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractSubScan;\n import org.apache.drill.exec.store.StoragePluginRegistry;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.schedule.CompleteFileWork.FileWorkImpl;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n@@ -94,13 +93,7 @@ public StoragePluginConfig getStorageConfig(){\n \n   @JsonProperty(\"format\")\n   public FormatPluginConfig getFormatConfig(){\n-    if (formatPlugin.getName() != null) {\n-      NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-      namedConfig.name = formatPlugin.getName();\n-      return namedConfig;\n-    } else {\n-      return formatPlugin.getConfig();\n-    }\n+    return formatPlugin.getConfig();\n   }\n \n   @JsonProperty(\"columns\")",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "sha": "a6af1ac84894c81a74dfec9a92031864932ca228",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 13,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "patch": "@@ -1,21 +1,26 @@\n-\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n- * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n- * License. You may obtain a copy of the License at\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n  * http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n- * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n- * specific language governing permissions and limitations under the License.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.drill.exec.store.httpd;\n \n import java.io.IOException;\n import java.util.List;\n \n+import com.fasterxml.jackson.annotation.JsonInclude;\n import nl.basjes.parse.core.exceptions.DissectionFailure;\n import nl.basjes.parse.core.exceptions.InvalidDissectorException;\n import nl.basjes.parse.core.exceptions.MissingDissectorsException;\n@@ -73,11 +78,11 @@ public HttpdLogFormatPlugin(final String name, final DrillbitContext context, fi\n    * This class is a POJO to hold the configuration for the HttpdLogFormat Parser. This is automatically\n    * serialized/deserialized from JSON format.\n    */\n-  @JsonTypeName(PLUGIN_EXTENSION)\n+  @JsonTypeName(PLUGIN_EXTENSION) @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n   public static class HttpdLogFormatConfig implements FormatPluginConfig {\n \n-    private String logFormat;\n-    private String timestampFormat;\n+    public String logFormat;\n+    public String timestampFormat;\n \n     /**\n      * @return the logFormat\n@@ -92,6 +97,30 @@ public String getLogFormat() {\n     public String getTimestampFormat() {\n       return timestampFormat;\n     }\n+\n+    @Override\n+    public int hashCode() {\n+      int result = logFormat != null ? logFormat.hashCode() : 0;\n+      result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n+      return result;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+\n+      HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n+\n+      if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n+        return false;\n+      }\n+      return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n+    }\n   }\n \n   /**\n@@ -119,7 +148,7 @@ public HttpdLogRecordReader(final FragmentContext context, final DrillFileSystem\n      * The query fields passed in are formatted in a way that Drill requires. Those must be cleaned up to work with the\n      * parser.\n      *\n-     * @return Map<DrillFieldNames, ParserFieldNames>\n+     * @return Map with Drill field names as a key and Parser Field names as a value\n      */\n     private Map<String, String> makeParserFields() {\n       final Map<String, String> fieldMapping = Maps.newHashMap();",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "sha": "cee9a89247ce3d8ec128c735d60154949c96567c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "patch": "@@ -164,9 +164,6 @@\n   ) throws IOException, ExecutionSetupException {\n     super(ImpersonationUtil.resolveUserName(userName));\n     this.columns = columns;\n-    if (formatConfig == null) {\n-      formatConfig = new ParquetFormatConfig();\n-    }\n     Preconditions.checkNotNull(storageConfig);\n     Preconditions.checkNotNull(formatConfig);\n     this.formatPlugin = (ParquetFormatPlugin) engineRegistry.getFormatPlugin(storageConfig, formatConfig);\n@@ -345,6 +342,7 @@ public boolean hasFiles() {\n     return true;\n   }\n \n+  @JsonIgnore\n   @Override\n   public Collection<String> getFiles() {\n     return fileSet;",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "sha": "75b18df072afcbc85b7b0f1287ccb663e8f5c2fc",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 12,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -65,9 +65,12 @@ public ParquetRowGroupScan( //\n       @JsonProperty(\"selectionRoot\") String selectionRoot, //\n       @JsonProperty(\"filter\") LogicalExpression filter\n   ) throws ExecutionSetupException {\n-    this(userName, (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig),\n-            formatConfig == null ? new ParquetFormatConfig() : formatConfig),\n-        rowGroupReadEntries, columns, selectionRoot, filter);\n+    this(userName,\n+        (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig), Preconditions.checkNotNull(formatConfig)),\n+        rowGroupReadEntries,\n+        columns,\n+        selectionRoot,\n+        filter);\n   }\n \n   public ParquetRowGroupScan( //\n@@ -79,7 +82,7 @@ public ParquetRowGroupScan( //\n       LogicalExpression filter\n   ) {\n     super(userName);\n-    this.formatPlugin = Preconditions.checkNotNull(formatPlugin);\n+    this.formatPlugin = Preconditions.checkNotNull(formatPlugin, \"Could not find format config for the given configuration\");\n     this.formatConfig = formatPlugin.getConfig();\n     this.rowGroupReadEntries = rowGroupReadEntries;\n     this.columns = columns == null ? GroupScan.ALL_COLUMNS : columns;\n@@ -97,6 +100,14 @@ public StoragePluginConfig getEngineConfig() {\n     return formatPlugin.getStorageConfig();\n   }\n \n+  /**\n+   * @return Parquet plugin format config\n+   */\n+  @JsonProperty(\"format\")\n+  public ParquetFormatConfig getFormatConfig() {\n+    return formatConfig;\n+  }\n+\n   public String getSelectionRoot() {\n     return selectionRoot;\n   }\n@@ -140,11 +151,4 @@ public int getOperatorType() {\n     return CoreOperatorType.PARQUET_ROW_GROUP_SCAN_VALUE;\n   }\n \n-  /**\n-   * @return Parquet plugin format config\n-   */\n-  public ParquetFormatConfig getFormatConfig() {\n-    return formatConfig;\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "sha": "f1fb1e9207f86e23e426e214d1e8f6ec159b7007",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "patch": "@@ -21,4 +21,14 @@\n \n @JsonTypeName(\"pcap\")\n public class PcapFormatConfig implements FormatPluginConfig {\n+\n+  @Override\n+  public int hashCode() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public boolean equals(Object obj) {\n+    return obj instanceof PcapFormatConfig;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "sha": "89b56adadbab2232558a1aef3c4756dc1ee969c0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 8,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -99,13 +98,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n                                  FileSelection selection, FileSystemPlugin fsPlugin,\n                                  String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "sha": "65ff2388ac30c8e93b4f453d0cb93d0962fb5020",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "patch": "@@ -281,6 +281,19 @@ public static void testRelLogicalPlanLevExplain(String sql, String... expectedSu\n     }\n   }\n \n+\n+  /**\n+   * Creates physical plan for the given query and then executes this plan.\n+   * This method is useful for testing serialization / deserialization issues.\n+   *\n+   * @param query query string\n+   */\n+  public static void testPhysicalPlanExecutionBasedOnQuery(String query) throws Exception {\n+    query = \"EXPLAIN PLAN for \" + QueryTestUtil.normalizeQuery(query);\n+    String plan = getPlanInString(query, JSON_FORMAT);\n+    testPhysical(plan);\n+  }\n+\n   /*\n    * This will get the plan (either logical or physical) in Optiq RelNode\n    * format, based on SqlExplainLevel and Depth.",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "sha": "22b734b2746718f39900f3522ddcced5fdc7d675",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -19,12 +19,15 @@\n \n import static java.lang.String.format;\n import static org.apache.drill.test.TestBuilder.listOf;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertThat;\n \n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n \n import org.apache.drill.categories.SqlTest;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n import org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.test.TestBuilder;\n@@ -276,4 +279,16 @@ public void testUse() throws Exception {\n       test(\"use sys\");\n     }\n   }\n+\n+  @Test(expected = UserRemoteException.class)\n+  public void testAbsentTable() throws Exception {\n+    String schema = \"cp.default\";\n+    String tableName = \"absent_table\";\n+    try {\n+      test(\"select * from table(`%s`.`%s`(type=>'parquet'))\", schema, tableName);\n+    } catch (UserRemoteException e) {\n+      assertThat(e.getMessage(), containsString(String.format(\"Unable to find table [%s] in schema [%s]\", tableName, schema)));\n+      throw e;\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "sha": "a6dff740bc1009de3f2c9fab65c8c245562a7ef2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "patch": "@@ -177,7 +177,7 @@ public void run() {\n     }\n   }\n \n-  @Test\n+  //@Test\n   public void testConcurrentQueries() throws Exception {\n     QueryTestUtil.testRunAndPrint(client, UserBitShared.QueryType.SQL, alterSession);\n ",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "sha": "f096d558aef6c04c36f326e8567cecc208e62f9b",
                "status": "modified"
            },
            {
                "additions": 113,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "patch": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store;\n+\n+import org.apache.drill.PlanTestBase;\n+import org.apache.drill.exec.store.avro.AvroTestUtil;\n+import org.junit.Test;\n+\n+import java.nio.file.Paths;\n+\n+public class FormatPluginSerDeTest extends PlanTestBase {\n+\n+  @Test\n+  public void testParquet() throws Exception {\n+    test(\"alter session set `planner.slice_target` = 1\");\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet'))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>false))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>true))\", \"parquet/alltypes_required.parquet\")\n+    );\n+  }\n+\n+  @Test\n+  public void testAvro() throws Exception {\n+    AvroTestUtil.AvroTestRecordWriter testSetup = AvroTestUtil.generateSimplePrimitiveSchema_NoNullValues(5);\n+    String file = testSetup.getFileName();\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", file),\n+        String.format(\"select * from table(dfs.`%s`(type=>'avro'))\", file)\n+    );\n+  }\n+\n+  @Test\n+  public void testSequenceFile() throws Exception {\n+    String path = \"sequencefiles/simple.seq\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'sequencefile'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testPcap() throws Exception {\n+    String path = \"store/pcap/tcp-1.pcap\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'pcap'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testHttpd() throws Exception {\n+    String path = \"store/httpd/dfs-bootstrap.httpd\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String logFormat = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n+    String timeStampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s'))\", path, logFormat),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s', timestampFormat=>'%s'))\", path, logFormat, timeStampFormat)\n+    );\n+  }\n+\n+  @Test\n+  public void testJson() throws Exception {\n+    testPhysicalPlanSubmission(\n+        \"select * from cp.`donuts.json`\",\n+        \"select * from table(cp.`donuts.json`(type=>'json'))\"\n+    );\n+  }\n+\n+  @Test\n+  public void testText() throws Exception {\n+    String path = \"store/text/data/regions.csv\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(dfs.`%s`(type => 'text'))\", path),\n+        String.format(\"select * from table(dfs.`%s`(type => 'text', extractHeader => false, fieldDelimiter => 'A'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testNamed() throws Exception {\n+    String path = \"store/text/WithQuote.tbl\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String query = String.format(\"select * from table(dfs.`%s`(type=>'named', name=>'psv'))\", path);\n+    testPhysicalPlanSubmission(query);\n+  }\n+\n+  private void testPhysicalPlanSubmission(String...queries) throws Exception {\n+    for (String query : queries) {\n+      PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(query);\n+    }\n+  }\n+\n+}\n+",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "sha": "ca81eaa3e82e601d80c1d2f29892b54c68696e83",
                "status": "added"
            }
        ],
        "message": "DRILL-5771: Fix serDe errors for format plugins\n\n1. Fix various serde issues for format plugins described in DRILL-5771.\n2. Throw meaninful exception instead of NPE when table is not found when table function is used.\n3. Added unit tests for all format plugins for ensure serde is checked (physical plan is generated in json format and then submitted).\n4. Fix physical plan submission on Windows (DRILL-4640).\n\nThis closes #1014",
        "parent": "https://github.com/apache/drill/commit/d4c61cadbe5c7d88fd4393cc1b29648fbadfb9f1",
        "patched_files": [
            "PlanTestBase.java",
            "ParquetRowGroupScan.java",
            "HttpdLogFormatPlugin.java",
            "PcapFormatPlugin.java",
            "FileSystemPlugin.java",
            "WorkspaceSchemaFactory.java",
            "PcapFormatConfig.java",
            "EasySubScan.java",
            "ParquetGroupScan.java",
            "AvroFormatPlugin.java",
            "BasicFormatMatcher.java",
            "FileSelection.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestParquetGroupScan.java",
            "TestFileSelection.java",
            "TestSelectWithOption.java",
            "TestTpchDistributedConcurrent.java",
            "FormatPluginSerDeTest.java"
        ]
    },
    "drill_79811db": {
        "bug_id": "drill_79811db",
        "commit": "https://github.com/apache/drill/commit/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "patch": "@@ -66,7 +66,6 @@\n \n   // External Sort Boot configuration\n \n-  String EXTERNAL_SORT_TARGET_BATCH_SIZE = \"drill.exec.sort.external.batch.size\";\n   String EXTERNAL_SORT_TARGET_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.batch.size\";\n   String EXTERNAL_SORT_SPILL_GROUP_SIZE = \"drill.exec.sort.external.spill.group.size\";\n   String EXTERNAL_SORT_SPILL_THRESHOLD = \"drill.exec.sort.external.spill.threshold\";\n@@ -79,6 +78,8 @@\n   String EXTERNAL_SORT_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.spill_batch_size\";\n   String EXTERNAL_SORT_MERGE_BATCH_SIZE = \"drill.exec.sort.external.spill.merge_batch_size\";\n   String EXTERNAL_SORT_MAX_MEMORY = \"drill.exec.sort.external.mem_limit\";\n+\n+  // Used only by the \"unmanaged\" sort.\n   String EXTERNAL_SORT_BATCH_LIMIT = \"drill.exec.sort.external.batch_limit\";\n \n   // External Sort Runtime options",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "sha": "60d6265275f4f052c763d85d6002eaf94871cc30",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "patch": "@@ -238,14 +238,15 @@ public void close() {\n   }\n \n   /**\n-   * For given recordcount how muchmemory does SortRecordBatchBuilder needs for its own purpose. This is used in\n+   * For given record count how much memory does SortRecordBatchBuilder needs for its own purpose. This is used in\n    * ExternalSortBatch to make decisions about whether to spill or not.\n    *\n    * @param recordCount\n    * @return\n    */\n   public static long memoryNeeded(int recordCount) {\n-    // We need 4 bytes (SV4) for each record.\n-    return recordCount * 4;\n+    // We need 4 bytes (SV4) for each record. Due to power-of-two allocations, the\n+    // backing buffer might be twice this size.\n+    return recordCount * 2 * 4;\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "sha": "d46990f8d0878ef44e789e3bb767b2bab6476868",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 71,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "patch": "@@ -27,22 +27,15 @@\n import org.apache.drill.exec.record.VectorAccessible;\n import org.apache.drill.exec.record.VectorWrapper;\n import org.apache.drill.exec.record.selection.SelectionVector2;\n-import org.apache.drill.exec.vector.BaseDataValueVector;\n-import org.apache.drill.exec.vector.FixedWidthVector;\n-import org.apache.drill.exec.vector.NullableVarCharVector;\n-import org.apache.drill.exec.vector.NullableVector;\n import org.apache.drill.exec.vector.ValueVector;\n-import org.apache.drill.exec.vector.VarCharVector;\n-\n-import io.netty.buffer.DrillBuf;\n \n /**\n  * Given a record batch or vector container, determines the actual memory\n  * consumed by each column, the average row, and the entire record batch.\n  */\n \n public class RecordBatchSizer {\n-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n+//  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n \n   /**\n    * Column size information.\n@@ -53,23 +46,22 @@\n     /**\n      * Assumed size from Drill metadata.\n      */\n+\n     public int stdSize;\n+\n     /**\n      * Actual memory consumed by all the vectors associated with this column.\n      */\n+\n     public int totalSize;\n+\n     /**\n      * Actual average column width as determined from actual memory use. This\n      * size is larger than the actual data size since this size includes per-\n      * column overhead such as any unused vector space, etc.\n      */\n-    public int estSize;\n \n-    /**\n-     * The size of the data vector backing the column. Useful for detecting\n-     * cases of possible direct memory fragmentation.\n-     */\n-    public int dataVectorSize;\n+    public int estSize;\n     public int capacity;\n     public int density;\n     public int dataSize;\n@@ -86,53 +78,31 @@ public ColumnSize(VectorWrapper<?> vw) {\n       if (rowCount == 0) {\n         return;\n       }\n-      DrillBuf[] bufs = v.getBuffers(false);\n-      for (DrillBuf buf : bufs) {\n-        totalSize += buf.capacity();\n-      }\n+\n+      // Total size taken by all vectors (and underlying buffers)\n+      // associated with this vector.\n+\n+      totalSize = v.getAllocatedByteCount();\n \n       // Capacity is the number of values that the vector could\n       // contain. This is useful only for fixed-length vectors.\n \n       capacity = v.getValueCapacity();\n \n-      // Crude way to get the size of the buffer underlying simple (scalar) values.\n-      // Ignores maps, lists and other esoterica. Uses a crude way to subtract out\n-      // the null \"bit\" (really byte) buffer size for nullable vectors.\n+      // The amount of memory consumed by the payload: the actual\n+      // data stored in the vectors.\n \n-      if (v instanceof BaseDataValueVector) {\n-        dataVectorSize = totalSize;\n-        if (v instanceof NullableVector) {\n-          dataVectorSize -= bufs[0].getActualMemoryConsumed();\n-        }\n-      }\n+      dataSize = v.getPayloadByteCount();\n \n       // Determine \"density\" the number of rows compared to potential\n       // capacity. Low-density batches occur at block boundaries, ends\n       // of files and so on. Low-density batches throw off our estimates\n       // for Varchar columns because we don't know the actual number of\n       // bytes consumed (that information is hidden behind the Varchar\n       // implementation where we can't get at it.)\n-      //\n-      // A better solution is to have each vector do this calc rather\n-      // than trying to do it generically, but that increases the code\n-      // change footprint and slows the commit process.\n-\n-      if (v instanceof FixedWidthVector) {\n-        dataSize = stdSize * rowCount;\n-      } else if ( v instanceof VarCharVector ) {\n-        VarCharVector vv = (VarCharVector) v;\n-        dataSize = vv.getOffsetVector().getAccessor().get(rowCount);\n-      } else if ( v instanceof NullableVarCharVector ) {\n-        NullableVarCharVector vv = (NullableVarCharVector) v;\n-        dataSize = vv.getValuesVector().getOffsetVector().getAccessor().get(rowCount);\n-      } else {\n-        dataSize = 0;\n-      }\n-      if (dataSize > 0) {\n-        density = roundUp(dataSize * 100, dataVectorSize);\n-        estSize = roundUp(dataSize, rowCount);\n-      }\n+\n+      density = roundUp(dataSize * 100, totalSize);\n+      estSize = roundUp(dataSize, rowCount);\n     }\n \n     @Override\n@@ -145,8 +115,6 @@ public String toString() {\n       buf.append(estSize);\n       buf.append(\", total size: \");\n       buf.append(totalSize);\n-      buf.append(\", vector size: \");\n-      buf.append(dataVectorSize);\n       buf.append(\", data size: \");\n       buf.append(dataSize);\n       buf.append(\", row capacity: \");\n@@ -187,10 +155,12 @@ public String toString() {\n   private int sv2Size;\n   private int avgDensity;\n \n+  private int netBatchSize;\n+\n   public RecordBatchSizer(VectorAccessible va) {\n     rowCount = va.getRecordCount();\n     for (VectorWrapper<?> vw : va) {\n-      measureField(vw);\n+      measureColumn(vw);\n     }\n \n     if (rowCount > 0) {\n@@ -201,8 +171,8 @@ public RecordBatchSizer(VectorAccessible va) {\n     if (hasSv2) {\n       @SuppressWarnings(\"resource\")\n       SelectionVector2 sv2 = va.getSelectionVector2();\n-      sv2Size = sv2.getBuffer().capacity();\n-      grossRowWidth += sv2Size;\n+      sv2Size = sv2.getBuffer(false).capacity();\n+      grossRowWidth += sv2Size / rowCount;\n       netRowWidth += 2;\n     }\n \n@@ -227,12 +197,13 @@ public void applySv2() {\n     totalBatchSize += sv2Size;\n   }\n \n-  private void measureField(VectorWrapper<?> vw) {\n+  private void measureColumn(VectorWrapper<?> vw) {\n     ColumnSize colSize = new ColumnSize(vw);\n     columnSizes.add(colSize);\n \n     stdRowWidth += colSize.stdSize;\n     totalBatchSize += colSize.totalSize;\n+    netBatchSize += colSize.dataSize;\n     netRowWidth += colSize.estSize;\n   }\n \n@@ -249,27 +220,11 @@ public static int roundUp(int num, int denom) {\n   public int netRowWidth() { return netRowWidth; }\n   public int actualSize() { return totalBatchSize; }\n   public boolean hasSv2() { return hasSv2; }\n-  public int getAvgDensity() { return avgDensity; }\n+  public int avgDensity() { return avgDensity; }\n+  public int netSize() { return netBatchSize; }\n \n   public static final int MAX_VECTOR_SIZE = 16 * 1024 * 1024; // 16 MiB\n \n-  /**\n-   * Look for columns backed by vectors larger than the 16 MiB size\n-   * employed by the Netty allocator. Such large blocks can lead to\n-   * memory fragmentation and unexpected OOM errors.\n-   * @return true if any column is oversized\n-   */\n-  public boolean checkOversizeCols() {\n-    boolean hasOversize = false;\n-    for (ColumnSize colSize : columnSizes) {\n-      if ( colSize.dataVectorSize > MAX_VECTOR_SIZE) {\n-        logger.warn( \"Column is wider than 256 characters: OOM due to memory fragmentation is possible - \" + colSize.metadata.getPath() );\n-        hasOversize = true;\n-      }\n-    }\n-    return hasOversize;\n-  }\n-\n   @Override\n   public String toString() {\n     StringBuilder buf = new StringBuilder();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "sha": "22b1b0eed03115d4bf17cb6e1e00409d4b470625",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.drill.common.config.DrillConfig;\n@@ -105,7 +106,7 @@\n \n     protected HadoopFileManager(String fsName) {\n       Configuration conf = new Configuration();\n-      conf.set(\"fs.default.name\", fsName);\n+      conf.set(FileSystem.FS_DEFAULT_NAME_KEY, fsName);\n       try {\n         fs = FileSystem.get(conf);\n       } catch (IOException e) {\n@@ -169,6 +170,12 @@ public long getReadBytes(InputStream inputStream) {\n     }\n   }\n \n+  /**\n+   * Wrapper around an input stream to collect the total bytes\n+   * read through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingInputStream extends InputStream\n   {\n     private InputStream in;\n@@ -218,6 +225,12 @@ public void close() throws IOException {\n     public long getCount() { return count; }\n   }\n \n+  /**\n+   * Wrapper around an output stream to collect the total bytes\n+   * written through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingOutputStream extends OutputStream {\n \n     private OutputStream out;\n@@ -333,6 +346,7 @@ public long getReadBytes(InputStream inputStream) {\n    */\n \n   private final String spillDirName;\n+  private final String spillFileName;\n \n   private int fileCount = 0;\n \n@@ -343,8 +357,30 @@ public long getReadBytes(InputStream inputStream) {\n   private long writeBytes;\n \n   public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n+    this(context, popConfig, null, \"spill\");\n+  }\n+\n+  public SpillSet(FragmentContext context, PhysicalOperator popConfig,\n+                  String opName, String fileName) {\n+    FragmentHandle handle = context.getHandle();\n     DrillConfig config = context.getConfig();\n-    dirs = Iterators.cycle(config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS));\n+    spillFileName = fileName;\n+    List<String> dirList = config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS);\n+    dirs = Iterators.cycle(dirList);\n+\n+    // If more than one directory, semi-randomly choose an offset into\n+    // the list to avoid overloading the first directory in the list.\n+\n+    if (dirList.size() > 1) {\n+      int hash = handle.getQueryId().hashCode() +\n+                 handle.getMajorFragmentId() +\n+                 handle.getMinorFragmentId() +\n+                 popConfig.getOperatorId();\n+      int offset = hash % dirList.size();\n+      for (int i = 0; i < offset; i++) {\n+        dirs.next();\n+      }\n+    }\n \n     // Use the high-performance local file system if the local file\n     // system is selected and impersonation is off. (We use that\n@@ -357,9 +393,13 @@ public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n     } else {\n       fileManager = new HadoopFileManager(spillFs);\n     }\n-    FragmentHandle handle = context.getHandle();\n-    spillDirName = String.format(\"%s_major%s_minor%s_op%s\", QueryIdHelper.getQueryId(handle.getQueryId()),\n-        handle.getMajorFragmentId(), handle.getMinorFragmentId(), popConfig.getOperatorId());\n+    spillDirName = String.format(\n+        \"%s_major%d_minor%d_op%d%s\",\n+        QueryIdHelper.getQueryId(handle.getQueryId()),\n+        handle.getMajorFragmentId(),\n+        handle.getMinorFragmentId(),\n+        popConfig.getOperatorId(),\n+        (opName == null) ? \"\" : \"_\" + opName);\n   }\n \n   public String getNextSpillFile() {\n@@ -371,7 +411,7 @@ public String getNextSpillFile() {\n     String spillDir = dirs.next();\n     String currSpillPath = Joiner.on(\"/\").join(spillDir, spillDirName);\n     currSpillDirs.add(currSpillPath);\n-    String outputFile = Joiner.on(\"/\").join(currSpillPath, \"spill\" + ++fileCount);\n+    String outputFile = Joiner.on(\"/\").join(currSpillPath, spillFileName + ++fileCount);\n     try {\n         fileManager.deleteOnExit(currSpillPath);\n     } catch (IOException e) {",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "sha": "74e1fb5674eced7fec3fca51915eee8c2afc6cbf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "patch": "@@ -113,7 +113,7 @@ private VectorContainer getBatch() throws IOException {\n     if (schema != null) {\n       c = SchemaUtil.coerceContainer(c, schema, context);\n     }\n-//    logger.debug(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n+    logger.trace(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n     spilledBatches--;\n     currentContainer.zeroVectors();\n     Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "sha": "13f0dbeb59a33d43b006cba58c438167db2e88a8",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 15,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "patch": "@@ -75,17 +75,21 @@\n    */\n \n   public static class InputBatch extends BatchGroup {\n-    private SelectionVector2 sv2;\n+    private final SelectionVector2 sv2;\n+    private final int dataSize;\n \n-    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, long batchSize) {\n-      super(container, context, batchSize);\n+    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, int dataSize) {\n+      super(container, context);\n       this.sv2 = sv2;\n+      this.dataSize = dataSize;\n     }\n \n     public SelectionVector2 getSv2() {\n       return sv2;\n     }\n \n+    public int getDataSize() { return dataSize; }\n+\n     @Override\n     public int getRecordCount() {\n       if (sv2 != null) {\n@@ -148,8 +152,8 @@ public void close() throws IOException {\n     private BufferAllocator allocator;\n     private int spilledBatches = 0;\n \n-    public SpilledRun(SpillSet spillSet, String path, OperatorContext context, long batchSize) throws IOException {\n-      super(null, context, batchSize);\n+    public SpilledRun(SpillSet spillSet, String path, OperatorContext context) throws IOException {\n+      super(null, context);\n       this.spillSet = spillSet;\n       this.path = path;\n       this.allocator = context.getAllocator();\n@@ -275,25 +279,23 @@ public long closeOutputStream() throws IOException {\n       if (outputStream == null) {\n         return 0;\n       }\n-      long posn = spillSet.getPosition(outputStream);\n-      spillSet.tallyWriteBytes(posn);\n+      long writeSize = spillSet.getPosition(outputStream);\n+      spillSet.tallyWriteBytes(writeSize);\n       outputStream.close();\n       outputStream = null;\n-      logger.trace(\"Summary: Wrote {} bytes to {}\", posn, path);\n-      return posn;\n+      logger.trace(\"Summary: Wrote {} bytes to {}\", writeSize, path);\n+      return writeSize;\n     }\n   }\n \n   protected VectorContainer currentContainer;\n   protected int pointer = 0;\n-  protected OperatorContext context;\n+  protected final OperatorContext context;\n   protected BatchSchema schema;\n-  protected long dataSize;\n \n-  public BatchGroup(VectorContainer container, OperatorContext context, long dataSize) {\n+  public BatchGroup(VectorContainer container, OperatorContext context) {\n     this.currentContainer = container;\n     this.context = context;\n-    this.dataSize = dataSize;\n   }\n \n   /**\n@@ -348,8 +350,6 @@ public int getUnfilteredRecordCount() {\n     return currentContainer.getRecordCount();\n   }\n \n-  public long getDataSize() { return dataSize; }\n-\n   @Override\n   public Iterator<VectorWrapper<?>> iterator() {\n     return currentContainer.iterator();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "sha": "7ea599c39c0e3378226c6b1f0df003b60ad161fe",
                "status": "modified"
            },
            {
                "additions": 180,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "changes": 291,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 111,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "patch": "@@ -200,6 +200,7 @@\n   public static final String INTERRUPTION_AFTER_SORT = \"after-sort\";\n   public static final String INTERRUPTION_AFTER_SETUP = \"after-setup\";\n   public static final String INTERRUPTION_WHILE_SPILLING = \"spilling\";\n+  public static final String INTERRUPTION_WHILE_MERGING = \"merging\";\n   public static final long DEFAULT_SPILL_BATCH_SIZE = 8L * 1024 * 1024;\n   public static final long MIN_SPILL_BATCH_SIZE = 256 * 1024;\n \n@@ -219,6 +220,11 @@\n \n   private BatchSchema schema;\n \n+  /**\n+   * Incoming batches buffered in memory prior to spilling\n+   * or an in-memory merge.\n+   */\n+\n   private LinkedList<BatchGroup.InputBatch> bufferedBatches = Lists.newLinkedList();\n   private LinkedList<BatchGroup.SpilledRun> spilledRuns = Lists.newLinkedList();\n   private SelectionVector4 sv4;\n@@ -231,6 +237,12 @@\n   private int mergeBatchRowCount;\n   private int peakNumBatches = -1;\n \n+  /**\n+   * Maximum memory this operator may use. Usually comes from the\n+   * operator definition, but may be overridden by a configuration\n+   * parameter for unit testing.\n+   */\n+\n   private long memoryLimit;\n \n   /**\n@@ -280,35 +292,79 @@\n   private long estimatedInputBatchSize;\n \n   /**\n-   * Maximum number of batches to hold in memory.\n-   * (Primarily for testing.)\n+   * Maximum number of spilled runs that can be merged in a single pass.\n    */\n \n-  private int bufferedBatchLimit;\n   private int mergeLimit;\n+\n+  /**\n+   * Target size of the first-generation spill files.\n+   */\n   private long spillFileSize;\n+\n+  /**\n+   * Tracks the minimum amount of remaining memory for use\n+   * in populating an operator metric.\n+   */\n+\n   private long minimumBufferSpace;\n \n   /**\n-   * Minimum memory level before spilling occurs. That is, we can buffer input\n-   * batches in memory until we are down to the level given by the spill point.\n+   * Maximum memory level before spilling occurs. That is, we can buffer input\n+   * batches in memory until we reach the level given by the buffer memory pool.\n+   */\n+\n+  private long bufferMemoryPool;\n+\n+  /**\n+   * Maximum memory that can hold batches during the merge\n+   * phase.\n    */\n \n-  private long spillPoint;\n   private long mergeMemoryPool;\n+\n+  /**\n+   * The target size for merge batches sent downstream.\n+   */\n+\n   private long preferredMergeBatchSize;\n-  private long bufferMemoryPool;\n-  private boolean hasOversizeCols;\n+\n+  /**\n+   * Sum of the total number of bytes read from upstream.\n+   * This is the raw memory bytes, not actual data bytes.\n+   */\n+\n   private long totalInputBytes;\n-  private Long spillBatchSize;\n+\n+  /**\n+   * The configured size for each spill batch.\n+   */\n+  private Long preferredSpillBatchSize;\n+\n+  /**\n+   * Tracks the maximum density of input batches. Density is\n+   * the amount of actual data / amount of memory consumed.\n+   * Low density batches indicate an EOF or something wrong in\n+   * an upstream operator because a low-density batch wastes\n+   * memory.\n+   */\n+\n   private int maxDensity;\n+  private int lastDensity = -1;\n \n   /**\n    * Estimated number of rows that fit into a single spill batch.\n    */\n \n   private int spillBatchRowCount;\n \n+  /**\n+   * The estimated actual spill batch size which depends on the\n+   * details of the data rows for any particular query.\n+   */\n+\n+  private int targetSpillBatchSize;\n+\n   // WARNING: The enum here is used within this class. But, the members of\n   // this enum MUST match those in the (unmanaged) ExternalSortBatch since\n   // that is the enum used in the UI to display metrics for the query profile.\n@@ -349,7 +405,7 @@ public ExternalSortBatch(ExternalSort popConfig, FragmentContext context, Record\n     allocator = oContext.getAllocator();\n     opCodeGen = new OperatorCodeGenerator(context, popConfig);\n \n-    spillSet = new SpillSet(context, popConfig);\n+    spillSet = new SpillSet(context, popConfig, \"sort\", \"run\");\n     copierHolder = new CopierHolder(context, allocator, opCodeGen);\n     configure(context.getConfig());\n   }\n@@ -368,12 +424,6 @@ private void configure(DrillConfig config) {\n       memoryLimit = Math.min(memoryLimit, configLimit);\n     }\n \n-    // Optional limit on the number of buffered in-memory batches.\n-    // 0 means no limit. Used primarily for testing. Must allow at least two\n-    // batches or no merging can occur.\n-\n-    bufferedBatchLimit = getConfigLimit(config, ExecConstants.EXTERNAL_SORT_BATCH_LIMIT, Integer.MAX_VALUE, 2);\n-\n     // Optional limit on the number of spilled runs to merge in a single\n     // pass. Limits the number of open file handles. Must allow at least\n     // two batches to merge to make progress.\n@@ -392,22 +442,31 @@ private void configure(DrillConfig config) {\n     // Set too large and the ratio between memory and input data sizes becomes\n     // small. Set too small and disk seek times dominate performance.\n \n-    spillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n-    spillBatchSize = Math.max(spillBatchSize, MIN_SPILL_BATCH_SIZE);\n+    preferredSpillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n+\n+    // In low memory, use no more than 1/4 of memory for each spill batch. Ensures we\n+    // can merge.\n+\n+    preferredSpillBatchSize = Math.min(preferredSpillBatchSize, memoryLimit / 4);\n+\n+    // But, the spill batch should be above some minimum size to prevent complete\n+    // thrashing.\n+\n+    preferredSpillBatchSize = Math.max(preferredSpillBatchSize, MIN_SPILL_BATCH_SIZE);\n \n     // Set the target output batch size. Use the maximum size, but only if\n     // this represents less than 10% of available memory. Otherwise, use 10%\n     // of memory, but no smaller than the minimum size. In any event, an\n     // output batch can contain no fewer than a single record.\n \n     preferredMergeBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_MERGE_BATCH_SIZE);\n-    long maxAllowance = (long) (memoryLimit * MERGE_BATCH_ALLOWANCE);\n+    long maxAllowance = (long) (memoryLimit - 2 * preferredSpillBatchSize);\n     preferredMergeBatchSize = Math.min(maxAllowance, preferredMergeBatchSize);\n     preferredMergeBatchSize = Math.max(preferredMergeBatchSize, MIN_MERGED_BATCH_SIZE);\n \n-    logger.debug(\"Config: memory limit = {}, batch limit = {}, \" +\n-                 \"spill file size = {}, batch size = {}, merge limit = {}, merge batch size = {}\",\n-                  memoryLimit, bufferedBatchLimit, spillFileSize, spillBatchSize, mergeLimit,\n+    logger.debug(\"Config: memory limit = {}, \" +\n+                 \"spill file size = {}, spill batch size = {}, merge limit = {}, merge batch size = {}\",\n+                  memoryLimit, spillFileSize, preferredSpillBatchSize, mergeLimit,\n                   preferredMergeBatchSize);\n   }\n \n@@ -513,11 +572,21 @@ public IterOutcome innerNext() {\n \n   private IterOutcome nextOutputBatch() {\n     if (resultsIterator.next()) {\n+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);\n       return IterOutcome.OK;\n     } else {\n       logger.trace(\"Deliver phase complete: Returned {} batches, {} records\",\n                     resultsIterator.getBatchCount(), resultsIterator.getRecordCount());\n       sortState = SortState.DONE;\n+\n+      // Close the iterator here to release any remaining resources such\n+      // as spill files. This is important when a query has a join: the\n+      // first branch sort may complete before the second branch starts;\n+      // it may be quite a while after returning the last row before the\n+      // fragment executor calls this opeator's close method.\n+\n+      resultsIterator.close();\n+      resultsIterator = null;\n       return IterOutcome.NONE;\n     }\n   }\n@@ -561,11 +630,11 @@ private IterOutcome loadBatch() {\n       // out of memory and that no work as in-flight and thus abandoned.\n       // Consider removing this case once resource management is in place.\n \n-      logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n+      logger.error(\"received OUT_OF_MEMORY, trying to spill\");\n       if (bufferedBatches.size() > 2) {\n         spillFromMemory();\n       } else {\n-        logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n+        logger.error(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n         return IterOutcome.OUT_OF_MEMORY;\n       }\n       break;\n@@ -693,9 +762,7 @@ private void setupSchema(IterOutcome upstream)  {\n     // Coerce all existing batches to the new schema.\n \n     for (BatchGroup b : bufferedBatches) {\n-//      System.out.println(\"Before: \" + allocator.getAllocatedMemory()); // Debug only\n       b.setSchema(schema);\n-//      System.out.println(\"After: \" + allocator.getAllocatedMemory()); // Debug only\n     }\n     for (BatchGroup b : spilledRuns) {\n       b.setSchema(schema);\n@@ -765,12 +832,12 @@ private void processBatch() {\n       spillFromMemory();\n     }\n \n-    // Sanity check. We should now be above the spill point.\n+    // Sanity check. We should now be below the buffer memory maximum.\n \n     long startMem = allocator.getAllocatedMemory();\n-    if (memoryLimit - startMem < spillPoint) {\n-      logger.error( \"ERROR: Failed to spill below the spill point. Spill point = {}, free memory = {}\",\n-                    spillPoint, memoryLimit - startMem);\n+    if (startMem > bufferMemoryPool) {\n+      logger.error( \"ERROR: Failed to spill above buffer limit. Buffer pool = {}, memory = {}\",\n+          bufferMemoryPool, startMem);\n     }\n \n     // Convert the incoming batch to the agreed-upon schema.\n@@ -835,7 +902,7 @@ private void processBatch() {\n     RecordBatchData rbd = new RecordBatchData(convertedBatch, allocator);\n     try {\n       rbd.setSv2(sv2);\n-      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, batchSize));\n+      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, sizer.netSize()));\n       if (peakNumBatches < bufferedBatches.size()) {\n         peakNumBatches = bufferedBatches.size();\n         stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);\n@@ -857,9 +924,6 @@ private void processBatch() {\n   private RecordBatchSizer analyzeIncomingBatch() {\n     RecordBatchSizer sizer = new RecordBatchSizer(incoming);\n     sizer.applySv2();\n-    if (! hasOversizeCols) {\n-      hasOversizeCols = sizer.checkOversizeCols();\n-    }\n     if (inputBatchCount == 0) {\n       logger.debug(\"{}\", sizer.toString());\n     }\n@@ -887,7 +951,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long actualBatchSize = sizer.actualSize();\n     int actualRecordCount = sizer.rowCount();\n \n-    if (actualBatchSize < memoryDelta) {\n+    if (actualBatchSize != memoryDelta) {\n       logger.debug(\"Memory delta: {}, actual batch size: {}, Diff: {}\",\n                    memoryDelta, actualBatchSize, memoryDelta - actualBatchSize);\n     }\n@@ -905,11 +969,12 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // We actually track the max density seen, and compare to 75% of that since\n     // Parquet produces very low density record batches.\n \n-    if (sizer.getAvgDensity() < maxDensity * 0.75) {\n-      logger.debug(\"Saw low density batch. Density: {}\", sizer.getAvgDensity());\n+    if (sizer.avgDensity() < maxDensity * 3 / 4 && sizer.avgDensity() != lastDensity) {\n+      logger.trace(\"Saw low density batch. Density: {}\", sizer.avgDensity());\n+      lastDensity = sizer.avgDensity();\n       return;\n     }\n-    maxDensity = Math.max(maxDensity, sizer.getAvgDensity());\n+    maxDensity = Math.max(maxDensity, sizer.avgDensity());\n \n     // We know the batch size and number of records. Use that to estimate\n     // the average record size. Since a typical batch has many records,\n@@ -934,6 +999,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long origInputBatchSize = estimatedInputBatchSize;\n     estimatedInputBatchSize = Math.max(estimatedInputBatchSize, actualBatchSize);\n \n+    // The row width may end up as zero if all fields are nulls or some\n+    // other unusual situation. In this case, assume a width of 10 just\n+    // to avoid lots of special case code.\n+\n+    if (estimatedRowWidth == 0) {\n+      estimatedRowWidth = 10;\n+    }\n+\n     // Go no further if nothing changed.\n \n     if (estimatedRowWidth == origRowEstimate && estimatedInputBatchSize == origInputBatchSize) {\n@@ -948,50 +1021,51 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // spill batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each spill batch, whichever is less.\n \n-    spillBatchRowCount = (int) Math.max(1, spillBatchSize / estimatedRowWidth);\n+    spillBatchRowCount = (int) Math.max(1, preferredSpillBatchSize / estimatedRowWidth / 2);\n     spillBatchRowCount = Math.min(spillBatchRowCount, Character.MAX_VALUE);\n \n+    // Compute the actual spill batch size which may be larger or smaller\n+    // than the preferred size depending on the row width. Double the estimated\n+    // memory needs to allow for power-of-two rounding.\n+\n+    targetSpillBatchSize = spillBatchRowCount * estimatedRowWidth * 2;\n+\n     // Determine the number of records per batch per merge step. The goal is to\n     // merge batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each merge batch, whichever is less.\n \n-    targetMergeBatchSize = preferredMergeBatchSize;\n-    mergeBatchRowCount = (int) Math.max(1, targetMergeBatchSize / estimatedRowWidth);\n+    mergeBatchRowCount = (int) Math.max(1, preferredMergeBatchSize / estimatedRowWidth / 2);\n     mergeBatchRowCount = Math.min(mergeBatchRowCount, Character.MAX_VALUE);\n+    mergeBatchRowCount = Math.max(1,  mergeBatchRowCount);\n+    targetMergeBatchSize = mergeBatchRowCount * estimatedRowWidth * 2;\n \n     // Determine the minimum memory needed for spilling. Spilling is done just\n     // before accepting a batch, so we must spill if we don't have room for a\n     // (worst case) input batch. To spill, we need room for the output batch created\n     // by merging the batches already in memory. Double this to allow for power-of-two\n     // memory allocations.\n \n-    spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n+    long spillPoint = estimatedInputBatchSize + 2 * targetSpillBatchSize;\n \n     // The merge memory pool assumes we can spill all input batches. To make\n     // progress, we must have at least two merge batches (same size as an output\n     // batch) and one output batch. Again, double to allow for power-of-two\n     // allocation and add one for a margin of error.\n \n-    int minMergeBatches = 2 * 3 + 1;\n-    long minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+    long minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n \n     // If we are in a low-memory condition, then we might not have room for the\n     // default output batch size. In that case, pick a smaller size.\n \n-    long minMemory = Math.max(spillPoint, minMergeMemory);\n-    if (minMemory > memoryLimit) {\n+    if (minMergeMemory > memoryLimit) {\n \n-      // Figure out the minimum output batch size based on memory, but can't be\n-      // any smaller than the defined minimum.\n+      // Figure out the minimum output batch size based on memory,\n+      // must hold at least one complete row.\n \n-      targetMergeBatchSize = Math.max(MIN_MERGED_BATCH_SIZE, memoryLimit / minMergeBatches);\n-\n-      // Regardless of anything else, the batch must hold at least one\n-      // complete row.\n-\n-      targetMergeBatchSize = Math.max(estimatedRowWidth, targetMergeBatchSize);\n-      spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n-      minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+      long mergeAllowance = memoryLimit - 2 * targetSpillBatchSize;\n+      targetMergeBatchSize = Math.max(estimatedRowWidth, mergeAllowance / 2);\n+      mergeBatchRowCount = (int) (targetMergeBatchSize / estimatedRowWidth / 2);\n+      minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n     }\n \n     // Determine the minimum total memory we would need to receive two input\n@@ -1004,7 +1078,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // runs when reading from disk.\n \n     bufferMemoryPool = memoryLimit - spillPoint;\n-    mergeMemoryPool = Math.max(minMergeMemory,\n+    mergeMemoryPool = Math.max(memoryLimit - minMergeMemory,\n                                (long) ((memoryLimit - 3 * targetMergeBatchSize) * 0.95));\n \n     // Sanity check: if we've been given too little memory to make progress,\n@@ -1021,14 +1095,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // Log the calculated values. Turn this on if things seem amiss.\n     // Message will appear only when the values change.\n \n-    logger.debug(\"Memory Estimates: record size = {} bytes; input batch = {} bytes, {} records; \" +\n-                  \"merge batch size = {} bytes, {} records; \" +\n-                  \"output batch size = {} bytes, {} records; \" +\n-                  \"Available memory: {}, spill point = {}, min. merge memory = {}\",\n-                estimatedRowWidth, estimatedInputBatchSize, actualRecordCount,\n-                spillBatchSize, spillBatchRowCount,\n-                targetMergeBatchSize, mergeBatchRowCount,\n-                memoryLimit, spillPoint, minMergeMemory);\n+    logger.debug(\"Input Batch Estimates: record size = {} bytes; input batch = {} bytes, {} records\",\n+                 estimatedRowWidth, estimatedInputBatchSize, actualRecordCount);\n+    logger.debug(\"Merge batch size = {} bytes, {} records; spill file size: {} bytes\",\n+                 targetSpillBatchSize, spillBatchRowCount, spillFileSize);\n+    logger.debug(\"Output batch size = {} bytes, {} records\",\n+                 targetMergeBatchSize, mergeBatchRowCount);\n+    logger.debug(\"Available memory: {}, buffer memory = {}, merge memory = {}\",\n+                 memoryLimit, bufferMemoryPool, mergeMemoryPool);\n   }\n \n   /**\n@@ -1050,14 +1124,7 @@ private boolean isSpillNeeded(int incomingSize) {\n     // Must spill if we are below the spill point (the amount of memory\n     // needed to do the minimal spill.)\n \n-    if (allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool) {\n-      return true; }\n-\n-    // For test purposes, configuration may have set a limit on the number of\n-    // batches in memory. Spill if we exceed this limit. (By default the number\n-    // of in-memory batches is unlimited.)\n-\n-    return bufferedBatches.size() > bufferedBatchLimit;\n+    return allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool;\n   }\n \n   /**\n@@ -1068,8 +1135,8 @@ private boolean isSpillNeeded(int incomingSize) {\n    */\n \n   private IterOutcome sortInMemory() {\n-    logger.info(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n-                bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n+    logger.debug(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n+                 bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n \n     // Note the difference between how we handle batches here and in the spill/merge\n     // case. In the spill/merge case, this class decides on the batch size to send\n@@ -1088,8 +1155,8 @@ private IterOutcome sortInMemory() {\n         sortState = SortState.DONE;\n         return IterOutcome.STOP;\n       } else {\n-        logger.info(\"Completed in-memory sort. Memory = {}\",\n-                allocator.getAllocatedMemory());\n+        logger.debug(\"Completed in-memory sort. Memory = {}\",\n+                     allocator.getAllocatedMemory());\n         resultsIterator = memoryMerge;\n         memoryMerge = null;\n         sortState = SortState.DELIVER;\n@@ -1111,9 +1178,9 @@ private IterOutcome sortInMemory() {\n    */\n \n   private IterOutcome mergeSpilledRuns() {\n-    logger.info(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n-                inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n-                bufferedBatches.size(), spilledRuns.size());\n+    logger.debug(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n+                 inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n+                 bufferedBatches.size(), spilledRuns.size());\n \n     // Consolidate batches to a number that can be merged in\n     // a single last pass.\n@@ -1132,7 +1199,8 @@ private IterOutcome mergeSpilledRuns() {\n     allBatches.addAll(spilledRuns);\n     spilledRuns.clear();\n \n-    logger.info(\"Starting merge phase. Runs = {}, Alloc. memory = {}\", allBatches.size(), allocator.getAllocatedMemory());\n+    logger.debug(\"Starting merge phase. Runs = {}, Alloc. memory = {}\",\n+                 allBatches.size(), allocator.getAllocatedMemory());\n \n     // Do the final merge as a results iterator.\n \n@@ -1153,9 +1221,13 @@ private boolean consolidateBatches() {\n \n     // Can't merge more than will fit into memory at one time.\n \n-    int maxMergeWidth = (int) (mergeMemoryPool / targetMergeBatchSize);\n+    int maxMergeWidth = (int) (mergeMemoryPool / targetSpillBatchSize);\n     maxMergeWidth = Math.min(mergeLimit, maxMergeWidth);\n \n+    // But, must merge at least two batches.\n+\n+    maxMergeWidth = Math.max(maxMergeWidth, 2);\n+\n     // If we can't fit all batches in memory, must spill any in-memory\n     // batches to make room for multiple spill-merge-spill cycles.\n \n@@ -1177,7 +1249,7 @@ private boolean consolidateBatches() {\n       // is available, spill some in-memory batches.\n \n       long allocated = allocator.getAllocatedMemory();\n-      long totalNeeds = spilledRunsCount * targetMergeBatchSize + allocated;\n+      long totalNeeds = spilledRunsCount * targetSpillBatchSize + allocated;\n       if (totalNeeds > mergeMemoryPool) {\n         spillFromMemory();\n         return true;\n@@ -1231,52 +1303,46 @@ private boolean consolidateBatches() {\n    * This method spills only half the accumulated batches\n    * minimizing unnecessary disk writes. The exact count must lie between\n    * the minimum and maximum spill counts.\n-    */\n+   */\n \n   private void spillFromMemory() {\n \n     // Determine the number of batches to spill to create a spill file\n     // of the desired size. The actual file size might be a bit larger\n     // or smaller than the target, which is expected.\n \n-    long estSize = 0;\n     int spillCount = 0;\n+    long spillSize = 0;\n     for (InputBatch batch : bufferedBatches) {\n-      estSize += batch.getDataSize();\n-      if (estSize > spillFileSize) {\n-        break; }\n+      long batchSize = batch.getDataSize();\n+      spillSize += batchSize;\n       spillCount++;\n+      if (spillSize + batchSize / 2 > spillFileSize) {\n+        break; }\n     }\n \n-    // Should not happen, but just to be sure...\n+    // Must always spill at least 2, even if this creates an over-size\n+    // spill file. But, if this is a final consolidation, we may have only\n+    // a single batch.\n \n-    if (spillCount == 0) {\n-      return; }\n+    spillCount = Math.max(spillCount, 2);\n+    spillCount = Math.min(spillCount, bufferedBatches.size());\n \n     // Do the actual spill.\n \n-    logger.trace(\"Starting spill from memory. Memory = {}, Buffered batch count = {}, Spill batch count = {}\",\n-                 allocator.getAllocatedMemory(), bufferedBatches.size(), spillCount);\n     mergeAndSpill(bufferedBatches, spillCount);\n   }\n \n   private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n-    if (count == 0) {\n-      return; }\n     spilledRuns.add(doMergeAndSpill(source, count));\n   }\n \n   private BatchGroup.SpilledRun doMergeAndSpill(LinkedList<? extends BatchGroup> batchGroups, int spillCount) {\n     List<BatchGroup> batchesToSpill = Lists.newArrayList();\n     spillCount = Math.min(batchGroups.size(), spillCount);\n     assert spillCount > 0 : \"Spill count to mergeAndSpill must not be zero\";\n-    long spillSize = 0;\n     for (int i = 0; i < spillCount; i++) {\n-      @SuppressWarnings(\"resource\")\n-      BatchGroup batch = batchGroups.pollFirst();\n-      assert batch != null : \"Encountered a null batch during merge and spill operation\";\n-      batchesToSpill.add(batch);\n-      spillSize += batch.getDataSize();\n+      batchesToSpill.add(batchGroups.pollFirst());\n     }\n \n     // Merge the selected set of matches and write them to the\n@@ -1288,8 +1354,11 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n     BatchGroup.SpilledRun newGroup = null;\n     try (AutoCloseable ignored = AutoCloseables.all(batchesToSpill);\n          CopierHolder.BatchMerger merger = copierHolder.startMerge(schema, batchesToSpill, spillBatchRowCount)) {\n-      logger.trace(\"Merging and spilling to {}\", outputFile);\n-      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext, spillSize);\n+      logger.trace(\"Spilling {} of {} batches, {} rows, memory = {}, write to {}\",\n+                   batchesToSpill.size(), bufferedBatches.size() + batchesToSpill.size(),\n+                   spillBatchRowCount,\n+                   allocator.getAllocatedMemory(), outputFile);\n+      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext);\n \n       // The copier will merge records from the buffered batches into\n       // the outputContainer up to targetRecordCount number of rows.\n@@ -1298,8 +1367,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       while (merger.next()) {\n \n         // Add a new batch of records (given by merger.getOutput()) to the spill\n-        // file, opening the file if not yet open, and creating the target\n-        // directory if it does not yet exist.\n+        // file.\n         //\n         // note that addBatch also clears the merger's output container\n \n@@ -1322,7 +1390,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       // It will release the memory in the close() call.\n \n       try {\n-        // Rethrow so we can organize how to handle the error.\n+        // Rethrow so we can decide how to handle the error.\n \n         throw e;\n       }\n@@ -1444,11 +1512,12 @@ public void close() {\n     } catch (RuntimeException e) {\n       ex = (ex == null) ? e : ex;\n     }\n-    try {\n-      allocator.close();\n-    } catch (RuntimeException e) {\n-      ex = (ex == null) ? e : ex;\n-    }\n+    // Note: allocator is closed by the FragmentManager\n+//    try {\n+//      allocator.close();\n+//    } catch (RuntimeException e) {\n+//      ex = (ex == null) ? e : ex;\n+//    }\n     if (ex != null) {\n       throw ex;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "sha": "a1162a02ecade691ba3b1054e0e1951b60d42517",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "patch": "@@ -78,6 +78,7 @@ public void clear() {\n   }\n \n \n+  @SuppressWarnings(\"resource\")\n   @Override\n   public VectorWrapper<?> getChildWrapper(int[] ids) {\n     if (ids.length == 1) {\n@@ -108,4 +109,13 @@ public void transfer(VectorWrapper<?> destination) {\n     vector.makeTransferPair(((SimpleVectorWrapper<?>)destination).vector).transfer();\n   }\n \n+  @Override\n+  public String toString() {\n+    if (vector == null) {\n+      return \"null\";\n+    } else {\n+      return vector.toString();\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "sha": "0a9f3d6e129b5cfcef7274b8cd3c8de06b1523bf",
                "status": "modified"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/FixedValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 73,
                "filename": "exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "patch": "@@ -69,7 +69,7 @@ public int getBufferSizeFor(final int valueCount) {\n \n   @Override\n   public int getValueCapacity(){\n-    return (int) (data.capacity() *1.0 / ${type.width});\n+    return data.capacity() / ${type.width};\n   }\n \n   @Override\n@@ -196,7 +196,7 @@ public void load(SerializedField metadata, DrillBuf buffer) {\n     data = buffer.slice(0, actualLength);\n     data.retain(1);\n     data.writerIndex(actualLength);\n-    }\n+  }\n \n   public TransferPair getTransferPair(BufferAllocator allocator){\n     return new TransferImpl(getField(), allocator);\n@@ -227,6 +227,11 @@ public void splitAndTransferTo(int startIndex, int length, ${minor.class}Vector\n     target.data.writerIndex(sliceLength);\n   }\n \n+  @Override\n+  public int getPayloadByteCount() {\n+    return getAccessor().getValueCount() * ${type.width};\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     private ${minor.class}Vector to;\n \n@@ -390,7 +395,6 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       return p.plusDays(days).plusMillis(millis);\n     }\n \n-\n     public StringBuilder getAsStringBuilder(int index) {\n       final int offsetIndex = index * ${type.width};\n \n@@ -539,6 +543,7 @@ public DateTime getObject(int index) {\n     public ${friendlyType} getObject(int index) {\n       return get(index);\n     }\n+\n     public ${minor.javaType!type.javaType} getPrimitiveObject(int index) {\n       return get(index);\n     }\n@@ -557,9 +562,7 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       holder.isSet = 1;\n       holder.value = data.get${(minor.javaType!type.javaType)?cap_first}(index * ${type.width});\n     }\n-\n-\n-   </#if> <#-- type.width -->\n+    </#if> <#-- type.width -->\n  }\n \n  /**\n@@ -728,84 +731,84 @@ public void generateTestData(int count) {\n    }\n \n    <#else> <#-- type.width <= 8 -->\n-   public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n-   }\n+    public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n+    }\n \n    public void setSafe(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n      while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, value);\n-   }\n-\n-   protected void set(int index, ${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+        reAlloc();\n+      }\n+      set(index, value);\n+    }\n \n-   public void setSafe(int index, ${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, ${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   protected void set(int index, Nullable${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+    public void setSafe(int index, ${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void setSafe(int index, Nullable${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, Nullable${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   @Override\n-   public void generateTestData(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n-       }else{\n-         set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n-       }\n-     }\n-   }\n+    public void setSafe(int index, Nullable${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void generateTestDataAlt(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, (${(minor.javaType!type.javaType)}) 1);\n-       }else{\n-         set(i, (${(minor.javaType!type.javaType)}) 0);\n-       }\n-     }\n-   }\n+    @Override\n+    public void generateTestData(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n+        } else {\n+          set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n+        }\n+      }\n+    }\n+\n+    public void generateTestDataAlt(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, (${(minor.javaType!type.javaType)}) 1);\n+        } else {\n+          set(i, (${(minor.javaType!type.javaType)}) 0);\n+        }\n+      }\n+    }\n \n   </#if> <#-- type.width -->\n \n-   @Override\n-   public void setValueCount(int valueCount) {\n-     final int currentValueCapacity = getValueCapacity();\n-     final int idx = (${type.width} * valueCount);\n-     while(valueCount > getValueCapacity()) {\n-       reAlloc();\n-     }\n-     if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n-       incrementAllocationMonitor();\n-     } else if (allocationMonitor > 0) {\n-       allocationMonitor = 0;\n-     }\n-     VectorTrimmer.trim(data, idx);\n-     data.writerIndex(valueCount * ${type.width});\n-   }\n- }\n+    @Override\n+    public void setValueCount(int valueCount) {\n+      final int currentValueCapacity = getValueCapacity();\n+      final int idx = (${type.width} * valueCount);\n+      while(valueCount > getValueCapacity()) {\n+        reAlloc();\n+      }\n+      if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n+        incrementAllocationMonitor();\n+      } else if (allocationMonitor > 0) {\n+        allocationMonitor = 0;\n+      }\n+      VectorTrimmer.trim(data, idx);\n+      data.writerIndex(valueCount * ${type.width});\n+    }\n+  }\n }\n \n </#if> <#-- type.major -->",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "sha": "b2a5dc389601dabde82592b3fd07621f352274e2",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/NullableValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "patch": "@@ -45,12 +45,24 @@\n  * NB: this class is automatically generated from ${.template_name} and ValueVectorTypes.tdd using FreeMarker.\n  */\n @SuppressWarnings(\"unused\")\n-public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector{\n+public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(${className}.class);\n \n   private final FieldReader reader = new Nullable${minor.class}ReaderImpl(Nullable${minor.class}Vector.this);\n \n   private final MaterializedField bitsField = MaterializedField.create(\"$bits$\", Types.required(MinorType.UINT1));\n+\n+  /**\n+   * Set value flag. Meaning:\n+   * <ul>\n+   * <li>0: value is not set (value is null).</li>\n+   * <li>1: value is set (value is not null).</li>\n+   * </ul>\n+   * That is, a 1 means that the values vector has a value. 0\n+   * means that the vector is null. Thus, all values start as\n+   * not set (null) and must be explicitly set (made not null).\n+   */\n+\n   private final UInt1Vector bits = new UInt1Vector(bitsField, allocator);\n   private final ${valuesName} values = new ${minor.class}Vector(field, allocator);\n \n@@ -108,8 +120,8 @@ public int getBufferSizeFor(final int valueCount) {\n       return 0;\n     }\n \n-    return values.getBufferSizeFor(valueCount)\n-        + bits.getBufferSizeFor(valueCount);\n+    return values.getBufferSizeFor(valueCount) +\n+           bits.getBufferSizeFor(valueCount);\n   }\n \n   @Override\n@@ -163,6 +175,18 @@ public boolean allocateNewSafe() {\n     return success;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return bits.getAllocatedByteCount() + values.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // For nullable, we include all values, null or not, in computing\n+    // the value length.\n+    return bits.getPayloadByteCount() + values.getPayloadByteCount();\n+  }\n+\n   <#if type.major == \"VarLen\">\n   @Override\n   public void allocateNew(int totalBytes, int valueCount) {",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "sha": "b242728078ebfa734cd4a8a868edbeba4e665949",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/UnionVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/codegen/templates/UnionVector.java",
                "patch": "@@ -201,6 +201,22 @@ public MaterializedField getField() {\n     return field;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Most vectors are held inside the internal map.\n+\n+    int count = internalMap.getAllocatedByteCount();\n+    if (bit != null) {\n+      count += bit.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return internalMap.getPayloadByteCount();\n+  }\n+\n   @Override\n   public TransferPair getTransferPair(BufferAllocator allocator) {\n     return new TransferImpl(field, allocator);",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "sha": "93854e782a59a445b2e854161248a61d83920164",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/VariableLengthVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "patch": "@@ -238,6 +238,25 @@ public boolean copyFromSafe(int fromIndex, int thisIndex, ${minor.class}Vector f\n     return true;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsetVector.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    UInt${type.width}Vector.Accessor a = offsetVector.getAccessor();\n+    int count = a.getValueCount();\n+    if (count == 0) {\n+      return 0;\n+    } else {\n+      // If 1 or more values, then the last value is set to\n+      // the offset of the next value, which is the same as\n+      // the length of existing values.\n+      return a.get(count-1);\n+    }\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     ${minor.class}Vector to;\n ",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "sha": "ea3c9de7c90ab37adfe7e24f8dcac2bc7f42c2f9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "patch": "@@ -87,4 +87,9 @@ public DrillBuf getBuffer() {\n    * the value vector. The purpose is to move the value vector to a \"mutate\" state\n    */\n   public void reset() {}\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return data.capacity();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "sha": "4def5b83761f8a90b872e2df2315c1035d687658",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "patch": "@@ -449,4 +449,10 @@ public void clear() {\n     this.valueCount = 0;\n     super.clear();\n   }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // One byte per value\n+    return valueCount;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "sha": "a6c0ceafdaefc5c6245beed98b31a75d6605acfb",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "patch": "@@ -218,4 +218,16 @@ public void get(int index, ObjectHolder holder){\n       holder.obj = getObject(index);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "sha": "f69dc9807165c46d6205296ce542131149122d1b",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "patch": "@@ -175,6 +175,18 @@\n    */\n   void load(SerializedField metadata, DrillBuf buffer);\n \n+  /**\n+   * Return the total memory consumed by all buffers within this vector.\n+   */\n+\n+  int getAllocatedByteCount();\n+\n+  /**\n+   * Return the number of value bytes consumed by actual data.\n+   */\n+\n+  int getPayloadByteCount();\n+\n   /**\n    * An abstraction that is used to read from this vector instance.\n    */",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "sha": "f4c793556f603e945eee08b2aebeb141d6a495b1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "patch": "@@ -17,9 +17,7 @@\n  */\n package org.apache.drill.exec.vector;\n \n-import io.netty.buffer.DrillBuf;\n-\n-public interface VariableWidthVector extends ValueVector{\n+public interface VariableWidthVector extends ValueVector {\n \n   /**\n    * Allocate a new memory space for this vector.  Must be called prior to using the ValueVector.",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "sha": "d04234c9f762c3683a78f8a7b7fe336e6afc9c86",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "patch": "@@ -176,4 +176,14 @@ public FieldReader getReader() {\n \n   @Override\n   public void load(UserBitShared.SerializedField metadata, DrillBuf buffer) { }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return 0;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "sha": "9181f2042fa6a3b292327fd9129faeb415ca1b38",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "patch": "@@ -266,7 +266,7 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n \n   @Override\n   public int getBufferSize() {\n-    int actualBufSize = 0 ;\n+    int actualBufSize = 0;\n \n     for (final ValueVector v : vectors.values()) {\n       for (final DrillBuf buf : v.getBuffers(false)) {\n@@ -275,4 +275,24 @@ public int getBufferSize() {\n     }\n     return actualBufSize;\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getPayloadByteCount();\n+    }\n+    return count;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "sha": "baba0865d8956fd835fa5e61600e69d71771afd3",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "patch": "@@ -209,6 +209,17 @@ protected void replaceDataVector(ValueVector v) {\n     vector = v;\n   }\n \n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + vector.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + vector.getPayloadByteCount();\n+  }\n+\n   public abstract class BaseRepeatedAccessor extends BaseValueVector.BaseAccessor implements RepeatedAccessor {\n \n     @Override\n@@ -256,5 +267,4 @@ public void setValueCount(int valueCount) {\n       vector.getMutator().setValueCount(childValueCount);\n     }\n   }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "sha": "1664b0a2930bc39c6643b93842658c6e09e84982",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "patch": "@@ -317,4 +317,14 @@ public void setValueCount(int valueCount) {\n       bits.getMutator().setValueCount(valueCount);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + bits.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + bits.getPayloadByteCount() + super.getPayloadByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "sha": "f71baa7e3c81a992f30cc24b49cedc77bf5ee603",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "patch": "@@ -426,4 +426,14 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n   public void copyFromSafe(int fromIndex, int thisIndex, RepeatedListVector from) {\n     delegate.copyFromSafe(fromIndex, thisIndex, from.delegate);\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return delegate.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return delegate.getPayloadByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "sha": "b5c97bf31bdba186be36560f546275adb3afda6f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "patch": "@@ -584,4 +584,9 @@ public void clear() {\n       vector.clear();\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return super.getAllocatedByteCount( ) + offsets.getAllocatedByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "sha": "3707ff0d3d6060c31fa3732e59ecc3acb268566d",
                "status": "modified"
            }
        ],
        "message": "DRILL-5284: Roll-up of final fixes for managed sort\n\nSee subtasks for details.\n\n* Provide detailed, accurate estimate of size consumed by a record batch\n* Managed external sort spills too often with Parquet data\n* Managed External Sort fails with OOM\n* External sort refers to the deprecated HDFS fs.default.name param\n* Config param drill.exec.sort.external.batch.size is not used\n* NPE in managed external sort while spilling to disk\n* External Sort BatchGroup leaks memory if an OOM occurs during read\n* DRILL-5294: Under certain low-memory conditions, need to force the sort to merge\ntwo batches to make progress, even though this is a bit more than\ncomfortably fits into memory.\n\nclose #761",
        "parent": "https://github.com/apache/drill/commit/69de3a1e409bb1fb9a25e679ce1750d9f9daf238",
        "patched_files": [
            "ValueVector.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestValueVector.java"
        ]
    },
    "drill_826fc5b": {
        "bug_id": "drill_826fc5b",
        "commit": "https://github.com/apache/drill/commit/826fc5b9c2a6f044101967a9a2e49b20af2dae76",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "patch": "@@ -344,7 +344,10 @@ private void setValueCountAndPopulatePartitionVectors(int recordCount) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (Exception e) {\n       logger.warn(\"Failure while closing Hive Record reader.\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "sha": "3c8b9ba419368f569cd463ee825354d859e9a073",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "patch": "@@ -144,7 +144,10 @@ public int next() {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception while closing stream.\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "sha": "254e0d89b761020aa9a44beb8141df4921d1c59e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 7,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "patch": "@@ -455,17 +455,22 @@ public void cleanup() {\n     // enable this for debugging when it is know that a whole file will be read\n     // limit kills upstream operators once it has enough records, so this assert will fail\n //    assert totalRecordsRead == footer.getBlocks().get(rowGroupIndex).getRowCount();\n-    for (ColumnReader column : columnStatuses) {\n-      column.clear();\n+    if (columnStatuses != null) {\n+      for (ColumnReader column : columnStatuses) {\n+        column.clear();\n+      }\n+      columnStatuses.clear();\n+      columnStatuses = null;\n     }\n-    columnStatuses.clear();\n \n     codecFactory.close();\n \n-    for (VarLengthColumn r : varLengthReader.columns) {\n-      r.clear();\n+    if (varLengthReader != null) {\n+      for (VarLengthColumn r : varLengthReader.columns) {\n+        r.clear();\n+      }\n+      varLengthReader.columns.clear();\n+      varLengthReader = null;\n     }\n-    varLengthReader.columns.clear();\n   }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "sha": "2f07fb3527dbc4e4de25f8af07bba7c1c85d88f8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "patch": "@@ -342,7 +342,10 @@ private int getPercentFilled() {\n   @Override\n   public void cleanup() {\n     try {\n-      pageReadStore.close();\n+      if (pageReadStore != null) {\n+        pageReadStore.close();\n+        pageReadStore = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Failure while closing PageReadStore\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "sha": "4e7d6288f19ec48556d2532a4b81827b37754c80",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "patch": "@@ -231,7 +231,10 @@ public int find(Text text, byte delimiter, int start) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception closing reader: {}\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "sha": "e25bd74084c25188d20b0cfd91a890fa816ede1d",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "patch": "@@ -59,6 +59,10 @@\n import com.google.common.base.Preconditions;\n import com.google.common.io.Resources;\n \n+import static org.hamcrest.core.StringContains.containsString;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertThat;\n+\n public class BaseTestQuery extends ExecTest {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BaseTestQuery.class);\n \n@@ -319,7 +323,7 @@ protected static void testNoResult(String query, Object... args) throws Exceptio\n \n   protected static void testNoResult(int interation, String query, Object... args) throws Exception {\n     query = String.format(query, args);\n-    logger.debug(\"Running query:\\n--------------\\n\"+query);\n+    logger.debug(\"Running query:\\n--------------\\n\" + query);\n     for (int i = 0; i < interation; i++) {\n       List<QueryDataBatch> results = client.runQuery(QueryType.SQL, query);\n       for (QueryDataBatch queryDataBatch : results) {\n@@ -364,6 +368,24 @@ protected static void testSqlFromFile(String file) throws Exception{\n     test(getFile(file));\n   }\n \n+  /**\n+   * Utility method which tests given query produces a {@link UserException} and the exception message contains\n+   * the given message.\n+   * @param testSqlQuery Test query\n+   * @param expectedErrorMsg Expected error message.\n+   */\n+  protected static void errorMsgTestHelper(final String testSqlQuery, final String expectedErrorMsg) throws Exception {\n+    UserException expException = null;\n+    try {\n+      test(testSqlQuery);\n+    } catch (final UserException ex) {\n+      expException = ex;\n+    }\n+\n+    assertNotNull(\"Expected a UserException\", expException);\n+    assertThat(expException.getMessage(), containsString(expectedErrorMsg));\n+  }\n+\n   public static String getFile(String resource) throws IOException{\n     URL url = Resources.getResource(resource);\n     if (url == null) {",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "sha": "db1ed348a7edc7c1ee3b92491e34dbce7047d27a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 4,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.drill;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.util.FileUtils;\n+import org.apache.drill.exec.work.ExecErrorConstants;\n import org.apache.drill.exec.work.foreman.SqlUnsupportedException;\n import org.apache.drill.exec.work.foreman.UnsupportedDataTypeException;\n import org.apache.drill.exec.work.foreman.UnsupportedFunctionException;\n@@ -355,13 +356,15 @@ public void testFlattenWithinDistinct() throws Exception {\n     }\n   }\n \n-  @Test(expected =  UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalCasts() throws Exception {\n-    test(\"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\");\n+    final String query = \"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n \n-  @Test(expected = UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalFromParquet() throws Exception {\n-    test(\"select * from cp.`parquet/decimal_dictionary.parquet`\");\n+    final String query = \"select * from cp.`parquet/decimal_dictionary.parquet`\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "sha": "adbf653efdd5fe04489460ac3634e1352ad3bb00",
                "status": "modified"
            }
        ],
        "message": "DRILL-3017: Safeguard against NPEs in RecordReader.cleanup()s",
        "parent": "https://github.com/apache/drill/commit/6076cc6435f7decb17856991c25dac77dfe0b203",
        "patched_files": [
            "HiveRecordReader.java",
            "BaseTestQuery.java",
            "DrillParquetReader.java",
            "DrillTextRecordReader.java",
            "ParquetRecordReader.java",
            "CompliantTextRecordReader.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "ParquetRecordReaderTest.java",
            "TestDisabledFunctionality.java",
            "TestDrillParquetReader.java"
        ]
    },
    "drill_9dec514": {
        "bug_id": "drill_9dec514",
        "commit": "https://github.com/apache/drill/commit/9dec5144f669b1fb97208a0b9c848af14b7cbccf",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java?ref=9dec5144f669b1fb97208a0b9c848af14b7cbccf",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "patch": "@@ -98,7 +98,7 @@\n   private DrillbitStateManager stateManager;\n   private boolean quiescentMode;\n   private boolean forcefulShutdown = false;\n-  GracefulShutdownThread gracefulShutdownThread;\n+  private GracefulShutdownThread gracefulShutdownThread;\n   private boolean interruptPollShutdown = true;\n \n   public void setQuiescentMode(boolean quiescentMode) {\n@@ -196,6 +196,7 @@ public int getWebServerPort() {\n   public void run() throws Exception {\n     final Stopwatch w = Stopwatch.createStarted();\n     logger.debug(\"Startup begun.\");\n+    gracefulShutdownThread = new GracefulShutdownThread(this, new StackTrace());\n     coord.start(10000);\n     stateManager.setState(DrillbitState.ONLINE);\n     storeProvider.start();\n@@ -222,7 +223,6 @@ public void run() throws Exception {\n     drillbitContext.startRM();\n \n     Runtime.getRuntime().addShutdownHook(new ShutdownThread(this, new StackTrace()));\n-    gracefulShutdownThread = new GracefulShutdownThread(this, new StackTrace());\n     gracefulShutdownThread.start();\n     logger.info(\"Startup completed ({} ms).\", w.elapsed(TimeUnit.MILLISECONDS));\n   }\n@@ -470,6 +470,11 @@ public DrillbitContext getContext() {\n     return manager.getContext();\n   }\n \n+  @VisibleForTesting\n+  public GracefulShutdownThread getGracefulShutdownThread() {\n+    return gracefulShutdownThread;\n+  }\n+\n   public static void main(final String[] cli) throws DrillbitStartupException {\n     final StartupOptions options = StartupOptions.parse(cli);\n     start(options);",
                "raw_url": "https://github.com/apache/drill/raw/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/main/java/org/apache/drill/exec/server/Drillbit.java",
                "sha": "8d413977f5c2587eea26ab0b438e17836ee54eb6",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/drill/blob/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java?ref=9dec5144f669b1fb97208a0b9c848af14b7cbccf",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "patch": "@@ -17,9 +17,11 @@\n  */\n package org.apache.drill.test;\n import org.apache.drill.categories.SlowTest;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n import org.apache.drill.exec.server.Drillbit;\n+import org.apache.hadoop.net.ServerSocketUtil;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n@@ -36,6 +38,9 @@\n import java.nio.file.Path;\n import java.util.Collection;\n \n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.fail;\n \n @Category({SlowTest.class})\n@@ -174,6 +179,44 @@ public void testRestApiShutdown() throws Exception {\n     }\n   }\n \n+  @Test // DRILL-6912\n+  public void gracefulShutdownThreadShouldBeInitializedBeforeClosingDrillbit() throws Exception {\n+    Drillbit drillbit = null;\n+    Drillbit drillbitWithSamePort = null;\n+\n+    int userPort = ServerSocketUtil.getPort(31170, 300);\n+    int bitPort = ServerSocketUtil.getPort(31180, 300);\n+    ClusterFixtureBuilder fixtureBuilder = ClusterFixture.bareBuilder(dirTestWatcher).withLocalZk()\n+        .configProperty(ExecConstants.INITIAL_USER_PORT, userPort)\n+        .configProperty(ExecConstants.INITIAL_BIT_PORT, bitPort);\n+    try (ClusterFixture clusterFixture = fixtureBuilder.build()) {\n+      drillbit = clusterFixture.drillbit();\n+\n+      // creating another drillbit instance using same config\n+      drillbitWithSamePort = new Drillbit(clusterFixture.config(), fixtureBuilder.configBuilder().getDefinitions(),\n+          clusterFixture.serviceSet());\n+\n+      try {\n+        drillbitWithSamePort.run();\n+        fail(\"drillbitWithSamePort.run() should throw UserException\");\n+      } catch (UserException e) {\n+        // it's expected that second drillbit can't be started because port is busy\n+        assertThat(e.getMessage(), containsString(\"RESOURCE ERROR: Drillbit could not bind to port\"));\n+      }\n+    } finally {\n+      // preconditions\n+      assertNotNull(drillbit);\n+      assertNotNull(drillbitWithSamePort);\n+      assertNotNull(\"gracefulShutdownThread should be initialized, otherwise NPE will be thrown from close()\",\n+          drillbit.getGracefulShutdownThread());\n+      // main test case\n+      assertNotNull(\"gracefulShutdownThread should be initialized, otherwise NPE will be thrown from close()\",\n+          drillbitWithSamePort.getGracefulShutdownThread());\n+      drillbit.close();\n+      drillbitWithSamePort.close();\n+    }\n+  }\n+\n   private static boolean waitAndAssertDrillbitCount(ClusterFixture cluster, int zkRefresh) throws InterruptedException {\n \n     while (true) {",
                "raw_url": "https://github.com/apache/drill/raw/9dec5144f669b1fb97208a0b9c848af14b7cbccf/exec/java-exec/src/test/java/org/apache/drill/test/TestGracefulShutdown.java",
                "sha": "9d649ba452130d42ccbd16741f10e822ba74ca52",
                "status": "modified"
            }
        ],
        "message": "DRILL-6912: NPE when other drillbit is already running\n\ncloses #1577",
        "parent": "https://github.com/apache/drill/commit/1df935f8f4ed0635ceb38f42fc5b5387c49bdf73",
        "patched_files": [
            "Drillbit.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestGracefulShutdown.java"
        ]
    },
    "drill_a26fbec": {
        "bug_id": "drill_a26fbec",
        "commit": "https://github.com/apache/drill/commit/a26fbec13134f249e258be6735b82cf09ab1f406",
        "file": [
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "patch": "@@ -29,6 +29,9 @@\n import freemarker.cache.WebappTemplateLoader;\n import freemarker.core.HTMLOutputFormat;\n import freemarker.template.Configuration;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n import org.apache.drill.common.config.DrillConfig;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.memory.BufferAllocator;\n@@ -108,10 +111,17 @@ public DrillRestServer(final WorkManager workManager, final ServletContext servl\n     provider.setMapper(workManager.getContext().getLpPersistence().getMapper());\n     register(provider);\n \n+    // Get an EventExecutor out of the BitServer EventLoopGroup to notify listeners for WebUserConnection. For\n+    // actual connections between Drillbits this EventLoopGroup is used to handle network related events. Though\n+    // there is no actual network connection associated with WebUserConnection but we need a CloseFuture in\n+    // WebSessionResources, so we are using EvenExecutor from network EventLoopGroup pool.\n+    final EventExecutor executor = workManager.getContext().getBitLoopGroup().next();\n+\n     register(new AbstractBinder() {\n       @Override\n       protected void configure() {\n         bind(workManager).to(WorkManager.class);\n+        bind(executor).to(EventExecutor.class);\n         bind(workManager.getContext().getLpPersistence().getMapper()).to(ObjectMapper.class);\n         bind(workManager.getContext().getStoreProvider()).to(PersistentStoreProvider.class);\n         bind(workManager.getContext().getStorage()).to(StoragePluginRegistry.class);\n@@ -159,6 +169,9 @@ private Configuration getFreemarkerConfiguration(ServletContext servletContext)\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -204,9 +217,15 @@ public WebUserConnection provide() {\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_RESERVATION),\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_MAXIMUM));\n \n+        // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+        // listener to known about channel close event from underlying layer. We use this future to notify Foreman\n+        // listeners when the Web session (not connection) between Web Client and WebServer is closed. This will help\n+        // Foreman to cancel all the running queries for this Web Client.\n+        final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n         // Create a WebSessionResource instance which owns the lifecycle of all the session resources.\n-        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed.\n-        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession);\n+        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed\n+        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession, closeFuture);\n         session.setAttribute(WebSessionResources.class.getSimpleName(), webSessionResources);\n       }\n       // Create a new WebUserConnection for the request\n@@ -227,6 +246,9 @@ public void dispose(WebUserConnection instance) {\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -260,8 +282,15 @@ public WebUserConnection provide() {\n         logger.trace(\"Failed to get the remote address of the http session request\", ex);\n       }\n \n-      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator,\n-              remoteAddress, drillUserSession);\n+      // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+      // listener to known about channel close event from underlying layer.\n+      //\n+      // The invocation of this close future is no-op as it will be triggered after query completion in unsecure case.\n+      // But we need this close future as it's expected by Foreman.\n+      final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress,\n+          drillUserSession, closeFuture);\n \n       // Create a AnonWenUserConnection for this request\n       return new AnonWebUserConnection(webSessionResources);",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "sha": "15458478df958d7923a2ad968cf754c2808d4ee8",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "patch": "@@ -19,7 +19,6 @@\n package org.apache.drill.exec.server.rest;\n \n import io.netty.channel.ChannelPromise;\n-import io.netty.channel.DefaultChannelPromise;\n import org.apache.drill.common.AutoCloseables;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.rpc.ChannelClosedException;\n@@ -43,11 +42,12 @@\n \n   private ChannelPromise closeFuture;\n \n-  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress, UserSession userSession) {\n+  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress,\n+                      UserSession userSession, ChannelPromise closeFuture) {\n     this.allocator = allocator;\n     this.remoteAddress = remoteAddress;\n     this.webUserSession = userSession;\n-    closeFuture = new DefaultChannelPromise(null);\n+    this.closeFuture = closeFuture;\n   }\n \n   public UserSession getSession() {\n@@ -68,16 +68,20 @@ public SocketAddress getRemoteAddress() {\n \n   @Override\n   public void close() {\n-\n     try {\n       AutoCloseables.close(webUserSession, allocator);\n     } catch (Exception ex) {\n       logger.error(\"Failure while closing the session resources\", ex);\n     }\n \n-    // Set the close future associated with this session.\n+    // Notify all the listeners of this closeFuture for failure events so that listeners can do cleanup related to this\n+    // WebSession. This will be called after every query execution by AnonymousWebUserConnection::cleanupSession and\n+    // for authenticated user it is called when session is invalidated.\n+    // For authenticated user it will cancel the in-flight queries based on session invalidation. Whereas for\n+    // unauthenticated user it's a no-op since there is no session associated with it. We don't have mechanism currently\n+    // to call this close future upon Http connection close.\n     if (closeFuture != null) {\n-      closeFuture.setFailure(new ChannelClosedException(\"Http Session of the user is closed.\"));\n+      closeFuture.setFailure(new ChannelClosedException(\"Http connection is closed by Web Client\"));\n       closeFuture = null;\n     }\n   }",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "sha": "2ca457c02e142e6ca6c53d1d37398436690484ac",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "patch": "@@ -42,9 +42,14 @@\n import java.util.Set;\n \n /**\n- * WebUserConnectionWrapper which represents the UserClientConnection for the WebUser submitting the query. It provides\n- * access to the UserSession executing the query. There is no actual physical channel corresponding to this connection\n- * wrapper.\n+ * WebUserConnectionWrapper which represents the UserClientConnection between WebServer and Foreman, for the WebUser\n+ * submitting the query. It provides access to the UserSession executing the query. There is no actual physical\n+ * channel corresponding to this connection wrapper.\n+ *\n+ * It returns a close future with no actual underlying {@link io.netty.channel.Channel} associated with it but do have an\n+ * EventExecutor out of BitServer EventLoopGroup. Since there is no actual connection established using this class,\n+ * hence the close event will never be fired by underlying layer and close future is set only when the\n+ * {@link WebSessionResources} are closed.\n  */\n \n public class WebUserConnection extends AbstractDisposableUserClientConnection implements ConnectionThrottle {",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "sha": "f46b5e5e69b4758bb3866e84965213feefdd4648",
                "status": "modified"
            },
            {
                "additions": 168,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "patch": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.server.rest;\n+\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n+import io.netty.util.concurrent.Future;\n+import io.netty.util.concurrent.GenericFutureListener;\n+import org.apache.drill.exec.memory.BufferAllocator;\n+import org.apache.drill.exec.rpc.TransportCheck;\n+import org.apache.drill.exec.rpc.user.UserSession;\n+import org.junit.Test;\n+\n+import java.net.SocketAddress;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+/**\n+ * Validates {@link WebSessionResources} close works as expected w.r.t {@link io.netty.channel.AbstractChannel.CloseFuture}\n+ * associated with it.\n+ */\n+public class WebSessionResourcesTest {\n+  //private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WebSessionResourcesTest.class);\n+\n+  private WebSessionResources webSessionResources;\n+\n+  private boolean listenerComplete;\n+\n+  private CountDownLatch latch;\n+\n+  private EventExecutor executor;\n+\n+  // A close listener added in close future in one of the test to see if it's invoked correctly.\n+  private class TestClosedListener implements GenericFutureListener<Future<Void>> {\n+    @Override\n+    public void operationComplete(Future<Void> future) throws Exception {\n+      listenerComplete = true;\n+      latch.countDown();\n+    }\n+  }\n+\n+  /**\n+   * Validates {@link WebSessionResources#close()} throws NPE when closefuture passed to WebSessionResources doesn't\n+   * have a valid channel and EventExecutor associated with it.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithNullExecutor() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      fail();\n+    } catch (Exception e) {\n+      assertTrue(e instanceof NullPointerException);\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and other parameters.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithValidExecutor() throws Exception {\n+    try {\n+      EventExecutor mockExecutor = mock(EventExecutor.class);\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mockExecutor);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      verify(mockExecutor).inEventLoop();\n+      verify(mockExecutor).execute(any(Runnable.class));\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+      assertTrue(!listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates double call to {@link WebSessionResources#close()} doesn't throw any exception.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDoubleClose() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mock(EventExecutor.class));\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      webSessionResources.close();\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and {@link TestClosedListener}\n+   * getting invoked which is added to the close future.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testCloseWithListener() throws Exception {\n+    try {\n+      // Assign latch, executor and closeListener for this test case\n+      GenericFutureListener<Future<Void>> closeListener = new TestClosedListener();\n+      latch = new CountDownLatch(1);\n+      executor = TransportCheck.createEventLoopGroup(1, \"Test-Thread\").next();\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      // create WebSessionResources with above ChannelPromise to notify listener\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class),\n+          mock(UserSession.class), closeFuture);\n+\n+      // Add the Test Listener to close future\n+      assertTrue(!listenerComplete);\n+      closeFuture.addListener(closeListener);\n+\n+      // Close the WebSessionResources\n+      webSessionResources.close();\n+\n+      // Verify the states\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      // Since listener will be invoked so test should not wait forever\n+      latch.await();\n+      assertTrue(listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    } finally {\n+      listenerComplete = false;\n+      executor.shutdownGracefully();\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "sha": "bb990de6ef28abac7e9c862a508784b7997a9dcb",
                "status": "added"
            }
        ],
        "message": "DRILL-5874: NPE in AnonWebUserConnection.cleanupSession()\n\ncloses #993",
        "parent": "https://github.com/apache/drill/commit/8eda4d7749c129c692f9e57db4c2a755a9139052",
        "patched_files": [
            "DrillRestServer.java",
            "WebSessionResources.java",
            "WebUserConnection.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "WebSessionResourcesTest.java"
        ]
    },
    "drill_aa127b7": {
        "bug_id": "drill_aa127b7",
        "commit": "https://github.com/apache/drill/commit/aa127b70b1e46f7f4aa19881f25eda583627830a",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/drill/blob/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java?ref=aa127b70b1e46f7f4aa19881f25eda583627830a",
                "deletions": 23,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "patch": "@@ -32,10 +32,12 @@\n import org.apache.drill.exec.physical.PhysicalPlan;\n import org.apache.drill.exec.planner.sql.DirectPlan;\n import org.apache.drill.exec.planner.sql.SchemaUtilites;\n+import org.apache.drill.exec.store.AbstractSchema;\n import org.apache.drill.exec.store.StoragePlugin;\n import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n import org.apache.drill.exec.store.dfs.FileSystemSchemaFactory;\n import org.apache.drill.exec.store.dfs.WorkspaceConfig;\n+import org.apache.drill.exec.work.foreman.ForemanSetupException;\n \n import java.util.List;\n import java.util.Map;\n@@ -68,33 +70,38 @@ public SerializableString getEscapeSequence(int i) {\n \n \n   @Override\n-  public PhysicalPlan getPlan(SqlNode sqlNode) {\n-    SqlIdentifier schema = ((SqlDescribeSchema) sqlNode).getSchema();\n-    SchemaPlus drillSchema = SchemaUtilites.findSchema(config.getConverter().getDefaultSchema(), schema.names);\n-\n-    if (drillSchema != null) {\n-      StoragePlugin storagePlugin;\n-      try {\n-        storagePlugin = context.getStorage().getPlugin(schema.names.get(0));\n-      } catch (ExecutionSetupException e) {\n-        throw new DrillRuntimeException(\"Failure while retrieving storage plugin\", e);\n+  public PhysicalPlan getPlan(SqlNode sqlNode) throws ForemanSetupException {\n+    SqlIdentifier schema = unwrap(sqlNode, SqlDescribeSchema.class).getSchema();\n+    SchemaPlus schemaPlus = SchemaUtilites.findSchema(config.getConverter().getDefaultSchema(), schema.names);\n+\n+    if (schemaPlus == null) {\n+      throw UserException.validationError()\n+        .message(\"Invalid schema name [%s]\", Joiner.on(\".\").join(schema.names))\n+        .build(logger);\n+    }\n+\n+    StoragePlugin storagePlugin;\n+    try {\n+      AbstractSchema drillSchema = SchemaUtilites.unwrapAsDrillSchemaInstance(schemaPlus);\n+      storagePlugin = context.getStorage().getPlugin(drillSchema.getSchemaPath().get(0));\n+      if (storagePlugin == null) {\n+        throw new DrillRuntimeException(String.format(\"Unable to find storage plugin with the following name [%s].\",\n+          drillSchema.getSchemaPath().get(0)));\n       }\n-      String properties;\n-      try {\n-        final Map configMap = mapper.convertValue(storagePlugin.getConfig(), Map.class);\n-        if (storagePlugin instanceof FileSystemPlugin) {\n-          transformWorkspaces(schema.names, configMap);\n-        }\n-        properties = mapper.writeValueAsString(configMap);\n-      } catch (JsonProcessingException e) {\n-        throw new DrillRuntimeException(\"Error while trying to convert storage config to json string\", e);\n+    } catch (ExecutionSetupException e) {\n+      throw new DrillRuntimeException(\"Failure while retrieving storage plugin\", e);\n+    }\n+\n+    try {\n+      Map configMap = mapper.convertValue(storagePlugin.getConfig(), Map.class);\n+      if (storagePlugin instanceof FileSystemPlugin) {\n+        transformWorkspaces(schema.names, configMap);\n       }\n+      String properties = mapper.writeValueAsString(configMap);\n       return DirectPlan.createDirectPlan(context, new DescribeSchemaResult(Joiner.on(\".\").join(schema.names), properties));\n+    } catch (JsonProcessingException e) {\n+      throw new DrillRuntimeException(\"Error while trying to convert storage config to json string\", e);\n     }\n-\n-    throw UserException.validationError()\n-          .message(String.format(\"Invalid schema name [%s]\", Joiner.on(\".\").join(schema.names)))\n-          .build(logger);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/drill/raw/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/main/java/org/apache/drill/exec/planner/sql/handlers/DescribeSchemaHandler.java",
                "sha": "bb51ef0b6238c71110e9356606ed529cada0b715",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java?ref=aa127b70b1e46f7f4aa19881f25eda583627830a",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "patch": "@@ -382,10 +382,16 @@ public void describeSchemaSyntax() throws Exception {\n     test(\"describe database dfs.`default`\");\n   }\n \n+  @Test\n+  public void describePartialSchema() throws Exception {\n+    test(\"use dfs\");\n+    test(\"describe schema tmp\");\n+  }\n+\n   @Test\n   public void describeSchemaOutput() throws Exception {\n     final List<QueryDataBatch> result = testSqlWithResults(\"describe schema dfs.tmp\");\n-    assertTrue(result.size() == 1);\n+    assertEquals(1, result.size());\n     final QueryDataBatch batch = result.get(0);\n     final RecordBatchLoader loader = new RecordBatchLoader(getDrillbitContext().getAllocator());\n     loader.load(batch.getHeader().getDef(), batch.getData());",
                "raw_url": "https://github.com/apache/drill/raw/aa127b70b1e46f7f4aa19881f25eda583627830a/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestInfoSchema.java",
                "sha": "e0ed2fb0faa81c1bac7bc1c3a5a42d363dd6f612",
                "status": "modified"
            }
        ],
        "message": "DRILL-6523: Fix NPE for describe of partial schema\n\ncloses #1332",
        "parent": "https://github.com/apache/drill/commit/6823a8f241a58a4ae1dc1c6e84ba6a9920cc1643",
        "patched_files": [
            "DescribeSchemaHandler.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestInfoSchema.java"
        ]
    },
    "drill_b4ffa40": {
        "bug_id": "drill_b4ffa40",
        "commit": "https://github.com/apache/drill/commit/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/drill/blob/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java?ref=b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724",
                "deletions": 12,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "patch": "@@ -31,21 +31,29 @@\n   static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(FieldIdUtil.class);\n \n   public static TypedFieldId getFieldIdIfMatchesUnion(UnionVector unionVector, TypedFieldId.Builder builder, boolean addToBreadCrumb, PathSegment seg) {\n-    if (seg.isNamed()) {\n-      ValueVector v = unionVector.getMap();\n-      if (v != null) {\n-        return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n-      } else {\n-        return null;\n+    if (seg != null) {\n+      if (seg.isNamed()) {\n+        ValueVector v = unionVector.getMap();\n+        if (v != null) {\n+          return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n+        } else {\n+          return null;\n+        }\n+      } else if (seg.isArray()) {\n+        ValueVector v = unionVector.getList();\n+        if (v != null) {\n+          return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n+        } else {\n+          return null;\n+        }\n       }\n-    } else if (seg.isArray()) {\n-      ValueVector v = unionVector.getList();\n-      if (v != null) {\n-        return getFieldIdIfMatches(v, builder, addToBreadCrumb, seg);\n-      } else {\n-        return null;\n+    } else {\n+      if (addToBreadCrumb) {\n+        builder.intermediateType(unionVector.getField().getType());\n       }\n+      return builder.finalType(unionVector.getField().getType()).build();\n     }\n+\n     return null;\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/FieldIdUtil.java",
                "sha": "6e72b6e50e644e86b496bbc1f87a8d86892c6a91",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/drill/blob/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java?ref=b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "patch": "@@ -35,6 +35,8 @@\n import java.util.List;\n import java.util.zip.GZIPOutputStream;\n \n+import org.apache.drill.exec.util.JsonStringHashMap;\n+import org.apache.drill.exec.util.Text;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.util.DrillFileUtils;\n@@ -720,4 +722,30 @@ public void testFieldWithDots() throws Exception {\n       .baselineValues(\"1\", \"2\", \"1\", null, \"a\")\n       .go();\n   }\n+\n+  @Test // DRILL-6020\n+  public void testUntypedPathWithUnion() throws Exception {\n+    String fileName = \"table.json\";\n+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(new File(dirTestWatcher.getRootDir(), fileName)))) {\n+      writer.write(\"{\\\"rk\\\": {\\\"a\\\": {\\\"b\\\": \\\"1\\\"}}}\");\n+      writer.write(\"{\\\"rk\\\": {\\\"a\\\": \\\"2\\\"}}\");\n+    }\n+\n+    JsonStringHashMap<String, Text> map = new JsonStringHashMap<>();\n+    map.put(\"b\", new Text(\"1\"));\n+\n+    try {\n+      testBuilder()\n+        .sqlQuery(\"select t.rk.a as a from dfs.`%s` t\", fileName)\n+        .ordered()\n+        .optionSettingQueriesForTestQuery(\"alter session set `exec.enable_union_type`=true\")\n+        .baselineColumns(\"a\")\n+        .baselineValues(map)\n+        .baselineValues(\"2\")\n+        .go();\n+\n+    } finally {\n+      testNoResult(\"alter session reset `exec.enable_union_type`\");\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/b4ffa40127c040d2f8d9ebe2fd4623dfac8c7724/exec/java-exec/src/test/java/org/apache/drill/exec/vector/complex/writer/TestJsonReader.java",
                "sha": "da1cddba5003fc1c57cfd35f8fd078449c9b65c4",
                "status": "modified"
            }
        ],
        "message": "DRILL-6020: Fix NullPointerException when querying JSON untyped path with Union setting on\n\ncloses #1068",
        "parent": "https://github.com/apache/drill/commit/ce80da857d1b28af7619f8402ffe1e4e3c833e1c",
        "patched_files": [
            "FieldIdUtil.java",
            "JsonReader.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestJsonReader.java"
        ]
    },
    "drill_bf1bdec": {
        "bug_id": "drill_bf1bdec",
        "commit": "https://github.com/apache/drill/commit/bf1bdec6069f6fdd2132608450357edea47d328c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/pom.xml?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 1,
                "filename": "exec/java-exec/pom.xml",
                "patch": "@@ -458,7 +458,7 @@\n     <dependency>\n       <groupId>nl.basjes.parse.httpdlog</groupId>\n       <artifactId>httpdlog-parser</artifactId>\n-      <version>2.4</version>\n+      <version>5.2</version>\n       <exclusions>\n         <exclusion>\n           <groupId>commons-codec</groupId>",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/pom.xml",
                "sha": "a341ab9bba688c4556a1ed21ca323a6097e92d0d",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "patch": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.httpd;\n+\n+import com.fasterxml.jackson.annotation.JsonInclude;\n+import com.fasterxml.jackson.annotation.JsonTypeName;\n+import org.apache.drill.common.logical.FormatPluginConfig;\n+\n+@JsonTypeName(\"httpd\")\n+@JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+public class HttpdLogFormatConfig implements FormatPluginConfig {\n+\n+  public String logFormat;\n+  public String timestampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n+\n+  /**\n+   * @return the log formatting string.  This string is the config string from httpd.conf or similar config file.\n+   */\n+  public String getLogFormat() {\n+    return logFormat;\n+  }\n+\n+  public void setLogFormat(String format) {\n+    this.logFormat = format;\n+  }\n+\n+  /**\n+   * @return the timestampFormat\n+   */\n+  public String getTimestampFormat() {\n+    return timestampFormat;\n+  }\n+\n+  /**\n+   * Sets the time stamp format\n+   * @param timestamp\n+   */\n+  public void setTimestampFormat(String timestamp) {\n+    this.timestampFormat = timestamp;\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    int result = logFormat != null ? logFormat.hashCode() : 0;\n+    result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n+    return result;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n+\n+    if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n+      return false;\n+    }\n+    return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatConfig.java",
                "sha": "c4c34b6750c5e44520101673bb3539d4723190a2",
                "status": "added"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 83,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "patch": "@@ -18,17 +18,16 @@\n package org.apache.drill.exec.store.httpd;\n \n import java.io.IOException;\n+import java.util.HashMap;\n import java.util.List;\n \n-import com.fasterxml.jackson.annotation.JsonInclude;\n import nl.basjes.parse.core.exceptions.DissectionFailure;\n import nl.basjes.parse.core.exceptions.InvalidDissectorException;\n import nl.basjes.parse.core.exceptions.MissingDissectorsException;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n-import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.ops.FragmentContext;\n@@ -55,26 +54,23 @@\n import org.apache.hadoop.mapred.LineRecordReader;\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.mapred.TextInputFormat;\n-\n-import com.fasterxml.jackson.annotation.JsonTypeName;\n-import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n-import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n+import java.util.Collections;\n import java.util.Map;\n import org.apache.drill.exec.store.RecordReader;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class HttpdLogFormatPlugin extends EasyFormatPlugin<HttpdLogFormatPlugin.HttpdLogFormatConfig> {\n+public class HttpdLogFormatPlugin extends EasyFormatPlugin<HttpdLogFormatConfig> {\n \n   private static final Logger LOG = LoggerFactory.getLogger(HttpdLogFormatPlugin.class);\n   private static final String PLUGIN_EXTENSION = \"httpd\";\n   private static final int VECTOR_MEMORY_ALLOCATION = 4095;\n \n   public HttpdLogFormatPlugin(final String name, final DrillbitContext context, final Configuration fsConf,\n-      final StoragePluginConfig storageConfig, final HttpdLogFormatConfig formatConfig) {\n+                              final StoragePluginConfig storageConfig, final HttpdLogFormatConfig formatConfig) {\n \n     super(name, context, fsConf, storageConfig, formatConfig, true, false, true, true,\n-        Lists.newArrayList(PLUGIN_EXTENSION), PLUGIN_EXTENSION);\n+            Collections.singletonList(PLUGIN_EXTENSION), PLUGIN_EXTENSION);\n   }\n \n   @Override\n@@ -92,55 +88,6 @@ public void writeStatistics(TableStatistics statistics, FileSystem fs, Path stat\n     throw new UnsupportedOperationException(\"unimplemented\");\n   }\n \n-  /**\n-   * This class is a POJO to hold the configuration for the HttpdLogFormat Parser. This is automatically\n-   * serialized/deserialized from JSON format.\n-   */\n-  @JsonTypeName(PLUGIN_EXTENSION) @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n-  public static class HttpdLogFormatConfig implements FormatPluginConfig {\n-\n-    public String logFormat;\n-    public String timestampFormat;\n-\n-    /**\n-     * @return the logFormat\n-     */\n-    public String getLogFormat() {\n-      return logFormat;\n-    }\n-\n-    /**\n-     * @return the timestampFormat\n-     */\n-    public String getTimestampFormat() {\n-      return timestampFormat;\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      int result = logFormat != null ? logFormat.hashCode() : 0;\n-      result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n-      return result;\n-    }\n-\n-    @Override\n-    public boolean equals(Object o) {\n-      if (this == o) {\n-        return true;\n-      }\n-      if (o == null || getClass() != o.getClass()) {\n-        return false;\n-      }\n-\n-      HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n-\n-      if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n-        return false;\n-      }\n-      return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n-    }\n-  }\n-\n   /**\n    * This class performs the work for the plugin. This is where all logic goes to read records. In this case httpd logs\n    * are lines terminated with a new line character.\n@@ -169,11 +116,15 @@ public HttpdLogRecordReader(final FragmentContext context, final DrillFileSystem\n      * @return Map with Drill field names as a key and Parser Field names as a value\n      */\n     private Map<String, String> makeParserFields() {\n-      final Map<String, String> fieldMapping = Maps.newHashMap();\n+      Map<String, String> fieldMapping = new HashMap<>();\n       for (final SchemaPath sp : getColumns()) {\n-        final String drillField = sp.getRootSegment().getPath();\n-        final String parserField = HttpdParser.parserFormattedFieldName(drillField);\n-        fieldMapping.put(drillField, parserField);\n+        String drillField = sp.getRootSegment().getPath();\n+        try {\n+          String parserField = HttpdParser.parserFormattedFieldName(drillField);\n+          fieldMapping.put(drillField, parserField);\n+        } catch (Exception e) {\n+          LOG.info(\"Putting field: \" + drillField + \" into map\", e);\n+        }\n       }\n       return fieldMapping;\n     }\n@@ -187,10 +138,11 @@ public void setup(final OperatorContext context, final OutputMutator output) thr\n          */\n         final Map<String, String> fieldMapping = !isStarQuery() ? makeParserFields() : null;\n         writer = new VectorContainerWriter(output);\n+\n         parser = new HttpdParser(writer.rootAsMap(), context.getManagedBuffer(),\n-            HttpdLogFormatPlugin.this.getConfig().getLogFormat(),\n-            HttpdLogFormatPlugin.this.getConfig().getTimestampFormat(),\n-            fieldMapping);\n+                HttpdLogFormatPlugin.this.getConfig().getLogFormat(),\n+                HttpdLogFormatPlugin.this.getConfig().getTimestampFormat(),\n+                fieldMapping);\n \n         final Path path = fs.makeQualified(work.getPath());\n         FileSplit split = new FileSplit(path, work.getStart(), work.getLength(), new String[]{\"\"});\n@@ -200,23 +152,21 @@ public void setup(final OperatorContext context, final OutputMutator output) thr\n         job.setInputFormat(inputFormat.getClass());\n         lineReader = (LineRecordReader) inputFormat.getRecordReader(split, job, Reporter.NULL);\n         lineNumber = lineReader.createKey();\n-      }\n-      catch (NoSuchMethodException | MissingDissectorsException | InvalidDissectorException e) {\n+      } catch (NoSuchMethodException | MissingDissectorsException | InvalidDissectorException e) {\n         throw handleAndGenerate(\"Failure creating HttpdParser\", e);\n-      }\n-      catch (IOException e) {\n+      } catch (IOException e) {\n         throw handleAndGenerate(\"Failure creating HttpdRecordReader\", e);\n       }\n     }\n \n     private RuntimeException handleAndGenerate(final String s, final Exception e) {\n       throw UserException.dataReadError(e)\n-          .message(s + \"\\n%s\", e.getMessage())\n-          .addContext(\"Path\", work.getPath())\n-          .addContext(\"Split Start\", work.getStart())\n-          .addContext(\"Split Length\", work.getLength())\n-          .addContext(\"Local Line Number\", lineNumber.get())\n-          .build(LOG);\n+              .message(s + \"\\n%s\", e.getMessage())\n+              .addContext(\"Path\", work.getPath())\n+              .addContext(\"Split Start\", work.getStart())\n+              .addContext(\"Split Length\", work.getLength())\n+              .addContext(\"Local Line Number\", lineNumber.get())\n+              .build(LOG);\n     }\n \n     /**\n@@ -241,8 +191,7 @@ public int next() {\n         writer.setValueCount(recordCount);\n \n         return recordCount;\n-      }\n-      catch (DissectionFailure | InvalidDissectorException | MissingDissectorsException | IOException e) {\n+      } catch (DissectionFailure | InvalidDissectorException | MissingDissectorsException | IOException e) {\n         throw handleAndGenerate(\"Failure while parsing log record.\", e);\n       }\n     }\n@@ -253,19 +202,18 @@ public void close() throws Exception {\n         if (lineReader != null) {\n           lineReader.close();\n         }\n-      }\n-      catch (IOException e) {\n+      } catch (IOException e) {\n         LOG.warn(\"Failure while closing Httpd reader.\", e);\n       }\n     }\n \n     @Override\n     public String toString() {\n       return \"HttpdLogRecordReader[Path=\" + work.getPath()\n-          + \", Start=\" + work.getStart()\n-          + \", Length=\" + work.getLength()\n-          + \", Line=\" + lineNumber.get()\n-          + \"]\";\n+              + \", Start=\" + work.getStart()\n+              + \", Length=\" + work.getLength()\n+              + \", Line=\" + lineNumber.get()\n+              + \"]\";\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "sha": "4785da13137999417d0ad7e6ad9dbccd6648f09d",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 24,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "patch": "@@ -20,31 +20,43 @@\n import org.apache.drill.shaded.guava.com.google.common.base.Charsets;\n import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n import io.netty.buffer.DrillBuf;\n+\n import java.util.EnumSet;\n+import java.util.HashMap;\n import java.util.Map;\n+\n import nl.basjes.parse.core.Casts;\n import nl.basjes.parse.core.Parser;\n import org.apache.drill.exec.vector.complex.writer.BaseWriter.MapWriter;\n import org.apache.drill.exec.vector.complex.writer.BigIntWriter;\n import org.apache.drill.exec.vector.complex.writer.Float8Writer;\n import org.apache.drill.exec.vector.complex.writer.VarCharWriter;\n+import org.apache.drill.exec.vector.complex.writer.TimeStampWriter;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+\n public class HttpdLogRecord {\n \n   private static final Logger LOG = LoggerFactory.getLogger(HttpdLogRecord.class);\n   private final Map<String, VarCharWriter> strings = Maps.newHashMap();\n   private final Map<String, BigIntWriter> longs = Maps.newHashMap();\n   private final Map<String, Float8Writer> doubles = Maps.newHashMap();\n+  private final Map<String, TimeStampWriter> times = new HashMap<>();\n   private final Map<String, MapWriter> wildcards = Maps.newHashMap();\n   private final Map<String, String> cleanExtensions = Maps.newHashMap();\n   private final Map<String, MapWriter> startedWildcards = Maps.newHashMap();\n   private final Map<String, MapWriter> wildcardWriters = Maps.newHashMap();\n+  private final SimpleDateFormat dateFormatter;\n   private DrillBuf managedBuffer;\n+  private String timeFormat;\n \n-  public HttpdLogRecord(final DrillBuf managedBuffer) {\n+  public HttpdLogRecord(final DrillBuf managedBuffer, final String timeFormat) {\n     this.managedBuffer = managedBuffer;\n+    this.timeFormat = timeFormat;\n+    this.dateFormatter = new SimpleDateFormat(this.timeFormat);\n   }\n \n   /**\n@@ -66,7 +78,7 @@ private DrillBuf buf(final int size) {\n     return managedBuffer;\n   }\n \n-  private void writeString(final VarCharWriter writer, final String value) {\n+  private void writeString(VarCharWriter writer, String value) {\n     final byte[] stringBytes = value.getBytes(Charsets.UTF_8);\n     final DrillBuf stringBuffer = buf(stringBytes.length);\n     stringBuffer.clear();\n@@ -82,14 +94,13 @@ private void writeString(final VarCharWriter writer, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final String value) {\n+  public void set(String field, String value) {\n     if (value != null) {\n       final VarCharWriter w = strings.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as string: {}\", field, value);\n         writeString(w, value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'string' writer found for field: {}\", field);\n       }\n     }\n@@ -103,19 +114,47 @@ public void set(final String field, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final Long value) {\n+  public void set(String field, Long value) {\n     if (value != null) {\n       final BigIntWriter w = longs.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as long: {}\", field, value);\n         w.writeBigInt(value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'long' writer found for field: {}\", field);\n       }\n     }\n   }\n \n+  /**\n+   * This method is referenced and called via reflection. This is added as a parsing target for the parser. It will get\n+   * called when the value of a log field is a timesstamp data type.\n+   *\n+   * @param field name of field\n+   * @param value value of field\n+   */\n+  @SuppressWarnings(\"unused\")\n+  public void setTimestamp(String field, String value) {\n+    if (value != null) {\n+      //Convert the date string into a long\n+      long ts = 0;\n+      try {\n+        Date d = this.dateFormatter.parse(value);\n+        ts = d.getTime();\n+      } catch (Exception e) {\n+        //If the date formatter does not successfully create a date, the timestamp will fall back to zero\n+        //Do not throw exception\n+      }\n+      final TimeStampWriter tw = times.get(field);\n+      if (tw != null) {\n+        LOG.trace(\"Parsed field: {}, as time: {}\", field, value);\n+        tw.writeTimeStamp(ts);\n+      } else {\n+        LOG.warn(\"No 'timestamp' writer found for field: {}\", field);\n+      }\n+    }\n+  }\n+\n   /**\n    * This method is referenced and called via reflection. This is added as a parsing target for the parser. It will get\n    * called when the value of a log field is a Double data type.\n@@ -124,14 +163,13 @@ public void set(final String field, final Long value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void set(final String field, final Double value) {\n+  public void set(String field, Double value) {\n     if (value != null) {\n       final Float8Writer w = doubles.get(field);\n       if (w != null) {\n         LOG.trace(\"Parsed field: {}, as double: {}\", field, value);\n         w.writeFloat8(value);\n-      }\n-      else {\n+      } else {\n         LOG.warn(\"No 'double' writer found for field: {}\", field);\n       }\n     }\n@@ -146,7 +184,7 @@ public void set(final String field, final Double value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final String value) {\n+  public void setWildcard(String field, String value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as string: {}\", field, value);\n@@ -164,7 +202,7 @@ public void setWildcard(final String field, final String value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final Long value) {\n+  public void setWildcard(String field, Long value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as long: {}\", field, value);\n@@ -182,7 +220,7 @@ public void setWildcard(final String field, final Long value) {\n    * @param value value of field\n    */\n   @SuppressWarnings(\"unused\")\n-  public void setWildcard(final String field, final Double value) {\n+  public void setWildcard(String field, Double value) {\n     if (value != null) {\n       final MapWriter mapWriter = getWildcardWriter(field);\n       LOG.trace(\"Parsed wildcard field: {}, as double: {}\", field, value);\n@@ -199,7 +237,7 @@ public void setWildcard(final String field, final Double value) {\n    * @param field like HTTP.URI:request.firstline.uri.query.old where 'old' is one of many different parameter names.\n    * @return the writer to be used for this field.\n    */\n-  private MapWriter getWildcardWriter(final String field) {\n+  private MapWriter getWildcardWriter(String field) {\n     MapWriter writer = startedWildcards.get(field);\n     if (writer == null) {\n       for (Map.Entry<String, MapWriter> entry : wildcards.entrySet()) {\n@@ -212,7 +250,7 @@ private MapWriter getWildcardWriter(final String field) {\n            * unsafe characters in it.\n            */\n           if (!cleanExtensions.containsKey(field)) {\n-            final String extension = field.substring(root.length() + 1, field.length());\n+            final String extension = field.substring(root.length() + 1);\n             final String cleanExtension = HttpdParser.drillFormattedFieldName(extension);\n             cleanExtensions.put(field, cleanExtension);\n             LOG.debug(\"Added extension: field='{}' with cleanExtension='{}'\", field, cleanExtension);\n@@ -255,6 +293,10 @@ private MapWriter getWildcardWriter(final String field) {\n     return doubles;\n   }\n \n+  public Map<String, TimeStampWriter> getTimes() {\n+    return times;\n+  }\n+\n   /**\n    * This record will be used with a single parser. For each field that is to be parsed a setter will be called. It\n    * registers a setter method for each field being parsed. It also builds the data writers to hold the data beings\n@@ -281,21 +323,23 @@ public void addField(final Parser<HttpdLogRecord> parser, final MapWriter mapWri\n       parser.addParseTarget(this.getClass().getMethod(\"setWildcard\", String.class, Double.class), parserFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"setWildcard\", String.class, Long.class), parserFieldName);\n       wildcards.put(cleanName, mapWriter.map(drillFieldName));\n-    }\n-    else if (type.contains(Casts.DOUBLE)) {\n+    } else if (type.contains(Casts.DOUBLE)) {\n       LOG.debug(\"Adding DOUBLE parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, Double.class), parserFieldName);\n       doubles.put(parserFieldName, mapWriter.float8(drillFieldName));\n-    }\n-    else if (type.contains(Casts.LONG)) {\n+    } else if (type.contains(Casts.LONG)) {\n       LOG.debug(\"Adding LONG parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n       parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, Long.class), parserFieldName);\n       longs.put(parserFieldName, mapWriter.bigInt(drillFieldName));\n-    }\n-    else {\n+    } else {\n       LOG.debug(\"Adding STRING parse target: {}, with field name: {}\", parserFieldName, drillFieldName);\n-      parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, String.class), parserFieldName);\n-      strings.put(parserFieldName, mapWriter.varChar(drillFieldName));\n+      if (parserFieldName.startsWith(\"TIME.STAMP:\")) {\n+        parser.addParseTarget(this.getClass().getMethod(\"setTimestamp\", String.class, String.class), parserFieldName);\n+        times.put(parserFieldName, mapWriter.timeStamp(drillFieldName));\n+      } else {\n+        parser.addParseTarget(this.getClass().getMethod(\"set\", String.class, String.class), parserFieldName);\n+        strings.put(parserFieldName, mapWriter.varChar(drillFieldName));\n+      }\n     }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogRecord.java",
                "sha": "95917cb6e3c0da8f00823875e6f6c6e0d1302369",
                "status": "modified"
            },
            {
                "additions": 276,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "changes": 396,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 120,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "patch": "@@ -45,112 +45,262 @@\n   private final Parser<HttpdLogRecord> parser;\n   private final HttpdLogRecord record;\n \n-    public static final HashMap<String, String> LOGFIELDS = new HashMap<String, String>();\n-    static\n-    {\n-        LOGFIELDS.put(\"request_receive_time_weekyear__utc\", \"TIME_YEAR:request_receive_time_weekyear__utc\");\n-        LOGFIELDS.put(\"request_referer_ref\", \"HTTP_REF:request_referer_ref\");\n-        LOGFIELDS.put(\"request_referer_protocol\", \"HTTP_PROTOCOL:request_referer_protocol\");\n-        LOGFIELDS.put(\"request_receive_time_timezone\", \"TIME_ZONE:request_receive_time_timezone\");\n-        LOGFIELDS.put(\"connection_client_host\", \"IP:connection_client_host\");\n-        LOGFIELDS.put(\"connection_client_ip\", \"IP:connection_client_ip\");\n-        LOGFIELDS.put(\"connection_client_peerip\", \"IP:connection_client_peerip\");\n-        LOGFIELDS.put(\"connection_server_ip\", \"IP:connection_server_ip\");\n-        LOGFIELDS.put(\"request_receive_time_day\", \"TIME_DAY:request_receive_time_day\");\n-        LOGFIELDS.put(\"request_receive_time_minute__utc\", \"TIME_MINUTE:request_receive_time_minute__utc\");\n-        LOGFIELDS.put(\"request_referer_query_$\", \"STRING:request_referer_query_$\");\n-        LOGFIELDS.put(\"request_receive_time_millisecond__utc\", \"TIME_MILLISECOND:request_receive_time_millisecond__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri_port\", \"HTTP_PORT:request_firstline_uri_port\");\n-        LOGFIELDS.put(\"request_referer_userinfo\", \"HTTP_USERINFO:request_referer_userinfo\");\n-        LOGFIELDS.put(\"request_receive_time_second__utc\", \"TIME_SECOND:request_receive_time_second__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri_protocol\", \"HTTP_PROTOCOL:request_firstline_uri_protocol\");\n-        LOGFIELDS.put(\"request_receive_time_month\", \"TIME_MONTH:request_receive_time_month\");\n-        LOGFIELDS.put(\"request_firstline_uri_query\", \"HTTP_QUERYSTRING:request_firstline_uri_query\");\n-        LOGFIELDS.put(\"request_firstline_uri_path\", \"HTTP_PATH:request_firstline_uri_path\");\n-        LOGFIELDS.put(\"request_receive_time_hour__utc\", \"TIME_HOUR:request_receive_time_hour__utc\");\n-        LOGFIELDS.put(\"request_receive_time_monthname\", \"TIME_MONTHNAME:request_receive_time_monthname\");\n-        LOGFIELDS.put(\"request_receive_time_year__utc\", \"TIME_YEAR:request_receive_time_year__utc\");\n-        LOGFIELDS.put(\"request_receive_time_second\", \"TIME_SECOND:request_receive_time_second\");\n-        LOGFIELDS.put(\"request_referer\", \"HTTP_URI:request_referer\");\n-        LOGFIELDS.put(\"request_receive_time_monthname__utc\", \"TIME_MONTHNAME:request_receive_time_monthname__utc\");\n-        LOGFIELDS.put(\"request_referer_path\", \"HTTP_PATH:request_referer_path\");\n-        LOGFIELDS.put(\"request_receive_time_weekyear\", \"TIME_YEAR:request_receive_time_weekyear\");\n-        LOGFIELDS.put(\"request_firstline_protocol\", \"HTTP_PROTOCOL:request_firstline_protocol\");\n-        LOGFIELDS.put(\"request_referer_port\", \"HTTP_PORT:request_referer_port\");\n-        LOGFIELDS.put(\"request_receive_time_minute\", \"TIME_MINUTE:request_receive_time_minute\");\n-        LOGFIELDS.put(\"request_status_last\", \"STRING:request_status_last\");\n-        LOGFIELDS.put(\"request_receive_time_hour\", \"TIME_HOUR:request_receive_time_hour\");\n-        LOGFIELDS.put(\"request_firstline_protocol_version\", \"HTTP_PROTOCOL_VERSION:request_firstline_protocol_version\");\n-        LOGFIELDS.put(\"request_receive_time\", \"TIME_STAMP:request_receive_time\");\n-        LOGFIELDS.put(\"request_firstline_method\", \"HTTP_METHOD:request_firstline_method\");\n-        LOGFIELDS.put(\"request_receive_time_epoch\", \"TIME_EPOCH:request_receive_time_epoch\");\n-        LOGFIELDS.put(\"request_receive_time_weekofweekyear\", \"TIME_WEEK:request_receive_time_weekofweekyear\");\n-        LOGFIELDS.put(\"request_firstline_uri_host\", \"HTTP_HOST:request_firstline_uri_host\");\n-        LOGFIELDS.put(\"request_referer_query\", \"HTTP_QUERYSTRING:request_referer_query\");\n-        LOGFIELDS.put(\"request_firstline_uri_userinfo\", \"HTTP_USERINFO:request_firstline_uri_userinfo\");\n-        LOGFIELDS.put(\"response_body_bytes\", \"BYTES:response_body_bytes\");\n-        LOGFIELDS.put(\"response_body_bytesclf\", \"BYTES:response_body_bytesclf\");\n-        LOGFIELDS.put(\"request_referer_host\", \"HTTP_HOST:request_referer_host\");\n-        LOGFIELDS.put(\"request_receive_time_weekofweekyear__utc\", \"TIME_WEEK:request_receive_time_weekofweekyear__utc\");\n-        LOGFIELDS.put(\"request_firstline_uri\", \"HTTP_URI:request_firstline_uri\");\n-        LOGFIELDS.put(\"request_firstline_uri_ref\", \"HTTP_REF:request_firstline_uri_ref\");\n-        LOGFIELDS.put(\"request_receive_time_year\", \"TIME_YEAR:request_receive_time_year\");\n-        LOGFIELDS.put(\"request_firstline\", \"HTTP_FIRSTLINE:request_firstline\");\n-        LOGFIELDS.put(\"request_user-agent\", \"HTTP_USERAGENT:request_user-agent\");\n-        LOGFIELDS.put(\"request_cookies\", \"HTTP_COOKIE:request_cookies\");\n-        LOGFIELDS.put(\"server_process_time\", \"MICROSECONDS:server_process_time\");\n-        LOGFIELDS.put(\"request_cookies_$\", \"HTTP_COOKIE:request_cookies_$\");\n-        LOGFIELDS.put(\"server_environment_$\", \"VARIABLE:server_environment_$\");\n-        LOGFIELDS.put(\"server_filename\", \"FILENAME:server_filename\");\n-        LOGFIELDS.put(\"request_protocol\", \"PROTOCOL:request_protocol\");\n-        LOGFIELDS.put(\"request_header_\", \"HTTP_HEADER:request_header_\");\n-        LOGFIELDS.put(\"connection_keepalivecount\", \"NUMBER:connection_keepalivecount\");\n-        LOGFIELDS.put(\"connection_client_logname\", \"NUMBER:connection_client_logname\");\n-        LOGFIELDS.put(\"request_errorlogid\", \"STRING:request_errorlogid\");\n-        LOGFIELDS.put(\"request_method\", \"HTTP_METHOD:request_method\");\n-        LOGFIELDS.put(\"server_module_note_$\", \"STRING:server_module_note_$\");\n-        LOGFIELDS.put(\"response_header_$\", \"HTTP_HEADER:response_header_$\");\n-        LOGFIELDS.put(\"request_server_port_canonical\", \"PORT:request_server_port_canonical\");\n-        LOGFIELDS.put(\"connection_server_port_canonical\", \"PORT:connection_server_port_canonical\");\n-        LOGFIELDS.put(\"connection_server_port\", \"PORT:connection_server_port\");\n-        LOGFIELDS.put(\"connection_client_port\", \"PORT:connection_client_port\");\n-        LOGFIELDS.put(\"connection_server_child_processid\", \"NUMBER:connection_server_child_processid\");\n-        LOGFIELDS.put(\"connection_server_child_threadid\", \"NUMBER:connection_server_child_threadid\");\n-        LOGFIELDS.put(\"connection_server_child_hexthreadid\", \"NUMBER:connection_server_child_hexthreadid\");\n-        LOGFIELDS.put(\"request_querystring\", \"HTTP_QUERYSTRING:request_querystring\");\n-        LOGFIELDS.put(\"request_handler\", \"STRING:request_handler\");\n-        LOGFIELDS.put(\"request_status_original\", \"STRING:request_status_original\");\n-        LOGFIELDS.put(\"request_status_last\", \"STRING:request_status_last\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec\", \"TIME_EPOCH:request_receive_time_begin_msec\");\n-        LOGFIELDS.put(\"request_receive_time_end_msec\", \"TIME_EPOCH:request_receive_time_end_msec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec\", \"TIME_EPOCH_USEC:request_receive_time_begin_usec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec\", \"TIME_EPOCH_USEC:request_receive_time_begin_usec\");\n-        LOGFIELDS.put(\"request_receive_time_end_usec\", \"TIME_EPOCH_USEC:request_receive_time_end_usec\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec_frac\", \"TIME_EPOCH:request_receive_time_begin_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_msec_frac\", \"TIME_EPOCH:request_receive_time_begin_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_end_msec_frac\", \"TIME_EPOCH:request_receive_time_end_msec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request_receive_time_begin_usec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_begin_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request.receive.time.begin.usec_frac\");\n-        LOGFIELDS.put(\"request_receive_time_end_usec_frac\", \"TIME_EPOCH_USEC_FRAC:request_receive_time_end_usec_frac\");\n-        LOGFIELDS.put(\"response_server_processing_time\", \"SECONDS:response_server_processing_time\");\n-        LOGFIELDS.put(\"connection_client_user\", \"STRING:connection_client_user\");\n-        LOGFIELDS.put(\"request_urlpath\", \"URI:request_urlpath\");\n-        LOGFIELDS.put(\"connection_server_name_canonical\", \"STRING:connection_server_name_canonical\");\n-        LOGFIELDS.put(\"connection_server_name\", \"STRING:connection_server_name\");\n-        LOGFIELDS.put(\"response_connection_status\", \"HTTP_CONNECTSTATUS:response_connection_status\");\n-        LOGFIELDS.put(\"request_bytes\", \"BYTES:request_bytes\");\n-        LOGFIELDS.put(\"response_bytes\", \"BYTES:response_bytes\");\n-    }\n+  public static final HashMap<String, String> LOGFIELDS = new HashMap<String, String>();\n \n-    //Map map = Collections.synchronizedMap(LOGFIELDS);\n+  static {\n+    LOGFIELDS.put(\"connection.client.ip\", \"IP:connection.client.ip\");\n+    LOGFIELDS.put(\"connection.client.ip.last\", \"IP:connection.client.ip.last\");\n+    LOGFIELDS.put(\"connection.client.ip.original\", \"IP:connection.client.ip.original\");\n+    LOGFIELDS.put(\"connection.client.ip.last\", \"IP:connection.client.ip.last\");\n+    LOGFIELDS.put(\"connection.client.peerip\", \"IP:connection.client.peerip\");\n+    LOGFIELDS.put(\"connection.client.peerip.last\", \"IP:connection.client.peerip.last\");\n+    LOGFIELDS.put(\"connection.client.peerip.original\", \"IP:connection.client.peerip.original\");\n+    LOGFIELDS.put(\"connection.client.peerip.last\", \"IP:connection.client.peerip.last\");\n+    LOGFIELDS.put(\"connection.server.ip\", \"IP:connection.server.ip\");\n+    LOGFIELDS.put(\"connection.server.ip.last\", \"IP:connection.server.ip.last\");\n+    LOGFIELDS.put(\"connection.server.ip.original\", \"IP:connection.server.ip.original\");\n+    LOGFIELDS.put(\"connection.server.ip.last\", \"IP:connection.server.ip.last\");\n+    LOGFIELDS.put(\"response.body.bytes\", \"BYTES:response.body.bytes\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTES:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytes.original\", \"BYTES:response.body.bytes.original\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTES:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytesclf\", \"BYTES:response.body.bytesclf\");\n+    LOGFIELDS.put(\"response.body.bytes\", \"BYTESCLF:response.body.bytes\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTESCLF:response.body.bytes.last\");\n+    LOGFIELDS.put(\"response.body.bytes.original\", \"BYTESCLF:response.body.bytes.original\");\n+    LOGFIELDS.put(\"response.body.bytes.last\", \"BYTESCLF:response.body.bytes.last\");\n+    LOGFIELDS.put(\"request.cookies.foobar\", \"HTTP.COOKIE:request.cookies.foobar\");\n+    LOGFIELDS.put(\"server.environment.foobar\", \"VARIABLE:server.environment.foobar\");\n+    LOGFIELDS.put(\"server.filename\", \"FILENAME:server.filename\");\n+    LOGFIELDS.put(\"server.filename.last\", \"FILENAME:server.filename.last\");\n+    LOGFIELDS.put(\"server.filename.original\", \"FILENAME:server.filename.original\");\n+    LOGFIELDS.put(\"server.filename.last\", \"FILENAME:server.filename.last\");\n+    LOGFIELDS.put(\"connection.client.host\", \"IP:connection.client.host\");\n+    LOGFIELDS.put(\"connection.client.host.last\", \"IP:connection.client.host.last\");\n+    LOGFIELDS.put(\"connection.client.host.original\", \"IP:connection.client.host.original\");\n+    LOGFIELDS.put(\"connection.client.host.last\", \"IP:connection.client.host.last\");\n+    LOGFIELDS.put(\"request.protocol\", \"PROTOCOL:request.protocol\");\n+    LOGFIELDS.put(\"request.protocol.last\", \"PROTOCOL:request.protocol.last\");\n+    LOGFIELDS.put(\"request.protocol.original\", \"PROTOCOL:request.protocol.original\");\n+    LOGFIELDS.put(\"request.protocol.last\", \"PROTOCOL:request.protocol.last\");\n+    LOGFIELDS.put(\"request.header.foobar\", \"HTTP.HEADER:request.header.foobar\");\n+    LOGFIELDS.put(\"request.trailer.foobar\", \"HTTP.TRAILER:request.trailer.foobar\");\n+    LOGFIELDS.put(\"connection.keepalivecount\", \"NUMBER:connection.keepalivecount\");\n+    LOGFIELDS.put(\"connection.keepalivecount.last\", \"NUMBER:connection.keepalivecount.last\");\n+    LOGFIELDS.put(\"connection.keepalivecount.original\", \"NUMBER:connection.keepalivecount.original\");\n+    LOGFIELDS.put(\"connection.keepalivecount.last\", \"NUMBER:connection.keepalivecount.last\");\n+    LOGFIELDS.put(\"connection.client.logname\", \"NUMBER:connection.client.logname\");\n+    LOGFIELDS.put(\"connection.client.logname.last\", \"NUMBER:connection.client.logname.last\");\n+    LOGFIELDS.put(\"connection.client.logname.original\", \"NUMBER:connection.client.logname.original\");\n+    LOGFIELDS.put(\"connection.client.logname.last\", \"NUMBER:connection.client.logname.last\");\n+    LOGFIELDS.put(\"request.errorlogid\", \"STRING:request.errorlogid\");\n+    LOGFIELDS.put(\"request.errorlogid.last\", \"STRING:request.errorlogid.last\");\n+    LOGFIELDS.put(\"request.errorlogid.original\", \"STRING:request.errorlogid.original\");\n+    LOGFIELDS.put(\"request.errorlogid.last\", \"STRING:request.errorlogid.last\");\n+    LOGFIELDS.put(\"request.method\", \"HTTP.METHOD:request.method\");\n+    LOGFIELDS.put(\"request.method.last\", \"HTTP.METHOD:request.method.last\");\n+    LOGFIELDS.put(\"request.method.original\", \"HTTP.METHOD:request.method.original\");\n+    LOGFIELDS.put(\"request.method.last\", \"HTTP.METHOD:request.method.last\");\n+    LOGFIELDS.put(\"server.module_note.foobar\", \"STRING:server.module_note.foobar\");\n+    LOGFIELDS.put(\"response.header.foobar\", \"HTTP.HEADER:response.header.foobar\");\n+    LOGFIELDS.put(\"response.trailer.foobar\", \"HTTP.TRAILER:response.trailer.foobar\");\n+    LOGFIELDS.put(\"request.server.port.canonical\", \"PORT:request.server.port.canonical\");\n+    LOGFIELDS.put(\"request.server.port.canonical.last\", \"PORT:request.server.port.canonical.last\");\n+    LOGFIELDS.put(\"request.server.port.canonical.original\", \"PORT:request.server.port.canonical.original\");\n+    LOGFIELDS.put(\"request.server.port.canonical.last\", \"PORT:request.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port.canonical\", \"PORT:connection.server.port.canonical\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.last\", \"PORT:connection.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.original\", \"PORT:connection.server.port.canonical.original\");\n+    LOGFIELDS.put(\"connection.server.port.canonical.last\", \"PORT:connection.server.port.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.port\", \"PORT:connection.server.port\");\n+    LOGFIELDS.put(\"connection.server.port.last\", \"PORT:connection.server.port.last\");\n+    LOGFIELDS.put(\"connection.server.port.original\", \"PORT:connection.server.port.original\");\n+    LOGFIELDS.put(\"connection.server.port.last\", \"PORT:connection.server.port.last\");\n+    LOGFIELDS.put(\"connection.client.port\", \"PORT:connection.client.port\");\n+    LOGFIELDS.put(\"connection.client.port.last\", \"PORT:connection.client.port.last\");\n+    LOGFIELDS.put(\"connection.client.port.original\", \"PORT:connection.client.port.original\");\n+    LOGFIELDS.put(\"connection.client.port.last\", \"PORT:connection.client.port.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid\", \"NUMBER:connection.server.child.processid\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid.original\", \"NUMBER:connection.server.child.processid.original\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid\", \"NUMBER:connection.server.child.processid\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.processid.original\", \"NUMBER:connection.server.child.processid.original\");\n+    LOGFIELDS.put(\"connection.server.child.processid.last\", \"NUMBER:connection.server.child.processid.last\");\n+    LOGFIELDS.put(\"connection.server.child.threadid\", \"NUMBER:connection.server.child.threadid\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.last\", \"NUMBER:connection.server.child.threadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.original\", \"NUMBER:connection.server.child.threadid.original\");\n+    LOGFIELDS.put(\"connection.server.child.threadid.last\", \"NUMBER:connection.server.child.threadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid\", \"NUMBER:connection.server.child.hexthreadid\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.last\", \"NUMBER:connection.server.child.hexthreadid.last\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.original\", \"NUMBER:connection.server.child.hexthreadid.original\");\n+    LOGFIELDS.put(\"connection.server.child.hexthreadid.last\", \"NUMBER:connection.server.child.hexthreadid.last\");\n+    LOGFIELDS.put(\"request.querystring\", \"HTTP.QUERYSTRING:request.querystring\");\n+    LOGFIELDS.put(\"request.querystring.last\", \"HTTP.QUERYSTRING:request.querystring.last\");\n+    LOGFIELDS.put(\"request.querystring.original\", \"HTTP.QUERYSTRING:request.querystring.original\");\n+    LOGFIELDS.put(\"request.querystring.last\", \"HTTP.QUERYSTRING:request.querystring.last\");\n+    LOGFIELDS.put(\"request.firstline\", \"HTTP.FIRSTLINE:request.firstline\");\n+    LOGFIELDS.put(\"request.firstline.original\", \"HTTP.FIRSTLINE:request.firstline.original\");\n+    LOGFIELDS.put(\"request.firstline.original\", \"HTTP.FIRSTLINE:request.firstline.original\");\n+    LOGFIELDS.put(\"request.firstline.last\", \"HTTP.FIRSTLINE:request.firstline.last\");\n+    LOGFIELDS.put(\"request.handler\", \"STRING:request.handler\");\n+    LOGFIELDS.put(\"request.handler.last\", \"STRING:request.handler.last\");\n+    LOGFIELDS.put(\"request.handler.original\", \"STRING:request.handler.original\");\n+    LOGFIELDS.put(\"request.handler.last\", \"STRING:request.handler.last\");\n+    LOGFIELDS.put(\"request.status\", \"STRING:request.status\");\n+    LOGFIELDS.put(\"request.status.original\", \"STRING:request.status.original\");\n+    LOGFIELDS.put(\"request.status.original\", \"STRING:request.status.original\");\n+    LOGFIELDS.put(\"request.status.last\", \"STRING:request.status.last\");\n+    LOGFIELDS.put(\"request.receive.time\", \"TIME.STAMP:request.receive.time\");\n+    LOGFIELDS.put(\"request.receive.time.last\", \"TIME.STAMP:request.receive.time.last\");\n+    LOGFIELDS.put(\"request.receive.time.original\", \"TIME.STAMP:request.receive.time.original\");\n+    LOGFIELDS.put(\"request.receive.time.last\", \"TIME.STAMP:request.receive.time.last\");\n+    LOGFIELDS.put(\"request.receive.time.year\", \"TIME.YEAR:request.receive.time.year\");\n+    LOGFIELDS.put(\"request.receive.time.begin.year\", \"TIME.YEAR:request.receive.time.begin.year\");\n+    LOGFIELDS.put(\"request.receive.time.end.year\", \"TIME.YEAR:request.receive.time.end.year\");\n+    LOGFIELDS.put(\"request.receive.time.sec\", \"TIME.SECONDS:request.receive.time.sec\");\n+    LOGFIELDS.put(\"request.receive.time.sec\", \"TIME.SECONDS:request.receive.time.sec\");\n+    LOGFIELDS.put(\"request.receive.time.sec.original\", \"TIME.SECONDS:request.receive.time.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.sec.last\", \"TIME.SECONDS:request.receive.time.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec\", \"TIME.SECONDS:request.receive.time.begin.sec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.last\", \"TIME.SECONDS:request.receive.time.begin.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.original\", \"TIME.SECONDS:request.receive.time.begin.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.sec.last\", \"TIME.SECONDS:request.receive.time.begin.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec\", \"TIME.SECONDS:request.receive.time.end.sec\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.last\", \"TIME.SECONDS:request.receive.time.end.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.original\", \"TIME.SECONDS:request.receive.time.end.sec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.sec.last\", \"TIME.SECONDS:request.receive.time.end.sec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec\", \"TIME.EPOCH:request.receive.time.begin.msec\");\n+    LOGFIELDS.put(\"request.receive.time.msec\", \"TIME.EPOCH:request.receive.time.msec\");\n+    LOGFIELDS.put(\"request.receive.time.msec.last\", \"TIME.EPOCH:request.receive.time.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.msec.original\", \"TIME.EPOCH:request.receive.time.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.msec.last\", \"TIME.EPOCH:request.receive.time.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec\", \"TIME.EPOCH:request.receive.time.begin.msec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.last\", \"TIME.EPOCH:request.receive.time.begin.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.original\", \"TIME.EPOCH:request.receive.time.begin.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec.last\", \"TIME.EPOCH:request.receive.time.begin.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec\", \"TIME.EPOCH:request.receive.time.end.msec\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.last\", \"TIME.EPOCH:request.receive.time.end.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.original\", \"TIME.EPOCH:request.receive.time.end.msec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec.last\", \"TIME.EPOCH:request.receive.time.end.msec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec\");\n+    LOGFIELDS.put(\"request.receive.time.usec\", \"TIME.EPOCH.USEC:request.receive.time.usec\");\n+    LOGFIELDS.put(\"request.receive.time.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.begin.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec\", \"TIME.EPOCH.USEC:request.receive.time.end.usec\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.original\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec.last\", \"TIME.EPOCH.USEC:request.receive.time.end.usec.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac\", \"TIME.EPOCH:request.receive.time.begin.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac\", \"TIME.EPOCH:request.receive.time.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.last\", \"TIME.EPOCH:request.receive.time.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.original\", \"TIME.EPOCH:request.receive.time.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.msec_frac.last\", \"TIME.EPOCH:request.receive.time.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac\", \"TIME.EPOCH:request.receive.time.begin.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.last\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.original\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.msec_frac.last\", \"TIME.EPOCH:request.receive.time.begin.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac\", \"TIME.EPOCH:request.receive.time.end.msec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.last\", \"TIME.EPOCH:request.receive.time.end.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.original\", \"TIME.EPOCH:request.receive.time.end.msec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.msec_frac.last\", \"TIME.EPOCH:request.receive.time.end.msec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac\", \"FRAC:request.receive.time.begin.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac\", \"FRAC:request.receive.time.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.last\", \"FRAC:request.receive.time.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.original\", \"FRAC:request.receive.time.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.usec_frac.last\", \"FRAC:request.receive.time.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac\", \"FRAC:request.receive.time.begin.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.last\", \"FRAC:request.receive.time.begin.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.original\", \"FRAC:request.receive.time.begin.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.begin.usec_frac.last\", \"FRAC:request.receive.time.begin.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac\", \"FRAC:request.receive.time.end.usec_frac\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.last\", \"FRAC:request.receive.time.end.usec_frac.last\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.original\", \"FRAC:request.receive.time.end.usec_frac.original\");\n+    LOGFIELDS.put(\"request.receive.time.end.usec_frac.last\", \"FRAC:request.receive.time.end.usec_frac.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"SECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"SECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"server.process.time\", \"MICROSECONDS:server.process.time\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MICROSECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MICROSECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MICROSECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MICROSECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MICROSECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"MILLISECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MILLISECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"MILLISECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"MILLISECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"response.server.processing.time\", \"SECONDS:response.server.processing.time\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.original\", \"SECONDS:response.server.processing.time.original\");\n+    LOGFIELDS.put(\"response.server.processing.time.last\", \"SECONDS:response.server.processing.time.last\");\n+    LOGFIELDS.put(\"connection.client.user\", \"STRING:connection.client.user\");\n+    LOGFIELDS.put(\"connection.client.user.last\", \"STRING:connection.client.user.last\");\n+    LOGFIELDS.put(\"connection.client.user.original\", \"STRING:connection.client.user.original\");\n+    LOGFIELDS.put(\"connection.client.user.last\", \"STRING:connection.client.user.last\");\n+    LOGFIELDS.put(\"request.urlpath\", \"URI:request.urlpath\");\n+    LOGFIELDS.put(\"request.urlpath.original\", \"URI:request.urlpath.original\");\n+    LOGFIELDS.put(\"request.urlpath.original\", \"URI:request.urlpath.original\");\n+    LOGFIELDS.put(\"request.urlpath.last\", \"URI:request.urlpath.last\");\n+    LOGFIELDS.put(\"connection.server.name.canonical\", \"STRING:connection.server.name.canonical\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.last\", \"STRING:connection.server.name.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.original\", \"STRING:connection.server.name.canonical.original\");\n+    LOGFIELDS.put(\"connection.server.name.canonical.last\", \"STRING:connection.server.name.canonical.last\");\n+    LOGFIELDS.put(\"connection.server.name\", \"STRING:connection.server.name\");\n+    LOGFIELDS.put(\"connection.server.name.last\", \"STRING:connection.server.name.last\");\n+    LOGFIELDS.put(\"connection.server.name.original\", \"STRING:connection.server.name.original\");\n+    LOGFIELDS.put(\"connection.server.name.last\", \"STRING:connection.server.name.last\");\n+    LOGFIELDS.put(\"response.connection.status\", \"HTTP.CONNECTSTATUS:response.connection.status\");\n+    LOGFIELDS.put(\"response.connection.status.last\", \"HTTP.CONNECTSTATUS:response.connection.status.last\");\n+    LOGFIELDS.put(\"response.connection.status.original\", \"HTTP.CONNECTSTATUS:response.connection.status.original\");\n+    LOGFIELDS.put(\"response.connection.status.last\", \"HTTP.CONNECTSTATUS:response.connection.status.last\");\n+    LOGFIELDS.put(\"request.bytes\", \"BYTES:request.bytes\");\n+    LOGFIELDS.put(\"request.bytes.last\", \"BYTES:request.bytes.last\");\n+    LOGFIELDS.put(\"request.bytes.original\", \"BYTES:request.bytes.original\");\n+    LOGFIELDS.put(\"request.bytes.last\", \"BYTES:request.bytes.last\");\n+    LOGFIELDS.put(\"response.bytes\", \"BYTES:response.bytes\");\n+    LOGFIELDS.put(\"response.bytes.last\", \"BYTES:response.bytes.last\");\n+    LOGFIELDS.put(\"response.bytes.original\", \"BYTES:response.bytes.original\");\n+    LOGFIELDS.put(\"response.bytes.last\", \"BYTES:response.bytes.last\");\n+    LOGFIELDS.put(\"total.bytes\", \"BYTES:total.bytes\");\n+    LOGFIELDS.put(\"total.bytes.last\", \"BYTES:total.bytes.last\");\n+    LOGFIELDS.put(\"total.bytes.original\", \"BYTES:total.bytes.original\");\n+    LOGFIELDS.put(\"total.bytes.last\", \"BYTES:total.bytes.last\");\n+    LOGFIELDS.put(\"request.cookies\", \"HTTP.COOKIES:request.cookies\");\n+    LOGFIELDS.put(\"request.cookies.last\", \"HTTP.COOKIES:request.cookies.last\");\n+    LOGFIELDS.put(\"request.cookies.original\", \"HTTP.COOKIES:request.cookies.original\");\n+    LOGFIELDS.put(\"request.cookies.last\", \"HTTP.COOKIES:request.cookies.last\");\n+    LOGFIELDS.put(\"response.cookies\", \"HTTP.SETCOOKIES:response.cookies\");\n+    LOGFIELDS.put(\"response.cookies.last\", \"HTTP.SETCOOKIES:response.cookies.last\");\n+    LOGFIELDS.put(\"response.cookies.original\", \"HTTP.SETCOOKIES:response.cookies.original\");\n+    LOGFIELDS.put(\"response.cookies.last\", \"HTTP.SETCOOKIES:response.cookies.last\");\n+    LOGFIELDS.put(\"request.user-agent\", \"HTTP.USERAGENT:request.user-agent\");\n+    LOGFIELDS.put(\"request.user-agent.last\", \"HTTP.USERAGENT:request.user-agent.last\");\n+    LOGFIELDS.put(\"request.user-agent.original\", \"HTTP.USERAGENT:request.user-agent.original\");\n+    LOGFIELDS.put(\"request.user-agent.last\", \"HTTP.USERAGENT:request.user-agent.last\");\n+    LOGFIELDS.put(\"request.referer\", \"HTTP.URI:request.referer\");\n+    LOGFIELDS.put(\"request.referer.last\", \"HTTP.URI:request.referer.last\");\n+    LOGFIELDS.put(\"request.referer.original\", \"HTTP.URI:request.referer.original\");\n+    LOGFIELDS.put(\"request.referer.last\", \"HTTP.URI:request.referer.last\");\n+  }\n \n   public HttpdParser(final MapWriter mapWriter, final DrillBuf managedBuffer, final String logFormat,\n-      final String timestampFormat, final Map<String, String> fieldMapping)\n-      throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n+                     final String timestampFormat, final Map<String, String> fieldMapping)\n+          throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n \n     Preconditions.checkArgument(logFormat != null && !logFormat.trim().isEmpty(), \"logFormat cannot be null or empty\");\n \n-    this.record = new HttpdLogRecord(managedBuffer);\n+    this.record = new HttpdLogRecord(managedBuffer, timestampFormat);\n     this.parser = new HttpdLoglineParser<>(HttpdLogRecord.class, logFormat, timestampFormat);\n \n     setupParser(mapWriter, logFormat, fieldMapping);\n@@ -167,7 +317,6 @@ public HttpdParser(final MapWriter mapWriter, final DrillBuf managedBuffer, fina\n    * We do not expose the underlying parser or the record which is used to manage the writers.\n    *\n    * @param line log line to tear apart.\n-   *\n    * @throws DissectionFailure\n    * @throws InvalidDissectorException\n    * @throws MissingDissectorsException\n@@ -181,7 +330,7 @@ public void parse(final String line) throws DissectionFailure, InvalidDissectorE\n    * In order to define a type remapping the format of the field configuration will look like: <br/>\n    * HTTP.URI:request.firstline.uri.query.[parameter name] <br/>\n    *\n-   * @param parser Add type remapping to this parser instance.\n+   * @param parser    Add type remapping to this parser instance.\n    * @param fieldName request.firstline.uri.query.[parameter_name]\n    * @param fieldType HTTP.URI, etc..\n    */\n@@ -198,11 +347,17 @@ private void addTypeRemapping(final Parser<HttpdLogRecord> parser, final String\n    * @param drillFieldName name to be cleansed.\n    * @return\n    */\n-  public static String parserFormattedFieldName(final String drillFieldName) {\n-      String tempFieldName;\n-      tempFieldName = LOGFIELDS.get(drillFieldName);\n-      return tempFieldName.replace(SAFE_WILDCARD, PARSER_WILDCARD).replaceAll(SAFE_SEPARATOR, \".\").replaceAll(\"\\\\.\\\\.\", \"_\");\n+  public static String parserFormattedFieldName(String drillFieldName) {\n \n+    //The Useragent fields contain a dash which causes potential problems if the field name is not escaped properly\n+    //This removes the dash\n+    if (drillFieldName.contains(\"useragent\")) {\n+      drillFieldName = drillFieldName.replace(\"useragent\", \"user-agent\");\n+    }\n+\n+    String tempFieldName;\n+    tempFieldName = LOGFIELDS.get(drillFieldName);\n+    return tempFieldName.replace(SAFE_WILDCARD, PARSER_WILDCARD).replaceAll(SAFE_SEPARATOR, \".\").replaceAll(\"\\\\.\\\\.\", \"_\");\n   }\n \n   /**\n@@ -213,19 +368,24 @@ public static String parserFormattedFieldName(final String drillFieldName) {\n    * @param parserFieldName name to be cleansed.\n    * @return\n    */\n-  public static String drillFormattedFieldName(final String parserFieldName) {\n+  public static String drillFormattedFieldName(String parserFieldName) {\n \n-      if (parserFieldName.contains(\":\") ) {\n-        String[] fieldPart= parserFieldName.split(\":\");\n-        return fieldPart[1].replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n-        }\n-    else{\n+    //The Useragent fields contain a dash which causes potential problems if the field name is not escaped properly\n+    //This removes the dash\n+    if (parserFieldName.contains(\"user-agent\")) {\n+      parserFieldName = parserFieldName.replace(\"user-agent\", \"useragent\");\n+    }\n+\n+    if (parserFieldName.contains(\":\")) {\n+      String[] fieldPart = parserFieldName.split(\":\");\n+      return fieldPart[1].replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n+    } else {\n       return parserFieldName.replaceAll(\"_\", \"__\").replace(PARSER_WILDCARD, SAFE_WILDCARD).replaceAll(\"\\\\.\", SAFE_SEPARATOR);\n     }\n   }\n \n   private void setupParser(final MapWriter mapWriter, final String logFormat, final Map<String, String> fieldMapping)\n-      throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n+          throws NoSuchMethodException, MissingDissectorsException, InvalidDissectorException {\n \n     /**\n      * If the user has selected fields, then we will use them to configure the parser because this would be the most\n@@ -236,8 +396,7 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n     if (fieldMapping != null && !fieldMapping.isEmpty()) {\n       LOG.debug(\"Using fields defined by user.\");\n       requestedPaths = fieldMapping;\n-    }\n-    else {\n+    } else {\n       /**\n        * Use all possible paths that the parser has determined from the specified log format.\n        */\n@@ -255,7 +414,6 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n      */\n     Parser<Object> dummy = new HttpdLoglineParser<>(Object.class, logFormat);\n     dummy.addParseTarget(String.class.getMethod(\"indexOf\", String.class), allParserPaths);\n-\n     for (final Map.Entry<String, String> entry : requestedPaths.entrySet()) {\n       final EnumSet<Casts> casts;\n \n@@ -270,15 +428,13 @@ private void setupParser(final MapWriter mapWriter, final String logFormat, fina\n \n         final String[] pieces = entry.getValue().split(\":\");\n         addTypeRemapping(parser, pieces[1], pieces[0]);\n-\n         casts = Casts.STRING_ONLY;\n-      }\n-      else {\n+      } else {\n         casts = dummy.getCasts(entry.getValue());\n       }\n \n       LOG.debug(\"Setting up drill field: {}, parser field: {}, which casts as: {}\", entry.getKey(), entry.getValue(), casts);\n       record.addField(parser, mapWriter, casts, entry.getValue(), entry.getKey());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParser.java",
                "sha": "5d3d7c0d3b5cd18f013d1fd4984ec9a4bf677c06",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/2364b02175bec69cee2f9ceb4e52e1333da39f70/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java?ref=2364b02175bec69cee2f9ceb4e52e1333da39f70",
                "deletions": 50,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "patch": "@@ -1,50 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.drill.exec.store.httpd;\n-\n-import io.netty.buffer.DrillBuf;\n-import java.util.Map;\n-import org.apache.drill.exec.vector.complex.writer.BaseWriter.MapWriter;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-public class HttpdParserTest {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(HttpdParserTest.class);\n-\n-  private void runTest(String logFormat, String logLine) throws Exception {\n-    MapWriter mapWriter = null;\n-    DrillBuf managedBuffer = null;\n-    Map<String, String> configuredFields = null;\n-    HttpdParser parser = new HttpdParser(mapWriter, managedBuffer, logFormat, null, configuredFields);\n-    parser.parse(logLine);\n-  }\n-\n-//  @Test\n-  public void testFirstPattern() throws Exception {\n-    LOG.info(\"testFirstPattern\");\n-//    final String format = \"common\";\n-//    final String format = \"%h %l %u %t \\\"%r\\\" %>s %b\";\n-    final String format = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n-    final String line = \"127.0.0.1 [31/Dec/2012:23:49:41 +0100] \"\n-        + \"\\\"GET /foo HTTP/1.1\\\" 200 \"\n-        + \"1213 \\\"http://localhost/index.php?mies=wim\\\"\";\n-    runTest(format, line);\n-  }\n-\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/2364b02175bec69cee2f9ceb4e52e1333da39f70/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdParserTest.java",
                "sha": "3897136a144d2805996c62c521e92f17588f3985",
                "status": "removed"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "changes": 178,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 89,
                "filename": "exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "patch": "@@ -1,178 +1,178 @@\n {\n   \"storage\":{\n-    dfs: {\n-      type: \"file\",\n-      connection: \"file:///\",\n-      workspaces: {\n+    \"dfs\": {\n+      \"type\": \"file\",\n+      \"connection\": \"file:///\",\n+      \"workspaces\": {\n         \"root\" : {\n-          location: \"/\",\n-          writable: false,\n-          allowAccessOutsideWorkspace: false\n+          \"location\": \"/\",\n+          \"writable\": false,\n+          \"allowAccessOutsideWorkspace\": false\n         },\n         \"tmp\" : {\n-          location: \"/tmp\",\n-          writable: true,\n-          allowAccessOutsideWorkspace: false\n+          \"location\": \"/tmp\",\n+          \"writable\": true,\n+          \"allowAccessOutsideWorkspace\": false\n         }\n       },\n-      formats: {\n+      \"formats\": {\n         \"psv\" : {\n-          type: \"text\",\n-          extensions: [ \"tbl\" ],\n-          delimiter: \"|\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tbl\" ],\n+          \"delimiter\": \"|\"\n         },\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-          type: \"text\",\n-          extensions: [ \"tsv\" ],\n-          delimiter: \"\\t\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tsv\" ],\n+          \"delimiter\": \"\\t\"\n         },\n         \"httpd\" : {\n-          type: \"httpd\",\n-          logFormat: \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\"\n-          /* timestampFormat: \"dd/MMM/yyyy:HH:mm:ss ZZ\" */\n+          \"type\": \"httpd\",\n+          \"logFormat\": \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\",\n+          \"timestampFormat\": \"dd/MMM/yyyy:HH:mm:ss ZZ\"\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"pcap\" : {\n-          type: \"pcap\"\n+          \"type\": \"pcap\"\n         },\n         \"pcapng\" : {\n-          type: \"pcapng\"\n+          \"type\": \"pcapng\"\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"sequencefile\": {\n-          type : \"sequencefile\",\n-          extensions: [ \"seq\" ]\n+          \"type\": \"sequencefile\",\n+          \"extensions\": [ \"seq\" ]\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         },\n         \"image\" : {\n-          type: \"image\",\n-          extensions: [\n+          \"type\": \"image\",\n+          \"extensions\": [\n             \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\",\n             \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\",\n             \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\",\n             \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\"\n           ]\n         }\n       },\n-      enabled : true\n+      \"enabled\" : true\n     },\n \n-    s3: {\n-      type: \"file\",\n-      connection: \"s3a://my.bucket.location.com\",\n-      config : {\n+    \"s3\": {\n+      \"type\": \"file\",\n+      \"connection\": \"s3a://my.bucket.location.com\",\n+      \"config\" : {\n         \"fs.s3a.access.key\": \"ID\",\n         \"fs.s3a.secret.key\": \"SECRET\"\n       },\n-      workspaces: {\n+      \"workspaces\": {\n         \"root\" : {\n-          location: \"/\",\n-          writable: false\n+          \"location\": \"/\",\n+          \"writable\": false\n         },\n         \"tmp\" : {\n-          location: \"/tmp\",\n-          writable: true\n+          \"location\": \"/tmp\",\n+          \"writable\": true\n         }\n       },\n-      formats: {\n+      \"formats\": {\n         \"psv\" : {\n-          type: \"text\",\n-          extensions: [ \"tbl\" ],\n-          delimiter: \"|\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tbl\" ],\n+          \"delimiter\": \"|\"\n         },\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-          type: \"text\",\n-          extensions: [ \"tsv\" ],\n-          delimiter: \"\\t\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"tsv\" ],\n+          \"delimiter\": \"\\t\"\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"sequencefile\": {\n-          type : \"sequencefile\",\n-          extensions: [ \"seq\" ]\n+          \"type\": \"sequencefile\",\n+          \"extensions\": [ \"seq\" ]\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         }\n       },\n-      enabled : false\n+      \"enabled\" : false\n     },\n \n-    cp: {\n-      type: \"file\",\n-      connection: \"classpath:///\",\n-      formats: {\n+    \"cp\": {\n+      \"type\": \"file\",\n+      \"connection\": \"classpath:///\",\n+      \"formats\": {\n         \"csv\" : {\n-          type: \"text\",\n-          extensions: [ \"csv\" ],\n-          delimiter: \",\"\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csv\" ],\n+          \"delimiter\": \",\"\n         },\n         \"tsv\" : {\n-                  type: \"text\",\n-                  extensions: [ \"tsv\" ],\n-                  delimiter: \"\\t\"\n+                  \"type\": \"text\",\n+                  \"extensions\": [ \"tsv\" ],\n+                  \"delimiter\": \"\\t\"\n         },\n         \"json\" : {\n-          type: \"json\",\n-          extensions: [ \"json\" ]\n+          \"type\": \"json\",\n+          \"extensions\": [ \"json\" ]\n         },\n         \"parquet\" : {\n-          type: \"parquet\"\n+          \"type\": \"parquet\"\n         },\n         \"avro\" : {\n-          type: \"avro\"\n+          \"type\": \"avro\"\n         },\n         \"csvh\" : {\n-          type: \"text\",\n-          extensions: [ \"csvh\" ],\n-          delimiter: \",\",\n-          extractHeader: true\n+          \"type\": \"text\",\n+          \"extensions\": [ \"csvh\" ],\n+          \"delimiter\": \",\",\n+          \"extractHeader\": true\n         },\n         \"image\" : {\n-          type: \"image\",\n-          extensions: [\n+          \"type\": \"image\",\n+          \"extensions\": [\n             \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\",\n             \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\",\n             \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\",\n             \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\"\n           ]\n         }\n       },\n-      enabled : true\n+      \"enabled\" : true\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/main/resources/bootstrap-storage-plugins.json",
                "sha": "afcf53d969d3e27554632b8daa67a6b87cc6b057",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 2,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "patch": "@@ -93,9 +93,9 @@ public void testPcap() throws Exception {\n \n   @Test\n   public void testHttpd() throws Exception {\n-    String path = \"store/httpd/dfs-bootstrap.httpd\";\n+    String path = \"store/httpd/dfs-test-bootstrap-test.httpd\";\n     dirTestWatcher.copyResourceToRoot(Paths.get(path));\n-    String logFormat = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n+    String logFormat = \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\";\n     String timeStampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n     testPhysicalPlanSubmission(\n         String.format(\"select * from dfs.`%s`\", path),",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "sha": "b4c8b146a6a96bdc0c6217f4311bc620d1ff429d",
                "status": "modified"
            },
            {
                "additions": 237,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "changes": 237,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "patch": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.httpd;\n+\n+import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.types.TypeProtos.MinorType;\n+import org.apache.drill.exec.record.BatchSchema;\n+import org.apache.drill.exec.record.metadata.SchemaBuilder;\n+import org.apache.drill.exec.rpc.RpcException;\n+import org.apache.drill.exec.server.Drillbit;\n+import org.apache.drill.exec.store.StoragePluginRegistry;\n+import org.apache.drill.exec.store.dfs.FileSystemConfig;\n+import org.apache.drill.exec.store.dfs.FileSystemPlugin;\n+import org.apache.drill.test.BaseDirTestWatcher;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterTest;\n+import org.apache.drill.test.rowSet.RowSet;\n+import org.apache.drill.test.rowSet.RowSetUtilities;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import java.time.LocalDateTime;\n+import java.util.HashMap;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestHTTPDLogReader extends ClusterTest {\n+\n+  @ClassRule\n+  public static final BaseDirTestWatcher dirTestWatcher = new BaseDirTestWatcher();\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterTest.startCluster(ClusterFixture.builder(dirTestWatcher));\n+    defineHTTPDPlugin();\n+  }\n+\n+  private static void defineHTTPDPlugin() throws ExecutionSetupException {\n+\n+    // Create an instance of the regex config.\n+    // Note: we can\"t use the \".log\" extension; the Drill .gitignore\n+    // file ignores such files, so they\"ll never get committed. Instead,\n+    // make up a fake suffix.\n+    HttpdLogFormatConfig sampleConfig = new HttpdLogFormatConfig();\n+    sampleConfig.setLogFormat(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\");\n+\n+    // Define a temporary format plugin for the \"cp\" storage plugin.\n+    Drillbit drillbit = cluster.drillbit();\n+    final StoragePluginRegistry pluginRegistry = drillbit.getContext().getStorage();\n+    final FileSystemPlugin plugin = (FileSystemPlugin) pluginRegistry.getPlugin(\"cp\");\n+    final FileSystemConfig pluginConfig = (FileSystemConfig) plugin.getConfig();\n+    pluginConfig.getFormats().put(\"sample\", sampleConfig);\n+    pluginRegistry.createOrUpdate(\"cp\", pluginConfig, false);\n+  }\n+\n+  @Test\n+  public void testDateField() throws RpcException {\n+    String sql = \"SELECT `request_receive_time` FROM cp.`httpd/hackers-access-small.httpd` LIMIT 5\";\n+    RowSet results = client.queryBuilder().sql(sql).rowSet();\n+\n+    BatchSchema expectedSchema = new SchemaBuilder()\n+            .addNullable(\"request_receive_time\", MinorType.TIMESTAMP)\n+            .build();\n+    RowSet expected = client.rowSetBuilder(expectedSchema)\n+            .addRow(1445742685000L)\n+            .addRow(1445742686000L)\n+            .addRow(1445742687000L)\n+            .addRow(1445743471000L)\n+            .addRow(1445743472000L)\n+            .build();\n+\n+    RowSetUtilities.verify(expected, results);\n+  }\n+\n+  @Test\n+  public void testSelectColumns() throws Exception {\n+    String sql = \"SELECT request_referer_ref,\\n\" +\n+            \"request_receive_time_last_time,\\n\" +\n+            \"request_firstline_uri_protocol,\\n\" +\n+            \"request_receive_time_microsecond,\\n\" +\n+            \"request_receive_time_last_microsecond__utc,\\n\" +\n+            \"request_firstline_original_protocol,\\n\" +\n+            \"request_firstline_original_uri_host,\\n\" +\n+            \"request_referer_host,\\n\" +\n+            \"request_receive_time_month__utc,\\n\" +\n+            \"request_receive_time_last_minute,\\n\" +\n+            \"request_firstline_protocol_version,\\n\" +\n+            \"request_receive_time_time__utc,\\n\" +\n+            \"request_referer_last_ref,\\n\" +\n+            \"request_receive_time_last_timezone,\\n\" +\n+            \"request_receive_time_last_weekofweekyear,\\n\" +\n+            \"request_referer_last,\\n\" +\n+            \"request_receive_time_minute,\\n\" +\n+            \"connection_client_host_last,\\n\" +\n+            \"request_receive_time_last_millisecond__utc,\\n\" +\n+            \"request_firstline_original_uri,\\n\" +\n+            \"request_firstline,\\n\" +\n+            \"request_receive_time_nanosecond,\\n\" +\n+            \"request_receive_time_last_millisecond,\\n\" +\n+            \"request_receive_time_day,\\n\" +\n+            \"request_referer_port,\\n\" +\n+            \"request_firstline_original_uri_port,\\n\" +\n+            \"request_receive_time_year,\\n\" +\n+            \"request_receive_time_last_date,\\n\" +\n+            \"request_receive_time_last_time__utc,\\n\" +\n+            \"request_receive_time_last_hour__utc,\\n\" +\n+            \"request_firstline_original_protocol_version,\\n\" +\n+            \"request_firstline_original_method,\\n\" +\n+            \"request_receive_time_last_year__utc,\\n\" +\n+            \"request_firstline_uri,\\n\" +\n+            \"request_referer_last_host,\\n\" +\n+            \"request_receive_time_last_minute__utc,\\n\" +\n+            \"request_receive_time_weekofweekyear,\\n\" +\n+            \"request_firstline_uri_userinfo,\\n\" +\n+            \"request_receive_time_epoch,\\n\" +\n+            \"connection_client_logname,\\n\" +\n+            \"response_body_bytes,\\n\" +\n+            \"request_receive_time_nanosecond__utc,\\n\" +\n+            \"request_firstline_protocol,\\n\" +\n+            \"request_receive_time_microsecond__utc,\\n\" +\n+            \"request_receive_time_hour,\\n\" +\n+            \"request_firstline_uri_host,\\n\" +\n+            \"request_referer_last_port,\\n\" +\n+            \"request_receive_time_last_epoch,\\n\" +\n+            \"request_receive_time_last_weekyear__utc,\\n\" +\n+            \"request_useragent,\\n\" +\n+            \"request_receive_time_weekyear,\\n\" +\n+            \"request_receive_time_timezone,\\n\" +\n+            \"response_body_bytesclf,\\n\" +\n+            \"request_receive_time_last_date__utc,\\n\" +\n+            \"request_receive_time_millisecond__utc,\\n\" +\n+            \"request_referer_last_protocol,\\n\" +\n+            \"request_status_last,\\n\" +\n+            \"request_firstline_uri_query,\\n\" +\n+            \"request_receive_time_minute__utc,\\n\" +\n+            \"request_firstline_original_uri_protocol,\\n\" +\n+            \"request_referer_query,\\n\" +\n+            \"request_receive_time_date,\\n\" +\n+            \"request_firstline_uri_port,\\n\" +\n+            \"request_receive_time_last_second__utc,\\n\" +\n+            \"request_referer_last_userinfo,\\n\" +\n+            \"request_receive_time_last_second,\\n\" +\n+            \"request_receive_time_last_monthname__utc,\\n\" +\n+            \"request_firstline_method,\\n\" +\n+            \"request_receive_time_last_month__utc,\\n\" +\n+            \"request_receive_time_millisecond,\\n\" +\n+            \"request_receive_time_day__utc,\\n\" +\n+            \"request_receive_time_year__utc,\\n\" +\n+            \"request_receive_time_weekofweekyear__utc,\\n\" +\n+            \"request_receive_time_second,\\n\" +\n+            \"request_firstline_original_uri_ref,\\n\" +\n+            \"connection_client_logname_last,\\n\" +\n+            \"request_receive_time_last_year,\\n\" +\n+            \"request_firstline_original_uri_path,\\n\" +\n+            \"connection_client_host,\\n\" +\n+            \"request_firstline_original_uri_query,\\n\" +\n+            \"request_referer_userinfo,\\n\" +\n+            \"request_receive_time_last_monthname,\\n\" +\n+            \"request_referer_path,\\n\" +\n+            \"request_receive_time_monthname,\\n\" +\n+            \"request_receive_time_last_month,\\n\" +\n+            \"request_referer_last_query,\\n\" +\n+            \"request_firstline_uri_ref,\\n\" +\n+            \"request_receive_time_last_day,\\n\" +\n+            \"request_receive_time_time,\\n\" +\n+            \"request_receive_time_last_weekofweekyear__utc,\\n\" +\n+            \"request_useragent_last,\\n\" +\n+            \"request_receive_time_last_weekyear,\\n\" +\n+            \"request_receive_time_last_microsecond,\\n\" +\n+            \"request_firstline_original,\\n\" +\n+            \"request_referer_last_path,\\n\" +\n+            \"request_receive_time_month,\\n\" +\n+            \"request_receive_time_last_day__utc,\\n\" +\n+            \"request_referer,\\n\" +\n+            \"request_referer_protocol,\\n\" +\n+            \"request_receive_time_monthname__utc,\\n\" +\n+            \"response_body_bytes_last,\\n\" +\n+            \"request_receive_time,\\n\" +\n+            \"request_receive_time_last_nanosecond,\\n\" +\n+            \"request_firstline_uri_path,\\n\" +\n+            \"request_firstline_original_uri_userinfo,\\n\" +\n+            \"request_receive_time_date__utc,\\n\" +\n+            \"request_receive_time_last,\\n\" +\n+            \"request_receive_time_last_nanosecond__utc,\\n\" +\n+            \"request_receive_time_last_hour,\\n\" +\n+            \"request_receive_time_hour__utc,\\n\" +\n+            \"request_receive_time_second__utc,\\n\" +\n+            \"connection_client_user_last,\\n\" +\n+            \"request_receive_time_weekyear__utc,\\n\" +\n+            \"connection_client_user\\n\" +\n+            \"FROM cp.`httpd/hackers-access-small.httpd`\\n\" +\n+            \"LIMIT 1\";\n+\n+    testBuilder()\n+            .sqlQuery(sql)\n+            .unOrdered()\n+            .baselineColumns(\"request_referer_ref\", \"request_receive_time_last_time\", \"request_firstline_uri_protocol\", \"request_receive_time_microsecond\", \"request_receive_time_last_microsecond__utc\", \"request_firstline_original_protocol\", \"request_firstline_original_uri_host\", \"request_referer_host\", \"request_receive_time_month__utc\", \"request_receive_time_last_minute\", \"request_firstline_protocol_version\", \"request_receive_time_time__utc\", \"request_referer_last_ref\", \"request_receive_time_last_timezone\", \"request_receive_time_last_weekofweekyear\", \"request_referer_last\", \"request_receive_time_minute\", \"connection_client_host_last\", \"request_receive_time_last_millisecond__utc\", \"request_firstline_original_uri\", \"request_firstline\", \"request_receive_time_nanosecond\", \"request_receive_time_last_millisecond\", \"request_receive_time_day\", \"request_referer_port\", \"request_firstline_original_uri_port\", \"request_receive_time_year\", \"request_receive_time_last_date\", \"request_receive_time_last_time__utc\", \"request_receive_time_last_hour__utc\", \"request_firstline_original_protocol_version\", \"request_firstline_original_method\", \"request_receive_time_last_year__utc\", \"request_firstline_uri\", \"request_referer_last_host\", \"request_receive_time_last_minute__utc\", \"request_receive_time_weekofweekyear\", \"request_firstline_uri_userinfo\", \"request_receive_time_epoch\", \"connection_client_logname\", \"response_body_bytes\", \"request_receive_time_nanosecond__utc\", \"request_firstline_protocol\", \"request_receive_time_microsecond__utc\", \"request_receive_time_hour\", \"request_firstline_uri_host\", \"request_referer_last_port\", \"request_receive_time_last_epoch\", \"request_receive_time_last_weekyear__utc\", \"request_useragent\", \"request_receive_time_weekyear\", \"request_receive_time_timezone\", \"response_body_bytesclf\", \"request_receive_time_last_date__utc\", \"request_receive_time_millisecond__utc\", \"request_referer_last_protocol\", \"request_status_last\", \"request_firstline_uri_query\", \"request_receive_time_minute__utc\", \"request_firstline_original_uri_protocol\", \"request_referer_query\", \"request_receive_time_date\", \"request_firstline_uri_port\", \"request_receive_time_last_second__utc\", \"request_referer_last_userinfo\", \"request_receive_time_last_second\", \"request_receive_time_last_monthname__utc\", \"request_firstline_method\", \"request_receive_time_last_month__utc\", \"request_receive_time_millisecond\", \"request_receive_time_day__utc\", \"request_receive_time_year__utc\", \"request_receive_time_weekofweekyear__utc\", \"request_receive_time_second\", \"request_firstline_original_uri_ref\", \"connection_client_logname_last\", \"request_receive_time_last_year\", \"request_firstline_original_uri_path\", \"connection_client_host\", \"request_firstline_original_uri_query\", \"request_referer_userinfo\", \"request_receive_time_last_monthname\", \"request_referer_path\", \"request_receive_time_monthname\", \"request_receive_time_last_month\", \"request_referer_last_query\", \"request_firstline_uri_ref\", \"request_receive_time_last_day\", \"request_receive_time_time\", \"request_receive_time_last_weekofweekyear__utc\", \"request_useragent_last\", \"request_receive_time_last_weekyear\", \"request_receive_time_last_microsecond\", \"request_firstline_original\", \"request_referer_last_path\", \"request_receive_time_month\", \"request_receive_time_last_day__utc\", \"request_referer\", \"request_referer_protocol\", \"request_receive_time_monthname__utc\", \"response_body_bytes_last\", \"request_receive_time\", \"request_receive_time_last_nanosecond\", \"request_firstline_uri_path\", \"request_firstline_original_uri_userinfo\", \"request_receive_time_date__utc\", \"request_receive_time_last\", \"request_receive_time_last_nanosecond__utc\", \"request_receive_time_last_hour\", \"request_receive_time_hour__utc\", \"request_receive_time_second__utc\", \"connection_client_user_last\", \"request_receive_time_weekyear__utc\", \"connection_client_user\")\n+            .baselineValues(null, \"04:11:25\", null, 0L, 0L, \"HTTP\", null, \"howto.basjes.nl\", 10L, 11L, \"1.1\", \"03:11:25\", null, null, 43L, \"http://howto.basjes.nl/\", 11L, \"195.154.46.135\", 0L, \"/linux/doing-pxe-without-dhcp-control\", \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\", 0L, 0L, 25L, null, null, 2015L, \"2015-10-25\", \"03:11:25\", 3L, \"1.1\", \"GET\", 2015L, \"/linux/doing-pxe-without-dhcp-control\", \"howto.basjes.nl\", 11L, 43L, null, 1445742685000L, null, 24323L, 0L, \"HTTP\", 0L, 4L, null, null, 1445742685000L, 2015L, \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\", 2015L, null, 24323L, \"2015-10-25\", 0L, \"http\", \"200\", \"\", 11L, null, \"\", \"2015-10-25\", null, 25L, null, 25L, \"October\", \"GET\", 10L, 0L, 25L, 2015L, 43L, 25L, null, null, 2015L, \"/linux/doing-pxe-without-dhcp-control\", \"195.154.46.135\", \"\", null, \"October\", \"/\", \"October\", 10L, \"\", null, 25L, \"04:11:25\", 43L, \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\", 2015L, 0L, \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\", \"/\", 10L, 25L, \"http://howto.basjes.nl/\", \"http\", \"October\", 24323L, LocalDateTime.parse(\"2015-10-25T03:11:25\"), 0L, \"/linux/doing-pxe-without-dhcp-control\", null, \"2015-10-25\", LocalDateTime.parse(\"2015-10-25T03:11:25\"), 0L, 4L, 3L, 25L, null, 2015L, null)\n+            .go();\n+  }\n+\n+\n+  @Test\n+  public void testCount() throws Exception {\n+    String sql = \"SELECT COUNT(*) FROM cp.`httpd/hackers-access-small.httpd`\";\n+    long result = client.queryBuilder().sql(sql).singletonLong();\n+    assertEquals(10, result);\n+  }\n+\n+  @Test\n+  public void testStar() throws Exception {\n+    String sql = \"SELECT * FROM cp.`httpd/hackers-access-small.httpd` LIMIT 1\";\n+\n+    testBuilder()\n+            .sqlQuery(sql)\n+            .unOrdered()\n+            .baselineColumns(\"request_referer_ref\",\"request_receive_time_last_time\",\"request_firstline_uri_protocol\",\"request_receive_time_microsecond\",\"request_receive_time_last_microsecond__utc\",\"request_firstline_original_uri_query_$\",\"request_firstline_original_protocol\",\"request_firstline_original_uri_host\",\"request_referer_host\",\"request_receive_time_month__utc\",\"request_receive_time_last_minute\",\"request_firstline_protocol_version\",\"request_receive_time_time__utc\",\"request_referer_last_ref\",\"request_receive_time_last_timezone\",\"request_receive_time_last_weekofweekyear\",\"request_referer_last\",\"request_receive_time_minute\",\"connection_client_host_last\",\"request_receive_time_last_millisecond__utc\",\"request_firstline_original_uri\",\"request_firstline\",\"request_receive_time_nanosecond\",\"request_receive_time_last_millisecond\",\"request_receive_time_day\",\"request_referer_port\",\"request_firstline_original_uri_port\",\"request_receive_time_year\",\"request_receive_time_last_date\",\"request_referer_query_$\",\"request_receive_time_last_time__utc\",\"request_receive_time_last_hour__utc\",\"request_firstline_original_protocol_version\",\"request_firstline_original_method\",\"request_receive_time_last_year__utc\",\"request_firstline_uri\",\"request_referer_last_host\",\"request_receive_time_last_minute__utc\",\"request_receive_time_weekofweekyear\",\"request_firstline_uri_userinfo\",\"request_receive_time_epoch\",\"connection_client_logname\",\"response_body_bytes\",\"request_receive_time_nanosecond__utc\",\"request_firstline_protocol\",\"request_receive_time_microsecond__utc\",\"request_receive_time_hour\",\"request_firstline_uri_host\",\"request_referer_last_port\",\"request_receive_time_last_epoch\",\"request_receive_time_last_weekyear__utc\",\"request_receive_time_weekyear\",\"request_receive_time_timezone\",\"response_body_bytesclf\",\"request_receive_time_last_date__utc\",\"request_useragent_last\",\"request_useragent\",\"request_receive_time_millisecond__utc\",\"request_referer_last_protocol\",\"request_status_last\",\"request_firstline_uri_query\",\"request_receive_time_minute__utc\",\"request_firstline_original_uri_protocol\",\"request_referer_query\",\"request_receive_time_date\",\"request_firstline_uri_port\",\"request_receive_time_last_second__utc\",\"request_referer_last_userinfo\",\"request_receive_time_last_second\",\"request_receive_time_last_monthname__utc\",\"request_firstline_method\",\"request_receive_time_last_month__utc\",\"request_receive_time_millisecond\",\"request_receive_time_day__utc\",\"request_receive_time_year__utc\",\"request_receive_time_weekofweekyear__utc\",\"request_receive_time_second\",\"request_firstline_original_uri_ref\",\"connection_client_logname_last\",\"request_receive_time_last_year\",\"request_firstline_original_uri_path\",\"connection_client_host\",\"request_referer_last_query_$\",\"request_firstline_original_uri_query\",\"request_referer_userinfo\",\"request_receive_time_last_monthname\",\"request_referer_path\",\"request_receive_time_monthname\",\"request_receive_time_last_month\",\"request_referer_last_query\",\"request_firstline_uri_ref\",\"request_receive_time_last_day\",\"request_receive_time_time\",\"request_receive_time_last_weekofweekyear__utc\",\"request_receive_time_last_weekyear\",\"request_receive_time_last_microsecond\",\"request_firstline_original\",\"request_firstline_uri_query_$\",\"request_referer_last_path\",\"request_receive_time_month\",\"request_receive_time_last_day__utc\",\"request_referer\",\"request_referer_protocol\",\"request_receive_time_monthname__utc\",\"response_body_bytes_last\",\"request_receive_time\",\"request_receive_time_last_nanosecond\",\"request_firstline_uri_path\",\"request_firstline_original_uri_userinfo\",\"request_receive_time_date__utc\",\"request_receive_time_last\",\"request_receive_time_last_nanosecond__utc\",\"request_receive_time_last_hour\",\"request_receive_time_hour__utc\",\"request_receive_time_second__utc\",\"connection_client_user_last\",\"request_receive_time_weekyear__utc\",\"connection_client_user\")\n+            .baselineValues(null,\"04:11:25\",null,0L,0L,new HashMap<>(),\"HTTP\",null,\"howto.basjes.nl\",10L,11L,\"1.1\",\"03:11:25\",null,null,43L,\"http://howto.basjes.nl/\",11L,\"195.154.46.135\",0L,\"/linux/doing-pxe-without-dhcp-control\",\"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\",0L,0L,25L,null,null,2015L,\"2015-10-25\",new HashMap<>(),\"03:11:25\",3L,\"1.1\",\"GET\",2015L,\"/linux/doing-pxe-without-dhcp-control\",\"howto.basjes.nl\",11L,43L,null,1445742685000L,null,24323L,0L,\"HTTP\",0L,4L,null,null,1445742685000L,2015L,2015L,null,24323L,\"2015-10-25\",\"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\",\"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\",0L,\"http\",\"200\",\"\",11L,null,\"\",\"2015-10-25\",null,25L,null,25L,\"October\",\"GET\",10L,0L,25L,2015L,43L,25L,null,null,2015L,\"/linux/doing-pxe-without-dhcp-control\",\"195.154.46.135\",new HashMap<>(),\"\",null,\"October\",\"/\",\"October\",10L,\"\",null,25L,\"04:11:25\",43L,2015L,0L,\"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\",new HashMap<>(),\"/\",10L,25L,\"http://howto.basjes.nl/\",\"http\",\"October\",24323L,LocalDateTime.parse(\"2015-10-25T03:11:25\"),0L,\"/linux/doing-pxe-without-dhcp-control\",null,\"2015-10-25\",LocalDateTime.parse(\"2015-10-25T03:11:25\"),0L,4L,3L,25L,null,2015L,null)\n+            .go();\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/java/org/apache/drill/exec/store/httpd/TestHTTPDLogReader.java",
                "sha": "ac85a920db82769ba8ae8970d3eb0db29ea1cb4f",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd?ref=bf1bdec6069f6fdd2132608450357edea47d328c",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "patch": "@@ -0,0 +1,10 @@\n+195.154.46.135 - - [25/Oct/2015:04:11:25 +0100] \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\" 200 24323 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+23.95.237.180 - - [25/Oct/2015:04:11:26 +0100] \"GET /join_form HTTP/1.0\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+23.95.237.180 - - [25/Oct/2015:04:11:27 +0100] \"POST /join_form HTTP/1.1\" 302 9093 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"\n+158.222.5.157 - - [25/Oct/2015:04:24:31 +0100] \"GET /join_form HTTP/1.0\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:32 +0100] \"POST /join_form HTTP/1.1\" 302 9093 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:37 +0100] \"GET /acl_users/credentials_cookie_auth/require_login?came_from=http%3A//howto.basjes.nl/join_form HTTP/1.1\" 200 10716 \"http://howto.basjes.nl/join_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:39 +0100] \"GET /login_form HTTP/1.1\" 200 10543 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+158.222.5.157 - - [25/Oct/2015:04:24:41 +0100] \"POST /login_form HTTP/1.1\" 200 16810 \"http://howto.basjes.nl/login_form\" \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0 AlexaToolbar/alxf-2.21\"\n+5.39.5.5 - - [25/Oct/2015:04:32:22 +0100] \"GET /join_form HTTP/1.1\" 200 11114 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:34.0) Gecko/20100101 Firefox/34.0\"\n+180.180.64.16 - - [25/Oct/2015:04:34:37 +0100] \"GET /linux/doing-pxe-without-dhcp-control HTTP/1.1\" 200 24323 \"http://howto.basjes.nl/\" \"Mozilla/5.0 (Windows NT 5.1; rv:35.0) Gecko/20100101 Firefox/35.0\"",
                "raw_url": "https://github.com/apache/drill/raw/bf1bdec6069f6fdd2132608450357edea47d328c/exec/java-exec/src/test/resources/httpd/hackers-access-small.httpd",
                "sha": "98af53288f09c7dcab8d5f85680f80741a112fb6",
                "status": "added"
            }
        ],
        "message": "DRILL-7021: HTTPD Throws NPE and Doesn't Recognize Timeformat",
        "parent": "https://github.com/apache/drill/commit/2364b02175bec69cee2f9ceb4e52e1333da39f70",
        "patched_files": [
            "pom.xml",
            "HttpdLogRecord.java",
            "HttpdLogFormatPlugin.java",
            "hackers-access-small.httpd",
            "bootstrap-storage-plugins.json",
            "HttpdLogFormatConfig.java",
            "HttpdParser.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "HttpdParserTest.java",
            "TestHTTPDLogReader.java",
            "FormatPluginSerDeTest.java"
        ]
    },
    "drill_c8a08c3": {
        "bug_id": "drill_c8a08c3",
        "commit": "https://github.com/apache/drill/commit/c8a08c3e793d53ae4c445f07caa639269c8b51b7",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/expression/IfExpression.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 1,
                "filename": "common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "patch": "@@ -23,7 +23,11 @@\n \n import org.apache.drill.common.expression.IfExpression.IfCondition;\n import org.apache.drill.common.expression.visitors.ExprVisitor;\n+import org.apache.drill.common.types.TypeProtos;\n+import org.apache.drill.common.types.TypeProtos.DataMode;\n import org.apache.drill.common.types.TypeProtos.MajorType;\n+import org.apache.drill.common.types.TypeProtos.MinorType;\n+import org.apache.drill.common.types.Types;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -102,7 +106,22 @@ public IfExpression build(){\n \n   @Override\n   public MajorType getMajorType() {\n-    return this.elseExpression.getMajorType();\n+    // If the return type of one of the \"then\" expression or \"else\" expression is nullable, return \"if\" expression\n+    // type as nullable\n+    MajorType majorType = elseExpression.getMajorType();\n+    if (majorType.getMode() == DataMode.OPTIONAL) {\n+      return majorType;\n+    }\n+\n+    for(IfCondition condition : conditions) {\n+      if (condition.expression.getMajorType().getMode() == DataMode.OPTIONAL) {\n+        assert condition.expression.getMajorType().getMinorType() == majorType.getMinorType();\n+\n+        return condition.expression.getMajorType();\n+      }\n+    }\n+\n+    return majorType;\n   }\n \n   public static Builder newBuilder(){",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "sha": "d1df7f7eb68ab3cf58705dc811e889058e6dc603",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "patch": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+<@pp.dropOutputFile />\n+<#list vv.types as type>\n+<#list type.minor as minor>\n+\n+<#assign className=\"GConvertToNullable${minor.class}Holder\" />\n+\n+<@pp.changeOutputFile name=\"/org/apache/drill/exec/expr/fn/impl/${className}.java\" />\n+\n+<#include \"/@includes/license.ftl\" />\n+\n+package org.apache.drill.exec.expr.fn.impl;\n+\n+import org.apache.drill.exec.expr.DrillSimpleFunc;\n+import org.apache.drill.exec.expr.annotations.*;\n+import org.apache.drill.exec.expr.holders.*;\n+import org.apache.drill.exec.record.RecordBatch;\n+\n+@FunctionTemplate(name = \"convertToNullable${minor.class?upper_case}\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.INTERNAL)\n+public class ${className} implements DrillSimpleFunc {\n+\n+  @Param ${minor.class}Holder input;\n+  @Output Nullable${minor.class}Holder output;\n+\n+  public void setup(RecordBatch incoming) { }\n+\n+  public void eval() {\n+    output.isSet = 1;\n+<#if type.major != \"VarLen\">\n+  <#if (minor.class == \"TimeStampTZ\")>\n+    output.value = input.value;\n+    output.index = input.index;\n+  <#elseif (minor.class == \"Interval\")>\n+    output.months = input.months;\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif (minor.class == \"IntervalDay\")>\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif minor.class.startsWith(\"Decimal\")>\n+    output.scale = input.scale;\n+    output.precision = input.precision;\n+    <#if minor.class.startsWith(\"Decimal28\") || minor.class.startsWith(\"Decimal38\")>\n+    output.sign = input.sign;\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+    <#else>\n+    output.value = input.value;\n+    </#if>\n+  <#elseif (type.width > 8)>\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+  <#else>\n+    output.value = input.value;\n+  </#if>\n+<#else>\n+    output.start = input.start;\n+    output.end = input.end;\n+    output.buffer = input.buffer;\n+</#if>\n+  }\n+}\n+</#list>\n+</#list>\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "sha": "548d64550c0a5f8b7d2c7d26f3cdfce07d59e54b",
                "status": "added"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "patch": "@@ -284,9 +284,45 @@ public boolean apply(LogicalExpression input) {\n         }\n       }\n \n+      // If the type of the IF expression is nullable, apply a convertToNullable*Holder function for \"THEN\"/\"ELSE\"\n+      // expressions whose type is not nullable.\n+      if (IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build().getMajorType().getMode()\n+          == DataMode.OPTIONAL) {\n+        for (int i = 0; i < conditions.size(); ++i) {\n+          IfExpression.IfCondition condition = conditions.get(i);\n+          if (condition.expression.getMajorType().getMode() != DataMode.OPTIONAL) {\n+            conditions.set(i, new IfExpression.IfCondition(condition.condition,\n+                getConvertToNullableExpr(ImmutableList.of(condition.expression),\n+                    condition.expression.getMajorType().getMinorType(), registry)));\n+          }\n+        }\n+\n+        if (newElseExpr.getMajorType().getMode() != DataMode.OPTIONAL) {\n+          newElseExpr = getConvertToNullableExpr(ImmutableList.of(newElseExpr),\n+              newElseExpr.getMajorType().getMinorType(), registry);\n+        }\n+      }\n+\n       return validateNewExpr(IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build());\n     }\n \n+    private LogicalExpression getConvertToNullableExpr(List<LogicalExpression> args, MinorType minorType,\n+        FunctionImplementationRegistry registry) {\n+      String funcName = \"convertToNullable\" + minorType.toString();\n+      FunctionCall funcCall = new FunctionCall(funcName, args, ExpressionPosition.UNKNOWN);\n+      FunctionResolver resolver = FunctionResolverFactory.getResolver(funcCall);\n+\n+      DrillFuncHolder matchedConvertToNullableFuncHolder =\n+          resolver.getBestMatch(registry.getDrillRegistry().getMethods().get(funcName), funcCall);\n+\n+      if (matchedConvertToNullableFuncHolder == null) {\n+        logFunctionResolutionError(errorCollector, funcCall);\n+        return NullExpression.INSTANCE;\n+      }\n+\n+      return new DrillFuncHolderExpr(funcName, matchedConvertToNullableFuncHolder, args, ExpressionPosition.UNKNOWN);\n+    }\n+\n     private LogicalExpression rewriteNullExpression(LogicalExpression expr, MajorType type) {\n       if(expr instanceof NullExpression) {\n         return new TypedNullConstant(type);",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "sha": "fca743bb2d308aaef6af763dc9ae34b44a8a58bc",
                "status": "modified"
            },
            {
                "additions": 61,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 21,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "patch": "@@ -338,21 +338,40 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {\n     public LogicalExpression visitLiteral(RexLiteral literal) {\n       switch(literal.getType().getSqlTypeName()){\n       case BIGINT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIGINT);\n+        }\n         long l = ((BigDecimal) literal.getValue()).longValue();\n-        return checkNullLiteral(literal, MinorType.BIGINT, ValueExpressions.getBigInt(l));\n+        return ValueExpressions.getBigInt(l);\n       case BOOLEAN:\n-        return checkNullLiteral(literal, MinorType.BIT, ValueExpressions.getBit(((Boolean) literal.getValue())));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIT);\n+        }\n+        return ValueExpressions.getBit(((Boolean) literal.getValue()));\n       case CHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case DOUBLE:\n+        if (isLiteralNull(literal)){\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double d = ((BigDecimal) literal.getValue()).doubleValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(d));\n+        return ValueExpressions.getFloat8(d);\n       case FLOAT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT4);\n+        }\n         float f = ((BigDecimal) literal.getValue()).floatValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT4, ValueExpressions.getFloat4(f));\n+        return ValueExpressions.getFloat4(f);\n       case INTEGER:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INT);\n+        }\n         int a = ((BigDecimal) literal.getValue()).intValue();\n-        return checkNullLiteral(literal, MinorType.INT, ValueExpressions.getInt(a));\n+        return ValueExpressions.getInt(a);\n+\n       case DECIMAL:\n         /* TODO: Enable using Decimal literals once we have more functions implemented for Decimal\n          * For now continue using Double instead of decimals\n@@ -367,24 +386,49 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n         } else if (precision <= 38) {\n             return ValueExpressions.getDecimal38((BigDecimal)literal.getValue());\n         } */\n-\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double dbl = ((BigDecimal) literal.getValue()).doubleValue();\n         logger.warn(\"Converting exact decimal into approximate decimal.  Should be fixed once decimal is implemented.\");\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(dbl));\n+        return ValueExpressions.getFloat8(dbl);\n       case VARCHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case SYMBOL:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(literal.getValue().toString()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(literal.getValue().toString());\n       case DATE:\n-        return checkNullLiteral(literal, MinorType.DATE, ValueExpressions.getDate((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.DATE);\n+        }\n+        return (ValueExpressions.getDate((GregorianCalendar)literal.getValue()));\n       case TIME:\n-        return checkNullLiteral(literal, MinorType.TIME, ValueExpressions.getTime((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIME);\n+        }\n+        return (ValueExpressions.getTime((GregorianCalendar)literal.getValue()));\n       case TIMESTAMP:\n-        return checkNullLiteral(literal, MinorType.TIMESTAMP, ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIMESTAMP);\n+        }\n+        return (ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n       case INTERVAL_YEAR_MONTH:\n-        return checkNullLiteral(literal, MinorType.INTERVALYEAR, ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALYEAR);\n+        }\n+        return (ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n       case INTERVAL_DAY_TIME:\n-        return checkNullLiteral(literal, MinorType.INTERVALDAY, ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALDAY);\n+        }\n+        return (ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+      case NULL:\n+        return NullExpression.INSTANCE;\n       case ANY:\n         if (isLiteralNull(literal)) {\n           return NullExpression.INSTANCE;\n@@ -395,12 +439,8 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n     }\n   }\n \n-  private static LogicalExpression checkNullLiteral(RexLiteral literal, MinorType type, LogicalExpression orExpr) {\n-    if(isLiteralNull(literal)) {\n-      return new TypedNullConstant(Types.optional(type));\n-    } else {\n-      return orExpr;\n-    }\n+  private static final TypedNullConstant createNullExpr(MinorType type) {\n+    return new TypedNullConstant(Types.optional(type));\n   }\n \n   private static boolean isLiteralNull(RexLiteral literal) {",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "sha": "8966f18aa2cf1977909966026e7ae5e83427f544",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "patch": "@@ -947,4 +947,52 @@ public Void apply(Connection connection) {\n       }\n     });\n   }\n+\n+  @Test\n+  public void testCaseWithNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=null\\n\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWithElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name ELSE 'Test' END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Test\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=null\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name ELSE 'Test' END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=Test\\n\"\n+        );\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "sha": "e6842286b89f7106cf32f598decc0cc511676b59",
                "status": "modified"
            }
        ],
        "message": "DRILL-665: Handle null values in case expressions (contd).\n\n1. Added functions for converting REQUIRED holder into NULLABLE holder where the minorType is same.\n2. Update in Optiq->Drill literal conversion. First check if it null type, before parsing the literal value. Parsing literal value will cause NPE if the type is NULL.\n3. Changed getReturnType of IfExpression to consider the nullable types of THEN and ELSE expressions.\n4. Added testcases.",
        "parent": "https://github.com/apache/drill/commit/2cdbd6000abc33965f1f7b960b954fd6a4f7b58f",
        "patched_files": [
            "IfExpression.java",
            "ExpressionTreeMaterializer.java",
            "ConvertToNullableHolder.java",
            "DrillOptiq.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "ExpressionTreeMaterializerTest.java",
            "TestJdbcQuery.java"
        ]
    },
    "drill_c8a7840": {
        "bug_id": "drill_c8a7840",
        "commit": "https://github.com/apache/drill/commit/c8a78409d9472b84eaa7fba719842c08302c81c3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java?ref=c8a78409d9472b84eaa7fba719842c08302c81c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "patch": "@@ -137,6 +137,7 @@ public void output() {\n   @Override\n   public void reset() {\n     value = new ObjectHolder();\n+    value.obj = new org.apache.drill.exec.expr.fn.impl.DrillByteArray();\n     init.value = 0;\n     nonNullCount.value = 0;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/main/codegen/templates/VarCharAggrFunctions1.java",
                "sha": "11b20b1c8276d53381a59951cdf4316c23e4bf71",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java?ref=c8a78409d9472b84eaa7fba719842c08302c81c3",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "patch": "@@ -19,7 +19,6 @@\n \n import org.apache.drill.BaseTestQuery;\n import org.apache.drill.PlanTestBase;\n-import org.apache.drill.common.types.TypeProtos;\n import org.apache.drill.common.util.TestTools;\n import org.junit.Ignore;\n import org.junit.Test;\n@@ -457,4 +456,8 @@ public void testGroupBySystemFuncFileSystemTable() throws Exception {\n         .build().run();\n   }\n \n+  @Test\n+  public void test4443() throws Exception {\n+    test(\"SELECT MIN(columns[1]) FROM dfs_test.`%s/agg/4443.csv` GROUP BY columns[0]\", TEST_RES_PATH);\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/java/org/apache/drill/exec/fn/impl/TestAggregateFunctions.java",
                "sha": "43e206e5dd870260e5796041efd0ad917d926f5f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/resources/agg/4443.csv",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/agg/4443.csv?ref=c8a78409d9472b84eaa7fba719842c08302c81c3",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/agg/4443.csv",
                "patch": "@@ -0,0 +1,2 @@\n+a,1\n+b,2\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/c8a78409d9472b84eaa7fba719842c08302c81c3/exec/java-exec/src/test/resources/agg/4443.csv",
                "sha": "bf85e264a48b2d4395980f7529e5d6aa71ae61fc",
                "status": "added"
            }
        ],
        "message": "DRILL-4443: MIN/MAX on VARCHAR throw a NullPointerException",
        "parent": "https://github.com/apache/drill/commit/447b093cd2b05bfeae001844a7e3573935e84389",
        "patched_files": [
            "4443.csv",
            "VarCharAggrFunctions1.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestAggregateFunctions.java"
        ]
    },
    "drill_d29a5b1": {
        "bug_id": "drill_d29a5b1",
        "commit": "https://github.com/apache/drill/commit/d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java?ref=d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "patch": "@@ -55,7 +55,7 @@ public QueueQueryParallelizer(boolean memoryPlanning, QueryContext queryContext)\n   // return the memory computed for a physical operator on a drillbitendpoint.\n   public BiFunction<DrillbitEndpoint, PhysicalOperator, Long> getMemory() {\n     return (endpoint, operator) -> {\n-      if (planHasMemory) {\n+      if (!planHasMemory) {\n         return operators.get(endpoint).get(operator);\n       }\n       else {",
                "raw_url": "https://github.com/apache/drill/raw/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/main/java/org/apache/drill/exec/planner/fragment/QueueQueryParallelizer.java",
                "sha": "5cd4a09e420e52dc6e84ff3d29df126b10feb239",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java?ref=d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.drill.exec.proto.UserProtos;\n import org.apache.drill.exec.rpc.user.UserSession;\n import org.apache.drill.exec.server.DrillbitContext;\n+import org.apache.drill.exec.work.foreman.rm.EmbeddedQueryQueue;\n import org.apache.drill.shaded.guava.com.google.common.collect.Iterables;\n import org.apache.drill.test.ClientFixture;\n import org.apache.drill.test.ClusterFixture;\n@@ -224,4 +225,18 @@ public void TestTwoMajorFragmentWithSortyProjectAndScan() throws Exception {\n     parallelizer.adjustMemory(planningSet, createSet(planningSet.getRootWrapper()), activeEndpoints);\n     assertTrue(\"memory requirement is different\", Iterables.all(resources.entrySet(), (e) -> e.getValue().getMemory() == 481490));\n   }\n+\n+  @Test\n+  public void TestZKBasedQueue() throws Exception {\n+    String sql = \"select * from cp.`employee.json`\";\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher).configProperty(EmbeddedQueryQueue.ENABLED, true);\n+\n+    try (ClusterFixture cluster = builder.build();\n+         ClientFixture client = cluster.clientFixture()) {\n+      client\n+        .queryBuilder()\n+        .sql(sql)\n+        .run();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/d29a5b1749a77b863be3eac1423f4ecf6d244ebf/exec/java-exec/src/test/java/org/apache/drill/exec/planner/rm/TestMemoryCalculator.java",
                "sha": "891b4a6a2262ce01dd85a70fdcfeaf96426d37fe",
                "status": "modified"
            }
        ],
        "message": "DRILL-7146: Query failing with NPE when ZK queue is enabled.",
        "parent": "https://github.com/apache/drill/commit/54384a992a0742aeab23afa82c7b7f4adcd388d3",
        "patched_files": [
            "MemoryCalculator.java",
            "QueueQueryParallelizer.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestMemoryCalculator.java"
        ]
    },
    "drill_e477480": {
        "bug_id": "drill_e477480",
        "commit": "https://github.com/apache/drill/commit/e477480e7b9626abc8efd70914d3bfd4321b7258",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.drill.exec.record.MaterializedField;\n import org.apache.drill.exec.record.RecordBatch;\n import org.apache.drill.exec.record.TypedFieldId;\n+import org.apache.drill.exec.server.options.OptionManager;\n import org.apache.drill.exec.store.ColumnExplorer;\n import org.apache.drill.exec.vector.ValueVector;\n import org.apache.drill.exec.vector.complex.FieldIdUtil;\n@@ -90,8 +91,9 @@ public StatisticsAggBatch(StatisticsAggregate popConfig, RecordBatch incoming,\n   /*\n    * Returns whether the given column is an implicit column\n    */\n-  private boolean isImplicitFileColumn(MaterializedField mf) {\n-    return implicitFileColumnsMap.get(SchemaPath.getSimplePath(mf.getName()).toString()) != null;\n+  private boolean isImplicitFileOrPartitionColumn(MaterializedField mf, OptionManager optionManager) {\n+    return implicitFileColumnsMap.get(SchemaPath.getSimplePath(mf.getName()).toString()) != null ||\n+       ColumnExplorer.isPartitionColumn(optionManager, SchemaPath.getSimplePath(mf.getName()));\n   }\n \n   /*\n@@ -191,7 +193,7 @@ protected StreamingAggregator createAggregatorInternal()\n           expr = ValueExpressions.getChar(DrillStatsTable.getMapper().writeValueAsString(mf.getType()), 0);\n         }\n         // Ignore implicit columns\n-        if (!isImplicitFileColumn(mf)) {\n+        if (!isImplicitFileOrPartitionColumn(mf, incoming.getContext().getOptions())) {\n           createNestedKeyColumn(\n               parent,\n               SchemaPath.getSimplePath(mf.getName()).toString(),\n@@ -213,7 +215,7 @@ protected StreamingAggregator createAggregatorInternal()\n       for (MaterializedField mf : incoming.getSchema()) {\n         // Check stats collection is only being done for supported data-types. Complex types\n         // such as MAP, LIST are not supported!\n-        if (isColMinorTypeValid(mf) && !isImplicitFileColumn(mf)) {\n+        if (isColMinorTypeValid(mf) && !isImplicitFileOrPartitionColumn(mf, incoming.getContext().getOptions())) {\n           List<LogicalExpression> args = Lists.newArrayList();\n           args.add(SchemaPath.getSimplePath(mf.getName()));\n           LogicalExpression call = FunctionCallFactory.createExpression(func, args);",
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/statistics/StatisticsAggBatch.java",
                "sha": "40fc445ec6dc16846ce4b490187747353e6bad17",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258",
                "deletions": 11,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "patch": "@@ -26,14 +26,14 @@\n import com.fasterxml.jackson.annotation.JsonTypeName;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.module.SimpleModule;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import org.apache.calcite.rel.RelNode;\n import org.apache.calcite.rel.RelVisitor;\n import org.apache.calcite.rel.core.TableScan;\n-import org.apache.drill.common.exceptions.DrillRuntimeException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.types.TypeProtos;\n@@ -196,7 +196,7 @@ public Histogram getHistogram(String col) {\n    * @param context - Query context\n    * @throws Exception\n    */\n-  public void materialize(final DrillTable table, final QueryContext context) throws IOException {\n+  public void materialize(final DrillTable table, final QueryContext context) {\n     if (materialized) {\n       return;\n     }\n@@ -223,8 +223,11 @@ public void materialize(final DrillTable table, final QueryContext context) thro\n         materialized = true;\n       }\n     } catch (IOException ex) {\n-      logger.warn(\"Failed to read the stats file.\", ex);\n-      throw ex;\n+      if (ex instanceof FileNotFoundException) {\n+        logger.debug(String.format(\"Did not find statistics file %s\", tablePath.toString()), ex);\n+      } else {\n+        logger.debug(String.format(\"Error trying to read statistics table %s\", tablePath.toString()), ex);\n+      }\n     }\n   }\n \n@@ -247,16 +250,15 @@ public void visit(RelNode node, int ordinal, RelNode parent) {\n       if (node instanceof TableScan) {\n         try {\n           final DrillTable drillTable = Utilities.getDrillTable(node.getTable());\n-          final DrillStatsTable statsTable = drillTable.getStatsTable();\n-          if (statsTable != null) {\n+          final DrillStatsTable statsTable = drillTable != null ? drillTable.getStatsTable() : null;\n+          if (drillTable != null && statsTable != null) {\n             statsTable.materialize(drillTable, context);\n-          } else {\n-            throw new DrillRuntimeException(\n-                String.format(\"Failed to find the stats for table [%s] in schema [%s]\",\n-                    node.getTable().getQualifiedName(), node.getTable().getRelOptSchema()));\n+          } else if (drillTable != null) {\n+            logger.debug(String.format(\"Unable to find statistics table info for table [%s] in schema [%s]\",\n+                node.getTable().getQualifiedName(), node.getTable().getRelOptSchema()));\n           }\n         } catch (Exception e) {\n-          // Log a warning and proceed. We don't want to fail a query.\n+          // Something unexpected happened! Log a warning and proceed. We don't want to fail the query.\n           logger.warn(\"Failed to materialize the stats. Continuing without stats.\", e);\n         }\n       }",
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillStatsTable.java",
                "sha": "65b0b6421e3ea1614e690ebb123b21d26ad89b71",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java?ref=e477480e7b9626abc8efd70914d3bfd4321b7258",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "patch": "@@ -170,6 +170,7 @@ public void testAnalyzeSupportedFormats() throws Exception {\n     }\n   }\n \n+  @Ignore(\"For 1.16.0, we do not plan to support statistics on dir columns\")\n   @Test\n   public void testAnalyzePartitionedTables() throws Exception {\n     //Computing statistics on columns, dir0, dir1\n@@ -330,7 +331,6 @@ public void testUseStatistics() throws Exception {\n     PlanTestBase.testPlanWithAttributesMatchingPatterns(query, expectedPlan11, new String[]{});\n   }\n \n-  @Ignore(\"Fails intermittently. Enable after fixing the issue.\")\n   @Test\n   public void testWithMetadataCaching() throws Exception {\n     test(\"ALTER SESSION SET `planner.slice_target` = 1\");",
                "raw_url": "https://github.com/apache/drill/raw/e477480e7b9626abc8efd70914d3bfd4321b7258/exec/java-exec/src/test/java/org/apache/drill/exec/sql/TestAnalyze.java",
                "sha": "30d23b3d9816456199b0c1b4c3408b7f127b1d75",
                "status": "modified"
            }
        ],
        "message": "DRILL-7076: Fix NPE in StatsMaterializationVisitor\n\ncloses #1722",
        "parent": "https://github.com/apache/drill/commit/d29a5b1749a77b863be3eac1423f4ecf6d244ebf",
        "patched_files": [
            "StatisticsAggBatch.java",
            "Analyze.java",
            "DrillStatsTable.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestAnalyze.java"
        ]
    },
    "drill_ecd8c81": {
        "bug_id": "drill_ecd8c81",
        "commit": "https://github.com/apache/drill/commit/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "patch": "@@ -406,8 +406,19 @@ private void addDataToTable(Packet packet, int networkType, RowSetLoader rowWrit\n     srcMacAddressWriter.setString(packet.getEthernetSource());\n     dstMacAddressWriter.setString(packet.getEthernetDestination());\n \n-    dstIPWriter.setString(packet.getDst_ip().getHostAddress());\n-    srcIPWriter.setString(packet.getSrc_ip().getHostAddress());\n+    String destinationIp = packet.getDestinationIpAddressString();\n+    if (destinationIp == null) {\n+      dstIPWriter.setNull();\n+    } else {\n+      dstIPWriter.setString(destinationIp);\n+    }\n+\n+    String sourceIp = packet.getSourceIpAddressString();\n+    if (sourceIp == null) {\n+      srcIPWriter.setNull();\n+    } else {\n+      srcIPWriter.setString(sourceIp);\n+    }\n     srcPortWriter.setInt(packet.getSrc_port());\n     dstPortWriter.setInt(packet.getDst_port());\n     packetLengthWriter.setInt(packet.getPacketLength());",
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapBatchReader.java",
                "sha": "4aeff2f359d8c846918052c4c691ad0480ed2b15",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "patch": "@@ -183,6 +183,24 @@ public InetAddress getDst_ip() {\n     return getIPAddress(false);\n   }\n \n+  public String getSourceIpAddressString() {\n+    InetAddress address = getSrc_ip();\n+    if (address == null) {\n+      return null;\n+    } else {\n+      return address.getHostAddress();\n+    }\n+  }\n+\n+  public String getDestinationIpAddressString() {\n+    InetAddress address = getDst_ip();\n+    if (address == null) {\n+      return null;\n+    } else {\n+      return address.getHostAddress();\n+    }\n+  }\n+\n   public String getEthernetSource() {\n     return getEthernetAddress(PacketConstants.ETHER_SRC_OFFSET);\n   }",
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/decoder/Packet.java",
                "sha": "148f94fa6c5ca1c4236a7af47f014685003ae9a1",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "patch": "@@ -99,4 +99,20 @@ public void testAggregateQuery() throws Exception {\n       .baselineValues(true, 16L)\n       .go();\n   }\n-}\n\\ No newline at end of file\n+\n+  @Test\n+  public void testArpPcapFile() throws Exception {\n+    String sql = \"SELECT src_ip, dst_ip FROM cp.`store/pcap/arpWithNullIP.pcap` WHERE src_port=1\";\n+    testBuilder()\n+      .sqlQuery(sql)\n+      .ordered()\n+      .baselineColumns(\"src_ip\", \"dst_ip\")\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .baselineValues((String)null, (String)null)\n+      .go();\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapEVFReader.java",
                "sha": "3b878c3b8141e951d019f336303b5164c84e65e0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap?ref=ecd8c81e039a7e9aa0590ffd8e705782f3b998c8",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "raw_url": "https://github.com/apache/drill/raw/ecd8c81e039a7e9aa0590ffd8e705782f3b998c8/exec/java-exec/src/test/resources/store/pcap/arpWithNullIP.pcap",
                "sha": "1485811381a24acdded5f35af97b215e904075ac",
                "status": "added"
            }
        ],
        "message": "DRILL-7485: NPE on PCAP Batch Reader\n\ncloses #1932",
        "parent": "https://github.com/apache/drill/commit/5339fc23eb1b177bbc20ed74637e6b11e3ffa803",
        "patched_files": [
            "arpWithNullIP.pcap",
            "Packet.java",
            "PcapBatchReader.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestPcapEVFReader.java"
        ]
    },
    "drill_ed51b3f": {
        "bug_id": "drill_ed51b3f",
        "commit": "https://github.com/apache/drill/commit/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java?ref=ed51b3f95ee4eaa3694629b1181b9e8cd13b932e",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "patch": "@@ -54,7 +54,6 @@ public static void main(String args[]) throws Exception {\n       System.out.println(e.getMessage());\n       String[] valid = {\"-f\", \"file\"};\n       new JCommander(o, valid).usage();\n-      jc.usage();\n       System.exit(-1);\n     }\n     if (o.help) {",
                "raw_url": "https://github.com/apache/drill/raw/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "sha": "7e8a4a26dca4f8d621836672e24b3adc7492c967",
                "status": "modified"
            }
        ],
        "message": "DRILL-559: Fix NPE in dumpcat when arguments are not provided",
        "parent": "https://github.com/apache/drill/commit/3f92e56aeb768cac90495951befaa0f03f13372e",
        "patched_files": [
            "DumpCat.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "DumpCatTest.java"
        ]
    },
    "drill_f88a73c": {
        "bug_id": "drill_f88a73c",
        "commit": "https://github.com/apache/drill/commit/f88a73c9e7a75f2d08cc54188816f591b003eff4",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 39,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "patch": "@@ -22,10 +22,8 @@\n import org.apache.drill.exec.expr.annotations.FunctionTemplate;\n import org.apache.drill.exec.expr.annotations.Output;\n import org.apache.drill.exec.expr.annotations.Param;\n-import org.apache.drill.exec.expr.annotations.Workspace;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n-import javax.crypto.Cipher;\n import javax.inject.Inject;\n \n public class CryptoFunctions {\n@@ -271,34 +269,25 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.ENCRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String encryptedText = \"\";\n       try {\n-        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(\"UTF-8\")));\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.ENCRYPT_MODE, secretKey);\n+        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(java.nio.charset.StandardCharsets.UTF_8)));\n       } catch (Exception e) {\n         //Exceptions are ignored\n       }\n@@ -331,33 +320,24 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.DECRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String decryptedText = \"\";\n       try {\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.DECRYPT_MODE, secretKey);\n         decryptedText = new String(cipher.doFinal(javax.xml.bind.DatatypeConverter.parseBase64Binary(input)));\n       } catch (Exception e) {\n         //Exceptions are ignored",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "sha": "f914fb99a371b72f88072d0a8cf121d308909e26",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "changes": 223,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 114,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "patch": "@@ -24,6 +24,8 @@\n import org.apache.drill.exec.expr.annotations.Param;\n import org.apache.drill.exec.expr.holders.BigIntHolder;\n import org.apache.drill.exec.expr.holders.BitHolder;\n+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;\n+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n import javax.inject.Inject;\n@@ -50,18 +52,15 @@ public void setup() {\n \n \n     public void eval() {\n-\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n \n-      int result = 0;\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      if (utils.getInfo().isInRange(ipString)) {\n-        result = 1;\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().isInRange(ipString) ? 1 : 0;\n+      } catch (IllegalArgumentException e) {\n+        // return false in case of invalid input\n       }\n-\n-      out.value = result;\n     }\n   }\n \n@@ -76,17 +75,20 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      out.value = utils.getInfo().getAddressCountLong();\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().getAddressCountLong();\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null in case of invalid input\n+      }\n     }\n \n   }\n@@ -101,7 +103,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -110,16 +112,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getBroadcastAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getBroadcastAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -134,7 +139,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -143,16 +148,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getNetmask();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getNetmask();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -167,7 +175,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -176,16 +184,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getLowAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getLowAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -200,7 +211,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -209,24 +220,27 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getHighAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getHighAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n   }\n \n   /**\n    * This function encodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_encode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urlencodeFunction implements DrillSimpleFunc {\n+  public static class UrlEncodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -261,7 +275,7 @@ public void eval() {\n    * This function decodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_decode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urldecodeFunction implements DrillSimpleFunc {\n+  public static class UrlDecodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -308,27 +322,20 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-\n     public void setup() {\n     }\n \n \n     public void eval() {\n       StringBuilder result = new StringBuilder(15);\n-\n       long inputInt = in.value;\n-\n       for (int i = 0; i < 4; i++) {\n-\n         result.insert(0, Long.toString(inputInt & 0xff));\n-\n         if (i < 3) {\n           result.insert(0, '.');\n         }\n-\n         inputInt = inputInt >> 8;\n       }\n-\n       String outputValue = result.toString();\n \n       out.buffer = buffer;\n@@ -356,13 +363,26 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n \n       String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      if (ipAddressInArray.length < 3) {\n+        return;\n+      }\n \n-      int[] octets = new int[3];\n-\n-      for (int i = 0; i < 3; i++) {\n-        octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+      // only first two octets are needed for the check\n+      int[] octets = new int[2];\n+      for (int i = 0; i < 2; i++) {\n+        try {\n+          octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return false\n+          return;\n+        }\n       }\n \n       int result = 0;\n@@ -392,28 +412,34 @@ public void eval() {\n     VarCharHolder inputTextA;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        String[] ipAddressInArray = ipString.split(\"\\\\.\");\n-\n-        long result = 0;\n-        for (int i = 0; i < ipAddressInArray.length; i++) {\n-          int power = 3 - i;\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n+\n+      String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      long result = 0;\n+      for (int i = 0; i < ipAddressInArray.length; i++) {\n+        int power = 3 - i;\n+        try {\n           int ip = Integer.parseInt(ipAddressInArray[i]);\n           result += ip * Math.pow(256, power);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return null\n+          return;\n         }\n-\n-        out.value = result;\n       }\n+      out.value = result;\n+      out.isSet = 1;\n     }\n   }\n \n@@ -435,18 +461,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValid(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValid(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -465,21 +481,10 @@ public void eval() {\n     public void setup() {\n     }\n \n-\n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet4Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet4Address(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -500,18 +505,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet6Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet6Address(ipString) ? 1 : 0;\n     }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "sha": "0dbaf87a1db236afc6433d1221be74991e526925",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 0,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "patch": "@@ -383,6 +383,7 @@ public void eval() {\n \n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String outputString = new org.apache.commons.codec.language.DoubleMetaphone().doubleMetaphone(input);\n+      outputString = outputString == null ? \"\" : outputString;\n \n       out.buffer = buffer;\n       out.start = 0;",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "sha": "66ab0da92aefe46b80d107f7a31d11662d9a7342",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 6,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "patch": "@@ -17,14 +17,23 @@\n  */\n package org.apache.drill.exec.udfs;\n \n-import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestCryptoFunctions extends BaseTestQuery {\n+public class TestCryptoFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testMD5() throws Exception {\n@@ -74,23 +83,48 @@ public void testSHA512() throws Exception {\n \n   @Test\n   public void testAESEncrypt() throws Exception {\n-    final String query = \"select aes_encrypt('testing', 'secret_key') as encrypted FROM (VALUES(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_encrypt('testing', 'secret_key') as encrypted from (values(1))\")\n       .ordered()\n       .baselineColumns(\"encrypted\")\n       .baselineValues(\"ICf+zdOrLitogB8HUDru0w==\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt(cast(null as varchar), 'secret_key') as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt('testing', cast (null as varchar)) as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n \n   @Test\n   public void testAESDecrypt() throws Exception {\n-    final String query = \"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\")\n       .ordered()\n       .baselineColumns(\"decrypt\")\n       .baselineValues(\"testing\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt(cast(null as varchar), 'secret_key') as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', cast(null as varchar)) as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "sha": "d382e96856186d676c21a1d9ad4e74f95610622e",
                "status": "modified"
            },
            {
                "additions": 125,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "changes": 139,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 14,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "patch": "@@ -19,23 +19,44 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseTestQuery;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestNetworkFunctions extends BaseTestQuery {\n+public class TestNetworkFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testInetAton() throws Exception {\n-    final String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n-    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(Long.parseLong(\"3232235521\")).go();\n+    String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(3232235521L).go();\n+\n+    query = \"select inet_aton('192.168.0') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton('') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton(cast(null as varchar)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n   }\n \n   @Test\n   public void testInetNtoa() throws Exception {\n-    final String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n+    String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select inet_ntoa(cast(null as int)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -46,32 +67,68 @@ public void testInNetwork() throws Exception {\n \n   @Test\n   public void testNotInNetwork() throws Exception {\n-    final String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net FROM (values(1))\";\n+    String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('10.10.10.10', '') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('', '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network(cast(null as varchar), '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n+\n+    query = \"select in_network('10.10.10.10', cast(null as varchar)) as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n   public void testBroadcastAddress() throws Exception {\n-    final String query = \"select broadcast_address( '192.168.0.0/28' ) AS broadcast_address FROM (values(1))\";\n+    String query = \"select broadcast_address('192.168.0.0/28') as broadcast_address from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues(\"192.168.0.15\").go();\n+\n+    query = \"select broadcast_address('192.168.') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n+\n+    query = \"select broadcast_address('') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testNetmask() throws Exception {\n-    final String query = \"select netmask('192.168.0.0/28') AS netmask FROM (values(1))\";\n+    String query = \"select netmask('192.168.0.0/28') as netmask from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues(\"255.255.255.240\").go();\n+\n+    query = \"select netmask('192222') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n+\n+    query = \"select netmask('') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testLowAddress() throws Exception {\n-    final String query = \"SELECT low_address('192.168.0.0/28') AS low FROM (values(1))\";\n+    String query = \"select low_address('192.168.0.0/28') as low from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testHighAddress() throws Exception {\n-    final String query = \"SELECT high_address('192.168.0.0/28') AS high FROM (values(1))\";\n+    String query = \"select high_address('192.168.0.0/28') as high from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues(\"192.168.0.14\").go();\n+\n+    query = \"select high_address('192.168.0.') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n+\n+    query = \"select high_address('') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -88,8 +145,20 @@ public void testDecodeUrl() throws Exception {\n \n   @Test\n   public void testNotPrivateIP() throws Exception {\n-    final String query = \"SELECT is_private_ip('8.8.8.8') AS is_private_ip FROM (values(1))\";\n+    String query = \"select is_private_ip('8.8.8.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('8.A.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('192.168') as is_private_ip from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip(cast(null as varchar)) as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -100,8 +169,17 @@ public void testPrivateIP() throws Exception {\n \n   @Test\n   public void testNotValidIP() throws Exception {\n-    final String query = \"SELECT is_valid_IP('258.257.234.23') AS is_valid_IP FROM (values(1))\";\n+    String query = \"select is_valid_IP('258.257.234.23') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('258.257.2') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('') as is_valid_IP from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP(cast(null as varchar)) as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -112,8 +190,17 @@ public void testIsValidIP() throws Exception {\n \n   @Test\n   public void testNotValidIPv4() throws Exception {\n-    final String query = \"SELECT is_valid_IPv4( '192.168.0.257') AS is_valid_IP4 FROM (values(1))\";\n+    String query = \"select is_valid_IPv4('192.168.0.257') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('192123') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('') as is_valid_IP4 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4(cast(null as varchar)) as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -130,8 +217,32 @@ public void testIsValidIPv6() throws Exception {\n \n   @Test\n   public void testNotValidIPv6() throws Exception {\n-    final String query = \"SELECT is_valid_IPv6('1050:0:0:0:5:600:300c:326g') AS is_valid_IP6 FROM (values(1))\";\n+    String query = \"select is_valid_IPv6('1050:0:0:0:5:600:300c:326g') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('1050:0:0:0:5:600_AAA') as is_valid_IP6 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6(cast(null as varchar)) as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues((Boolean) null).go();\n+  }\n+\n+  @Test\n+  public void testAddressCount() throws Exception {\n+    String query = \"select address_count('192.168.0.1/30') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues(2L).go();\n+\n+    query = \"select address_count('192.168') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('192.168.0.1/100') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n   }\n \n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "sha": "349e09794b015c97a3567a58b3e35739120d283d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 5,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "patch": "@@ -19,12 +19,10 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseDirTestWatcher;\n import org.apache.drill.test.ClusterFixture;\n import org.apache.drill.test.ClusterFixtureBuilder;\n import org.apache.drill.test.ClusterTest;\n import org.junit.BeforeClass;\n-import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n@@ -33,9 +31,6 @@\n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n public class TestPhoneticFunctions extends ClusterTest {\n \n-  @Rule\n-  public final BaseDirTestWatcher baseDirTestWatcher = new BaseDirTestWatcher();\n-\n   @BeforeClass\n   public static void setup() throws Exception {\n     ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n@@ -112,5 +107,10 @@ public void testDoubleMetaphone() throws Exception {\n         .sql(\"SELECT double_metaphone('Phoenix') AS meta FROM (VALUES(1))\")\n         .singletonString();\n     assertEquals(\"FNKS\", result);\n+\n+    result = queryBuilder()\n+        .sql(\"SELECT double_metaphone('') AS meta FROM (VALUES(1))\")\n+        .singletonString();\n+    assertEquals(\"\", result);\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "sha": "64fb6daa0b124dc9c0af21c208140f891b2f2703",
                "status": "modified"
            }
        ],
        "message": "DRILL-6705: Fix various failures in Crypto / Network / Phonetic functions when invalid input is given\n\n1. aes_decrypt / aes_ecrypt - moved cyper init part into eval method since it not a constant and can be different for each input\n2. double_metaphone - fixed NPE when given string is empty\n3. in_network / address_count / broadcast_address / netmask / low_address / high_address / - fixed IllegalArgumentException in case of invalid input\n4. is_private_ip / inet_aton - fixed ArrayIndexOutOfBoundsException / NumberFormatException in case of invalid input\n5. is_valid_IP / is_valid_IPv4 / is_valid_IPv6 - removed unnecessary checks\n6. Added appropriate unit tests\n\ncloses #1443",
        "parent": "https://github.com/apache/drill/commit/ea83672edc75fe379a4e19764232b10f5199d06e",
        "patched_files": [
            "CryptoFunctions.java",
            "NetworkFunctions.java",
            "PhoneticFunctions.java"
        ],
        "repo": "drill",
        "unit_tests": [
            "TestCryptoFunctions.java",
            "TestNetworkFunctions.java",
            "TestPhoneticFunctions.java"
        ]
    }
}