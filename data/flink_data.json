[{"commit": "https://github.com/apache/flink/commit/f257cff0b5251b29c8540b0861855c3a73522541", "parent": "https://github.com/apache/flink/commit/49af1027c84edd8ded770bdb648b31485bd91417", "message": "[hotfix] Inverse comparison order in ExecutorFactory.isCompatibleWith to avoiod NPE", "bug_id": "flink_1", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/f257cff0b5251b29c8540b0861855c3a73522541/flink-clients/src/main/java/org/apache/flink/client/deployment/executors/StandaloneSessionClusterExecutorFactory.java", "blob_url": "https://github.com/apache/flink/blob/f257cff0b5251b29c8540b0861855c3a73522541/flink-clients/src/main/java/org/apache/flink/client/deployment/executors/StandaloneSessionClusterExecutorFactory.java", "sha": "c6c6332e93c27efac5a17c9735437dc92420bed5", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/main/java/org/apache/flink/client/deployment/executors/StandaloneSessionClusterExecutorFactory.java?ref=f257cff0b5251b29c8540b0861855c3a73522541", "patch": "@@ -34,8 +34,7 @@\n \n \t@Override\n \tpublic boolean isCompatibleWith(@Nonnull final Configuration configuration) {\n-\t\treturn configuration.get(DeploymentOptions.TARGET)\n-\t\t\t\t.equalsIgnoreCase(StandaloneSessionClusterExecutor.NAME);\n+\t\treturn StandaloneSessionClusterExecutor.NAME.equalsIgnoreCase(configuration.get(DeploymentOptions.TARGET));\n \t}\n \n \t@Override", "filename": "flink-clients/src/main/java/org/apache/flink/client/deployment/executors/StandaloneSessionClusterExecutorFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/flink/raw/f257cff0b5251b29c8540b0861855c3a73522541/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnJobClusterExecutorFactory.java", "blob_url": "https://github.com/apache/flink/blob/f257cff0b5251b29c8540b0861855c3a73522541/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnJobClusterExecutorFactory.java", "sha": "0e8ab953c4ab123af255525818ee188690115c81", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnJobClusterExecutorFactory.java?ref=f257cff0b5251b29c8540b0861855c3a73522541", "patch": "@@ -34,8 +34,7 @@\n \n \t@Override\n \tpublic boolean isCompatibleWith(@Nonnull final Configuration configuration) {\n-\t\treturn configuration.get(DeploymentOptions.TARGET)\n-\t\t\t\t.equalsIgnoreCase(YarnJobClusterExecutor.NAME);\n+\t\treturn YarnJobClusterExecutor.NAME.equalsIgnoreCase(configuration.get(DeploymentOptions.TARGET));\n \t}\n \n \t@Override", "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnJobClusterExecutorFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/flink/raw/f257cff0b5251b29c8540b0861855c3a73522541/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnSessionClusterExecutorFactory.java", "blob_url": "https://github.com/apache/flink/blob/f257cff0b5251b29c8540b0861855c3a73522541/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnSessionClusterExecutorFactory.java", "sha": "3ae140791e7477cd16f368a1f17cb3ad733174ae", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnSessionClusterExecutorFactory.java?ref=f257cff0b5251b29c8540b0861855c3a73522541", "patch": "@@ -34,8 +34,7 @@\n \n \t@Override\n \tpublic boolean isCompatibleWith(@Nonnull final Configuration configuration) {\n-\t\treturn configuration.get(DeploymentOptions.TARGET)\n-\t\t\t\t.equalsIgnoreCase(YarnSessionClusterExecutor.NAME);\n+\t\treturn YarnSessionClusterExecutor.NAME.equalsIgnoreCase(configuration.get(DeploymentOptions.TARGET));\n \t}\n \n \t@Override", "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/executors/YarnSessionClusterExecutorFactory.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/26bc3c8c65c757285c58b2cfcb0ba81111395ea4", "parent": "https://github.com/apache/flink/commit/f22f1eba8f7695857a2015ed178365191849dac4", "message": "[FLINK-14337][hs] Prevent NPE on corrupt archives", "bug_id": "flink_2", "file": [{"additions": 12, "raw_url": "https://github.com/apache/flink/raw/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java", "blob_url": "https://github.com/apache/flink/blob/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java", "sha": "ab1e34d74078a89fda138bc8d4fcc9c656e58700", "changes": 19, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java?ref=26bc3c8c65c757285c58b2cfcb0ba81111395ea4", "patch": "@@ -109,15 +109,20 @@ public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJso\n \t\t\tByteArrayOutputStream output = new ByteArrayOutputStream()) {\n \t\t\tIOUtils.copyBytes(input, output);\n \n-\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n+\t\t\ttry {\n+\t\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n \n-\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n-\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n-\t\t\t\tString path = archivePart.get(PATH).asText();\n-\t\t\t\tString json = archivePart.get(JSON).asText();\n-\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n+\t\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n+\t\t\t\t\tString path = archivePart.get(PATH).asText();\n+\t\t\t\t\tString json = archivePart.get(JSON).asText();\n+\t\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\t}\n+\t\t\t\treturn archives;\n+\t\t\t} catch (NullPointerException npe) {\n+\t\t\t\t// occurs if the archive is empty or any of the expected fields are not present\n+\t\t\t\tthrow new IOException(\"Job archive (\" + file.getPath() + \") did not conform to expected format.\");\n \t\t\t}\n-\t\t\treturn archives;\n \t\t}\n \t}\n }", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/101552bf503cf0ca59493397ec4cd01bcc4c45a7", "parent": "https://github.com/apache/flink/commit/886419f12f60df803c9d757e381f201920a8061a", "message": "[FLINK-13159] Fix the NPE when PojoSerializer restored", "bug_id": "flink_3", "file": [{"additions": 2, "raw_url": "https://github.com/apache/flink/raw/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java", "blob_url": "https://github.com/apache/flink/blob/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java", "sha": "5c43d1e172eba312bfc6efdb385bc1add3d852fa", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java?ref=101552bf503cf0ca59493397ec4cd01bcc4c45a7", "patch": "@@ -123,7 +123,7 @@ public PojoSerializer(\n \t\t\tthis.fields[i].setAccessible(true);\n \t\t}\n \n-\t\tcl = Thread.currentThread().getContextClassLoader();\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \n \t\t// We only want those classes that are not our own class and are actually sub-classes.\n \t\tLinkedHashSet<Class<?>> registeredSubclasses =\n@@ -156,6 +156,7 @@ public PojoSerializer(\n \t\tthis.registeredSerializers = checkNotNull(registeredSerializers);\n \t\tthis.subclassSerializerCache = checkNotNull(subclassSerializerCache);\n \t\tthis.executionConfig = checkNotNull(executionConfig);\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \t}\n \t\n \t@Override", "filename": "flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/ce5de6be563ffe6e36b8f917a7f500ffc2b49c08", "parent": "https://github.com/apache/flink/commit/09f96b339f4890d7a44ae92c915ea8c0f6f244cb", "message": "[FLINK-11321] Clarify NPE on fetching nonexistent topic (#7487)\n\n[FLINK-11321][connector/kafka] Throw explicit exception on fetching nonexistent topic.\r\n\r\nBefore the fix, NPE will be thrown if a consumer is trying to fetch from a nonexistent topic. The fix throws a RuntimeException with a clear message.", "bug_id": "flink_4", "file": [{"additions": 8, "raw_url": "https://github.com/apache/flink/raw/ce5de6be563ffe6e36b8f917a7f500ffc2b49c08/flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09PartitionDiscoverer.java", "blob_url": "https://github.com/apache/flink/blob/ce5de6be563ffe6e36b8f917a7f500ffc2b49c08/flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09PartitionDiscoverer.java", "sha": "a9ebaeb16b5fc0d387c85dacf6e207c7796adcd1", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09PartitionDiscoverer.java?ref=ce5de6be563ffe6e36b8f917a7f500ffc2b49c08", "patch": "@@ -69,12 +69,18 @@ protected void initializeConnections() {\n \t}\n \n \t@Override\n-\tprotected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws WakeupException {\n+\tprotected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws WakeupException, RuntimeException {\n \t\tList<KafkaTopicPartition> partitions = new LinkedList<>();\n \n \t\ttry {\n \t\t\tfor (String topic : topics) {\n-\t\t\t\tfor (PartitionInfo partitionInfo : kafkaConsumer.partitionsFor(topic)) {\n+\t\t\t\tfinal List<PartitionInfo> kafkaPartitions = kafkaConsumer.partitionsFor(topic);\n+\n+\t\t\t\tif (kafkaPartitions == null) {\n+\t\t\t\t\tthrow new RuntimeException(\"Could not fetch partitions for %s. Make sure that the topic exists.\".format(topic));\n+\t\t\t\t}\n+\n+\t\t\t\tfor (PartitionInfo partitionInfo : kafkaPartitions) {\n \t\t\t\t\tpartitions.add(new KafkaTopicPartition(partitionInfo.topic(), partitionInfo.partition()));\n \t\t\t\t}\n \t\t\t}", "filename": "flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09PartitionDiscoverer.java"}, {"additions": 10, "raw_url": "https://github.com/apache/flink/raw/ce5de6be563ffe6e36b8f917a7f500ffc2b49c08/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaPartitionDiscoverer.java", "blob_url": "https://github.com/apache/flink/blob/ce5de6be563ffe6e36b8f917a7f500ffc2b49c08/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaPartitionDiscoverer.java", "sha": "1c871bca42366caad6a3442f721e61daf1585b4b", "changes": 14, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaPartitionDiscoverer.java?ref=ce5de6be563ffe6e36b8f917a7f500ffc2b49c08", "patch": "@@ -69,18 +69,24 @@ protected void initializeConnections() {\n \t}\n \n \t@Override\n-\tprotected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws AbstractPartitionDiscoverer.WakeupException {\n-\t\tList<KafkaTopicPartition> partitions = new LinkedList<>();\n+\tprotected List<KafkaTopicPartition> getAllPartitionsForTopics(List<String> topics) throws WakeupException, RuntimeException {\n+\t\tfinal List<KafkaTopicPartition> partitions = new LinkedList<>();\n \n \t\ttry {\n \t\t\tfor (String topic : topics) {\n-\t\t\t\tfor (PartitionInfo partitionInfo : kafkaConsumer.partitionsFor(topic)) {\n+\t\t\t\tfinal List<PartitionInfo> kafkaPartitions = kafkaConsumer.partitionsFor(topic);\n+\n+\t\t\t\tif (kafkaPartitions == null) {\n+\t\t\t\t\tthrow new RuntimeException(\"Could not fetch partitions for %s. Make sure that the topic exists.\".format(topic));\n+\t\t\t\t}\n+\n+\t\t\t\tfor (PartitionInfo partitionInfo : kafkaPartitions) {\n \t\t\t\t\tpartitions.add(new KafkaTopicPartition(partitionInfo.topic(), partitionInfo.partition()));\n \t\t\t\t}\n \t\t\t}\n \t\t} catch (org.apache.kafka.common.errors.WakeupException e) {\n \t\t\t// rethrow our own wakeup exception\n-\t\t\tthrow new AbstractPartitionDiscoverer.WakeupException();\n+\t\t\tthrow new WakeupException();\n \t\t}\n \n \t\treturn partitions;", "filename": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaPartitionDiscoverer.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/b2c592a87139c587777f9897613943639fac1d61", "parent": "https://github.com/apache/flink/commit/861bf73ada01e4d2d8c671e507974e5bfacd9218", "message": "[hotfix] [connectors] Fix shadowed NPE in elasticsearch sink connector\n\nThis closes #8849.", "bug_id": "flink_5", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java", "blob_url": "https://github.com/apache/flink/blob/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java", "sha": "96f4431493c9f6348f8675439391558c242360d7", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java?ref=b2c592a87139c587777f9897613943639fac1d61", "patch": "@@ -427,7 +427,7 @@ public void afterBulk(long executionId, BulkRequest request, BulkResponse respon\n \n \t\t@Override\n \t\tpublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n-\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure.getCause());\n+\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure);\n \n \t\t\ttry {\n \t\t\t\tfor (ActionRequest action : request.requests()) {", "filename": "flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/4064b5b67d6d220e1d5518bca96688f51cbbb891", "parent": "https://github.com/apache/flink/commit/c5c59feec0ed8a0ac5213802d79956c815a2b812", "message": "[FLINK-14315] Make heartbeat manager fields non-nullable\n\nThis commit introduces the NoOpHeartbeatManager which can be used to initialize\nan unset heartbeat manager field. This allows to make the heartbeat manager fields\nnon-nullable which in turn avoid NPE.\n\nMoreover, this commit makes the heartbeat manager fields of the TaskExecutor\nfinal.\n\nThis closes #9837.", "bug_id": "flink_6", "file": [{"additions": 58, "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java", "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java", "sha": "965a50b3f3539236a7e706dd51137b6b66ffcc57", "changes": 58, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891", "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.heartbeat;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+\n+/**\n+ * {@link HeartbeatManager} implementation which does nothing.\n+ *\n+ * @param <I> ignored\n+ * @param <O> ignored\n+ */\n+public class NoOpHeartbeatManager<I, O> implements HeartbeatManager<I, O> {\n+\tprivate static final NoOpHeartbeatManager<Object, Object> INSTANCE = new NoOpHeartbeatManager<>();\n+\n+\tprivate NoOpHeartbeatManager() {}\n+\n+\t@Override\n+\tpublic void monitorTarget(ResourceID resourceID, HeartbeatTarget<O> heartbeatTarget) {}\n+\n+\t@Override\n+\tpublic void unmonitorTarget(ResourceID resourceID) {}\n+\n+\t@Override\n+\tpublic void stop() {}\n+\n+\t@Override\n+\tpublic long getLastHeartbeatFrom(ResourceID resourceId) {\n+\t\treturn 0;\n+\t}\n+\n+\t@Override\n+\tpublic void receiveHeartbeat(ResourceID heartbeatOrigin, I heartbeatPayload) {}\n+\n+\t@Override\n+\tpublic void requestHeartbeat(ResourceID requestOrigin, I heartbeatPayload) {}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic static <A, B> NoOpHeartbeatManager<A, B> getInstance() {\n+\t\treturn (NoOpHeartbeatManager<A, B>) INSTANCE;\n+\t}\n+}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java"}, {"additions": 5, "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java", "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java", "sha": "665c4aa479d08a98efeed8e1c6b3fee49a99cb38", "changes": 14, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891", "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.io.network.partition.PartitionTracker;\n import org.apache.flink.runtime.io.network.partition.PartitionTrackerFactory;\n@@ -269,6 +270,8 @@ public JobMaster(\n \t\tthis.establishedResourceManagerConnection = null;\n \n \t\tthis.accumulators = new HashMap<>();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.resourceManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \tprivate SchedulerNG createScheduler(final JobManagerJobMetricGroup jobManagerJobMetricGroup) throws Exception {\n@@ -785,15 +788,8 @@ private Acknowledge suspendExecution(final Exception cause) {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n-\t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n+\t\ttaskManagerHeartbeatManager.stop();\n+\t\tresourceManagerHeartbeatManager.stop();\n \t}\n \n \tprivate void startHeartbeatServices() {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java", "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java", "sha": "8698e842ac548596330b545a4d74601cfd473046", "changes": 11, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891", "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.InstanceID;\n@@ -178,6 +179,9 @@ public ResourceManager(\n \t\tthis.jmResourceIdRegistrations = new HashMap<>(4);\n \t\tthis.taskExecutors = new HashMap<>(8);\n \t\tthis.taskExecutorGatewayFutures = new HashMap<>(8);\n+\n+\t\tthis.jobManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \n@@ -972,15 +976,8 @@ private void startHeartbeatServices() {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n \t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (jobManagerHeartbeatManager != null) {\n \t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n \t}\n \n \t/**", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java"}, {"additions": 22, "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java", "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java", "sha": "f0db4cd82b2d371a72c1a12cbf6f650cfbc876e3", "changes": 58, "status": "modified", "deletions": 36, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891", "patch": "@@ -157,8 +157,6 @@\n \t/** The task manager configuration. */\n \tprivate final TaskManagerConfiguration taskManagerConfiguration;\n \n-\tprivate final HeartbeatServices heartbeatServices;\n-\n \t/** The fatal error handler to use in case of a fatal error. */\n \tprivate final FatalErrorHandler fatalErrorHandler;\n \n@@ -207,10 +205,10 @@\n \tprivate FileCache fileCache;\n \n \t/** The heartbeat manager for job manager in the task manager. */\n-\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n \n \t/** The heartbeat manager for resource manager in the task manager. */\n-\tprivate HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n \n \tprivate final PartitionTable<JobID> partitionTable;\n \n@@ -249,7 +247,6 @@ public TaskExecutor(\n \t\tcheckArgument(taskManagerConfiguration.getNumberSlots() > 0, \"The number of slots has to be larger than 0.\");\n \n \t\tthis.taskManagerConfiguration = checkNotNull(taskManagerConfiguration);\n-\t\tthis.heartbeatServices = checkNotNull(heartbeatServices);\n \t\tthis.taskExecutorServices = checkNotNull(taskExecutorServices);\n \t\tthis.haServices = checkNotNull(haServices);\n \t\tthis.fatalErrorHandler = checkNotNull(fatalErrorHandler);\n@@ -278,6 +275,26 @@ public TaskExecutor(\n \n \t\tthis.stackTraceSampleService = new StackTraceSampleService(rpcService.getScheduledExecutor());\n \t\tthis.taskCompletionTracker = new TaskCompletionTracker();\n+\n+\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n+\t\tthis.jobManagerHeartbeatManager = createJobManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t\tthis.resourceManagerHeartbeatManager = createResourceManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t}\n+\n+\tprivate HeartbeatManager<Void, SlotReport> createResourceManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew ResourceManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n+\t}\n+\n+\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> createJobManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew JobManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n \t}\n \n \t@Override\n@@ -304,8 +321,6 @@ public void onStart() throws Exception {\n \n \tprivate void startTaskExecutorServices() throws Exception {\n \t\ttry {\n-\t\t\tstartHeartbeatServices();\n-\n \t\t\t// start by connecting to the ResourceManager\n \t\t\tresourceManagerLeaderRetriever.start(new ResourceManagerLeaderListener());\n \n@@ -412,38 +427,9 @@ private void stopTaskExecutorServices() throws Exception {\n \t\t// it will call close() recursively from the parent to children\n \t\ttaskManagerMetricGroup.close();\n \n-\t\tstopHeartbeatServices();\n-\n \t\tExceptionUtils.tryRethrowException(exception);\n \t}\n \n-\tprivate void startHeartbeatServices() {\n-\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n-\t\tjobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew JobManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\n-\t\tresourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew ResourceManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\t}\n-\n-\tprivate void stopHeartbeatServices() {\n-\t\tif (jobManagerHeartbeatManager != null) {\n-\t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n-\t}\n-\n \t// ======================================================================\n \t//  RPC methods\n \t// ======================================================================", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/191b9dff2f3faf281a77e211c6ef47243d6a9e8d", "parent": "https://github.com/apache/flink/commit/8a174833bee081f4f4a24caa5ddc5fe45996de13", "message": "[FLINK-12247][rest] Fix NPE when writing the archive json file to FileSystem\n\nThis closes #8250.", "bug_id": "flink_7", "file": [{"additions": 9, "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java", "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java", "sha": "97da25a1230e65b32d6d207cbd94f2f347160b53", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d", "patch": "@@ -100,13 +100,15 @@ protected SubtaskExecutionAttemptAccumulatorsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null){\n+\t\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java"}, {"additions": 9, "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java", "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java", "sha": "75fd100062e22e175fc4a748aa4271f044991737", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d", "patch": "@@ -114,13 +114,15 @@ protected SubtaskExecutionAttemptDetailsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null) {\n+\t\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/168660ab29a1bc45a75d9b8360c3ef857ee70883", "parent": "https://github.com/apache/flink/commit/c2d6424123e3800aa815e662f85491b7350caa92", "message": "[hotfix] Repair ineffective LocalRecoveryITCase\n\nThe test did not actually run since the class was refactored with JUnit's parameterized, because it was always running into a NPE and the NPE was then silently swallowed in a shutdown catch-block.", "bug_id": "flink_8", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/168660ab29a1bc45a75d9b8360c3ef857ee70883/flink-tests/src/test/java/org/apache/flink/test/checkpointing/LocalRecoveryITCase.java", "blob_url": "https://github.com/apache/flink/blob/168660ab29a1bc45a75d9b8360c3ef857ee70883/flink-tests/src/test/java/org/apache/flink/test/checkpointing/LocalRecoveryITCase.java", "sha": "a8626f34e33d60bc3487d9cba4086fbe132bf3ab", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests/src/test/java/org/apache/flink/test/checkpointing/LocalRecoveryITCase.java?ref=168660ab29a1bc45a75d9b8360c3ef857ee70883", "patch": "@@ -87,13 +87,15 @@ protected Configuration createClusterConfig() throws IOException {\n \n \tprivate void executeTest(EventTimeWindowCheckpointingITCase delegate) throws Exception {\n \t\tdelegate.name = testName;\n+\t\tdelegate.stateBackendEnum = backendEnum;\n \t\ttry {\n \t\t\tdelegate.setupTestCluster();\n \t\t\ttry {\n \t\t\t\tdelegate.testTumblingTimeWindow();\n \t\t\t\tdelegate.stopTestCluster();\n \t\t\t} catch (Exception e) {\n \t\t\t\tdelegate.stopTestCluster();\n+\t\t\t\tthrow new RuntimeException(e);\n \t\t\t}\n \n \t\t\tdelegate.setupTestCluster();\n@@ -102,9 +104,10 @@ private void executeTest(EventTimeWindowCheckpointingITCase delegate) throws Exc\n \t\t\t\tdelegate.stopTestCluster();\n \t\t\t} catch (Exception e) {\n \t\t\t\tdelegate.stopTestCluster();\n+\t\t\t\tthrow new RuntimeException(e);\n \t\t\t}\n \t\t} finally {\n-\t\t\tdelegate.tempFolder.delete();\n+\t\t\tEventTimeWindowCheckpointingITCase.tempFolder.delete();\n \t\t}\n \t}\n }", "filename": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/LocalRecoveryITCase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/5377c772d3c9f54cd2040ca67743cad128999f57", "parent": "https://github.com/apache/flink/commit/1906a1ee3f02ebe829837515f5eb065981421a37", "message": "[FLINK-10663][streaming] Fix NPE when StreamingFileSink is closed without initialization. (#6915)", "bug_id": "flink_9", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "blob_url": "https://github.com/apache/flink/blob/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "sha": "6f57fee81f3030e831a5f8c6a1b14d26477724cb", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java?ref=5377c772d3c9f54cd2040ca67743cad128999f57", "patch": "@@ -375,6 +375,8 @@ public void invoke(IN value, SinkFunction.Context context) throws Exception {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tbuckets.close();\n+\t\tif (buckets != null) {\n+\t\t\tbuckets.close();\n+\t\t}\n \t}\n }", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java"}, {"additions": 10, "raw_url": "https://github.com/apache/flink/raw/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java", "blob_url": "https://github.com/apache/flink/blob/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java", "sha": "8bb35ff244a449c4d825aa9314a8e21c83ce2d1b", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java?ref=5377c772d3c9f54cd2040ca67743cad128999f57", "patch": "@@ -59,6 +59,16 @@ public void testClosingWithoutInput() throws Exception {\n \t\t}\n \t}\n \n+\t@Test\n+\tpublic void testClosingWithoutInitializingStateShouldNotFail() throws Exception {\n+\t\tfinal File outDir = TEMP_FOLDER.newFolder();\n+\n+\t\ttry (OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Object> testHarness =\n+\t\t\t\t\tTestUtils.createRescalingTestSink(outDir, 1, 0, 100L, 124L)) {\n+\t\t\ttestHarness.setup();\n+\t\t}\n+\t}\n+\n \t@Test\n \tpublic void testTruncateAfterRecoveryAndOverwrite() throws Exception {\n \t\tfinal File outDir = TEMP_FOLDER.newFolder();", "filename": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/bc16485cc89fbe5b0dd1534737d0b5cd1ced885b", "parent": "https://github.com/apache/flink/commit/c53c446486d58e3db149a9ea6fe1984227e415b2", "message": "[FLINK-12642][network][metrics] Fix In/OutputBufferPoolUsageGauge failure with NPE\n\nThe result partition metrics are initialised before `ResultPartitiion#setup` was called. If a reporter tries to access a In/OutputBufferPoolUsageGauge in between it will fail with an `NullPointerException` since the `BufferPool` of the partition is still `null`. Currently, the quick fix is to return zero metrics until the `BufferPool` is initialised. When we have a single-threaded access from `Task#run`, we can merge partition/gate create and setup then it should not be the case anymore.", "bug_id": "flink_10", "file": [{"additions": 6, "raw_url": "https://github.com/apache/flink/raw/bc16485cc89fbe5b0dd1534737d0b5cd1ced885b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/InputBufferPoolUsageGauge.java", "blob_url": "https://github.com/apache/flink/blob/bc16485cc89fbe5b0dd1534737d0b5cd1ced885b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/InputBufferPoolUsageGauge.java", "sha": "c7a6d4e76a9c9fb63ba573ea791a15dcd34a2c5a", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/InputBufferPoolUsageGauge.java?ref=bc16485cc89fbe5b0dd1534737d0b5cd1ced885b", "patch": "@@ -19,6 +19,7 @@\n package org.apache.flink.runtime.io.network.metrics;\n \n import org.apache.flink.metrics.Gauge;\n+import org.apache.flink.runtime.io.network.buffer.BufferPool;\n import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;\n \n /**\n@@ -38,8 +39,11 @@ public Float getValue() {\n \t\tint bufferPoolSize = 0;\n \n \t\tfor (SingleInputGate inputGate : inputGates) {\n-\t\t\tusedBuffers += inputGate.getBufferPool().bestEffortGetNumOfUsedBuffers();\n-\t\t\tbufferPoolSize += inputGate.getBufferPool().getNumBuffers();\n+\t\t\tBufferPool bufferPool = inputGate.getBufferPool();\n+\t\t\tif (bufferPool != null) {\n+\t\t\t\tusedBuffers += bufferPool.bestEffortGetNumOfUsedBuffers();\n+\t\t\t\tbufferPoolSize += bufferPool.getNumBuffers();\n+\t\t\t}\n \t\t}\n \n \t\tif (bufferPoolSize != 0) {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/InputBufferPoolUsageGauge.java"}, {"additions": 7, "raw_url": "https://github.com/apache/flink/raw/bc16485cc89fbe5b0dd1534737d0b5cd1ced885b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/OutputBufferPoolUsageGauge.java", "blob_url": "https://github.com/apache/flink/blob/bc16485cc89fbe5b0dd1534737d0b5cd1ced885b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/OutputBufferPoolUsageGauge.java", "sha": "b8f771ba823267c983a544b555eeddee45c3a80c", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/OutputBufferPoolUsageGauge.java?ref=bc16485cc89fbe5b0dd1534737d0b5cd1ced885b", "patch": "@@ -19,6 +19,7 @@\n package org.apache.flink.runtime.io.network.metrics;\n \n import org.apache.flink.metrics.Gauge;\n+import org.apache.flink.runtime.io.network.buffer.BufferPool;\n import org.apache.flink.runtime.io.network.partition.ResultPartition;\n \n /**\n@@ -38,8 +39,12 @@ public Float getValue() {\n \t\tint bufferPoolSize = 0;\n \n \t\tfor (ResultPartition resultPartition : resultPartitions) {\n-\t\t\tusedBuffers += resultPartition.getBufferPool().bestEffortGetNumOfUsedBuffers();\n-\t\t\tbufferPoolSize += resultPartition.getBufferPool().getNumBuffers();\n+\t\t\tBufferPool bufferPool = resultPartition.getBufferPool();\n+\n+\t\t\tif (bufferPool != null) {\n+\t\t\t\tusedBuffers += bufferPool.bestEffortGetNumOfUsedBuffers();\n+\t\t\t\tbufferPoolSize += bufferPool.getNumBuffers();\n+\t\t\t}\n \t\t}\n \n \t\tif (bufferPoolSize != 0) {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/metrics/OutputBufferPoolUsageGauge.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/26bac51cae1d298078902a02e196fffc16ea5704", "parent": "https://github.com/apache/flink/commit/e58cc14db007123c6325c7e51291650da69a4ca2", "message": "[FLINK-10358] fix NPE when running flink-kinesis connector against dynamodb streams\n\nThis closes #6708.", "bug_id": "flink_11", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java", "blob_url": "https://github.com/apache/flink/blob/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java", "sha": "36a4e92c179135fcb9f459fcff746be4069c184a", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java?ref=26bac51cae1d298078902a02e196fffc16ea5704", "patch": "@@ -373,7 +373,10 @@ private GetRecordsResult getRecords(String shardItr, int maxNumberOfRecords) thr\n \t\t\t\tgetRecordsResult = kinesis.getRecords(shardItr, maxNumberOfRecords);\n \n \t\t\t\t// Update millis behind latest so it gets reported by the millisBehindLatest gauge\n-\t\t\t\tshardMetricsReporter.setMillisBehindLatest(getRecordsResult.getMillisBehindLatest());\n+\t\t\t\tLong millisBehindLatest = getRecordsResult.getMillisBehindLatest();\n+\t\t\t\tif (millisBehindLatest != null) {\n+\t\t\t\t\tshardMetricsReporter.setMillisBehindLatest(millisBehindLatest);\n+\t\t\t\t}\n \t\t\t} catch (ExpiredIteratorException eiEx) {\n \t\t\t\tLOG.warn(\"Encountered an unexpected expired iterator {} for shard {};\" +\n \t\t\t\t\t\" refreshing the iterator ...\", shardItr, subscribedShard);", "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65", "parent": "https://github.com/apache/flink/commit/e8e74a648a134fe054081a49a36b8c45f30c21bc", "message": "[hotfix] [sql-client] Fix NPE when column is null", "bug_id": "flink_12", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java", "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java", "sha": "1e8f696bdf26cb4826f9cbe6c96f8c15d279a209", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65", "patch": "@@ -35,6 +35,7 @@ private CliStrings() {\n \n \tpublic static final String CLI_NAME = \"Flink SQL CLI Client\";\n \tpublic static final String DEFAULT_MARGIN = \" \";\n+\tpublic static final String NULL_COLUMN = \"(NULL)\";\n \n \t// --------------------------------------------------------------------------------------------\n ", "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java"}, {"additions": 6, "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java", "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java", "sha": "77894e8b7e192dc37b305e43f2530c91bae7c034", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65", "patch": "@@ -93,7 +93,12 @@ public static void normalizeColumn(AttributedStringBuilder sb, String col, int m\n \tpublic static String[] rowToString(Row row) {\n \t\tfinal String[] fields = new String[row.getArity()];\n \t\tfor (int i = 0; i < row.getArity(); i++) {\n-\t\t\tfields[i] = row.getField(i).toString();\n+\t\t\tfinal Object field = row.getField(i);\n+\t\t\tif (field == null) {\n+\t\t\t\tfields[i] = CliStrings.NULL_COLUMN;\n+\t\t\t} else {\n+\t\t\t\tfields[i] = field.toString();\n+\t\t\t}\n \t\t}\n \t\treturn fields;\n \t}", "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/cc5c8554740f8a5e554d98de236ca9e17bf2c67a", "parent": "https://github.com/apache/flink/commit/378cbb7c2e580ba73f215234e7dff542c3e2bc97", "message": "[FLINK-9694][table] Fix NPE in CRowSerializerConfigSnapshot constructor\n\nThis closes #6392.", "bug_id": "flink_13", "file": [{"additions": 2, "raw_url": "https://github.com/apache/flink/raw/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala", "blob_url": "https://github.com/apache/flink/blob/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala", "sha": "9418095d5f7ca581ce63d5c448b9ad194eee778b", "changes": 8, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala?ref=cc5c8554740f8a5e554d98de236ca9e17bf2c67a", "patch": "@@ -115,12 +115,8 @@ class CRowSerializer(val rowSerializer: TypeSerializer[Row]) extends TypeSeriali\n \n object CRowSerializer {\n \n-  class CRowSerializerConfigSnapshot(\n-      private val rowSerializer: TypeSerializer[Row])\n-    extends CompositeTypeSerializerConfigSnapshot(rowSerializer) {\n-\n-    /** This empty nullary constructor is required for deserializing the configuration. */\n-    def this() = this(null)\n+  class CRowSerializerConfigSnapshot(rowSerializers: TypeSerializer[Row]*)\n+    extends CompositeTypeSerializerConfigSnapshot(rowSerializers: _*) {\n \n     override def getVersion: Int = CRowSerializerConfigSnapshot.VERSION\n   }", "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala"}, {"additions": 34, "raw_url": "https://github.com/apache/flink/raw/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala", "blob_url": "https://github.com/apache/flink/blob/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala", "sha": "7483b04d9cac10fd8df849d07312644a73ad8457", "changes": 34, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala?ref=cc5c8554740f8a5e554d98de236ca9e17bf2c67a", "patch": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.types\n+\n+import org.apache.flink.util.TestLogger\n+import org.junit.Test\n+\n+class CRowSerializerTest extends TestLogger {\n+\n+  /**\n+    * This empty constructor is required for deserializing the configuration.\n+    */\n+  @Test\n+  def testDefaultConstructor(): Unit = {\n+    new CRowSerializer.CRowSerializerConfigSnapshot()\n+  }\n+\n+}", "filename": "flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/65ad34ca128d6e9b7f81df71cb7f0bbcc54e016a", "parent": "https://github.com/apache/flink/commit/4bcf0b99a6256e5dc1ccde4a2a8c72b9c000fe32", "message": "[FLINK-9627] [table] Extending KafkaJsonTableSource according to comments will result in NPE.\n\nThis closes #6198.", "bug_id": "flink_14", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/65ad34ca128d6e9b7f81df71cb7f0bbcc54e016a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTableSource.java", "blob_url": "https://github.com/apache/flink/blob/65ad34ca128d6e9b7f81df71cb7f0bbcc54e016a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTableSource.java", "sha": "29654b061c790884ca973dd22d5a3f36d35200d5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTableSource.java?ref=65ad34ca128d6e9b7f81df71cb7f0bbcc54e016a", "patch": "@@ -76,7 +76,7 @@\n \tprivate List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors;\n \n \t/** The startup mode for the contained consumer (default is {@link StartupMode#GROUP_OFFSETS}). */\n-\tprivate StartupMode startupMode;\n+\tprivate StartupMode startupMode = StartupMode.GROUP_OFFSETS;\n \n \t/** Specific startup offsets; only relevant when startup mode is {@link StartupMode#SPECIFIC_OFFSETS}. */\n \tprivate Map<KafkaTopicPartition, Long> specificStartupOffsets;", "filename": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/KafkaTableSource.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/eb525b7f889600fa4f4dbbdbee161848e5d570dd", "parent": "https://github.com/apache/flink/commit/a161606a6e9c7191a8afd8a003c8a46be2350f76", "message": "[FLINK-9524] [table] Check for expired clean-up timers to prevent NPE in ProcTimeBoundedRangeOver.\n\nThis closes #6180.", "bug_id": "flink_15", "file": [{"additions": 9, "raw_url": "https://github.com/apache/flink/raw/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala", "blob_url": "https://github.com/apache/flink/blob/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala", "sha": "591b942571f196400866b2d8f8cdb261bf0d8017", "changes": 11, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala?ref=eb525b7f889600fa4f4dbbdbee161848e5d570dd", "patch": "@@ -132,6 +132,14 @@ class ProcTimeBoundedRangeOver(\n \n     val currentTime = timestamp - 1\n     var i = 0\n+    // get the list of elements of current proctime\n+    val currentElements = rowMapState.get(currentTime)\n+\n+    // Expired clean-up timers pass the needToCleanupState() check.\n+    // Perform a null check to verify that we have data to process.\n+    if (null == currentElements) {\n+      return\n+    }\n \n     // initialize the accumulators\n     var accumulators = accumulatorState.value()\n@@ -172,8 +180,7 @@ class ProcTimeBoundedRangeOver(\n       i += 1\n     }\n \n-    // get the list of elements of current proctime\n-    val currentElements = rowMapState.get(currentTime)\n+\n     // add current elements to aggregator. Multiple elements might\n     // have arrived in the same proctime\n     // the same accumulator value will be computed for all elements", "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala"}, {"additions": 19, "raw_url": "https://github.com/apache/flink/raw/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala", "blob_url": "https://github.com/apache/flink/blob/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala", "sha": "218cae2dc193e71acc1c6ecc5eed4eeded05e548", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala?ref=eb525b7f889600fa4f4dbbdbee161848e5d570dd", "patch": "@@ -208,6 +208,21 @@ class OverWindowHarnessTest extends HarnessTestBase{\n \n     testHarness.setProcessingTime(11006)\n \n+    // test for clean-up timer NPE\n+    testHarness.setProcessingTime(20000)\n+\n+    // timer registered for 23000\n+    testHarness.processElement(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 10L: JLong), change = true)))\n+\n+    // update clean-up timer to 25500. Previous timer should not clean up\n+    testHarness.setProcessingTime(22500)\n+    testHarness.processElement(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 20L: JLong), change = true)))\n+\n+    // 23000 clean-up timer should fire but not fail with an NPE\n+    testHarness.setProcessingTime(23001)\n+\n     val result = testHarness.getOutput\n \n     val expectedOutput = new ConcurrentLinkedQueue[Object]()\n@@ -241,6 +256,10 @@ class OverWindowHarnessTest extends HarnessTestBase{\n       CRow(Row.of(0L: JLong, \"aaa\", 10L: JLong, 7L: JLong, 10L: JLong), change = true)))\n     expectedOutput.add(new StreamRecord(\n       CRow(Row.of(0L: JLong, \"bbb\", 40L: JLong, 40L: JLong, 40L: JLong), change = true)))\n+    expectedOutput.add(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 10L: JLong, 10L: JLong, 10L: JLong), change = true)))\n+    expectedOutput.add(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 20L: JLong, 10L: JLong, 20L: JLong), change = true)))\n \n     verify(expectedOutput, result, new RowResultSortComparator())\n ", "filename": "flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/bcead3be32c624008730555d828fd8e9447fbeff", "parent": "https://github.com/apache/flink/commit/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb", "message": "[FLINK-8230] [orc] Fix NPEs when reading nested columns.\n\n- fixes NPEs for null-valued structs, lists, and maps\n- fixes NPEs for repeating structs, lists, and maps\n- adds test for deeply nested data with nulls\n- adds test for columns with repeating values\n\nThis closes #5373.", "bug_id": "flink_16", "file": [{"additions": 1402, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java", "sha": "3ecdeb3c956dbeaacca29ebbfed1218d6a465a63", "changes": 1402, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -0,0 +1,1402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.MapTypeInfo;\n+import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.apache.orc.TypeDescription;\n+\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TimeZone;\n+import java.util.function.DoubleFunction;\n+import java.util.function.Function;\n+import java.util.function.LongFunction;\n+\n+/**\n+ * A class that provides utility methods for orc file reading.\n+ */\n+class OrcBatchReader {\n+\n+\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n+\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n+\n+\t/**\n+\t * Converts an ORC schema to a Flink TypeInformation.\n+\t *\n+\t * @param schema The ORC schema.\n+\t * @return The TypeInformation that corresponds to the ORC schema.\n+\t */\n+\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n+\t\t\tcase INT:\n+\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n+\t\t\tcase LONG:\n+\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n+\t\t\tcase STRING:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n+\t\t\tcase DATE:\n+\t\t\t\treturn SqlTimeTypeInfo.DATE;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n+\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n+\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n+\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n+\t\t\t\t}\n+\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n+\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n+\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n+\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n+\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n+\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n+\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n+\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Fills an ORC batch into an array of Row.\n+\t *\n+\t * @param rows The batch of rows need to be filled.\n+\t * @param schema The schema of the ORC data.\n+\t * @param batch The ORC data.\n+\t * @param selectedFields The list of selected ORC fields.\n+\t * @return The number of rows that were filled.\n+\t */\n+\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n+\n+\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n+\n+\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t// read each selected field\n+\t\tfor (int fieldIdx = 0; fieldIdx < selectedFields.length; fieldIdx++) {\n+\t\t\tint orcIdx = selectedFields[fieldIdx];\n+\t\t\treadField(rows, fieldIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], rowsToRead);\n+\t\t}\n+\t\treturn rowsToRead;\n+\t}\n+\n+\t/**\n+\t * Reads a vector of data into an array of objects.\n+\t *\n+\t * @param vals The array that needs to be filled.\n+\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param schema The schema of the vector to read.\n+\t * @param vector The vector to read.\n+\t * @param childCount The number of vector entries to read.\n+\t */\n+\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, int childCount) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BYTE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase SHORT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase INT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LONG:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase FLOAT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DOUBLE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DATE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BINARY:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DECIMAL:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase STRUCT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LIST:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase MAP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tString repeatingValue = readString(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\t// we don't have to handle isRepeating because ORC assumes that it is propagated into the children.\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\n+\t\t\tint offset = (int) list.offsets[0];\n+\t\t\tint length = (int) list.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t// get function to copy list\n+\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t// create first list that will be copied\n+\t\t\tObject[] first;\n+\t\t\tif (offset == 0) {\n+\t\t\t\tfirst = children;\n+\t\t\t} else {\n+\t\t\t\tfirst = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, first, 0, length);\n+\t\t\t}\n+\n+\t\t\t// create copies of first list and set copies as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tObject[] copy = (Object[]) copyList.apply(first);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = mapsVector.keys;\n+\t\tColumnVector values = mapsVector.values;\n+\n+\t\tif (mapsVector.isRepeating) {\n+\t\t\t// first map is repeated\n+\n+\t\t\t// get map copy function\n+\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t// set all key and value entries except those of the first map to null\n+\t\t\tint offset = (int) mapsVector.offsets[0];\n+\t\t\tint length = (int) mapsVector.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t// create first map that will be copied\n+\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t// copy first map and set copy as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copyMap.apply(map);\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(map));\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\n+\t\t\tObject[] keyRows = new Object[mapsVector.childCount];\n+\t\t\tObject[] valueRows = new Object[mapsVector.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n+\t\t\tint offset = 0;\n+\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n+\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n+\t\t\t\toffset += numMapEntries;\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = map;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t}\n+\n+\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call.\n+\t\t\t\treadNonNullLongColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\tvals[i] = ts;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\n+\t\t// Early out if struct column is repeating and always null.\n+\t\t// This is the only repeating case we need to handle.\n+\t\t// ORC assumes that repeating values have been pushed to the children.\n+\t\tif (structVector.isRepeating && structVector.isNull[0]) {\n+\t\t\tif (fieldIdx < 0) {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn;\n+\t\t}\n+\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\tColumnVector fieldVector = structVector.fields[i];\n+\t\t\tif (!fieldVector.isRepeating) {\n+\t\t\t\t// Reduce fieldVector reads by setting all entries null where struct is null.\n+\t\t\t\tif (fieldVector.noNulls) {\n+\t\t\t\t\t// fieldVector had no nulls. Just use struct null information.\n+\t\t\t\t\tSystem.arraycopy(structVector.isNull, 0, fieldVector.isNull, 0, structVector.isNull.length);\n+\t\t\t\t\tstructVector.fields[i].noNulls = false;\n+\t\t\t\t} else {\n+\t\t\t\t\t// fieldVector had nulls. Merge field nulls with struct nulls.\n+\t\t\t\t\tfor (int j = 0; j < structVector.isNull.length; j++) {\n+\t\t\t\t\t\tstructVector.fields[i].isNull[j] = structVector.isNull[j] || structVector.fields[i].isNull[j];\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tboolean[] isNullVector = structVector.isNull;\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tvals[i] = structs[i];\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\t\t\t// list values are repeating. we only need to read the first list and copy it.\n+\n+\t\t\tif (list.isNull[0]) {\n+\t\t\t\t// Even better. The first list is null and so are all lists are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy list\n+\t\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) list.offsets[0];\n+\t\t\t\tint length = (int) list.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\t// read entries\n+\t\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t\t// create first list which will be copied\n+\t\t\t\tObject[] temp;\n+\t\t\t\tif (offset == 0) {\n+\t\t\t\t\ttemp = children;\n+\t\t\t\t} else {\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\t// copy repeated list and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tObject[] copy = (Object[]) copyList.apply(temp);\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\t\t\tif (!list.child.isRepeating) {\n+\t\t\t\tboolean[] childIsNull = new boolean[list.childCount];\n+\t\t\t\tArrays.fill(childIsNull, true);\n+\t\t\t\t// forward info of null lists into child vector\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info of entries of non-null lists\n+\t\t\t\t\tif (!list.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\t\tint length = (int) list.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(list.child.isNull, offset, childIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of children vector\n+\t\t\t\tlist.child.isNull = childIsNull;\n+\t\t\t\tlist.child.noNulls = false;\n+\t\t\t}\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\tObject[] temp;\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\n+\t\t\t\tif (list.isNull[i]) {\n+\t\t\t\t\ttemp = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = map.keys;\n+\t\tColumnVector values = map.values;\n+\n+\t\tif (map.isRepeating) {\n+\t\t\t// map values are repeating. we only need to read the first map and copy it.\n+\n+\t\t\tif (map.isNull[0]) {\n+\t\t\t\t// Even better. The first map is null and so are all maps are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy map\n+\t\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) map.offsets[0];\n+\t\t\t\tint length = (int) map.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t\t// read map keys and values\n+\t\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t\t// create first map which will be copied\n+\t\t\t\tHashMap temp = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t\t// copy repeated map and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copyMap.apply(temp);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(temp));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// ensure only keys and values that are referenced by non-null maps are set to non-null\n+\n+\t\t\tif (!keys.isRepeating) {\n+\t\t\t\t// propagate is null info of map into keys vector\n+\t\t\t\tboolean[] keyIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(keyIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for keys of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(keys.isNull, offset, keyIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of keys vector\n+\t\t\t\tkeys.isNull = keyIsNull;\n+\t\t\t\tkeys.noNulls = false;\n+\t\t\t}\n+\t\t\tif (!values.isRepeating) {\n+\t\t\t\t// propagate is null info of map into values vector\n+\t\t\t\tboolean[] valIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(valIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for vals of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(values.isNull, offset, valIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of values vector\n+\t\t\t\tvalues.isNull = valIsNull;\n+\t\t\t\tvalues.noNulls = false;\n+\t\t\t}\n+\n+\t\t\tObject[] keyRows = new Object[map.childCount];\n+\t\t\tObject[] valueRows = new Object[map.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tboolean[] isNullVector = map.isNull;\n+\t\t\tlong[] lengths = map.lengths;\n+\t\t\tlong[] offsets = map.offsets;\n+\n+\t\t\tif (fieldIdx == -1) { // set map as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set map as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Sets a repeating value to all objects or row fields of the passed vals array.\n+\t *\n+\t * @param vals The array of objects or Rows.\n+\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param repeatingValue The value that is set.\n+\t * @param childCount The number of times the value is set.\n+\t */\n+\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n+\n+\t\tif (fieldIdx == -1) {\n+\t\t\t// set value as an object\n+\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n+\t\t} else {\n+\t\t\t// set value as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static Class<?> getClassForType(TypeDescription schema) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.class;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn Byte.class;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn Short.class;\n+\t\t\tcase INT:\n+\t\t\t\treturn Integer.class;\n+\t\t\tcase LONG:\n+\t\t\t\treturn Long.class;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.class;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn Double.class;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\treturn String.class;\n+\t\t\tcase DATE:\n+\t\t\t\treturn Date.class;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn Timestamp.class;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn byte[].class;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BigDecimal.class;\n+\t\t\tcase STRUCT:\n+\t\t\t\treturn Row.class;\n+\t\t\tcase LIST:\n+\t\t\t\tClass<?> childClass = getClassForType(schema.getChildren().get(0));\n+\t\t\t\treturn Array.newInstance(childClass, 0).getClass();\n+\t\t\tcase MAP:\n+\t\t\t\treturn HashMap.class;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Boolean readBoolean(long l) {\n+\t\treturn l != 0;\n+\t}\n+\n+\tprivate static Byte readByte(long l) {\n+\t\treturn (byte) l;\n+\t}\n+\n+\tprivate static Short readShort(long l) {\n+\t\treturn (short) l;\n+\t}\n+\n+\tprivate static Integer readInt(long l) {\n+\t\treturn (int) l;\n+\t}\n+\n+\tprivate static Long readLong(long l) {\n+\t\treturn l;\n+\t}\n+\n+\tprivate static Float readFloat(double d) {\n+\t\treturn (float) d;\n+\t}\n+\n+\tprivate static Double readDouble(double d) {\n+\t\treturn d;\n+\t}\n+\n+\tprivate static Date readDate(long l) {\n+\t\t// day to milliseconds\n+\t\tfinal long t = l * MILLIS_PER_DAY;\n+\t\t// adjust by local timezone\n+\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n+\t}\n+\n+\tprivate static String readString(byte[] bytes, int start, int length) {\n+\t\treturn new String(bytes, start, length, StandardCharsets.UTF_8);\n+\t}\n+\n+\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n+\t\tbyte[] result = new byte[length];\n+\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n+\t\treturn result;\n+\t}\n+\n+\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n+\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n+\t\treturn hiveDecimal.bigDecimalValue();\n+\t}\n+\n+\tprivate static Timestamp readTimestamp(long time, int nanos) {\n+\t\tTimestamp ts = new Timestamp(time);\n+\t\tts.setNanos(nanos);\n+\t\treturn ts;\n+\t}\n+\n+\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n+\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n+\t\tfor (int j = 0; j < length; j++) {\n+\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n+\t\t\toffset++;\n+\t\t}\n+\t\treturn resultMap;\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static Function<Object, Object> getCopyFunction(TypeDescription schema) {\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BYTE:\n+\t\t\tcase SHORT:\n+\t\t\tcase INT:\n+\t\t\tcase LONG:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn OrcBatchReader::returnImmutable;\n+\t\t\tcase DATE:\n+\t\t\t\treturn OrcBatchReader::copyDate;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn OrcBatchReader::copyTimestamp;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn OrcBatchReader::copyBinary;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t\t\tFunction<Object, Object>[] copyFields = new Function[fieldTypes.size()];\n+\t\t\t\tfor (int i = 0; i < fieldTypes.size(); i++) {\n+\t\t\t\t\tcopyFields[i] = getCopyFunction(fieldTypes.get(i));\n+\t\t\t\t}\n+\t\t\t\treturn new CopyStruct(copyFields);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription entryType = schema.getChildren().get(0);\n+\t\t\t\tFunction<Object, Object> copyEntry = getCopyFunction(entryType);\n+\t\t\t\tClass entryClass = getClassForType(entryType);\n+\t\t\t\treturn new CopyList(copyEntry, entryClass);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keyType = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valueType = schema.getChildren().get(1);\n+\t\t\t\tFunction<Object, Object> copyKey = getCopyFunction(keyType);\n+\t\t\t\tFunction<Object, Object> copyValue = getCopyFunction(valueType);\n+\t\t\t\treturn new CopyMap(copyKey, copyValue);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Object returnImmutable(Object o) {\n+\t\treturn o;\n+\t}\n+\n+\tprivate static Date copyDate(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong date = ((Date) o).getTime();\n+\t\t\treturn new Date(date);\n+\t\t}\n+\t}\n+\n+\tprivate static Timestamp copyTimestamp(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong millis = ((Timestamp) o).getTime();\n+\t\t\tint nanos = ((Timestamp) o).getNanos();\n+\t\t\tTimestamp copy = new Timestamp(millis);\n+\t\t\tcopy.setNanos(nanos);\n+\t\t\treturn copy;\n+\t\t}\n+\t}\n+\n+\tprivate static byte[] copyBinary(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tint length = ((byte[]) o).length;\n+\t\t\treturn Arrays.copyOf((byte[]) o, length);\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyStruct implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object>[] copyFields;\n+\n+\t\tCopyStruct(Function<Object, Object>[] copyFields) {\n+\t\t\tthis.copyFields = copyFields;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tRow r = (Row) o;\n+\t\t\t\tRow copy = new Row(copyFields.length);\n+\t\t\t\tfor (int i = 0; i < copyFields.length; i++) {\n+\t\t\t\t\tcopy.setField(i, copyFields[i].apply(r.getField(i)));\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyList implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyEntry;\n+\t\tprivate final Class entryClass;\n+\n+\t\tCopyList(Function<Object, Object> copyEntry, Class entryClass) {\n+\t\t\tthis.copyEntry = copyEntry;\n+\t\t\tthis.entryClass = entryClass;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tObject[] l = (Object[]) o;\n+\t\t\t\tObject[] copy = (Object[]) Array.newInstance(entryClass, l.length);\n+\t\t\t\tfor (int i = 0; i < l.length; i++) {\n+\t\t\t\t\tcopy[i] = copyEntry.apply(l[i]);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static class CopyMap implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyKey;\n+\t\tprivate final Function<Object, Object> copyValue;\n+\n+\t\tCopyMap(Function<Object, Object> copyKey, Function<Object, Object> copyValue) {\n+\t\t\tthis.copyKey = copyKey;\n+\t\t\tthis.copyValue = copyValue;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tMap<Object, Object> m = (Map<Object, Object>) o;\n+\t\t\t\tHashMap<Object, Object> copy = new HashMap<>(m.size());\n+\n+\t\t\t\tfor (Map.Entry<Object, Object> e : m.entrySet()) {\n+\t\t\t\t\tObject keyCopy = copyKey.apply(e.getKey());\n+\t\t\t\t\tObject valueCopy = copyValue.apply(e.getValue());\n+\t\t\t\t\tcopy.put(keyCopy, valueCopy);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+}", "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java", "sha": "61575ad727cb1338d029fcc87501156d79453925", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -55,7 +55,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n-import static org.apache.flink.orc.OrcUtils.fillRows;\n+import static org.apache.flink.orc.OrcBatchReader.fillRows;\n \n /**\n  * InputFormat to read ORC files.\n@@ -128,7 +128,7 @@ public OrcRowInputFormat(String path, TypeDescription orcSchema, Configuration o\n \n \t\t// configure OrcRowInputFormat\n \t\tthis.schema = orcSchema;\n-\t\tthis.rowType = (RowTypeInfo) OrcUtils.schemaToTypeInfo(schema);\n+\t\tthis.rowType = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(schema);\n \t\tthis.conf = orcConfig;\n \t\tthis.batchSize = batchSize;\n ", "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java"}, {"additions": 1, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java", "sha": "0eab4a043da3cd9c3d0429e90b62014621ef1f76", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -127,7 +127,7 @@ private OrcTableSource(String path, TypeDescription orcSchema, Configuration orc\n \t\tthis.predicates = predicates;\n \n \t\t// determine the type information from the ORC schema\n-\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcUtils.schemaToTypeInfo(this.orcSchema);\n+\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(this.orcSchema);\n \n \t\t// set return type info\n \t\tif (selectedFields == null) {", "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java", "blob_url": "https://github.com/apache/flink/blob/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java", "sha": "cfb4e0e66a8183da7b44fcf4672b376b01e53766", "changes": 1508, "status": "removed", "deletions": 1508, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java?ref=3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb", "patch": "@@ -1,1508 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.orc;\n-\n-import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n-import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n-import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.MapTypeInfo;\n-import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n-import org.apache.flink.types.Row;\n-\n-import org.apache.hadoop.hive.common.type.HiveDecimal;\n-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n-import org.apache.orc.TypeDescription;\n-\n-import java.lang.reflect.Array;\n-import java.math.BigDecimal;\n-import java.nio.charset.StandardCharsets;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.TimeZone;\n-import java.util.function.DoubleFunction;\n-import java.util.function.IntFunction;\n-import java.util.function.LongFunction;\n-\n-/**\n- * A class that provides utility methods for orc file reading.\n- */\n-class OrcUtils {\n-\n-\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n-\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n-\n-\t/**\n-\t * Converts an ORC schema to a Flink TypeInformation.\n-\t *\n-\t * @param schema The ORC schema.\n-\t * @return The TypeInformation that corresponds to the ORC schema.\n-\t */\n-\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n-\t\t\tcase BYTE:\n-\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n-\t\t\tcase SHORT:\n-\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n-\t\t\tcase INT:\n-\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n-\t\t\tcase LONG:\n-\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n-\t\t\tcase FLOAT:\n-\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n-\t\t\tcase DOUBLE:\n-\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n-\t\t\tcase DECIMAL:\n-\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n-\t\t\tcase STRING:\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n-\t\t\tcase DATE:\n-\t\t\t\treturn SqlTimeTypeInfo.DATE;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n-\t\t\tcase BINARY:\n-\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n-\t\t\tcase STRUCT:\n-\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n-\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n-\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n-\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n-\t\t\t\t}\n-\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n-\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n-\t\t\tcase LIST:\n-\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n-\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n-\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n-\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n-\t\t\tcase MAP:\n-\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n-\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n-\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n-\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n-\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Fills an ORC batch into an array of Row.\n-\t *\n-\t * @param rows The batch of rows need to be filled.\n-\t * @param schema The schema of the ORC data.\n-\t * @param batch The ORC data.\n-\t * @param selectedFields The list of selected ORC fields.\n-\t * @return The number of rows that were filled.\n-\t */\n-\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n-\n-\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n-\n-\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n-\t\t// read each selected field\n-\t\tfor (int rowIdx = 0; rowIdx < selectedFields.length; rowIdx++) {\n-\t\t\tint orcIdx = selectedFields[rowIdx];\n-\t\t\treadField(rows, rowIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], null, rowsToRead);\n-\t\t}\n-\t\treturn rowsToRead;\n-\t}\n-\n-\t/**\n-\t * Reads a vector of data into an array of objects.\n-\t *\n-\t * @param vals The array that needs to be filled.\n-\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param schema The schema of the vector to read.\n-\t * @param vector The vector to read.\n-\t * @param lengthVector If the vector is of type List or Map, the number of sub-elements to read for each field. Otherwise, it must be null.\n-\t * @param childCount The number of vector entries to read.\n-\t */\n-\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check the type of the vector to decide how to read it.\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BYTE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase SHORT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase INT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LONG:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase FLOAT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DOUBLE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\tcase STRING:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DATE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BINARY:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DECIMAL:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase STRUCT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LIST:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase MAP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tString[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0], StandardCharsets.UTF_8);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset], StandardCharsets.UTF_8);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tbyte[][] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tDate[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tTimestamp[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tBigDecimal[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tBigDecimal repeatingValue = readBigDecimal(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\n-\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tSystem.arraycopy(structs, offset, temp, 0, temp.length);\n-\t\t\t\toffset = offset + temp.length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the list need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedLists = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedLists, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedLists[0].getClass();\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedLists, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = mapsVector.keys;\n-\t\tColumnVector values = mapsVector.values;\n-\t\tObject[] keyRows = new Object[mapsVector.childCount];\n-\t\tObject[] valueRows = new Object[mapsVector.childCount];\n-\n-\t\t// read map keys and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint offset = 0;\n-\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n-\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n-\t\t\t\toffset += numMapEntries;\n-\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = map;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tlong numMapEntries = lengthVectorMap[offset];\n-\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, numMapEntries);\n-\t\t\t\t\tmapOffset += numMapEntries;\n-\t\t\t\t\toffset++;\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == 1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (bytes.isRepeating) { // fill list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::stringArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tString[] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the binary need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::binaryArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tbyte[][] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::dateArray);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tDate[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\tvals[i] = ts;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::timestampArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tTimestamp[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::decimalArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tBigDecimal[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\tboolean[] isNullVector = structVector.isNull;\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = structs[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = structs[offset++];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) { // set list of structs as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of structs as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the lists need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedList = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedList, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedList[0].getClass();\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedList, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) { // set list of list as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of list as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = map.keys;\n-\t\tColumnVector values = map.values;\n-\t\tObject[] keyRows = new Object[map.childCount];\n-\t\tObject[] valueRows = new Object[map.childCount];\n-\n-\t\t// read map kes and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\tboolean[] isNullVector = map.isNull;\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint offset = 0;\n-\t\t\tif (fieldIdx == -1) { // set map as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]);\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set map as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]));\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, lengthVectorMap[offset]);\n-\t\t\t\t\t\tmapOffset += lengthVectorMap[offset];\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets a repeating value to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param repeatingValue The value that is set.\n-\t * @param childCount The number of times the value is set.\n-\t */\n-\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set value as an object\n-\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n-\t\t} else {\n-\t\t\t// set value as a field of Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets arrays containing only null values to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows to which the empty arrays are set.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param lengthVector The vector containing the lengths of the individual empty arrays.\n-\t * @param childCount The number of objects or Rows to fill.\n-\t * @param array A method to create arrays of the appropriate type.\n-\t * @param <T> The type of the arrays to create.\n-\t */\n-\tprivate static <T> void fillListWithRepeatingNull(Object[] vals, int fieldIdx, long[] lengthVector, int childCount, IntFunction<T[]> array) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set empty array as object\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tvals[i] = array.apply((int) lengthVector[i]);\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// set empty array as field in Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, array.apply((int) lengthVector[i]));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static Boolean readBoolean(long l) {\n-\t\treturn l != 0;\n-\t}\n-\n-\tprivate static Byte readByte(long l) {\n-\t\treturn (byte) l;\n-\t}\n-\n-\tprivate static Short readShort(long l) {\n-\t\treturn (short) l;\n-\t}\n-\n-\tprivate static Integer readInt(long l) {\n-\t\treturn (int) l;\n-\t}\n-\n-\tprivate static Long readLong(long l) {\n-\t\treturn l;\n-\t}\n-\n-\tprivate static Float readFloat(double d) {\n-\t\treturn (float) d;\n-\t}\n-\n-\tprivate static Double readDouble(double d) {\n-\t\treturn d;\n-\t}\n-\n-\tprivate static Date readDate(long l) {\n-\t\t// day to milliseconds\n-\t\tfinal long t = l * MILLIS_PER_DAY;\n-\t\t// adjust by local timezone\n-\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n-\t}\n-\n-\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n-\t\tbyte[] result = new byte[length];\n-\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n-\t\treturn result;\n-\t}\n-\n-\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n-\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n-\t\treturn hiveDecimal.bigDecimalValue();\n-\t}\n-\n-\tprivate static Timestamp readTimestamp(long time, int nanos) {\n-\t\tTimestamp ts = new Timestamp(time);\n-\t\tts.setNanos(nanos);\n-\t\treturn ts;\n-\t}\n-\n-\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n-\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n-\t\tfor (int j = 0; j < length; j++) {\n-\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n-\t\t\toffset++;\n-\t\t}\n-\t\treturn resultMap;\n-\t}\n-\n-\tprivate static Boolean[] boolArray(int len) {\n-\t\treturn new Boolean[len];\n-\t}\n-\n-\tprivate static Byte[] byteArray(int len) {\n-\t\treturn new Byte[len];\n-\t}\n-\n-\tprivate static Short[] shortArray(int len) {\n-\t\treturn new Short[len];\n-\t}\n-\n-\tprivate static Integer[] intArray(int len) {\n-\t\treturn new Integer[len];\n-\t}\n-\n-\tprivate static Long[] longArray(int len) {\n-\t\treturn new Long[len];\n-\t}\n-\n-\tprivate static Float[] floatArray(int len) {\n-\t\treturn new Float[len];\n-\t}\n-\n-\tprivate static Double[] doubleArray(int len) {\n-\t\treturn new Double[len];\n-\t}\n-\n-\tprivate static Date[] dateArray(int len) {\n-\t\treturn new Date[len];\n-\t}\n-\n-\tprivate static byte[][] binaryArray(int len) {\n-\t\treturn new byte[len][];\n-\t}\n-\n-\tprivate static String[] stringArray(int len) {\n-\t\treturn new String[len];\n-\t}\n-\n-\tprivate static BigDecimal[] decimalArray(int len) {\n-\t\treturn new BigDecimal[len];\n-\t}\n-\n-\tprivate static Timestamp[] timestampArray(int len) {\n-\t\treturn new Timestamp[len];\n-\t}\n-\n-}", "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java", "previous_filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcUtilsTest.java", "sha": "b90313ea2aaa1f7217989182618d94cf90e129cf", "changes": 8, "status": "renamed", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -31,10 +31,10 @@\n import org.junit.Test;\n \n /**\n- * Unit tests for {@link OrcUtils}.\n+ * Unit tests for {@link OrcBatchReader}.\n  *\n  */\n-public class OrcUtilsTest {\n+public class OrcBatchReaderTest {\n \n \t@Test\n \tpublic void testFlatSchemaToTypeInfo1() {\n@@ -54,7 +54,7 @@ public void testFlatSchemaToTypeInfo1() {\n \t\t\t\t\"timestamp1:timestamp,\" +\n \t\t\t\t\"decimal1:decimal(5,2)\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);\n@@ -106,7 +106,7 @@ public void testNestedSchemaToTypeInfo1() {\n \t\t\t\t\t\">\" +\n \t\t\t\t\">\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);", "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java"}, {"additions": 211, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java", "sha": "2eb3231eedabcfb5bf2e949b3390768803a52f7b", "changes": 218, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -26,6 +26,7 @@\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.core.fs.FileInputSplit;\n import org.apache.flink.core.fs.Path;\n+import org.apache.flink.orc.util.OrcTestFileGenerator;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.InstantiationUtil;\n \n@@ -50,6 +51,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.any;\n@@ -124,6 +126,32 @@ public void tearDown() throws IOException {\n \tprivate static final String TEST_FILE_NESTEDLIST = \"test-data-nestedlist.orc\";\n \tprivate static final String TEST_SCHEMA_NESTEDLIST = \"struct<mylist1:array<array<struct<mylong1:bigint>>>>\";\n \n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithNullsFile(String)}. */\n+\tprivate static final String TEST_FILE_COMPOSITES_NULLS = \"test-data-composites-with-nulls.orc\";\n+\tprivate static final String TEST_SCHEMA_COMPOSITES_NULLS =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\">\";\n+\n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithRepeatingFile(String)}. */\n+\tprivate static final String TEST_FILE_REPEATING = \"test-data-repeating.orc\";\n+\tprivate static final String TEST_SCHEMA_REPEATING =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"int2:int,\" +\n+\t\t\t\"int3:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<int>,\" +\n+\t\t\t\"list2:array<int>,\" +\n+\t\t\t\"list3:array<int>,\" +\n+\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\"map2:map<int,string>\" +\n+\t\t\">\";\n+\n \t@Test(expected = FileNotFoundException.class)\n \tpublic void testInvalidPath() throws IOException{\n \t\trowOrcInputFormat =\n@@ -477,7 +505,7 @@ public void testPredicateWithInvalidColumn() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadNestedFile() throws IOException{\n+\tpublic void testReadNestedFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -563,7 +591,7 @@ public void testReadNestedFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadTimeTypeFile() throws IOException{\n+\tpublic void testReadTimeTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_TIMETYPES), TEST_SCHEMA_TIMETYPES, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -590,7 +618,7 @@ public void testReadTimeTypeFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadDecimalTypeFile() throws IOException{\n+\tpublic void testReadDecimalTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_DECIMAL), TEST_SCHEMA_DECIMAL, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -653,7 +681,183 @@ public void testReadNestedListFile() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadWithProjection() throws IOException{\n+\tpublic void testReadCompositesNullsFile() throws Exception {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_COMPOSITES_NULLS),\n+\t\t\tTEST_SCHEMA_COMPOSITES_NULLS,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tint structNullCnt = 0;\n+\t\tint nestedListNullCnt = 0;\n+\t\tint mapListNullCnt = 0;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(4, row.getArity());\n+\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\n+\t\t\tif (row.getField(1) == null) {\n+\t\t\t\tstructNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(1);\n+\t\t\t\tassertTrue(f instanceof Row);\n+\t\t\t\tassertEquals(2, ((Row) f).getArity());\n+\t\t\t}\n+\n+\t\t\tif (row.getField(2) == null) {\n+\t\t\t\tnestedListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(2);\n+\t\t\t\tassertTrue(f instanceof Row[][][]);\n+\t\t\t\tassertEquals(4, ((Row[][][]) f).length);\n+\t\t\t}\n+\n+\t\t\tif (row.getField(3) == null) {\n+\t\t\t\tmapListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(3);\n+\t\t\t\tassertTrue(f instanceof HashMap[]);\n+\t\t\t\tassertEquals(3, ((HashMap[]) f).length);\n+\t\t\t}\n+\t\t\tcnt++;\n+\t\t}\n+\t\t// number of rows in file\n+\t\tassertEquals(2500, cnt);\n+\t\t// check number of null fields\n+\t\tassertEquals(1250, structNullCnt);\n+\t\tassertEquals(835, nestedListNullCnt);\n+\t\tassertEquals(835, mapListNullCnt);\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\t@Test\n+\tpublic void testReadRepeatingValuesFile() throws IOException {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_REPEATING),\n+\t\t\tTEST_SCHEMA_REPEATING,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tRow firstRow1 = null;\n+\t\tInteger[] firstList1 = null;\n+\t\tHashMap firstMap1 = null;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\tcnt++;\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(10, row.getArity());\n+\n+\t\t\t// check first int field (always 42)\n+\t\t\tassertNotNull(row.getField(0));\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\t\t\tassertEquals(42, ((Integer) row.getField(0)).intValue());\n+\n+\t\t\t// check second int field (always null)\n+\t\t\tassertNull(row.getField(1));\n+\n+\t\t\t// check first int field (always 99)\n+\t\t\tassertNotNull(row.getField(2));\n+\t\t\tassertTrue(row.getField(2) instanceof Integer);\n+\t\t\tassertEquals(99, ((Integer) row.getField(2)).intValue());\n+\n+\t\t\t// check first row field (always (23, null))\n+\t\t\tassertNotNull(row.getField(3));\n+\t\t\tassertTrue(row.getField(3) instanceof Row);\n+\t\t\tRow nestedRow = (Row) row.getField(3);\n+\t\t\t// check first field of nested row\n+\t\t\tassertNotNull(nestedRow.getField(0));\n+\t\t\tassertTrue(nestedRow.getField(0) instanceof Integer);\n+\t\t\tassertEquals(23, ((Integer) nestedRow.getField(0)).intValue());\n+\t\t\t// check second field of nested row\n+\t\t\tassertNull(nestedRow.getField(1));\n+\t\t\t// validate reference\n+\t\t\tif (firstRow1 == null) {\n+\t\t\t\tfirstRow1 = nestedRow;\n+\t\t\t} else {\n+\t\t\t\t// repeated rows must be different instances\n+\t\t\t\tassertTrue(firstRow1 != nestedRow);\n+\t\t\t}\n+\n+\t\t\t// check second row field (always null)\n+\t\t\tassertNull(row.getField(4));\n+\n+\t\t\t// check first list field (always [1, 2, 3])\n+\t\t\tassertNotNull(row.getField(5));\n+\t\t\tassertTrue(row.getField(5) instanceof Integer[]);\n+\t\t\tInteger[] list1 = ((Integer[]) row.getField(5));\n+\t\t\tassertEquals(1, list1[0].intValue());\n+\t\t\tassertEquals(2, list1[1].intValue());\n+\t\t\tassertEquals(3, list1[2].intValue());\n+\t\t\t// validate reference\n+\t\t\tif (firstList1 == null) {\n+\t\t\t\tfirstList1 = list1;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstList1 != list1);\n+\t\t\t}\n+\n+\t\t\t// check second list field (always [7, 7, 7])\n+\t\t\tassertNotNull(row.getField(6));\n+\t\t\tassertTrue(row.getField(6) instanceof Integer[]);\n+\t\t\tInteger[] list2 = ((Integer[]) row.getField(6));\n+\t\t\tassertEquals(7, list2[0].intValue());\n+\t\t\tassertEquals(7, list2[1].intValue());\n+\t\t\tassertEquals(7, list2[2].intValue());\n+\n+\t\t\t// check third list field (always null)\n+\t\t\tassertNull(row.getField(7));\n+\n+\t\t\t// check first map field (always {2->\"Hello\", 4->\"Hello})\n+\t\t\tassertNotNull(row.getField(8));\n+\t\t\tassertTrue(row.getField(8) instanceof HashMap);\n+\t\t\tHashMap<Integer, String> map = (HashMap<Integer, String>) row.getField(8);\n+\t\t\tassertEquals(2, map.size());\n+\t\t\tassertEquals(\"Hello\", map.get(2));\n+\t\t\tassertEquals(\"Hello\", map.get(4));\n+\t\t\t// validate reference\n+\t\t\tif (firstMap1 == null) {\n+\t\t\t\tfirstMap1 = map;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstMap1 != map);\n+\t\t\t}\n+\n+\t\t\t// check second map field (always null)\n+\t\t\tassertNull(row.getField(9));\n+\t\t}\n+\n+\t\trowOrcInputFormat.close();\n+\t\trowOrcInputFormat.closeInputFormat();\n+\n+\t\tassertEquals(256, cnt);\n+\t}\n+\n+\t@Test\n+\tpublic void testReadWithProjection() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\trowOrcInputFormat.selectFields(7, 0, 10, 8);\n@@ -691,7 +895,7 @@ public void testReadWithProjection() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileInSplits() throws IOException{\n+\tpublic void testReadFileInSplits() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -717,7 +921,7 @@ public void testReadFileInSplits() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithFilter() throws IOException{\n+\tpublic void testReadFileWithFilter() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -751,7 +955,7 @@ public void testReadFileWithFilter() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithEvolvedSchema() throws IOException{\n+\tpublic void testReadFileWithEvolvedSchema() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(\n \t\t\tgetPath(TEST_FILE_FLAT),", "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java"}, {"additions": 373, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java", "sha": "9d3be63b294c0c22e35f44abe450e22abc6062d3", "changes": 373, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java?ref=bcead3be32c624008730555d828fd8e9447fbeff", "patch": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc.util;\n+\n+import org.apache.flink.orc.OrcRowInputFormatTest;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.Writer;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * A generator for ORC test files.\n+ */\n+public class OrcTestFileGenerator {\n+\n+\tpublic static void main(String[] args) throws IOException {\n+\t\twriteCompositeTypesWithNullsFile(args[0]);\n+//\t\twriteCompositeTypesWithRepeatingFile(args[0]);\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and null values on different levels.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_COMPOSITES_NULLS}.\n+\t */\n+\tprivate static void writeCompositeTypesWithNullsFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[1];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[2];\n+\t\tListColumnVector nestedList = (ListColumnVector) list1.child;\n+\t\tListColumnVector nestedList2 = (ListColumnVector) nestedList.child;\n+\t\tStructColumnVector listEntries = (StructColumnVector) nestedList2.child;\n+\t\tBytesColumnVector entryField1 = (BytesColumnVector) listEntries.fields[0];\n+\t\tBytesColumnVector entryField2 = (BytesColumnVector) listEntries.fields[1];\n+\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[3];\n+\t\tMapColumnVector map1 = (MapColumnVector) list2.child;\n+\t\tBytesColumnVector keys = (BytesColumnVector) map1.keys;\n+\t\tLongColumnVector vals = (LongColumnVector) map1.values;\n+\n+\t\tfinal int list1Size = 4;\n+\t\tfinal int nestedListSize = 3;\n+\t\tfinal int nestedList2Size = 2;\n+\t\tfinal int list2Size = 3;\n+\t\tfinal int mapSize = 3;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tnestedList.ensureSize(batchSize * list1Size, false);\n+\t\tnestedList2.ensureSize(batchSize * list1Size * nestedListSize, false);\n+\t\tlistEntries.ensureSize(batchSize * list1Size * nestedListSize * nestedList2Size, false);\n+\t\tmap1.ensureSize(batchSize * list2Size, false);\n+\t\tkeys.ensureSize(batchSize * list2Size * mapSize, false);\n+\t\tvals.ensureSize(batchSize * list2Size * mapSize, false);\n+\n+\t\t// add 2500 rows to file\n+\t\tfor (int r = 0; r < 2500; ++r) {\n+\t\t\tint row = batch.size++;\n+\n+\t\t\t// mark nullable fields\n+\t\t\tlist1.noNulls = false;\n+\t\t\tnestedList.noNulls = false;\n+\t\t\tlistEntries.noNulls = false;\n+\t\t\tentryField1.noNulls = false;\n+\t\t\trecord1.noNulls = false;\n+\t\t\trecord1F2.noNulls = false;\n+\t\t\tlist2.noNulls = false;\n+\t\t\tmap1.noNulls = false;\n+\t\t\tkeys.noNulls = false;\n+\t\t\tvals.noNulls = false;\n+\n+\t\t\t// first field: int1\n+\t\t\tint1.vector[row] = r;\n+\n+\t\t\t// second field: struct\n+\t\t\tif (row % 2 != 0) {\n+\t\t\t\t// in every second row, the struct is null\n+\t\t\t\trecord1F1.vector[row] = row;\n+\t\t\t\tif (row % 5 != 0) {\n+\t\t\t\t\t// in every fifth row, the second field of the struct is null\n+\t\t\t\t\trecord1F2.setVal(row, (\"f2-\" + row).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t} else {\n+\t\t\t\t\trecord1F2.isNull[row] = true;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\trecord1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// third field: deeply nested list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the nested list is null\n+\t\t\t\tlist1.offsets[row] = list1.childCount;\n+\t\t\t\tlist1.lengths[row] = list1Size;\n+\t\t\t\tlist1.childCount += list1Size;\n+\n+\t\t\t\tfor (int i = 0; i < list1Size; i++) {\n+\n+\t\t\t\t\tint listOffset = (int) list1.offsets[row] + i;\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second nested list is always null\n+\t\t\t\t\t\tnestedList.offsets[listOffset] = nestedList.childCount;\n+\t\t\t\t\t\tnestedList.lengths[listOffset] = nestedListSize;\n+\t\t\t\t\t\tnestedList.childCount += nestedListSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < nestedListSize; j++) {\n+\t\t\t\t\t\t\tint nestedOffset = (int) nestedList.offsets[listOffset] + j;\n+\t\t\t\t\t\t\tnestedList2.offsets[nestedOffset] = nestedList2.childCount;\n+\t\t\t\t\t\t\tnestedList2.lengths[nestedOffset] = nestedList2Size;\n+\t\t\t\t\t\t\tnestedList2.childCount += nestedList2Size;\n+\n+\t\t\t\t\t\t\tfor (int k = 0; k < nestedList2Size; k++) {\n+\t\t\t\t\t\t\t\tint nestedOffset2 = (int) nestedList2.offsets[nestedOffset] + k;\n+\t\t\t\t\t\t\t\t// list entries\n+\t\t\t\t\t\t\t\tif (k != 1) {\n+\t\t\t\t\t\t\t\t\t// second struct is always null\n+\t\t\t\t\t\t\t\t\tif (k != 0) {\n+\t\t\t\t\t\t\t\t\t\t// first struct field in first struct is always null\n+\t\t\t\t\t\t\t\t\t\tentryField1.setVal(nestedOffset2, (\"f1-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\t\tentryField1.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\tentryField2.setVal(nestedOffset2, (\"f2-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\tlistEntries.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tnestedList.isNull[listOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// forth field: map in list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the map list is null\n+\t\t\t\tlist2.offsets[row] = list2.childCount;\n+\t\t\t\tlist2.lengths[row] = list2Size;\n+\t\t\t\tlist2.childCount += list2Size;\n+\n+\t\t\t\tfor (int i = 0; i < list2Size; i++) {\n+\t\t\t\t\tint mapOffset = (int) list2.offsets[row] + i;\n+\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second map list entry is always null\n+\t\t\t\t\t\tmap1.offsets[mapOffset] = map1.childCount;\n+\t\t\t\t\t\tmap1.lengths[mapOffset] = mapSize;\n+\t\t\t\t\t\tmap1.childCount += mapSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < mapSize; j++) {\n+\t\t\t\t\t\t\tint mapEntryOffset = (int) map1.offsets[mapOffset] + j;\n+\n+\t\t\t\t\t\t\tif (j != 1) {\n+\t\t\t\t\t\t\t\t// key in second map entry is always null\n+\t\t\t\t\t\t\t\tkeys.setVal(mapEntryOffset, (\"key-\" + row + \"-\" + j).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tkeys.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tif (j != 2) {\n+\t\t\t\t\t\t\t\t// value in third map entry is always null\n+\t\t\t\t\t\t\t\tvals.vector[mapEntryOffset] = row + i + j;\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tvals.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tmap1.isNull[mapOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist2.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\tif (row == batchSize - 1) {\n+\t\t\t\twriter.addRowBatch(batch);\n+\t\t\t\tbatch.reset();\n+\t\t\t}\n+\t\t}\n+\t\tif (batch.size != 0) {\n+\t\t\twriter.addRowBatch(batch);\n+\t\t\tbatch.reset();\n+\t\t}\n+\t\twriter.close();\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and repeated values.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_REPEATING}.\n+\t */\n+\tprivate static void writeCompositeTypesWithRepeatingFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"int2:int,\" +\n+\t\t\t\t\t\"int3:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<int>,\" +\n+\t\t\t\t\t\"list2:array<int>,\" +\n+\t\t\t\t\t\"list3:array<int>,\" +\n+\t\t\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\t\t\"map2:map<int,string>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\t\tLongColumnVector int2 = (LongColumnVector) batch.cols[1];\n+\t\tLongColumnVector int3 = (LongColumnVector) batch.cols[2];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[3];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\t\tStructColumnVector record2 = (StructColumnVector) batch.cols[4];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[5];\n+\t\tLongColumnVector list1int = (LongColumnVector) list1.child;\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[6];\n+\t\tLongColumnVector list2int = (LongColumnVector) list2.child;\n+\t\tListColumnVector list3 = (ListColumnVector) batch.cols[7];\n+\n+\t\tMapColumnVector map1 = (MapColumnVector) batch.cols[8];\n+\t\tLongColumnVector map1keys = (LongColumnVector) map1.keys;\n+\t\tBytesColumnVector map1vals = (BytesColumnVector) map1.values;\n+\t\tMapColumnVector map2 = (MapColumnVector) batch.cols[9];\n+\n+\t\tfinal int listSize = 3;\n+\t\tfinal int mapSize = 2;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tlist1int.ensureSize(batchSize * listSize, false);\n+\t\tlist2int.ensureSize(batchSize * listSize, false);\n+\t\tmap1keys.ensureSize(batchSize * mapSize, false);\n+\t\tmap1vals.ensureSize(batchSize * mapSize, false);\n+\n+\t\t// int1: all values are 42\n+\t\tint1.noNulls = true;\n+\t\tint1.setRepeating(true);\n+\t\tint1.vector[0] = 42;\n+\n+\t\t// int2: all values are null\n+\t\tint2.noNulls = false;\n+\t\tint2.setRepeating(true);\n+\t\tint2.isNull[0] = true;\n+\n+\t\t// int3: all values are 99\n+\t\tint3.noNulls = false;\n+\t\tint3.setRepeating(true);\n+\t\tint3.isNull[0] = false;\n+\t\tint3.vector[0] = 99;\n+\n+\t\t// record1: all records are [23, \"Hello\"]\n+\t\trecord1.noNulls = true;\n+\t\trecord1.setRepeating(true);\n+\t\tfor (int i = 0; i < batchSize; i++) {\n+\t\t\trecord1F1.vector[i] = i + 23;\n+\t\t}\n+\t\trecord1F2.noNulls = false;\n+\t\trecord1F2.isNull[0] = true;\n+\n+\t\t// record2: all records are null\n+\t\trecord2.noNulls = false;\n+\t\trecord2.setRepeating(true);\n+\t\trecord2.isNull[0] = true;\n+\n+\t\t// list1: all lists are [1, 2, 3]\n+\t\tlist1.noNulls = true;\n+\t\tlist1.setRepeating(true);\n+\t\tlist1.lengths[0] = listSize;\n+\t\tlist1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * listSize; i++) {\n+\t\t\tlist1int.vector[i] = i;\n+\t\t}\n+\n+\t\t// list2: all lists are [7, 7, 7]\n+\t\tlist2.noNulls = true;\n+\t\tlist2.setRepeating(true);\n+\t\tlist2.lengths[0] = listSize;\n+\t\tlist2.offsets[0] = 0;\n+\t\tlist2int.setRepeating(true);\n+\t\tlist2int.vector[0] = 7;\n+\n+\t\t// list3: all lists are null\n+\t\tlist3.noNulls = false;\n+\t\tlist3.setRepeating(true);\n+\t\tlist3.isNull[0] = true;\n+\n+\t\t// map1: all maps are [2 -> \"HELLO\", 4 -> \"HELLO\"]\n+\t\tmap1.noNulls = true;\n+\t\tmap1.setRepeating(true);\n+\t\tmap1.lengths[0] = mapSize;\n+\t\tmap1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * mapSize; i++) {\n+\t\t\tmap1keys.vector[i] = i * 2;\n+\t\t}\n+\t\tmap1vals.setRepeating(true);\n+\t\tmap1vals.setVal(0, \"Hello\".getBytes(StandardCharsets.UTF_8));\n+\n+\t\t// map2: all maps are null\n+\t\tmap2.noNulls = false;\n+\t\tmap2.setRepeating(true);\n+\t\tmap2.isNull[0] = true;\n+\n+\t\tbatch.size = 256;\n+\n+\t\twriter.addRowBatch(batch);\n+\t\tbatch.reset();\n+\t\twriter.close();\n+\t}\n+\n+}", "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc", "sha": "eed1c554cc019a6181e1e65e0f1a049e22c00dd3", "changes": 0, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff", "filename": "flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc", "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc", "sha": "ff2c917c6d47587813dee98a33192430550ef979", "changes": 0, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff", "filename": "flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/a95ec5acf259884347ae539913bcffcad5bfc340", "parent": "https://github.com/apache/flink/commit/f4e03689dd5fef8eafeb0996a31ea021c5ea2203", "message": "[FLINK-9358] Avoid NPE when closing an unestablished ResourceManager connection\n\nA NPE occurred when trying to disconnect an unestablished ResourceManager connection.\nIn order to fix this problem, we now check whether the connection has been established\nor not.\n\nThis closes #6011.", "bug_id": "flink_17", "file": [{"additions": 59, "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java", "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java", "sha": "46c1b4bde7a3c0458bf49b046648fb352a5d156e", "changes": 59, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340", "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.jobmaster;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerId;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * Class which contains the connection details of an established\n+ * connection with the ResourceManager.\n+ */\n+class EstablishedResourceManagerConnection {\n+\n+\tprivate final ResourceManagerGateway resourceManagerGateway;\n+\n+\tprivate final ResourceManagerId resourceManagerId;\n+\n+\tprivate final ResourceID resourceManagerResourceID;\n+\n+\tEstablishedResourceManagerConnection(\n+\t\t\t@Nonnull ResourceManagerGateway resourceManagerGateway,\n+\t\t\t@Nonnull ResourceManagerId resourceManagerId,\n+\t\t\t@Nonnull ResourceID resourceManagerResourceID) {\n+\t\tthis.resourceManagerGateway = resourceManagerGateway;\n+\t\tthis.resourceManagerId = resourceManagerId;\n+\t\tthis.resourceManagerResourceID = resourceManagerResourceID;\n+\t}\n+\n+\tpublic ResourceManagerGateway getResourceManagerGateway() {\n+\t\treturn resourceManagerGateway;\n+\t}\n+\n+\tpublic ResourceManagerId getResourceManagerId() {\n+\t\treturn resourceManagerId;\n+\t}\n+\n+\tpublic ResourceID getResourceManagerResourceID() {\n+\t\treturn resourceManagerResourceID;\n+\t}\n+}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java"}, {"additions": 48, "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java", "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java", "sha": "aff32808dd99070afac0e475efd0381734f40a6a", "changes": 83, "status": "modified", "deletions": 35, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340", "patch": "@@ -191,9 +191,6 @@\n \n \tprivate LeaderRetrievalService resourceManagerLeaderRetriever;\n \n-\t@Nullable\n-\tprivate ResourceManagerConnection resourceManagerConnection;\n-\n \t// --------- TaskManagers --------\n \n \tprivate final Map<ResourceID, Tuple2<TaskManagerLocation, TaskExecutorGateway>> registeredTaskManagers;\n@@ -211,6 +208,12 @@\n \t@Nullable\n \tprivate String lastInternalSavepoint;\n \n+\t@Nullable\n+\tprivate ResourceManagerConnection resourceManagerConnection;\n+\n+\t@Nullable\n+\tprivate EstablishedResourceManagerConnection establishedResourceManagerConnection;\n+\n \t// ------------------------------------------------------------------------\n \n \tpublic JobMaster(\n@@ -290,6 +293,9 @@ public JobMaster(\n \t\tthis.jobManagerJobMetricGroup = jobMetricGroupFactory.create(jobGraph);\n \t\tthis.executionGraph = createAndRestoreExecutionGraph(jobManagerJobMetricGroup);\n \t\tthis.jobStatusListener = null;\n+\n+\t\tthis.resourceManagerConnection = null;\n+\t\tthis.establishedResourceManagerConnection = null;\n \t}\n \n \t//----------------------------------------------------------------------------------------------\n@@ -881,12 +887,16 @@ public void disconnectResourceManager(\n \t\t\tfinal ResourceManagerId resourceManagerId,\n \t\t\tfinal Exception cause) {\n \n-\t\tif (resourceManagerConnection != null\n-\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId)) {\n+\t\tif (isConnectingToResourceManager(resourceManagerId)) {\n \t\t\tcloseResourceManagerConnection(cause);\n \t\t}\n \t}\n \n+\tprivate boolean isConnectingToResourceManager(ResourceManagerId resourceManagerId) {\n+\t\treturn resourceManagerConnection != null\n+\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId);\n+\t}\n+\n \t@Override\n \tpublic void heartbeatFromTaskManager(final ResourceID resourceID, AccumulatorReport accumulatorReport) {\n \t\ttaskManagerHeartbeatManager.receiveHeartbeat(resourceID, accumulatorReport);\n@@ -1238,11 +1248,11 @@ private void notifyOfNewResourceManagerLeader(final String resourceManagerAddres\n \t\t\t\t\treturn;\n \t\t\t\t}\n \n-\t\t\t\tcloseResourceManagerConnection(new Exception(\n-\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n-\n \t\t\t\tlog.info(\"ResourceManager leader changed from {} to {}. Registering at new leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress(), resourceManagerAddress);\n+\n+\t\t\t\tcloseResourceManagerConnection(new Exception(\n+\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n \t\t\t} else {\n \t\t\t\tlog.info(\"Current ResourceManager {} lost leader status. Waiting for new ResourceManager leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress());\n@@ -1277,9 +1287,16 @@ private void establishResourceManagerConnection(final JobMasterRegistrationSucce\n \n \t\t\tfinal ResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n \n+\t\t\tfinal ResourceID resourceManagerResourceId = success.getResourceManagerResourceId();\n+\n+\t\t\testablishedResourceManagerConnection = new EstablishedResourceManagerConnection(\n+\t\t\t\tresourceManagerGateway,\n+\t\t\t\tsuccess.getResourceManagerId(),\n+\t\t\t\tresourceManagerResourceId);\n+\n \t\t\tslotPoolGateway.connectToResourceManager(resourceManagerGateway);\n \n-\t\t\tresourceManagerHeartbeatManager.monitorTarget(success.getResourceManagerResourceId(), new HeartbeatTarget<Void>() {\n+\t\t\tresourceManagerHeartbeatManager.monitorTarget(resourceManagerResourceId, new HeartbeatTarget<Void>() {\n \t\t\t\t@Override\n \t\t\t\tpublic void receiveHeartbeat(ResourceID resourceID, Void payload) {\n \t\t\t\t\tresourceManagerGateway.heartbeatFromJobManager(resourceID);\n@@ -1297,22 +1314,31 @@ public void requestHeartbeat(ResourceID resourceID, Void payload) {\n \t}\n \n \tprivate void closeResourceManagerConnection(Exception cause) {\n-\t\tif (resourceManagerConnection != null) {\n-\t\t\tif (log.isDebugEnabled()) {\n-\t\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause);\n-\t\t\t} else {\n-\t\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause.getMessage());\n-\t\t\t}\n-\n-\t\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerConnection.getResourceManagerResourceID());\n-\n-\t\t\tResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n-\t\t\tresourceManagerGateway.disconnectJobManager(resourceManagerConnection.getJobID(), cause);\n+\t\tif (establishedResourceManagerConnection != null) {\n+\t\t\tdissolveResourceManagerConnection(establishedResourceManagerConnection, cause);\n+\t\t\testablishedResourceManagerConnection = null;\n+\t\t}\n \n+\t\tif (resourceManagerConnection != null) {\n+\t\t\t// stop a potentially ongoing registration process\n \t\t\tresourceManagerConnection.close();\n \t\t\tresourceManagerConnection = null;\n \t\t}\n+\t}\n+\n+\tprivate void dissolveResourceManagerConnection(EstablishedResourceManagerConnection establishedResourceManagerConnection, Exception cause) {\n+\t\tfinal ResourceID resourceManagerResourceID = establishedResourceManagerConnection.getResourceManagerResourceID();\n \n+\t\tif (log.isDebugEnabled()) {\n+\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerResourceID, cause);\n+\t\t} else {\n+\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerResourceID, cause.getMessage());\n+\t\t}\n+\n+\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerResourceID);\n+\n+\t\tResourceManagerGateway resourceManagerGateway = establishedResourceManagerConnection.getResourceManagerGateway();\n+\t\tresourceManagerGateway.disconnectJobManager(jobGraph.getJobID(), cause);\n \t\tslotPoolGateway.disconnectResourceManager();\n \t}\n \n@@ -1473,8 +1499,6 @@ public void handleError(final Exception exception) {\n \n \t\tprivate final JobMasterId jobMasterId;\n \n-\t\tprivate ResourceID resourceManagerResourceID;\n-\n \t\tResourceManagerConnection(\n \t\t\t\tfinal Logger log,\n \t\t\t\tfinal JobID jobID,\n@@ -1498,7 +1522,7 @@ public void handleError(final Exception exception) {\n \t\t\t\t\tgetTargetAddress(), getTargetLeaderId()) {\n \t\t\t\t@Override\n \t\t\t\tprotected CompletableFuture<RegistrationResponse> invokeRegistration(\n-\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) throws Exception {\n+\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) {\n \t\t\t\t\tTime timeout = Time.milliseconds(timeoutMillis);\n \n \t\t\t\t\treturn gateway.registerJobManager(\n@@ -1513,24 +1537,13 @@ public void handleError(final Exception exception) {\n \n \t\t@Override\n \t\tprotected void onRegistrationSuccess(final JobMasterRegistrationSuccess success) {\n-\t\t\trunAsync(() -> {\n-\t\t\t\tresourceManagerResourceID = success.getResourceManagerResourceId();\n-\t\t\t\testablishResourceManagerConnection(success);\n-\t\t\t});\n+\t\t\trunAsync(() -> establishResourceManagerConnection(success));\n \t\t}\n \n \t\t@Override\n \t\tprotected void onRegistrationFailure(final Throwable failure) {\n \t\t\thandleJobMasterError(failure);\n \t\t}\n-\n-\t\tpublic ResourceID getResourceManagerResourceID() {\n-\t\t\treturn resourceManagerResourceID;\n-\t\t}\n-\n-\t\tpublic JobID getJobID() {\n-\t\t\treturn jobID;\n-\t\t}\n \t}\n \n \t//----------------------------------------------------------------------------------------------", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java"}, {"additions": 48, "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java", "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java", "sha": "c0c916246637c173a961ee808831cec32640bbdb", "changes": 48, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340", "patch": "@@ -23,6 +23,7 @@\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.configuration.BlobServerOptions;\n import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.testutils.OneShotLatch;\n import org.apache.flink.runtime.blob.BlobServer;\n import org.apache.flink.runtime.blob.VoidBlobStore;\n import org.apache.flink.runtime.checkpoint.CheckpointProperties;\n@@ -361,6 +362,53 @@ public void testCheckpointPrecedesSavepointRecovery() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Tests that we can close an unestablished ResourceManager connection.\n+\t */\n+\t@Test\n+\tpublic void testCloseUnestablishedResourceManagerConnection() throws Exception {\n+\t\tfinal JobMaster jobMaster = createJobMaster(\n+\t\t\tJobMasterConfiguration.fromConfiguration(configuration),\n+\t\t\tjobGraph,\n+\t\t\thaServices,\n+\t\t\tnew TestingJobManagerSharedServicesBuilder().build());\n+\n+\t\ttry {\n+\t\t\tjobMaster.start(JobMasterId.generate(), testingTimeout).get();\n+\t\t\tfinal ResourceManagerId resourceManagerId = ResourceManagerId.generate();\n+\t\t\tfinal String firstResourceManagerAddress = \"address1\";\n+\t\t\tfinal String secondResourceManagerAddress = \"address2\";\n+\n+\t\t\tfinal TestingResourceManagerGateway firstResourceManagerGateway = new TestingResourceManagerGateway();\n+\t\t\tfinal TestingResourceManagerGateway secondResourceManagerGateway = new TestingResourceManagerGateway();\n+\n+\t\t\trpcService.registerGateway(firstResourceManagerAddress, firstResourceManagerGateway);\n+\t\t\trpcService.registerGateway(secondResourceManagerAddress, secondResourceManagerGateway);\n+\n+\t\t\tfinal OneShotLatch firstJobManagerRegistration = new OneShotLatch();\n+\t\t\tfinal OneShotLatch secondJobManagerRegistration = new OneShotLatch();\n+\n+\t\t\tfirstResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> firstJobManagerRegistration.trigger());\n+\n+\t\t\tsecondResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> secondJobManagerRegistration.trigger());\n+\n+\t\t\trmLeaderRetrievalService.notifyListener(firstResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// wait until we have seen the first registration attempt\n+\t\t\tfirstJobManagerRegistration.await();\n+\n+\t\t\t// this should stop the connection attempts towards the first RM\n+\t\t\trmLeaderRetrievalService.notifyListener(secondResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// check that we start registering at the second RM\n+\t\t\tsecondJobManagerRegistration.await();\n+\t\t} finally {\n+\t\t\tRpcUtils.terminateRpcEndpoint(jobMaster, testingTimeout);\n+\t\t}\n+\t}\n+\n \tprivate File createSavepoint(long savepointId) throws IOException {\n \t\tfinal File savepointFile = temporaryFolder.newFile();\n \t\tfinal SavepointV2 savepoint = new SavepointV2(savepointId, Collections.emptyList(), Collections.emptyList());", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "parent": "https://github.com/apache/flink/commit/b957bf26fb588cab072e51240e9026456b862ce7", "message": "[FLINK-9256] [network] Fix NPE in SingleInputGate#updateInputChannel() for non-credit based flow control\n\nThis closes #5914", "bug_id": "flink_18", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java", "sha": "f25475638723d8d364fe5e04c3b6b05bd2eed56d", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -157,6 +157,10 @@ public int getPartitionRequestMaxBackoff() {\n \t\treturn partitionRequestMaxBackoff;\n \t}\n \n+\tpublic boolean isCreditBased() {\n+\t\treturn enableCreditBased;\n+\t}\n+\n \tpublic KvStateRegistry getKvStateRegistry() {\n \t\treturn kvStateRegistry;\n \t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java"}, {"additions": 15, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java", "sha": "06e80ff531aaff98c8de7fd0ecc221e800578bf0", "changes": 20, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -157,9 +157,11 @@\n \t */\n \tprivate BufferPool bufferPool;\n \n-\t/** Global network buffer pool to request and recycle exclusive buffers. */\n+\t/** Global network buffer pool to request and recycle exclusive buffers (only for credit-based). */\n \tprivate NetworkBufferPool networkBufferPool;\n \n+\tprivate final boolean isCreditBased;\n+\n \tprivate boolean hasReceivedAllEndOfPartitionEvents;\n \n \t/** Flag indicating whether partitions have been requested. */\n@@ -189,7 +191,8 @@ public SingleInputGate(\n \t\tint consumedSubpartitionIndex,\n \t\tint numberOfInputChannels,\n \t\tTaskActions taskActions,\n-\t\tTaskIOMetricGroup metrics) {\n+\t\tTaskIOMetricGroup metrics,\n+\t\tboolean isCreditBased) {\n \n \t\tthis.owningTaskName = checkNotNull(owningTaskName);\n \t\tthis.jobId = checkNotNull(jobId);\n@@ -208,6 +211,7 @@ public SingleInputGate(\n \t\tthis.enqueuedInputChannelsWithData = new BitSet(numberOfInputChannels);\n \n \t\tthis.taskActions = checkNotNull(taskActions);\n+\t\tthis.isCreditBased = isCreditBased;\n \t}\n \n \t// ------------------------------------------------------------------------\n@@ -288,6 +292,7 @@ public void setBufferPool(BufferPool bufferPool) {\n \t * @param networkBuffersPerChannel The number of exclusive buffers for each channel\n \t */\n \tpublic void assignExclusiveSegments(NetworkBufferPool networkBufferPool, int networkBuffersPerChannel) throws IOException {\n+\t\tcheckState(this.isCreditBased, \"Bug in input gate setup logic: exclusive buffers only exist with credit-based flow control.\");\n \t\tcheckState(this.networkBufferPool == null, \"Bug in input gate setup logic: global buffer pool has\" +\n \t\t\t\"already been set for this input gate.\");\n \n@@ -347,8 +352,13 @@ public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOE\n \t\t\t\t}\n \t\t\t\telse if (partitionLocation.isRemote()) {\n \t\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n-\t\t\t\t\t((RemoteInputChannel)newChannel).assignExclusiveSegments(\n-\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\n+\t\t\t\t\tif (this.isCreditBased) {\n+\t\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n+\t\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n+\t\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n+\t\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n@@ -661,7 +671,7 @@ public static SingleInputGate create(\n \n \t\tfinal SingleInputGate inputGate = new SingleInputGate(\n \t\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n-\t\t\ticdd.length, taskActions, metrics);\n+\t\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n \n \t\t// Create the input channels. There is one input channel for each consumed partition.\n \t\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java", "sha": "f790b5f02b960c85f954b7778be1b4d99f1366d4", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -329,7 +329,7 @@ private static ResultPartition createResultPartition(\n \t *\n \t * @return input gate with some fake settings\n \t */\n-\tprivate static SingleInputGate createSingleInputGate(\n+\tprivate SingleInputGate createSingleInputGate(\n \t\t\tfinal ResultPartitionType partitionType, final int channels) {\n \t\treturn spy(new SingleInputGate(\n \t\t\t\"Test Task Name\",\n@@ -339,7 +339,8 @@ private static SingleInputGate createSingleInputGate(\n \t\t\t0,\n \t\t\tchannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()));\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl));\n \t}\n \n \tprivate static void createRemoteInputChannel(", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java", "sha": "842aed8186d2ee53f34901ed0c5ef110d2a1626c", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -221,7 +221,8 @@ static SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \t/**", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java"}, {"additions": 6, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java", "sha": "73f3cfbc49a392925f56a09cab1dd3ccda650c18", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -66,7 +66,8 @@ public void testConsumptionWithLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -102,7 +103,8 @@ public void testConsumptionWithRemoteChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tRemoteInputChannel channel = new RemoteInputChannel(\n@@ -151,7 +153,8 @@ public void testConsumptionWithMixedChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0, local = 0; i < numChannels; i++) {\n \t\t\tif (localOrRemote.get(i)) {", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java"}, {"additions": 11, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java", "sha": "82a27cc92c0255b97102f4d95910534b73252041", "changes": 17, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -93,7 +93,8 @@ public void testFairConsumptionLocalChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -146,7 +147,8 @@ public void testFairConsumptionLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -196,7 +198,8 @@ public void testFairConsumptionRemoteChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -251,7 +254,8 @@ public void testFairConsumptionRemoteChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -349,11 +353,12 @@ public FairnessVerifyingInputGate(\n \t\t\t\tint consumedSubpartitionIndex,\n \t\t\t\tint numberOfInputChannels,\n \t\t\t\tTaskActions taskActions,\n-\t\t\t\tTaskIOMetricGroup metrics) {\n+\t\t\t\tTaskIOMetricGroup metrics,\n+\t\t\t\tboolean isCreditBased) {\n \n \t\t\tsuper(owningTaskName, jobId, consumedResultId, ResultPartitionType.PIPELINED,\n \t\t\t\tconsumedSubpartitionIndex,\n-\t\t\t\t\tnumberOfInputChannels, taskActions, metrics);\n+\t\t\t\t\tnumberOfInputChannels, taskActions, metrics, isCreditBased);\n \n \t\t\ttry {\n \t\t\t\tField f = SingleInputGate.class.getDeclaredField(\"inputChannelsWithData\");", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java", "sha": "1ecb67ff82c62f50a753a7c519081d7e37e647fb", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -293,7 +293,8 @@ public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue\n \t\t);\n \n \t\tResultPartitionManager partitionManager = mock(ResultPartitionManager.class);\n@@ -490,7 +491,8 @@ public TestLocalInputChannelConsumer(\n \t\t\t\t\tsubpartitionIndex,\n \t\t\t\t\tnumberOfInputChannels,\n \t\t\t\t\tmock(TaskActions.class),\n-\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\t\ttrue);\n \n \t\t\t// Set buffer pool\n \t\t\tinputGate.setBufferPool(bufferPool);", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java", "sha": "802cb936c09493db28adb8820bce809fc37a77cf", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -889,7 +889,8 @@ private SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \tprivate RemoteInputChannel createRemoteInputChannel(SingleInputGate inputGate)", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java"}, {"additions": 248, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java", "sha": "c24466880369b5ba00ddaf0d8efc626145e55bdb", "changes": 318, "status": "modified", "deletions": 70, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -19,13 +19,13 @@\n package org.apache.flink.runtime.io.network.partition.consumer;\n \n import org.apache.flink.api.common.JobID;\n-import org.apache.flink.core.memory.MemorySegment;\n import org.apache.flink.core.memory.MemorySegmentFactory;\n import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.ResultPartitionLocation;\n import org.apache.flink.runtime.event.TaskEvent;\n import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n import org.apache.flink.runtime.io.network.ConnectionID;\n import org.apache.flink.runtime.io.network.ConnectionManager;\n import org.apache.flink.runtime.io.network.LocalConnectionManager;\n@@ -45,31 +45,51 @@\n import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;\n import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.query.KvStateRegistry;\n import org.apache.flink.runtime.taskmanager.TaskActions;\n \n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyInt;\n-import static org.mockito.Matchers.anyListOf;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+/**\n+ * Tests for {@link SingleInputGate}.\n+ */\n+@RunWith(Parameterized.class)\n public class SingleInputGateTest {\n \n+\t@Parameterized.Parameter\n+\tpublic boolean enableCreditBasedFlowControl;\n+\n+\t@Parameterized.Parameters(name = \"Credit-based = {0}\")\n+\tpublic static List<Boolean> parameters() {\n+\t\treturn Arrays.asList(Boolean.TRUE, Boolean.FALSE);\n+\t}\n+\n \t/**\n \t * Tests basic correctness of buffer-or-event interleaving and correct <code>null</code> return\n \t * value after receiving all end-of-partition events.\n@@ -324,12 +344,7 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\tint initialBackoff = 137;\n \t\tint maxBackoff = 1001;\n \n-\t\tNetworkEnvironment netEnv = mock(NetworkEnvironment.class);\n-\t\twhen(netEnv.getResultPartitionManager()).thenReturn(new ResultPartitionManager());\n-\t\twhen(netEnv.getTaskEventDispatcher()).thenReturn(new TaskEventDispatcher());\n-\t\twhen(netEnv.getPartitionRequestInitialBackoff()).thenReturn(initialBackoff);\n-\t\twhen(netEnv.getPartitionRequestMaxBackoff()).thenReturn(maxBackoff);\n-\t\twhen(netEnv.getConnectionManager()).thenReturn(new LocalConnectionManager());\n+\t\tfinal NetworkEnvironment netEnv = createNetworkEnvironment(2, 8, initialBackoff, maxBackoff);\n \n \t\tSingleInputGate gate = SingleInputGate.create(\n \t\t\t\"TestTask\",\n@@ -340,37 +355,43 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\t\tmock(TaskActions.class),\n \t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n \n-\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n+\t\ttry {\n+\t\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n \n-\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n+\t\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n \n-\t\tassertEquals(3, channelMap.size());\n-\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n-\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n+\t\t\tassertEquals(3, channelMap.size());\n+\t\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n+\t\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n \n-\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n-\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n+\t\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n+\t\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n \n-\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n-\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n+\t\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n+\t\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n \n-\t\tInputChannel[] channels = new InputChannel[]{localChannel, remoteChannel, unknownChannel};\n-\t\tfor (InputChannel ch : channels) {\n-\t\t\tassertEquals(0, ch.getCurrentBackoff());\n+\t\t\tInputChannel[] channels =\n+\t\t\t\tnew InputChannel[] {localChannel, remoteChannel, unknownChannel};\n+\t\t\tfor (InputChannel ch : channels) {\n+\t\t\t\tassertEquals(0, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tgate.releaseAllResources();\n+\t\t\tnetEnv.shutdown();\n \t\t}\n \t}\n \n@@ -379,26 +400,39 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithRemoteInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = new SingleInputGate(\n-\t\t\t\"t1\",\n-\t\t\tnew JobID(),\n-\t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED_BOUNDED,\n-\t\t\t0,\n-\t\t\t1,\n-\t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n-\n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tinputGate.setInputChannel(new IntermediateResultPartitionID(), remote);\n-\n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\t// Trigger requests of segments from global pool and assign buffers to remote input channel\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n-\n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\taddRemoteInputChannel(network, inputGate, connectionId, resultPartitionId, 0);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t/**\n@@ -407,51 +441,195 @@ public void testRequestBuffersWithRemoteInputChannel() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithUnknownInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = createInputGate(1);\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, extraNetworkBuffersPerGate, 0, 0);\n \n-\t\tUnknownInputChannel unknown = mock(UnknownInputChannel.class);\n-\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n-\t\tinputGate.setInputChannel(resultPartitionId.getPartitionId(), unknown);\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, resultPartitionId, 0);\n \n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n-\t\twhen(unknown.toRemoteInputChannel(connectionId)).thenReturn(remote);\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n \n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool, times(0)).requestMemorySegments(buffersPerChannel);\n \n-\t\t// Trigger updates to remote input channel from unknown input channel\n-\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n-\t\t\tresultPartitionId,\n-\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments(),\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tresultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n+\t}\n \n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t/**\n+\t * Tests that input gate can successfully convert unknown input channels into local and remote\n+\t * channels.\n+\t */\n+\t@Test\n+\tpublic void testUpdateUnknownInputChannel() throws Exception {\n+\t\tfinal SingleInputGate inputGate = createInputGate(2);\n+\t\tint buffersPerChannel = 2;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, 8, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID localResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, localResultPartitionId, 0);\n+\n+\t\t\tfinal ResultPartitionID remoteResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, remoteResultPartitionId, 1);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID remoteConnectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tremoteResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(remoteConnectionId)));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to local input channel from unknown input channel\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tlocalResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createLocal()));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((LocalInputChannel.class))));\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t// ---------------------------------------------------------------------------------------------\n \n-\tprivate static SingleInputGate createInputGate() {\n+\tprivate NetworkEnvironment createNetworkEnvironment(\n+\t\t\tint buffersPerChannel,\n+\t\t\tint extraNetworkBuffersPerGate,\n+\t\t\tint initialBackoff,\n+\t\t\tint maxBackoff) {\n+\t\treturn new NetworkEnvironment(\n+\t\t\tspy(new NetworkBufferPool(100, 32)),\n+\t\t\tnew LocalConnectionManager(),\n+\t\t\tnew ResultPartitionManager(),\n+\t\t\tnew TaskEventDispatcher(),\n+\t\t\tnew KvStateRegistry(),\n+\t\t\tnull,\n+\t\t\tnull,\n+\t\t\tIOManager.IOMode.SYNC,\n+\t\t\tinitialBackoff,\n+\t\t\tmaxBackoff,\n+\t\t\tbuffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate,\n+\t\t\tenableCreditBasedFlowControl);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate() {\n \t\treturn createInputGate(2);\n \t}\n \n-\tprivate static SingleInputGate createInputGate(int numberOfInputChannels) {\n+\tprivate SingleInputGate createInputGate(int numberOfInputChannels) {\n+\t\treturn createInputGate(numberOfInputChannels, ResultPartitionType.PIPELINED);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate(\n+\t\t\tint numberOfInputChannels, ResultPartitionType partitionType) {\n \t\tSingleInputGate inputGate = new SingleInputGate(\n \t\t\t\"Test Task Name\",\n \t\t\tnew JobID(),\n \t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED,\n+\t\t\tpartitionType,\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl);\n \n-\t\tassertEquals(ResultPartitionType.PIPELINED, inputGate.getConsumedPartitionType());\n+\t\tassertEquals(partitionType, inputGate.getConsumedPartitionType());\n \n \t\treturn inputGate;\n \t}\n \n+\tprivate void addUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tUnknownInputChannel unknown =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), unknown);\n+\t}\n+\n+\tprivate UnknownInputChannel createUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\treturn new UnknownInputChannel(\n+\t\t\tinputGate,\n+\t\t\tchannelIndex,\n+\t\t\tpartitionId,\n+\t\t\tnetwork.getResultPartitionManager(),\n+\t\t\tnetwork.getTaskEventDispatcher(),\n+\t\t\tnetwork.getConnectionManager(),\n+\t\t\tnetwork.getPartitionRequestInitialBackoff(),\n+\t\t\tnetwork.getPartitionRequestMaxBackoff(),\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t);\n+\t}\n+\n+\tprivate void addRemoteInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tConnectionID connectionId,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tRemoteInputChannel remote =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex)\n+\t\t\t\t.toRemoteInputChannel(connectionId);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), remote);\n+\t}\n+\n \tstatic void verifyBufferOrEvent(\n \t\t\tInputGate inputGate,\n \t\t\tboolean expectedIsBuffer,", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java", "sha": "33dc1ca205dc84fe92134c8db734151ddf61274f", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -60,7 +60,8 @@ public TestSingleInputGate(int numberOfInputChannels, boolean initialize) {\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tthis.inputGate = spy(realGate);\n ", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java", "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java", "sha": "081d97d5cbe409e05d350c0c17ce0d68ab90b339", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240", "patch": "@@ -50,13 +50,15 @@ public void testBasicGetNextLogic() throws Exception {\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 3,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t\tfinal SingleInputGate ig2 = new SingleInputGate(\n \t\t\ttestTaskName, new JobID(),\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 5,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tfinal UnionInputGate union = new UnionInputGate(new SingleInputGate[]{ig1, ig2});\n ", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/9e139a72ba45f2dd820bd3b9ecdf8428588666fd", "parent": "https://github.com/apache/flink/commit/24c30878ed6f6ed1599a5ec23362055e0e88916f", "message": "[FLINK-8423] OperatorChain#pushToOperator catch block may fail with NPE\n\nThis closes #5447.", "bug_id": "flink_19", "file": [{"additions": 14, "raw_url": "https://github.com/apache/flink/raw/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java", "blob_url": "https://github.com/apache/flink/blob/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java", "sha": "f3c7293fe1b17e04a8057c56aa4c2af732ae68e5", "changes": 25, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java?ref=9e139a72ba45f2dd820bd3b9ecdf8428588666fd", "patch": "@@ -591,17 +591,20 @@ public void collect(StreamRecord<T> record) {\n \t\t\t\toperator.setKeyContextElement1(copy);\n \t\t\t\toperator.processElement(copy);\n \t\t\t} catch (ClassCastException e) {\n-\t\t\t\t// Enrich error message\n-\t\t\t\tClassCastException replace = new ClassCastException(\n-\t\t\t\t\tString.format(\n-\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n-\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n-\t\t\t\t\t\t\"but identical names are being used.\",\n-\t\t\t\t\t\te.getMessage(),\n-\t\t\t\t\t\toutputTag.getId()));\n-\n-\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n-\n+\t\t\t\tif (outputTag != null) {\n+\t\t\t\t\t// Enrich error message\n+\t\t\t\t\tClassCastException replace = new ClassCastException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n+\t\t\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n+\t\t\t\t\t\t\t\t\"but identical names are being used.\",\n+\t\t\t\t\t\t\te.getMessage(),\n+\t\t\t\t\t\t\toutputTag.getId()));\n+\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n+\t\t\t\t}\n \t\t\t} catch (Exception e) {\n \t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n \t\t\t}", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/d7911c5a8a6896261c55b61ea4633e706270baa1", "parent": "https://github.com/apache/flink/commit/d7c2c417213502130b1aeab1868313df178555cc", "message": "[FLINK-6294] Fix potential NPE in BucketingSink.close()", "bug_id": "flink_20", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java", "blob_url": "https://github.com/apache/flink/blob/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java", "sha": "db0a5d859bed4592627860748d687762a35964a7", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java?ref=d7911c5a8a6896261c55b61ea4633e706270baa1", "patch": "@@ -414,8 +414,10 @@ private void initFileSystem() throws IOException {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n-\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\tif (state != null) {\n+\t\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n+\t\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\t\t}\n \t\t}\n \t}\n ", "filename": "flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/9d20e898c846cca117efd2a246f82c0374fcbda2", "parent": "https://github.com/apache/flink/commit/cd532516189b801bb02650665cbb109f8d8f8887", "message": "[FLINK-7971] [table] Fix potential NPE in non-windowed aggregation.\n\nThis closes #4941.", "bug_id": "flink_21", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/9d20e898c846cca117efd2a246f82c0374fcbda2/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala", "blob_url": "https://github.com/apache/flink/blob/9d20e898c846cca117efd2a246f82c0374fcbda2/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala", "sha": "397032003ec815ec8288d2728be5780381ae97d3", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala?ref=9d20e898c846cca117efd2a246f82c0374fcbda2", "patch": "@@ -97,11 +97,14 @@ class GroupAggProcessFunction(\n     if (null == accumulators) {\n       firstRow = true\n       accumulators = function.createAccumulators()\n-      inputCnt = 0L\n     } else {\n       firstRow = false\n     }\n \n+    if (null == inputCnt) {\n+      inputCnt = 0L\n+    }\n+\n     // Set group keys value to the final output\n     function.setForwardedFields(input, newRow.row)\n     function.setForwardedFields(input, prevRow.row)", "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/GroupAggProcessFunction.scala"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "parent": "https://github.com/apache/flink/commit/a2ec3ee664b540c1213991d7fcf56d8873e60d40", "message": "[FLINK-6445] [cep] Fix NPE in no-condition patterns.", "bug_id": "flink_22", "file": [{"additions": 6, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java", "sha": "b100bc5f80135981aa94027bb068a0f75a2ceb51", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -127,8 +127,9 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> where(IterativeCondition<F> condition) {\n-\t\tClosureCleaner.clean(condition, true);\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n \n+\t\tClosureCleaner.clean(condition, true);\n \t\tif (this.condition == null) {\n \t\t\tthis.condition = condition;\n \t\t} else {\n@@ -148,6 +149,8 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> or(IterativeCondition<F> condition) {\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n+\n \t\tClosureCleaner.clean(condition, true);\n \n \t\tif (this.condition == null) {\n@@ -167,6 +170,8 @@ public Quantifier getQuantifier() {\n \t * @return The same pattern with the new subtype constraint\n \t */\n \tpublic <S extends F> Pattern<T, S> subtype(final Class<S> subtypeClass) {\n+\t\tPreconditions.checkNotNull(subtypeClass, \"The class cannot be null.\");\n+\n \t\tif (condition == null) {\n \t\t\tthis.condition = new SubtypeCondition<F>(subtypeClass);\n \t\t} else {", "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java", "sha": "ac34c41301b9e24c2c5818f94b4a12dfad5f930f", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code AND} and returns {@code true} if both are {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic AndCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override", "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java"}, {"additions": 1, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java", "sha": "9318c2f67726fb2cb6d30c1bad6824c302e2fc39", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -35,6 +35,6 @@ public NotCondition(final IterativeCondition<T> original) {\n \n \t@Override\n \tpublic boolean filter(T value, Context<T> ctx) throws Exception {\n-\t\treturn !original.filter(value, ctx);\n+\t\treturn original != null && !original.filter(value, ctx);\n \t}\n }", "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java", "sha": "d3690ab4da065734b933c95dd954dc559798501d", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code OR} and returns {@code true} if at least one is {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic OrCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override", "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java", "sha": "cff8693c588aacb7f1ed0d702b59e50fb768d4a3", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which filters elements of the given type.\n  * An element is filtered out iff it is not assignable to the given subtype of {@code T}.\n@@ -31,7 +33,7 @@\n \tprivate final Class<? extends T> subtype;\n \n \tpublic SubtypeCondition(final Class<? extends T> subtype) {\n-\t\tthis.subtype = subtype;\n+\t\tthis.subtype = Preconditions.checkNotNull(subtype, \"The subtype cannot be null.\");\n \t}\n \n \t@Override", "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java"}, {"additions": 66, "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java", "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java", "sha": "fe31564af30af948fa8149b5a62215f800ab3bb2", "changes": 66, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57", "patch": "@@ -50,6 +50,72 @@\n @SuppressWarnings(\"unchecked\")\n public class NFAITCase extends TestLogger {\n \n+\t@Test\n+\tpublic void testNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedBy(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n+\t@Test\n+\tpublic void testAnyWithNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedByAny(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(a, c),\n+\t\t\t\tLists.newArrayList(a, d),\n+\t\t\t\tLists.newArrayList(a, e),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(b, d),\n+\t\t\t\tLists.newArrayList(b, e),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(c, e),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n \t@Test\n \tpublic void testSimplePatternNFA() {\n \t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();", "filename": "flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/cc412859001a9437e5176596dc284f05bd740a40", "parent": "https://github.com/apache/flink/commit/fdabce0339a5ba89787f8719e463cd01f35b4601", "message": "[FLINK-9458] Ignore SharedSlot in CoLocationConstraint when not using legacy mode\n\nThe SharedSlot in CoLocationConstraint is only set when using the legacy mode. Thus,\nCoLocationConstraint#isAssignedAlive should only check the SharedSlot if it was\npreviously set. This fixes the NPE when restarting a job with a co-location constraint\nwhen using the new mode.\n\nThis closes #6119.", "bug_id": "flink_23", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java", "sha": "23c9c214ccc31ba6a8437c1948a16d37004edd8e", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -99,9 +99,11 @@ public boolean isAssigned() {\n \t *\n \t * @return True if the location has been assigned and the shared slot is alive,\n \t *         false otherwise.\n+\t * @deprecated Should only be called by legacy code (if using {@link Scheduler})\n \t */\n+\t@Deprecated\n \tpublic boolean isAssignedAndAlive() {\n-\t\treturn lockedLocation != null && sharedSlot.isAlive();\n+\t\treturn lockedLocation != null && sharedSlot != null && sharedSlot.isAlive();\n \t}\n \n \t/**", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java"}, {"additions": 143, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java", "sha": "aba6bfad5ec22a10e07b43d98ca532939c280e9d", "changes": 143, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.executiongraph;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.runtime.concurrent.ScheduledExecutor;\n+import org.apache.flink.runtime.executiongraph.restart.RestartCallback;\n+import org.apache.flink.runtime.executiongraph.restart.RestartStrategy;\n+import org.apache.flink.runtime.jobgraph.JobStatus;\n+import org.apache.flink.runtime.jobgraph.JobVertex;\n+import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n+import org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase;\n+import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.util.FlinkException;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.runtime.jobgraph.JobStatus.FINISHED;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Additional {@link ExecutionGraph} restart tests {@link ExecutionGraphRestartTest} which\n+ * require the usage of a {@link SlotProvider}.\n+ */\n+@RunWith(Parameterized.class)\n+public class ExecutionGraphCoLocationRestartTest extends SchedulerTestBase {\n+\n+\tprivate static final int NUM_TASKS = 31;\n+\n+\tpublic ExecutionGraphCoLocationRestartTest(SchedulerType schedulerType) {\n+\t\tsuper(schedulerType);\n+\t}\n+\n+\t@Test\n+\tpublic void testConstraintsAfterRestart() throws Exception {\n+\t\tfinal long timeout = 5000L;\n+\n+\t\t//setting up\n+\t\ttestingSlotProvider.addTaskManager(NUM_TASKS);\n+\n+\t\tJobVertex groupVertex = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\t\tJobVertex groupVertex2 = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\n+\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n+\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n+\n+\t\t//initiate and schedule job\n+\t\tfinal ExecutionGraph eg = ExecutionGraphTestUtils.createSimpleTestGraph(\n+\t\t\tnew JobID(),\n+\t\t\ttestingSlotProvider,\n+\t\t\tnew OneTimeDirectRestartStrategy(),\n+\t\t\tgroupVertex,\n+\t\t\tgroupVertex2);\n+\n+\t\tif (schedulerType == SchedulerType.SLOT_POOL) {\n+\t\t\t// enable the queued scheduling for the slot pool\n+\t\t\teg.setQueuedSchedulingAllowed(true);\n+\t\t}\n+\n+\t\tassertEquals(JobStatus.CREATED, eg.getState());\n+\n+\t\teg.scheduleForExecution();\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n+\n+\t\t//sanity checks\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new FlinkException(\"Test exception\"));\n+\n+\t\t// wait until we have restarted\n+\t\tExecutionGraphTestUtils.waitUntilJobStatus(eg, JobStatus.RUNNING, timeout);\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\t//checking execution vertex properties\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.finishAllVertices(eg);\n+\n+\t\tassertThat(eg.getState(), is(FINISHED));\n+\t}\n+\n+\tprivate void validateConstraints(ExecutionGraph eg) {\n+\n+\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n+\n+\t\tfor(int i = 0; i < NUM_TASKS; i++){\n+\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tassertThat(constr1.isAssigned(), is(true));\n+\t\t\tassertThat(constr1.getLocation(), equalTo(constr2.getLocation()));\n+\t\t}\n+\n+\t}\n+\n+\tprivate static final class OneTimeDirectRestartStrategy implements RestartStrategy {\n+\t\tprivate boolean hasRestarted = false;\n+\n+\t\t@Override\n+\t\tpublic boolean canRestart() {\n+\t\t\treturn !hasRestarted;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void restart(RestartCallback restarter, ScheduledExecutor executor) {\n+\t\t\thasRestarted = true;\n+\t\t\trestarter.triggerFullRecovery();\n+\t\t}\n+\t}\n+}", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java"}, {"additions": 18, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java", "sha": "9b98de7814377a76c3dfbab96da76b3ee3d4b2c7", "changes": 119, "status": "modified", "deletions": 101, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -41,19 +41,18 @@\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionType;\n import org.apache.flink.runtime.jobgraph.DistributionPattern;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.ScheduleMode;\n-import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n import org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n import org.apache.flink.runtime.testingUtils.TestingUtils;\n import org.apache.flink.runtime.testtasks.NoOpInvokable;\n@@ -73,6 +72,7 @@\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Consumer;\n \n@@ -125,62 +125,6 @@ public void testNoManualRestart() throws Exception {\n \t\tassertEquals(JobStatus.FAILED, eg.getState());\n \t}\n \n-\t@Test\n-\tpublic void testConstraintsAfterRestart() throws Exception {\n-\t\t\n-\t\t//setting up\n-\t\tInstance instance = ExecutionGraphTestUtils.getInstance(\n-\t\t\tnew ActorTaskManagerGateway(\n-\t\t\t\tnew SimpleActorGateway(TestingUtils.directExecutionContext())),\n-\t\t\tNUM_TASKS);\n-\t\t\n-\t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n-\t\tscheduler.newInstanceAvailable(instance);\n-\n-\t\tJobVertex groupVertex = newJobVertex(\"Task1\", NUM_TASKS, NoOpInvokable.class);\n-\t\tJobVertex groupVertex2 = newJobVertex(\"Task2\", NUM_TASKS, NoOpInvokable.class);\n-\n-\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n-\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n-\t\t\n-\t\t//initiate and schedule job\n-\t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", groupVertex, groupVertex2);\n-\t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 0L), scheduler);\n-\t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n-\n-\t\tassertEquals(JobStatus.CREATED, eg.getState());\n-\t\t\n-\t\teg.scheduleForExecution();\n-\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n-\t\t\n-\t\t//sanity checks\n-\t\tvalidateConstraints(eg);\n-\n-\t\t//restart automatically\n-\t\trestartAfterFailure(eg, new FiniteDuration(2, TimeUnit.MINUTES), false);\n-\t\t\n-\t\t//checking execution vertex properties\n-\t\tvalidateConstraints(eg);\n-\n-\t\thaltExecution(eg);\n-\t}\n-\n-\tprivate void validateConstraints(ExecutionGraph eg) {\n-\t\t\n-\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n-\t\t\n-\t\tfor(int i=0; i<NUM_TASKS; i++){\n-\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tassertNotNull(constr1.getSharedSlot());\n-\t\t\tassertTrue(constr1.isAssigned());\n-\t\t\tassertEquals(constr1, constr2);\n-\t\t}\n-\t\t\n-\t}\n-\n \t@Test\n \tpublic void testRestartAutomatically() throws Exception {\n \t\tRestartStrategy restartStrategy = new FixedDelayRestartStrategy(1, 1000);\n@@ -383,8 +327,8 @@ public void testFailingExecutionAfterRestart() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task1\", 1, NoOpInvokable.class);\n-\t\tJobVertex receiver = newJobVertex(\"Task2\", 1, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task1\", 1, NoOpInvokable.class);\n+\t\tJobVertex receiver = ExecutionGraphTestUtils.createJobVertex(\"Task2\", 1, NoOpInvokable.class);\n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender, receiver);\n \t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 1000), scheduler);\n \t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n@@ -447,7 +391,7 @@ public void testFailExecutionAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -493,7 +437,7 @@ public void testFailExecutionGraphAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -918,7 +862,7 @@ public void run() {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n \n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender);\n \n@@ -935,13 +879,6 @@ public void run() {\n \t\treturn new Tuple2<>(eg, instance);\n \t}\n \n-\tprivate static JobVertex newJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n-\t\tJobVertex groupVertex = new JobVertex(task1);\n-\t\tgroupVertex.setInvokableClass(invokable);\n-\t\tgroupVertex.setParallelism(numTasks);\n-\t\treturn groupVertex;\n-\t}\n-\n \tprivate static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy, Scheduler scheduler) throws IOException {\n \t\treturn new ExecutionGraph(\n \t\t\tTestingUtils.defaultExecutor(),\n@@ -955,8 +892,11 @@ private static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy,\n \t\t\tscheduler);\n \t}\n \n-\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException {\n-\t\tmakeAFailureAndWait(eg, timeout);\n+\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException, TimeoutException {\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new Exception(\"Test Exception\"));\n+\n+\t\t// Wait for async restart\n+\t\twaitForAsyncRestart(eg, timeout);\n \n \t\tassertEquals(JobStatus.RUNNING, eg.getState());\n \n@@ -973,32 +913,11 @@ private static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeou\n \t\t}\n \t}\n \n-\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws InterruptedException {\n-\t\tboolean success = false;\n-\n-\t\twhile (deadline.hasTimeLeft() && !success) {\n-\t\t\tsuccess = true;\n-\n-\t\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\t\tif (vertex.getCurrentExecutionAttempt().getAssignedResource() == null) {\n-\t\t\t\t\tsuccess = false;\n-\t\t\t\t\tThread.sleep(100);\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void makeAFailureAndWait(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n-\t\teg.getAllExecutionVertices().iterator().next().fail(new Exception(\"Test Exception\"));\n-\t\tassertEquals(JobStatus.FAILING, eg.getState());\n-\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n-\t\t}\n-\n-\t\t// Wait for async restart\n-\t\twaitForAsyncRestart(eg, timeout);\n+\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws TimeoutException {\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\tdeadline.timeLeft().toMillis());\n \t}\n \n \tprivate static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n@@ -1009,9 +928,7 @@ private static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeou\n \t}\n \n \tprivate static void haltExecution(ExecutionGraph eg) {\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().markFinished();\n-\t\t}\n+\t\tfinishAllVertices(eg);\n \n \t\tassertEquals(JobStatus.FINISHED, eg.getState());\n \t}", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java"}, {"additions": 70, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java", "sha": "9cfe90ec070763028624cfc4c5a9b184db00304c", "changes": 73, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.time.Deadline;\n import org.apache.flink.api.common.time.Time;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;\n@@ -41,16 +42,16 @@\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.instance.SimpleSlot;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.runtime.instance.SimpleSlotContext;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n-import org.apache.flink.runtime.instance.SimpleSlotContext;\n-import org.apache.flink.runtime.jobmaster.SlotOwner;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.SlotOwner;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.messages.Acknowledge;\n import org.apache.flink.runtime.messages.TaskMessages.CancelTask;\n import org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;\n@@ -65,11 +66,14 @@\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n+\n import java.lang.reflect.Field;\n import java.net.InetAddress;\n+import java.time.Duration;\n import java.util.List;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n \n import scala.concurrent.ExecutionContext;\n import scala.concurrent.ExecutionContext$;\n@@ -144,6 +148,53 @@ public static void waitUntilExecutionState(Execution execution, ExecutionState s\n \t\t}\n \t}\n \n+\t/**\n+\t * Waits until all executions fulfill the given predicate.\n+\t *\n+\t * @param executionGraph for which to check the executions\n+\t * @param executionPredicate predicate which is to be fulfilled\n+\t * @param maxWaitMillis timeout for the wait operation\n+\t * @throws TimeoutException if the executions did not reach the target state in time\n+\t */\n+\tpublic static void waitForAllExecutionsPredicate(\n+\t\t\tExecutionGraph executionGraph,\n+\t\t\tPredicate<Execution> executionPredicate,\n+\t\t\tlong maxWaitMillis) throws TimeoutException {\n+\t\tfinal Iterable<ExecutionVertex> allExecutionVertices = executionGraph.getAllExecutionVertices();\n+\n+\t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofMillis(maxWaitMillis));\n+\t\tboolean predicateResult;\n+\n+\t\tdo {\n+\t\t\tpredicateResult = true;\n+\t\t\tfor (ExecutionVertex executionVertex : allExecutionVertices) {\n+\t\t\t\tfinal Execution currentExecution = executionVertex.getCurrentExecutionAttempt();\n+\n+\t\t\t\tif (currentExecution == null || !executionPredicate.test(currentExecution)) {\n+\t\t\t\t\tpredicateResult = false;\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (!predicateResult) {\n+\t\t\t\ttry {\n+\t\t\t\t\tThread.sleep(2L);\n+\t\t\t\t} catch (InterruptedException ignored) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} while (!predicateResult && deadline.hasTimeLeft());\n+\n+\t\tif (!predicateResult) {\n+\t\t\tthrow new TimeoutException(\"Not all executions fulfilled the predicate in time.\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Predicate which is true if the given {@link Execution} has a resource assigned.\n+\t */\n+\tpublic static final Predicate<Execution> hasResourceAssigned = (Execution execution) -> execution.getAssignedResource() != null;\n+\n \tpublic static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus status, long maxWaitMillis)\n \t\t\tthrows TimeoutException {\n \n@@ -165,6 +216,15 @@ public static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus\n \t\t}\n \t}\n \n+\tpublic static void failExecutionGraph(ExecutionGraph executionGraph, Exception cause) {\n+\t\texecutionGraph.getAllExecutionVertices().iterator().next().fail(cause);\n+\t\tassertEquals(JobStatus.FAILING, executionGraph.getState());\n+\n+\t\tfor (ExecutionVertex vertex : executionGraph.getAllExecutionVertices()) {\n+\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n+\t\t}\n+\t}\n+\n \t/**\n \t * Takes all vertices in the given ExecutionGraph and switches their current\n \t * execution to RUNNING.\n@@ -382,6 +442,13 @@ public static Instance getInstance(final TaskManagerGateway gateway, final int n\n \t\treturn new Instance(gateway, connection, new InstanceID(), hardwareDescription, numberOfSlots);\n \t}\n \n+\tpublic static JobVertex createJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n+\t\tJobVertex groupVertex = new JobVertex(task1);\n+\t\tgroupVertex.setInvokableClass(invokable);\n+\t\tgroupVertex.setParallelism(numTasks);\n+\t\treturn groupVertex;\n+\t}\n+\n \t@SuppressWarnings(\"serial\")\n \tpublic static class SimpleActorGateway extends BaseTestingActorGateway {\n ", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java", "sha": "3d5441293bd5ae25470df4724a762b46be51c1cd", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -72,11 +72,11 @@\n \n \tprotected TestingSlotProvider testingSlotProvider;\n \n-\tprivate SchedulerType schedulerType;\n+\tprotected SchedulerType schedulerType;\n \n \tprivate RpcService rpcService;\n \n-\tenum SchedulerType {\n+\tpublic enum SchedulerType {\n \t\tSCHEDULER,\n \t\tSLOT_POOL\n \t}", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java"}, {"additions": 2, "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java", "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java", "sha": "3c074d1b6e0f4cef175d06c0baa8b9a457ffef3b", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40", "patch": "@@ -23,12 +23,11 @@\n import org.apache.flink.runtime.executiongraph.Execution;\n import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;\n import org.apache.flink.runtime.executiongraph.ExecutionVertex;\n-import org.apache.flink.runtime.instance.DummyActorGateway;\n+import org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n-import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n \n import java.net.InetAddress;\n@@ -75,7 +74,7 @@ public static Instance getRandomInstance(int numSlots) {\n \t\tHardwareDescription resources = new HardwareDescription(4, 4*GB, 3*GB, 2*GB);\n \t\t\n \t\treturn new Instance(\n-\t\t\tnew ActorTaskManagerGateway(DummyActorGateway.INSTANCE),\n+\t\t\tnew SimpleAckingTaskManagerGateway(),\n \t\t\tci,\n \t\t\tnew InstanceID(),\n \t\t\tresources,", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/4b19e272043907b70791bff8a85bd493e212947c", "parent": "https://github.com/apache/flink/commit/11fe3dc89f6b6b24fa21cc51d5e935e91634dbe5", "message": "[FLINK-6182] Fix possible NPE in SourceStreamTask\n\nThis closes #3606.", "bug_id": "flink_24", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java", "blob_url": "https://github.com/apache/flink/blob/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java", "sha": "18291408d0e35e73db8cdceb731870759b5f7c70", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java?ref=4b19e272043907b70791bff8a85bd493e212947c", "patch": "@@ -58,6 +58,8 @@ protected void run() throws Exception {\n \t\n \t@Override\n \tprotected void cancelTask() throws Exception {\n-\t\theadOperator.cancel();\n+\t\tif (headOperator != null) {\n+\t\t\theadOperator.cancel();\n+\t\t}\n \t}\n }", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/eece0dd05bc38b88fcb6cbcef15add7f98eab456", "parent": "https://github.com/apache/flink/commit/72e6b760fd951764c3ecc6fc191dc99a42d55e0b", "message": "[hotfix] [kafka] Fix NPE in Kafka09Fetcher", "bug_id": "flink_25", "file": [{"additions": 2, "raw_url": "https://github.com/apache/flink/raw/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java", "blob_url": "https://github.com/apache/flink/blob/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java", "sha": "ad7efa28014ffe6ac21c5d26f0b98b8fd2cb1ef5", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java?ref=eece0dd05bc38b88fcb6cbcef15add7f98eab456", "patch": "@@ -291,9 +291,9 @@ public void commitSpecificOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets)\n \t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n \t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n \t\t\t// This does not affect Flink's checkpoints/saved state.\n-\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition()) + 1;\n+\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition());\n \t\t\tif (offsetToCommit != null) {\n-\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit + 1));\n \t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n \t\t\t}\n \t\t}", "filename": "flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3", "parent": "https://github.com/apache/flink/commit/42328bd9b7f216e4c3aae2086b822b4a3a564970", "message": "[FLINK-6311] [kinesis] NPE in FlinkKinesisConsumer if source was closed before run\n\nThis closes #3738.", "bug_id": "flink_26", "file": [{"additions": 5, "raw_url": "https://github.com/apache/flink/raw/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java", "blob_url": "https://github.com/apache/flink/blob/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java", "sha": "8f7ca6c40f1e4871054374ae6b49797a665fdb15", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java?ref=a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3", "patch": "@@ -149,7 +149,7 @@\n \tprivate final KinesisProxyInterface kinesis;\n \n \t/** Thread that executed runFetcher() */\n-\tprivate Thread mainThread;\n+\tprivate volatile Thread mainThread;\n \n \t/**\n \t * The current number of shards that are actively read by this fetcher.\n@@ -408,7 +408,10 @@ public void runFetcher() throws Exception {\n \t */\n \tpublic void shutdownFetcher() {\n \t\trunning = false;\n-\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\n+\t\tif (mainThread != null) {\n+\t\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\t\t}\n \n \t\tif (LOG.isInfoEnabled()) {\n \t\t\tLOG.info(\"Shutting down the shard consumer threads of subtask {} ...\", indexOfThisConsumerSubtask);", "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/705938e5965a98b17bd6ba3f1e06728a35e4f8a9", "parent": "https://github.com/apache/flink/commit/c96002cef5f1867573a473746241f86b59aeddd2", "message": "[FLINK-6271] [jdbc] Fix NPE when there's a single split\n\nThis closes #3686.", "bug_id": "flink_27", "file": [{"additions": 31, "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java", "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java", "sha": "44201723a44b114452a7ee1dc5f6ad2bef50e12f", "changes": 52, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9", "patch": "@@ -17,48 +17,58 @@\n  */\n package org.apache.flink.api.java.io.jdbc.split;\n \n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n import java.io.Serializable;\n \n /** \n  * \n- * This query generator assumes that the query to parameterize contains a BETWEEN constraint on a numeric column.\n- * The generated query set will be of size equal to the configured fetchSize (apart the last one range),\n- * ranging from the min value up to the max.\n+ * This query parameters generator is an helper class to parameterize from/to queries on a numeric column.\n+ * The generated array of from/to values will be equally sized to fetchSize (apart from the last one),\n+ * ranging from minVal up to maxVal.\n  * \n  * For example, if there's a table <CODE>BOOKS</CODE> with a numeric PK <CODE>id</CODE>, using a query like:\n  * <PRE>\n  *   SELECT * FROM BOOKS WHERE id BETWEEN ? AND ?\n  * </PRE>\n  *\n- * you can use this class to automatically generate the parameters of the BETWEEN clause,\n+ * you can take advantage of this class to automatically generate the parameters of the BETWEEN clause,\n  * based on the passed constructor parameters.\n  * \n  * */\n public class NumericBetweenParametersProvider implements ParameterValuesProvider {\n \n-\tprivate long fetchSize;\n-\tprivate final long min;\n-\tprivate final long max;\n+\tprivate final long fetchSize;\n+\tprivate final long minVal;\n+\tprivate final long maxVal;\n \t\n-\tpublic NumericBetweenParametersProvider(long fetchSize, long min, long max) {\n+\t/**\n+\t * NumericBetweenParametersProvider constructor.\n+\t * \n+\t * @param fetchSize the max distance between the produced from/to pairs\n+\t * @param minVal the lower bound of the produced \"from\" values\n+\t * @param maxVal the upper bound of the produced \"to\" values\n+\t */\n+\tpublic NumericBetweenParametersProvider(long fetchSize, long minVal, long maxVal) {\n+\t\tcheckArgument(fetchSize > 0, \"Fetch size must be greater than 0.\");\n+\t\tcheckArgument(minVal <= maxVal, \"Min value cannot be greater than max value.\");\n \t\tthis.fetchSize = fetchSize;\n-\t\tthis.min = min;\n-\t\tthis.max = max;\n+\t\tthis.minVal = minVal;\n+\t\tthis.maxVal = maxVal;\n \t}\n \n \t@Override\n-\tpublic Serializable[][] getParameterValues(){\n-\t\tdouble maxElemCount = (max - min) + 1;\n-\t\tint size = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n-\t\tSerializable[][] parameters = new Serializable[size][2];\n-\t\tint count = 0;\n-\t\tfor (long i = min; i < max; i += fetchSize, count++) {\n-\t\t\tlong currentLimit = i + fetchSize - 1;\n-\t\t\tparameters[count] = new Long[]{i,currentLimit};\n-\t\t\tif (currentLimit + 1 + fetchSize > max) {\n-\t\t\t\tparameters[count + 1] = new Long[]{currentLimit + 1, max};\n-\t\t\t\tbreak;\n+\tpublic Serializable[][] getParameterValues() {\n+\t\tdouble maxElemCount = (maxVal - minVal) + 1;\n+\t\tint numBatches = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n+\t\tSerializable[][] parameters = new Serializable[numBatches][2];\n+\t\tint batchIndex = 0;\n+\t\tfor (long start = minVal; start <= maxVal; start += fetchSize, batchIndex++) {\n+\t\t\tlong end = start + fetchSize - 1;\n+\t\t\tif (end > maxVal) {\n+\t\t\t\tend = maxVal;\n \t\t\t}\n+\t\t\tparameters[batchIndex] = new Long[]{start, end};\n \t\t}\n \t\treturn parameters;\n \t}", "filename": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java"}, {"additions": 107, "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java", "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java", "sha": "bee3d251a986f81ffa2f0ba582f8ca08677daf65", "changes": 127, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9", "patch": "@@ -18,19 +18,19 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import java.io.IOException;\n-import java.io.Serializable;\n-import java.sql.ResultSet;\n-\n import org.apache.flink.api.java.io.jdbc.split.GenericParameterValuesProvider;\n import org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider;\n import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;\n-import org.apache.flink.types.Row;\n import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.types.Row;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.sql.ResultSet;\n+\n public class JDBCInputFormatTest extends JDBCTestBase {\n \n \tprivate JDBCInputFormat jdbcInputFormat;\n@@ -116,11 +116,21 @@ public void testJDBCInputFormatWithoutParallelism() throws IOException, Instanti\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\t\n-\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\tif (next.getField(0) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(1) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(2) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(3) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(4) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t}\n \n \t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -162,11 +172,78 @@ public void testJDBCInputFormatWithParallelismAndNumericColumnSplitting() throws\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n+\n+\t\t\t\tfor (int x = 0; x < 5; x++) {\n+\t\t\t\t\tif(testData[recordCount][x]!=null) {\n+\t\t\t\t\t\tAssert.assertEquals(testData[recordCount][x], next.getField(x));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\trecordCount++;\n+\t\t\t}\n+\t\t\tjdbcInputFormat.close();\n+\t\t}\n+\t\tjdbcInputFormat.closeInputFormat();\n+\t\tAssert.assertEquals(testData.length, recordCount);\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCInputFormatWithoutParallelismAndNumericColumnSplitting() throws IOException, InstantiationException, IllegalAccessException {\n+\t\tfinal Long min = new Long(JDBCTestBase.testData[0][0] + \"\");\n+\t\tfinal Long max = new Long(JDBCTestBase.testData[JDBCTestBase.testData.length - 1][0] + \"\");\n+\t\tfinal long fetchSize = max + 1;//generate a single split\n+\t\tParameterValuesProvider pramProvider = new NumericBetweenParametersProvider(fetchSize, min, max);\n+\t\tjdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n+\t\t\t\t.setDrivername(DRIVER_CLASS)\n+\t\t\t\t.setDBUrl(DB_URL)\n+\t\t\t\t.setQuery(JDBCTestBase.SELECT_ALL_BOOKS_SPLIT_BY_ID)\n+\t\t\t\t.setRowTypeInfo(rowTypeInfo)\n+\t\t\t\t.setParametersProvider(pramProvider)\n+\t\t\t\t.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)\n+\t\t\t\t.finish();\n+\n+\t\tjdbcInputFormat.openInputFormat();\n+\t\tInputSplit[] splits = jdbcInputFormat.createInputSplits(1);\n+\t\t//assert that a single split was generated\n+\t\tAssert.assertEquals(1, splits.length);\n+\t\tint recordCount = 0;\n+\t\tRow row =  new Row(5);\n+\t\tfor (int i = 0; i < splits.length; i++) {\n+\t\t\tjdbcInputFormat.open(splits[i]);\n+\t\t\twhile (!jdbcInputFormat.reachedEnd()) {\n+\t\t\t\tRow next = jdbcInputFormat.nextRecord(row);\n+\t\t\t\tif (next == null) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -208,11 +285,21 @@ public void testJDBCInputFormatWithParallelismAndGenericSplitting() throws IOExc\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\trecordCount++;\n \t\t\t}", "filename": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/47db9cb1a867870a8da0b403e0ec217ac461ba04", "parent": "https://github.com/apache/flink/commit/dc5dd5106738e393761a62a56d9e684c722c516f", "message": "[FLINK-5147] Prevent NPE in LocalFS#delete()\n\nThis closes #2859.", "bug_id": "flink_28", "file": [{"additions": 7, "raw_url": "https://github.com/apache/flink/raw/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java", "blob_url": "https://github.com/apache/flink/blob/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java", "sha": "7ad68b35d7a5dedc5db6a8dcf212146c233fc481", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java?ref=47db9cb1a867870a8da0b403e0ec217ac461ba04", "patch": "@@ -184,8 +184,13 @@ public boolean delete(final Path f, final boolean recursive) throws IOException\n \t\tfinal File file = pathToFile(f);\n \t\tif (file.isFile()) {\n \t\t\treturn file.delete();\n-\t\t} else if ((!recursive) && file.isDirectory() && (file.listFiles().length != 0)) {\n-\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t} else if ((!recursive) && file.isDirectory()) {\n+\t\t\tFile[] containedFiles = file.listFiles();\n+\t\t\tif (containedFiles == null) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" does not exist or an I/O error occurred\");\n+\t\t\t} else if (containedFiles.length != 0) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t\t}\n \t\t}\n \n \t\treturn delete(file);", "filename": "flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/7ee0425a3467f9cf8d1dc967c99b5bb6c29c1745", "parent": "https://github.com/apache/flink/commit/7206b0ed2adb10c94e1ffd3dbe851250b44edcf4", "message": "[hotfix] Fix NPE in NetworkStackThroughputITCase", "bug_id": "flink_29", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/7ee0425a3467f9cf8d1dc967c99b5bb6c29c1745/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java", "blob_url": "https://github.com/apache/flink/blob/7ee0425a3467f9cf8d1dc967c99b5bb6c29c1745/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java", "sha": "3d0e5ab46ab0acdc8fdc68f1af8a3f9cc4632711", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java?ref=7ee0425a3467f9cf8d1dc967c99b5bb6c29c1745", "patch": "@@ -287,9 +287,12 @@ public void testThroughput() throws Exception {\n \t\t\tconfig.setInteger(NUM_SLOTS_PER_TM_CONFIG_KEY, (Integer) p[5]);\n \n \t\t\tTestBaseWrapper test = new TestBaseWrapper(config);\n+\t\t\ttest.startCluster();\n \n \t\t\tSystem.out.println(Arrays.toString(p));\n \t\t\ttest.testProgram();\n+\n+\t\t\ttest.stopCluster();\n \t\t}\n \t}\n ", "filename": "flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/884d3e2a4a894c79676a1b2cf0b1bd075550bbbd", "parent": "https://github.com/apache/flink/commit/9bcbcf4a5d75de65671bf4cc4e1e3129d386a29b", "message": "[FLINK-4258] fix potential NPE in SavepointCoordinator", "bug_id": "flink_30", "file": [{"additions": 2, "raw_url": "https://github.com/apache/flink/raw/884d3e2a4a894c79676a1b2cf0b1bd075550bbbd/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointCoordinator.java", "blob_url": "https://github.com/apache/flink/blob/884d3e2a4a894c79676a1b2cf0b1bd075550bbbd/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointCoordinator.java", "sha": "cd9eb0cbb1bd1c356b691ee126c4782922adfa99", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointCoordinator.java?ref=884d3e2a4a894c79676a1b2cf0b1bd075550bbbd", "patch": "@@ -200,8 +200,6 @@ public void restoreSavepoint(\n \n \t\t\tSavepoint savepoint = savepointStore.loadSavepoint(savepointPath);\n \n-\t\t\tlong recoveryTimestamp = System.currentTimeMillis();\n-\n \t\t\tfor (TaskState taskState : savepoint.getTaskStates()) {\n \t\t\t\tExecutionJobVertex executionJobVertex = tasks.get(taskState.getJobVertexID());\n \n@@ -292,8 +290,9 @@ protected void onFullyAcknowledgedCheckpoint(CompletedCheckpoint checkpoint) {\n \t\tPromise<String> promise = savepointPromises.remove(checkpoint.getCheckpointID());\n \n \t\tif (promise == null) {\n-\t\t\tLOG.info(\"Pending savepoint with ID \" + checkpoint.getCheckpointID() + \"  has been \" +\n+\t\t\tLOG.warn(\"Pending savepoint with ID \" + checkpoint.getCheckpointID() + \"  has been \" +\n \t\t\t\t\t\"removed before receiving acknowledgment.\");\n+\t\t\treturn;\n \t\t}\n \n \t\t// Sanity check", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointCoordinator.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/3ce8596b43f88b2b6d51dab687ab224a43b825fb", "parent": "https://github.com/apache/flink/commit/211f5db9d764efe5318867993f1b33f4eee48117", "message": "[FLINK-4631] Prevent NPE in OneInputStreamTask\n\nThis closes #2709.", "bug_id": "flink_31", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java", "blob_url": "https://github.com/apache/flink/blob/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java", "sha": "0f8f4a4cf43914ca7a894f6793bbea1abe81737b", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java?ref=3ce8596b43f88b2b6d51dab687ab224a43b825fb", "patch": "@@ -69,7 +69,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/ad3454069fa091cd453f6cefb0e56a8021a3c269", "parent": "https://github.com/apache/flink/commit/5ccd9071580e196d150905b2d05eef71e399a24c", "message": "[FLINK-4309] Fix potential NPE in DelegatingConfiguration\n\nThis closes #2371.", "bug_id": "flink_32", "file": [{"additions": 9, "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java", "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java", "sha": "dba77f372326db7477c6c656e9aa2645d7fe0929", "changes": 12, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269", "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.flink.core.memory.DataInputView;\n import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.util.Preconditions;\n \n import java.io.IOException;\n import java.util.HashSet;\n@@ -56,7 +57,7 @@ public DelegatingConfiguration() {\n \t */\n \tpublic DelegatingConfiguration(Configuration backingConfig, String prefix)\n \t{\n-\t\tthis.backingConfig = backingConfig;\n+\t\tthis.backingConfig = Preconditions.checkNotNull(backingConfig);\n \t\tthis.prefix = prefix;\n \t}\n \n@@ -178,14 +179,19 @@ public String toString() {\n \n \t@Override\n \tpublic Set<String> keySet() {\n+\t\tif (this.prefix == null) {\n+\t\t\treturn this.backingConfig.keySet();\n+\t\t}\n+\n \t\tfinal HashSet<String> set = new HashSet<String>();\n-\t\tfinal int prefixLen = this.prefix == null ? 0 : this.prefix.length();\n+\t\tint prefixLen = this.prefix.length();\n \n \t\tfor (String key : this.backingConfig.keySet()) {\n-\t\t\tif (key.startsWith(this.prefix)) {\n+\t\t\tif (key.startsWith(prefix)) {\n \t\t\t\tset.add(key.substring(prefixLen));\n \t\t\t}\n \t\t}\n+\n \t\treturn set;\n \t}\n ", "filename": "flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java"}, {"additions": 45, "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java", "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java", "sha": "d8b782d4dbaf1c0d2d523deba0f1b73025bf0e27", "changes": 45, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269", "patch": "@@ -26,8 +26,10 @@\n import java.lang.reflect.Modifier;\n import java.util.Arrays;\n import java.util.Comparator;\n+import java.util.Set;\n \n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n \n \n public class DelegatingConfigurationTest {\n@@ -88,4 +90,47 @@ private String typeParamToString(Class<?>[] classes) {\n \t\t\tassertTrue(\"Foo method '\" + configurationMethod.getName() + \"' has not been wrapped correctly in DelegatingConfiguration wrapper\", hasMethod);\n \t\t}\n \t}\n+\t\n+\t@Test\n+\tpublic void testDelegationConfigurationWithNullPrefix() {\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(\n+\t\t\t\tbackingConf, null);\n+\t\tSet<String> keySet = configuration.keySet();\n+\n+\t\tassertEquals(keySet, backingConf.keySet());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void testDelegationConfigurationWithPrefix() {\n+\t\tString prefix = \"pref-\";\n+\t\tString expectedKey = \"key\";\n+\n+\t\t/*\n+\t\t * Key matches the prefix\n+\t\t */\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(prefix + expectedKey, \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tSet<String> keySet = configuration.keySet();\n+\t\t\n+\n+\t\tassertEquals(keySet.size(), 1);\n+\t\tassertEquals(keySet.iterator().next(), expectedKey);\n+\n+\t\t/*\n+\t\t * Key does not match the prefix\n+\t\t */\n+\t\tbackingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tconfiguration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tkeySet = configuration.keySet();\n+\n+\t\tassertTrue(keySet.isEmpty());\n+\t}\n }", "filename": "flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/3a89ab2ed130a2f75245efae20ec4b925a2b7c19", "parent": "https://github.com/apache/flink/commit/818cccd3d447ccc4235840fd8bd9a35486ed4f49", "message": "[hotfix] [hbase] Set root log level to OFF for flink-hbase tests.\n\nLog level is changed due to a buggy Calcite check that causes a NPE.\nThe check is only performed if log level DEBUG is enabled.\n\nThis closes #4771", "bug_id": "flink_33", "file": [{"additions": 5, "raw_url": "https://github.com/apache/flink/raw/3a89ab2ed130a2f75245efae20ec4b925a2b7c19/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties", "blob_url": "https://github.com/apache/flink/blob/3a89ab2ed130a2f75245efae20ec4b925a2b7c19/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties", "sha": "d4d059b5cbf1fba251d7c53e0ebe76a0b6feda7d", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties?ref=3a89ab2ed130a2f75245efae20ec4b925a2b7c19", "patch": "@@ -15,9 +15,12 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-log4j.rootLogger=DEBUG, stdout\n+# NOTE: The current Calcite version has a bug that causes execution to fail if the\n+#       log level is set to DEBUG!\n+\n+log4j.rootLogger=OFF, stdout\n log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n log4j.appender.stdout.Target=System.out\n-log4j.appender.stdout.threshold=INFO\n+log4j.appender.stdout.threshold=OFF\n log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n log4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %-5p %30c{1}:%4L - %m%n", "filename": "flink-connectors/flink-hbase/src/test/resources/log4j-test.properties"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/0fb6e0dde3fbfea666c6bfe8d6f738bfd80d3af0", "parent": "https://github.com/apache/flink/commit/9692c836884b8f7e3436aacc2d47627dc93a3c97", "message": "[FLINK-3434] Prevent NPE in ClientFrontend#getClient()\n\nThis closes #1695", "bug_id": "flink_34", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/0fb6e0dde3fbfea666c6bfe8d6f738bfd80d3af0/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnClientBase.java", "blob_url": "https://github.com/apache/flink/blob/0fb6e0dde3fbfea666c6bfe8d6f738bfd80d3af0/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnClientBase.java", "sha": "4e4a2f512508f7b46918ba04a17763047cd12c35", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnClientBase.java?ref=0fb6e0dde3fbfea666c6bfe8d6f738bfd80d3af0", "patch": "@@ -367,9 +367,8 @@ protected AbstractFlinkYarnCluster deployInternal() throws Exception {\n \t\ttry {\n \t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n \t\t} catch (IOException e) {\n-\t\t\tLOG.error(\"Error while setting the default \" +\n+\t\t\tthrow new IOException(\"Error while setting the default \" +\n \t\t\t\t\"filesystem scheme from configuration.\", e);\n-\t\t\treturn null;\n \t\t}\n \t\t// ------------------ Check if the specified queue exists --------------\n ", "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/FlinkYarnClientBase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/13150a4ba26127b9ee2035fd3509b57bc3f7aa61", "parent": "https://github.com/apache/flink/commit/4410c04a68c7b247bb3d7113e5f40f2a9c2165af", "message": "[FLINK-4631] Prevent NPE in TwoInputStreamTask\n\nCheck that the input processor has been created before cleaning it up.", "bug_id": "flink_35", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java", "blob_url": "https://github.com/apache/flink/blob/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java", "sha": "233e9f10db0c809213cafdedac435b7c84af65ef", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java?ref=13150a4ba26127b9ee2035fd3509b57bc3f7aa61", "patch": "@@ -91,7 +91,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b", "parent": "https://github.com/apache/flink/commit/41ae13122bd1ca16f1c6779983dc0d17e3633e97", "message": "[FLINK-9057][network] fix an NPE when cleaning up before requesting a subpartition view\n\nIn PartitionRequestServerHandler, the view reader was created and immediately\nafterwards added to the PartitionRequestQueue which would attempt a cleanup of\nthe view reader's subpartition view. This view, however, was currently only\ncreated after adding the reader to the PartitionRequestQueue and may thus result\nin a NullPointerException if the cleanup happens very early in the\ninitialization phase, e.g. due to failures.\n\nThis closes #5747.", "bug_id": "flink_36", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java", "blob_url": "https://github.com/apache/flink/blob/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java", "sha": "e9ee10cbc4d953248955811899711ba3bfd12208", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java?ref=f486a3fd6ed80b67e8eeed9245ad37b6b0be740b", "patch": "@@ -20,9 +20,9 @@\n \n import org.apache.flink.runtime.io.network.NetworkSequenceViewReader;\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n+import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CancelPartitionRequest;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CloseRequest;\n-import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionProvider;\n import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;\n@@ -99,12 +99,12 @@ protected void channelRead0(ChannelHandlerContext ctx, NettyMessage msg) throws\n \t\t\t\t\t\t\toutboundQueue);\n \t\t\t\t\t}\n \n-\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n-\n \t\t\t\t\treader.requestSubpartitionView(\n \t\t\t\t\t\tpartitionProvider,\n \t\t\t\t\t\trequest.partitionId,\n \t\t\t\t\t\trequest.queueIndex);\n+\n+\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n \t\t\t\t} catch (PartitionNotFoundException notFound) {\n \t\t\t\t\trespondWithError(ctx, notFound, request.receiverId);\n \t\t\t\t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/924830ffa01d1fa1ba73acde32aa65d4a8e4dbac", "parent": "https://github.com/apache/flink/commit/3ef4e68bf8e7367c81b420d919e2172aebbcd507", "message": "[runtime] Fix possible NPE during cancelling", "bug_id": "flink_37", "file": [{"additions": 19, "raw_url": "https://github.com/apache/flink/raw/924830ffa01d1fa1ba73acde32aa65d4a8e4dbac/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "blob_url": "https://github.com/apache/flink/blob/924830ffa01d1fa1ba73acde32aa65d4a8e4dbac/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "sha": "5964b49f7bd4450768e6259b6014be5cd437e9be", "changes": 26, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java?ref=924830ffa01d1fa1ba73acde32aa65d4a8e4dbac", "patch": "@@ -45,7 +45,6 @@\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n \n-import static com.google.common.base.Preconditions.checkNotNull;\n import static com.google.common.base.Preconditions.checkState;\n \n class PartitionRequestClientHandler extends ChannelInboundHandlerAdapter {\n@@ -377,15 +376,28 @@ public void onEvent(Buffer buffer) {\n \t\t\tboolean success = false;\n \n \t\t\ttry {\n-\t\t\t\tcheckNotNull(buffer, \"Buffer request could not be satisfied.\");\n+\t\t\t\tif (buffer != null) {\n+\t\t\t\t\tif (availableBuffer.compareAndSet(null, buffer)) {\n+\t\t\t\t\t\tctx.channel().eventLoop().execute(this);\n \n-\t\t\t\tif (availableBuffer.compareAndSet(null, buffer)) {\n-\t\t\t\t\tctx.channel().eventLoop().execute(this);\n-\n-\t\t\t\t\tsuccess = true;\n+\t\t\t\t\t\tsuccess = true;\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tthrow new IllegalStateException(\"Received a buffer notification, \" +\n+\t\t\t\t\t\t\t\t\" but the previous one has not been handled yet.\");\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n-\t\t\t\t\tthrow new IllegalStateException(\"Received a buffer notification, but the previous one has not been handled yet.\");\n+\t\t\t\t\t// The buffer pool has been destroyed\n+\t\t\t\t\tstagedBufferResponse = null;\n+\n+\t\t\t\t\tif (stagedMessages.isEmpty()) {\n+\t\t\t\t\t\tctx.channel().config().setAutoRead(true);\n+\t\t\t\t\t\tctx.channel().read();\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tctx.channel().eventLoop().execute(stagedMessagesHandler);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (Throwable t) {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/ac9a911723d0d81ee9c4d8231ee81bbbafff2575", "parent": "https://github.com/apache/flink/commit/d5a90279fec810965da1a06bf7c90e7123c2719b", "message": "[FLINK-2599] [test-stability] Fix possible NPE in SlotCountExceedingParallelismTest", "bug_id": "flink_38", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/ac9a911723d0d81ee9c4d8231ee81bbbafff2575/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java", "blob_url": "https://github.com/apache/flink/blob/ac9a911723d0d81ee9c4d8231ee81bbbafff2575/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java", "sha": "2783b6cc0abfa4dec74dfbee3e4e745bc3483e54", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java?ref=ac9a911723d0d81ee9c4d8231ee81bbbafff2575", "patch": "@@ -54,7 +54,9 @@ public static void setUp() throws Exception {\n \n \t@AfterClass\n \tpublic static void tearDown() throws Exception {\n-\t\tflink.stop();\n+\t\tif (flink != null) {\n+\t\t\tflink.stop();\n+\t\t}\n \t}\n \n \t@Test", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7", "parent": "https://github.com/apache/flink/commit/cf6b3fb22c67a540dbfa96497d66041ba95ad358", "message": "[FLINK-5643] Fix NPE in StateUtil\n\nIntroduces a null check to deal with state futures which have a null value.\n\nThis closes #3212.", "bug_id": "flink_39", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java", "blob_url": "https://github.com/apache/flink/blob/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java", "sha": "c6f5c8698f3d59c8b80a1e88fcac1b2a076da25d", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java?ref=cf9f4c77c8e785567f036cc2fbb97c0cd16979a7", "patch": "@@ -78,7 +78,9 @@ public static void discardStateFuture(RunnableFuture<? extends StateObject> stat\n \t\t\tif (!stateFuture.cancel(true)) {\n \t\t\t\tStateObject stateObject = FutureUtil.runIfNotDoneAndGet(stateFuture);\n \n-\t\t\t\tstateObject.discardState();\n+\t\t\t\tif (null != stateObject) {\n+\t\t\t\t\tstateObject.discardState();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java"}, {"additions": 36, "raw_url": "https://github.com/apache/flink/raw/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java", "blob_url": "https://github.com/apache/flink/blob/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java", "sha": "e59d027e87befe7fe8d87a7d66f958b9a8ebd9b4", "changes": 36, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java?ref=cf9f4c77c8e785567f036cc2fbb97c0cd16979a7", "patch": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.state;\n+\n+import org.apache.flink.util.TestLogger;\n+import org.junit.Test;\n+\n+import java.util.concurrent.RunnableFuture;\n+\n+public class StateUtilTest extends TestLogger {\n+\n+\t/**\n+\t * Tests that {@link StateUtil#discardStateFuture} can handle state futures with null value.\n+\t */\n+\t@Test\n+\tpublic void testDiscardRunnableFutureWithNullValue() throws Exception {\n+\t\tRunnableFuture<StateHandle<?>> stateFuture = new DoneFuture<>(null);\n+\t\tStateUtil.discardStateFuture(stateFuture);\n+\t}\n+}", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/f6d147d91252cc175737dd52feb168204f84fc1e", "parent": "https://github.com/apache/flink/commit/0970212cf0ee49c6031afedb89dff8d75e68a347", "message": "Fixed NPE in OperatorFactory", "bug_id": "flink_40", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/f6d147d91252cc175737dd52feb168204f84fc1e/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/OperatorFactory.java", "blob_url": "https://github.com/apache/flink/blob/f6d147d91252cc175737dd52feb168204f84fc1e/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/OperatorFactory.java", "sha": "1045c508d7e5c342a943557bfeb180317bdb9eb1", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/OperatorFactory.java?ref=f6d147d91252cc175737dd52feb168204f84fc1e", "patch": "@@ -70,7 +70,7 @@ public String choose(final String[] nouns, final String[] verbs, final String[]\n \t\t}\n \n \t\tprivate String firstOrNull(final String[] names) {\n-\t\t\treturn names.length == 0 ? null : names[0];\n+\t\t\treturn names == null || names.length == 0 ? null : names[0];\n \t\t}\n \t}\n ", "filename": "sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/OperatorFactory.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39", "parent": "https://github.com/apache/flink/commit/ae446388b91ecc0f08887da19400395b96b32f6c", "message": "[runtime] [tests] Fix possible NPE and add Netty serialization test", "bug_id": "flink_41", "file": [{"additions": 8, "raw_url": "https://github.com/apache/flink/raw/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java", "blob_url": "https://github.com/apache/flink/blob/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java", "sha": "d0840b07ad843a51e439baac0d449dff2d8512ce", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java?ref=e0d1fd504d6313b1b02a06ef7935cf7fd7069a39", "patch": "@@ -147,8 +147,11 @@ else if (msgId == TaskEventRequest.ID) {\n \t\t\telse if (msgId == ErrorResponse.ID) {\n \t\t\t\tdecodedMsg = new ErrorResponse();\n \t\t\t}\n+\t\t\telse if (msgId == CancelPartitionRequest.ID) {\n+\t\t\t\tdecodedMsg = new CancelPartitionRequest();\n+\t\t\t}\n \t\t\telse {\n-\t\t\t\tthrow new IllegalStateException(\"Received unknown message from producer: \" + decodedMsg.getClass());\n+\t\t\t\tthrow new IllegalStateException(\"Received unknown message from producer: \" + msg);\n \t\t\t}\n \n \t\t\tif (decodedMsg != null) {\n@@ -486,6 +489,9 @@ public void readFrom(ByteBuf buffer) {\n \n \t\tInputChannelID receiverId;\n \n+\t\tpublic CancelPartitionRequest() {\n+\t\t}\n+\n \t\tpublic CancelPartitionRequest(InputChannelID receiverId) {\n \t\t\tthis.receiverId = receiverId;\n \t\t}\n@@ -495,7 +501,7 @@ ByteBuf write(ByteBufAllocator allocator) throws Exception {\n \t\t\tByteBuf result = null;\n \n \t\t\ttry {\n-\t\t\t\tresult = allocateBuffer(allocator, ID);\n+\t\t\t\tresult = allocateBuffer(allocator, ID, 16);\n \t\t\t\treceiverId.writeTo(result);\n \t\t\t}\n \t\t\tcatch (Throwable t) {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java"}, {"additions": 7, "raw_url": "https://github.com/apache/flink/raw/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java", "blob_url": "https://github.com/apache/flink/blob/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java", "sha": "b464692032647a0a3bf6747626a0352c5124eecd", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java?ref=e0d1fd504d6313b1b02a06ef7935cf7fd7069a39", "patch": "@@ -140,6 +140,13 @@ public void testEncodeDecode() {\n \t\t\tassertEquals(expected.partitionId, actual.partitionId);\n \t\t\tassertEquals(expected.receiverId, actual.receiverId);\n \t\t}\n+\n+\t\t{\n+\t\t\tNettyMessage.CancelPartitionRequest expected = new NettyMessage.CancelPartitionRequest(new InputChannelID());\n+\t\t\tNettyMessage.CancelPartitionRequest actual = encodeAndDecode(expected);\n+\n+\t\t\tassertEquals(expected.receiverId, actual.receiverId);\n+\t\t}\n \t}\n \n \t@SuppressWarnings(\"unchecked\")", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/a1d2df6149491fe9a087882f88c87e09476bb429", "parent": "https://github.com/apache/flink/commit/48e2cb5e8be7c4f305b947fb25ea7d312844e032", "message": "[FLINK-1957] Fix NPE during cancelling of partition request", "bug_id": "flink_42", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/a1d2df6149491fe9a087882f88c87e09476bb429/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "blob_url": "https://github.com/apache/flink/blob/a1d2df6149491fe9a087882f88c87e09476bb429/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "sha": "f1dba423e6f3249feba59dccb92c9b0973f29524", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java?ref=a1d2df6149491fe9a087882f88c87e09476bb429", "patch": "@@ -429,7 +429,7 @@ public void run() {\n \t\t\t\t\tsuccess = true;\n \t\t\t\t}\n \t\t\t\telse {\n-\t\t\t\t\tcancelRequestFor(inputChannel.getInputChannelId());\n+\t\t\t\t\tcancelRequestFor(stagedBufferResponse.receiverId);\n \t\t\t\t}\n \n \t\t\t\tstagedBufferResponse = null;", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/fd95d9743b929d246f61e9bd7aa769dd9469ed00", "parent": "https://github.com/apache/flink/commit/b5f2a3fe74d0b077cd08c9093c7a3374a0c4941b", "message": "Fixed NPE in pact testing", "bug_id": "flink_43", "file": [{"additions": 2, "raw_url": "https://github.com/apache/flink/raw/fd95d9743b929d246f61e9bd7aa769dd9469ed00/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/AbstractValueMatcher.java", "blob_url": "https://github.com/apache/flink/blob/fd95d9743b929d246f61e9bd7aa769dd9469ed00/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/AbstractValueMatcher.java", "sha": "09aac4bf361a3d0dce5dd4948843e63a887aa769", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/AbstractValueMatcher.java?ref=fd95d9743b929d246f61e9bd7aa769dd9469ed00", "patch": "@@ -20,6 +20,7 @@\n \n import eu.stratosphere.pact.common.type.PactRecord;\n import eu.stratosphere.pact.common.type.Value;\n+import eu.stratosphere.pact.testing.Equaler.SafeEquals;\n \n /**\n  * Simple matching algorithm that returns unmatched values but allows a value from one bag to be matched several times\n@@ -42,7 +43,7 @@ protected double getDistance(Class<? extends Value>[] schema, Int2ObjectMap<List\n \n \t\t\tList<ValueSimilarity<?>> sims = similarities.get(index);\n \t\t\tif (sims == null) {\n-\t\t\t\tif (!actual.equals(expected))\n+\t\t\t\tif (!SafeEquals.SafeEquals.equal(actual, expected) )\n \t\t\t\t\treturn ValueSimilarity.NO_MATCH;\n \t\t\t\tcontinue;\n \t\t\t}", "filename": "pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/AbstractValueMatcher.java"}, {"additions": 5, "raw_url": "https://github.com/apache/flink/raw/fd95d9743b929d246f61e9bd7aa769dd9469ed00/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactList.java", "blob_url": "https://github.com/apache/flink/blob/fd95d9743b929d246f61e9bd7aa769dd9469ed00/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactList.java", "sha": "0c69afcc7d22965b7fa6a3a8788cd5a8e4b4edce", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactList.java?ref=fd95d9743b929d246f61e9bd7aa769dd9469ed00", "patch": "@@ -201,7 +201,11 @@ public void clear() {\n \tpublic boolean contains(final Object o) {\n \t\treturn this.list.contains(o);\n \t}\n-\n+\t\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn this.list.toString();\n+\t}\n \t/*\n \t * (non-Javadoc)\n \t * @see java.util.List#containsAll(java.util.Collection)", "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactList.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5", "parent": "https://github.com/apache/flink/commit/0c37ffe118c97a2e80029e1ee9e91f4aa2064fdd", "message": "Fixed NPE for TestRecords sorting", "bug_id": "flink_44", "file": [{"additions": 19, "raw_url": "https://github.com/apache/flink/raw/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java", "blob_url": "https://github.com/apache/flink/blob/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java", "sha": "b464b5ba447c1a03e0b722a4e1371ec8d164227a", "changes": 27, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java?ref=d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5", "patch": "@@ -388,8 +388,7 @@ public void assertEquals(final TestRecords expectedValues, FuzzyValueMatcher fuz\n \n \t\t\t// initialize with null\n \t\t\tList<Key> currentKeys = new ArrayList<Key>(Arrays.asList(new Key[sortInfo.sortKeys.size()])), nextKeys =\n-\t\t\t\tnew ArrayList<Key>(\n-\t\t\t\t\tcurrentKeys);\n+\t\t\t\tnew ArrayList<Key>(currentKeys);\n \t\t\tint itemIndex = 0;\n \t\t\tList<PactRecord> expectedValuesWithCurrentKey = new ArrayList<PactRecord>();\n \t\t\tList<PactRecord> actualValuesWithCurrentKey = new ArrayList<PactRecord>();\n@@ -497,24 +496,30 @@ private void matchValues(final Iterator<PactRecord> actualIterator, List<Key> cu\n \n \t\tif (actualValuesWithCurrentKey.isEmpty())\n \t\t\tthrow new ArrayComparisonFailure(\"Unexpected value for key \" + currentKeys, new AssertionFailedError(\n-\t\t\t\tAssert.format(\" \", expectedValuesWithCurrentKey, actualRecord)), itemIndex\n+\t\t\t\tAssert.format(\" \", toString(expectedValuesWithCurrentKey.iterator(), schema), toString(actualRecord,\n+\t\t\t\t\tschema))), itemIndex\n \t\t\t\t+ expectedValuesWithCurrentKey.size() - 1);\n \n \t\tfuzzyMatcher.removeMatchingValues(similarityMap, schema, expectedValuesWithCurrentKey,\n \t\t\tactualValuesWithCurrentKey);\n \n \t\tif (!expectedValuesWithCurrentKey.isEmpty() || !actualValuesWithCurrentKey.isEmpty())\n \t\t\tthrow new ArrayComparisonFailure(\"Unexpected values for key \" + currentKeys + \": \",\n-\t\t\t\tnew AssertionFailedError(Assert.format(\" \", expectedValuesWithCurrentKey, actualValuesWithCurrentKey)),\n+\t\t\t\tnew AssertionFailedError(Assert.format(\" \", toString(expectedValuesWithCurrentKey.iterator(), schema),\n+\t\t\t\t\ttoString(actualValuesWithCurrentKey.iterator(), schema))),\n \t\t\t\titemIndex - expectedValuesWithCurrentKey.size());\n \n \t\tif (actualRecord != null)\n \t\t\tactualValuesWithCurrentKey.add(actualRecord);\n \t}\n \n \tprivate static Object toString(Iterator<PactRecord> iterator, Class<? extends Value>[] schema) {\n+\t\treturn toString(iterator, schema, 20);\n+\t}\n+\n+\tprivate static Object toString(Iterator<PactRecord> iterator, Class<? extends Value>[] schema, int maxNum) {\n \t\tStringBuilder builder = new StringBuilder();\n-\t\tfor (int index = 0; index < 20 && iterator.hasNext(); index++) {\n+\t\tfor (int index = 0; index < maxNum && iterator.hasNext(); index++) {\n \t\t\tbuilder.append(toString(iterator.next(), schema));\n \t\t\tif (iterator.hasNext())\n \t\t\t\tbuilder.append(\", \");\n@@ -660,9 +665,15 @@ public void setSchema(Class<? extends Value>[] schema) {\n \t\t\t\t@Override\n \t\t\t\tpublic int compare(PactRecord o1, PactRecord o2) {\n \t\t\t\t\tfor (int index = 0; index < info.keyClasses.size(); index++) {\n-\t\t\t\t\t\tint comparison = info.comparators.get(index).compare(\n-\t\t\t\t\t\t\to1.getField(info.sortKeys.get(index), info.keyClasses.get(index)),\n-\t\t\t\t\t\t\to2.getField(info.sortKeys.get(index), info.keyClasses.get(index)));\n+\t\t\t\t\t\tKey f1 = o1.getField(info.sortKeys.get(index), info.keyClasses.get(index));\n+\t\t\t\t\t\tKey f2 = o2.getField(info.sortKeys.get(index), info.keyClasses.get(index));\n+\t\t\t\t\t\tif (f1 == f2)\n+\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\tif (f1 == null)\n+\t\t\t\t\t\t\treturn -1;\n+\t\t\t\t\t\tif (f2 == null)\n+\t\t\t\t\t\t\treturn 1;\n+\t\t\t\t\t\tint comparison = info.comparators.get(index).compare(f1, f2);\n \t\t\t\t\t\tif (comparison != 0)\n \t\t\t\t\t\t\treturn comparison;\n \t\t\t\t\t}", "filename": "pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/c4a2d60c38a88aa6a7f1744726372601593165af", "parent": "https://github.com/apache/flink/commit/74b535d56a81fdf7460b9ce632e14e3c3d119355", "message": "[FLINK-3156] Fix NPE in KafkaConsumer when reading from SOME empty topics/partitions", "bug_id": "flink_45", "file": [{"additions": 6, "raw_url": "https://github.com/apache/flink/raw/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer.java", "blob_url": "https://github.com/apache/flink/blob/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer.java", "sha": "c4fd6540cfbe65171781e9bb1fb86804ba9d215a", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer.java?ref=c4a2d60c38a88aa6a7f1744726372601593165af", "patch": "@@ -661,7 +661,12 @@ public void close() {\n \tprivate static <T> void commitOffsets(HashMap<KafkaTopicPartition, Long> toCommit, FlinkKafkaConsumer<T> consumer) throws Exception {\n \t\tMap<KafkaTopicPartition, Long> offsetsToCommit = new HashMap<>();\n \t\tfor (KafkaTopicPartitionLeader tp : consumer.subscribedPartitions) {\n-\t\t\tlong offset = toCommit.get(tp.getTopicPartition());\n+\t\t\tLong offset = toCommit.get(tp.getTopicPartition());\n+\t\t\tif(offset == null) {\n+\t\t\t\t// There was no data ever consumed from this topic, that's why there is no entry\n+\t\t\t\t// for this topicPartition in the map.\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\tLong lastCommitted = consumer.committedOffsets.get(tp.getTopicPartition());\n \t\t\tif (lastCommitted == null) {\n \t\t\t\tlastCommitted = OFFSET_NOT_SET;", "filename": "flink-streaming-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer.java"}, {"additions": 32, "raw_url": "https://github.com/apache/flink/raw/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java", "blob_url": "https://github.com/apache/flink/blob/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java", "sha": "4d8b7c3f3b44a3b2fe4be8f0ee70c57205224c40", "changes": 37, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java?ref=c4a2d60c38a88aa6a7f1744726372601593165af", "patch": "@@ -120,7 +120,12 @@\n \t// ------------------------------------------------------------------------\n \n \tprotected abstract <T> FlinkKafkaConsumer<T> getConsumer(\n-\t\t\tString topic, DeserializationSchema<T> deserializationSchema, Properties props);\n+\t\t\tList<String> topics, DeserializationSchema<T> deserializationSchema, Properties props);\n+\n+\tprotected <T> FlinkKafkaConsumer<T> getConsumer(\n+\t\t\tString topic, DeserializationSchema<T> deserializationSchema, Properties props) {\n+\t\treturn getConsumer(Collections.singletonList(topic), deserializationSchema, props);\n+\t}\n \n \t// ------------------------------------------------------------------------\n \t//  Suite of Tests\n@@ -343,19 +348,34 @@ public void runOffsetAutocommitTest() throws Exception {\n \t * We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer\n \t * does not guarantee exactly-once output. Hence a recovery would introduce duplicates that\n \t * cause the test to fail.\n+\t *\n+\t * This test also ensures that FLINK-3156 doesn't happen again:\n+\t *\n+\t * The following situation caused a NPE in the FlinkKafkaConsumer\n+\t *\n+\t * topic-1 <-- elements are only produced into topic1.\n+\t * topic-2\n+\t *\n+\t * Therefore, this test is consuming as well from an empty topic.\n+\t *\n \t */\n \t@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)\n \tpublic void runSimpleConcurrentProducerConsumerTopology() throws Exception {\n \t\tfinal String topic = \"concurrentProducerConsumerTopic_\" + UUID.randomUUID().toString();\n+\t\tfinal String additionalEmptyTopic = \"additionalEmptyTopic_\" + UUID.randomUUID().toString();\n+\n \t\tfinal int parallelism = 3;\n \t\tfinal int elementsPerPartition = 100;\n \t\tfinal int totalElements = parallelism * elementsPerPartition;\n \n \t\tcreateTestTopic(topic, parallelism, 2);\n+\t\tcreateTestTopic(additionalEmptyTopic, parallelism, 1); // create an empty topic which will remain empty all the time\n \n \t\tfinal StreamExecutionEnvironment env =\n \t\t\t\tStreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.setParallelism(parallelism);\n+\t\tenv.enableCheckpointing(500);\n+\t\tenv.setNumberOfExecutionRetries(0); // fail immediately\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tTypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse(\"Tuple2<Long, String>\");\n@@ -373,14 +393,17 @@ public void runSimpleConcurrentProducerConsumerTopology() throws Exception {\n \t\t\tprivate boolean running = true;\n \n \t\t\t@Override\n-\t\t\tpublic void run(SourceContext<Tuple2<Long, String>> ctx) {\n+\t\t\tpublic void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {\n \t\t\t\tint cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition;\n \t\t\t\tint limit = cnt + elementsPerPartition;\n \n \n \t\t\t\twhile (running && cnt < limit) {\n \t\t\t\t\tctx.collect(new Tuple2<>(1000L + cnt, \"kafka-\" + cnt));\n \t\t\t\t\tcnt++;\n+\t\t\t\t\t// we delay data generation a bit so that we are sure that some checkpoints are\n+\t\t\t\t\t// triggered (for FLINK-3156)\n+\t\t\t\t\tThread.sleep(50);\n \t\t\t\t}\n \t\t\t}\n \n@@ -393,7 +416,10 @@ public void cancel() {\n \n \t\t// ----------- add consumer dataflow ----------\n \n-\t\tFlinkKafkaConsumer<Tuple2<Long, String>> source = getConsumer(topic, sourceSchema, standardProps);\n+\t\tList<String> topics = new ArrayList<>();\n+\t\ttopics.add(topic);\n+\t\ttopics.add(additionalEmptyTopic);\n+\t\tFlinkKafkaConsumer<Tuple2<Long, String>> source = getConsumer(topics, sourceSchema, standardProps);\n \n \t\tDataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism);\n \n@@ -1047,8 +1073,6 @@ public void runKeyValueTest() throws Exception {\n \t\tcreateTestTopic(topic, 1, 1);\n \t\tfinal int ELEMENT_COUNT = 5000;\n \n-\n-\n \t\t// ----------- Write some data into Kafka -------------------\n \n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n@@ -1111,6 +1135,8 @@ public void flatMap(Tuple2<Long, PojoValue> value, Collector<Object> out) throws\n \t\t});\n \n \t\ttryExecute(env, \"Read KV from Kafka\");\n+\n+\t\tdeleteTestTopic(topic);\n \t}\n \n \tpublic static class PojoValue {\n@@ -1121,6 +1147,7 @@ public PojoValue() {}\n \t}\n \n \n+\n \t// ------------------------------------------------------------------------\n \t//  Reading writing test data sets\n \t// ------------------------------------------------------------------------", "filename": "flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaITCase.java", "blob_url": "https://github.com/apache/flink/blob/c4a2d60c38a88aa6a7f1744726372601593165af/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaITCase.java", "sha": "3abbd71876421947c473abbd395fa50011a832eb", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaITCase.java?ref=c4a2d60c38a88aa6a7f1744726372601593165af", "patch": "@@ -21,14 +21,15 @@\n \n import org.junit.Test;\n \n+import java.util.List;\n import java.util.Properties;\n \n \n public class KafkaITCase extends KafkaConsumerTestBase {\n \t\n \t@Override\n-\tprotected <T> FlinkKafkaConsumer<T> getConsumer(String topic, DeserializationSchema<T> deserializationSchema, Properties props) {\n-\t\treturn new FlinkKafkaConsumer081<>(topic, deserializationSchema, props);\n+\tprotected <T> FlinkKafkaConsumer<T> getConsumer(List<String> topics, DeserializationSchema<T> deserializationSchema, Properties props) {\n+\t\treturn new FlinkKafkaConsumer082<>(topics, deserializationSchema, props);\n \t}\n \t\n \t// ------------------------------------------------------------------------", "filename": "flink-streaming-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaITCase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "parent": "https://github.com/apache/flink/commit/c0199f5d181a8d249201004f6ec6f897f9b799c4", "message": "[FLINK-7502][metrics] Improve PrometheusReporter\n\n* Do not throw exception when same metric is added twice\n* Add possibility to configure port range\n* Bump prometheus.version 0.0.21 -> 0.0.26\n* Use simpleclient_httpserver instead of nanohttpd\n* guard gauge report against null\n* guard close() vs NPE\n\nThis closes #4586.", "bug_id": "flink_46", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md", "sha": "64d7318dab3d625343015d8550be1e93686cfea4", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/monitoring/metrics.md?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -424,7 +424,7 @@ of your Flink distribution.\n \n Parameters:\n \n-- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations).\n+- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations). In order to be able to run several instances of the reporter on one host (e.g. when one TaskManager is colocated with the JobManager) it is advisable to use a port range like `9250-9260`.\n \n Example configuration:\n \n@@ -440,11 +440,11 @@ Flink metric types are mapped to Prometheus metric types as follows:\n | Flink     | Prometheus | Note                                     |\n | --------- |------------|------------------------------------------|\n | Counter   | Gauge      |Prometheus counters cannot be decremented.|\n-| Gauge     | Gauge      |                                          |\n+| Gauge     | Gauge      |Only numbers and booleans are supported.  |\n | Histogram | Summary    |Quantiles .5, .75, .95, .98, .99 and .999 |\n | Meter     | Gauge      |The gauge exports the meter's rate.       |\n \n-All Flink metrics variables, such as `<host>`, `<job_name>`, `<tm_id>`, `<subtask_index>`, `<task_name>` and `<operator_name>`, are exported to Prometheus as labels. \n+All Flink metrics variables (see [List of all Variables](#list-of-all-variables)) are exported to Prometheus as labels. \n \n ### StatsD (org.apache.flink.metrics.statsd.StatsDReporter)\n ", "filename": "docs/monitoring/metrics.md"}, {"additions": 8, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml", "sha": "0e9b2611db6dec8eb47aae4441a3df09c43b5424", "changes": 19, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -40,6 +40,13 @@ under the License.\n \t\t\t<scope>provided</scope>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-core</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n \t\t\t<artifactId>flink-runtime_${scala.binary.version}</artifactId>\n@@ -62,16 +69,10 @@ under the License.\n \n \t\t<dependency>\n \t\t\t<groupId>io.prometheus</groupId>\n-\t\t\t<artifactId>simpleclient_servlet</artifactId>\n+\t\t\t<artifactId>simpleclient_httpserver</artifactId>\n \t\t\t<version>${prometheus.version}</version>\n \t\t</dependency>\n \n-\t\t<dependency>\n-\t\t\t<groupId>org.nanohttpd</groupId>\n-\t\t\t<artifactId>nanohttpd</artifactId>\n-\t\t\t<version>2.2.0</version>\n-\t\t</dependency>\n-\n \t\t<!-- test dependencies -->\n \n \t\t<dependency>\n@@ -114,10 +115,6 @@ under the License.\n \t\t\t\t\t\t\t\t\t<pattern>io.prometheus.client</pattern>\n \t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.io.prometheus.client</shadedPattern>\n \t\t\t\t\t\t\t\t</relocation>\n-\t\t\t\t\t\t\t\t<relocation>\n-\t\t\t\t\t\t\t\t\t<pattern>fi.iki.elonen</pattern>\n-\t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.fi.iki.elonen</shadedPattern>\n-\t\t\t\t\t\t\t\t</relocation>\n \t\t\t\t\t\t\t</relocations>\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>", "filename": "flink-metrics/flink-metrics-prometheus/pom.xml"}, {"additions": 134, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java", "sha": "1e44ab966cf8478eb36dc2eb303532730986fbc1", "changes": 229, "status": "modified", "deletions": 95, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -24,28 +24,28 @@\n import org.apache.flink.metrics.Counter;\n import org.apache.flink.metrics.Gauge;\n import org.apache.flink.metrics.Histogram;\n-import org.apache.flink.metrics.HistogramStatistics;\n import org.apache.flink.metrics.Meter;\n import org.apache.flink.metrics.Metric;\n import org.apache.flink.metrics.MetricConfig;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.flink.metrics.reporter.MetricReporter;\n import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;\n import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;\n+import org.apache.flink.util.NetUtils;\n \n-import fi.iki.elonen.NanoHTTPD;\n import io.prometheus.client.Collector;\n import io.prometheus.client.CollectorRegistry;\n-import io.prometheus.client.exporter.common.TextFormat;\n+import io.prometheus.client.exporter.HTTPServer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.io.IOException;\n-import java.io.StringWriter;\n+import java.util.AbstractMap;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -59,7 +59,7 @@\n \tprivate static final Logger LOG = LoggerFactory.getLogger(PrometheusReporter.class);\n \n \tstatic final String ARG_PORT = \"port\";\n-\tprivate static final int DEFAULT_PORT = 9249;\n+\tprivate static final String DEFAULT_PORT = \"9249\";\n \n \tprivate static final Pattern UNALLOWED_CHAR_PATTERN = Pattern.compile(\"[^a-zA-Z0-9:_]\");\n \tprivate static final CharacterFilter CHARACTER_FILTER = new CharacterFilter() {\n@@ -72,8 +72,8 @@ public String filterCharacters(String input) {\n \tprivate static final char SCOPE_SEPARATOR = '_';\n \tprivate static final String SCOPE_PREFIX = \"flink\" + SCOPE_SEPARATOR;\n \n-\tprivate PrometheusEndpoint prometheusEndpoint;\n-\tprivate final Map<String, Collector> collectorsByMetricName = new HashMap<>();\n+\tprivate HTTPServer httpServer;\n+\tprivate final Map<String, AbstractMap.SimpleImmutableEntry<Collector, Integer>> collectorsWithCountByMetricName = new HashMap<>();\n \n \t@VisibleForTesting\n \tstatic String replaceInvalidChars(final String input) {\n@@ -84,27 +84,34 @@ static String replaceInvalidChars(final String input) {\n \n \t@Override\n \tpublic void open(MetricConfig config) {\n-\t\tint port = config.getInteger(ARG_PORT, DEFAULT_PORT);\n-\t\tLOG.info(\"Using port {}.\", port);\n-\t\tprometheusEndpoint = new PrometheusEndpoint(port);\n-\t\ttry {\n-\t\t\tprometheusEndpoint.start(NanoHTTPD.SOCKET_READ_TIMEOUT, true);\n-\t\t} catch (IOException e) {\n-\t\t\tfinal String msg = \"Could not start PrometheusEndpoint on port \" + port;\n-\t\t\tLOG.warn(msg, e);\n-\t\t\tthrow new RuntimeException(msg, e);\n+\t\tString portsConfig = config.getString(ARG_PORT, DEFAULT_PORT);\n+\t\tIterator<Integer> ports = NetUtils.getPortRangeFromString(portsConfig);\n+\n+\t\twhile (ports.hasNext()) {\n+\t\t\tint port = ports.next();\n+\t\t\ttry {\n+\t\t\t\thttpServer = new HTTPServer(port);\n+\t\t\t\tLOG.info(\"Started PrometheusReporter HTTP server on port {}.\", port);\n+\t\t\t\tbreak;\n+\t\t\t} catch (IOException ioe) { //assume port conflict\n+\t\t\t\tLOG.debug(\"Could not start PrometheusReporter HTTP server on port {}.\", port, ioe);\n+\t\t\t}\n+\t\t}\n+\t\tif (httpServer == null) {\n+\t\t\tthrow new RuntimeException(\"Could not start PrometheusReporter HTTP server on any configured port. Ports: \" + portsConfig);\n \t\t}\n \t}\n \n \t@Override\n \tpublic void close() {\n-\t\tprometheusEndpoint.stop();\n+\t\tif (httpServer != null) {\n+\t\t\thttpServer.stop();\n+\t\t}\n \t\tCollectorRegistry.defaultRegistry.clear();\n \t}\n \n \t@Override\n \tpublic void notifyOfAddedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tfinal String scope = SCOPE_PREFIX + getLogicalScope(group);\n \n \t\tList<String> dimensionKeys = new LinkedList<>();\n \t\tList<String> dimensionValues = new LinkedList<>();\n@@ -114,146 +121,178 @@ public void notifyOfAddedMetric(final Metric metric, final String metricName, fi\n \t\t\tdimensionValues.add(CHARACTER_FILTER.filterCharacters(dimension.getValue()));\n \t\t}\n \n-\t\tfinal String validMetricName = scope + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n-\t\tfinal String metricIdentifier = group.getMetricIdentifier(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tfinal String helpString = metricName + \" (scope: \" + getLogicalScope(group) + \")\";\n+\n \t\tfinal Collector collector;\n+\t\tInteger count = 0;\n+\n+\t\tsynchronized (this) {\n+\t\t\tif (collectorsWithCountByMetricName.containsKey(scopedMetricName)) {\n+\t\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\t\tcollector = collectorWithCount.getKey();\n+\t\t\t\tcount = collectorWithCount.getValue();\n+\t\t\t} else {\n+\t\t\t\tcollector = createCollector(metric, dimensionKeys, dimensionValues, scopedMetricName, helpString);\n+\t\t\t\ttry {\n+\t\t\t\t\tcollector.register();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem registering metric {}.\", metricName, e);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\taddMetric(metric, dimensionValues, collector);\n+\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count + 1));\n+\t\t}\n+\t}\n+\n+\tprivate static String getScopedName(String metricName, MetricGroup group) {\n+\t\treturn SCOPE_PREFIX + getLogicalScope(group) + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n+\t}\n+\n+\tprivate static Collector createCollector(Metric metric, List<String> dimensionKeys, List<String> dimensionValues, String scopedMetricName, String helpString) {\n+\t\tCollector collector;\n+\t\tif (metric instanceof Gauge || metric instanceof Counter || metric instanceof Meter) {\n+\t\t\tcollector = io.prometheus.client.Gauge\n+\t\t\t\t.build()\n+\t\t\t\t.name(scopedMetricName)\n+\t\t\t\t.help(helpString)\n+\t\t\t\t.labelNames(toArray(dimensionKeys))\n+\t\t\t\t.create();\n+\t\t} else if (metric instanceof Histogram) {\n+\t\t\tcollector = new HistogramSummaryProxy((Histogram) metric, scopedMetricName, helpString, dimensionKeys, dimensionValues);\n+\t\t} else {\n+\t\t\tLOG.warn(\"Cannot create collector for unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n+\t\t\t\tmetric.getClass().getName());\n+\t\t\tcollector = null;\n+\t\t}\n+\t\treturn collector;\n+\t}\n+\n+\tprivate static void addMetric(Metric metric, List<String> dimensionValues, Collector collector) {\n \t\tif (metric instanceof Gauge) {\n-\t\t\tcollector = createGauge((Gauge) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Gauge) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Counter) {\n-\t\t\tcollector = createGauge((Counter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Counter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Meter) {\n-\t\t\tcollector = createGauge((Meter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Meter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Histogram) {\n-\t\t\tcollector = createSummary((Histogram) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((HistogramSummaryProxy) collector).addChild((Histogram) metric, dimensionValues);\n \t\t} else {\n \t\t\tLOG.warn(\"Cannot add unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n \t\t\t\tmetric.getClass().getName());\n-\t\t\treturn;\n \t\t}\n-\t\tcollector.register();\n-\t\tcollectorsByMetricName.put(metricName, collector);\n \t}\n \n \t@Override\n \tpublic void notifyOfRemovedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tCollectorRegistry.defaultRegistry.unregister(collectorsByMetricName.get(metricName));\n-\t\tcollectorsByMetricName.remove(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tsynchronized (this) {\n+\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\tfinal Integer count = collectorWithCount.getValue();\n+\t\t\tfinal Collector collector = collectorWithCount.getKey();\n+\t\t\tif (count == 1) {\n+\t\t\t\ttry {\n+\t\t\t\t\tCollectorRegistry.defaultRegistry.unregister(collector);\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem unregistering metric {}.\", scopedMetricName, e);\n+\t\t\t\t}\n+\t\t\t\tcollectorsWithCountByMetricName.remove(scopedMetricName);\n+\t\t\t} else {\n+\t\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count - 1));\n+\t\t\t}\n+\t\t}\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n \tprivate static String getLogicalScope(MetricGroup group) {\n \t\treturn ((FrontMetricGroup<AbstractMetricGroup<?>>) group).getLogicalScope(CHARACTER_FILTER, SCOPE_SEPARATOR);\n \t}\n \n-\tprivate Collector createGauge(final Gauge gauge, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\t@VisibleForTesting\n+\tstatic io.prometheus.client.Gauge.Child gaugeFrom(Gauge gauge) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\tfinal Object value = gauge.getValue();\n+\t\t\t\tif (value == null) {\n+\t\t\t\t\tLOG.debug(\"Gauge {} is null-valued, defaulting to 0.\", gauge);\n+\t\t\t\t\treturn 0;\n+\t\t\t\t}\n \t\t\t\tif (value instanceof Double) {\n \t\t\t\t\treturn (double) value;\n \t\t\t\t}\n \t\t\t\tif (value instanceof Number) {\n \t\t\t\t\treturn ((Number) value).doubleValue();\n-\t\t\t\t} else if (value instanceof Boolean) {\n+\t\t\t\t}\n+\t\t\t\tif (value instanceof Boolean) {\n \t\t\t\t\treturn ((Boolean) value) ? 1 : 0;\n-\t\t\t\t} else {\n-\t\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n-\t\t\t\t\t\tgauge, value.getClass().getName());\n-\t\t\t\t\treturn 0;\n \t\t\t\t}\n+\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n+\t\t\t\t\tgauge, value.getClass().getName());\n+\t\t\t\treturn 0;\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate static Collector createGauge(final Counter counter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Counter counter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn (double) counter.getCount();\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate Collector createGauge(final Meter meter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Meter meter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn meter.getRate();\n \t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate static Collector newGauge(String name, String identifier, List<String> labelNames, List<String> labelValues, io.prometheus.client.Gauge.Child child) {\n-\t\treturn io.prometheus.client.Gauge\n-\t\t\t.build()\n-\t\t\t.name(name)\n-\t\t\t.help(identifier)\n-\t\t\t.labelNames(toArray(labelNames))\n-\t\t\t.create()\n-\t\t\t.setChild(child, toArray(labelValues));\n-\t}\n-\n-\tprivate static HistogramSummaryProxy createSummary(final Histogram histogram, final String name, final String identifier, final List<String> dimensionKeys, final List<String> dimensionValues) {\n-\t\treturn new HistogramSummaryProxy(histogram, name, identifier, dimensionKeys, dimensionValues);\n-\t}\n-\n-\tstatic class PrometheusEndpoint extends NanoHTTPD {\n-\t\tstatic final String MIME_TYPE = \"plain/text\";\n-\n-\t\tPrometheusEndpoint(int port) {\n-\t\t\tsuper(port);\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Response serve(IHTTPSession session) {\n-\t\t\tif (session.getUri().equals(\"/metrics\")) {\n-\t\t\t\tStringWriter writer = new StringWriter();\n-\t\t\t\ttry {\n-\t\t\t\t\tTextFormat.write004(writer, CollectorRegistry.defaultRegistry.metricFamilySamples());\n-\t\t\t\t} catch (IOException e) {\n-\t\t\t\t\treturn newFixedLengthResponse(Response.Status.INTERNAL_ERROR, MIME_TYPE, \"Unable to output metrics\");\n-\t\t\t\t}\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.OK, TextFormat.CONTENT_TYPE_004, writer.toString());\n-\t\t\t} else {\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.NOT_FOUND, MIME_TYPE, \"Not found\");\n-\t\t\t}\n-\t\t}\n+\t\t};\n \t}\n \n-\tprivate static class HistogramSummaryProxy extends Collector {\n-\t\tprivate static final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n+\t@VisibleForTesting\n+\tstatic class HistogramSummaryProxy extends Collector {\n+\t\tstatic final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n \n-\t\tprivate final Histogram histogram;\n \t\tprivate final String metricName;\n-\t\tprivate final String metricIdentifier;\n+\t\tprivate final String helpString;\n \t\tprivate final List<String> labelNamesWithQuantile;\n-\t\tprivate final List<String> labelValues;\n \n-\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String metricIdentifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\t\tthis.histogram = histogram;\n+\t\tprivate final Map<List<String>, Histogram> histogramsByLabelValues = new HashMap<>();\n+\n+\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String helpString, final List<String> labelNames, final List<String> labelValues) {\n \t\t\tthis.metricName = metricName;\n-\t\t\tthis.metricIdentifier = metricIdentifier;\n+\t\t\tthis.helpString = helpString;\n \t\t\tthis.labelNamesWithQuantile = addToList(labelNames, \"quantile\");\n-\t\t\tthis.labelValues = labelValues;\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n \t\t}\n \n \t\t@Override\n \t\tpublic List<MetricFamilySamples> collect() {\n \t\t\t// We cannot use SummaryMetricFamily because it is impossible to get a sum of all values (at least for Dropwizard histograms,\n \t\t\t// whose snapshot's values array only holds a sample of recent values).\n \n-\t\t\tfinal HistogramStatistics statistics = histogram.getStatistics();\n-\n \t\t\tList<MetricFamilySamples.Sample> samples = new LinkedList<>();\n+\t\t\tfor (Map.Entry<List<String>, Histogram> labelValuesToHistogram : histogramsByLabelValues.entrySet()) {\n+\t\t\t\taddSamples(labelValuesToHistogram.getKey(), labelValuesToHistogram.getValue(), samples);\n+\t\t\t}\n+\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, helpString, samples));\n+\t\t}\n+\n+\t\tvoid addChild(final Histogram histogram, final List<String> labelValues) {\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n+\t\t}\n+\n+\t\tprivate void addSamples(final List<String> labelValues, final Histogram histogram, final List<MetricFamilySamples.Sample> samples) {\n \t\t\tsamples.add(new MetricFamilySamples.Sample(metricName + \"_count\",\n \t\t\t\tlabelNamesWithQuantile.subList(0, labelNamesWithQuantile.size() - 1), labelValues, histogram.getCount()));\n \t\t\tfor (final Double quantile : QUANTILES) {\n \t\t\t\tsamples.add(new MetricFamilySamples.Sample(metricName, labelNamesWithQuantile,\n \t\t\t\t\taddToList(labelValues, quantile.toString()),\n-\t\t\t\t\tstatistics.getQuantile(quantile)));\n+\t\t\t\t\thistogram.getStatistics().getQuantile(quantile)));\n \t\t\t}\n-\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, metricIdentifier, samples));\n \t\t}\n \t}\n \n@@ -263,7 +302,7 @@ public Response serve(IHTTPSession session) {\n \t\treturn result;\n \t}\n \n-\tprivate static String[] toArray(List<String> labelNames) {\n-\t\treturn labelNames.toArray(new String[labelNames.size()]);\n+\tprivate static String[] toArray(List<String> list) {\n+\t\treturn list.toArray(new String[list.size()]);\n \t}\n }", "filename": "flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java"}, {"additions": 188, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java", "sha": "c7d4040734ec7451e19a17649c2372d18bff3bfb", "changes": 188, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.metrics.prometheus;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.metrics.Counter;\n+import org.apache.flink.metrics.Gauge;\n+import org.apache.flink.metrics.Histogram;\n+import org.apache.flink.metrics.Meter;\n+import org.apache.flink.metrics.SimpleCounter;\n+import org.apache.flink.metrics.util.TestMeter;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.metrics.MetricRegistry;\n+import org.apache.flink.runtime.metrics.MetricRegistryConfiguration;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskMetricGroup;\n+import org.apache.flink.runtime.metrics.util.TestingHistogram;\n+import org.apache.flink.util.AbstractID;\n+\n+import com.mashape.unirest.http.exceptions.UnirestException;\n+import io.prometheus.client.CollectorRegistry;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.createConfigWithOneReporter;\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.pollMetrics;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Test for {@link PrometheusReporter} that registers several instances of the same metric for different subtasks.\n+ */\n+public class PrometheusReporterTaskScopeTest {\n+\tprivate static final String[] LABEL_NAMES = {\"job_id\", \"task_id\", \"task_attempt_id\", \"host\", \"task_name\", \"task_attempt_num\", \"job_name\", \"tm_id\", \"subtask_index\"};\n+\n+\tprivate static final String TASK_MANAGER_HOST = \"taskManagerHostName\";\n+\tprivate static final String TASK_MANAGER_ID = \"taskManagerId\";\n+\tprivate static final String JOB_NAME = \"jobName\";\n+\tprivate static final String TASK_NAME = \"taskName\";\n+\tprivate static final int ATTEMPT_NUMBER = 0;\n+\tprivate static final int SUBTASK_INDEX_1 = 0;\n+\tprivate static final int SUBTASK_INDEX_2 = 1;\n+\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9429\")));\n+\n+\tprivate final JobID jobId = new JobID();\n+\tprivate final JobVertexID taskId1 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId1 = new AbstractID();\n+\tprivate final String[] labelValues1 = {jobId.toString(), taskId1.toString(), taskAttemptId1.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_1};\n+\tprivate final JobVertexID taskId2 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId2 = new AbstractID();\n+\tprivate final String[] labelValues2 = {jobId.toString(), taskId2.toString(), taskAttemptId2.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_2};\n+\n+\tprivate final TaskManagerMetricGroup tmMetricGroup = new TaskManagerMetricGroup(registry, TASK_MANAGER_HOST, TASK_MANAGER_ID);\n+\tprivate final TaskManagerJobMetricGroup tmJobMetricGroup = new TaskManagerJobMetricGroup(registry, tmMetricGroup, jobId, JOB_NAME);\n+\tprivate final TaskMetricGroup taskMetricGroup1 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId1, taskAttemptId1, TASK_NAME, SUBTASK_INDEX_1, ATTEMPT_NUMBER);\n+\tprivate final TaskMetricGroup taskMetricGroup2 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId2, taskAttemptId2, TASK_NAME, SUBTASK_INDEX_2, ATTEMPT_NUMBER);\n+\n+\t@Test\n+\tpublic void countersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\t}\n+\n+\t@Test\n+\tpublic void gaugesCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tGauge<Integer> gauge1 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 3;\n+\t\t\t}\n+\t\t};\n+\t\tGauge<Integer> gauge2 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 4;\n+\t\t\t}\n+\t\t};\n+\n+\t\ttaskMetricGroup1.gauge(\"my_gauge\", gauge1);\n+\t\ttaskMetricGroup2.gauge(\"my_gauge\", gauge2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(3.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(4.));\n+\t}\n+\n+\t@Test\n+\tpublic void metersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tMeter meter = new TestMeter();\n+\n+\t\ttaskMetricGroup1.meter(\"my_meter\", meter);\n+\t\ttaskMetricGroup2.meter(\"my_meter\", meter);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(5.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(5.));\n+\t}\n+\n+\t@Test\n+\tpublic void histogramsCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tHistogram histogram = new TestingHistogram();\n+\n+\t\ttaskMetricGroup1.histogram(\"my_histogram\", histogram);\n+\t\ttaskMetricGroup2.histogram(\"my_histogram\", histogram);\n+\n+\t\tfinal String exportedMetrics = pollMetrics().getBody();\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"0\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"1\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\n+\t\tfinal String[] labelNamesWithQuantile = addToArray(LABEL_NAMES, \"quantile\");\n+\t\tfor (Double quantile : PrometheusReporter.HistogramSummaryProxy.QUANTILES) {\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues1, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues2, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void removingSingleInstanceOfMetricDoesNotBreakOtherInstances() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\n+\t\ttaskMetricGroup2.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\n+\t\ttaskMetricGroup1.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tnullValue());\n+\t}\n+\n+\tprivate String[] addToArray(String[] array, String element) {\n+\t\tfinal String[] labelNames = Arrays.copyOf(array, LABEL_NAMES.length + 1);\n+\t\tlabelNames[LABEL_NAMES.length] = element;\n+\t\treturn labelNames;\n+\t}\n+\n+\t@After\n+\tpublic void shutdownRegistry() {\n+\t\tregistry.shutdown();\n+\t}\n+\n+}", "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java"}, {"additions": 143, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java", "sha": "956339b818930d9334d877f1cbc80c7e12e508a1", "changes": 185, "status": "modified", "deletions": 42, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -47,13 +47,13 @@\n import java.util.Arrays;\n \n import static org.apache.flink.metrics.prometheus.PrometheusReporter.ARG_PORT;\n-import static org.apache.flink.runtime.metrics.scope.ScopeFormat.SCOPE_SEPARATOR;\n import static org.hamcrest.Matchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n import static org.junit.Assert.assertThat;\n \n /**\n- * Test for {@link PrometheusReporter}.\n+ * Basic test for {@link PrometheusReporter}.\n  */\n public class PrometheusReporterTest extends TestLogger {\n \tprivate static final int NON_DEFAULT_PORT = 9429;\n@@ -70,22 +70,21 @@\n \t@Rule\n \tpublic ExpectedException thrown = ExpectedException.none();\n \n-\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter()));\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"\" + NON_DEFAULT_PORT)));\n+\tprivate final FrontMetricGroup<TaskManagerMetricGroup> metricGroup = new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER));\n \tprivate final MetricReporter reporter = registry.getReporters().get(0);\n \n+\t/**\n+\t * {@link io.prometheus.client.Counter} may not decrease, so report {@link Counter} as {@link io.prometheus.client.Gauge}.\n+\t *\n+\t * @throws UnirestException Might be thrown on HTTP problems.\n+\t */\n \t@Test\n \tpublic void counterIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\t//Prometheus counters may not decrease\n \t\tCounter testCounter = new SimpleCounter();\n \t\ttestCounter.inc(7);\n \n-\t\tString counterName = \"testCounter\";\n-\t\tString gaugeName = SCOPE_PREFIX + counterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testCounter, counterName),\n-\t\t\tequalTo(HELP_PREFIX + gaugeName + \" \" + getFullMetricName(counterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + gaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tgaugeName + DEFAULT_LABELS + \" 7.0\" + \"\\n\"));\n+\t\tassertThatGaugeIsExported(testCounter, \"testCounter\", \"7.0\");\n \t}\n \n \t@Test\n@@ -97,13 +96,34 @@ public Integer getValue() {\n \t\t\t}\n \t\t};\n \n-\t\tString gaugeName = \"testGauge\";\n-\t\tString prometheusGaugeName = SCOPE_PREFIX + gaugeName;\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"1.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void nullGaugeDoesNotBreakReporter() throws UnirestException {\n+\t\tGauge<Integer> testGauge = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"0.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n+\t\tMeter testMeter = new TestMeter();\n+\n+\t\tassertThatGaugeIsExported(testMeter, \"testMeter\", \"5.0\");\n+\t}\n \n-\t\tassertThat(addMetricAndPollResponse(testGauge, gaugeName),\n-\t\t\tequalTo(HELP_PREFIX + prometheusGaugeName + \" \" + getFullMetricName(gaugeName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + prometheusGaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tprometheusGaugeName + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n+\tprivate void assertThatGaugeIsExported(Metric metric, String name, String expectedValue) throws UnirestException {\n+\t\tfinal String prometheusName = SCOPE_PREFIX + name;\n+\t\tassertThat(addMetricAndPollResponse(metric, name),\n+\t\t\tcontainsString(HELP_PREFIX + prometheusName + \" \" + name + \" (scope: taskmanager)\\n\" +\n+\t\t\t\tTYPE_PREFIX + prometheusName + \" gauge\" + \"\\n\" +\n+\t\t\t\tprometheusName + DEFAULT_LABELS + \" \" + expectedValue + \"\\n\"));\n \t}\n \n \t@Test\n@@ -114,7 +134,7 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\tString summaryName = SCOPE_PREFIX + histogramName;\n \n \t\tString response = addMetricAndPollResponse(testHistogram, histogramName);\n-\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + getFullMetricName(histogramName) + \"\\n\" +\n+\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + histogramName + \" (scope: taskmanager)\\n\" +\n \t\t\tTYPE_PREFIX + summaryName + \" summary\" + \"\\n\" +\n \t\t\tsummaryName + \"_count\" + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n \t\tfor (String quantile : Arrays.asList(\"0.5\", \"0.75\", \"0.95\", \"0.98\", \"0.99\", \"0.999\")) {\n@@ -123,19 +143,6 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\t}\n \t}\n \n-\t@Test\n-\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\tMeter testMeter = new TestMeter();\n-\n-\t\tString meterName = \"testMeter\";\n-\t\tString counterName = SCOPE_PREFIX + meterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testMeter, meterName),\n-\t\t\tequalTo(HELP_PREFIX + counterName + \" \" + getFullMetricName(meterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + counterName + \" gauge\" + \"\\n\" +\n-\t\t\t\tcounterName + DEFAULT_LABELS + \" 5.0\" + \"\\n\"));\n-\t}\n-\n \t@Test\n \tpublic void endpointIsUnavailableAfterReporterIsClosed() throws UnirestException {\n \t\treporter.close();\n@@ -160,25 +167,119 @@ public void invalidCharactersAreReplacedWithUnderscore() {\n \t\tassertThat(PrometheusReporter.replaceInvalidChars(\"a,=;:?'b,=;:?'c\"), equalTo(\"a___:__b___:__c\"));\n \t}\n \n+\t@Test\n+\tpublic void doubleGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Double>() {\n+\t\t\t@Override\n+\t\t\tpublic Double getValue() {\n+\t\t\t\treturn 3.14;\n+\t\t\t}\n+\t\t}).get(), equalTo(3.14));\n+\t}\n+\n+\t@Test\n+\tpublic void shortGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Short>() {\n+\t\t\t@Override\n+\t\t\tpublic Short getValue() {\n+\t\t\t\treturn 13;\n+\t\t\t}\n+\t\t}).get(), equalTo(13.));\n+\t}\n+\n+\t@Test\n+\tpublic void booleanGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Boolean>() {\n+\t\t\t@Override\n+\t\t\tpublic Boolean getValue() {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\t\t}).get(), equalTo(1.));\n+\t}\n+\n+\t/**\n+\t * Prometheus only supports numbers, so report non-numeric gauges as 0.\n+\t */\n+\t@Test\n+\tpublic void stringGaugeCannotBeConverted() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<String>() {\n+\t\t\t@Override\n+\t\t\tpublic String getValue() {\n+\t\t\t\treturn \"I am not a number\";\n+\t\t\t}\n+\t\t}).get(), equalTo(0.));\n+\t}\n+\n+\t@Test\n+\tpublic void registeringSameMetricTwiceDoesNotThrowException() {\n+\t\tCounter counter = new SimpleCounter();\n+\t\tcounter.inc();\n+\t\tString counterName = \"testCounter\";\n+\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void addingUnknownMetricTypeDoesNotThrowException(){\n+\t\tclass SomeMetricType implements Metric{}\n+\n+\t\treporter.notifyOfAddedMetric(new SomeMetricType(), \"name\", metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartTwoReportersOnSamePort() {\n+\t\tfinal MetricRegistry fixedPort1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"12345\")));\n+\t\tfinal MetricRegistry fixedPort2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"12345\")));\n+\n+\t\tassertThat(fixedPort1.getReporters(), hasSize(1));\n+\t\tassertThat(fixedPort2.getReporters(), hasSize(0));\n+\n+\t\tfixedPort1.shutdown();\n+\t\tfixedPort2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void canStartTwoReportersWhenUsingPortRange() {\n+\t\tfinal MetricRegistry portRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9249-9252\")));\n+\t\tfinal MetricRegistry portRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9249-9252\")));\n+\n+\t\tassertThat(portRange1.getReporters(), hasSize(1));\n+\t\tassertThat(portRange2.getReporters(), hasSize(1));\n+\n+\t\tportRange1.shutdown();\n+\t\tportRange2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartThreeReportersWhenPortRangeIsTooSmall() {\n+\t\tfinal MetricRegistry smallPortRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange3 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test3\", \"9253-9254\")));\n+\n+\t\tassertThat(smallPortRange1.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange2.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange3.getReporters(), hasSize(0));\n+\n+\t\tsmallPortRange1.shutdown();\n+\t\tsmallPortRange2.shutdown();\n+\t\tsmallPortRange3.shutdown();\n+\t}\n+\n \tprivate String addMetricAndPollResponse(Metric metric, String metricName) throws UnirestException {\n-\t\treporter.notifyOfAddedMetric(metric, metricName, new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER)));\n+\t\treporter.notifyOfAddedMetric(metric, metricName, metricGroup);\n \t\treturn pollMetrics().getBody();\n \t}\n \n-\tprivate static HttpResponse<String> pollMetrics() throws UnirestException {\n+\tstatic HttpResponse<String> pollMetrics() throws UnirestException {\n \t\treturn Unirest.get(\"http://localhost:\" + NON_DEFAULT_PORT + \"/metrics\").asString();\n \t}\n \n-\tprivate static String getFullMetricName(String metricName) {\n-\t\treturn HOST_NAME + SCOPE_SEPARATOR + \"taskmanager\" + SCOPE_SEPARATOR + TASK_MANAGER + SCOPE_SEPARATOR + metricName;\n-\t}\n-\n-\tprivate static Configuration createConfigWithOneReporter() {\n+\tstatic Configuration createConfigWithOneReporter(String reporterName, String portString) {\n \t\tConfiguration cfg = new Configuration();\n-\t\tcfg.setString(MetricOptions.REPORTERS_LIST, \"test1\");\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" +\n-\t\t\tConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" + ARG_PORT, \"\" + NON_DEFAULT_PORT);\n+\t\tcfg.setString(MetricOptions.REPORTERS_LIST, reporterName);\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ARG_PORT, portString);\n \t\treturn cfg;\n \t}\n ", "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml", "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml", "sha": "9384a48fe8472c0708f01a7cd5061d6e12d3b03e", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72", "patch": "@@ -116,7 +116,7 @@ under the License.\n \t\t<curator.version>2.12.0</curator.version>\n \t\t<jackson.version>2.7.4</jackson.version>\n \t\t<metrics.version>3.1.5</metrics.version>\n-\t\t<prometheus.version>0.0.21</prometheus.version>\n+\t\t<prometheus.version>0.0.26</prometheus.version>\n \t\t<junit.version>4.12</junit.version>\n \t\t<mockito.version>1.10.19</mockito.version>\n \t\t<powermock.version>1.6.5</powermock.version>", "filename": "pom.xml"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/04aae3061aa0184e3ad610cda454c661f872a45d", "parent": "https://github.com/apache/flink/commit/9d3f127e4f0ddd15ae7af393e0c5b0d3dd81d812", "message": "[FLINK-6294] Add close without input test for BucketingSink\n\nAnd earlier version of the code was throwing an NPE if the sink was\nclosed without ever seeing any input.", "bug_id": "flink_47", "file": [{"additions": 13, "raw_url": "https://github.com/apache/flink/raw/04aae3061aa0184e3ad610cda454c661f872a45d/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java", "blob_url": "https://github.com/apache/flink/blob/04aae3061aa0184e3ad610cda454c661f872a45d/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java", "sha": "090c54a7566604f67d4cbdab3ffc7f1e80a1f5c7", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java?ref=04aae3061aa0184e3ad610cda454c661f872a45d", "patch": "@@ -150,6 +150,19 @@ public static void destroyHDFS() {\n \t\thdfsCluster.shutdown();\n \t}\n \n+\t@Test\n+\tpublic void testClosingWithoutInput() throws Exception {\n+\t\tfinal File outDir = tempFolder.newFolder();\n+\n+\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n+\t\ttestHarness.setup();\n+\t\ttestHarness.open();\n+\n+\t\t// verify that we can close without ever having an input. An earlier version of the code\n+\t\t// was throwing an NPE because we never initialized some internal state\n+\t\ttestHarness.close();\n+\t}\n+\n \t@Test\n \tpublic void testInactivityPeriodWithLateNotify() throws Exception {\n \t\tfinal File outDir = tempFolder.newFolder();", "filename": "flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/d433ba9f032e5361ae894562b7a8be13cd3efe13", "parent": "https://github.com/apache/flink/commit/58b9a3772f5027f58335fb299b122e8ecb9db218", "message": "[FLINK-2177] [runtime] Fix possible NPE when closing Netty channel, before it is active", "bug_id": "flink_48", "file": [{"additions": 5, "raw_url": "https://github.com/apache/flink/raw/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "blob_url": "https://github.com/apache/flink/blob/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java", "sha": "51b436bb0f8ebcc2b25c3a6a811f8f7a8be49cb3", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java?ref=d433ba9f032e5361ae894562b7a8be13cd3efe13", "patch": "@@ -66,7 +66,7 @@\n \t */\n \tprivate final ConcurrentMap<InputChannelID, InputChannelID> cancelled = Maps.newConcurrentMap();\n \n-\tprivate ChannelHandlerContext ctx;\n+\tprivate volatile ChannelHandlerContext ctx;\n \n \t// ------------------------------------------------------------------------\n \t// Input channel/receiver registration\n@@ -85,6 +85,10 @@ void removeInputChannel(RemoteInputChannel listener) {\n \t}\n \n \tvoid cancelRequestFor(InputChannelID inputChannelId) {\n+\t\tif (inputChannelId == null || ctx == null) {\n+\t\t\treturn;\n+\t\t}\n+\n \t\tif (cancelled.putIfAbsent(inputChannelId, inputChannelId) == null) {\n \t\t\tctx.writeAndFlush(new NettyMessage.CancelPartitionRequest(inputChannelId));\n \t\t}", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java"}, {"additions": 16, "raw_url": "https://github.com/apache/flink/raw/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java", "blob_url": "https://github.com/apache/flink/blob/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java", "sha": "2c08cc5a2e4f821e41d570fa2441eba16f4566da", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java?ref=d433ba9f032e5361ae894562b7a8be13cd3efe13", "patch": "@@ -142,6 +142,22 @@ public void testReceivePartitionNotFoundException() throws Exception {\n \t\tverify(inputChannel, times(1)).onFailedPartitionRequest();\n \t}\n \n+\t@Test\n+\tpublic void testCancelBeforeActive() throws Exception {\n+\n+\t\tfinal RemoteInputChannel inputChannel = mock(RemoteInputChannel.class);\n+\t\twhen(inputChannel.getInputChannelId()).thenReturn(new InputChannelID());\n+\n+\t\tfinal PartitionRequestClientHandler client = new PartitionRequestClientHandler();\n+\t\tclient.addInputChannel(inputChannel);\n+\n+\t\t// Don't throw NPE\n+\t\tclient.cancelRequestFor(null);\n+\n+\t\t// Don't throw NPE, because channel is not active yet\n+\t\tclient.cancelRequestFor(inputChannel.getInputChannelId());\n+\t}\n+\n \t// ---------------------------------------------------------------------------------------------\n \n \t/**", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98", "parent": "https://github.com/apache/flink/commit/9a18e579021304f6ee0687cd1c9579740b11b98d", "message": "[FLINK-1922] [runtime] Fixes NPE when TM receives a null input split\n\nThis closes #631", "bug_id": "flink_49", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java", "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java", "sha": "baed9474dcefaff1d5bfcf97dde0a280334ce4d6", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98", "patch": "@@ -328,7 +328,7 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t// register this execution at the execution graph, to receive call backs\n \t\t\tvertex.getExecutionGraph().registerExecution(this);\n \n-\t\t\tInstance instance = slot.getInstance();\n+\t\t\tfinal Instance instance = slot.getInstance();\n \t\t\tFuture<Object> deployAction = Patterns.ask(instance.getTaskManager(),\n \t\t\t\t\tnew SubmitTask(deployment), new Timeout(timeout));\n \n@@ -338,7 +338,9 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t\tpublic void onComplete(Throwable failure, Object success) throws Throwable {\n \t\t\t\t\tif (failure != null) {\n \t\t\t\t\t\tif (failure instanceof TimeoutException) {\n-\t\t\t\t\t\t\tmarkFailed(new Exception(\"Cannot deploy task - TaskManager not responding.\", failure));\n+\t\t\t\t\t\t\tmarkFailed(new Exception(\n+\t\t\t\t\t\t\t\t\t\"Cannot deploy task - TaskManager \" + instance + \" not responding.\",\n+\t\t\t\t\t\t\t\t\tfailure));\n \t\t\t\t\t\t}\n \t\t\t\t\t\telse {\n \t\t\t\t\t\t\tmarkFailed(failure);", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java"}, {"additions": 8, "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java", "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java", "sha": "5a698509c9ed068135a2c26cc2881620882cccc6", "changes": 15, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98", "patch": "@@ -68,10 +68,6 @@ public InputSplit getNextInputSplit() {\n \n \t\t\tfinal Object result = Await.result(response, timeout.duration());\n \n-\t\t\tif (result == null) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\n \t\t\tif(!(result instanceof JobManagerMessages.NextInputSplit)){\n \t\t\t\tthrow new RuntimeException(\"RequestNextInputSplit requires a response of type \" +\n \t\t\t\t\t\t\"NextInputSplit. Instead response is of type \" + result.getClass() + \".\");\n@@ -80,9 +76,14 @@ public InputSplit getNextInputSplit() {\n \t\t\t\t\t\t(JobManagerMessages.NextInputSplit) result;\n \n \t\t\t\tbyte[] serializedData = nextInputSplit.splitData();\n-\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n-\t\t\t\t\t\tusercodeClassLoader);\n-\t\t\t\treturn (InputSplit) deserialized;\n+\n+\t\t\t\tif(serializedData == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t} else {\n+\t\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n+\t\t\t\t\t\t\tusercodeClassLoader);\n+\t\t\t\t\treturn (InputSplit) deserialized;\n+\t\t\t\t}\n \t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Requesting the next InputSplit failed.\", e);", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java"}, {"additions": 93, "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java", "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java", "sha": "f0689789a2d223e6ac814bbf774b38da27786595", "changes": 93, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98", "patch": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.taskmanager;\n+\n+import akka.actor.ActorRef;\n+import akka.actor.ActorSystem;\n+import akka.actor.Props;\n+import akka.actor.Status;\n+import akka.actor.UntypedActor;\n+import akka.testkit.JavaTestKit;\n+import akka.util.Timeout;\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.messages.JobManagerMessages;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+public class TaskInputSplitProviderTest {\n+\n+\tprivate static ActorSystem system;\n+\n+\t@BeforeClass\n+\tpublic static void setup() throws Exception {\n+\t\tsystem = ActorSystem.create(\"TestActorSystem\", TestingUtils.testConfig());\n+\t}\n+\n+\t@AfterClass\n+\tpublic static void teardown() throws Exception {\n+\t\tJavaTestKit.shutdownActorSystem(system);\n+\t\tsystem = null;\n+\t}\n+\n+\t@Test\n+\tpublic void testRequestNextInputSplitWithInvalidExecutionID() {\n+\n+\t\tfinal JobID jobID = new JobID();\n+\t\tfinal JobVertexID vertexID = new JobVertexID();\n+\t\tfinal ExecutionAttemptID executionID = new ExecutionAttemptID();\n+\t\tfinal Timeout timeout = new Timeout(10, TimeUnit.SECONDS);\n+\n+\t\tfinal ActorRef jobManagerRef = system.actorOf(Props.create(NullInputSplitJobManager.class));\n+\n+\t\tfinal TaskInputSplitProvider provider = new TaskInputSplitProvider(\n+\t\t\t\tjobManagerRef,\n+\t\t\t\tjobID,\n+\t\t\t\tvertexID,\n+\t\t\t\texecutionID,\n+\t\t\t\tgetClass().getClassLoader(),\n+\t\t\t\ttimeout\n+\t\t);\n+\n+\t\t// The jobManager will return a\n+\t\tInputSplit nextInputSplit = provider.getNextInputSplit();\n+\n+\t\tassertTrue(nextInputSplit == null);\n+\t}\n+\n+\tpublic static class NullInputSplitJobManager extends UntypedActor {\n+\n+\t\t@Override\n+\t\tpublic void onReceive(Object message) throws Exception {\n+\t\t\tif(message instanceof JobManagerMessages.RequestNextInputSplit) {\n+\t\t\t\tsender().tell(new JobManagerMessages.NextInputSplit(null), getSelf());\n+\t\t\t} else {\n+\t\t\t\tsender().tell(new Status.Failure(new Exception(\"Invalid message type\")), getSelf());\n+\t\t\t}\n+\t\t}\n+\t}\n+}", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/c5dd1f11f71471ba42e3a075651649e2ca258551", "parent": "https://github.com/apache/flink/commit/692318593be6e5f7aa5fc62bf624ef34cb5a357a", "message": "Fix NPE in Ordering toString method: Keytypes may be null", "bug_id": "flink_50", "file": [{"additions": 4, "raw_url": "https://github.com/apache/flink/raw/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java", "blob_url": "https://github.com/apache/flink/blob/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java", "sha": "ca0e4f8e38e1825fea35a9a382e5642ab5ce8615", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=c5dd1f11f71471ba42e3a075651649e2ca258551", "patch": "@@ -180,8 +180,10 @@ public String toString()\n \t\t\t\tbuf.append(\",\");\n \t\t\t}\n \t\t\tbuf.append(this.indexes.get(i));\n-\t\t\tbuf.append(\":\");\n-\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\tif (this.types.get(i) != null) {\n+\t\t\t\tbuf.append(\":\");\n+\t\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\t}\n \t\t\tbuf.append(\":\");\n \t\t\tbuf.append(this.orders.get(i).name());\n \t\t}", "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/181ebbb0982b29bfd931383a474eea9dbf5a2026", "parent": "https://github.com/apache/flink/commit/84da181d291bf7362ef601b2b585a4aa608a2189", "message": "Fixed NPE when determining global properties in UnionNode", "bug_id": "flink_51", "file": [{"additions": 12, "raw_url": "https://github.com/apache/flink/raw/181ebbb0982b29bfd931383a474eea9dbf5a2026/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/UnionNode.java", "blob_url": "https://github.com/apache/flink/blob/181ebbb0982b29bfd931383a474eea9dbf5a2026/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/UnionNode.java", "sha": "e1e20a10109ae6decc13be9972523b7bdbc456a5", "changes": 23, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/UnionNode.java?ref=181ebbb0982b29bfd931383a474eea9dbf5a2026", "patch": "@@ -160,17 +160,18 @@ public void calcAlternatives(List<OptimizerNode> target, Stack<OptimizerNode> ne\n \t\t\tMap<OptimizerNode, OptimizerNode> newBranchPlan = new HashMap<OptimizerNode, OptimizerNode>(branchPlan);\n \t\t\t\n \t\t\tboolean isCompatible = true;\n-\t\t\t\n-\t\t\tfor (UnclosedBranchDescriptor branch : alternative.openBranches) {\n-\t\t\t\tOptimizerNode brancher = branch.getBranchingNode();\n-\t\t\t\tif (newBranchPlan.containsKey(brancher)) {\n-\t\t\t\t\tif (newBranchPlan.get(brancher) != alternative.branchPlan.get(brancher)) {\n-\t\t\t\t\t\tisCompatible = false;\n-\t\t\t\t\t\tbreak;\n+\t\t\tif (alternative.openBranches != null) {\n+\t\t\t\tfor (UnclosedBranchDescriptor branch : alternative.openBranches) {\n+\t\t\t\t\tOptimizerNode brancher = branch.getBranchingNode();\n+\t\t\t\t\tif (newBranchPlan.containsKey(brancher)) {\n+\t\t\t\t\t\tif (newBranchPlan.get(brancher) != alternative.branchPlan.get(brancher)) {\n+\t\t\t\t\t\t\tisCompatible = false;\n+\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} \n+\t\t\t\t\telse {\n+\t\t\t\t\t\tnewBranchPlan.put(brancher, alternative.branchPlan.get(brancher));\n \t\t\t\t\t}\n-\t\t\t\t} \n-\t\t\t\telse {\n-\t\t\t\t\tnewBranchPlan.put(brancher, alternative.branchPlan.get(brancher));\n \t\t\t\t}\n \t\t\t}\n \t\t\t\n@@ -182,7 +183,7 @@ public void calcAlternatives(List<OptimizerNode> target, Stack<OptimizerNode> ne\n \t\t\t\tFieldList newPartitionedFieldsInCommon = partitionedFieldsInCommon;\n \t\t\t\t\n \t\t\t\t// only property which would survive is a hash partitioning on every input\n-\t\t\t\tGlobalProperties gpForInput = PactConnection.getGlobalPropertiesAfterConnection(alternative, this, ShipStrategy.FORWARD);\n+\t\t\t\tGlobalProperties gpForInput = alternative.getGlobalProperties();\n \t\t\t\tif (index == 0 && gpForInput.getPartitioning() == PartitionProperty.HASH_PARTITIONED) {\n \t\t\t\t\tnewPartitionedFieldsInCommon = gpForInput.getPartitionedFields();\n \t\t\t\t}", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/UnionNode.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/5dfc897beb99d2d8f6d7becba972ff5756b3fb19", "parent": "https://github.com/apache/flink/commit/2475c82d425b4a6a564f0d38f352a0f3b8753d72", "message": "[FLINK-2817] [streaming] FileMonitoring function logs on empty location\n\nInstead of throwing NPE when location is empty\n\nCloses #1251", "bug_id": "flink_52", "file": [{"additions": 20, "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java", "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java", "sha": "a2179238c396e621c441403f829ffe5ada392ec8", "changes": 35, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19", "patch": "@@ -17,20 +17,20 @@\n \n package org.apache.flink.streaming.api.functions.source;\n \n-import java.io.IOException;\n-import java.net.URI;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.core.fs.FileStatus;\n import org.apache.flink.core.fs.FileSystem;\n import org.apache.flink.core.fs.Path;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n public class FileMonitoringFunction implements SourceFunction<Tuple3<String, Long, Long>> {\n \tprivate static final long serialVersionUID = 1L;\n \n@@ -95,16 +95,21 @@ public void run(SourceContext<Tuple3<String, Long, Long>> ctx) throws Exception\n \n \t\tFileStatus[] statuses = fileSystem.listStatus(new Path(path));\n \n-\t\tfor (FileStatus status : statuses) {\n-\t\t\tPath filePath = status.getPath();\n-\t\t\tString fileName = filePath.getName();\n-\t\t\tlong modificationTime = status.getModificationTime();\n-\n-\t\t\tif (!isFiltered(fileName, modificationTime)) {\n-\t\t\t\tfiles.add(filePath.toString());\n-\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\tif (statuses == null) {\n+\t\t\tLOG.warn(\"Path does not exist: {}\", path);\n+\t\t} else {\n+\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\tPath filePath = status.getPath();\n+\t\t\t\tString fileName = filePath.getName();\n+\t\t\t\tlong modificationTime = status.getModificationTime();\n+\n+\t\t\t\tif (!isFiltered(fileName, modificationTime)) {\n+\t\t\t\t\tfiles.add(filePath.toString());\n+\t\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n+\n \t\treturn files;\n \t}\n ", "filename": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java"}, {"additions": 63, "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java", "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java", "sha": "2d9921ae943e149880bacc889ab87dfa456619e0", "changes": 63, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19", "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *\t http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.source;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for the {@link org.apache.flink.streaming.api.functions.source.FileMonitoringFunction}.\n+ */\n+public class FileMonitoringFunctionTest {\n+\n+\t@Test\n+\tpublic void testForEmptyLocation() throws Exception {\n+\t\tfinal FileMonitoringFunction fileMonitoringFunction\n+\t\t\t= new FileMonitoringFunction(\"?non-existing-path\", 1L, FileMonitoringFunction.WatchType.ONLY_NEW_FILES);\n+\n+        new Thread() {\n+            @Override\n+            public void run() {\n+                try {\n+                    Thread.sleep(1000L);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+                fileMonitoringFunction.cancel();\n+            }\n+        }.start();\n+\n+\t\tfileMonitoringFunction.run(\n+            new StreamSource.NonWatermarkContext<Tuple3<String, Long, Long>>(\n+                new Object(),\n+                new Output<StreamRecord<Tuple3<String, Long, Long>>>() {\n+                    @Override\n+                    public void emitWatermark(Watermark mark) { }\n+                    @Override\n+                    public void collect(StreamRecord<Tuple3<String, Long, Long>> record) { }\n+                    @Override\n+                    public void close() { }\n+                })\n+        );\n+\t}\n+}\n\\ No newline at end of file", "filename": "flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "parent": "https://github.com/apache/flink/commit/04ba9a85920bcab6c7b6c001a58c7570e987aabb", "message": "[FLINK-9795][mesos, docs] Update Mesos documentation\n\n[FLINK-9795][mesos, docs] Remove unnecessary remark about task reconciliation.\n\nThe config key high-availability.zookeeper.path.mesos-workers already has a\ndefault value. Even without explicitly setting the key, the task reconciliation\nwill work. Moreover, if there would not be a default key, the code would throw an NPE. So\neither way, the remark is only confusing the reader.\n\n[FLINK-9795][mesos, docs] Remove configuration keys from Mesos Setup page.\n\n- Remove the Mesos specific configuration keys from the Mesos Setup page because\nthey duplicate what is already on the configuration page.\n- Add missing descriptions for some of the keys that are under the Mesos section of the configuration\npage.\n- Improve formatting of the descriptions.\n\n[FLINK-9795][mesos, docs] Document which config options are only used in legacy mode.\n\n[FLINK-9795][mesos, docs] Document that mesos.initial-tasks is only needed in legacy mode.\n\n[FLINK-9795][mesos, docs] Clarify necessity of Marathon in documentation.\n\n[FLINK-9795][mesos, docs] Rewrite \"Flink's JobManager and Web Interface\" section.\n\n[FLINK-9795][mesos, docs] Add missing period at the end of sentence.\n\nThis closes #6533.", "bug_id": "flink_53", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html", "sha": "6577878674b3de994b64bd5f540ce14ba1e4d249", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/high_availability_zookeeper_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -60,7 +60,7 @@\n         <tr>\n             <td><h5>high-availability.zookeeper.path.mesos-workers</h5></td>\n             <td style=\"word-wrap: break-word;\">\"/mesos-workers\"</td>\n-            <td>ZooKeeper root path (ZNode) for Mesos workers.</td>\n+            <td>The ZooKeeper root path for persisting the Mesos worker information.</td>\n         </tr>\n         <tr>\n             <td><h5>high-availability.zookeeper.path.root</h5></td>", "filename": "docs/_includes/generated/high_availability_zookeeper_configuration.html"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html", "sha": "54e92e5680c51e766cc07c1a976eb52aca573280", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -15,17 +15,17 @@\n         <tr>\n             <td><h5>mesos.initial-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td>The initial workers to bring up when the master starts</td>\n+            <td>The initial workers to bring up when the master starts. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.master</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>The Mesos master URL. The value should be in one of the following forms: \"host:port\", \"zk://host1:port1,host2:port2,.../path\", \"zk://username:password@host1:port1,host2:port2,.../path\" or \"file:///path/to/file\"</td>\n+            <td>The Mesos master URL. The value should be in one of the following forms: <ul><li>host:port</li><li>zk://host1:port1,host2:port2,.../path</li><li>zk://username:password@host1:port1,host2:port2,.../path</li><li>file:///path/to/file</li></ul></td>\n         </tr>\n         <tr>\n             <td><h5>mesos.maximum-failed-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">-1</td>\n-            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature</td>\n+            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.artifactserver.port</h5></td>\n@@ -65,7 +65,7 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.port-assignments</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Comma-separated list of configuration keys which represent a configurable port.All port keys will dynamically get a port assigned through Mesos.</td>\n+            <td>Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.</td>\n         </tr>\n     </tbody>\n </table>", "filename": "docs/_includes/generated/mesos_configuration.html"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html", "sha": "1e67f8429d74665ac0033d38fd26a330f2c4a892", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_task_manager_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -10,12 +10,12 @@\n         <tr>\n             <td><h5>mesos.constraints.hard.hostattribute</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Constraints for task placement on mesos.</td>\n+            <td>Constraints for task placement on Mesos based on agent attributes. Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. Example: az:eu-west-1a,series:t2</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.bootstrap-cmd</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>A command which is executed before the TaskManager is started.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.container.docker.force-pull-image</h5></td>\n@@ -50,12 +50,12 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.gpus</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td></td>\n+            <td>GPUs to assign to the Mesos workers.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.hostname</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>Optional value to define the TaskManager\u2019s hostname. The pattern _TASK_ is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.mem</h5></td>", "filename": "docs/_includes/generated/mesos_task_manager_configuration.html"}, {"additions": 14, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md", "sha": "1ff8afad74ebb5edd81836abe3e4e1d53b78e21c", "changes": 83, "status": "modified", "deletions": 69, "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/ops/deployment/mesos.md?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -59,13 +59,11 @@ or configuration files. For instance, in non-containerized environments, the\n artifact server will provide the Flink binaries. What files will be served\n depends on the configuration overlay used.\n \n-### Flink's JobManager and Web Interface\n+### Flink's Dispatcher and Web Interface\n \n-The Mesos scheduler currently resides with the JobManager but will be started\n-independently of the JobManager in future versions (see\n-[FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)). The\n-proposed changes will also add a Dispatcher component which will be the central\n-point for job submission and monitoring.\n+The Dispatcher and the web interface provide a central point for monitoring,\n+job submission, and other client interaction with the cluster\n+(see [FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)).\n \n ### Startup script and configuration overlays\n \n@@ -139,7 +137,7 @@ More information about the deployment scripts can be found [here](http://mesos.a\n \n ### Installing Marathon\n \n-Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which will be necessary to run Flink in high availability (HA) mode.\n+Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which enables you to run Flink in [high availability (HA) mode](#high-availability).\n \n ### Pre-installing Flink vs Docker/Mesos containers\n \n@@ -171,8 +169,6 @@ which manage the Flink processes in a Mesos cluster:\n    It is automatically launched by the Mesos worker node to bring up a new TaskManager.\n \n In order to run the `mesos-appmaster.sh` script you have to define `mesos.master` in the `flink-conf.yaml` or pass it via `-Dmesos.master=...` to the Java process.\n-Additionally, you should define the number of task managers which are started by Mesos via `mesos.initial-tasks`.\n-This value can also be defined in the `flink-conf.yaml` or passed as a Java property.\n \n When executing `mesos-appmaster.sh`, it will create a job manager on the machine where you executed the script.\n In contrast to that, the task managers will be run as Mesos tasks in the Mesos cluster.\n@@ -188,19 +184,21 @@ For example:\n         -Djobmanager.heap.mb=1024 \\\n         -Djobmanager.rpc.port=6123 \\\n         -Drest.port=8081 \\\n-        -Dmesos.initial-tasks=10 \\\n         -Dmesos.resourcemanager.tasks.mem=4096 \\\n         -Dtaskmanager.heap.mb=3500 \\\n         -Dtaskmanager.numberOfTaskSlots=2 \\\n         -Dparallelism.default=10\n \n+<div class=\"alert alert-info\">\n+  <strong>Note:</strong> If Flink is in <a href=\"{{ site.baseurl }}/ops/config.html#legacy\">legacy mode</a>,\n+  you should additionally define the number of task managers that are started by Mesos via\n+  <a href=\"{{ site.baseurl }}/ops/config.html#mesos-initial-tasks\"><code>mesos.initial-tasks</code></a>.\n+</div>\n \n ### High Availability\n \n You will need to run a service like Marathon or Apache Aurora which takes care of restarting the Flink master process in case of node or process failures.\n-In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html)\n-\n-For the reconciliation of tasks to work correctly, please also set `high-availability.zookeeper.path.mesos-workers` to a valid Zookeeper path.\n+In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html).\n \n #### Marathon\n \n@@ -211,7 +209,7 @@ Here is an example configuration for Marathon:\n \n     {\n         \"id\": \"flink\",\n-        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.initial-tasks=1 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n+        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n         \"cpus\": 1.0,\n         \"mem\": 1024\n     }\n@@ -220,60 +218,7 @@ When running Flink with Marathon, the whole Flink cluster including the job mana\n \n ### Configuration parameters\n \n-`mesos.initial-tasks`: The initial workers to bring up when the master starts (**DEFAULT**: The number of workers specified at cluster startup).\n-\n-`mesos.constraints.hard.hostattribute`: Constraints for task placement on Mesos based on agent attributes (**DEFAULT**: None).\n-Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target\n-mesos agents.  Example: `az:eu-west-1a,series:t2`\n-\n-`mesos.maximum-failed-tasks`: The maximum number of failed workers before the cluster fails (**DEFAULT**: Number of initial workers).\n-May be set to -1 to disable this feature.\n-\n-`mesos.master`: The Mesos master URL. The value should be in one of the following forms:\n-\n-* `host:port`\n-* `zk://host1:port1,host2:port2,.../path`\n-* `zk://username:password@host1:port1,host2:port2,.../path`\n-* `file:///path/to/file`\n-\n-`mesos.failover-timeout`: The failover timeout in seconds for the Mesos scheduler, after which running tasks are automatically shut down (**DEFAULT:** 600).\n-\n-`mesos.resourcemanager.artifactserver.port`:The config parameter defining the Mesos artifact server port to use. Setting the port to 0 will let the OS choose an available port.\n-\n-`mesos.resourcemanager.framework.name`: Mesos framework name (**DEFAULT:** Flink)\n-\n-`mesos.resourcemanager.framework.role`: Mesos framework role definition (**DEFAULT:** *)\n-\n-`high-availability.zookeeper.path.mesos-workers`: The ZooKeeper root path for persisting the Mesos worker information.\n-\n-`mesos.resourcemanager.framework.principal`: Mesos framework principal (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.secret`: Mesos framework secret (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.user`: Mesos framework user (**DEFAULT:**\"\")\n-\n-`mesos.resourcemanager.artifactserver.ssl.enabled`: Enables SSL for the Flink artifact server (**DEFAULT**: true). Note that `security.ssl.enabled` also needs to be set to `true` encryption to enable encryption.\n-\n-`mesos.resourcemanager.tasks.mem`: Memory to assign to the Mesos workers in MB (**DEFAULT**: 1024)\n-\n-`mesos.resourcemanager.tasks.cpus`: CPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.gpus`: GPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.container.type`: Type of the containerization used: \"mesos\" or \"docker\" (DEFAULT: mesos);\n-\n-`mesos.resourcemanager.tasks.container.image.name`: Image name to use for the container (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.volumes`: A comma separated list of `[host_path:]`container_path`[:RO|RW]`. This allows for mounting additional volumes into your container. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.parameters`: Custom parameters to be passed into docker run command when using the docker containerizer. Comma separated list of `key=value` pairs. `value` may contain '=' (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.uris`: A comma separated list of URIs of custom artifacts to be downloaded into the sandbox of Mesos workers. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.force-pull-image`: Instruct the docker containerizer to forcefully pull the image rather than reuse a cached version. (**DEFAULT**: false)\n-\n-`mesos.resourcemanager.tasks.hostname`: Optional value to define the TaskManager's hostname. The pattern `_TASK_` is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. `_TASK_.flink-service.mesos`) for name lookups. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.bootstrap-cmd`: A command which is executed before the TaskManager is started (**NO DEFAULT**).\n+For a list of Mesos specific configuration, refer to the [Mesos section]({{ site.baseurl }}/ops/config.html#mesos)\n+of the configuration documentation.\n \n {% top %}", "filename": "docs/ops/deployment/mesos.md"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java", "sha": "787efffa3ede1004219cfd4d5192cb75c4e7b692", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.annotation.docs.ConfigGroup;\n import org.apache.flink.annotation.docs.ConfigGroups;\n import org.apache.flink.annotation.docs.Documentation;\n+import org.apache.flink.configuration.description.Description;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -157,7 +158,9 @@\n \t\t\tkey(\"high-availability.zookeeper.path.mesos-workers\")\n \t\t\t.defaultValue(\"/mesos-workers\")\n \t\t\t.withDeprecatedKeys(\"recovery.zookeeper.path.mesos-workers\")\n-\t\t\t.withDescription(\"ZooKeeper root path (ZNode) for Mesos workers.\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The ZooKeeper root path for persisting the Mesos worker information.\")\n+\t\t\t\t.build());\n \n \t// ------------------------------------------------------------------------\n \t//  ZooKeeper Client Settings", "filename": "flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java"}, {"additions": 23, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java", "sha": "426a891e814237e410169e04c8210e2b4da9667b", "changes": 31, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -19,6 +19,9 @@\n package org.apache.flink.mesos.configuration;\n \n import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.description.Description;\n+import org.apache.flink.configuration.description.LinkElement;\n+import org.apache.flink.configuration.description.TextElement;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -33,7 +36,10 @@\n \tpublic static final ConfigOption<Integer> INITIAL_TASKS =\n \t\tkey(\"mesos.initial-tasks\")\n \t\t\t.defaultValue(0)\n-\t\t\t.withDescription(\"The initial workers to bring up when the master starts\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The initial workers to bring up when the master starts. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The maximum number of failed Mesos tasks before entirely stopping\n@@ -44,8 +50,10 @@\n \tpublic static final ConfigOption<Integer> MAX_FAILED_TASKS =\n \t\tkey(\"mesos.maximum-failed-tasks\")\n \t\t\t.defaultValue(-1)\n-\t\t\t.withDescription(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable\" +\n-\t\t\t\t\" this feature\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The Mesos master URL.\n@@ -63,9 +71,14 @@\n \tpublic static final ConfigOption<String> MASTER_URL =\n \t\tkey(\"mesos.master\")\n \t\t\t.noDefaultValue()\n-\t\t\t.withDescription(\"The Mesos master URL. The value should be in one of the following forms:\" +\n-\t\t\t\t\" \\\"host:port\\\", \\\"zk://host1:port1,host2:port2,.../path\\\",\" +\n-\t\t\t\t\" \\\"zk://username:password@host1:port1,host2:port2,.../path\\\" or \\\"file:///path/to/file\\\"\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The Mesos master URL. The value should be in one of the following forms: \")\n+\t\t\t\t.list(\n+\t\t\t\t\tTextElement.text(\"host:port\"),\n+\t\t\t\t\tTextElement.text(\"zk://host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"zk://username:password@host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"file:///path/to/file\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The failover timeout for the Mesos scheduler, after which running tasks are automatically shut down.\n@@ -125,7 +138,9 @@\n \t */\n \tpublic static final ConfigOption<String> PORT_ASSIGNMENTS = key(\"mesos.resourcemanager.tasks.port-assignments\")\n \t\t.defaultValue(\"\")\n-\t\t.withDescription(\"Comma-separated list of configuration keys which represent a configurable port.\" +\n-\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Comma-separated list of configuration keys which represent a configurable port. \" +\n+\t\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\")\n+\t\t\t.build());\n \n }", "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java"}, {"additions": 18, "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java", "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java", "sha": "03156297188617e6670e72900264658fe6ec78e4", "changes": 22, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c", "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.IllegalConfigurationException;\n import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.configuration.description.Description;\n import org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters;\n import org.apache.flink.util.Preconditions;\n \n@@ -65,7 +66,8 @@\n \n \tpublic static final ConfigOption<Integer> MESOS_RM_TASKS_GPUS =\n \t\tkey(\"mesos.resourcemanager.tasks.gpus\")\n-\t\t.defaultValue(0);\n+\t\t.defaultValue(0)\n+\t\t.withDescription(Description.builder().text(\"GPUs to assign to the Mesos workers.\").build());\n \n \tpublic static final ConfigOption<String> MESOS_RM_CONTAINER_TYPE =\n \t\tkey(\"mesos.resourcemanager.tasks.container.type\")\n@@ -79,15 +81,23 @@\n \n \tpublic static final ConfigOption<String> MESOS_TM_HOSTNAME =\n \t\tkey(\"mesos.resourcemanager.tasks.hostname\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Optional value to define the TaskManager\u2019s hostname. \" +\n+\t\t\t\t\"The pattern _TASK_ is replaced by the actual id of the Mesos task. \" +\n+\t\t\t\t\"This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.taskmanager-cmd\")\n \t\t.defaultValue(\"$FLINK_HOME/bin/mesos-taskmanager.sh\"); // internal\n \n \tpublic static final ConfigOption<String> MESOS_TM_BOOTSTRAP_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.bootstrap-cmd\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"A command which is executed before the TaskManager is started.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_URIS =\n \t\tkey(\"mesos.resourcemanager.tasks.uris\")\n@@ -116,7 +126,11 @@\n \tpublic static final ConfigOption<String> MESOS_CONSTRAINTS_HARD_HOSTATTR =\n \t\tkey(\"mesos.constraints.hard.hostattribute\")\n \t\t.noDefaultValue()\n-\t\t.withDescription(\"Constraints for task placement on mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Constraints for task placement on Mesos based on agent attributes. \" +\n+\t\t\t\t\"Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. \" +\n+\t\t\t\t\"Example: az:eu-west-1a,series:t2\")\n+\t\t\t.build());\n \n \t/**\n \t * Value for {@code MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE} setting. Tells to use the Mesos containerizer.", "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "parent": "https://github.com/apache/flink/commit/b746f452e7187dad08340b9cfdc2fa18a516a6c7", "message": "Fixes bugs where the TypeExtractor throws an NPE instead of the operators", "bug_id": "flink_54", "file": [{"additions": 15, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java", "sha": "758cbf2133192b330a4719ef71ad3d718a6ebc23", "changes": 15, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -130,6 +130,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> MapOperator<T, R> map(MapFunction<T, R> mapper) {\n+\t\tif (mapper == null) {\n+\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n+\t\t}\n \t\treturn new MapOperator<T, R>(this, mapper);\n \t}\n \t\n@@ -146,6 +149,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> FlatMapOperator<T, R> flatMap(FlatMapFunction<T, R> flatMapper) {\n+\t\tif (flatMapper == null) {\n+\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n+\t\t}\n \t\treturn new FlatMapOperator<T, R>(this, flatMapper);\n \t}\n \t\n@@ -163,6 +169,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic FilterOperator<T> filter(FilterFunction<T> filter) {\n+\t\tif (filter == null) {\n+\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n+\t\t}\n \t\treturn new FilterOperator<T>(this, filter);\n \t}\n \t\n@@ -229,6 +238,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -246,6 +258,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java", "sha": "ca4b1db6cb9f978ab5ab60580490a4a7087e28c3", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -439,6 +439,9 @@ private CoGroupOperatorWithoutFunction(Keys<I2> keys2) {\n \t\t\t\t * @see DataSet\n \t\t\t\t */\n \t\t\t\tpublic <R> CoGroupOperator<I1, I2, R> with(CoGroupFunction<I1, I2, R> function) {\n+\t\t\t\t\tif (function == null) {\n+\t\t\t\t\t\tthrow new NullPointerException(\"CoGroup function must not be null.\");\n+\t\t\t\t\t}\n \t\t\t\t\tTypeInformation<R> returnType = TypeExtractor.getCoGroupReturnTypes(function, input1.getType(), input2.getType());\n \t\t\t\t\treturn new CoGroupOperator<I1, I2, R>(input1, input2, keys1, keys2, function, returnType);\n \t\t\t\t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java", "sha": "3566224eac5ee4b6a5da772318d054b63277a594", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -113,6 +113,9 @@ public DefaultCross(DataSet<I1> input1, DataSet<I2> input2) {\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> CrossOperator<I1, I2, R> with(CrossFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Cross function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getCrossReturnTypes(function, input1.getType(), input2.getType());\n \t\t\treturn new CrossOperator<I1, I2, R>(input1, input2, function, returnType);\n \t\t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java", "sha": "adbe77f3782e234af68cfbf53c1f9476eda43c84", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -34,10 +34,6 @@\n \tpublic FilterOperator(DataSet<T> input, FilterFunction<T> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java", "sha": "32f2343fa82fff567cf03d3a4f66224dbcc3c0b8", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -37,10 +37,6 @@\n \tpublic FlatMapOperator(DataSet<IN> input, FlatMapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getFlatMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java", "sha": "992cc0a74f5434945e25fed90d03006e42ffa9ee", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -421,6 +421,9 @@ protected DefaultJoin(DataSet<I1> input1, DataSet<I2> input2,\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> EquiJoin<I1, I2, R> with(JoinFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Join function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getJoinReturnTypes(function, getInput1Type(), getInput2Type());\n \t\t\treturn new EquiJoin<I1, I2, R>(getInput1(), getInput2(), getKeys1(), getKeys2(), function, returnType, getJoinHint());\n \t\t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java", "sha": "00bbb27f89f3c85fb801d56ec32a8f518d69e1a1", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -39,10 +39,6 @@\n \tpublic MapOperator(DataSet<IN> input, MapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java", "sha": "e001d910b6bb292d9f885b2f4e4af19e5fab5422", "changes": 8, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -58,10 +58,6 @@\n \tpublic ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getGroupReduceReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\tcheckCombinability();\n@@ -76,10 +72,6 @@ public ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> funct\n \tpublic ReduceGroupOperator(Grouping<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input != null ? input.getDataSet() : null, TypeExtractor.getGroupReduceReturnTypes(function, input.getDataSet().getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\tcheckCombinability();", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java"}, {"additions": 0, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java", "sha": "6056e8b545ee7d21a2482c990c3c4b4c524f5786", "changes": 8, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -53,10 +53,6 @@\n \tpublic ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\t\n@@ -67,10 +63,6 @@ public ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \tpublic ReduceOperator(Grouping<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input.getDataSet(), input.getDataSet().getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\t", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java", "sha": "dc26a2b697345e49b2f134f416831033cd7bf38a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -72,6 +72,9 @@ public SortedGrouping(DataSet<T> set, Keys<T> keys, int field, Order order) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java"}, {"additions": 6, "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java", "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java", "sha": "95e40bc44e3abbad5f6a307be680381cb8b84f94", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc", "patch": "@@ -64,6 +64,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -81,6 +84,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n ", "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/bc1432a2f605f97a27b0075c9f54b46c74e42d56", "parent": "https://github.com/apache/flink/commit/21f47d9c69441c17b5f90ea2c7cb8f4d47f7fcb5", "message": "[FLINK-1533] [runtime] Fixes NPE in the scheduler where the allocated shared slots are not properly checked.\n\nThis closes #388.", "bug_id": "flink_55", "file": [{"additions": 12, "raw_url": "https://github.com/apache/flink/raw/bc1432a2f605f97a27b0075c9f54b46c74e42d56/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java", "blob_url": "https://github.com/apache/flink/blob/bc1432a2f605f97a27b0075c9f54b46c74e42d56/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java", "sha": "3d7d29a3e9bf3c6b95e1df50d6b2c4a81b8f4c65", "changes": 18, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java?ref=bc1432a2f605f97a27b0075c9f54b46c74e42d56", "patch": "@@ -432,17 +432,23 @@ protected SimpleSlot getFreeSubSlotForTask(ExecutionVertex vertex,\n \t\t\t\t// root SharedSlot\n \t\t\t\tSharedSlot sharedSlot = instanceToUse.allocateSharedSlot(vertex.getJobId(), groupAssignment, groupID);\n \n-\t\t\t\t// If constraint != null, then slot nested in a SharedSlot nested in sharedSlot\n-\t\t\t\t// If constraint == null, then slot nested in sharedSlot\n-\t\t\t\tSimpleSlot slot = groupAssignment.addSharedSlotAndAllocateSubSlot(sharedSlot, locality, groupID, constraint);\n-\n \t\t\t\t// if the instance has further available slots, re-add it to the set of available resources.\n \t\t\t\tif (instanceToUse.hasResourcesAvailable()) {\n \t\t\t\t\tthis.instancesWithAvailableResources.add(instanceToUse);\n \t\t\t\t}\n \n-\t\t\t\tif (slot != null) {\n-\t\t\t\t\treturn slot;\n+\t\t\t\tif(sharedSlot != null){\n+\t\t\t\t\t// If constraint != null, then slot nested in a SharedSlot nested in sharedSlot\n+\t\t\t\t\t// If constraint == null, then slot nested in sharedSlot\n+\t\t\t\t\tSimpleSlot slot = groupAssignment.addSharedSlotAndAllocateSubSlot(sharedSlot,\n+\t\t\t\t\t\t\tlocality, groupID, constraint);\n+\n+\t\t\t\t\tif(slot != null){\n+\t\t\t\t\t\treturn slot;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t// release shared slot\n+\t\t\t\t\t\tsharedSlot.releaseSlot();\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (InstanceDiedException e) {", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/Scheduler.java"}, {"additions": 36, "raw_url": "https://github.com/apache/flink/raw/bc1432a2f605f97a27b0075c9f54b46c74e42d56/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java", "blob_url": "https://github.com/apache/flink/blob/bc1432a2f605f97a27b0075c9f54b46c74e42d56/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java", "sha": "32c810cf74b7d67e024b7406febc04d1471a8aa4", "changes": 54, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java?ref=bc1432a2f605f97a27b0075c9f54b46c74e42d56", "patch": "@@ -65,38 +65,56 @@ public SimpleSlot addSharedSlotAndAllocateSubSlot(SharedSlot sharedSlot, Localit\n \n \t\t\tSimpleSlot subSlot = null;\n \n-\t\t\tif(constraint == null){\n+\t\t\tif (constraint == null) {\n \t\t\t\t// allocate us a sub slot to return\n \t\t\t\tsubSlot = sharedSlot.allocateSubSlot(groupId);\n \t\t\t} else {\n \t\t\t\t// we need a colocation slot --> a SimpleSlot nested in a SharedSlot to host other colocated tasks\n \t\t\t\tSharedSlot constraintGroupSlot = sharedSlot.allocateSharedSlot(groupId);\n-\t\t\t\tsubSlot = constraintGroupSlot.allocateSubSlot(null);\n+\n+\t\t\t\tif(constraintGroupSlot == null) {\n+\t\t\t\t\tsubSlot = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tsubSlot = constraintGroupSlot.allocateSubSlot(null);\n+\n+\t\t\t\t\t// could not create a sub slot --> release constraintGroupSlot\n+\t\t\t\t\tif(subSlot == null){\n+\t\t\t\t\t\tconstraintGroupSlot.releaseSlot();\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \n-\t\t\t// preserve the locality information\n-\t\t\tsubSlot.setLocality(locality);\n+\t\t\t// if sharedSlot is dead, but this should never happen since we just created a fresh\n+\t\t\t// SharedSlot in the caller\n+\t\t\tif(subSlot == null) {\n+\t\t\t\tLOG.warn(\"Could not allocate a sub slot.\");\n \n-\t\t\tboolean entryForNewJidExists = false;\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\t// preserve the locality information\n+\t\t\t\tsubSlot.setLocality(locality);\n \n-\t\t\t// let the other vertex types know about this one as well\n-\t\t\tfor (Map.Entry<AbstractID, Map<Instance, List<SharedSlot>>> entry : availableSlotsPerJid.entrySet()) {\n+\t\t\t\tboolean entryForNewJidExists = false;\n \n-\t\t\t\tif (entry.getKey().equals(groupId)) {\n-\t\t\t\t\tentryForNewJidExists = true;\n-\t\t\t\t\tcontinue;\n+\t\t\t\t// let the other vertex types know about this one as well\n+\t\t\t\tfor (Map.Entry<AbstractID, Map<Instance, List<SharedSlot>>> entry : availableSlotsPerJid.entrySet()) {\n+\n+\t\t\t\t\tif (entry.getKey().equals(groupId)) {\n+\t\t\t\t\t\tentryForNewJidExists = true;\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tMap<Instance, List<SharedSlot>> available = entry.getValue();\n+\t\t\t\t\tputIntoMultiMap(available, location, sharedSlot);\n \t\t\t\t}\n \n-\t\t\t\tMap<Instance, List<SharedSlot>> available = entry.getValue();\n-\t\t\t\tputIntoMultiMap(available, location, sharedSlot);\n-\t\t\t}\n+\t\t\t\t// make sure an empty entry exists for this group, if no other entry exists\n+\t\t\t\tif (!entryForNewJidExists) {\n+\t\t\t\t\tavailableSlotsPerJid.put(groupId, new LinkedHashMap<Instance, List<SharedSlot>>());\n+\t\t\t\t}\n \n-\t\t\t// make sure an empty entry exists for this group, if no other entry exists\n-\t\t\tif (!entryForNewJidExists) {\n-\t\t\t\tavailableSlotsPerJid.put(groupId, new LinkedHashMap<Instance, List<SharedSlot>>());\n+\t\t\t\treturn subSlot;\n \t\t\t}\n-\n-\t\t\treturn subSlot;\n \t\t}\n \t}\n ", "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroupAssignment.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "parent": "https://github.com/apache/flink/commit/dc1954428c82c85bb6a731edc7c68c248ce1fc98", "message": "Fixed small bugs in Compiler (NPE, Maximal Costs of IP, Cloning of IPs)", "bug_id": "flink_56", "file": [{"additions": 8, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java", "sha": "a357491a6c09f53b61a16b7d11040c4d85c6399e", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -82,4 +82,12 @@ public Ordering createNewOrderingUpToIndex(int exclusiveIndex) {\n \t\t}\n \t\treturn newOrdering;\n \t}\n+\t\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic Ordering clone() {\n+\t\tOrdering newOrdering = new Ordering();\n+\t\tnewOrdering.indexes = (ArrayList<Integer>) this.indexes.clone();\n+\t\tnewOrdering.orders = (ArrayList<Order>) this.orders.clone();\n+\t\treturn this;\n+\t}\n }", "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java"}, {"additions": 12, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java", "sha": "d18269622a12284567778195fd65a098c26de900", "changes": 19, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -236,15 +236,13 @@ public boolean isMetBy(GlobalProperties other) {\n \t\t\tif (other.partitionedFields == null) {\n \t\t\t\treturn false;\n \t\t\t}\n-\t\t\tif (this.partitionedFields.size() > otherPartitionedFields.size()) {\n+\t\t\tif (this.partitionedFields.size() < otherPartitionedFields.size()) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\t\n-\t\t\tfor (Integer fieldIndex : this.partitionedFields) {\n-\t\t\t\tif (otherPartitionedFields.contains(fieldIndex) == false) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\t\n+\t\t\tif (this.partitionedFields.containsAll(otherPartitionedFields) == false) {\n+\t\t\t\treturn false;\n+\t\t\t}\n \t\t}\n \t\t\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -322,7 +320,14 @@ public String toString() {\n \t * @see java.lang.Object#clone()\n \t */\n \tpublic GlobalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (GlobalProperties) super.clone();\n+\t\tGlobalProperties newProps = (GlobalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.partitionedFields != null) {\n+\t\t\tnewProps.partitionedFields = (FieldSet) this.partitionedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java"}, {"additions": 13, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java", "sha": "8f8b5b2cd2bc6137d804e1c90537b1fe86e99a24", "changes": 18, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -223,11 +223,12 @@ public boolean isMetBy(LocalProperties other) {\n \t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\tgroupingFulfilled = true;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif (groupingFulfilled == false) {\n+\t\t\t\treturn false;\n \t\t\t}\n-\t\t}\n-\n-\t\tif (groupingFulfilled == false) {\n-\t\t\treturn false;\n \t\t}\n \t\t// check the order\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -296,7 +297,14 @@ public String toString() {\n \t */\n \t@Override\n \tpublic LocalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (LocalProperties) super.clone();\n+\t\tLocalProperties newProps = (LocalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.groupedFields != null) {\n+\t\t\tnewProps.groupedFields = (FieldSet) this.groupedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java"}, {"additions": 12, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java", "sha": "062d470cd8365a2c3edb434b7cd976e86bbbe42d", "changes": 17, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -48,8 +48,8 @@\n \t * properties and a maximal cost of infinite.\n \t */\n \tpublic InterestingProperties() {\n-\t\t// instantiate the maximal costs to the possible maximum\n-\t\tthis.maximalCosts = new Costs(Long.MAX_VALUE, Long.MAX_VALUE);\n+\t\t// instantiate the maximal costs to 0\n+\t\tthis.maximalCosts = new Costs(0, 0);\n \n \t\tthis.globalProps = new GlobalProperties();\n \t\tthis.localProps = new LocalProperties();\n@@ -314,11 +314,18 @@ public static void mergeUnionOfInterestingProperties(List<InterestingProperties>\n \t\tList<InterestingProperties> preserved = new ArrayList<InterestingProperties>();\n \t\t\n \t\tfor (InterestingProperties p : props) {\n-\t\t\tboolean nonTrivial = p.getGlobalProperties().filterByNodesConstantSet(node, input);\n-\t\t\tnonTrivial |= p.getLocalProperties().filterByNodesConstantSet(node, input);\n+\t\t\tGlobalProperties preservedGp = p.getGlobalProperties().createCopy();\n+\t\t\tLocalProperties preservedLp = p.getLocalProperties().createCopy();\n+\t\t\tboolean nonTrivial = preservedGp.filterByNodesConstantSet(node, input);\n+\t\t\tnonTrivial |= preservedLp.filterByNodesConstantSet(node, input);\n \n \t\t\tif (nonTrivial) {\n-\t\t\t\tpreserved.add(p);\n+\t\t\t\ttry {\n+\t\t\t\t\tpreserved.add(new InterestingProperties(p.getMaximalCosts().clone(), preservedGp, preservedLp));\n+\t\t\t\t} catch (CloneNotSupportedException cnse) {\n+\t\t\t\t\t// should never happen, but propagate just in case\n+\t\t\t\t\tthrow new RuntimeException(cnse);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n ", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java"}, {"additions": 4, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java", "sha": "ecff08f1e0759b3e391a25689d02aa46442d4146", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -476,6 +476,10 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:\n \t\t\treturn (constantSet != null && Arrays.binarySearch(constantSet, fieldNumber) >= 0);", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java", "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java", "sha": "db8216a0d9bc92c12877d5a40b5d8ffbdfc7d75a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a", "patch": "@@ -858,6 +858,9 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n \t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:", "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "parent": "https://github.com/apache/flink/commit/d85b8f06a7fc66bf8e319c930f568c455a78255d", "message": "* fixed some bugs (endless recursion in latency path construction, some NPEs)\n* added some toString() functions", "bug_id": "flink_57", "file": [{"additions": 5, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementGroupVertex.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementGroupVertex.java", "sha": "ba12247dbcc017f8ce58eee37304f7108fd71fb2", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementGroupVertex.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -415,4 +415,9 @@ public void write(final DataOutput out) throws IOException {\n \n \t\treturn predecessors;\n \t}\n+\t\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn String.format(\"ManagementGroupVertex(%s)\", getName());\n+\t}\n }", "filename": "nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementGroupVertex.java"}, {"additions": 5, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementVertex.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementVertex.java", "sha": "5805cc6c146e62772caca46cb11e881017d4ca79", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementVertex.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -370,4 +370,9 @@ public void write(final DataOutput out) throws IOException {\n \t\tStringRecord.writeString(out, this.instanceType);\n \t\tStringRecord.writeString(out, this.checkpointState);\n \t}\n+\t\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn String.format(\"ManagementVertex(%s_%d)\", getGroupVertex().getName(), indexInGroup);\n+\t}\n }", "filename": "nephele/nephele-management/src/main/java/eu/stratosphere/nephele/managementgraph/ManagementVertex.java"}, {"additions": 5, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/StreamingJobManagerPlugin.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/StreamingJobManagerPlugin.java", "sha": "4900bacb724193001d77d4dcc30fc969d4251687", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/StreamingJobManagerPlugin.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -34,13 +34,13 @@\n \n public class StreamingJobManagerPlugin implements JobManagerPlugin, JobStatusListener {\n \n-\tprivate ConcurrentHashMap<JobID, LatencyOptimizerThread> latencyOptimizerThreads = new ConcurrentHashMap<JobID, LatencyOptimizerThread>();\n-\n \t/**\n \t * The log object.\n \t */\n \tprivate static final Log LOG = LogFactory.getLog(StreamingJobManagerPlugin.class);\n \n+\tprivate ConcurrentHashMap<JobID, LatencyOptimizerThread> latencyOptimizerThreads = new ConcurrentHashMap<JobID, LatencyOptimizerThread>();\n+\n \tStreamingJobManagerPlugin(final Configuration pluginConfiguration) {\n \t}\n \n@@ -78,7 +78,7 @@ private void shutdownLatencyOptimizerThreads() {\n \t\twhile (iter.hasNext()) {\n \t\t\tLatencyOptimizerThread thread = iter.next();\n \t\t\tthread.interrupt();\n-\t\t\t\n+\n \t\t\t// also removes jobID mappings from underlying map\n \t\t\titer.remove();\n \t\t}\n@@ -94,7 +94,7 @@ public void sendData(final IOReadableWritable data) throws IOException {\n \t\t\tLOG.error(\"Received unexpected data of type \" + data);\n \t\t\treturn;\n \t\t}\n-\t\t\n+\n \t\tAbstractStreamingData streamingData = (AbstractStreamingData) data;\n \t\tLatencyOptimizerThread optimizerThread = latencyOptimizerThreads.get(streamingData.getJobID());\n \t\toptimizerThread.handOffStreamingData(streamingData);\n@@ -118,7 +118,7 @@ public IOReadableWritable requestData(final IOReadableWritable data) throws IOEx\n \tpublic void jobStatusHasChanged(ExecutionGraph executionGraph,\n \t\t\tInternalJobStatus newJobStatus,\n \t\t\tString optionalMessage) {\n-\t\t\n+\n \t\tif (newJobStatus == InternalJobStatus.FAILED\n \t\t\t|| newJobStatus == InternalJobStatus.CANCELED\n \t\t\t|| newJobStatus == InternalJobStatus.FINISHED) {", "filename": "nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/StreamingJobManagerPlugin.java"}, {"additions": 6, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyModel.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyModel.java", "sha": "548fceb264d66cd9b9a8057f18bcaa682b3998c7", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyModel.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -1,11 +1,16 @@\n package eu.stratosphere.nephele.streaming.latency;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n import eu.stratosphere.nephele.executiongraph.ExecutionGraph;\n import eu.stratosphere.nephele.executiongraph.ExecutionGroupVertex;\n import eu.stratosphere.nephele.streaming.PathLatency;\n \n public class LatencyModel {\n \t\n+\tprivate static Log LOG = LogFactory.getLog(LatencyModel.class);  \n+\t\n \tprivate ExecutionGraph executionGraph;\n \t\n \tprivate LatencySubgraph latencySubgraph;\n@@ -22,6 +27,6 @@ public LatencyModel(ExecutionGraph executionGraph) {\n \t}\n \t\n \tpublic void refreshEdgeLatency(PathLatency pathLatency) {\n-\t\t\n+\t\tLOG.info(\"Received \" + pathLatency);\n \t}\n }", "filename": "nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyModel.java"}, {"additions": 16, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyOptimizerThread.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyOptimizerThread.java", "sha": "cebca038a5063f4bd5a771613eaa679b4a1cd5d8", "changes": 23, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyOptimizerThread.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -2,29 +2,36 @@\n \n import java.util.concurrent.LinkedBlockingQueue;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n import eu.stratosphere.nephele.executiongraph.ExecutionGraph;\n import eu.stratosphere.nephele.streaming.AbstractStreamingData;\n import eu.stratosphere.nephele.streaming.PathLatency;\n \n public class LatencyOptimizerThread extends Thread {\n-\t\n+\n+\tprivate Log LOG = LogFactory.getLog(LatencyOptimizerThread.class);\n+\n \tprivate LinkedBlockingQueue<AbstractStreamingData> streamingDataQueue;\n-\t\n+\n \tprivate ExecutionGraph executionGraph;\n-\t\n+\n \tprivate LatencyModel latencyModel;\n-\t\n+\n \tpublic LatencyOptimizerThread(ExecutionGraph executionGraph) {\n+\t\tthis.executionGraph = executionGraph;\n \t\tthis.latencyModel = new LatencyModel(executionGraph);\n \t\tthis.streamingDataQueue = new LinkedBlockingQueue<AbstractStreamingData>();\n \t}\n-\t\n+\n \tpublic void run() {\n+\t\tLOG.info(\"Started optimizer thread for job \" + executionGraph.getJobName());\n \n \t\ttry {\n \t\t\twhile (!interrupted()) {\n \t\t\t\tAbstractStreamingData streamingData = streamingDataQueue.take();\n-\t\t\t\t\n+\n \t\t\t\tif (streamingData instanceof PathLatency) {\n \t\t\t\t\tlatencyModel.refreshEdgeLatency((PathLatency) streamingData);\n \t\t\t\t}\n@@ -33,8 +40,10 @@ public void run() {\n \n \t\t} catch (InterruptedException e) {\n \t\t}\n+\n+\t\tLOG.info(\"Stopped optimizer thread for job \" + executionGraph.getJobName());\n \t}\n-\t\n+\n \tpublic void handOffStreamingData(AbstractStreamingData data) {\n \t\tstreamingDataQueue.add(data);\n \t}", "filename": "nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyOptimizerThread.java"}, {"additions": 18, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyPath.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyPath.java", "sha": "a980321b19db4b459ee86a4de523359672ad6313", "changes": 19, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyPath.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -49,7 +49,7 @@ public ManagementVertex getBegin() {\n \t}\n \n \tpublic ManagementVertex getEnd() {\n-\t\treturn pathVertices.getFirst();\n+\t\treturn pathVertices.getLast();\n \t}\n \n \tpublic ManagementEdge getIngoingEdge(ManagementVertex vertex) {\n@@ -106,4 +106,21 @@ public long refreshPathLatency() {\n \tpublic long getPathLatencyInMillis() {\n \t\treturn this.pathLatencyInMillis;\n \t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\tStringBuilder builder = new StringBuilder();\n+\t\tbuilder.append(\"LatencyPath[\");\n+\t\tManagementVertex previous = null;\n+\t\tfor (ManagementVertex vertex : pathVertices) {\n+\t\t\tif (previous != null) {\n+\t\t\t\tbuilder.append(\"->\");\n+\t\t\t}\n+\t\t\tbuilder.append(vertex);\n+\t\t\tprevious = vertex;\n+\t\t}\n+\t\tbuilder.append(\"]\");\n+\n+\t\treturn builder.toString();\n+\t}\n }", "filename": "nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencyPath.java"}, {"additions": 7, "raw_url": "https://github.com/apache/flink/raw/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencySubgraph.java", "blob_url": "https://github.com/apache/flink/blob/bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencySubgraph.java", "sha": "ea10dc2183aebd00d393db92beb26ee25be066c7", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencySubgraph.java?ref=bcdd97dfbb8df67f7a4e5a69d0a3a096075ba0ba", "patch": "@@ -4,6 +4,9 @@\n import java.util.LinkedList;\n import java.util.List;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n import eu.stratosphere.nephele.executiongraph.ExecutionGraph;\n import eu.stratosphere.nephele.executiongraph.ExecutionGroupVertex;\n import eu.stratosphere.nephele.executiongraph.ExecutionVertex;\n@@ -24,16 +27,18 @@\n  * @author Bjoern Lohrmann\n  */\n public class LatencySubgraph {\n+\t\n+\tprivate static Log LOG = LogFactory.getLog(LatencySubgraph.class);\n \n \tprivate ManagementGroupVertex subgraphStart;\n \n \tprivate ManagementGroupVertex subgraphEnd;\n \n \tprivate List<LatencyPath> latencyPaths;\n \n-\tprivate HashMap<ManagementVertexID, VertexLatency> vertexLatencies;\n+\tprivate HashMap<ManagementVertexID, VertexLatency> vertexLatencies = new HashMap<ManagementVertexID, VertexLatency>();\n \n-\tprivate HashMap<ManagementEdgeID, EdgeLatency> edgeLatencies;\n+\tprivate HashMap<ManagementEdgeID, EdgeLatency> edgeLatencies = new HashMap<ManagementEdgeID, EdgeLatency>();\n \n \tpublic LatencySubgraph(ExecutionGraph executionGraph, ExecutionGroupVertex subgraphStart,\n \t\t\tExecutionGroupVertex subgraphEnd) {", "filename": "nephele/nephele-streaming/src/main/java/eu/stratosphere/nephele/streaming/latency/LatencySubgraph.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8", "parent": "https://github.com/apache/flink/commit/47862afbef98faee61e07ca4a00f41f34a764cf5", "message": "[FLINK-6945] Fix TaskCancelAsyncProducerConsumerITCase by removing race condition\n\nThe TaskCacnelAsyncProducerConsumerITCase#testCancelAsyncProducerAndConsumer test case\nsometimes failed with a NPE because of a race condition. The problem was that some\ninvokables set static fields which are checked in the main thread. Since we checked\nthe wrong field, the one for the consumer, after making sure that the producer\nis running, this could lead to a race condition if the consumer wasn't running yet.\n\nThis closes #4139.", "bug_id": "flink_58", "file": [{"additions": 3, "raw_url": "https://github.com/apache/flink/raw/0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java", "blob_url": "https://github.com/apache/flink/blob/0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java", "sha": "69f1a4998cf5efbe51a4ff682064db315cabfd76", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java?ref=0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8", "patch": "@@ -126,12 +126,12 @@ public void testCancelAsyncProducerAndConsumer() throws Exception {\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n \t\t\t// Verify that async producer is in blocking request\n-\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n+\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_PRODUCER_THREAD.getStackTrace()), producerBlocked);\n \n \t\t\tboolean consumerWaiting = false;\n \t\t\tfor (int i = 0; i < 50; i++) {\n@@ -145,7 +145,7 @@ public void testCancelAsyncProducerAndConsumer() throws Exception {\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n ", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/b452c8bbbaa9efb5de6cc66d2817b398ac9da041", "parent": "https://github.com/apache/flink/commit/11ebf484280314231d146dcfb0b973934448f00b", "message": "[FLINK-5699] [savepoints] Check target dir when cancelling with savepoint\n\nProblem: when cancelling a job with a savepoint and no savepoint directory\nis configured, triggering the savepoint fails with an NPE. This is then\nreturned to the user as the root cause.\n\nSolution: Instead of simply forwarding the argument (which is possibly\nnull), we check it for null and return a IllegalStateException with\na meaningful message.\n\nThis closes #3263.", "bug_id": "flink_59", "file": [{"additions": 1, "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java", "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java", "sha": "4d3405faa09906cdc722d0fe89395eded967d2cd", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041", "patch": "@@ -181,7 +181,7 @@ public void testCancelWithSavepoint() throws Exception {\n \t\t}\n \n \t\t{\n-\t\t\t// Cancel with savepoint (no target directory)and no job ID\n+\t\t\t// Cancel with savepoint (no target directory) and no job ID\n \t\t\tJobID jid = new JobID();\n \t\t\tUUID leaderSessionID = UUID.randomUUID();\n ", "filename": "flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java"}, {"additions": 43, "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala", "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala", "sha": "81e9fc4f6fdb0a0bc6074dab5d9aecb03a9d280c", "changes": 76, "status": "modified", "deletions": 33, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041", "patch": "@@ -586,44 +586,54 @@ class JobManager(\n           defaultSavepointDir\n         }\n \n-        log.info(s\"Trying to cancel job $jobId with savepoint to $targetDirectory\")\n+        if (targetDirectory == null) {\n+          log.info(s\"Trying to cancel job $jobId with savepoint, but no \" +\n+            \"savepoint directory configured.\")\n+\n+          sender ! decorateMessage(CancellationFailure(jobId, new IllegalStateException(\n+            \"No savepoint directory configured. You can either specify a directory \" +\n+              \"while cancelling via -s :targetDirectory or configure a cluster-wide \" +\n+              \"default via key '\" + ConfigConstants.SAVEPOINT_DIRECTORY_KEY + \"'.\")))\n+        } else {\n+          log.info(s\"Trying to cancel job $jobId with savepoint to $targetDirectory\")\n \n-        currentJobs.get(jobId) match {\n-          case Some((executionGraph, _)) =>\n-            // We don't want any checkpoint between the savepoint and cancellation\n-            val coord = executionGraph.getCheckpointCoordinator\n-            coord.stopCheckpointScheduler()\n+          currentJobs.get(jobId) match {\n+            case Some((executionGraph, _)) =>\n+              // We don't want any checkpoint between the savepoint and cancellation\n+              val coord = executionGraph.getCheckpointCoordinator\n+              coord.stopCheckpointScheduler()\n \n-            // Trigger the savepoint\n-            val future = coord.triggerSavepoint(System.currentTimeMillis(), targetDirectory)\n+              // Trigger the savepoint\n+              val future = coord.triggerSavepoint(System.currentTimeMillis(), targetDirectory)\n \n-            val senderRef = sender()\n-            future.handleAsync[Void](\n-              new BiFunction[CompletedCheckpoint, Throwable, Void] {\n-                override def apply(success: CompletedCheckpoint, cause: Throwable): Void = {\n-                  if (success != null) {\n-                    val path = success.getExternalPath()\n-                    log.info(s\"Savepoint stored in $path. Now cancelling $jobId.\")\n-                    executionGraph.cancel()\n-                    senderRef ! decorateMessage(CancellationSuccess(jobId, path))\n-                  } else {\n-                    val msg = CancellationFailure(\n-                      jobId,\n-                      new Exception(\"Failed to trigger savepoint.\", cause))\n-                    senderRef ! decorateMessage(msg)\n+              val senderRef = sender()\n+              future.handleAsync[Void](\n+                new BiFunction[CompletedCheckpoint, Throwable, Void] {\n+                  override def apply(success: CompletedCheckpoint, cause: Throwable): Void = {\n+                    if (success != null) {\n+                      val path = success.getExternalPath()\n+                      log.info(s\"Savepoint stored in $path. Now cancelling $jobId.\")\n+                      executionGraph.cancel()\n+                      senderRef ! decorateMessage(CancellationSuccess(jobId, path))\n+                    } else {\n+                      val msg = CancellationFailure(\n+                        jobId,\n+                        new Exception(\"Failed to trigger savepoint.\", cause))\n+                      senderRef ! decorateMessage(msg)\n+                    }\n+                    null\n                   }\n-                  null\n-                }\n-              },\n-              context.dispatcher)\n+                },\n+                context.dispatcher)\n \n-          case None =>\n-            log.info(s\"No job found with ID $jobId.\")\n-            sender ! decorateMessage(\n-              CancellationFailure(\n-                jobId,\n-                new IllegalArgumentException(s\"No job found with ID $jobId.\"))\n-            )\n+            case None =>\n+              log.info(s\"No job found with ID $jobId.\")\n+              sender ! decorateMessage(\n+                CancellationFailure(\n+                  jobId,\n+                  new IllegalArgumentException(s\"No job found with ID $jobId.\"))\n+              )\n+          }\n         }\n       } catch {\n         case t: Throwable =>", "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala"}, {"additions": 104, "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java", "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java", "sha": "b62727329851846dc8034dd2cad13e5311bfb57e", "changes": 107, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041", "patch": "@@ -894,9 +894,110 @@ public void testCancelWithSavepoint() throws Exception {\n \t}\n \n \t/**\n-\t * Tests that we can trigger a\n-\t *\n-\t * @throws Exception\n+\t * Tests that a meaningful exception is returned if no savepoint directory is\n+\t * configured.\n+\t */\n+\t@Test\n+\tpublic void testCancelWithSavepointNoDirectoriesConfigured() throws Exception {\n+\t\tFiniteDuration timeout = new FiniteDuration(30, TimeUnit.SECONDS);\n+\t\tConfiguration config = new Configuration();\n+\n+\t\tActorSystem actorSystem = null;\n+\t\tActorGateway jobManager = null;\n+\t\tActorGateway archiver = null;\n+\t\tActorGateway taskManager = null;\n+\t\ttry {\n+\t\t\tactorSystem = AkkaUtils.createLocalActorSystem(new Configuration());\n+\n+\t\t\tTuple2<ActorRef, ActorRef> master = JobManager.startJobManagerActors(\n+\t\t\t\tconfig,\n+\t\t\t\tactorSystem,\n+\t\t\t\tactorSystem.dispatcher(),\n+\t\t\t\tactorSystem.dispatcher(),\n+\t\t\t\tOption.apply(\"jm\"),\n+\t\t\t\tOption.apply(\"arch\"),\n+\t\t\t\tTestingJobManager.class,\n+\t\t\t\tTestingMemoryArchivist.class);\n+\n+\t\t\tjobManager = new AkkaActorGateway(master._1(), null);\n+\t\t\tarchiver = new AkkaActorGateway(master._2(), null);\n+\n+\t\t\tActorRef taskManagerRef = TaskManager.startTaskManagerComponentsAndActor(\n+\t\t\t\tconfig,\n+\t\t\t\tResourceID.generate(),\n+\t\t\t\tactorSystem,\n+\t\t\t\t\"localhost\",\n+\t\t\t\tOption.apply(\"tm\"),\n+\t\t\t\tOption.<LeaderRetrievalService>apply(new StandaloneLeaderRetrievalService(jobManager.path())),\n+\t\t\t\ttrue,\n+\t\t\t\tTestingTaskManager.class);\n+\n+\t\t\ttaskManager = new AkkaActorGateway(taskManagerRef, null);\n+\n+\t\t\t// Wait until connected\n+\t\t\tObject msg = new TestingTaskManagerMessages.NotifyWhenRegisteredAtJobManager(jobManager.actor());\n+\t\t\tAwait.ready(taskManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Create job graph\n+\t\t\tJobVertex sourceVertex = new JobVertex(\"Source\");\n+\t\t\tsourceVertex.setInvokableClass(BlockingStatefulInvokable.class);\n+\t\t\tsourceVertex.setParallelism(1);\n+\n+\t\t\tJobGraph jobGraph = new JobGraph(\"TestingJob\", sourceVertex);\n+\n+\t\t\tJobSnapshottingSettings snapshottingSettings = new JobSnapshottingSettings(\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\t3600000,\n+\t\t\t\t3600000,\n+\t\t\t\t0,\n+\t\t\t\tInteger.MAX_VALUE,\n+\t\t\t\tExternalizedCheckpointSettings.none(),\n+\t\t\t\ttrue);\n+\n+\t\t\tjobGraph.setSnapshotSettings(snapshottingSettings);\n+\n+\t\t\t// Submit job graph\n+\t\t\tmsg = new JobManagerMessages.SubmitJob(jobGraph, ListeningBehaviour.DETACHED);\n+\t\t\tAwait.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Wait for all tasks to be running\n+\t\t\tmsg = new TestingJobManagerMessages.WaitForAllVerticesToBeRunning(jobGraph.getJobID());\n+\t\t\tAwait.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Cancel with savepoint\n+\t\t\tmsg = new JobManagerMessages.CancelJobWithSavepoint(jobGraph.getJobID(), null);\n+\t\t\tCancellationResponse cancelResp = (CancellationResponse) Await.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\tif (cancelResp instanceof CancellationFailure) {\n+\t\t\t\tCancellationFailure failure = (CancellationFailure) cancelResp;\n+\t\t\t\tassertTrue(failure.cause() instanceof IllegalStateException);\n+\t\t\t\tassertTrue(failure.cause().getMessage().contains(\"savepoint directory\"));\n+\t\t\t} else {\n+\t\t\t\tfail(\"Unexpected cancellation response from JobManager: \" + cancelResp);\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tif (actorSystem != null) {\n+\t\t\t\tactorSystem.shutdown();\n+\t\t\t}\n+\n+\t\t\tif (archiver != null) {\n+\t\t\t\tarchiver.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\n+\t\t\tif (jobManager != null) {\n+\t\t\t\tjobManager.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\n+\t\t\tif (taskManager != null) {\n+\t\t\t\ttaskManager.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Tests that we can trigger a savepoint when periodic checkpoints are disabled.\n \t */\n \t@Test\n \tpublic void testSavepointWithDeactivatedPeriodicCheckpointing() throws Exception {", "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/83fb2fa8907596da263327fc063b1271e49395ec", "parent": "https://github.com/apache/flink/commit/ac42d150441f968dfb2eaa578928f0e0d987e828", "message": "[hotfix] [webserver] Add sanity check to avoid NPE in case the web server could not be instantiated\n\nThe method WebMonitorUtils.startWebRuntimeMonitor instantiates the WebMonitor. However, in case of a failure null is returned. This failure case has to be handled in the method FlinkMiniCluster.startWebServer in order to avoid a possible NullPointerException.", "bug_id": "flink_60", "file": [{"additions": 8, "raw_url": "https://github.com/apache/flink/raw/83fb2fa8907596da263327fc063b1271e49395ec/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/FlinkMiniCluster.scala", "blob_url": "https://github.com/apache/flink/blob/83fb2fa8907596da263327fc063b1271e49395ec/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/FlinkMiniCluster.scala", "sha": "bdb62ba5793c2129e7c7c56e27684da33043b129", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/FlinkMiniCluster.scala?ref=83fb2fa8907596da263327fc063b1271e49395ec", "patch": "@@ -282,12 +282,16 @@ abstract class FlinkMiniCluster(\n       LOG.info(\"Starting JobManger web frontend\")\n       // start the new web frontend. we need to load this dynamically\n       // because it is not in the same project/dependencies\n-      val webServer = WebMonitorUtils.startWebRuntimeMonitor(\n-        config, leaderRetrievalService, actorSystem)\n+      val webServer = Option(\n+        WebMonitorUtils.startWebRuntimeMonitor(\n+          config,\n+          leaderRetrievalService,\n+          actorSystem)\n+      )\n \n-      webServer.start(jobManagerAkkaURL)\n+      webServer.foreach(_.start(jobManagerAkkaURL))\n \n-      Option(webServer)\n+      webServer\n     } else {\n       None\n     }", "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/minicluster/FlinkMiniCluster.scala"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90", "parent": "https://github.com/apache/flink/commit/5c2da21f25741502dd8ca64ce9d314a1ebea1441", "message": "[FLINK-4123] [cassandra] Fix concurrency issue in CassandraTupleWriteAheadSink\n\nThe updatesCount variable in the CassandraTupleWriteAheadSink.sendValues did not have\nguaranteed visibility. Thus, it was possible that the callback thread would read an\noutdated value for updatesCount, resulting in a deadlock. Replacing IntValue updatesCount\nwith AtomicInteger updatesCount fixes this issue.\n\nFurthermore, the PR hardens the CassandraTupleWriteAheadSinkTest which could have failed\nwith a NPE if the callback runnable was not set in time.", "bug_id": "flink_61", "file": [{"additions": 12, "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java", "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java", "sha": "192843107ef3546b6a5896b1dd1a2564bd8529a8", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90", "patch": "@@ -31,7 +31,6 @@\n import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink;\n-import org.apache.flink.types.IntValue;\n \n import java.util.UUID;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -97,7 +96,7 @@ public void close() throws Exception {\n \n \t@Override\n \tprotected boolean sendValues(Iterable<IN> values, long timestamp) throws Exception {\n-\t\tfinal IntValue updatesCount = new IntValue(0);\n+\t\tfinal AtomicInteger updatesCount = new AtomicInteger(0);\n \t\tfinal AtomicInteger updatesConfirmed = new AtomicInteger(0);\n \n \t\tfinal AtomicReference<Throwable> exception = new AtomicReference<>();\n@@ -106,8 +105,8 @@ protected boolean sendValues(Iterable<IN> values, long timestamp) throws Excepti\n \t\t\t@Override\n \t\t\tpublic void onSuccess(ResultSet resultSet) {\n \t\t\t\tupdatesConfirmed.incrementAndGet();\n-\t\t\t\tif (updatesCount.getValue() > 0) { // only set if all updates have been sent\n-\t\t\t\t\tif (updatesCount.getValue() == updatesConfirmed.get()) {\n+\t\t\t\tif (updatesCount.get() > 0) { // only set if all updates have been sent\n+\t\t\t\t\tif (updatesCount.get() == updatesConfirmed.get()) {\n \t\t\t\t\t\tsynchronized (updatesConfirmed) {\n \t\t\t\t\t\t\tupdatesConfirmed.notifyAll();\n \t\t\t\t\t\t}\n@@ -142,18 +141,19 @@ public void onFailure(Throwable throwable) {\n \t\t\t\tFutures.addCallback(result, callback);\n \t\t\t}\n \t\t}\n-\t\tupdatesCount.setValue(updatesSent);\n+\t\tupdatesCount.set(updatesSent);\n \n \t\tsynchronized (updatesConfirmed) {\n-\t\t\twhile (updatesSent != updatesConfirmed.get()) {\n-\t\t\t\tif (exception.get() != null) { // verify that no query failed until now\n-\t\t\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\twhile (exception.get() == null && updatesSent != updatesConfirmed.get()) {\n \t\t\t\tupdatesConfirmed.wait();\n \t\t\t}\n \t\t}\n-\t\tboolean success = updatesSent == updatesConfirmed.get();\n-\t\treturn success;\n+\n+\t\tif (exception.get() != null) {\n+\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\treturn true;\n+\t\t}\n \t}\n }", "filename": "flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java"}, {"additions": 39, "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java", "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java", "previous_filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraConnectorUnitTest.java", "sha": "847d1a049e576a6f650fe84192e9719e55d1c966", "changes": 109, "status": "renamed", "deletions": 70, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90", "patch": "@@ -25,39 +25,33 @@\n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.java.tuple.Tuple0;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n-import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n-import org.apache.flink.util.IterableIterator;\n-import org.junit.Assert;\n import org.junit.Test;\n-import org.junit.runner.RunWith;\n import org.mockito.Matchers;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n-import org.powermock.core.classloader.annotations.PowerMockIgnore;\n-import org.powermock.core.classloader.annotations.PrepareForTest;\n-import org.powermock.modules.junit4.PowerMockRunner;\n \n-import java.util.Iterator;\n+import java.util.Collections;\n import java.util.concurrent.Executor;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.junit.Assert.assertFalse;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyString;\n import static org.powermock.api.mockito.PowerMockito.doAnswer;\n import static org.powermock.api.mockito.PowerMockito.mock;\n import static org.powermock.api.mockito.PowerMockito.when;\n \n-@RunWith(PowerMockRunner.class)\n-@PrepareForTest({ResultPartitionWriter.class, CassandraTupleWriteAheadSink.class})\n-@PowerMockIgnore({\"javax.management.*\", \"com.sun.jndi.*\"})\n-public class CassandraConnectorUnitTest {\n-\t@Test\n+public class CassandraTupleWriteAheadSinkTest {\n+\n+\t@Test(timeout=20000)\n \tpublic void testAckLoopExitOnException() throws Exception {\n-\t\tfinal AtomicReference<Runnable> callback = new AtomicReference<>();\n+\t\tfinal AtomicReference<Runnable> runnableFuture = new AtomicReference<>();\n \n \t\tfinal ClusterBuilder clusterBuilder = new ClusterBuilder() {\n+\t\t\tprivate static final long serialVersionUID = 4624400760492936756L;\n+\n \t\t\t@Override\n \t\t\tprotected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\ttry {\n@@ -73,7 +67,10 @@ protected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\t\tdoAnswer(new Answer<Void>() {\n \t\t\t\t\t\t@Override\n \t\t\t\t\t\tpublic Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\t\t\tcallback.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\t\t\t\trunnableFuture.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\t\trunnableFuture.notifyAll();\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t}\n \t\t\t\t\t}).when(future).addListener(any(Runnable.class), any(Executor.class));\n@@ -91,68 +88,40 @@ public Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n \t\t\t}\n \t\t};\n \n-\t\tfinal IterableIterator<Tuple0> iter = new IterableIterator<Tuple0>() {\n-\t\t\tprivate boolean exhausted = false;\n-\n-\t\t\t@Override\n-\t\t\tpublic boolean hasNext() {\n-\t\t\t\treturn !exhausted;\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic Tuple0 next() {\n-\t\t\t\texhausted = true;\n-\t\t\t\treturn new Tuple0();\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic void remove() {\n-\t\t\t}\n-\n+\t\t// Our asynchronous executor thread\n+\t\tnew Thread(new Runnable() {\n \t\t\t@Override\n-\t\t\tpublic Iterator<Tuple0> iterator() {\n-\t\t\t\treturn this;\n-\t\t\t}\n-\t\t};\n-\n-\t\tfinal AtomicReference<Boolean> exceptionCaught = new AtomicReference<>();\n-\n-\t\tThread t = new Thread() {\n \t\t\tpublic void run() {\n-\t\t\t\ttry {\n-\t\t\t\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n-\t\t\t\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n-\t\t\t\t\t\t\"abc\",\n-\t\t\t\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n-\t\t\t\t\t\tclusterBuilder,\n-\t\t\t\t\t\tcc\n-\t\t\t\t\t);\n-\n-\t\t\t\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n-\t\t\t\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n-\n-\t\t\t\t\tharness.setup();\n-\t\t\t\t\tsink.open();\n-\t\t\t\t\tboolean result = sink.sendValues(iter, 0L);\n-\t\t\t\t\tsink.close();\n-\t\t\t\t\texceptionCaught.set(result == false);\n-\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tthrow new RuntimeException(e);\n+\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\twhile (runnableFuture.get() == null) {\n+\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\trunnableFuture.wait();\n+\t\t\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\t\t\t// ignore interrupts\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\trunnableFuture.get().run();\n \t\t\t}\n-\t\t};\n-\t\tt.start();\n+\t\t}).start();\n+\n+\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n+\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n+\t\t\t\"abc\",\n+\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n+\t\t\tclusterBuilder,\n+\t\t\tcc\n+\t\t);\n \n-\t\tint count = 0;\n-\t\twhile (t.getState() != Thread.State.WAITING && count < 100) { // 10 second timeout 10 * 10 * 100ms\n-\t\t\tThread.sleep(100);\n-\t\t\tcount++;\n-\t\t}\n+\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n+\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n \n-\t\tcallback.get().run();\n+\t\tharness.setup();\n+\t\tsink.open();\n \n-\t\tt.join();\n+\t\t// we should leave the loop and return false since we've seen an exception\n+\t\tassertFalse(sink.sendValues(Collections.singleton(new Tuple0()), 0L));\n \n-\t\tAssert.assertTrue(exceptionCaught.get());\n+\t\tsink.close();\n \t}\n }", "filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java"}, {"additions": 3, "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java", "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java", "sha": "5545717b24466fc5e1c6ae209c7323c9e551a8eb", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90", "patch": "@@ -190,6 +190,9 @@ public void processWatermark(Watermark mark) throws Exception {\n \t * used since the last completed checkpoint.\n \t **/\n \tpublic static class ExactlyOnceState implements StateHandle<Serializable> {\n+\n+\t\tprivate static final long serialVersionUID = -3571063495273460743L;\n+\n \t\tprotected TreeMap<Long, Tuple2<Long, StateHandle<DataInputView>>> pendingHandles;\n \n \t\tpublic ExactlyOnceState() {", "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java"}], "repo": "flink"}, {"commit": "https://github.com/apache/flink/commit/6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc", "parent": "https://github.com/apache/flink/commit/be662bf7ebcefb289988a24392104c3385029568", "message": "[FLINK-6836] [tests] Fix YARNSessionCapacitySchedulerITCase to work with Hadoop 2.6.5, 2.7.3 and 2.8.0\n\nDue to MNG-5899, maven cannot resolve dependency reduced poms in a multi project build. Therefore,\nflink-yarn-tests pulls in a wrong version of org.apache.httpcomponents.httpclient which does not work\nwith Hadoop's ServletUtils together. As a solution we have to move the dependency management for the\nhttpclient and httpcore version into the parent pom.xml.\n\nAnother problem is the version of these libraries which has been recently bumped. In 4.4, httpclient\nchanged its behaviour such that URLEncodedUtils#parse(String, Charset) now throws a NPE if the first\nparameter is null. In 4.2.6, an empty list was returned instead. Due to this incompatibility, we reverted\nthe change and set the version to its previous value.\n\nBump httpclient to 4.5.3 and httpcore to 4.4.6\n\nThis closes #4120.", "bug_id": "flink_62", "file": [{"additions": 0, "raw_url": "https://github.com/apache/flink/raw/6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc/flink-shaded-hadoop/flink-shaded-hadoop2/pom.xml", "blob_url": "https://github.com/apache/flink/blob/6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc/flink-shaded-hadoop/flink-shaded-hadoop2/pom.xml", "sha": "ff6f84d541db77083ca167f8dca953acffcb8567", "changes": 16, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-shaded-hadoop/flink-shaded-hadoop2/pom.xml?ref=6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc", "patch": "@@ -676,22 +676,6 @@ under the License.\n \t\t</dependency>\n \t</dependencies>\n \n-\t<dependencyManagement>\n-\t\t<dependencies>\n-\t\t\t<dependency>\n-\t\t\t\t<groupId>org.apache.httpcomponents</groupId>\n-\t\t\t\t<artifactId>httpcore</artifactId>\n-\t\t\t\t<version>4.4.4</version>\n-\t\t\t</dependency>\n-\n-\t\t\t<dependency>\n-\t\t\t\t<groupId>org.apache.httpcomponents</groupId>\n-\t\t\t\t<artifactId>httpclient</artifactId>\n-\t\t\t\t<version>4.5.2</version>\n-\t\t\t</dependency>\n-\t\t</dependencies>\n-\t</dependencyManagement>\n-\n \t<profiles>\n \t\t<profile>\n \t\t\t<!-- MapR build profile -->", "filename": "flink-shaded-hadoop/flink-shaded-hadoop2/pom.xml"}, {"additions": 16, "raw_url": "https://github.com/apache/flink/raw/6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc/pom.xml", "blob_url": "https://github.com/apache/flink/blob/6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc/pom.xml", "sha": "5fb895d17799fe52455a89553b23b9d2558c9eae", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/flink/contents/pom.xml?ref=6fdeed326ec7221c51e1f2a8f235ed0b8c58c6cc", "patch": "@@ -445,6 +445,22 @@ under the License.\n \t\t\t\t<version>4.0.27.Final</version>\n \t\t\t</dependency>\n \n+\t\t\t<!-- We have to define the versions for httpcore and httpclient here such that a consistent\n+\t\t\t version is used by the shaded hadoop jars and the flink-yarn-test project because of MNG-5899.\n+\n+\t\t\t See FLINK-6836 for more details -->\n+\t\t\t<dependency>\n+\t\t\t\t<groupId>org.apache.httpcomponents</groupId>\n+\t\t\t\t<artifactId>httpcore</artifactId>\n+\t\t\t\t<version>4.4.6</version>\n+\t\t\t</dependency>\n+\n+\t\t\t<dependency>\n+\t\t\t\t<groupId>org.apache.httpcomponents</groupId>\n+\t\t\t\t<artifactId>httpclient</artifactId>\n+\t\t\t\t<version>4.5.3</version>\n+\t\t\t</dependency>\n+\n \t\t\t<dependency>\n \t\t\t\t<groupId>tv.cntt</groupId>\n \t\t\t\t<artifactId>netty-router</artifactId>", "filename": "pom.xml"}], "repo": "flink"}]
