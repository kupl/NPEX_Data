{
    "incubator-druid_04d3054": {
        "bug_id": "incubator-druid_04d3054",
        "commit": "https://github.com/apache/incubator-druid/commit/04d30543538deac2b6fb5263b66d04e6e0522966",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/04d30543538deac2b6fb5263b66d04e6e0522966/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java?ref=04d30543538deac2b6fb5263b66d04e6e0522966",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Throwables;\n import com.google.common.collect.Lists;\n+import com.metamx.common.IAE;\n import com.metamx.common.ISE;\n import com.metamx.emitter.EmittingLogger;\n import io.druid.data.input.Firehose;\n@@ -84,6 +85,9 @@ public StringInputRowParser getParser()\n   @Override\n   public Firehose connect(StringInputRowParser firehoseParser) throws IOException\n   {\n+    if (baseDir == null) {\n+      throw new IAE(\"baseDir is null\");\n+    }\n     log.info(\"Searching for all [%s] in and beneath [%s]\", filter, baseDir.getAbsoluteFile());\n \n     Collection<File> foundFiles = FileUtils.listFiles(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/04d30543538deac2b6fb5263b66d04e6e0522966/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "sha": "4d530f4ba738862d3f6a2d60f1fa479822475615",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #2303 from CHOIJAEHONG1/localfirehouse-basedir-npe\n\nThrow an IAE when baseDir is null in LocalFireHose",
        "parent": "https://github.com/apache/incubator-druid/commit/201539260c4d111530c0699f6bf0a799a68f1472",
        "repo": "incubator-druid",
        "unit_tests": [
            "LocalFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_0712941": {
        "bug_id": "incubator-druid_0712941",
        "commit": "https://github.com/apache/incubator-druid/commit/07129418257496c2ceb4b716c40c68717d28036b",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/07129418257496c2ceb4b716c40c68717d28036b/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java?ref=07129418257496c2ceb4b716c40c68717d28036b",
                "deletions": 0,
                "filename": "realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "patch": "@@ -662,6 +662,14 @@ private File computePersistDir(Schema schema, Interval interval)\n    */\n   private int persistHydrant(FireHydrant indexToPersist, Schema schema, Interval interval)\n   {\n+    if (indexToPersist.hasSwapped()) {\n+      log.info(\n+          \"DataSource[%s], Interval[%s], Hydrant[%s] already swapped. Ignoring request to persist.\",\n+          schema.getDataSource(), interval, indexToPersist\n+      );\n+      return 0;\n+    }\n+\n     log.info(\"DataSource[%s], Interval[%s], persisting Hydrant[%s]\", schema.getDataSource(), interval, indexToPersist);\n     try {\n       int numRows = indexToPersist.getIndex().size();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/07129418257496c2ceb4b716c40c68717d28036b/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "sha": "a429fbef9d589aff71b1b87e068b81bb20d5f640",
                "status": "modified"
            }
        ],
        "message": "1) Add check whether a Hydrant has already been persisted before persisting.  Persisting happens synchronously on the same thread, but multiple persist requests can be queued up on that thread which means that subsequent ones would fail with an NPE.  Fixes #178",
        "parent": "https://github.com/apache/incubator-druid/commit/fd1d73e83abdff8dcea9d3c8bcc9c3278ce92dea",
        "repo": "incubator-druid",
        "unit_tests": [
            "RealtimePlumberSchoolTest.java"
        ]
    },
    "incubator-druid_0ab7c31": {
        "bug_id": "incubator-druid_0ab7c31",
        "commit": "https://github.com/apache/incubator-druid/commit/0ab7c315dd57efb4303d15de14666f1a826c40b1",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0ab7c315dd57efb4303d15de14666f1a826c40b1/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java?ref=0ab7c315dd57efb4303d15de14666f1a826c40b1",
                "deletions": 1,
                "filename": "client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -20,13 +20,15 @@\n package com.metamx.druid.query;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Throwables;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Ordering;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.MergeIterable;\n import com.metamx.common.guava.Sequence;\n import com.metamx.common.guava.Sequences;\n+import com.metamx.common.logger.Logger;\n import com.metamx.druid.Query;\n \n import java.util.Arrays;\n@@ -52,6 +54,8 @@\n  */\n public class ChainedExecutionQueryRunner<T> implements QueryRunner<T>\n {\n+  private static final Logger log = new Logger(ChainedExecutionQueryRunner.class);\n+\n   private final Iterable<QueryRunner<T>> queryables;\n   private final ExecutorService exec;\n   private final Ordering<T> ordering;\n@@ -100,7 +104,13 @@ public ChainedExecutionQueryRunner(\n                               @Override\n                               public List<T> call() throws Exception\n                               {\n-                                return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                try {\n+                                  return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                }\n+                                catch (Exception e) {\n+                                  log.error(e, \"Exception with one of the sequences!\");\n+                                  throw Throwables.propagate(e);\n+                                }\n                               }\n                             }\n                         );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0ab7c315dd57efb4303d15de14666f1a826c40b1/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "52fc99a35b1c6ebac943e4c30d89352fe2323c3c",
                "status": "modified"
            }
        ],
        "message": "more logging in CEQR to track down NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/e72695111c9e77e53224c90192add88406493b6f",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java"
        ]
    },
    "incubator-druid_1d67e39": {
        "bug_id": "incubator-druid_1d67e39",
        "commit": "https://github.com/apache/incubator-druid/commit/1d67e399492e90beebadab76df947492f3a49e97",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 2,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -762,8 +762,6 @@ public ShardSpec apply(DimPartition dimPartition)\n         log.info(\"  %s\", HadoopDruidIndexerConfig.jsonMapper.writeValueAsString(shardSpec));\n       }\n \n-      System.out.println(HadoopDruidIndexerConfig.jsonMapper.writeValueAsString(chosenShardSpecs));\n-\n       try {\n         HadoopDruidIndexerConfig.jsonMapper\n                                 .writerWithType(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "sha": "7a6856505283ce008a912b4ed0138e9fd8cce478",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/UnionDataSource.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/UnionDataSource.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/UnionDataSource.java",
                "patch": "@@ -43,6 +43,9 @@\n   public UnionDataSource(@JsonProperty(\"dataSources\") List<DataSource> dataSources)\n   {\n     Preconditions.checkNotNull(dataSources, \"datasources cannot be null for uniondatasource\");\n+    for(DataSource ds : dataSources){\n+      Preconditions.checkArgument(ds instanceof TableDataSource, \"Union DataSource only supports TableDatasource\");\n+    }\n     this.dataSources = dataSources;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/UnionDataSource.java",
                "sha": "83b9f0acbe7a0db81367ac9b7abf36527df6e9b2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/select/PagingSpec.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "patch": "@@ -40,7 +40,7 @@ public PagingSpec(\n       @JsonProperty(\"threshold\") int threshold\n   )\n   {\n-    this.pagingIdentifiers = pagingIdentifiers;\n+    this.pagingIdentifiers = pagingIdentifiers == null ? new LinkedHashMap<String, Integer>() : pagingIdentifiers;\n     this.threshold = threshold;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "sha": "0ad5b56635dc9831757d3418fbbf1ec3f4c2a142",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQuery.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "patch": "@@ -167,7 +167,7 @@ public TopNQuery withQuerySegmentSpec(QuerySegmentSpec querySegmentSpec)\n   public Query<Result<TopNResultValue>> withDataSource(DataSource dataSource)\n   {\n     return new TopNQuery(\n-        getDataSource(),\n+        dataSource,\n         dimensionSpec,\n         topNMetricSpec,\n         threshold,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "sha": "b456e487039b63835d74a6b9f6359eeae12df387",
                "status": "modified"
            }
        ],
        "message": "Fixes\n\nfix NPE in select query, calculating cache key\nremove unwanted logging\nfix ds in topNQuery",
        "parent": "https://github.com/apache/incubator-druid/commit/728a606d32ba31939e6a4d20b47ea72b2d2de49f",
        "repo": "incubator-druid",
        "unit_tests": [
            "TopNQueryTest.java"
        ]
    },
    "incubator-druid_23133f3": {
        "bug_id": "incubator-druid_23133f3",
        "commit": "https://github.com/apache/incubator-druid/commit/23133f3b5b064856a11ec4c4b130f562467e0170",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/23133f3b5b064856a11ec4c4b130f562467e0170/processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/jackson/JacksonModule.java?ref=23133f3b5b064856a11ec4c4b130f562467e0170",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "patch": "@@ -49,7 +49,7 @@ public ObjectMapper jsonMapper()\n   public ObjectMapper smileMapper()\n   {\n     ObjectMapper retVal = new DefaultObjectMapper(new SmileFactory());\n-    retVal.getJsonFactory().setCodec(retVal);\n+    retVal.getFactory().setCodec(retVal);\n     return retVal;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/23133f3b5b064856a11ec4c4b130f562467e0170/processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "sha": "16d19cf95441e3c36b54473929cb40850c4b67bc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/23133f3b5b064856a11ec4c4b130f562467e0170/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/DirectDruidClient.java?ref=23133f3b5b064856a11ec4c4b130f562467e0170",
                "deletions": 3,
                "filename": "server/src/main/java/io/druid/client/DirectDruidClient.java",
                "patch": "@@ -89,7 +89,7 @@ public DirectDruidClient(\n     this.httpClient = httpClient;\n     this.host = host;\n \n-    this.isSmile = this.objectMapper.getJsonFactory() instanceof SmileFactory;\n+    this.isSmile = this.objectMapper.getFactory() instanceof SmileFactory;\n     this.openConnections = new AtomicInteger();\n   }\n \n@@ -269,7 +269,7 @@ private void init()\n     {\n       if (jp == null) {\n         try {\n-          jp = objectMapper.getJsonFactory().createJsonParser(future.get());\n+          jp = objectMapper.getFactory().createParser(future.get());\n           if (jp.nextToken() != JsonToken.START_ARRAY) {\n             throw new IAE(\"Next token wasn't a START_ARRAY, was[%s]\", jp.getCurrentToken());\n           } else {\n@@ -292,7 +292,9 @@ private void init()\n     @Override\n     public void close() throws IOException\n     {\n-      jp.close();\n+      if(jp != null) {\n+        jp.close();\n+      }\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/23133f3b5b064856a11ec4c4b130f562467e0170/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "sha": "8befef5cabb3f0fe4ebc0994ea442e7677c40660",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #305 from metamx/npe-deprecated-fix\n\nRemove deprecated jackson calls and fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/00df13af0634fa08a369a5d4c60328dc353b880d",
        "repo": "incubator-druid",
        "unit_tests": [
            "DirectDruidClientTest.java"
        ]
    },
    "incubator-druid_3134aff": {
        "bug_id": "incubator-druid_3134aff",
        "commit": "https://github.com/apache/incubator-druid/commit/3134affac9e1f6fc628289f345c1f0ebfdbf6307",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/DirectDruidClient.java?ref=3134affac9e1f6fc628289f345c1f0ebfdbf6307",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/client/DirectDruidClient.java",
                "patch": "@@ -292,7 +292,9 @@ private void init()\n     @Override\n     public void close() throws IOException\n     {\n-      jp.close();\n+      if(jp != null) {\n+        jp.close();\n+      }\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "sha": "8befef5cabb3f0fe4ebc0994ea442e7677c40660",
                "status": "modified"
            }
        ],
        "message": "fix NPE in DirectDruidClient",
        "parent": "https://github.com/apache/incubator-druid/commit/d0fe70a21f716a26bbaf6dc0e38c50e535a539e7",
        "repo": "incubator-druid",
        "unit_tests": [
            "DirectDruidClientTest.java"
        ]
    },
    "incubator-druid_35c89d2": {
        "bug_id": "incubator-druid_35c89d2",
        "commit": "https://github.com/apache/incubator-druid/commit/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/CachingQueryRunner.java?ref=35c89d29a0e31ce4bbd568bb3218b3c79386ccb4",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "patch": "@@ -84,11 +84,16 @@ public CachingQueryRunner(\n         && strategy != null\n         && cacheConfig.isPopulateCache();\n \n-    final Cache.NamedKey key = CacheUtil.computeSegmentCacheKey(\n-        segmentIdentifier,\n-        segmentDescriptor,\n-        strategy.computeCacheKey(query)\n-    );\n+    final Cache.NamedKey key;\n+    if(strategy != null && (useCache || populateCache)) {\n+      key = CacheUtil.computeSegmentCacheKey(\n+          segmentIdentifier,\n+          segmentDescriptor,\n+          strategy.computeCacheKey(query)\n+      );\n+    } else {\n+      key = null;\n+    }\n \n     if(useCache) {\n       final Function cacheFn = strategy.pullFromCache();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "sha": "d92db6415fba83fcc3e18c357f8eff913e3aac43",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #473 from metamx/fix-caching-npe\n\nfix npe in CachingQueryRunner",
        "parent": "https://github.com/apache/incubator-druid/commit/c05f16917115cf19257e7c928a14d9eeec9b005c",
        "repo": "incubator-druid",
        "unit_tests": [
            "CachingQueryRunnerTest.java"
        ]
    },
    "incubator-druid_40aa6ec": {
        "bug_id": "incubator-druid_40aa6ec",
        "commit": "https://github.com/apache/incubator-druid/commit/40aa6ecaefd3becd05d685198037087fa9550f34",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/40aa6ecaefd3becd05d685198037087fa9550f34/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=40aa6ecaefd3becd05d685198037087fa9550f34",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -92,7 +92,7 @@ public void doPost(\n     final long start = System.currentTimeMillis();\n     Query query = null;\n     byte[] requestQuery = null;\n-    String queryID;\n+    String queryId;\n \n     final boolean isSmile = \"application/smile\".equals(req.getContentType());\n \n@@ -105,9 +105,10 @@ public void doPost(\n     try {\n       requestQuery = ByteStreams.toByteArray(req.getInputStream());\n       query = objectMapper.readValue(requestQuery, Query.class);\n-      queryID = query.getId();\n-      if (queryID == null) {\n-        query = query.withId(idProvider.next(query));\n+      queryId = query.getId();\n+      if (queryId == null) {\n+        queryId = idProvider.next(query);\n+        query = query.withId(queryId);\n       }\n \n       requestLogger.log(\n@@ -136,7 +137,7 @@ public void doPost(\n               .setUser6(String.valueOf(query.hasFilters()))\n               .setUser7(req.getRemoteAddr())\n               .setUser9(query.getDuration().toPeriod().toStandardMinutes().toString())\n-              .setUser10(queryID)\n+              .setUser10(queryId)\n               .build(\"request/time\", requestTime)\n       );\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/40aa6ecaefd3becd05d685198037087fa9550f34/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "97bca835ddb4274bea34cb80c9f85c64d1b84a8c",
                "status": "modified"
            }
        ],
        "message": "fix npe queryID",
        "parent": "https://github.com/apache/incubator-druid/commit/e54949ca981ca5a446991c4a1ac671e2ec94736a",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_4338af0": {
        "bug_id": "incubator-druid_4338af0",
        "commit": "https://github.com/apache/incubator-druid/commit/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=4338af0e3f2ef3f91c3a8cf78e91122c54c89e21",
                "deletions": 2,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -130,7 +130,6 @@ public Response doPost(\n   {\n     final long start = System.currentTimeMillis();\n     Query query = null;\n-    byte[] requestQuery = null;\n     String queryId = null;\n \n     final String reqContentType = req.getContentType();\n@@ -267,9 +266,10 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n       ).build();\n     }\n     catch (Exception e) {\n+      // Input stream has already been consumed by the json object mapper if query == null\n       final String queryString =\n           query == null\n-          ? (isSmile ? \"smile_unknown\" : new String(requestQuery, Charsets.UTF_8))\n+          ? \"unparsable query\"\n           : query.toString();\n \n       log.warn(e, \"Exception occurred on request [%s]\", queryString);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "165904668935fc1f7379739ab147a424108c4d28",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in QueryResource on bad query",
        "parent": "https://github.com/apache/incubator-druid/commit/63df70cbe25d44a36cab6dbcd5a21894c7977f34",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_4ace65a": {
        "bug_id": "incubator-druid_4ace65a",
        "commit": "https://github.com/apache/incubator-druid/commit/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java?ref=4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "patch": "@@ -454,7 +454,7 @@ public int getPartition(BytesWritable bytesWritable, Writable value, int numPart\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       int shardNum = bytes.getInt();\n-      if (config.get(\"mapred.job.tracker\").equals(\"local\")) {\n+      if (\"local\".equals(config.get(\"mapreduce.jobtracker.address\")) || \"local\".equals(config.get(\"mapred.job.tracker\"))) {\n         return shardNum % numPartitions;\n       } else {\n         if (shardNum >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "sha": "2a07fa521590092b44910d0fc1419647bca45685",
                "status": "modified"
            }
        ],
        "message": "fix NPE in IndexGeneratorJob (#4371)\n\n* fix NPE in IndexGeneratorJob\r\n\r\n* address review comment\r\n\r\n* review comments",
        "parent": "https://github.com/apache/incubator-druid/commit/2d15215cd0953a3ffbc719752edb309c04a717de",
        "repo": "incubator-druid",
        "unit_tests": [
            "IndexGeneratorJobTest.java"
        ]
    },
    "incubator-druid_4db9e39": {
        "bug_id": "incubator-druid_4db9e39",
        "commit": "https://github.com/apache/incubator-druid/commit/4db9e39a715ec64e59c985ec534b54bcf153b2df",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4db9e39a715ec64e59c985ec534b54bcf153b2df/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java?ref=4db9e39a715ec64e59c985ec534b54bcf153b2df",
                "deletions": 2,
                "filename": "java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "patch": "@@ -19,7 +19,6 @@\n \n package io.druid.java.util.common.io.smoosh;\n \n-import com.google.common.base.Throwables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.io.Closeables;\n@@ -143,6 +142,9 @@ public void close()\n   {\n     Throwable thrown = null;\n     for (MappedByteBuffer mappedByteBuffer : buffersList) {\n+      if (mappedByteBuffer == null) {\n+        continue;\n+      }\n       try {\n         ByteBufferUtils.unmap(mappedByteBuffer);\n       }\n@@ -154,6 +156,9 @@ public void close()\n         }\n       }\n     }\n-    Throwables.propagateIfPossible(thrown);\n+    buffersList.clear();\n+    if (thrown != null) {\n+      throw new RuntimeException(thrown);\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4db9e39a715ec64e59c985ec534b54bcf153b2df/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "sha": "d7c284b8a83012e24e220fc3357830e4d50f15c4",
                "status": "modified"
            }
        ],
        "message": "fix NPE when buffersList contains null in SmooshedFileMapper (#5689)\n\n* fix NPE when buffersList contains null\r\n\r\n* address the comment",
        "parent": "https://github.com/apache/incubator-druid/commit/86746f82d8a5e0fa36748240009e838eb0dff6d9",
        "repo": "incubator-druid",
        "unit_tests": [
            "SmooshedFileMapperTest.java"
        ]
    },
    "incubator-druid_54139c6": {
        "bug_id": "incubator-druid_54139c6",
        "commit": "https://github.com/apache/incubator-druid/commit/54139c68159149017ea4ed9510346934d4e68997",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54139c68159149017ea4ed9510346934d4e68997/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java?ref=54139c68159149017ea4ed9510346934d4e68997",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "patch": "@@ -55,7 +55,7 @@ public RegisteredLookupExtractionFn(\n     this.replaceMissingValueWith = replaceMissingValueWith;\n     this.retainMissingValue = retainMissingValue;\n     this.injective = injective;\n-    this.optimize = optimize;\n+    this.optimize = optimize == null ? true : optimize;\n     this.lookup = lookup;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54139c68159149017ea4ed9510346934d4e68997/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "sha": "ab58beb25a2e174b9308d49285812dd8b67b6e32",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in registeredLookup extractionFn when \"optimize\" is not provided. (#3064)",
        "parent": "https://github.com/apache/incubator-druid/commit/2db5f49f3526ec16e7535cebdb977b65cb762583",
        "repo": "incubator-druid",
        "unit_tests": [
            "RegisteredLookupExtractionFnTest.java"
        ]
    },
    "incubator-druid_5ecd909": {
        "bug_id": "incubator-druid_5ecd909",
        "commit": "https://github.com/apache/incubator-druid/commit/5ecd909f7c04527904cf3ec5b2575c3c6f69c210",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5ecd909f7c04527904cf3ec5b2575c3c6f69c210/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=5ecd909f7c04527904cf3ec5b2575c3c6f69c210",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -84,6 +84,11 @@ public ChainedExecutionQueryRunner(\n   {\n     final int priority = Integer.parseInt(query.getContextValue(\"priority\", \"0\"));\n \n+    if (Iterables.isEmpty(queryables)) {\n+      log.warn(\"No queryables found.\");\n+      return Sequences.empty();\n+    }\n+\n     return new BaseSequence<T, Iterator<T>>(\n         new BaseSequence.IteratorMaker<T, Iterator<T>>()\n         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5ecd909f7c04527904cf3ec5b2575c3c6f69c210/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "316c8d8675e6bbdf20ede3166551b892b55ec3d6",
                "status": "modified"
            }
        ],
        "message": "reduce NPEs in CQE",
        "parent": "https://github.com/apache/incubator-druid/commit/84233238b1845011d6097dd040093e12342f747e",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java"
        ]
    },
    "incubator-druid_6171e07": {
        "bug_id": "incubator-druid_6171e07",
        "commit": "https://github.com/apache/incubator-druid/commit/6171e078c8fdd2250e331f1c225db2b4e3be8809",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6171e078c8fdd2250e331f1c225db2b4e3be8809/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java?ref=6171e078c8fdd2250e331f1c225db2b4e3be8809",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "patch": "@@ -26,10 +26,10 @@\n import com.google.common.base.Strings;\n import com.metamx.common.StringUtils;\n import io.druid.query.extraction.ExtractionFn;\n+import io.druid.query.filter.DimFilterCacheHelper;\n import io.druid.query.lookup.LookupExtractionFn;\n import io.druid.query.lookup.LookupExtractor;\n import io.druid.query.lookup.LookupReferencesManager;\n-import io.druid.query.filter.DimFilterCacheHelper;\n import io.druid.segment.DimensionSelector;\n \n import javax.annotation.Nullable;\n@@ -129,18 +129,18 @@ public ExtractionFn getExtractionFn()\n     final LookupExtractor lookupExtractor = Strings.isNullOrEmpty(name)\n                                             ? this.lookup\n                                             : Preconditions.checkNotNull(\n-                                                this.lookupReferencesManager.get(name).get(),\n-                                                \"can not find lookup with name [%s]\",\n+                                                lookupReferencesManager.get(name),\n+                                                \"Lookup [%s] not found\",\n                                                 name\n-                                            );\n+                                            ).get();\n+\n     return new LookupExtractionFn(\n         lookupExtractor,\n         retainMissingValue,\n         replaceMissingValueWith,\n         lookupExtractor.isOneToOne(),\n         optimize\n     );\n-\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6171e078c8fdd2250e331f1c225db2b4e3be8809/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "sha": "06d9eb91c8c6222ae9d3a3b0d3a2037b87668a97",
                "status": "modified"
            }
        ],
        "message": "Improve NPE message in LookupDimensionSpec when lookup does not exist. (#3065)\n\nThe message used to be empty, which made things hard to debug.",
        "parent": "https://github.com/apache/incubator-druid/commit/603fbbcc2009ba1e1cff66669df266a525d2aeb7",
        "repo": "incubator-druid",
        "unit_tests": [
            "LookupDimensionSpecTest.java"
        ]
    },
    "incubator-druid_674f940": {
        "bug_id": "incubator-druid_674f940",
        "commit": "https://github.com/apache/incubator-druid/commit/674f94083e381f302fab9e43f15fb940a3b3934c",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/incubator-druid/blob/674f94083e381f302fab9e43f15fb940a3b3934c/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=674f94083e381f302fab9e43f15fb940a3b3934c",
                "deletions": 4,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.inject.Inject;\n import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n+import com.metamx.common.StringUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n import io.druid.segment.loading.SegmentLoadingException;\n@@ -133,18 +134,33 @@ public Void call() throws Exception\n                     s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n                   log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\", s3Bucket, s3Path);\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n+                  final String copyMsg = StringUtils.safeFormat(\n+                      \"[s3://%s/%s] to [s3://%s/%s]\", s3Bucket,\n                       s3Path,\n                       targetS3Bucket,\n                       targetS3Path\n                   );\n+                  log.info(\n+                      \"Moving file %s\",\n+                      copyMsg\n+                  );\n                   final S3Object target = new S3Object(targetS3Path);\n                   if (!config.getDisableAcl()) {\n                     target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n+                  final Map<String, Object> copyResult = s3Client.moveObject(\n+                      s3Bucket,\n+                      s3Path,\n+                      targetS3Bucket,\n+                      target,\n+                      false\n+                  );\n+                  if (copyResult != null && copyResult.containsKey(\"DeleteException\")) {\n+                    log.error(\"Error Deleting data after copy %s: %s\", copyMsg, copyResult);\n+                    // Maybe retry deleting here?\n+                  } else {\n+                    log.debug(\"Finished moving file %s\", copyMsg);\n+                  }\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/674f94083e381f302fab9e43f15fb940a3b3934c/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "6726569d056463b96037be02948bc6b87341c9f6",
                "status": "modified"
            }
        ],
        "message": "Add more logging around failed S3DataSegmentMover DeleteExceptions (#3104)\n\n* Add more logging around failed S3DataSegmentMover DeleteExceptions\r\n\r\n* Fix test NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/6c2fd75e4645613cf26b9e1f654fc7d1d7691458",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    },
    "incubator-druid_677e24b": {
        "bug_id": "incubator-druid_677e24b",
        "commit": "https://github.com/apache/incubator-druid/commit/677e24b76027d833da2987416a5439f474faff7a",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/677e24b76027d833da2987416a5439f474faff7a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java?ref=677e24b76027d833da2987416a5439f474faff7a",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "patch": "@@ -883,7 +883,10 @@ public void onFailure(Throwable t)\n       if (chatHandlerProvider.isPresent()) {\n         chatHandlerProvider.get().unregister(getId());\n       }\n-      publishExecService.shutdownNow();\n+\n+      if (publishExecService != null) {\n+        publishExecService.shutdownNow();\n+      }\n \n       toolbox.getDruidNodeAnnouncer().unannounce(discoveryDruidNode);\n       toolbox.getDataSegmentServerAnnouncer().unannounce();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/677e24b76027d833da2987416a5439f474faff7a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "sha": "6a17872d7230dbf1a21cf6a416c4c06730a79211",
                "status": "modified"
            }
        ],
        "message": "prevent NPE from supressing actual exception (#5146)",
        "parent": "https://github.com/apache/incubator-druid/commit/64848c7ebfd5b95c528c0890ef47990de4e0a8f3",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaIndexTaskTest.java"
        ]
    },
    "incubator-druid_67f4bba": {
        "bug_id": "incubator-druid_67f4bba",
        "commit": "https://github.com/apache/incubator-druid/commit/67f4bbae74b45af24146551a3a6208475e3d8f34",
        "file": [
            {
                "additions": 149,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 257,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 108,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -130,146 +130,148 @@ public IncrementalIndex(\n     int currAggSize = 0;\n     for (int i = 0; i < metrics.length; i++) {\n       final AggregatorFactory agg = metrics[i];\n-      aggs[i] = agg.factorizeBuffered(\n-          new ColumnSelectorFactory()\n-          {\n-            @Override\n-            public TimestampColumnSelector makeTimestampColumnSelector()\n-            {\n-              return new TimestampColumnSelector()\n+      aggs[i] = new ThreadSafeAggregator(\n+          agg.factorizeBuffered(\n+              new ColumnSelectorFactory()\n               {\n                 @Override\n-                public long getTimestamp()\n+                public TimestampColumnSelector makeTimestampColumnSelector()\n                 {\n-                  return in.get().getTimestampFromEpoch();\n+                  return new TimestampColumnSelector()\n+                  {\n+                    @Override\n+                    public long getTimestamp()\n+                    {\n+                      return in.get().getTimestampFromEpoch();\n+                    }\n+                  };\n                 }\n-              };\n-            }\n \n-            @Override\n-            public FloatColumnSelector makeFloatColumnSelector(String columnName)\n-            {\n-              final String metricName = columnName.toLowerCase();\n-              return new FloatColumnSelector()\n-              {\n                 @Override\n-                public float get()\n+                public FloatColumnSelector makeFloatColumnSelector(String columnName)\n                 {\n-                  return in.get().getFloatMetric(metricName);\n+                  final String metricName = columnName.toLowerCase();\n+                  return new FloatColumnSelector()\n+                  {\n+                    @Override\n+                    public float get()\n+                    {\n+                      return in.get().getFloatMetric(metricName);\n+                    }\n+                  };\n                 }\n-              };\n-            }\n \n-            @Override\n-            public ObjectColumnSelector makeObjectColumnSelector(String column)\n-            {\n-              final String typeName = agg.getTypeName();\n-              final String columnName = column.toLowerCase();\n-\n-              final ObjectColumnSelector<Object> rawColumnSelector = new ObjectColumnSelector<Object>()\n-              {\n                 @Override\n-                public Class classOfObject()\n+                public ObjectColumnSelector makeObjectColumnSelector(String column)\n                 {\n-                  return Object.class;\n-                }\n+                  final String typeName = agg.getTypeName();\n+                  final String columnName = column.toLowerCase();\n \n-                @Override\n-                public Object get()\n-                {\n-                  return in.get().getRaw(columnName);\n-                }\n-              };\n+                  final ObjectColumnSelector<Object> rawColumnSelector = new ObjectColumnSelector<Object>()\n+                  {\n+                    @Override\n+                    public Class classOfObject()\n+                    {\n+                      return Object.class;\n+                    }\n \n-              if (!deserializeComplexMetrics) {\n-                return rawColumnSelector;\n-              } else {\n-                if (typeName.equals(\"float\")) {\n-                  return rawColumnSelector;\n-                }\n+                    @Override\n+                    public Object get()\n+                    {\n+                      return in.get().getRaw(columnName);\n+                    }\n+                  };\n \n-                final ComplexMetricSerde serde = ComplexMetrics.getSerdeForType(typeName);\n-                if (serde == null) {\n-                  throw new ISE(\"Don't know how to handle type[%s]\", typeName);\n-                }\n+                  if (!deserializeComplexMetrics) {\n+                    return rawColumnSelector;\n+                  } else {\n+                    if (typeName.equals(\"float\")) {\n+                      return rawColumnSelector;\n+                    }\n \n-                final ComplexMetricExtractor extractor = serde.getExtractor();\n-                return new ObjectColumnSelector()\n-                {\n-                  @Override\n-                  public Class classOfObject()\n-                  {\n-                    return extractor.extractedClass();\n-                  }\n+                    final ComplexMetricSerde serde = ComplexMetrics.getSerdeForType(typeName);\n+                    if (serde == null) {\n+                      throw new ISE(\"Don't know how to handle type[%s]\", typeName);\n+                    }\n \n-                  @Override\n-                  public Object get()\n-                  {\n-                    return extractor.extractValue(in.get(), columnName);\n+                    final ComplexMetricExtractor extractor = serde.getExtractor();\n+                    return new ObjectColumnSelector()\n+                    {\n+                      @Override\n+                      public Class classOfObject()\n+                      {\n+                        return extractor.extractedClass();\n+                      }\n+\n+                      @Override\n+                      public Object get()\n+                      {\n+                        return extractor.extractValue(in.get(), columnName);\n+                      }\n+                    };\n                   }\n-                };\n-              }\n-            }\n+                }\n \n-            @Override\n-            public DimensionSelector makeDimensionSelector(final String dimension)\n-            {\n-              final String dimensionName = dimension.toLowerCase();\n-              return new DimensionSelector()\n-              {\n                 @Override\n-                public IndexedInts getRow()\n+                public DimensionSelector makeDimensionSelector(final String dimension)\n                 {\n-                  final List<String> dimensionValues = in.get().getDimension(dimensionName);\n-                  final ArrayList<Integer> vals = Lists.newArrayList();\n-                  if (dimensionValues != null) {\n-                    for (int i = 0; i < dimensionValues.size(); ++i) {\n-                      vals.add(i);\n+                  final String dimensionName = dimension.toLowerCase();\n+                  return new DimensionSelector()\n+                  {\n+                    @Override\n+                    public IndexedInts getRow()\n+                    {\n+                      final List<String> dimensionValues = in.get().getDimension(dimensionName);\n+                      final ArrayList<Integer> vals = Lists.newArrayList();\n+                      if (dimensionValues != null) {\n+                        for (int i = 0; i < dimensionValues.size(); ++i) {\n+                          vals.add(i);\n+                        }\n+                      }\n+\n+                      return new IndexedInts()\n+                      {\n+                        @Override\n+                        public int size()\n+                        {\n+                          return vals.size();\n+                        }\n+\n+                        @Override\n+                        public int get(int index)\n+                        {\n+                          return vals.get(index);\n+                        }\n+\n+                        @Override\n+                        public Iterator<Integer> iterator()\n+                        {\n+                          return vals.iterator();\n+                        }\n+                      };\n                     }\n-                  }\n \n-                  return new IndexedInts()\n-                  {\n                     @Override\n-                    public int size()\n+                    public int getValueCardinality()\n                     {\n-                      return vals.size();\n+                      throw new UnsupportedOperationException(\"value cardinality is unknown in incremental index\");\n                     }\n \n                     @Override\n-                    public int get(int index)\n+                    public String lookupName(int id)\n                     {\n-                      return vals.get(index);\n+                      return in.get().getDimension(dimensionName).get(id);\n                     }\n \n                     @Override\n-                    public Iterator<Integer> iterator()\n+                    public int lookupId(String name)\n                     {\n-                      return vals.iterator();\n+                      return in.get().getDimension(dimensionName).indexOf(name);\n                     }\n                   };\n                 }\n-\n-                @Override\n-                public int getValueCardinality()\n-                {\n-                  throw new UnsupportedOperationException(\"value cardinality is unknown in incremental index\");\n-                }\n-\n-                @Override\n-                public String lookupName(int id)\n-                {\n-                  return in.get().getDimension(dimensionName).get(id);\n-                }\n-\n-                @Override\n-                public int lookupId(String name)\n-                {\n-                  return in.get().getDimension(dimensionName).indexOf(name);\n-                }\n-              };\n-            }\n-          }\n+              }\n+          )\n       );\n       aggPositionOffsets[i] = currAggSize;\n       currAggSize += agg.getMaxIntermediateSize();\n@@ -458,9 +460,7 @@ public int add(InputRow row)\n     in.set(row);\n     int rowOffset = facts.get(key);\n     for (int i = 0; i < aggs.length; i++) {\n-      synchronized (aggs[i]) {\n         aggs[i].aggregate(bufferHolder.get(), getMetricPosition(rowOffset, i));\n-      }\n     }\n     in.set(null);\n     return numEntries.get();\n@@ -473,7 +473,7 @@ public boolean isEmpty()\n \n   /**\n    *\n-   * @return true if the underlying buffer for IncrementalIndex is full and cannot accomodate more rows.\n+   * @return true if the underlying buffer for IncrementalIndex is full and cannot accommodate more rows.\n    */\n   public boolean isFull()\n   {\n@@ -861,4 +861,45 @@ private void assertSorted()\n       }\n     }\n   }\n+\n+  private static class ThreadSafeAggregator implements BufferAggregator\n+  {\n+\n+    private final BufferAggregator delegate;\n+\n+    public ThreadSafeAggregator(BufferAggregator delegate)\n+    {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public synchronized void init(ByteBuffer buf, int position)\n+    {\n+      delegate.init(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized void aggregate(ByteBuffer buf, int position)\n+    {\n+      delegate.aggregate(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized Object get(ByteBuffer buf, int position)\n+    {\n+      return delegate.get(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized float getFloat(ByteBuffer buf, int position)\n+    {\n+      return delegate.getFloat(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized void close()\n+    {\n+      delegate.close();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "124035e612f6326e6f05d97a17450a131227d481",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "patch": "@@ -36,7 +36,7 @@\n  */\n public class RealtimeTuningConfig implements TuningConfig\n {\n-  private static final int defaultBufferSize = 512 * 1024 * 1024;\n+  private static final int defaultBufferSize = 256 * 1024 * 1024;\n   private static final Period defaultIntermediatePersistPeriod = new Period(\"PT10M\");\n   private static final Period defaultWindowPeriod = new Period(\"PT10M\");\n   private static final File defaultBasePersistDirectory = Files.createTempDir();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "sha": "52df7cafbe4d54075906a502b55e3b570281d02a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "patch": "@@ -136,7 +136,7 @@ public boolean isEmpty()\n   public boolean isFull()\n   {\n     synchronized (currHydrant){\n-      return currHydrant.getIndex().isFull();\n+      return currHydrant != null && currHydrant.getIndex().isFull();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "sha": "033152015206c92bcc9c9635f9695817a37d70f7",
                "status": "modified"
            }
        ],
        "message": "fixes from review comments\n\nfix sync of aggs,\nfix NPE in sink.isFull,\nRealtimeTuningConfig lower the bufferSize to 256m",
        "parent": "https://github.com/apache/incubator-druid/commit/d64879ccca9f2b472d60c5db897f003eb83580cf",
        "repo": "incubator-druid",
        "unit_tests": [
            "SinkTest.java"
        ]
    },
    "incubator-druid_69a8723": {
        "bug_id": "incubator-druid_69a8723",
        "commit": "https://github.com/apache/incubator-druid/commit/69a8723f1851a32c557670f2d0b7c9beb29a78ab",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/69a8723f1851a32c557670f2d0b7c9beb29a78ab/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/CachingQueryRunner.java?ref=69a8723f1851a32c557670f2d0b7c9beb29a78ab",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "patch": "@@ -84,11 +84,16 @@ public CachingQueryRunner(\n         && strategy != null\n         && cacheConfig.isPopulateCache();\n \n-    final Cache.NamedKey key = CacheUtil.computeSegmentCacheKey(\n-        segmentIdentifier,\n-        segmentDescriptor,\n-        strategy.computeCacheKey(query)\n-    );\n+    final Cache.NamedKey key;\n+    if(strategy != null && (useCache || populateCache)) {\n+      key = CacheUtil.computeSegmentCacheKey(\n+          segmentIdentifier,\n+          segmentDescriptor,\n+          strategy.computeCacheKey(query)\n+      );\n+    } else {\n+      key = null;\n+    }\n \n     if(useCache) {\n       final Function cacheFn = strategy.pullFromCache();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/69a8723f1851a32c557670f2d0b7c9beb29a78ab/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "sha": "d92db6415fba83fcc3e18c357f8eff913e3aac43",
                "status": "modified"
            }
        ],
        "message": "fix npe in CachingQueryRunner",
        "parent": "https://github.com/apache/incubator-druid/commit/c05f16917115cf19257e7c928a14d9eeec9b005c",
        "repo": "incubator-druid",
        "unit_tests": [
            "CachingQueryRunnerTest.java"
        ]
    },
    "incubator-druid_7f0319a": {
        "bug_id": "incubator-druid_7f0319a",
        "commit": "https://github.com/apache/incubator-druid/commit/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579/server/src/main/java/io/druid/server/ClientInfoResource.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/ClientInfoResource.java?ref=7f0319a8ae4dc30c2e21a3ace14384eedf3fc579",
                "deletions": 4,
                "filename": "server/src/main/java/io/druid/server/ClientInfoResource.java",
                "patch": "@@ -101,7 +101,12 @@ public ClientInfoResource(\n       @QueryParam(\"interval\") String interval\n   )\n   {\n-    List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final Set<String> dims = Sets.newHashSet();\n+\n+    if (segments == null || segments.isEmpty()) {\n+      return dims;\n+    }\n \n     Interval theInterval;\n     if (interval == null || interval.isEmpty()) {\n@@ -111,7 +116,6 @@ public ClientInfoResource(\n       theInterval = new Interval(interval);\n     }\n \n-    Set<String> dims = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n       if (theInterval.overlaps(segment.getInterval())) {\n         dims.addAll(segment.getDimensions());\n@@ -129,8 +133,13 @@ public ClientInfoResource(\n       @QueryParam(\"interval\") String interval\n   )\n   {\n-    List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final Set<String> metrics = Sets.newHashSet();\n \n+    if (segments == null || segments.isEmpty()) {\n+      return metrics;\n+    }\n+    \n     Interval theInterval;\n     if (interval == null || interval.isEmpty()) {\n       DateTime now = new DateTime();\n@@ -139,7 +148,6 @@ public ClientInfoResource(\n       theInterval = new Interval(interval);\n     }\n \n-    Set<String> metrics = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n       if (theInterval.overlaps(segment.getInterval())) {\n         metrics.addAll(segment.getMetrics());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579/server/src/main/java/io/druid/server/ClientInfoResource.java",
                "sha": "cbd253525bb44c40fa9bde38c7d3e8c8d88b383f",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/174ff66a880e2381bd2c93735265242e3c958b98",
        "repo": "incubator-druid",
        "unit_tests": [
            "ClientInfoResourceTest.java"
        ]
    },
    "incubator-druid_8799d46": {
        "bug_id": "incubator-druid_8799d46",
        "commit": "https://github.com/apache/incubator-druid/commit/8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java?ref=8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
                "deletions": 1,
                "filename": "api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "patch": "@@ -28,7 +28,6 @@\n   public static final long DEFAULT_MAX_CACHE_CAPACITY_BYTES = 1024 * 1024 * 1024; // 1GB\n   public static final long DEFAULT_MAX_FETCH_CAPACITY_BYTES = 1024 * 1024 * 1024; // 1GB\n   public static final long DEFAULT_FETCH_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(60);\n-  public static final int DEFAULT_MAX_FETCH_RETRY = 3;\n \n   // A roughly max size of total fetched objects, but the actual fetched size can be bigger. The reason is our current\n   // client implementations for cloud storages like s3 don't support range scan yet, so we must download the whole file",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "sha": "a1c5fe47a76ccb46c298377bb674c756690166d5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java?ref=8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
                "deletions": 1,
                "filename": "api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "patch": "@@ -91,6 +91,8 @@\n {\n   private static final Logger LOG = new Logger(PrefetchableTextFilesFirehoseFactory.class);\n \n+  public static final int DEFAULT_MAX_FETCH_RETRY = 3;\n+\n   private final CacheManager<T> cacheManager;\n   private final PrefetchConfig prefetchConfig;\n \n@@ -114,7 +116,7 @@ public PrefetchableTextFilesFirehoseFactory(\n     this.cacheManager = new CacheManager<>(\n         prefetchConfig.getMaxCacheCapacityBytes()\n     );\n-    this.maxFetchRetry = maxFetchRetry;\n+    this.maxFetchRetry = maxFetchRetry == null ? DEFAULT_MAX_FETCH_RETRY : maxFetchRetry;\n   }\n \n   @JsonProperty",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "sha": "370262c05937c650b21cc67af9b31dccabe36403",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in PrefetchableTextFilesFirehoseFactory (#5802)",
        "parent": "https://github.com/apache/incubator-druid/commit/bc0ff251a3dcdd3284e92441778805c9d3ac8201",
        "repo": "incubator-druid",
        "unit_tests": [
            "PrefetchableTextFilesFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_9ac5eee": {
        "bug_id": "incubator-druid_9ac5eee",
        "commit": "https://github.com/apache/incubator-druid/commit/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java?ref=9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc",
                "deletions": 1,
                "filename": "indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -146,7 +146,7 @@ public boolean run()\n         log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n \n         if(!groupByJob.waitForCompletion(true)) {\n-          log.error(\"Job failed: %s\", groupByJob.getJobID().toString());\n+          log.error(\"Job failed: %s\", groupByJob.getJobID());\n           return false;\n         }\n       } else {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "sha": "13490e3ef00c344912636faae2c4d3e0a17f663f",
                "status": "modified"
            }
        ],
        "message": "1) Fix NPE in DeterminePartitionsJob when it fails",
        "parent": "https://github.com/apache/incubator-druid/commit/ec2b906fada9255f779d9d3ebfb13ab9f0ef9129",
        "repo": "incubator-druid",
        "unit_tests": [
            "DeterminePartitionsJobTest.java"
        ]
    },
    "incubator-druid_9b5889b": {
        "bug_id": "incubator-druid_9b5889b",
        "commit": "https://github.com/apache/incubator-druid/commit/9b5889b39333127b538e55571d46d79df8b13bef",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9b5889b39333127b538e55571d46d79df8b13bef/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java?ref=9b5889b39333127b538e55571d46d79df8b13bef",
                "deletions": 9,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "patch": "@@ -215,15 +215,18 @@ public TaskStatus call()\n                               }\n \n                               // Override task specific properties\n-                              for (String propName : task.getContext().keySet()) {\n-                                if (propName.startsWith(CHILD_PROPERTY_PREFIX)) {\n-                                  command.add(\n-                                      String.format(\n-                                          \"-D%s=%s\",\n-                                          propName.substring(CHILD_PROPERTY_PREFIX.length()),\n-                                          task.getContextValue(propName)\n-                                      )\n-                                  );\n+                              final Map<String, Object> context = task.getContext();\n+                              if (context != null) {\n+                                for (String propName : context.keySet()) {\n+                                  if (propName.startsWith(CHILD_PROPERTY_PREFIX)) {\n+                                    command.add(\n+                                        String.format(\n+                                            \"-D%s=%s\",\n+                                            propName.substring(CHILD_PROPERTY_PREFIX.length()),\n+                                            task.getContextValue(propName)\n+                                        )\n+                                    );\n+                                  }\n                                 }\n                               }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9b5889b39333127b538e55571d46d79df8b13bef/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "sha": "6d249f4284d03e6cc2f7e6ffbe340910a828e75d",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1703 from metamx/fix-npe\n\nadd null check for task context.",
        "parent": "https://github.com/apache/incubator-druid/commit/75a582974b8d3305981af30e691c67dee35acb63",
        "repo": "incubator-druid",
        "unit_tests": [
            "ForkingTaskRunnerTest.java"
        ]
    },
    "incubator-druid_a0c2ae7": {
        "bug_id": "incubator-druid_a0c2ae7",
        "commit": "https://github.com/apache/incubator-druid/commit/a0c2ae7a388920a6118d4461018247390aea603a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "patch": "@@ -440,7 +440,7 @@ public void run(Context context)\n     public int getPartition(LongWritable interval, BytesWritable text, int numPartitions)\n     {\n \n-      if (\"local\".equals(config.get(\"mapred.job.tracker\")) || determineIntervals) {\n+      if (\"local\".equals(JobHelper.getJobTrackerAddress(config)) || determineIntervals) {\n         return 0;\n       } else {\n         return reducerLookup.get(interval);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "sha": "a85f56602607dd1d5ec6cdd6840ea3ef07a6c127",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -485,7 +485,8 @@ public int getPartition(BytesWritable bytesWritable, Text text, int numPartition\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       final int index = bytes.getInt();\n-      if (config.get(\"mapred.job.tracker\").equals(\"local\")) {\n+      String jobTrackerAddress = JobHelper.getJobTrackerAddress(config);\n+      if (\"local\".equals(jobTrackerAddress)) {\n         return index % numPartitions;\n       } else {\n         if (index >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "sha": "0064a4363d550689f9b972c86e49583aa0502309",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "patch": "@@ -535,7 +535,7 @@ public int getPartition(BytesWritable bytesWritable, Writable value, int numPart\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       int shardNum = bytes.getInt();\n-      if (\"local\".equals(config.get(\"mapreduce.jobtracker.address\")) || \"local\".equals(config.get(\"mapred.job.tracker\"))) {\n+      if (\"local\".equals(JobHelper.getJobTrackerAddress(config))) {\n         return shardNum % numPartitions;\n       } else {\n         if (shardNum >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "sha": "a848714c8eb9cdeebd81169f5108e1e543030f45",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 0,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "patch": "@@ -831,4 +831,14 @@ public static boolean deleteWithRetry(final FileSystem fs, final Path path, fina\n       throw Throwables.propagate(e);\n     }\n   }\n+\n+  public static String getJobTrackerAddress(Configuration config)\n+  {\n+    String jobTrackerAddress = config.get(\"mapred.job.tracker\");\n+    if (jobTrackerAddress == null) {\n+      // New Property name for Hadoop 3.0 and later versions\n+      jobTrackerAddress = config.get(\"mapreduce.jobtracker.address\");\n+    }\n+    return jobTrackerAddress;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "sha": "bbe4f2bc37e307c18cf04b427c9634fc8b5c67f2",
                "status": "modified"
            }
        ],
        "message": "Fix NullPointerException when in DeterminePartitionsJob for Hadoop 3.0 and later versions (#5724)\n\nIn DeterminePartitonsJob -\r\nconfig.get(\"mapred.job.tracker\").equals(\"local\") throws NPE as the\r\nproperty name is changed in hadoop 3.0 to mapreduce.jobtracker.address\r\n\r\nThis patch extracts the logic to fetch jobTrackerAddress in JobHelper\r\nand reuses it when needed.",
        "parent": "https://github.com/apache/incubator-druid/commit/754c80e74a2dc725d79569e89f95682b209a518c",
        "repo": "incubator-druid",
        "unit_tests": [
            "JobHelperTest.java"
        ]
    },
    "incubator-druid_a4777ed": {
        "bug_id": "incubator-druid_a4777ed",
        "commit": "https://github.com/apache/incubator-druid/commit/a4777ede94a4798c19859e57ed82bd3a3ecedf67",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a4777ede94a4798c19859e57ed82bd3a3ecedf67/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java?ref=a4777ede94a4798c19859e57ed82bd3a3ecedf67",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "patch": "@@ -255,13 +255,18 @@ public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> d\n                       final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n                       for (String dim : dims) {\n                         final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim);\n-                        dimSelectors.put(dim, dimSelector);\n+                        // dimSelector is null if the dimension is not present\n+                        if (dimSelector != null) {\n+                          dimSelectors.put(dim, dimSelector);\n+                        }\n                       }\n \n                       final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n                       for (String metric : metrics) {\n                         final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n-                        metSelectors.put(metric, metricSelector);\n+                        if (metricSelector != null) {\n+                          metSelectors.put(metric, metricSelector);\n+                        }\n                       }\n \n                       return Sequences.simple(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a4777ede94a4798c19859e57ed82bd3a3ecedf67/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "sha": "5744fa7f006f3e46227093b2be78e6da7545fba2",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #747 from metamx/ingest-segment-npe-fix\n\nfix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/a17794a51668cb83d07a4c7220960bfc29ad309e",
        "repo": "incubator-druid",
        "unit_tests": [
            "IngestSegmentFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_a72c442": {
        "bug_id": "incubator-druid_a72c442",
        "commit": "https://github.com/apache/incubator-druid/commit/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java?ref=a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719",
                "deletions": 8,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "patch": "@@ -61,6 +61,7 @@\n import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;\n import org.apache.curator.utils.ZKPaths;\n import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n import org.jboss.netty.handler.codec.http.HttpResponseStatus;\n import org.joda.time.DateTime;\n \n@@ -335,7 +336,7 @@ public void shutdown(final String taskId)\n       pendingTaskPayloads.remove(taskId);\n       log.info(\"Removed task from pending queue: %s\", taskId);\n     } else if (completeTasks.containsKey(taskId)) {\n-      cleanup(completeTasks.get(taskId).getWorker().getHost(), taskId);\n+      cleanup(taskId);\n     } else {\n       final ZkWorker zkWorker = findWorkerRunningTask(taskId);\n \n@@ -469,28 +470,32 @@ public Void call() throws Exception\n   /**\n    * Removes a task from the complete queue and clears out the ZK status path of the task.\n    *\n-   * @param workerId - the worker that was previously running the task\n    * @param taskId   - the task to cleanup\n    */\n-  private void cleanup(final String workerId, final String taskId)\n+  private void cleanup(final String taskId)\n   {\n     if (!started) {\n       return;\n     }\n-    if (completeTasks.remove(taskId) == null) {\n+    final RemoteTaskRunnerWorkItem removed = completeTasks.remove(taskId);\n+    final Worker worker = removed.getWorker();\n+    if (removed == null || worker == null) {\n       log.makeAlert(\"WTF?! Asked to cleanup nonexistent task\")\n-         .addData(\"workerId\", workerId)\n          .addData(\"taskId\", taskId)\n          .emit();\n     } else {\n+      final String workerId = worker.getHost();\n       log.info(\"Cleaning up task[%s] on worker[%s]\", taskId, workerId);\n       final String statusPath = JOINER.join(zkPaths.getIndexerStatusPath(), workerId, taskId);\n       try {\n         cf.delete().guaranteed().forPath(statusPath);\n       }\n-      catch (Exception e) {\n+      catch (KeeperException.NoNodeException e) {\n         log.info(\"Tried to delete status path[%s] that didn't exist! Must've gone away already?\", statusPath);\n       }\n+      catch (Exception e) {\n+        throw Throwables.propagate(e);\n+      }\n     }\n   }\n \n@@ -593,7 +598,6 @@ private void announceTask(\n               elapsed,\n               config.getTaskAssignmentTimeout()\n           );\n-\n           taskComplete(taskRunnerWorkItem, theZkWorker, TaskStatus.failure(task.getId()));\n           break;\n         }\n@@ -666,7 +670,7 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) th\n                             SettableFuture.<TaskStatus>create(),\n                             zkWorker.getWorker()\n                         );\n-                        runningTasks.put(taskId, taskRunnerWorkItem);\n+                        runningTasks.put(taskId, taskRunnerWorkItem.withWorker(zkWorker.getWorker()));\n                       }\n \n                       if (taskStatus.isComplete()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "sha": "0dbc9031200fa1ac43d9ff376e914fc1153f79ba",
                "status": "modified"
            }
        ],
        "message": "RemoteTaskRunner: Fix NPE on cleanup due to missing withWorker",
        "parent": "https://github.com/apache/incubator-druid/commit/35ed3f74bf4963670061cd05881af93f4d50b27c",
        "repo": "incubator-druid",
        "unit_tests": [
            "RemoteTaskRunnerTest.java"
        ]
    },
    "incubator-druid_bcfeac2": {
        "bug_id": "incubator-druid_bcfeac2",
        "commit": "https://github.com/apache/incubator-druid/commit/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223/server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/coordination/ServerManager.java?ref=bcfeac2d8c4d75589c1c74d52bb9c92cb1596223",
                "deletions": 0,
                "filename": "server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "patch": "@@ -20,6 +20,7 @@\n package com.metamx.druid.coordination;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Predicate;\n import com.google.common.collect.Ordering;\n import com.metamx.common.ISE;\n import com.metamx.common.guava.FunctionalIterable;\n@@ -257,6 +258,16 @@ public void dropSegment(final DataSegment segment) throws SegmentLoadingExceptio\n                     );\n               }\n             }\n+        )\n+        .filter(\n+            new Predicate<QueryRunner<T>>()\n+            {\n+              @Override\n+              public boolean apply(@Nullable QueryRunner<T> input)\n+              {\n+                return (input != null);\n+              }\n+            }\n         );\n \n     return new FinalizeResultsQueryRunner<T>(toolChest.mergeResults(factory.mergeRunners(exec, adapters)), toolChest);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223/server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "sha": "00721d042ade6264029922e342d692a413cab197",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/8e38a85788e07852960b0b148e110a0ce2e33078",
        "repo": "incubator-druid",
        "unit_tests": [
            "ServerManagerTest.java"
        ]
    },
    "incubator-druid_c56a980": {
        "bug_id": "incubator-druid_c56a980",
        "commit": "https://github.com/apache/incubator-druid/commit/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=c56a9807d4bb95e0e7280476e8f4607bc62b1e4f",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -2117,7 +2117,7 @@ private void updateLatestOffsetsFromKafka()\n                      && latestOffsetsFromKafka.get(e.getKey()) != null\n                      && e.getValue() != null\n                      ? latestOffsetsFromKafka.get(e.getKey()) - e.getValue()\n-                     : null\n+                     : Integer.MIN_VALUE\n             )\n         );\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "e9165ffbe3472dcdf98265e1c4d4832845f085b9",
                "status": "modified"
            }
        ],
        "message": "prevent npe on mismatch between number of kafka partitions and task count (#5139)",
        "parent": "https://github.com/apache/incubator-druid/commit/11814119019232e7c17d68b665526f6462c3e521",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaSupervisorTest.java"
        ]
    },
    "incubator-druid_d02f152": {
        "bug_id": "incubator-druid_d02f152",
        "commit": "https://github.com/apache/incubator-druid/commit/d02f152498750f213c38e35a41e0d5e2d6a3f55a",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d02f152498750f213c38e35a41e0d5e2d6a3f55a/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java?ref=d02f152498750f213c38e35a41e0d5e2d6a3f55a",
                "deletions": 0,
                "filename": "client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "patch": "@@ -288,6 +288,10 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) th\n \n         final String inventoryKey = ZKPaths.getNodeFromPath(child.getPath());\n \n+        if (inventoryKey == null) {\n+          return;\n+        }\n+\n         switch (event.getType()) {\n           case CHILD_ADDED:\n           case CHILD_UPDATED:",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d02f152498750f213c38e35a41e0d5e2d6a3f55a/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "sha": "bfd75488be848bd06ffedd0c79c645f710b35bf9",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/5a57539736770af6e581622f48e0d2617f57dd1f",
        "repo": "incubator-druid",
        "unit_tests": [
            "CuratorInventoryManagerTest.java"
        ]
    },
    "incubator-druid_d63107f": {
        "bug_id": "incubator-druid_d63107f",
        "commit": "https://github.com/apache/incubator-druid/commit/d63107f8908f64360be620ec95000696811e033d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -74,7 +74,7 @@\n             <dependency>\n                 <groupId>com.metamx</groupId>\n                 <artifactId>emitter</artifactId>\n-                <version>0.2.10</version>\n+                <version>0.2.11</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.metamx</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/pom.xml",
                "sha": "c0c9f69a4b0d6a35510b353467ad9dfe14215e95",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -35,8 +35,10 @@\n import java.util.Arrays;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n /**\n@@ -110,7 +112,11 @@ public ChainedExecutionQueryRunner(\n                                   if (input == null) {\n                                     throw new ISE(\"Input is null?! How is this possible?!\");\n                                   }\n-                                  return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                  Sequence<T> result = input.run(query);\n+                                  if (result == null) {\n+                                    throw new ISE(\"Got a null result! Segments are missing!\");\n+                                  }\n+                                  return Sequences.toList(result, Lists.<T>newArrayList());\n                                 }\n                                 catch (Exception e) {\n                                   log.error(e, \"Exception with one of the sequences!\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "4dbb75b3b04064a4442977f3f05a661437a16586",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 10,
                "filename": "processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "patch": "@@ -70,7 +70,7 @@\n   private final StupidPool<ByteBuffer> intermediateResultsBufferPool;\n \n   @Inject\n-  public GroupByQueryEngine (\n+  public GroupByQueryEngine(\n       Supplier<GroupByQueryConfig> config,\n       @Global StupidPool<ByteBuffer> intermediateResultsBufferPool\n   )\n@@ -81,6 +81,12 @@ public GroupByQueryEngine (\n \n   public Sequence<Row> process(final GroupByQuery query, StorageAdapter storageAdapter)\n   {\n+    if (storageAdapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> intervals = query.getQuerySegmentSpec().getIntervals();\n     if (intervals.size() != 1) {\n       throw new IAE(\"Should only have one interval, got[%s]\", intervals);\n@@ -187,8 +193,7 @@ public int getNumRows()\n           ByteBuffer newKey = key.duplicate();\n           newKey.putInt(dimSelector.getValueCardinality());\n           unaggregatedBuffers = updateValues(newKey, dims.subList(1, dims.size()));\n-        }\n-        else {\n+        } else {\n           for (Integer dimValue : row) {\n             ByteBuffer newKey = key.duplicate();\n             newKey.putInt(dimValue);\n@@ -202,8 +207,7 @@ public int getNumRows()\n           retVal.addAll(unaggregatedBuffers);\n         }\n         return retVal;\n-      }\n-      else {\n+      } else {\n         key.clear();\n         Integer position = positions.get(key);\n         int[] increments = positionMaintainer.getIncrements();\n@@ -267,8 +271,7 @@ public Integer getNext()\n     {\n       if (nextVal > max) {\n         return null;\n-      }\n-      else {\n+      } else {\n         int retVal = (int) nextVal;\n         nextVal += increment;\n         return retVal;\n@@ -402,7 +405,7 @@ public Row apply(@Nullable Map.Entry<ByteBuffer, Integer> input)\n                     final DimExtractionFn fn = dimensionSpecs.get(i).getDimExtractionFn();\n                     final int dimVal = keyBuffer.getInt();\n                     if (dimSelector.getValueCardinality() != dimVal) {\n-                      if(fn != null) {\n+                      if (fn != null) {\n                         theEvent.put(dimNames.get(i), fn.apply(dimSelector.lookupName(dimVal)));\n                       } else {\n                         theEvent.put(dimNames.get(i), dimSelector.lookupName(dimVal));\n@@ -434,9 +437,10 @@ public void remove()\n       throw new UnsupportedOperationException();\n     }\n \n-    public void close() {\n+    public void close()\n+    {\n       // cleanup\n-      for(BufferAggregator agg : aggregators) {\n+      for (BufferAggregator agg : aggregators) {\n         agg.close();\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "sha": "1c75f1390f17561fc5df2d2a388dff3fce03b5ab",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 35,
                "filename": "processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "patch": "@@ -104,47 +104,47 @@\n   )\n   {\n     return new ConcatQueryRunner<SegmentAnalysis>(\n-            Sequences.map(\n-                Sequences.simple(queryRunners),\n-                new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+        Sequences.map(\n+            Sequences.simple(queryRunners),\n+            new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+            {\n+              @Override\n+              public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+              {\n+                return new QueryRunner<SegmentAnalysis>()\n                 {\n                   @Override\n-                  public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+                  public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n                   {\n-                    return new QueryRunner<SegmentAnalysis>()\n-                    {\n-                      @Override\n-                      public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n-                      {\n \n-                        Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n-                            new Callable<Sequence<SegmentAnalysis>>()\n-                            {\n-                              @Override\n-                              public Sequence<SegmentAnalysis> call() throws Exception\n-                              {\n-                                return new ExecutorExecutingSequence<SegmentAnalysis>(\n-                                    input.run(query),\n-                                    queryExecutor\n-                                );\n-                              }\n-                            }\n-                        );\n-                        try {\n-                          return future.get();\n+                    Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n+                        new Callable<Sequence<SegmentAnalysis>>()\n+                        {\n+                          @Override\n+                          public Sequence<SegmentAnalysis> call() throws Exception\n+                          {\n+                            return new ExecutorExecutingSequence<SegmentAnalysis>(\n+                                input.run(query),\n+                                queryExecutor\n+                            );\n+                          }\n                         }\n-                        catch (InterruptedException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                        catch (ExecutionException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                      }\n-                    };\n+                    );\n+                    try {\n+                      return future.get();\n+                    }\n+                    catch (InterruptedException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n+                    catch (ExecutionException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n                   }\n-                }\n-            )\n-        );\n+                };\n+              }\n+            }\n+        )\n+    );\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "sha": "c5b64c2d46ac1af275c5a42451015e56b30b5ca5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "patch": "@@ -32,7 +32,6 @@\n \n public class SegmentMetadataQuery extends BaseQuery<SegmentAnalysis>\n {\n-\n   private final ColumnIncluderator toInclude;\n   private final boolean merge;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "sha": "c6d6ecca0bde4d22a13a50f3b2ebdaccb84a3ffa",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 38,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "patch": "@@ -55,7 +55,7 @@\n import java.util.TreeSet;\n \n /**\n-*/\n+ */\n public class SearchQueryRunner implements QueryRunner<Result<SearchResultValue>>\n {\n   private static final EmittingLogger log = new EmittingLogger(SearchQueryRunner.class);\n@@ -99,12 +99,10 @@ public SearchQueryRunner(Segment segment)\n           ConciseSet set = new ConciseSet();\n           set.add(0);\n           baseFilter = ImmutableConciseSet.newImmutableFromMutable(set);\n-        }\n-        else {\n+        } else {\n           baseFilter = ImmutableConciseSet.complement(new ImmutableConciseSet(), index.getNumRows());\n         }\n-      }\n-      else {\n+      } else {\n         baseFilter = filter.goConcise(new ColumnSelectorBitmapIndexSelector(index));\n       }\n \n@@ -133,49 +131,52 @@ public SearchQueryRunner(Segment segment)\n     }\n \n     final StorageAdapter adapter = segment.asStorageAdapter();\n-    if (adapter != null) {\n-      Iterable<String> dimsToSearch;\n-      if (dimensions == null || dimensions.isEmpty()) {\n-        dimsToSearch = adapter.getAvailableDimensions();\n-      } else {\n-        dimsToSearch = dimensions;\n-      }\n \n-      final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n+    if (adapter == null) {\n+      log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n+         .addData(\"segment\", segment.getIdentifier())\n+         .addData(\"query\", query).emit();\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n \n-      final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n-      for (Cursor cursor : cursors) {\n-        Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n-        for (String dim : dimsToSearch) {\n-          dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n-        }\n+    Iterable<String> dimsToSearch;\n+    if (dimensions == null || dimensions.isEmpty()) {\n+      dimsToSearch = adapter.getAvailableDimensions();\n+    } else {\n+      dimsToSearch = dimensions;\n+    }\n+\n+    final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n \n-        while (!cursor.isDone()) {\n-          for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n-            final DimensionSelector selector = entry.getValue();\n-            final IndexedInts vals = selector.getRow();\n-            for (int i = 0; i < vals.size(); ++i) {\n-              final String dimVal = selector.lookupName(vals.get(i));\n-              if (searchQuerySpec.accept(dimVal)) {\n-                retVal.add(new SearchHit(entry.getKey(), dimVal));\n-                if (retVal.size() >= limit) {\n-                  return makeReturnResult(limit, retVal);\n-                }\n+    final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n+    for (Cursor cursor : cursors) {\n+      Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n+      for (String dim : dimsToSearch) {\n+        dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n+      }\n+\n+      while (!cursor.isDone()) {\n+        for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n+          final DimensionSelector selector = entry.getValue();\n+          final IndexedInts vals = selector.getRow();\n+          for (int i = 0; i < vals.size(); ++i) {\n+            final String dimVal = selector.lookupName(vals.get(i));\n+            if (searchQuerySpec.accept(dimVal)) {\n+              retVal.add(new SearchHit(entry.getKey(), dimVal));\n+              if (retVal.size() >= limit) {\n+                return makeReturnResult(limit, retVal);\n               }\n             }\n           }\n-\n-          cursor.advance();\n         }\n-      }\n \n-      return makeReturnResult(limit, retVal);\n+        cursor.advance();\n+      }\n     }\n \n-    log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n-       .addData(\"segment\", segment.getIdentifier())\n-       .addData(\"query\", query);\n-    return Sequences.empty();\n+    return makeReturnResult(limit, retVal);\n   }\n \n   private Sequence<Result<SearchResultValue>> makeReturnResult(int limit, TreeSet<SearchHit> retVal)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "sha": "4070a274b4fe374a6d2e582e0d8c4631beb43050",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Function;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -54,6 +55,12 @@\n           {\n             final StorageAdapter adapter = segment.asStorageAdapter();\n \n+            if (adapter == null) {\n+              throw new ISE(\n+                  \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+              );\n+            }\n+\n             final Iterable<String> dims;\n             if (query.getDimensions() == null || query.getDimensions().isEmpty()) {\n               dims = adapter.getAvailableDimensions();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "sha": "f5bc7aba043c425bffb6d40885d082a07136ffcd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "patch": "@@ -87,6 +87,12 @@ public TimeBoundaryQueryRunner(Segment segment)\n             @Override\n             public Iterator<Result<TimeBoundaryResultValue>> make()\n             {\n+              if (adapter == null) {\n+                throw new ISE(\n+                    \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+                );\n+              }\n+\n               return legacyQuery.buildResult(\n                   adapter.getInterval().getStart(),\n                   adapter.getMinTime(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "sha": "16e9ae832fae74154c2ad1380350931455320343",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "patch": "@@ -20,6 +20,7 @@\n package io.druid.query.timeseries;\n \n import com.google.common.base.Function;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -40,6 +41,12 @@\n {\n   public Sequence<Result<TimeseriesResultValue>> process(final TimeseriesQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     return new BaseSequence<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>(\n         new BaseSequence.IteratorMaker<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>()\n         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "sha": "6ab42477890e9791a42cc60e4066be26f02d033e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 12,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "patch": "@@ -21,7 +21,7 @@\n \n import com.google.common.base.Function;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.FunctionalIterable;\n import com.metamx.common.logger.Logger;\n import io.druid.collections.StupidPool;\n@@ -53,6 +53,12 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n \n   public Iterable<Result<TopNResultValue>> query(final TopNQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> queryIntervals = query.getQuerySegmentSpec().getIntervals();\n     final Filter filter = Filters.convertDimensionFilters(query.getDimensionsFilter());\n     final QueryGranularity granularity = query.getGranularity();\n@@ -62,10 +68,6 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n         queryIntervals.size() == 1, \"Can only handle a single interval, got[%s]\", queryIntervals\n     );\n \n-    if (mapFn == null) {\n-      return Lists.newArrayList();\n-    }\n-\n     return FunctionalIterable\n         .create(adapter.makeCursors(filter, queryIntervals.get(0), granularity))\n         .transform(\n@@ -84,13 +86,6 @@ public Cursor apply(Cursor input)\n \n   private Function<Cursor, Result<TopNResultValue>> getMapFn(TopNQuery query, final StorageAdapter adapter)\n   {\n-    if (adapter == null) {\n-      log.warn(\n-          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped. Returning empty results.\"\n-      );\n-      return null;\n-    }\n-\n     final Capabilities capabilities = adapter.getCapabilities();\n     final int cardinality = adapter.getDimensionCardinality(query.getDimensionSpec().getDimension());\n     int numBytesPerRecord = 0;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "sha": "1f3a8892733b2f3f2fc9a84a5eb92bade1224721",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 13,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -29,8 +29,7 @@\n import com.google.inject.Inject;\n import com.metamx.common.guava.Sequence;\n import com.metamx.common.guava.Sequences;\n-import com.metamx.common.logger.Logger;\n-import com.metamx.emitter.service.AlertEvent;\n+import com.metamx.emitter.EmittingLogger;\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n import io.druid.guice.annotations.Json;\n@@ -57,7 +56,7 @@\n @Path(\"/druid/v2/\")\n public class QueryResource\n {\n-  private static final Logger log = new Logger(QueryResource.class);\n+  private static final EmittingLogger log = new EmittingLogger(QueryResource.class);\n   private static final Charset UTF8 = Charset.forName(\"UTF-8\");\n   private static final Joiner COMMA_JOIN = Joiner.on(\",\");\n \n@@ -192,16 +191,11 @@ public void doPost(\n         log.error(e2, \"Unable to log query [%s]!\", queryString);\n       }\n \n-      emitter.emit(\n-          new AlertEvent.Builder().build(\n-              \"Exception handling request\",\n-              ImmutableMap.<String, Object>builder()\n-                          .put(\"exception\", e.toString())\n-                          .put(\"query\", queryString)\n-                          .put(\"peer\", req.getRemoteAddr())\n-                          .build()\n-          )\n-      );\n+      log.makeAlert(e, \"Exception handling request\")\n+         .addData(\"exception\", e.toString())\n+         .addData(\"query\", queryString)\n+         .addData(\"peer\", req.getRemoteAddr())\n+         .emit();\n     }\n     finally {\n       resp.flushBuffer();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "77d0c1bf04c0eba991bf85579a1def6800eb9347",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #502 from metamx/fix-NPE-CQE\n\nBetter handle null adapters and NPEs in the CQE",
        "parent": "https://github.com/apache/incubator-druid/commit/560045ddd2a37ed96703d31623e059248c62b944",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_e2653a8": {
        "bug_id": "incubator-druid_e2653a8",
        "commit": "https://github.com/apache/incubator-druid/commit/e2653a8cf4a1d727c5a0e0406092002e68634052",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e2653a8cf4a1d727c5a0e0406092002e68634052/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java?ref=e2653a8cf4a1d727c5a0e0406092002e68634052",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "patch": "@@ -551,6 +551,10 @@ public void run()\n                 LOG.info(\"Not started. Returning\");\n                 return;\n               }\n+              if (allLookupTiers == null) {\n+                LOG.info(\"Not updating lookups because no data exists\");\n+                return;\n+              }\n               for (final String tier : allLookupTiers.keySet()) {\n                 try {\n                   final Map<String, Map<String, Object>> allLookups = allLookupTiers.get(tier);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e2653a8cf4a1d727c5a0e0406092002e68634052/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "sha": "f9aaee2f64fb7ab6a0b27c5099a7e03a31c133a2",
                "status": "modified"
            }
        ],
        "message": "handle a NPE in LookupCoordinatorManager.start() (#3026)",
        "parent": "https://github.com/apache/incubator-druid/commit/ebb6831770e6da5d84b1c39c3e6d27554852e784",
        "repo": "incubator-druid",
        "unit_tests": [
            "LookupCoordinatorManagerTest.java"
        ]
    },
    "incubator-druid_e6d93a3": {
        "bug_id": "incubator-druid_e6d93a3",
        "commit": "https://github.com/apache/incubator-druid/commit/e6d93a307019406c9e4ec26d9d865fd3717feeb7",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e6d93a307019406c9e4ec26d9d865fd3717feeb7/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java?ref=e6d93a307019406c9e4ec26d9d865fd3717feeb7",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "patch": "@@ -255,13 +255,18 @@ public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> d\n                       final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n                       for (String dim : dims) {\n                         final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim);\n-                        dimSelectors.put(dim, dimSelector);\n+                        // dimSelector is null if the dimension is not present\n+                        if (dimSelector != null) {\n+                          dimSelectors.put(dim, dimSelector);\n+                        }\n                       }\n \n                       final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n                       for (String metric : metrics) {\n                         final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n-                        metSelectors.put(metric, metricSelector);\n+                        if (metricSelector != null) {\n+                          metSelectors.put(metric, metricSelector);\n+                        }\n                       }\n \n                       return Sequences.simple(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e6d93a307019406c9e4ec26d9d865fd3717feeb7/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "sha": "5744fa7f006f3e46227093b2be78e6da7545fba2",
                "status": "modified"
            }
        ],
        "message": "fix NPE\n\nfix NPE when the dimension of metric is not present one of the segments\nto be reIndexed.",
        "parent": "https://github.com/apache/incubator-druid/commit/a17794a51668cb83d07a4c7220960bfc29ad309e",
        "repo": "incubator-druid",
        "unit_tests": [
            "IngestSegmentFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_f2e0dc9": {
        "bug_id": "incubator-druid_f2e0dc9",
        "commit": "https://github.com/apache/incubator-druid/commit/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=f2e0dc9327aaacf8ca479b712eb40c04cbde7cab",
                "deletions": 15,
                "filename": "s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n import com.google.inject.Inject;\n+import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n@@ -120,23 +121,30 @@ public Void call() throws Exception\n               if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n                 if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) {\n                   log.info(\"No need to move file[s3://%s/%s] onto itself\", s3Bucket, s3Path);\n-                } else if (s3Client.getObjectDetails(s3Bucket, s3Path)\n-                                   .getStorageClass()\n-                                   .equals(S3Object.STORAGE_CLASS_GLACIER)) {\n-                  log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier.\");\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n-                      s3Path,\n-                      targetS3Bucket,\n-                      targetS3Path\n-                  );\n-                  final S3Object target = new S3Object(targetS3Path);\n-                  if(!config.getDisableAcl()) {\n-                    target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                  final S3Object[] list = s3Client.listObjects(s3Bucket, s3Path, \"\");\n+                  if (list.length == 0) {\n+                    // should never happen\n+                    throw new ISE(\"Unable to list object [s3://%s/%s]\", s3Bucket, s3Path);\n+                  }\n+                  final S3Object s3Object = list[0];\n+                  if (s3Object.getStorageClass() != null &&\n+                      s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n+                    log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\");\n+                  } else {\n+                    log.info(\n+                        \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n+                        s3Bucket,\n+                        s3Path,\n+                        targetS3Bucket,\n+                        targetS3Path\n+                    );\n+                    final S3Object target = new S3Object(targetS3Path);\n+                    if (!config.getDisableAcl()) {\n+                      target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                    }\n+                    s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "379dd8374fab0d5c67b1601469d9302226318c93",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #637 from metamx/fix-s3-storageclass-check\n\nfix storage class npe",
        "parent": "https://github.com/apache/incubator-druid/commit/7a7386be74247dbfc42963dac491e8fcddad7eeb",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    },
    "incubator-druid_f36d030": {
        "bug_id": "incubator-druid_f36d030",
        "commit": "https://github.com/apache/incubator-druid/commit/f36d030ef20e9cf810ae229fa673b61ee6f69271",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f36d030ef20e9cf810ae229fa673b61ee6f69271/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=f36d030ef20e9cf810ae229fa673b61ee6f69271",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -286,11 +286,6 @@ public int lookupId(String name)\n \n     this.dimensionOrder = Maps.newLinkedHashMap();\n     this.dimensions = new CopyOnWriteArrayList<>();\n-    int index = 0;\n-    for (String dim : incrementalIndexSchema.getDimensionsSpec().getDimensions()) {\n-      dimensionOrder.put(dim, index++);\n-      dimensions.add(dim);\n-    }\n     // This should really be more generic\n     List<SpatialDimensionSchema> spatialDimensions = incrementalIndexSchema.getDimensionsSpec().getSpatialDimensions();\n     if (!spatialDimensions.isEmpty()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f36d030ef20e9cf810ae229fa673b61ee6f69271/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "26985edc024b7e154d53d222b29fb0f0e4b6b61d",
                "status": "modified"
            }
        ],
        "message": "fix NPE while indexing",
        "parent": "https://github.com/apache/incubator-druid/commit/c6712739dc499fbae8cdfd063ef9931626675626",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexTest.java"
        ]
    },
    "incubator-druid_f56d60b": {
        "bug_id": "incubator-druid_f56d60b",
        "commit": "https://github.com/apache/incubator-druid/commit/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=f56d60b4511d15c3caaeda1dee56b8deb3f14c4b",
                "deletions": 15,
                "filename": "s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n import com.google.inject.Inject;\n+import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n@@ -120,23 +121,30 @@ public Void call() throws Exception\n               if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n                 if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) {\n                   log.info(\"No need to move file[s3://%s/%s] onto itself\", s3Bucket, s3Path);\n-                } else if (s3Client.getObjectDetails(s3Bucket, s3Path)\n-                                   .getStorageClass()\n-                                   .equals(S3Object.STORAGE_CLASS_GLACIER)) {\n-                  log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier.\");\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n-                      s3Path,\n-                      targetS3Bucket,\n-                      targetS3Path\n-                  );\n-                  final S3Object target = new S3Object(targetS3Path);\n-                  if(!config.getDisableAcl()) {\n-                    target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                  final S3Object[] list = s3Client.listObjects(s3Bucket, s3Path, \"\");\n+                  if (list.length == 0) {\n+                    // should never happen\n+                    throw new ISE(\"Unable to list object [s3://%s/%s]\", s3Bucket, s3Path);\n+                  }\n+                  final S3Object s3Object = list[0];\n+                  if (s3Object.getStorageClass() != null &&\n+                      s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n+                    log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\");\n+                  } else {\n+                    log.info(\n+                        \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n+                        s3Bucket,\n+                        s3Path,\n+                        targetS3Bucket,\n+                        targetS3Path\n+                    );\n+                    final S3Object target = new S3Object(targetS3Path);\n+                    if (!config.getDisableAcl()) {\n+                      target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                    }\n+                    s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "379dd8374fab0d5c67b1601469d9302226318c93",
                "status": "modified"
            }
        ],
        "message": "fix storage class npe",
        "parent": "https://github.com/apache/incubator-druid/commit/b8347cf4af247b259cda138a40cf45236528b85f",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    }
}