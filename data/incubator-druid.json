{
    "incubator-druid_04d3054": {
        "bug_id": "incubator-druid_04d3054",
        "commit": "https://github.com/apache/incubator-druid/commit/04d30543538deac2b6fb5263b66d04e6e0522966",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/04d30543538deac2b6fb5263b66d04e6e0522966/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java?ref=04d30543538deac2b6fb5263b66d04e6e0522966",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Throwables;\n import com.google.common.collect.Lists;\n+import com.metamx.common.IAE;\n import com.metamx.common.ISE;\n import com.metamx.emitter.EmittingLogger;\n import io.druid.data.input.Firehose;\n@@ -84,6 +85,9 @@ public StringInputRowParser getParser()\n   @Override\n   public Firehose connect(StringInputRowParser firehoseParser) throws IOException\n   {\n+    if (baseDir == null) {\n+      throw new IAE(\"baseDir is null\");\n+    }\n     log.info(\"Searching for all [%s] in and beneath [%s]\", filter, baseDir.getAbsoluteFile());\n \n     Collection<File> foundFiles = FileUtils.listFiles(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/04d30543538deac2b6fb5263b66d04e6e0522966/server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
                "sha": "4d530f4ba738862d3f6a2d60f1fa479822475615",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #2303 from CHOIJAEHONG1/localfirehouse-basedir-npe\n\nThrow an IAE when baseDir is null in LocalFireHose",
        "parent": "https://github.com/apache/incubator-druid/commit/201539260c4d111530c0699f6bf0a799a68f1472",
        "repo": "incubator-druid",
        "unit_tests": [
            "LocalFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_0712941": {
        "bug_id": "incubator-druid_0712941",
        "commit": "https://github.com/apache/incubator-druid/commit/07129418257496c2ceb4b716c40c68717d28036b",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/07129418257496c2ceb4b716c40c68717d28036b/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java?ref=07129418257496c2ceb4b716c40c68717d28036b",
                "deletions": 0,
                "filename": "realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "patch": "@@ -662,6 +662,14 @@ private File computePersistDir(Schema schema, Interval interval)\n    */\n   private int persistHydrant(FireHydrant indexToPersist, Schema schema, Interval interval)\n   {\n+    if (indexToPersist.hasSwapped()) {\n+      log.info(\n+          \"DataSource[%s], Interval[%s], Hydrant[%s] already swapped. Ignoring request to persist.\",\n+          schema.getDataSource(), interval, indexToPersist\n+      );\n+      return 0;\n+    }\n+\n     log.info(\"DataSource[%s], Interval[%s], persisting Hydrant[%s]\", schema.getDataSource(), interval, indexToPersist);\n     try {\n       int numRows = indexToPersist.getIndex().size();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/07129418257496c2ceb4b716c40c68717d28036b/realtime/src/main/java/com/metamx/druid/realtime/plumber/RealtimePlumberSchool.java",
                "sha": "a429fbef9d589aff71b1b87e068b81bb20d5f640",
                "status": "modified"
            }
        ],
        "message": "1) Add check whether a Hydrant has already been persisted before persisting.  Persisting happens synchronously on the same thread, but multiple persist requests can be queued up on that thread which means that subsequent ones would fail with an NPE.  Fixes #178",
        "parent": "https://github.com/apache/incubator-druid/commit/fd1d73e83abdff8dcea9d3c8bcc9c3278ce92dea",
        "repo": "incubator-druid",
        "unit_tests": [
            "RealtimePlumberSchoolTest.java"
        ]
    },
    "incubator-druid_08a527d": {
        "bug_id": "incubator-druid_08a527d",
        "commit": "https://github.com/apache/incubator-druid/commit/08a527d01ab7209c92918ddb2ad5dee478e13d5d",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/08a527d01ab7209c92918ddb2ad5dee478e13d5d/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java?ref=08a527d01ab7209c92918ddb2ad5dee478e13d5d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "patch": "@@ -20,6 +20,7 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n import com.metamx.common.StringUtils;\n \n import java.nio.ByteBuffer;\n@@ -59,8 +60,11 @@ public RegexDimExtractionFn(\n   @Override\n   public String apply(String dimValue)\n   {\n+    if (dimValue == null) {\n+      return null;\n+    }\n     Matcher matcher = pattern.matcher(dimValue);\n-    return matcher.find() ? matcher.group(1) : dimValue;\n+    return Strings.emptyToNull(matcher.find() ? matcher.group(1) : dimValue);\n   }\n \n   @JsonProperty(\"expr\")",
                "raw_url": "https://github.com/apache/incubator-druid/raw/08a527d01ab7209c92918ddb2ad5dee478e13d5d/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "sha": "b3c33b68e140b9419a8817e14d0329a83465ed78",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/08a527d01ab7209c92918ddb2ad5dee478e13d5d/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java?ref=08a527d01ab7209c92918ddb2ad5dee478e13d5d",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "patch": "@@ -102,6 +102,20 @@ public void testStringExtraction()\n     Assert.assertTrue(extracted.contains(\"c\"));\n   }\n \n+\n+  @Test\n+  public void testNullAndEmpty()\n+  {\n+    String regex = \"(.*)/.*/.*\";\n+    ExtractionFn extractionFn = new RegexDimExtractionFn(regex);\n+    // no match, map empty input value to null\n+    Assert.assertEquals(null, extractionFn.apply(\"\"));\n+    // null value, returns null\n+    Assert.assertEquals(null, extractionFn.apply(null));\n+    // empty match, map empty result to null\n+    Assert.assertEquals(null, extractionFn.apply(\"/a/b\"));\n+  }\n+\n   @Test\n   public void testSerde() throws Exception\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/08a527d01ab7209c92918ddb2ad5dee478e13d5d/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "sha": "3da70ba3f533d3cce13b688388f6595356bb0ae7",
                "status": "modified"
            }
        ],
        "message": "fix NPE with regex extraction function",
        "parent": "https://github.com/apache/incubator-druid/commit/5ff92664f8e3ed560299b896dcc79825f1c6f43b",
        "repo": "incubator-druid",
        "unit_tests": [
            "RegexDimExtractionFnTest.java"
        ]
    },
    "incubator-druid_0ab7c31": {
        "bug_id": "incubator-druid_0ab7c31",
        "commit": "https://github.com/apache/incubator-druid/commit/0ab7c315dd57efb4303d15de14666f1a826c40b1",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0ab7c315dd57efb4303d15de14666f1a826c40b1/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java?ref=0ab7c315dd57efb4303d15de14666f1a826c40b1",
                "deletions": 1,
                "filename": "client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -20,13 +20,15 @@\n package com.metamx.druid.query;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Throwables;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Ordering;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.MergeIterable;\n import com.metamx.common.guava.Sequence;\n import com.metamx.common.guava.Sequences;\n+import com.metamx.common.logger.Logger;\n import com.metamx.druid.Query;\n \n import java.util.Arrays;\n@@ -52,6 +54,8 @@\n  */\n public class ChainedExecutionQueryRunner<T> implements QueryRunner<T>\n {\n+  private static final Logger log = new Logger(ChainedExecutionQueryRunner.class);\n+\n   private final Iterable<QueryRunner<T>> queryables;\n   private final ExecutorService exec;\n   private final Ordering<T> ordering;\n@@ -100,7 +104,13 @@ public ChainedExecutionQueryRunner(\n                               @Override\n                               public List<T> call() throws Exception\n                               {\n-                                return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                try {\n+                                  return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                }\n+                                catch (Exception e) {\n+                                  log.error(e, \"Exception with one of the sequences!\");\n+                                  throw Throwables.propagate(e);\n+                                }\n                               }\n                             }\n                         );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0ab7c315dd57efb4303d15de14666f1a826c40b1/client/src/main/java/com/metamx/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "52fc99a35b1c6ebac943e4c30d89352fe2323c3c",
                "status": "modified"
            }
        ],
        "message": "more logging in CEQR to track down NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/e72695111c9e77e53224c90192add88406493b6f",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java"
        ]
    },
    "incubator-druid_0d42792": {
        "bug_id": "incubator-druid_0d42792",
        "commit": "https://github.com/apache/incubator-druid/commit/0d427923c01cd27fff4a270e9e72393a3ae9cdc5",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/main/java/io/druid/query/search/SearchBinaryFn.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchBinaryFn.java?ref=0d427923c01cd27fff4a270e9e72393a3ae9cdc5",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/SearchBinaryFn.java",
                "patch": "@@ -86,12 +86,17 @@ public SearchBinaryFn(\n         continue;\n       }\n       if (prev.equals(searchHit)) {\n-        if (prev.getCount() != null) {\n+        if (prev.getCount() != null && searchHit.getCount() != null) {\n           prev = new SearchHit(\n               prev.getDimension(),\n               prev.getValue(),\n               prev.getCount() + searchHit.getCount()\n           );\n+        } else {\n+          prev = new SearchHit(\n+                  prev.getDimension(),\n+                  prev.getValue()\n+          );\n         }\n       } else {\n         results.add(prev);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/main/java/io/druid/query/search/SearchBinaryFn.java",
                "sha": "734bdbd2260980e5781db3638e1f5b36a514ed3a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java?ref=0d427923c01cd27fff4a270e9e72393a3ae9cdc5",
                "deletions": 3,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java",
                "patch": "@@ -62,7 +62,7 @@\n  */\n public class SearchQueryQueryToolChest extends QueryToolChest<Result<SearchResultValue>, SearchQuery>\n {\n-  private static final byte SEARCH_QUERY = 0x2;\n+  private static final byte SEARCH_QUERY = 0x15;\n   private static final TypeReference<Result<SearchResultValue>> TYPE_REFERENCE = new TypeReference<Result<SearchResultValue>>()\n   {\n   };\n@@ -221,8 +221,9 @@ public SearchHit apply(@Nullable Object input)\n                           {\n                             if (input instanceof Map) {\n                               return new SearchHit(\n-                                  (String) ((Map) input).get(\"dimension\"),\n-                                  (String) ((Map) input).get(\"value\")\n+                                      (String) ((Map) input).get(\"dimension\"),\n+                                      (String) ((Map) input).get(\"value\"),\n+                                      (Integer) ((Map) input).get(\"count\")\n                               );\n                             } else if (input instanceof SearchHit) {\n                               return (SearchHit) input;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/main/java/io/druid/query/search/SearchQueryQueryToolChest.java",
                "sha": "e7c008b805070e23404027027b833483fcbab695",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java?ref=0d427923c01cd27fff4a270e9e72393a3ae9cdc5",
                "deletions": 4,
                "filename": "processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java",
                "patch": "@@ -41,12 +41,15 @@\n {\n   private final DateTime currTime = new DateTime();\n \n-  private void assertSearchMergeResult(Object o1, Object o2)\n+  private void assertSearchMergeResult(SearchResultValue o1, SearchResultValue o2)\n   {\n-    Iterator i1 = ((Iterable) o1).iterator();\n-    Iterator i2 = ((Iterable) o2).iterator();\n+    Iterator<SearchHit> i1 = ((Iterable) o1).iterator();\n+    Iterator<SearchHit> i2 = ((Iterable) o2).iterator();\n     while (i1.hasNext() && i2.hasNext()) {\n-      Assert.assertEquals(i1.next(), i2.next());\n+      SearchHit s1 = i1.next();\n+      SearchHit s2 = i2.next();\n+      Assert.assertEquals(s1, s2);\n+      Assert.assertEquals(s1.getCount(), s2.getCount());\n     }\n     Assert.assertTrue(!i1.hasNext() && !i2.hasNext());\n   }\n@@ -336,4 +339,38 @@ public void testMergeLimit(){\n     Assert.assertEquals(expected.getTimestamp(), actual.getTimestamp());\n     assertSearchMergeResult(expected.getValue(), actual.getValue());\n   }\n+\n+  @Test\n+  public void testMergeCountWithNull() {\n+    Result<SearchResultValue> r1 = new Result<SearchResultValue>(\n+            currTime,\n+            new SearchResultValue(\n+                    ImmutableList.<SearchHit>of(\n+                            new SearchHit(\n+                                    \"blah\",\n+                                    \"foo\"\n+                            )\n+                    )\n+            )\n+    );\n+\n+    Result<SearchResultValue> r2 = new Result<SearchResultValue>(\n+            currTime,\n+            new SearchResultValue(\n+                    ImmutableList.<SearchHit>of(\n+                            new SearchHit(\n+                                    \"blah\",\n+                                    \"foo\",\n+                                    3\n+                            )\n+                    )\n+            )\n+    );\n+\n+    Result<SearchResultValue> expected = r1;\n+\n+    Result<SearchResultValue> actual = new SearchBinaryFn(new LexicographicSearchSortSpec(), QueryGranularities.ALL, Integer.MAX_VALUE).apply(r1, r2);\n+    Assert.assertEquals(expected.getTimestamp(), actual.getTimestamp());\n+    assertSearchMergeResult(expected.getValue(), actual.getValue());\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java",
                "sha": "6217da09740bd79e65aceb9627b6c843529b9820",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/incubator-druid/blob/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/client/CachingClusteredClientTest.java?ref=0d427923c01cd27fff4a270e9e72393a3ae9cdc5",
                "deletions": 25,
                "filename": "server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "patch": "@@ -1107,27 +1107,27 @@ public void testSearchCaching() throws Exception\n         client,\n         builder.build(),\n         new Interval(\"2011-01-01/2011-01-02\"),\n-        makeSearchResults(new DateTime(\"2011-01-01\"), \"how\", \"howdy\", \"howwwwww\", \"howwy\"),\n+        makeSearchResults(new DateTime(\"2011-01-01\"), \"how\", 1, \"howdy\", 2, \"howwwwww\", 3, \"howwy\", 4),\n \n         new Interval(\"2011-01-02/2011-01-03\"),\n-        makeSearchResults(new DateTime(\"2011-01-02\"), \"how1\", \"howdy1\", \"howwwwww1\", \"howwy1\"),\n+        makeSearchResults(new DateTime(\"2011-01-02\"), \"how1\", 1, \"howdy1\", 2, \"howwwwww1\", 3, \"howwy1\", 4),\n \n         new Interval(\"2011-01-05/2011-01-10\"),\n         makeSearchResults(\n-            new DateTime(\"2011-01-05\"), \"how2\", \"howdy2\", \"howwwwww2\", \"howww2\",\n-            new DateTime(\"2011-01-06\"), \"how3\", \"howdy3\", \"howwwwww3\", \"howww3\",\n-            new DateTime(\"2011-01-07\"), \"how4\", \"howdy4\", \"howwwwww4\", \"howww4\",\n-            new DateTime(\"2011-01-08\"), \"how5\", \"howdy5\", \"howwwwww5\", \"howww5\",\n-            new DateTime(\"2011-01-09\"), \"how6\", \"howdy6\", \"howwwwww6\", \"howww6\"\n+            new DateTime(\"2011-01-05\"), \"how2\", 1, \"howdy2\", 2, \"howwwwww2\", 3, \"howww2\", 4,\n+            new DateTime(\"2011-01-06\"), \"how3\", 1, \"howdy3\", 2, \"howwwwww3\", 3, \"howww3\", 4,\n+            new DateTime(\"2011-01-07\"), \"how4\", 1, \"howdy4\", 2, \"howwwwww4\", 3, \"howww4\", 4,\n+            new DateTime(\"2011-01-08\"), \"how5\", 1, \"howdy5\", 2, \"howwwwww5\", 3, \"howww5\", 4,\n+            new DateTime(\"2011-01-09\"), \"how6\", 1, \"howdy6\", 2, \"howwwwww6\", 3, \"howww6\", 4\n         ),\n \n         new Interval(\"2011-01-05/2011-01-10\"),\n         makeSearchResults(\n-            new DateTime(\"2011-01-05T01\"), \"how2\", \"howdy2\", \"howwwwww2\", \"howww2\",\n-            new DateTime(\"2011-01-06T01\"), \"how3\", \"howdy3\", \"howwwwww3\", \"howww3\",\n-            new DateTime(\"2011-01-07T01\"), \"how4\", \"howdy4\", \"howwwwww4\", \"howww4\",\n-            new DateTime(\"2011-01-08T01\"), \"how5\", \"howdy5\", \"howwwwww5\", \"howww5\",\n-            new DateTime(\"2011-01-09T01\"), \"how6\", \"howdy6\", \"howwwwww6\", \"howww6\"\n+            new DateTime(\"2011-01-05T01\"), \"how2\", 1, \"howdy2\", 2, \"howwwwww2\", 3, \"howww2\", 4,\n+            new DateTime(\"2011-01-06T01\"), \"how3\", 1, \"howdy3\", 2, \"howwwwww3\", 3, \"howww3\", 4,\n+            new DateTime(\"2011-01-07T01\"), \"how4\", 1, \"howdy4\", 2, \"howwwwww4\", 3, \"howww4\", 4,\n+            new DateTime(\"2011-01-08T01\"), \"how5\", 1, \"howdy5\", 2, \"howwwwww5\", 3, \"howww5\", 4,\n+            new DateTime(\"2011-01-09T01\"), \"how6\", 1, \"howdy6\", 2, \"howwwwww6\", 3, \"howww6\", 4\n         )\n     );\n \n@@ -1140,18 +1140,18 @@ public void testSearchCaching() throws Exception\n     HashMap<String, Object> context = new HashMap<String, Object>();\n     TestHelper.assertExpectedResults(\n         makeSearchResults(\n-            new DateTime(\"2011-01-01\"), \"how\", \"howdy\", \"howwwwww\", \"howwy\",\n-            new DateTime(\"2011-01-02\"), \"how1\", \"howdy1\", \"howwwwww1\", \"howwy1\",\n-            new DateTime(\"2011-01-05\"), \"how2\", \"howdy2\", \"howwwwww2\", \"howww2\",\n-            new DateTime(\"2011-01-05T01\"), \"how2\", \"howdy2\", \"howwwwww2\", \"howww2\",\n-            new DateTime(\"2011-01-06\"), \"how3\", \"howdy3\", \"howwwwww3\", \"howww3\",\n-            new DateTime(\"2011-01-06T01\"), \"how3\", \"howdy3\", \"howwwwww3\", \"howww3\",\n-            new DateTime(\"2011-01-07\"), \"how4\", \"howdy4\", \"howwwwww4\", \"howww4\",\n-            new DateTime(\"2011-01-07T01\"), \"how4\", \"howdy4\", \"howwwwww4\", \"howww4\",\n-            new DateTime(\"2011-01-08\"), \"how5\", \"howdy5\", \"howwwwww5\", \"howww5\",\n-            new DateTime(\"2011-01-08T01\"), \"how5\", \"howdy5\", \"howwwwww5\", \"howww5\",\n-            new DateTime(\"2011-01-09\"), \"how6\", \"howdy6\", \"howwwwww6\", \"howww6\",\n-            new DateTime(\"2011-01-09T01\"), \"how6\", \"howdy6\", \"howwwwww6\", \"howww6\"\n+            new DateTime(\"2011-01-01\"), \"how\", 1, \"howdy\", 2, \"howwwwww\", 3, \"howwy\", 4,\n+            new DateTime(\"2011-01-02\"), \"how1\", 1, \"howdy1\", 2, \"howwwwww1\", 3, \"howwy1\", 4,\n+            new DateTime(\"2011-01-05\"), \"how2\", 1, \"howdy2\", 2, \"howwwwww2\", 3, \"howww2\", 4,\n+            new DateTime(\"2011-01-05T01\"), \"how2\", 1, \"howdy2\", 2, \"howwwwww2\", 3, \"howww2\", 4,\n+            new DateTime(\"2011-01-06\"), \"how3\", 1, \"howdy3\", 2, \"howwwwww3\", 3, \"howww3\", 4,\n+            new DateTime(\"2011-01-06T01\"), \"how3\", 1, \"howdy3\", 2, \"howwwwww3\", 3, \"howww3\", 4,\n+            new DateTime(\"2011-01-07\"), \"how4\", 1, \"howdy4\", 2, \"howwwwww4\", 3, \"howww4\", 4,\n+            new DateTime(\"2011-01-07T01\"), \"how4\", 1, \"howdy4\", 2, \"howwwwww4\", 3, \"howww4\", 4,\n+            new DateTime(\"2011-01-08\"), \"how5\", 1, \"howdy5\", 2, \"howwwwww5\", 3, \"howww5\", 4,\n+            new DateTime(\"2011-01-08T01\"), \"how5\", 1, \"howdy5\", 2, \"howwwwww5\", 3, \"howww5\", 4,\n+            new DateTime(\"2011-01-09\"), \"how6\", 1, \"howdy6\", 2, \"howwwwww6\", 3, \"howww6\", 4,\n+            new DateTime(\"2011-01-09T01\"), \"how6\", 1, \"howdy6\", 2, \"howwwwww6\", 3, \"howww6\", 4\n         ),\n         runner.run(\n             builder.intervals(\"2011-01-01/2011-01-10\")\n@@ -2118,7 +2118,7 @@ public Result apply(\n \n       List<SearchHit> values = Lists.newArrayList();\n       while (index < objects.length && !(objects[index] instanceof DateTime)) {\n-        values.add(new SearchHit(TOP_DIM, objects[index++].toString()));\n+        values.add(new SearchHit(TOP_DIM, objects[index++].toString(), (Integer) objects[index++]));\n       }\n \n       retVal.add(new Result<>(timestamp, new SearchResultValue(values)));",
                "raw_url": "https://github.com/apache/incubator-druid/raw/0d427923c01cd27fff4a270e9e72393a3ae9cdc5/server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "sha": "1ca49b2a1c40cb49613ee2175ce20e6b71889794",
                "status": "modified"
            }
        ],
        "message": "fix caching for search results (#3119)\n\n* fix caching for search results\r\n\r\nproperly read count when reading from cache.\r\n\r\n* fix NPE during merging search count and add test\r\n\r\n* Update cache key to invalidate prev results",
        "parent": "https://github.com/apache/incubator-druid/commit/c2155e13bdb7f307feace0e060c2be554feb61ec",
        "repo": "incubator-druid",
        "unit_tests": [
            "SearchBinaryFnTest.java",
            "SearchQueryQueryToolChestTest.java"
        ]
    },
    "incubator-druid_1ad898b": {
        "bug_id": "incubator-druid_1ad898b",
        "commit": "https://github.com/apache/incubator-druid/commit/1ad898bde29820a8ff41267457a4370395406ae5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/.travis.yml",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/.travis.yml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 0,
                "filename": ".travis.yml",
                "patch": "@@ -47,6 +47,7 @@ matrix:\n     - sudo: false\n       env:\n         - NAME=\"other modules test\"\n+        - AWS_REGION=us-east-1 # set a aws region for unit tests\n       install: echo \"MAVEN_OPTS='-Xmx3000m'\" > ~/.mavenrc && mvn install -q -ff -DskipTests -B\n       before_script:\n         - unset _JAVA_OPTIONS",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/.travis.yml",
                "sha": "4ec30507b4f4a829625fa6dcc462ba4f1bf8d526",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/api/src/main/java/io/druid/guice/JsonConfigProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/guice/JsonConfigProvider.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "api/src/main/java/io/druid/guice/JsonConfigProvider.java",
                "patch": "@@ -130,8 +130,8 @@\n       Key<Supplier<T>> supplierKey\n   )\n   {\n-    binder.bind(supplierKey).toProvider((Provider) of(propertyBase, clazz)).in(LazySingleton.class);\n-    binder.bind(instanceKey).toProvider(new SupplierProvider<T>(supplierKey));\n+    binder.bind(supplierKey).toProvider(of(propertyBase, clazz)).in(LazySingleton.class);\n+    binder.bind(instanceKey).toProvider(new SupplierProvider<>(supplierKey));\n   }\n \n   @SuppressWarnings(\"unchecked\")",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/api/src/main/java/io/druid/guice/JsonConfigProvider.java",
                "sha": "609567b2dd7d6b3248ca007399fbb123964df6f2",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/InputRowParserSerdeTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/test/java/io/druid/data/input/impl/InputRowParserSerdeTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 4,
                "filename": "api/src/test/java/io/druid/data/input/impl/InputRowParserSerdeTest.java",
                "patch": "@@ -101,9 +101,9 @@ public void testMapInputRowParserSerde() throws Exception\n             null\n         )\n     );\n-    final MapInputRowParser parser2 = jsonMapper.readValue(\n+    final MapInputRowParser parser2 = (MapInputRowParser) jsonMapper.readValue(\n         jsonMapper.writeValueAsBytes(parser),\n-        MapInputRowParser.class\n+        InputRowParser.class\n     );\n     final InputRow parsed = parser2.parseBatch(\n         ImmutableMap.<String, Object>of(\n@@ -134,9 +134,9 @@ public void testMapInputRowParserNumbersSerde() throws Exception\n             null\n         )\n     );\n-    final MapInputRowParser parser2 = jsonMapper.readValue(\n+    final MapInputRowParser parser2 = (MapInputRowParser) jsonMapper.readValue(\n         jsonMapper.writeValueAsBytes(parser),\n-        MapInputRowParser.class\n+        InputRowParser.class\n     );\n     final InputRow parsed = parser2.parseBatch(\n         ImmutableMap.<String, Object>of(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/InputRowParserSerdeTest.java",
                "sha": "c7cb2df6340c1c097687d4c2fef76f064b79d76a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/JSONParseSpecTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/test/java/io/druid/data/input/impl/JSONParseSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "api/src/test/java/io/druid/data/input/impl/JSONParseSpecTest.java",
                "patch": "@@ -91,9 +91,9 @@ public void testSerde() throws IOException\n         feature\n     );\n \n-    final JSONParseSpec serde = jsonMapper.readValue(\n+    final JSONParseSpec serde = (JSONParseSpec) jsonMapper.readValue(\n         jsonMapper.writeValueAsString(spec),\n-        JSONParseSpec.class\n+        ParseSpec.class\n     );\n     Assert.assertEquals(\"timestamp\", serde.getTimestampSpec().getTimestampColumn());\n     Assert.assertEquals(\"iso\", serde.getTimestampSpec().getTimestampFormat());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/JSONParseSpecTest.java",
                "sha": "c7c73802f73dc43e033e7def624638a206b888f8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/JavaScriptParseSpecTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/test/java/io/druid/data/input/impl/JavaScriptParseSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "api/src/test/java/io/druid/data/input/impl/JavaScriptParseSpecTest.java",
                "patch": "@@ -58,9 +58,9 @@ public void testSerde() throws IOException\n         \"abc\",\n         JavaScriptConfig.getEnabledInstance()\n     );\n-    final JavaScriptParseSpec serde = jsonMapper.readValue(\n+    final JavaScriptParseSpec serde = (JavaScriptParseSpec) jsonMapper.readValue(\n         jsonMapper.writeValueAsString(spec),\n-        JavaScriptParseSpec.class\n+        ParseSpec.class\n     );\n     Assert.assertEquals(\"abc\", serde.getTimestampSpec().getTimestampColumn());\n     Assert.assertEquals(\"iso\", serde.getTimestampSpec().getTimestampFormat());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/JavaScriptParseSpecTest.java",
                "sha": "b63caf43ff330b324ead4bbabd80d7d30066f3cd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/RegexParseSpecTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/test/java/io/druid/data/input/impl/RegexParseSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "api/src/test/java/io/druid/data/input/impl/RegexParseSpecTest.java",
                "patch": "@@ -43,9 +43,9 @@ public void testSerde() throws IOException\n         Collections.singletonList(\"abc\"),\n         \"abc\"\n     );\n-    final RegexParseSpec serde = jsonMapper.readValue(\n+    final RegexParseSpec serde = (RegexParseSpec) jsonMapper.readValue(\n         jsonMapper.writeValueAsString(spec),\n-        RegexParseSpec.class\n+        ParseSpec.class\n     );\n     Assert.assertEquals(\"abc\", serde.getTimestampSpec().getTimestampColumn());\n     Assert.assertEquals(\"iso\", serde.getTimestampSpec().getTimestampFormat());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/api/src/test/java/io/druid/data/input/impl/RegexParseSpecTest.java",
                "sha": "5468ae0302f9b90902b6b56f96c49c576c7d3444",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/aws-common/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "aws-common/pom.xml",
                "patch": "@@ -37,7 +37,7 @@\n         </dependency>\n         <dependency>\n             <groupId>com.amazonaws</groupId>\n-            <artifactId>aws-java-sdk-ec2</artifactId>\n+            <artifactId>aws-java-sdk-bundle</artifactId>\n         </dependency>\n \n         <!-- Tests -->",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/pom.xml",
                "sha": "c6e69b099f6672e269dd9561a6bdc8a4a1298113",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/src/main/java/io/druid/common/aws/AWSEndpointConfig.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/aws-common/src/main/java/io/druid/common/aws/AWSEndpointConfig.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 0,
                "filename": "aws-common/src/main/java/io/druid/common/aws/AWSEndpointConfig.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.common.aws;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+public class AWSEndpointConfig\n+{\n+  @JsonProperty\n+  private String url;\n+\n+  @JsonProperty\n+  private String serviceName;\n+\n+  @JsonProperty\n+  private String signingRegion;\n+\n+  @JsonProperty\n+  public String getUrl()\n+  {\n+    return url;\n+  }\n+\n+  @JsonProperty\n+  public String getServiceName()\n+  {\n+    return serviceName;\n+  }\n+\n+  @JsonProperty\n+  public String getSigningRegion()\n+  {\n+    return signingRegion;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/src/main/java/io/druid/common/aws/AWSEndpointConfig.java",
                "sha": "773a2ab15013f65cb35ba18b872b13ea9c5e0d68",
                "status": "added"
            },
            {
                "additions": 61,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/src/main/java/io/druid/common/aws/AWSProxyConfig.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/aws-common/src/main/java/io/druid/common/aws/AWSProxyConfig.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 0,
                "filename": "aws-common/src/main/java/io/druid/common/aws/AWSProxyConfig.java",
                "patch": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.common.aws;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+public class AWSProxyConfig\n+{\n+  @JsonProperty\n+  private String host;\n+\n+  @JsonProperty\n+  private int port = -1; // AWS's default proxy port is -1\n+\n+  @JsonProperty\n+  private String username;\n+\n+  @JsonProperty\n+  private String password;\n+\n+  @JsonProperty\n+  public String getHost()\n+  {\n+    return host;\n+  }\n+\n+  @JsonProperty\n+  public int getPort()\n+  {\n+    return port;\n+  }\n+\n+  @JsonProperty\n+  public String getUsername()\n+  {\n+    return username;\n+  }\n+\n+  @JsonProperty\n+  public String getPassword()\n+  {\n+    return password;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/aws-common/src/main/java/io/druid/common/aws/AWSProxyConfig.java",
                "sha": "eda04bb37152318ba6888426139efdb7337f5e05",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/common/pom.xml",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/common/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 15,
                "filename": "common/pom.xml",
                "patch": "@@ -157,21 +157,7 @@\n             <groupId>com.lmax</groupId>\n             <artifactId>disruptor</artifactId>\n         </dependency>\n-        <!--\n-            Druid doesn't ACTUALLY depend on jets3t in its core, but quite a few of the extensions do. This leads to a nasty ClassLoader problem\n-            There's a bug in\n-            https://github.com/apache/httpclient/blob/4.5.2/httpclient/src/main/java-deprecated/org/apache/http/impl/client/AbstractHttpClient.java#L332\n-            Where httpclient does not care what your context classloader is when looking for the connection manager factories. See https://issues.apache.org/jira/browse/HTTPCLIENT-1727\n-            A few extensions depend on jets3t, so we include it explicitly here to make sure it can load up its\n-            org.jets3t.service.utils.RestUtils$ConnManagerFactory\n-            properly\n-            Future releases which include HTTPCLIENT-1727 should probably set the context loader whenever jets3t calls are used\n-        -->\n-        <dependency>\n-            <groupId>net.java.dev.jets3t</groupId>\n-            <artifactId>jets3t</artifactId>\n-            <version>0.9.4</version>\n-        </dependency>\n+\n         <dependency>\n             <groupId>org.antlr</groupId>\n             <artifactId>antlr4-runtime</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/common/pom.xml",
                "sha": "0f07a4105ab808129cc4d5c0ed7e5f19956005fd",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/docs/content/development/extensions-core/s3.md",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/development/extensions-core/s3.md?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 6,
                "filename": "docs/content/development/extensions-core/s3.md",
                "patch": "@@ -12,12 +12,18 @@ S3-compatible deep storage is basically either S3 or something like Google Stora\n \n ### Configuration\n \n-|Property|Possible Values|Description|Default|\n-|--------|---------------|-----------|-------|\n-|`druid.s3.accessKey`||S3 access key.|Must be set.|\n-|`druid.s3.secretKey`||S3 secret key.|Must be set.|\n-|`druid.storage.bucket`||Bucket to store in.|Must be set.|\n-|`druid.storage.baseKey`||Base key prefix to use, i.e. what directory.|Must be set.|\n+|Property|Description|Default|\n+|--------|-----------|-------|\n+|`druid.s3.accessKey`|S3 access key.|Must be set.|\n+|`druid.s3.secretKey`|S3 secret key.|Must be set.|\n+|`druid.storage.bucket`|Bucket to store in.|Must be set.|\n+|`druid.storage.baseKey`|Base key prefix to use, i.e. what directory.|Must be set.|\n+|`druid.s3.endpoint.url`|Service endpoint either with or without the protocol.|None|\n+|`druid.s3.endpoint.signingRegion`|Region to use for SigV4 signing of requests (e.g. us-west-1).|None|\n+|`druid.s3.proxy.host`|Proxy host to connect through.|None|\n+|`druid.s3.proxy.port`|Port on the proxy host to connect through.|None|\n+|`druid.s3.proxy.username`|User name to use when connecting through a proxy.|None|\n+|`druid.s3.proxy.password`|Password to use when connecting through a proxy.|None|\n \n ## StaticS3Firehose\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/docs/content/development/extensions-core/s3.md",
                "sha": "3d32c9de8953ced891fbcc5b5f43dbc2c6b695d7",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/druid-kerberos/src/main/java/io/druid/security/kerberos/KerberosAuthenticator.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-kerberos/src/main/java/io/druid/security/kerberos/KerberosAuthenticator.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 9,
                "filename": "extensions-core/druid-kerberos/src/main/java/io/druid/security/kerberos/KerberosAuthenticator.java",
                "patch": "@@ -334,13 +334,14 @@ public Principal getUserPrincipal()\n               };\n               if (newToken && !token.isExpired() && token != AuthenticationToken.ANONYMOUS) {\n                 String signedToken = mySigner.sign(token.toString());\n-                tokenToAuthCookie(httpResponse,\n-                                 signedToken,\n-                                 getCookieDomain(),\n-                                 getCookiePath(),\n-                                 token.getExpires(),\n-                                 !token.isExpired() && token.getExpires() > 0,\n-                                 isHttps\n+                tokenToAuthCookie(\n+                    httpResponse,\n+                    signedToken,\n+                    getCookieDomain(),\n+                    getCookiePath(),\n+                    token.getExpires(),\n+                    !token.isExpired() && token.getExpires() > 0,\n+                    isHttps\n                 );\n               }\n               doFilter(filterChain, httpRequest, httpResponse);\n@@ -361,8 +362,14 @@ public Principal getUserPrincipal()\n         }\n         if (unauthorizedResponse) {\n           if (!httpResponse.isCommitted()) {\n-            tokenToAuthCookie(httpResponse, \"\", getCookieDomain(),\n-                             getCookiePath(), 0, false, isHttps\n+            tokenToAuthCookie(\n+                httpResponse,\n+                \"\",\n+                getCookieDomain(),\n+                getCookiePath(),\n+                0,\n+                false,\n+                isHttps\n             );\n             // If response code is 401. Then WWW-Authenticate Header should be\n             // present.. reset to 403 if not found..",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/druid-kerberos/src/main/java/io/druid/security/kerberos/KerberosAuthenticator.java",
                "sha": "215de9caa4fbe03288b17d64d3566622579edeeb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/hdfs-storage/pom.xml",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/hdfs-storage/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 10,
                "filename": "extensions-core/hdfs-storage/pom.xml",
                "patch": "@@ -145,16 +145,7 @@\n           <groupId>org.apache.hadoop</groupId>\n           <artifactId>hadoop-aws</artifactId>\n           <version>${hadoop.compile.version}</version>\n-          <exclusions>\n-            <exclusion>\n-              <groupId>com.amazonaws</groupId>\n-              <artifactId>aws-java-sdk</artifactId>\n-            </exclusion>\n-          </exclusions>\n-        </dependency>\n-        <dependency>\n-          <groupId>com.amazonaws</groupId>\n-          <artifactId>aws-java-sdk-s3</artifactId>\n+          <scope>provided</scope>\n         </dependency>\n         <dependency>\n             <groupId>commons-io</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/hdfs-storage/pom.xml",
                "sha": "4c1a853744fb598f500cba2d86fbb2073b29548e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentFinder.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentFinder.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentFinder.java",
                "patch": "@@ -22,11 +22,11 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.collect.Sets;\n import com.google.inject.Inject;\n+import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentFinder;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n-import io.druid.java.util.common.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.LocatedFileStatus;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/hdfs-storage/src/main/java/io/druid/storage/hdfs/HdfsDataSegmentFinder.java",
                "sha": "c960541e634eb7a1898d32e2b080de96f2531f21",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/pom.xml",
                "changes": 137,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 71,
                "filename": "extensions-core/s3-extensions/pom.xml",
                "patch": "@@ -18,80 +18,75 @@\n \n <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n-    <modelVersion>4.0.0</modelVersion>\n+  <modelVersion>4.0.0</modelVersion>\n \n-    <groupId>io.druid.extensions</groupId>\n-    <artifactId>druid-s3-extensions</artifactId>\n-    <name>druid-s3-extensions</name>\n-    <description>druid-s3-extensions</description>\n+  <groupId>io.druid.extensions</groupId>\n+  <artifactId>druid-s3-extensions</artifactId>\n+  <name>druid-s3-extensions</name>\n+  <description>druid-s3-extensions</description>\n \n-    <parent>\n-        <groupId>io.druid</groupId>\n-        <artifactId>druid</artifactId>\n-        <version>0.13.0-SNAPSHOT</version>\n-        <relativePath>../../pom.xml</relativePath>\n-    </parent>\n+  <parent>\n+    <groupId>io.druid</groupId>\n+    <artifactId>druid</artifactId>\n+    <version>0.13.0-SNAPSHOT</version>\n+    <relativePath>../../pom.xml</relativePath>\n+  </parent>\n \n-    <dependencies>\n-        <dependency>\n-            <groupId>io.druid</groupId>\n-            <artifactId>druid-api</artifactId>\n-            <version>${project.parent.version}</version>\n-            <scope>provided</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>io.druid</groupId>\n-            <artifactId>druid-aws-common</artifactId>\n-            <version>${project.parent.version}</version>\n-            <scope>provided</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>io.druid</groupId>\n-            <artifactId>java-util</artifactId>\n-            <version>${project.parent.version}</version>\n-            <scope>provided</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>net.java.dev.jets3t</groupId>\n-            <artifactId>jets3t</artifactId>\n-            <scope>provided</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>commons-io</groupId>\n-            <artifactId>commons-io</artifactId>\n-            <scope>provided</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>com.fasterxml.jackson.module</groupId>\n-            <artifactId>jackson-module-guice</artifactId>\n-            <version>${jackson.version}</version>\n-            <scope>provided</scope>\n-        </dependency>\n+  <dependencies>\n+    <dependency>\n+      <groupId>io.druid</groupId>\n+      <artifactId>druid-api</artifactId>\n+      <version>${project.parent.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>io.druid</groupId>\n+      <artifactId>druid-aws-common</artifactId>\n+      <version>${project.parent.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>io.druid</groupId>\n+      <artifactId>java-util</artifactId>\n+      <version>${project.parent.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>commons-io</groupId>\n+      <artifactId>commons-io</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.module</groupId>\n+      <artifactId>jackson-module-guice</artifactId>\n+      <version>${jackson.version}</version>\n+      <scope>provided</scope>\n+    </dependency>\n \n-        <!-- Tests -->\n-        <dependency>\n-            <groupId>io.druid</groupId>\n-            <artifactId>druid-server</artifactId>\n-            <version>${project.parent.version}</version>\n-            <scope>test</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>io.druid</groupId>\n-            <artifactId>druid-processing</artifactId>\n-            <version>${project.parent.version}</version>\n-            <type>test-jar</type>\n-            <scope>test</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>junit</groupId>\n-            <artifactId>junit</artifactId>\n-            <scope>test</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>org.easymock</groupId>\n-            <artifactId>easymock</artifactId>\n-            <scope>test</scope>\n-        </dependency>\n-    </dependencies>\n+    <!-- Tests -->\n+    <dependency>\n+      <groupId>io.druid</groupId>\n+      <artifactId>druid-server</artifactId>\n+      <version>${project.parent.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>io.druid</groupId>\n+      <artifactId>druid-processing</artifactId>\n+      <version>${project.parent.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>junit</groupId>\n+      <artifactId>junit</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.easymock</groupId>\n+      <artifactId>easymock</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+  </dependencies>\n \n </project>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/pom.xml",
                "sha": "487d337825420682d28a89683bc95b016ffded40",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/firehose/s3/StaticS3FirehoseFactory.java",
                "changes": 127,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/firehose/s3/StaticS3FirehoseFactory.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 65,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/firehose/s3/StaticS3FirehoseFactory.java",
                "patch": "@@ -19,47 +19,51 @@\n \n package io.druid.firehose.s3;\n \n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.fasterxml.jackson.annotation.JacksonInject;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Predicate;\n+import com.google.common.collect.Lists;\n import io.druid.data.input.impl.prefetch.PrefetchableTextFilesFirehoseFactory;\n import io.druid.java.util.common.CompressionUtils;\n import io.druid.java.util.common.IAE;\n+import io.druid.java.util.common.IOE;\n+import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.storage.s3.S3Utils;\n-import org.jets3t.service.S3ServiceException;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.StorageObjectsChunk;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n \n import java.io.IOException;\n import java.io.InputStream;\n import java.net.URI;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Objects;\n import java.util.stream.Collectors;\n \n /**\n  * Builds firehoses that read from a predefined list of S3 objects and then dry up.\n  */\n-public class StaticS3FirehoseFactory extends PrefetchableTextFilesFirehoseFactory<S3Object>\n+public class StaticS3FirehoseFactory extends PrefetchableTextFilesFirehoseFactory<S3ObjectSummary>\n {\n   private static final Logger log = new Logger(StaticS3FirehoseFactory.class);\n-  private static final long MAX_LISTING_LENGTH = 1024;\n+  private static final int MAX_LISTING_LENGTH = 1024;\n \n-  private final RestS3Service s3Client;\n+  private final AmazonS3 s3Client;\n   private final List<URI> uris;\n   private final List<URI> prefixes;\n \n   @JsonCreator\n   public StaticS3FirehoseFactory(\n-      @JacksonInject(\"s3Client\") RestS3Service s3Client,\n+      @JacksonInject(\"s3Client\") AmazonS3 s3Client,\n       @JsonProperty(\"uris\") List<URI> uris,\n       @JsonProperty(\"prefixes\") List<URI> prefixes,\n       @JsonProperty(\"maxCacheCapacityBytes\") Long maxCacheCapacityBytes,\n@@ -70,7 +74,7 @@ public StaticS3FirehoseFactory(\n   )\n   {\n     super(maxCacheCapacityBytes, maxFetchCapacityBytes, prefetchTriggerBytes, fetchTimeout, maxFetchRetry);\n-    this.s3Client = Preconditions.checkNotNull(s3Client, \"null s3Client\");\n+    this.s3Client = Preconditions.checkNotNull(s3Client, \"s3Client\");\n     this.uris = uris == null ? new ArrayList<>() : uris;\n     this.prefixes = prefixes == null ? new ArrayList<>() : prefixes;\n \n@@ -104,7 +108,7 @@ public StaticS3FirehoseFactory(\n   }\n \n   @Override\n-  protected Collection<S3Object> initObjects() throws IOException\n+  protected Collection<S3ObjectSummary> initObjects() throws IOException\n   {\n     // Here, the returned s3 objects contain minimal information without data.\n     // Getting data is deferred until openObjectStream() is called for each object.\n@@ -113,53 +117,49 @@ public StaticS3FirehoseFactory(\n           .map(\n               uri -> {\n                 final String s3Bucket = uri.getAuthority();\n-                final S3Object s3Object = new S3Object(extractS3Key(uri));\n-                s3Object.setBucketName(s3Bucket);\n-                return s3Object;\n+                final String key = S3Utils.extractS3Key(uri);\n+                return S3Utils.getSingleObjectSummary(s3Client, s3Bucket, key);\n               }\n           )\n           .collect(Collectors.toList());\n     } else {\n-      final List<S3Object> objects = new ArrayList<>();\n+      final List<S3ObjectSummary> objects = new ArrayList<>();\n       for (URI uri : prefixes) {\n         final String bucket = uri.getAuthority();\n-        final String prefix = extractS3Key(uri);\n+        final String prefix = S3Utils.extractS3Key(uri);\n+\n         try {\n-          String lastKey = null;\n-          StorageObjectsChunk objectsChunk;\n-          do {\n-            objectsChunk = s3Client.listObjectsChunked(\n-                bucket,\n-                prefix,\n-                null,\n-                MAX_LISTING_LENGTH,\n-                lastKey\n-            );\n-            Arrays.stream(objectsChunk.getObjects())\n-                  .filter(storageObject -> !storageObject.isDirectoryPlaceholder())\n-                  .forEach(storageObject -> objects.add((S3Object) storageObject));\n-            lastKey = objectsChunk.getPriorLastKey();\n-          } while (!objectsChunk.isListingComplete());\n+          final Iterator<S3ObjectSummary> objectSummaryIterator = S3Utils.objectSummaryIterator(\n+              s3Client,\n+              bucket,\n+              prefix,\n+              MAX_LISTING_LENGTH\n+          );\n+          objects.addAll(Lists.newArrayList(objectSummaryIterator));\n         }\n-        catch (ServiceException outerException) {\n+        catch (AmazonS3Exception outerException) {\n           log.error(outerException, \"Exception while listing on %s\", uri);\n \n-          if (outerException.getResponseCode() == 403) {\n+          if (outerException.getStatusCode() == 403) {\n             // The \"Access Denied\" means users might not have a proper permission for listing on the given uri.\n             // Usually this is not a problem, but the uris might be the full paths to input objects instead of prefixes.\n             // In this case, users should be able to get objects if they have a proper permission for GetObject.\n \n             log.warn(\"Access denied for %s. Try to get the object from the uri without listing\", uri);\n             try {\n-              final S3Object s3Object = s3Client.getObject(bucket, prefix);\n-              if (!s3Object.isDirectoryPlaceholder()) {\n-                objects.add(s3Object);\n+              final ObjectMetadata objectMetadata = s3Client.getObjectMetadata(bucket, prefix);\n+\n+              if (!S3Utils.isDirectoryPlaceholder(prefix, objectMetadata)) {\n+                objects.add(S3Utils.getSingleObjectSummary(s3Client, bucket, prefix));\n               } else {\n-                throw new IOException(uri + \" is a directory placeholder, \"\n-                                      + \"but failed to get the object list under the directory due to permission\");\n+                throw new IOE(\n+                    \"[%s] is a directory placeholder, \"\n+                    + \"but failed to get the object list under the directory due to permission\",\n+                    uri\n+                );\n               }\n             }\n-            catch (S3ServiceException innerException) {\n+            catch (AmazonS3Exception innerException) {\n               throw new IOException(innerException);\n             }\n           } else {\n@@ -171,49 +171,46 @@ public StaticS3FirehoseFactory(\n     }\n   }\n \n-  private static String extractS3Key(URI uri)\n-  {\n-    return uri.getPath().startsWith(\"/\")\n-           ? uri.getPath().substring(1)\n-           : uri.getPath();\n-  }\n-\n   @Override\n-  protected InputStream openObjectStream(S3Object object) throws IOException\n+  protected InputStream openObjectStream(S3ObjectSummary object) throws IOException\n   {\n     try {\n       // Get data of the given object and open an input stream\n-      return s3Client.getObject(object.getBucketName(), object.getKey()).getDataInputStream();\n+      final S3Object s3Object = s3Client.getObject(object.getBucketName(), object.getKey());\n+      if (s3Object == null) {\n+        throw new ISE(\"Failed to get an s3 object for bucket[%s] and key[%s]\", object.getBucketName(), object.getKey());\n+      }\n+      return s3Object.getObjectContent();\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonS3Exception e) {\n       throw new IOException(e);\n     }\n   }\n \n   @Override\n-  protected InputStream openObjectStream(S3Object object, long start) throws IOException\n+  protected InputStream openObjectStream(S3ObjectSummary object, long start) throws IOException\n   {\n+    final GetObjectRequest request = new GetObjectRequest(object.getBucketName(), object.getKey());\n+    request.setRange(start);\n     try {\n-      final S3Object result = s3Client.getObject(\n-          object.getBucketName(),\n-          object.getKey(),\n-          null,\n-          null,\n-          null,\n-          null,\n-          start,\n-          null\n-      );\n-\n-      return result.getDataInputStream();\n+      final S3Object s3Object = s3Client.getObject(request);\n+      if (s3Object == null) {\n+        throw new ISE(\n+            \"Failed to get an s3 object for bucket[%s], key[%s], and start[%d]\",\n+            object.getBucketName(),\n+            object.getKey(),\n+            start\n+        );\n+      }\n+      return s3Object.getObjectContent();\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonS3Exception e) {\n       throw new IOException(e);\n     }\n   }\n \n   @Override\n-  protected InputStream wrapObjectStream(S3Object object, InputStream stream) throws IOException\n+  protected InputStream wrapObjectStream(S3ObjectSummary object, InputStream stream) throws IOException\n   {\n     return object.getKey().endsWith(\".gz\") ? CompressionUtils.gzipInputStream(stream) : stream;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/firehose/s3/StaticS3FirehoseFactory.java",
                "sha": "8827fc9ae31d61896829af8dcb5d6c008d933826",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/885b975c9574e3cfc4aae1cfccfd749b4dbb6e35/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/AWSSessionCredentialsAdapter.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/AWSSessionCredentialsAdapter.java?ref=885b975c9574e3cfc4aae1cfccfd749b4dbb6e35",
                "deletions": 70,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/AWSSessionCredentialsAdapter.java",
                "patch": "@@ -1,70 +0,0 @@\n-/*\n- * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. Metamarkets licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License. You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied. See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package io.druid.storage.s3;\n-\n-import com.amazonaws.auth.AWSCredentialsProvider;\n-import org.jets3t.service.security.AWSSessionCredentials;\n-\n-public class AWSSessionCredentialsAdapter extends AWSSessionCredentials\n-{\n-  private final AWSCredentialsProvider provider;\n-\n-  public AWSSessionCredentialsAdapter(AWSCredentialsProvider provider)\n-  {\n-    super(null, null, null);\n-    if (provider.getCredentials() instanceof com.amazonaws.auth.AWSSessionCredentials) {\n-      this.provider = provider;\n-    } else {\n-      throw new IllegalArgumentException(\"provider does not contain session credentials\");\n-    }\n-  }\n-\n-  @Override\n-  protected String getTypeName()\n-  {\n-    return \"AWSSessionCredentialsAdapter\";\n-  }\n-\n-  @Override\n-  public String getVersionPrefix()\n-  {\n-    return \"AWSSessionCredentialsAdapter, version: \";\n-  }\n-\n-  @Override\n-  public String getAccessKey()\n-  {\n-    return provider.getCredentials().getAWSAccessKeyId();\n-  }\n-\n-  @Override\n-  public String getSecretKey()\n-  {\n-    return provider.getCredentials().getAWSSecretKey();\n-  }\n-\n-  @Override\n-  public String getSessionToken()\n-  {\n-    com.amazonaws.auth.AWSSessionCredentials sessionCredentials =\n-        (com.amazonaws.auth.AWSSessionCredentials) provider.getCredentials();\n-    return sessionCredentials.getSessionToken();\n-  }\n-}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/885b975c9574e3cfc4aae1cfccfd749b4dbb6e35/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/AWSSessionCredentialsAdapter.java",
                "sha": "7a64a81e7c96cb8027efc789d02cfe1e34bab015",
                "status": "removed"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentArchiver.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentArchiver.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentArchiver.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Objects;\n import com.google.common.collect.ImmutableMap;\n@@ -28,7 +29,6 @@\n import io.druid.segment.loading.LoadSpec;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n \n \n public class S3DataSegmentArchiver extends S3DataSegmentMover implements DataSegmentArchiver\n@@ -40,7 +40,7 @@\n   @Inject\n   public S3DataSegmentArchiver(\n       @Json ObjectMapper mapper,\n-      RestS3Service s3Client,\n+      AmazonS3 s3Client,\n       S3DataSegmentArchiverConfig archiveConfig,\n       S3DataSegmentPusherConfig restoreConfig\n   )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentArchiver.java",
                "sha": "42eef5ce819ebcb4d412663b7ca7a6055c9a53a4",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentFinder.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentFinder.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 22,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentFinder.java",
                "patch": "@@ -19,22 +19,24 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Throwables;\n import com.google.common.collect.Sets;\n import com.google.inject.Inject;\n-\n+import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentFinder;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n-import org.jets3t.service.model.StorageObject;\n \n+import java.io.ByteArrayInputStream;\n import java.io.IOException;\n-import java.io.InputStream;\n import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n@@ -43,13 +45,13 @@\n {\n   private static final Logger log = new Logger(S3DataSegmentFinder.class);\n \n-  private final RestS3Service s3Client;\n+  private final AmazonS3 s3Client;\n   private final ObjectMapper jsonMapper;\n   private final S3DataSegmentPusherConfig config;\n \n   @Inject\n   public S3DataSegmentFinder(\n-      RestS3Service s3Client,\n+      AmazonS3 s3Client,\n       S3DataSegmentPusherConfig config,\n       ObjectMapper jsonMapper\n   )\n@@ -65,24 +67,24 @@ public S3DataSegmentFinder(\n     final Set<DataSegment> segments = Sets.newHashSet();\n \n     try {\n-      Iterator<StorageObject> objectsIterator = S3Utils.storageObjectsIterator(\n+      final Iterator<S3ObjectSummary> objectSummaryIterator = S3Utils.objectSummaryIterator(\n           s3Client,\n           config.getBucket(),\n           workingDirPath.length() == 0 ? config.getBaseKey() : workingDirPath,\n-          config.getMaxListingLength());\n+          config.getMaxListingLength()\n+      );\n \n-      while (objectsIterator.hasNext()) {\n-        StorageObject storageObject = objectsIterator.next();\n-        storageObject.closeDataInputStream();\n+      while (objectSummaryIterator.hasNext()) {\n+        final S3ObjectSummary objectSummary = objectSummaryIterator.next();\n \n-        if (S3Utils.toFilename(storageObject.getKey()).equals(\"descriptor.json\")) {\n-          final String descriptorJson = storageObject.getKey();\n+        if (S3Utils.toFilename(objectSummary.getKey()).equals(\"descriptor.json\")) {\n+          final String descriptorJson = objectSummary.getKey();\n           String indexZip = S3Utils.indexZipForSegmentPath(descriptorJson);\n \n-          if (S3Utils.isObjectInBucket(s3Client, config.getBucket(), indexZip)) {\n-            S3Object indexObject = s3Client.getObject(config.getBucket(), descriptorJson);\n-\n-            try (InputStream is = indexObject.getDataInputStream()) {\n+          if (S3Utils.isObjectInBucketIgnoringPermission(s3Client, config.getBucket(), indexZip)) {\n+            try (S3Object indexObject = s3Client.getObject(config.getBucket(), descriptorJson);\n+                 S3ObjectInputStream is = indexObject.getObjectContent()) {\n+              final ObjectMetadata objectMetadata = indexObject.getObjectMetadata();\n               final DataSegment dataSegment = jsonMapper.readValue(is, DataSegment.class);\n               log.info(\"Found segment [%s] located at [%s]\", dataSegment.getIdentifier(), indexZip);\n \n@@ -99,8 +101,10 @@ public S3DataSegmentFinder(\n                       descriptorJson,\n                       indexObject\n                   );\n-                  S3Object newDescJsonObject = new S3Object(descriptorJson, jsonMapper.writeValueAsString(dataSegment));\n-                  s3Client.putObject(config.getBucket(), newDescJsonObject);\n+                  final ByteArrayInputStream bais = new ByteArrayInputStream(\n+                      StringUtils.toUtf8(jsonMapper.writeValueAsString(dataSegment))\n+                  );\n+                  s3Client.putObject(config.getBucket(), descriptorJson, bais, objectMetadata);\n                 }\n               }\n               segments.add(dataSegment);\n@@ -114,7 +118,7 @@ public S3DataSegmentFinder(\n         }\n       }\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       throw new SegmentLoadingException(e, \"Problem interacting with S3\");\n     }\n     catch (IOException e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentFinder.java",
                "sha": "649554e7564b074d1d841ed7d05323b9fb56390d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentKiller.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentKiller.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 7,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentKiller.java",
                "patch": "@@ -19,14 +19,14 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n import com.google.inject.Inject;\n import io.druid.java.util.common.MapUtils;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentKiller;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n \n import java.util.Map;\n \n@@ -36,11 +36,11 @@\n {\n   private static final Logger log = new Logger(S3DataSegmentKiller.class);\n \n-  private final RestS3Service s3Client;\n+  private final AmazonS3 s3Client;\n \n   @Inject\n   public S3DataSegmentKiller(\n-      RestS3Service s3Client\n+      AmazonS3 s3Client\n   )\n   {\n     this.s3Client = s3Client;\n@@ -55,16 +55,16 @@ public void kill(DataSegment segment) throws SegmentLoadingException\n       String s3Path = MapUtils.getString(loadSpec, \"key\");\n       String s3DescriptorPath = S3Utils.descriptorPathForSegmentPath(s3Path);\n \n-      if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n+      if (s3Client.doesObjectExist(s3Bucket, s3Path)) {\n         log.info(\"Removing index file[s3://%s/%s] from s3!\", s3Bucket, s3Path);\n         s3Client.deleteObject(s3Bucket, s3Path);\n       }\n-      if (s3Client.isObjectInBucket(s3Bucket, s3DescriptorPath)) {\n+      if (s3Client.doesObjectExist(s3Bucket, s3DescriptorPath)) {\n         log.info(\"Removing descriptor file[s3://%s/%s] from s3!\", s3Bucket, s3DescriptorPath);\n         s3Client.deleteObject(s3Bucket, s3DescriptorPath);\n       }\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       throw new SegmentLoadingException(e, \"Couldn't kill segment[%s]: [%s]\", segment.getIdentifier(), e);\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentKiller.java",
                "sha": "4053fdd6056aabb61068e995ec7290baacf11eb7",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 33,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -19,6 +19,13 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.CopyObjectRequest;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n+import com.amazonaws.services.s3.model.StorageClass;\n import com.google.common.base.Predicate;\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableMap;\n@@ -34,10 +41,6 @@\n import io.druid.segment.loading.DataSegmentPusher;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.acl.gs.GSAccessControlList;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n \n import java.io.IOException;\n import java.util.Map;\n@@ -46,12 +49,12 @@\n {\n   private static final Logger log = new Logger(S3DataSegmentMover.class);\n \n-  private final RestS3Service s3Client;\n+  private final AmazonS3 s3Client;\n   private final S3DataSegmentPusherConfig config;\n \n   @Inject\n   public S3DataSegmentMover(\n-      RestS3Service s3Client,\n+      AmazonS3 s3Client,\n       S3DataSegmentPusherConfig config\n   )\n   {\n@@ -103,7 +106,7 @@ public boolean apply(String input)\n               .build()\n       );\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       throw new SegmentLoadingException(e, \"Unable to move segment[%s]: [%s]\", segment.getIdentifier(), e);\n     }\n   }\n@@ -113,7 +116,7 @@ private void safeMove(\n       final String s3Path,\n       final String targetS3Bucket,\n       final String targetS3Path\n-  ) throws ServiceException, SegmentLoadingException\n+  ) throws SegmentLoadingException\n   {\n     try {\n       S3Utils.retryS3Operation(\n@@ -129,15 +132,15 @@ private void safeMove(\n               selfCheckingMove(s3Bucket, targetS3Bucket, s3Path, targetS3Path, copyMsg);\n               return null;\n             }\n-            catch (ServiceException | IOException | SegmentLoadingException e) {\n+            catch (AmazonServiceException | IOException | SegmentLoadingException e) {\n               log.info(e, \"Error while trying to move \" + copyMsg);\n               throw e;\n             }\n           }\n       );\n     }\n     catch (Exception e) {\n-      Throwables.propagateIfInstanceOf(e, ServiceException.class);\n+      Throwables.propagateIfInstanceOf(e, AmazonServiceException.class);\n       Throwables.propagateIfInstanceOf(e, SegmentLoadingException.class);\n       throw Throwables.propagate(e);\n     }\n@@ -155,40 +158,41 @@ private void selfCheckingMove(\n       String s3Path,\n       String targetS3Path,\n       String copyMsg\n-  ) throws ServiceException, IOException, SegmentLoadingException\n+  ) throws IOException, SegmentLoadingException\n   {\n     if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) {\n       log.info(\"No need to move file[s3://%s/%s] onto itself\", s3Bucket, s3Path);\n       return;\n     }\n-    if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n-      final S3Object[] list = s3Client.listObjects(s3Bucket, s3Path, \"\");\n-      if (list.length == 0) {\n+    if (s3Client.doesObjectExist(s3Bucket, s3Path)) {\n+      final ListObjectsV2Result listResult = s3Client.listObjectsV2(\n+          new ListObjectsV2Request()\n+              .withBucketName(s3Bucket)\n+              .withPrefix(s3Path)\n+              .withMaxKeys(1)\n+      );\n+      if (listResult.getKeyCount() == 0) {\n         // should never happen\n         throw new ISE(\"Unable to list object [s3://%s/%s]\", s3Bucket, s3Path);\n       }\n-      final S3Object s3Object = list[0];\n-      if (s3Object.getStorageClass() != null &&\n-          s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n-        throw new ServiceException(StringUtils.format(\n-            \"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\",\n-            s3Bucket,\n-            s3Path\n-        ));\n+      final S3ObjectSummary objectSummary = listResult.getObjectSummaries().get(0);\n+      if (objectSummary.getStorageClass() != null &&\n+          StorageClass.fromValue(StringUtils.toUpperCase(objectSummary.getStorageClass())).equals(StorageClass.Glacier)) {\n+        throw new AmazonServiceException(\n+            StringUtils.format(\n+                \"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\",\n+                s3Bucket,\n+                s3Path\n+            )\n+        );\n       } else {\n         log.info(\"Moving file %s\", copyMsg);\n-        final S3Object target = new S3Object(targetS3Path);\n+        final CopyObjectRequest copyRequest = new CopyObjectRequest(s3Bucket, s3Path, targetS3Bucket, targetS3Path);\n         if (!config.getDisableAcl()) {\n-          target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+          copyRequest.setAccessControlList(S3Utils.grantFullControlToBucketOwner(s3Client, targetS3Bucket));\n         }\n-        s3Client.copyObject(\n-            s3Bucket,\n-            s3Path,\n-            targetS3Bucket,\n-            target,\n-            false\n-        );\n-        if (!s3Client.isObjectInBucket(targetS3Bucket, targetS3Path)) {\n+        s3Client.copyObject(copyRequest);\n+        if (!s3Client.doesObjectExist(targetS3Bucket, targetS3Path)) {\n           throw new IOE(\n               \"After copy was reported as successful the file doesn't exist in the target location [%s]\",\n               copyMsg\n@@ -199,7 +203,7 @@ private void selfCheckingMove(\n       }\n     } else {\n       // ensure object exists in target location\n-      if (s3Client.isObjectInBucket(targetS3Bucket, targetS3Path)) {\n+      if (s3Client.doesObjectExist(targetS3Bucket, targetS3Path)) {\n         log.info(\n             \"Not moving file [s3://%s/%s], already present in target location [s3://%s/%s]\",\n             s3Bucket, s3Path,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "e50ea2cca700ca8850e2a3f0dab5ad6751555908",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPuller.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPuller.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 30,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPuller.java",
                "patch": "@@ -19,6 +19,11 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.google.common.base.Predicate;\n import com.google.common.base.Strings;\n import com.google.common.base.Throwables;\n@@ -33,17 +38,15 @@\n import io.druid.java.util.common.RE;\n import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.UOE;\n+import io.druid.java.util.common.io.Closer;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.segment.loading.URIDataPuller;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.S3ServiceException;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.StorageObject;\n \n import javax.tools.FileObject;\n import java.io.File;\n+import java.io.FilterInputStream;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -59,17 +62,15 @@\n {\n   public static final int DEFAULT_RETRY_COUNT = 3;\n \n-  public static FileObject buildFileObject(final URI uri, final RestS3Service s3Client) throws ServiceException\n+  private static FileObject buildFileObject(final URI uri, final AmazonS3 s3Client) throws AmazonServiceException\n   {\n     final S3Coords coords = new S3Coords(checkURI(uri));\n-    final StorageObject s3Obj = s3Client.getObjectDetails(coords.bucket, coords.path);\n+    final S3ObjectSummary objectSummary = S3Utils.getSingleObjectSummary(s3Client, coords.bucket, coords.path);\n     final String path = uri.getPath();\n \n     return new FileObject()\n     {\n-      final Object inputStreamOpener = new Object();\n-      volatile boolean streamAcquired = false;\n-      volatile StorageObject storageObject = s3Obj;\n+      S3Object s3Object = null;\n \n       @Override\n       public URI toUri()\n@@ -84,22 +85,33 @@ public String getName()\n         return Files.getNameWithoutExtension(path) + (Strings.isNullOrEmpty(ext) ? \"\" : (\".\" + ext));\n       }\n \n+      /**\n+       * Returns an input stream for a s3 object. The returned input stream is not thread-safe.\n+       */\n       @Override\n       public InputStream openInputStream() throws IOException\n       {\n         try {\n-          synchronized (inputStreamOpener) {\n-            if (streamAcquired) {\n-              return storageObject.getDataInputStream();\n-            }\n+          if (s3Object == null) {\n             // lazily promote to full GET\n-            storageObject = s3Client.getObject(s3Obj.getBucketName(), s3Obj.getKey());\n-            final InputStream stream = storageObject.getDataInputStream();\n-            streamAcquired = true;\n-            return stream;\n+            s3Object = s3Client.getObject(objectSummary.getBucketName(), objectSummary.getKey());\n           }\n+\n+          final InputStream in = s3Object.getObjectContent();\n+          final Closer closer = Closer.create();\n+          closer.register(in);\n+          closer.register(s3Object);\n+\n+          return new FilterInputStream(in)\n+          {\n+            @Override\n+            public void close() throws IOException\n+            {\n+              closer.close();\n+            }\n+          };\n         }\n-        catch (ServiceException e) {\n+        catch (AmazonServiceException e) {\n           throw new IOE(e, \"Could not load S3 URI [%s]\", uri);\n         }\n       }\n@@ -131,7 +143,7 @@ public Writer openWriter()\n       @Override\n       public long getLastModified()\n       {\n-        return s3Obj.getLastModifiedDate().getTime();\n+        return objectSummary.getLastModified().getTime();\n       }\n \n       @Override\n@@ -149,11 +161,11 @@ public boolean delete()\n   protected static final String BUCKET = \"bucket\";\n   protected static final String KEY = \"key\";\n \n-  protected final RestS3Service s3Client;\n+  protected final AmazonS3 s3Client;\n \n   @Inject\n   public S3DataSegmentPuller(\n-      RestS3Service s3Client\n+      AmazonS3 s3Client\n   )\n   {\n     this.s3Client = s3Client;\n@@ -180,7 +192,7 @@ public InputStream openStream() throws IOException\n           try {\n             return buildFileObject(uri, s3Client).openInputStream();\n           }\n-          catch (ServiceException e) {\n+          catch (AmazonServiceException e) {\n             if (e.getCause() != null) {\n               if (S3Utils.S3RETRY.apply(e)) {\n                 throw new IOException(\"Recoverable exception\", e);\n@@ -242,7 +254,7 @@ public InputStream getInputStream(URI uri) throws IOException\n     try {\n       return buildFileObject(uri, s3Client).openInputStream();\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       throw new IOE(e, \"Could not load URI [%s]\", uri);\n     }\n   }\n@@ -259,8 +271,8 @@ public boolean apply(Throwable e)\n         if (e == null) {\n           return false;\n         }\n-        if (e instanceof ServiceException) {\n-          return S3Utils.isServiceExceptionRecoverable((ServiceException) e);\n+        if (e instanceof AmazonServiceException) {\n+          return S3Utils.isServiceExceptionRecoverable((AmazonServiceException) e);\n         }\n         if (S3Utils.S3RETRY.apply(e)) {\n           return true;\n@@ -284,10 +296,11 @@ public boolean apply(Throwable e)\n   public String getVersion(URI uri) throws IOException\n   {\n     try {\n-      final FileObject object = buildFileObject(uri, s3Client);\n-      return StringUtils.format(\"%d\", object.getLastModified());\n+      final S3Coords coords = new S3Coords(checkURI(uri));\n+      final S3ObjectSummary objectSummary = S3Utils.getSingleObjectSummary(s3Client, coords.bucket, coords.path);\n+      return StringUtils.format(\"%d\", objectSummary.getLastModified().getTime());\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       if (S3Utils.isServiceExceptionRecoverable(e)) {\n         // The recoverable logic is always true for IOException, so we want to only pass IOException if it is recoverable\n         throw new IOE(e, \"Could not fetch last modified timestamp from URI [%s]\", uri);\n@@ -301,10 +314,10 @@ private boolean isObjectInBucket(final S3Coords coords) throws SegmentLoadingExc\n   {\n     try {\n       return S3Utils.retryS3Operation(\n-          () -> S3Utils.isObjectInBucket(s3Client, coords.bucket, coords.path)\n+          () -> S3Utils.isObjectInBucketIgnoringPermission(s3Client, coords.bucket, coords.path)\n       );\n     }\n-    catch (S3ServiceException | IOException e) {\n+    catch (AmazonS3Exception | IOException e) {\n       throw new SegmentLoadingException(e, \"S3 fail! Key[%s]\", coords);\n     }\n     catch (Exception e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPuller.java",
                "sha": "55a00a76b8b8ed3d3df5c0a06833fb23d4a142ea",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 42,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java",
                "patch": "@@ -19,21 +19,20 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.PutObjectRequest;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.inject.Inject;\n-import io.druid.java.util.emitter.EmittingLogger;\n import io.druid.java.util.common.CompressionUtils;\n import io.druid.java.util.common.StringUtils;\n+import io.druid.java.util.emitter.EmittingLogger;\n import io.druid.segment.SegmentUtils;\n import io.druid.segment.loading.DataSegmentPusher;\n import io.druid.timeline.DataSegment;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.acl.gs.GSAccessControlList;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n \n import java.io.File;\n import java.io.IOException;\n@@ -46,13 +45,13 @@\n {\n   private static final EmittingLogger log = new EmittingLogger(S3DataSegmentPusher.class);\n \n-  private final RestS3Service s3Client;\n+  private final AmazonS3 s3Client;\n   private final S3DataSegmentPusherConfig config;\n   private final ObjectMapper jsonMapper;\n \n   @Inject\n   public S3DataSegmentPusher(\n-      RestS3Service s3Client,\n+      AmazonS3 s3Client,\n       S3DataSegmentPusherConfig config,\n       ObjectMapper jsonMapper\n   )\n@@ -97,45 +96,43 @@ public DataSegment push(final File indexFilesDir, final DataSegment inSegment, f\n     final File zipOutFile = File.createTempFile(\"druid\", \"index.zip\");\n     final long indexSize = CompressionUtils.zip(indexFilesDir, zipOutFile);\n \n+    final DataSegment outSegment = inSegment.withSize(indexSize)\n+                                            .withLoadSpec(makeLoadSpec(config.getBucket(), s3Path))\n+                                            .withBinaryVersion(SegmentUtils.getVersionFromDir(indexFilesDir));\n+\n+    final File descriptorFile = File.createTempFile(\"druid\", \"descriptor.json\");\n+    // Avoid using Guava in DataSegmentPushers because they might be used with very diverse Guava versions in\n+    // runtime, and because Guava deletes methods over time, that causes incompatibilities.\n+    Files.write(descriptorFile.toPath(), jsonMapper.writeValueAsBytes(outSegment));\n+\n     try {\n       return S3Utils.retryS3Operation(\n           () -> {\n-            S3Object toPush = new S3Object(zipOutFile);\n-            putObject(config.getBucket(), s3Path, toPush, replaceExisting);\n-\n-            final DataSegment outSegment = inSegment.withSize(indexSize)\n-                                                    .withLoadSpec(makeLoadSpec(config.getBucket(), toPush.getKey()))\n-                                                    .withBinaryVersion(SegmentUtils.getVersionFromDir(indexFilesDir));\n-\n-            File descriptorFile = File.createTempFile(\"druid\", \"descriptor.json\");\n-            // Avoid using Guava in DataSegmentPushers because they might be used with very diverse Guava versions in\n-            // runtime, and because Guava deletes methods over time, that causes incompatibilities.\n-            Files.write(descriptorFile.toPath(), jsonMapper.writeValueAsBytes(outSegment));\n-            S3Object descriptorObject = new S3Object(descriptorFile);\n-\n-            putObject(\n+            uploadFileIfPossible(s3Client, config.getBucket(), s3Path, zipOutFile, replaceExisting);\n+            uploadFileIfPossible(\n+                s3Client,\n                 config.getBucket(),\n                 S3Utils.descriptorPathForSegmentPath(s3Path),\n-                descriptorObject,\n+                descriptorFile,\n                 replaceExisting\n             );\n \n-            log.info(\"Deleting zipped index File[%s]\", zipOutFile);\n-            zipOutFile.delete();\n-\n-            log.info(\"Deleting descriptor file[%s]\", descriptorFile);\n-            descriptorFile.delete();\n-\n             return outSegment;\n           }\n       );\n     }\n-    catch (ServiceException e) {\n+    catch (AmazonServiceException e) {\n       throw new IOException(e);\n     }\n     catch (Exception e) {\n       throw Throwables.propagate(e);\n     }\n+    finally {\n+      log.info(\"Deleting temporary cached index.zip\");\n+      zipOutFile.delete();\n+      log.info(\"Deleting temporary cached descriptor.json\");\n+      descriptorFile.delete();\n+    }\n   }\n \n   @Override\n@@ -163,21 +160,26 @@ public DataSegment push(final File indexFilesDir, final DataSegment inSegment, f\n     );\n   }\n \n-  private void putObject(String bucketName, String path, S3Object object, boolean replaceExisting)\n-      throws ServiceException\n+  private void uploadFileIfPossible(\n+      AmazonS3 s3Client,\n+      String bucket,\n+      String key,\n+      File file,\n+      boolean replaceExisting\n+  )\n   {\n-    object.setBucketName(bucketName);\n-    object.setKey(path);\n-    if (!config.getDisableAcl()) {\n-      object.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n-    }\n-\n-    log.info(\"Pushing %s.\", object);\n-\n-    if (!replaceExisting && S3Utils.isObjectInBucket(s3Client, bucketName, object.getKey())) {\n-      log.info(\"Skipping push because key [%s] exists && replaceExisting == false\", object.getKey());\n+    if (!replaceExisting && S3Utils.isObjectInBucketIgnoringPermission(s3Client, bucket, key)) {\n+      log.info(\"Skipping push because key [%s] exists && replaceExisting == false\", key);\n     } else {\n-      s3Client.putObject(bucketName, object);\n+      final PutObjectRequest indexFilePutRequest = new PutObjectRequest(bucket, key, file);\n+\n+      if (!config.getDisableAcl()) {\n+        indexFilePutRequest.setAccessControlList(\n+            S3Utils.grantFullControlToBucketOwner(s3Client, bucket)\n+        );\n+      }\n+      log.info(\"Pushing [%s] to bucket[%s] and key[%s].\", file, bucket, key);\n+      s3Client.putObject(indexFilePutRequest);\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentPusher.java",
                "sha": "981d24a7ef5e9b91891bdc7c2e1d50fe5436d60b",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3StorageDruidModule.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3StorageDruidModule.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 11,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3StorageDruidModule.java",
                "patch": "@@ -19,22 +19,27 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.ClientConfiguration;\n+import com.amazonaws.ClientConfigurationFactory;\n import com.amazonaws.auth.AWSCredentialsProvider;\n-import com.amazonaws.auth.AWSSessionCredentials;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.S3ClientOptions;\n import com.fasterxml.jackson.core.Version;\n import com.fasterxml.jackson.databind.Module;\n import com.google.common.collect.ImmutableList;\n import com.google.inject.Binder;\n import com.google.inject.Provides;\n import com.google.inject.multibindings.MapBinder;\n import io.druid.common.aws.AWSCredentialsConfig;\n+import io.druid.common.aws.AWSEndpointConfig;\n+import io.druid.common.aws.AWSProxyConfig;\n import io.druid.data.SearchableVersionedDataFinder;\n import io.druid.guice.Binders;\n import io.druid.guice.JsonConfigProvider;\n import io.druid.guice.LazySingleton;\n import io.druid.initialization.DruidModule;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.security.AWSCredentials;\n+import org.apache.commons.lang.StringUtils;\n \n import java.util.List;\n \n@@ -75,6 +80,8 @@ public void setupModule(SetupContext context)\n   public void configure(Binder binder)\n   {\n     JsonConfigProvider.bind(binder, \"druid.s3\", AWSCredentialsConfig.class);\n+    JsonConfigProvider.bind(binder, \"druid.s3.proxy\", AWSProxyConfig.class);\n+    JsonConfigProvider.bind(binder, \"druid.s3.endpoint\", AWSEndpointConfig.class);\n     MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n              .addBinding(\"s3\")\n              .to(S3TimestampVersionedDataFinder.class)\n@@ -101,15 +108,44 @@ public void configure(Binder binder)\n \n   @Provides\n   @LazySingleton\n-  public RestS3Service getRestS3Service(AWSCredentialsProvider provider)\n+  public AmazonS3 getAmazonS3Client(\n+      AWSCredentialsProvider provider,\n+      AWSProxyConfig proxyConfig,\n+      AWSEndpointConfig endpointConfig\n+  )\n   {\n-    if (provider.getCredentials() instanceof AWSSessionCredentials) {\n-      return new RestS3Service(new AWSSessionCredentialsAdapter(provider));\n-    } else {\n-      return new RestS3Service(new AWSCredentials(\n-          provider.getCredentials().getAWSAccessKeyId(),\n-          provider.getCredentials().getAWSSecretKey()\n-      ));\n+    // AmazonS3ClientBuilder can't be used because it makes integration tests failed\n+    final ClientConfiguration configuration = new ClientConfigurationFactory().getConfig();\n+    final AmazonS3Client client = new AmazonS3Client(provider, setProxyConfig(configuration, proxyConfig));\n+\n+    if (StringUtils.isNotEmpty(endpointConfig.getUrl())) {\n+      if (StringUtils.isNotEmpty(endpointConfig.getServiceName()) &&\n+          StringUtils.isNotEmpty(endpointConfig.getSigningRegion())) {\n+        client.setEndpoint(endpointConfig.getUrl(), endpointConfig.getServiceName(), endpointConfig.getSigningRegion());\n+      } else {\n+        client.setEndpoint(endpointConfig.getUrl());\n+      }\n+    }\n+\n+    client.setS3ClientOptions(S3ClientOptions.builder().enableForceGlobalBucketAccess().build());\n+\n+    return client;\n+  }\n+\n+  private static ClientConfiguration setProxyConfig(ClientConfiguration conf, AWSProxyConfig proxyConfig)\n+  {\n+    if (StringUtils.isNotEmpty(proxyConfig.getHost())) {\n+      conf.setProxyHost(proxyConfig.getHost());\n+    }\n+    if (proxyConfig.getPort() != -1) {\n+      conf.setProxyPort(proxyConfig.getPort());\n+    }\n+    if (StringUtils.isNotEmpty(proxyConfig.getUsername())) {\n+      conf.setProxyUsername(proxyConfig.getUsername());\n+    }\n+    if (StringUtils.isNotEmpty(proxyConfig.getPassword())) {\n+      conf.setProxyPassword(proxyConfig.getPassword());\n     }\n+    return conf;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3StorageDruidModule.java",
                "sha": "ac7839f43631b306bbf80b076e1768b55b47fa65",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TaskLogs.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TaskLogs.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 28,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TaskLogs.java",
                "patch": "@@ -19,6 +19,11 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n import com.google.common.base.Optional;\n import com.google.common.base.Throwables;\n import com.google.common.io.ByteSource;\n@@ -27,10 +32,6 @@\n import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.tasklogs.TaskLogs;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.StorageService;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.StorageObject;\n \n import java.io.File;\n import java.io.IOException;\n@@ -43,11 +44,11 @@\n {\n   private static final Logger log = new Logger(S3TaskLogs.class);\n \n-  private final StorageService service;\n+  private final AmazonS3 service;\n   private final S3TaskLogsConfig config;\n \n   @Inject\n-  public S3TaskLogs(S3TaskLogsConfig config, RestS3Service service)\n+  public S3TaskLogs(S3TaskLogsConfig config, AmazonS3 service)\n   {\n     this.config = config;\n     this.service = service;\n@@ -59,46 +60,41 @@ public S3TaskLogs(S3TaskLogsConfig config, RestS3Service service)\n     final String taskKey = getTaskLogKey(taskid);\n \n     try {\n-      final StorageObject objectDetails = service.getObjectDetails(config.getS3Bucket(), taskKey, null, null, null, null);\n+      final ObjectMetadata objectMetadata = service.getObjectMetadata(config.getS3Bucket(), taskKey);\n \n-      return Optional.<ByteSource>of(\n+      return Optional.of(\n           new ByteSource()\n           {\n             @Override\n             public InputStream openStream() throws IOException\n             {\n               try {\n                 final long start;\n-                final long end = objectDetails.getContentLength() - 1;\n+                final long end = objectMetadata.getContentLength() - 1;\n \n-                if (offset > 0 && offset < objectDetails.getContentLength()) {\n+                if (offset > 0 && offset < objectMetadata.getContentLength()) {\n                   start = offset;\n-                } else if (offset < 0 && (-1 * offset) < objectDetails.getContentLength()) {\n-                  start = objectDetails.getContentLength() + offset;\n+                } else if (offset < 0 && (-1 * offset) < objectMetadata.getContentLength()) {\n+                  start = objectMetadata.getContentLength() + offset;\n                 } else {\n                   start = 0;\n                 }\n \n-                return service.getObject(\n-                    config.getS3Bucket(),\n-                    taskKey,\n-                    null,\n-                    null,\n-                    new String[]{objectDetails.getETag()},\n-                    null,\n-                    start,\n-                    end\n-                ).getDataInputStream();\n+                final GetObjectRequest request = new GetObjectRequest(config.getS3Bucket(), taskKey)\n+                    .withMatchingETagConstraint(objectMetadata.getETag())\n+                    .withRange(start, end);\n+\n+                return service.getObject(request).getObjectContent();\n               }\n-              catch (ServiceException e) {\n+              catch (AmazonServiceException e) {\n                 throw new IOException(e);\n               }\n             }\n           }\n       );\n     }\n-    catch (ServiceException e) {\n-      if (404 == e.getResponseCode()\n+    catch (AmazonS3Exception e) {\n+      if (404 == e.getStatusCode()\n           || \"NoSuchKey\".equals(e.getErrorCode())\n           || \"NoSuchBucket\".equals(e.getErrorCode())) {\n         return Optional.absent();\n@@ -117,9 +113,7 @@ public void pushTaskLog(final String taskid, final File logFile) throws IOExcept\n     try {\n       S3Utils.retryS3Operation(\n           () -> {\n-            final StorageObject object = new StorageObject(logFile);\n-            object.setKey(taskKey);\n-            service.putObject(config.getS3Bucket(), object);\n+            service.putObject(config.getS3Bucket(), taskKey, logFile);\n             return null;\n           }\n       );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TaskLogs.java",
                "sha": "afef97a8892c91880256cb739d1f2a51c8af7e36",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TimestampVersionedDataFinder.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TimestampVersionedDataFinder.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 12,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TimestampVersionedDataFinder.java",
                "patch": "@@ -19,16 +19,17 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.google.common.base.Throwables;\n import com.google.inject.Inject;\n import io.druid.data.SearchableVersionedDataFinder;\n import io.druid.java.util.common.RetryUtils;\n import io.druid.java.util.common.StringUtils;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n \n import javax.annotation.Nullable;\n import java.net.URI;\n+import java.util.Iterator;\n import java.util.regex.Pattern;\n \n /**\n@@ -37,8 +38,10 @@\n  */\n public class S3TimestampVersionedDataFinder extends S3DataSegmentPuller implements SearchableVersionedDataFinder<URI>\n {\n+  private static final int MAX_LISTING_KEYS = 1000;\n+\n   @Inject\n-  public S3TimestampVersionedDataFinder(RestS3Service s3Client)\n+  public S3TimestampVersionedDataFinder(AmazonS3 s3Client)\n   {\n     super(s3Client);\n   }\n@@ -65,23 +68,27 @@ public URI getLatestVersion(final URI uri, final @Nullable Pattern pattern)\n             final S3Coords coords = new S3Coords(checkURI(uri));\n             long mostRecent = Long.MIN_VALUE;\n             URI latest = null;\n-            S3Object[] objects = s3Client.listObjects(coords.bucket, coords.path, null);\n-            if (objects == null) {\n-              return null;\n-            }\n-            for (S3Object storageObject : objects) {\n-              storageObject.closeDataInputStream();\n-              String keyString = storageObject.getKey().substring(coords.path.length());\n+            final Iterator<S3ObjectSummary> objectSummaryIterator = S3Utils.objectSummaryIterator(\n+                s3Client,\n+                coords.bucket,\n+                coords.path,\n+                MAX_LISTING_KEYS\n+            );\n+            while (objectSummaryIterator.hasNext()) {\n+              final S3ObjectSummary objectSummary = objectSummaryIterator.next();\n+              String keyString = objectSummary.getKey().substring(coords.path.length());\n               if (keyString.startsWith(\"/\")) {\n                 keyString = keyString.substring(1);\n               }\n               if (pattern != null && !pattern.matcher(keyString).matches()) {\n                 continue;\n               }\n-              final long latestModified = storageObject.getLastModifiedDate().getTime();\n+              final long latestModified = objectSummary.getLastModified().getTime();\n               if (latestModified >= mostRecent) {\n                 mostRecent = latestModified;\n-                latest = new URI(StringUtils.format(\"s3://%s/%s\", storageObject.getBucketName(), storageObject.getKey()));\n+                latest = new URI(\n+                    StringUtils.format(\"s3://%s/%s\", objectSummary.getBucketName(), objectSummary.getKey())\n+                );\n               }\n             }\n             return latest;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3TimestampVersionedDataFinder.java",
                "sha": "2d4724851b7cb023469df149acdd96ebb0557524",
                "status": "modified"
            },
            {
                "additions": 134,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3Utils.java",
                "changes": 215,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3Utils.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 81,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3Utils.java",
                "patch": "@@ -19,45 +19,40 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.AccessControlList;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.CanonicalGrantee;\n+import com.amazonaws.services.s3.model.Grant;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import com.amazonaws.services.s3.model.Permission;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.google.common.base.Joiner;\n import com.google.common.base.Predicate;\n-import com.google.common.base.Throwables;\n+import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.RetryUtils;\n import io.druid.java.util.common.RetryUtils.Task;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.StorageObjectsChunk;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n-import org.jets3t.service.model.StorageObject;\n \n import java.io.IOException;\n+import java.net.URI;\n import java.util.Iterator;\n+import java.util.NoSuchElementException;\n \n /**\n  *\n  */\n public class S3Utils\n {\n   private static final Joiner JOINER = Joiner.on(\"/\").skipNulls();\n+  private static final String MIMETYPE_JETS3T_DIRECTORY = \"application/x-directory\";\n \n-  public static void closeStreamsQuietly(S3Object s3Obj)\n-  {\n-    if (s3Obj == null) {\n-      return;\n-    }\n-\n-    try {\n-      s3Obj.closeDataInputStream();\n-    }\n-    catch (IOException e) {\n-\n-    }\n-  }\n-\n-  public static boolean isServiceExceptionRecoverable(ServiceException ex)\n+  static boolean isServiceExceptionRecoverable(AmazonServiceException ex)\n   {\n     final boolean isIOException = ex.getCause() instanceof IOException;\n-    final boolean isTimeout = \"RequestTimeout\".equals(((ServiceException) ex).getErrorCode());\n+    final boolean isTimeout = \"RequestTimeout\".equals(ex.getErrorCode());\n     return isIOException || isTimeout;\n   }\n \n@@ -70,8 +65,8 @@ public boolean apply(Throwable e)\n         return false;\n       } else if (e instanceof IOException) {\n         return true;\n-      } else if (e instanceof ServiceException) {\n-        return isServiceExceptionRecoverable((ServiceException) e);\n+      } else if (e instanceof AmazonServiceException) {\n+        return isServiceExceptionRecoverable((AmazonServiceException) e);\n       } else {\n         return apply(e.getCause());\n       }\n@@ -88,91 +83,81 @@ public boolean apply(Throwable e)\n     return RetryUtils.retry(f, S3RETRY, maxTries);\n   }\n \n-  public static boolean isObjectInBucket(RestS3Service s3Client, String bucketName, String objectKey)\n-      throws ServiceException\n+  static boolean isObjectInBucketIgnoringPermission(AmazonS3 s3Client, String bucketName, String objectKey)\n   {\n     try {\n-      s3Client.getObjectDetails(bucketName, objectKey);\n+      return s3Client.doesObjectExist(bucketName, objectKey);\n     }\n-    catch (ServiceException e) {\n-      if (404 == e.getResponseCode()\n-          || \"NoSuchKey\".equals(e.getErrorCode())\n-          || \"NoSuchBucket\".equals(e.getErrorCode())) {\n-        return false;\n-      }\n-      if (\"AccessDenied\".equals(e.getErrorCode())) {\n+    catch (AmazonS3Exception e) {\n+      if (e.getStatusCode() == 404) {\n         // Object is inaccessible to current user, but does exist.\n         return true;\n       }\n       // Something else has gone wrong\n       throw e;\n     }\n-    return true;\n   }\n \n-  public static Iterator<StorageObject> storageObjectsIterator(\n-      final RestS3Service s3Client,\n+  public static Iterator<S3ObjectSummary> objectSummaryIterator(\n+      final AmazonS3 s3Client,\n       final String bucket,\n       final String prefix,\n-      final long maxListingLength\n+      final int numMaxKeys\n   )\n   {\n-    return new Iterator<StorageObject>()\n+    final ListObjectsV2Request request = new ListObjectsV2Request()\n+        .withBucketName(bucket)\n+        .withPrefix(prefix)\n+        .withMaxKeys(numMaxKeys);\n+\n+    return new Iterator<S3ObjectSummary>()\n     {\n-      private StorageObjectsChunk objectsChunk;\n-      private int objectsChunkOffset;\n+      private ListObjectsV2Result result;\n+      private Iterator<S3ObjectSummary> objectSummaryIterator;\n \n-      @Override\n-      public boolean hasNext()\n       {\n-        if (objectsChunk == null) {\n-          objectsChunk = listObjectsChunkedAfter(\"\");\n-          objectsChunkOffset = 0;\n-        }\n-\n-        if (objectsChunk.getObjects().length <= objectsChunkOffset) {\n-          if (objectsChunk.isListingComplete()) {\n-            return false;\n-          } else {\n-            objectsChunk = listObjectsChunkedAfter(objectsChunk.getPriorLastKey());\n-            objectsChunkOffset = 0;\n-          }\n-        }\n+        fetchNextBatch();\n+      }\n \n-        return true;\n+      private void fetchNextBatch()\n+      {\n+        result = s3Client.listObjectsV2(request);\n+        objectSummaryIterator = result.getObjectSummaries().iterator();\n+        request.setContinuationToken(result.getContinuationToken());\n       }\n \n-      private StorageObjectsChunk listObjectsChunkedAfter(final String priorLastKey)\n+      @Override\n+      public boolean hasNext()\n       {\n-        try {\n-          return retryS3Operation(\n-              () -> s3Client.listObjectsChunked(bucket, prefix, null, maxListingLength, priorLastKey)\n-          );\n-        }\n-        catch (Exception e) {\n-          throw Throwables.propagate(e);\n-        }\n+        return objectSummaryIterator.hasNext() || result.isTruncated();\n       }\n \n       @Override\n-      public StorageObject next()\n+      public S3ObjectSummary next()\n       {\n         if (!hasNext()) {\n-          throw new IllegalStateException();\n+          throw new NoSuchElementException();\n         }\n-        StorageObject storageObject = objectsChunk.getObjects()[objectsChunkOffset];\n-        objectsChunkOffset++;\n \n-        return storageObject;\n-      }\n+        if (objectSummaryIterator.hasNext()) {\n+          return objectSummaryIterator.next();\n+        }\n \n-      @Override\n-      public void remove()\n-      {\n-        throw new UnsupportedOperationException();\n-      }\n+        if (result.isTruncated()) {\n+          fetchNextBatch();\n+        }\n \n+        if (!objectSummaryIterator.hasNext()) {\n+          throw new ISE(\n+              \"Failed to further iterate on bucket[%s] and prefix[%s]. The last continuationToken was [%s]\",\n+              bucket,\n+              prefix,\n+              result.getContinuationToken()\n+          );\n+        }\n \n+        return objectSummaryIterator.next();\n+      }\n     };\n   }\n \n@@ -184,25 +169,93 @@ public static String constructSegmentPath(String baseKey, String storageDir)\n     ) + \"/index.zip\";\n   }\n \n-  public static String descriptorPathForSegmentPath(String s3Path)\n+  static String descriptorPathForSegmentPath(String s3Path)\n   {\n     return s3Path.substring(0, s3Path.lastIndexOf(\"/\")) + \"/descriptor.json\";\n   }\n \n-  public static String indexZipForSegmentPath(String s3Path)\n+  static String indexZipForSegmentPath(String s3Path)\n   {\n     return s3Path.substring(0, s3Path.lastIndexOf(\"/\")) + \"/index.zip\";\n   }\n \n-  public static String toFilename(String key)\n+  static String toFilename(String key)\n   {\n     return toFilename(key, \"\");\n   }\n \n-  public static String toFilename(String key, final String suffix)\n+  static String toFilename(String key, final String suffix)\n   {\n     String filename = key.substring(key.lastIndexOf(\"/\") + 1); // characters after last '/'\n     filename = filename.substring(0, filename.length() - suffix.length()); // remove the suffix from the end\n     return filename;\n   }\n+\n+  static AccessControlList grantFullControlToBucketOwner(AmazonS3 s3Client, String bucket)\n+  {\n+    final AccessControlList acl = s3Client.getBucketAcl(bucket);\n+    acl.grantAllPermissions(new Grant(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl));\n+    return acl;\n+  }\n+\n+  public static String extractS3Key(URI uri)\n+  {\n+    return uri.getPath().startsWith(\"/\") ? uri.getPath().substring(1) : uri.getPath();\n+  }\n+\n+  // Copied from org.jets3t.service.model.StorageObject.isDirectoryPlaceholder()\n+  public static boolean isDirectoryPlaceholder(String key, ObjectMetadata objectMetadata)\n+  {\n+    // Recognize \"standard\" directory place-holder indications used by\n+    // Amazon's AWS Console and Panic's Transmit.\n+    if (key.endsWith(\"/\") && objectMetadata.getContentLength() == 0) {\n+      return true;\n+    }\n+    // Recognize s3sync.rb directory placeholders by MD5/ETag value.\n+    if (\"d66759af42f282e1ba19144df2d405d0\".equals(objectMetadata.getETag())) {\n+      return true;\n+    }\n+    // Recognize place-holder objects created by the Google Storage console\n+    // or S3 Organizer Firefox extension.\n+    if (key.endsWith(\"_$folder$\") && objectMetadata.getContentLength() == 0) {\n+      return true;\n+    }\n+\n+    // We don't use JetS3t APIs anymore, but the below check is still needed for backward compatibility.\n+\n+    // Recognize legacy JetS3t directory place-holder objects, only gives\n+    // accurate results if an object's metadata is populated.\n+    if (objectMetadata.getContentLength() == 0 && MIMETYPE_JETS3T_DIRECTORY.equals(objectMetadata.getContentType())) {\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Gets a single {@link S3ObjectSummary} from s3. Since this method might return a wrong object if there are multiple\n+   * objects that match the given key, this method should be used only when it's guaranteed that the given key is unique\n+   * in the given bucket.\n+   *\n+   * @param s3Client s3 client\n+   * @param bucket   s3 bucket\n+   * @param key      unique key for the object to be retrieved\n+   */\n+  public static S3ObjectSummary getSingleObjectSummary(AmazonS3 s3Client, String bucket, String key)\n+  {\n+    final ListObjectsV2Request request = new ListObjectsV2Request()\n+        .withBucketName(bucket)\n+        .withPrefix(key)\n+        .withMaxKeys(1);\n+    final ListObjectsV2Result result = s3Client.listObjectsV2(request);\n+\n+    if (result.getKeyCount() == 0) {\n+      throw new ISE(\"Cannot find object for bucket[%s] and key[%s]\", bucket, key);\n+    }\n+    final S3ObjectSummary objectSummary = result.getObjectSummaries().get(0);\n+    if (!objectSummary.getBucketName().equals(bucket) || !objectSummary.getKey().equals(key)) {\n+      throw new ISE(\"Wrong object[%s] for bucket[%s] and key[%s]\", objectSummary, bucket, key);\n+    }\n+\n+    return objectSummary;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3Utils.java",
                "sha": "c4fa1576106632e959d1b43290fc668972bc3866",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/firehose/s3/StaticS3FirehoseFactoryTest.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/firehose/s3/StaticS3FirehoseFactoryTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 9,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/firehose/s3/StaticS3FirehoseFactoryTest.java",
                "patch": "@@ -19,8 +19,13 @@\n \n package io.druid.firehose.s3;\n \n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.fasterxml.jackson.core.JsonParser;\n+import com.fasterxml.jackson.databind.DeserializationContext;\n import com.fasterxml.jackson.databind.Module;\n import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.deser.std.StdDeserializer;\n import com.fasterxml.jackson.databind.module.SimpleModule;\n import com.fasterxml.jackson.module.guice.ObjectMapperModule;\n import com.google.common.collect.ImmutableList;\n@@ -29,8 +34,6 @@\n import com.google.inject.Injector;\n import com.google.inject.Provides;\n import io.druid.initialization.DruidModule;\n-import io.druid.jackson.DefaultObjectMapper;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -42,7 +45,7 @@\n  */\n public class StaticS3FirehoseFactoryTest\n {\n-  private static final RestS3Service SERVICE = new RestS3Service(null);\n+  private static final AmazonS3Client SERVICE = new AmazonS3Client();\n \n   @Test\n   public void testSerde() throws Exception\n@@ -75,22 +78,24 @@ public void testSerde() throws Exception\n \n   private static ObjectMapper createObjectMapper(DruidModule baseModule)\n   {\n-    final ObjectMapper baseMapper = new DefaultObjectMapper();\n-    baseModule.getJacksonModules().forEach(baseMapper::registerModule);\n-\n     final Injector injector = Guice.createInjector(\n         new ObjectMapperModule(),\n         baseModule\n     );\n-    return injector.getInstance(ObjectMapper.class);\n+    final ObjectMapper baseMapper = injector.getInstance(ObjectMapper.class);\n+\n+    baseModule.getJacksonModules().forEach(baseMapper::registerModule);\n+    return baseMapper;\n   }\n \n   private static class TestS3Module implements DruidModule\n   {\n     @Override\n     public List<? extends Module> getJacksonModules()\n     {\n-      return ImmutableList.of(new SimpleModule());\n+      // Deserializer is need for AmazonS3Client even though it is injected.\n+      // See https://github.com/FasterXML/jackson-databind/issues/962.\n+      return ImmutableList.of(new SimpleModule().addDeserializer(AmazonS3.class, new ItemDeserializer()));\n     }\n \n     @Override\n@@ -100,9 +105,28 @@ public void configure(Binder binder)\n     }\n \n     @Provides\n-    public RestS3Service getRestS3Service()\n+    public AmazonS3 getAmazonS3Client()\n     {\n       return SERVICE;\n     }\n   }\n+\n+  public static class ItemDeserializer extends StdDeserializer<AmazonS3>\n+  {\n+    public ItemDeserializer()\n+    {\n+      this(null);\n+    }\n+\n+    public ItemDeserializer(Class<?> vc)\n+    {\n+      super(vc);\n+    }\n+\n+    @Override\n+    public AmazonS3 deserialize(JsonParser jp, DeserializationContext ctxt)\n+    {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/firehose/s3/StaticS3FirehoseFactoryTest.java",
                "sha": "e602101269435a9a247f8761c82c8ad285233115",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentArchiverTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentArchiverTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentArchiverTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3Client;\n import com.fasterxml.jackson.databind.BeanProperty;\n import com.fasterxml.jackson.databind.DeserializationContext;\n import com.fasterxml.jackson.databind.InjectableValues;\n@@ -30,7 +31,6 @@\n import io.druid.java.util.common.Intervals;\n import io.druid.timeline.DataSegment;\n import org.easymock.EasyMock;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -65,7 +65,7 @@ public String getArchiveBaseKey()\n     }\n   };\n   private static final S3DataSegmentPusherConfig PUSHER_CONFIG = new S3DataSegmentPusherConfig();\n-  private static final RestS3Service S3_SERVICE = EasyMock.createStrictMock(RestS3Service.class);\n+  private static final AmazonS3Client S3_SERVICE = EasyMock.createStrictMock(AmazonS3Client.class);\n   private static final S3DataSegmentPuller PULLER = new S3DataSegmentPuller(S3_SERVICE);\n   private static final DataSegment SOURCE_SEGMENT = DataSegment\n       .builder()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentArchiverTest.java",
                "sha": "d93dfc08c553937a8c5444e403388dd70610f4d5",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentFinderTest.java",
                "changes": 181,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentFinderTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 84,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentFinderTest.java",
                "patch": "@@ -19,6 +19,15 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import com.amazonaws.services.s3.model.PutObjectResult;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.jsontype.NamedType;\n import com.google.common.base.Predicate;\n@@ -31,17 +40,13 @@\n import com.google.common.collect.Ordering;\n import com.google.common.collect.Sets;\n import io.druid.java.util.common.Intervals;\n+import io.druid.java.util.common.StringUtils;\n import io.druid.segment.TestHelper;\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n import io.druid.timeline.partition.NumberedShardSpec;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.io.IOUtils;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.StorageObjectsChunk;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n-import org.jets3t.service.model.StorageObject;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -50,8 +55,12 @@\n import org.junit.rules.TemporaryFolder;\n \n import javax.annotation.Nullable;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.InputStream;\n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -105,7 +114,7 @@\n   @Rule\n   public final TemporaryFolder temporaryFolder = new TemporaryFolder();\n \n-  RestS3Service mockS3Client;\n+  MockAmazonS3Client mockS3Client;\n   S3DataSegmentPusherConfig config;\n \n   private String bucket;\n@@ -122,8 +131,6 @@\n   private String indexZip4_0;\n   private String indexZip4_1;\n \n-\n-\n   @BeforeClass\n   public static void setUpStatic()\n   {\n@@ -140,7 +147,7 @@ public void setUp() throws Exception\n     config.setBucket(bucket);\n     config.setBaseKey(baseKey);\n \n-    mockS3Client = new MockStorageService(temporaryFolder.newFolder());\n+    mockS3Client = new MockAmazonS3Client(temporaryFolder.newFolder());\n \n \n     descriptor1 = S3Utils.descriptorPathForSegmentPath(baseKey + \"/interval1/v1/0/\");\n@@ -154,17 +161,17 @@ public void setUp() throws Exception\n     indexZip4_0 = S3Utils.indexZipForSegmentPath(descriptor4_0);\n     indexZip4_1 = S3Utils.indexZipForSegmentPath(descriptor4_1);\n \n-    mockS3Client.putObject(bucket, new S3Object(descriptor1, mapper.writeValueAsString(SEGMENT_1)));\n-    mockS3Client.putObject(bucket, new S3Object(descriptor2, mapper.writeValueAsString(SEGMENT_2)));\n-    mockS3Client.putObject(bucket, new S3Object(descriptor3, mapper.writeValueAsString(SEGMENT_3)));\n-    mockS3Client.putObject(bucket, new S3Object(descriptor4_0, mapper.writeValueAsString(SEGMENT_4_0)));\n-    mockS3Client.putObject(bucket, new S3Object(descriptor4_1, mapper.writeValueAsString(SEGMENT_4_1)));\n-\n-    mockS3Client.putObject(bucket, new S3Object(indexZip1, \"dummy\"));\n-    mockS3Client.putObject(bucket, new S3Object(indexZip2, \"dummy\"));\n-    mockS3Client.putObject(bucket, new S3Object(indexZip3, \"dummy\"));\n-    mockS3Client.putObject(bucket, new S3Object(indexZip4_0, \"dummy\"));\n-    mockS3Client.putObject(bucket, new S3Object(indexZip4_1, \"dummy\"));\n+    mockS3Client.putObject(bucket, descriptor1, mapper.writeValueAsString(SEGMENT_1));\n+    mockS3Client.putObject(bucket, descriptor2, mapper.writeValueAsString(SEGMENT_2));\n+    mockS3Client.putObject(bucket, descriptor3, mapper.writeValueAsString(SEGMENT_3));\n+    mockS3Client.putObject(bucket, descriptor4_0, mapper.writeValueAsString(SEGMENT_4_0));\n+    mockS3Client.putObject(bucket, descriptor4_1, mapper.writeValueAsString(SEGMENT_4_1));\n+\n+    mockS3Client.putObject(bucket, indexZip1, \"dummy\");\n+    mockS3Client.putObject(bucket, indexZip2, \"dummy\");\n+    mockS3Client.putObject(bucket, indexZip3, \"dummy\");\n+    mockS3Client.putObject(bucket, indexZip4_0, \"dummy\");\n+    mockS3Client.putObject(bucket, indexZip4_1, \"dummy\");\n   }\n \n   @Test\n@@ -210,34 +217,34 @@ public void testFindSegments() throws Exception\n     final String serializedSegment4_1 = mapper.writeValueAsString(updatedSegment4_1);\n \n     Assert.assertNotEquals(serializedSegment1,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor1).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor1).getObjectContent()));\n     Assert.assertNotEquals(serializedSegment2,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor2).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor2).getObjectContent()));\n     Assert.assertNotEquals(serializedSegment3,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor3).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor3).getObjectContent()));\n     Assert.assertNotEquals(serializedSegment4_0,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_0).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_0).getObjectContent()));\n     Assert.assertNotEquals(serializedSegment4_1,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_1).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_1).getObjectContent()));\n \n     final Set<DataSegment> segments2 = s3DataSegmentFinder.findSegments(\"\", true);\n \n     Assert.assertEquals(segments, segments2);\n \n     Assert.assertEquals(serializedSegment1,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor1).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor1).getObjectContent()));\n     Assert.assertEquals(serializedSegment2,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor2).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor2).getObjectContent()));\n     Assert.assertEquals(serializedSegment3,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor3).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor3).getObjectContent()));\n     Assert.assertEquals(serializedSegment4_0,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_0).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_0).getObjectContent()));\n     Assert.assertEquals(serializedSegment4_1,\n-                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_1).getDataInputStream()));\n+                           IOUtils.toString(mockS3Client.getObject(bucket, descriptor4_1).getObjectContent()));\n   }\n \n   @Test(expected = SegmentLoadingException.class)\n-  public void testFindSegmentsFail() throws SegmentLoadingException, ServiceException\n+  public void testFindSegmentsFail() throws SegmentLoadingException\n   {\n     mockS3Client.deleteObject(bucket, indexZip4_1);\n \n@@ -275,21 +282,8 @@ public void testFindSegmentsUpdateLoadSpec() throws Exception\n     final String descriptorPath = S3Utils.descriptorPathForSegmentPath(segmentPath);\n     final String indexPath = S3Utils.indexZipForSegmentPath(segmentPath);\n \n-    mockS3Client.putObject(\n-        config.getBucket(),\n-        new S3Object(\n-            descriptorPath,\n-            mapper.writeValueAsString(segmentMissingLoadSpec)\n-        )\n-    );\n-\n-    mockS3Client.putObject(\n-        config.getBucket(),\n-        new S3Object(\n-            indexPath,\n-            \"dummy\"\n-        )\n-    );\n+    mockS3Client.putObject(config.getBucket(), descriptorPath, mapper.writeValueAsString(segmentMissingLoadSpec));\n+    mockS3Client.putObject(config.getBucket(), indexPath, \"dummy\");\n \n     Set<DataSegment> segments = s3DataSegmentFinder.findSegments(segmentPath, false);\n     Assert.assertEquals(1, segments.size());\n@@ -308,24 +302,34 @@ private String getDescriptorPath(DataSegment segment)\n     return S3Utils.descriptorPathForSegmentPath(String.valueOf(segment.getLoadSpec().get(\"key\")));\n   }\n \n-  private static class MockStorageService extends RestS3Service\n+  private static class MockAmazonS3Client extends AmazonS3Client\n   {\n     private final File baseDir;\n     private final Map<String, Set<String>> storage = Maps.newHashMap();\n \n-    public MockStorageService(File baseDir)\n+    public MockAmazonS3Client(File baseDir)\n     {\n-      super(null);\n+      super();\n       this.baseDir = baseDir;\n     }\n \n     @Override\n-    public StorageObjectsChunk listObjectsChunked(\n-        final String bucketName, final String prefix, final String delimiter,\n-        final long maxListingLength, final String priorLastKey\n-    ) throws ServiceException\n+    public boolean doesObjectExist(String bucketName, String objectName)\n+    {\n+      final Set<String> keys = storage.get(bucketName);\n+      if (keys != null) {\n+        return keys.contains(objectName);\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public ListObjectsV2Result listObjectsV2(ListObjectsV2Request listObjectsV2Request)\n     {\n-      List<String> keysOrigin = Lists.newArrayList(storage.get(bucketName));\n+      final String bucketName = listObjectsV2Request.getBucketName();\n+      final String prefix = listObjectsV2Request.getPrefix();\n+\n+      final List<String> keysOrigin = Lists.newArrayList(storage.get(bucketName));\n \n       Predicate<String> prefixFilter = new Predicate<String>()\n       {\n@@ -341,11 +345,11 @@ public boolean apply(@Nullable String input)\n       );\n \n       int startOffset = 0;\n-      if (priorLastKey != null) {\n-        startOffset = keys.indexOf(priorLastKey) + 1;\n+      if (listObjectsV2Request.getContinuationToken() != null) {\n+        startOffset = keys.indexOf(listObjectsV2Request.getContinuationToken()) + 1;\n       }\n \n-      int endOffset = startOffset + (int) maxListingLength; // exclusive\n+      int endOffset = startOffset + listObjectsV2Request.getMaxKeys(); // exclusive\n       if (endOffset > keys.size()) {\n         endOffset = keys.size();\n       }\n@@ -355,72 +359,81 @@ public boolean apply(@Nullable String input)\n         newPriorLastkey = null;\n       }\n \n-      List<StorageObject> objects = Lists.newArrayList();\n+      List<S3ObjectSummary> objects = new ArrayList<>();\n       for (String objectKey : keys.subList(startOffset, endOffset)) {\n-        objects.add(getObjectDetails(bucketName, objectKey));\n+        final S3ObjectSummary objectSummary = new S3ObjectSummary();\n+        objectSummary.setBucketName(bucketName);\n+        objectSummary.setKey(objectKey);\n+        objects.add(objectSummary);\n       }\n \n-      return new StorageObjectsChunk(\n-          prefix, delimiter, objects.toArray(new StorageObject[]{}), null, newPriorLastkey);\n+      final ListObjectsV2Result result = new ListObjectsV2Result();\n+      result.setBucketName(bucketName);\n+      result.setKeyCount(objects.size());\n+      result.getObjectSummaries().addAll(objects);\n+      result.setContinuationToken(newPriorLastkey);\n+      result.setTruncated(newPriorLastkey != null);\n+\n+      return result;\n     }\n \n     @Override\n-    public StorageObject getObjectDetails(String bucketName, String objectKey) throws ServiceException\n+    public S3Object getObject(String bucketName, String objectKey)\n     {\n-\n       if (!storage.containsKey(bucketName)) {\n-        ServiceException ex = new ServiceException();\n-        ex.setResponseCode(404);\n+        AmazonServiceException ex = new AmazonS3Exception(\"S3DataSegmentFinderTest\");\n+        ex.setStatusCode(404);\n         ex.setErrorCode(\"NoSuchBucket\");\n         throw ex;\n       }\n \n       if (!storage.get(bucketName).contains(objectKey)) {\n-        ServiceException ex = new ServiceException();\n-        ex.setResponseCode(404);\n+        AmazonServiceException ex = new AmazonS3Exception(\"S3DataSegmentFinderTest\");\n+        ex.setStatusCode(404);\n         ex.setErrorCode(\"NoSuchKey\");\n         throw ex;\n       }\n \n       final File objectPath = new File(baseDir, objectKey);\n-      StorageObject storageObject = new StorageObject();\n+      S3Object storageObject = new S3Object();\n       storageObject.setBucketName(bucketName);\n       storageObject.setKey(objectKey);\n-      storageObject.setDataInputFile(objectPath);\n+      try {\n+        storageObject.setObjectContent(new FileInputStream(objectPath));\n+      }\n+      catch (FileNotFoundException e) {\n+        AmazonServiceException ex = new AmazonS3Exception(\"S3DataSegmentFinderTest\", e);\n+        ex.setStatusCode(500);\n+        ex.setErrorCode(\"InternalError\");\n+        throw ex;\n+      }\n \n       return storageObject;\n     }\n \n     @Override\n-    public S3Object getObject(String bucketName, String objectKey)\n+    public PutObjectResult putObject(String bucketName, String key, String data)\n     {\n-      final File objectPath = new File(baseDir, objectKey);\n-      S3Object s3Object = new S3Object();\n-      s3Object.setBucketName(bucketName);\n-      s3Object.setKey(objectKey);\n-      s3Object.setDataInputFile(objectPath);\n-\n-      return s3Object;\n-\n+      return putObject(bucketName, key, new ByteArrayInputStream(StringUtils.toUtf8(data)), null);\n     }\n \n     @Override\n-    public S3Object putObject(final String bucketName, final S3Object object)\n+    public PutObjectResult putObject(String bucketName, String key, InputStream input, ObjectMetadata metadata)\n     {\n       if (!storage.containsKey(bucketName)) {\n-        storage.put(bucketName, Sets.<String>newHashSet());\n+        storage.put(bucketName, Sets.newHashSet());\n       }\n-      storage.get(bucketName).add(object.getKey());\n+      storage.get(bucketName).add(key);\n \n-      final File objectPath = new File(baseDir, object.getKey());\n+      final File objectPath = new File(baseDir, key);\n \n       if (!objectPath.getParentFile().exists()) {\n         objectPath.getParentFile().mkdirs();\n       }\n \n       try {\n         try (\n-            InputStream in = object.getDataInputStream()\n+            InputStream in = input\n         ) {\n           FileUtils.copyInputStreamToFile(in, objectPath);\n         }\n@@ -429,7 +442,7 @@ public S3Object putObject(final String bucketName, final S3Object object)\n         throw Throwables.propagate(e);\n       }\n \n-      return object;\n+      return new PutObjectResult();\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentFinderTest.java",
                "sha": "5c449faf2e100a3ac33a149fff93373538637631",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentMoverTest.java",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentMoverTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 35,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentMoverTest.java",
                "patch": "@@ -19,6 +19,22 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.model.AccessControlList;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.CanonicalGrantee;\n+import com.amazonaws.services.s3.model.CopyObjectRequest;\n+import com.amazonaws.services.s3.model.CopyObjectResult;\n+import com.amazonaws.services.s3.model.GetObjectMetadataRequest;\n+import com.amazonaws.services.s3.model.Grant;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import com.amazonaws.services.s3.model.Owner;\n+import com.amazonaws.services.s3.model.Permission;\n+import com.amazonaws.services.s3.model.PutObjectResult;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n+import com.amazonaws.services.s3.model.StorageClass;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n@@ -28,12 +44,11 @@\n import io.druid.segment.loading.SegmentLoadingException;\n import io.druid.timeline.DataSegment;\n import io.druid.timeline.partition.NoneShardSpec;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n-import org.jets3t.service.model.StorageObject;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.io.File;\n+import java.util.HashSet;\n import java.util.Map;\n import java.util.Set;\n \n@@ -59,11 +74,17 @@\n   @Test\n   public void testMove() throws Exception\n   {\n-    MockStorageService mockS3Client = new MockStorageService();\n+    MockAmazonS3Client mockS3Client = new MockAmazonS3Client();\n     S3DataSegmentMover mover = new S3DataSegmentMover(mockS3Client, new S3DataSegmentPusherConfig());\n \n-    mockS3Client.putObject(\"main\", new S3Object(\"baseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/index.zip\"));\n-    mockS3Client.putObject(\"main\", new S3Object(\"baseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/descriptor.json\"));\n+    mockS3Client.putObject(\n+        \"main\",\n+        \"baseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/index.zip\"\n+    );\n+    mockS3Client.putObject(\n+        \"main\",\n+        \"baseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/descriptor.json\"\n+    );\n \n     DataSegment movedSegment = mover.move(\n         sourceSegment,\n@@ -79,11 +100,17 @@ public void testMove() throws Exception\n   @Test\n   public void testMoveNoop() throws Exception\n   {\n-    MockStorageService mockS3Client = new MockStorageService();\n+    MockAmazonS3Client mockS3Client = new MockAmazonS3Client();\n     S3DataSegmentMover mover = new S3DataSegmentMover(mockS3Client, new S3DataSegmentPusherConfig());\n \n-    mockS3Client.putObject(\"archive\", new S3Object(\"targetBaseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/index.zip\"));\n-    mockS3Client.putObject(\"archive\", new S3Object(\"targetBaseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/descriptor.json\"));\n+    mockS3Client.putObject(\n+        \"archive\",\n+        \"targetBaseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/index.zip\"\n+    );\n+    mockS3Client.putObject(\n+        \"archive\",\n+        \"targetBaseKey/test/2013-01-01T00:00:00.000Z_2013-01-02T00:00:00.000Z/1/0/descriptor.json\"\n+    );\n \n     DataSegment movedSegment = mover.move(\n         sourceSegment,\n@@ -100,7 +127,7 @@ public void testMoveNoop() throws Exception\n   @Test(expected = SegmentLoadingException.class)\n   public void testMoveException() throws Exception\n   {\n-    MockStorageService mockS3Client = new MockStorageService();\n+    MockAmazonS3Client mockS3Client = new MockAmazonS3Client();\n     S3DataSegmentMover mover = new S3DataSegmentMover(mockS3Client, new S3DataSegmentPusherConfig());\n \n     mover.move(\n@@ -112,7 +139,7 @@ public void testMoveException() throws Exception\n   @Test\n   public void testIgnoresGoneButAlreadyMoved() throws Exception\n   {\n-    MockStorageService mockS3Client = new MockStorageService();\n+    MockAmazonS3Client mockS3Client = new MockAmazonS3Client();\n     S3DataSegmentMover mover = new S3DataSegmentMover(mockS3Client, new S3DataSegmentPusherConfig());\n     mover.move(new DataSegment(\n         \"test\",\n@@ -135,7 +162,7 @@ public void testIgnoresGoneButAlreadyMoved() throws Exception\n   @Test(expected = SegmentLoadingException.class)\n   public void testFailsToMoveMissing() throws Exception\n   {\n-    MockStorageService mockS3Client = new MockStorageService();\n+    MockAmazonS3Client mockS3Client = new MockAmazonS3Client();\n     S3DataSegmentMover mover = new S3DataSegmentMover(mockS3Client, new S3DataSegmentPusherConfig());\n     mover.move(new DataSegment(\n         \"test\",\n@@ -155,15 +182,15 @@ public void testFailsToMoveMissing() throws Exception\n     ), ImmutableMap.<String, Object>of(\"bucket\", \"DOES NOT EXIST\", \"baseKey\", \"baseKey2\"));\n   }\n \n-  private static class MockStorageService extends RestS3Service\n+  private static class MockAmazonS3Client extends AmazonS3Client\n   {\n     Map<String, Set<String>> storage = Maps.newHashMap();\n     boolean copied = false;\n     boolean deletedOld = false;\n \n-    private MockStorageService()\n+    private MockAmazonS3Client()\n     {\n-      super(null);\n+      super();\n     }\n \n     public boolean didMove()\n@@ -172,37 +199,68 @@ public boolean didMove()\n     }\n \n     @Override\n-    public boolean isObjectInBucket(String bucketName, String objectKey)\n+    public AccessControlList getBucketAcl(String bucketName)\n+    {\n+      final AccessControlList acl = new AccessControlList();\n+      acl.setOwner(new Owner(\"ownerId\", \"owner\"));\n+      acl.grantAllPermissions(new Grant(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl));\n+      return acl;\n+    }\n+\n+    @Override\n+    public ObjectMetadata getObjectMetadata(GetObjectMetadataRequest getObjectMetadataRequest)\n+    {\n+      return new ObjectMetadata();\n+    }\n+\n+    @Override\n+    public boolean doesObjectExist(String bucketName, String objectKey)\n     {\n       Set<String> objects = storage.get(bucketName);\n       return (objects != null && objects.contains(objectKey));\n     }\n \n     @Override\n-    public S3Object[] listObjects(String bucketName, String objectKey, String separator)\n+    public ListObjectsV2Result listObjectsV2(ListObjectsV2Request listObjectsV2Request)\n     {\n-      if (isObjectInBucket(bucketName, objectKey)) {\n-        final S3Object object = new S3Object(objectKey);\n-        object.setStorageClass(S3Object.STORAGE_CLASS_STANDARD);\n-        return new S3Object[]{object};\n+      final String bucketName = listObjectsV2Request.getBucketName();\n+      final String objectKey = listObjectsV2Request.getPrefix();\n+      if (doesObjectExist(bucketName, objectKey)) {\n+        final S3ObjectSummary objectSummary = new S3ObjectSummary();\n+        objectSummary.setBucketName(bucketName);\n+        objectSummary.setKey(objectKey);\n+        objectSummary.setStorageClass(StorageClass.Standard.name());\n+\n+        final ListObjectsV2Result result = new ListObjectsV2Result();\n+        result.setBucketName(bucketName);\n+        result.setPrefix(objectKey);\n+        result.setKeyCount(1);\n+        result.getObjectSummaries().add(objectSummary);\n+        result.setTruncated(true);\n+        return result;\n+      } else {\n+        return new ListObjectsV2Result();\n       }\n-      return new S3Object[]{};\n     }\n \n     @Override\n-    public Map<String, Object> copyObject(\n-        String sourceBucketName,\n-        String sourceObjectKey,\n-        String destinationBucketName,\n-        StorageObject destinationObject,\n-        boolean replaceMetadata\n-    )\n+    public CopyObjectResult copyObject(CopyObjectRequest copyObjectRequest)\n     {\n+      final String sourceBucketName = copyObjectRequest.getSourceBucketName();\n+      final String sourceObjectKey = copyObjectRequest.getSourceKey();\n+      final String destinationBucketName = copyObjectRequest.getDestinationBucketName();\n+      final String destinationObjectKey = copyObjectRequest.getDestinationKey();\n       copied = true;\n-      if (isObjectInBucket(sourceBucketName, sourceObjectKey)) {\n-        this.putObject(destinationBucketName, new S3Object(destinationObject.getKey()));\n+      if (doesObjectExist(sourceBucketName, sourceObjectKey)) {\n+        storage.computeIfAbsent(destinationBucketName, k -> new HashSet<>())\n+               .add(destinationObjectKey);\n+        return new CopyObjectResult();\n+      } else {\n+        final AmazonS3Exception exception = new AmazonS3Exception(\"S3DataSegmentMoverTest\");\n+        exception.setErrorCode(\"NoSuchKey\");\n+        exception.setStatusCode(404);\n+        throw exception;\n       }\n-      return null;\n     }\n \n     @Override\n@@ -212,14 +270,19 @@ public void deleteObject(String bucket, String objectKey)\n       storage.get(bucket).remove(objectKey);\n     }\n \n+    public PutObjectResult putObject(String bucketName, String key)\n+    {\n+      return putObject(bucketName, key, (File) null);\n+    }\n+\n     @Override\n-    public S3Object putObject(String bucketName, S3Object object)\n+    public PutObjectResult putObject(String bucketName, String key, File file)\n     {\n       if (!storage.containsKey(bucketName)) {\n         storage.put(bucketName, Sets.newHashSet());\n       }\n-      storage.get(bucketName).add(object.getKey());\n-      return object;\n+      storage.get(bucketName).add(key);\n+      return new PutObjectResult();\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentMoverTest.java",
                "sha": "a848eb1808227efd5fe562420829313521efe2cc",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPullerTest.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPullerTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 39,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPullerTest.java",
                "patch": "@@ -19,9 +19,21 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import io.druid.java.util.common.FileUtils;\n import io.druid.java.util.common.StringUtils;\n import io.druid.segment.loading.SegmentLoadingException;\n+import org.easymock.EasyMock;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.FileOutputStream;\n@@ -30,15 +42,6 @@\n import java.net.URI;\n import java.util.Date;\n import java.util.zip.GZIPOutputStream;\n-import org.easymock.EasyMock;\n-import org.jets3t.service.S3ServiceException;\n-import org.jets3t.service.ServiceException;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n /**\n  *\n@@ -50,38 +53,41 @@\n   public TemporaryFolder temporaryFolder = new TemporaryFolder();\n \n   @Test\n-  public void testSimpleGetVersion() throws ServiceException, IOException\n+  public void testSimpleGetVersion() throws IOException\n   {\n     String bucket = \"bucket\";\n     String keyPrefix = \"prefix/dir/0\";\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    S3Object object0 = new S3Object();\n+    final S3ObjectSummary objectSummary = new S3ObjectSummary();\n+    objectSummary.setBucketName(bucket);\n+    objectSummary.setKey(keyPrefix + \"/renames-0.gz\");\n+    objectSummary.setLastModified(new Date(0));\n \n-    object0.setBucketName(bucket);\n-    object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n+    final ListObjectsV2Result result = new ListObjectsV2Result();\n+    result.setKeyCount(1);\n+    result.getObjectSummaries().add(objectSummary);\n \n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(bucket), EasyMock.eq(object0.getKey())))\n-            .andReturn(object0)\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(result)\n             .once();\n     S3DataSegmentPuller puller = new S3DataSegmentPuller(s3Client);\n \n     EasyMock.replay(s3Client);\n \n-    String version = puller.getVersion(URI.create(StringUtils.format(\"s3://%s/%s\", bucket, object0.getKey())));\n+    String version = puller.getVersion(URI.create(StringUtils.format(\"s3://%s/%s\", bucket, objectSummary.getKey())));\n \n     EasyMock.verify(s3Client);\n \n     Assert.assertEquals(StringUtils.format(\"%d\", new Date(0).getTime()), version);\n   }\n \n   @Test\n-  public void testGZUncompress() throws ServiceException, IOException, SegmentLoadingException\n+  public void testGZUncompress() throws IOException, SegmentLoadingException\n   {\n     final String bucket = \"bucket\";\n     final String keyPrefix = \"prefix/dir/0\";\n-    final RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    final AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n     final byte[] value = bucket.getBytes(\"utf8\");\n \n     final File tmpFile = temporaryFolder.newFile(\"gzTest.gz\");\n@@ -91,19 +97,27 @@ public void testGZUncompress() throws ServiceException, IOException, SegmentLoad\n     }\n \n     final S3Object object0 = new S3Object();\n-\n     object0.setBucketName(bucket);\n     object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n-    object0.setDataInputStream(new FileInputStream(tmpFile));\n+    object0.getObjectMetadata().setLastModified(new Date(0));\n+    object0.setObjectContent(new FileInputStream(tmpFile));\n+\n+    final S3ObjectSummary objectSummary = new S3ObjectSummary();\n+    objectSummary.setBucketName(bucket);\n+    objectSummary.setKey(keyPrefix + \"/renames-0.gz\");\n+    objectSummary.setLastModified(new Date(0));\n+\n+    final ListObjectsV2Result listObjectsResult = new ListObjectsV2Result();\n+    listObjectsResult.setKeyCount(1);\n+    listObjectsResult.getObjectSummaries().add(objectSummary);\n \n     final File tmpDir = temporaryFolder.newFolder(\"gzTestDir\");\n \n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n-            .andReturn(null)\n+    EasyMock.expect(s3Client.doesObjectExist(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n+            .andReturn(true)\n             .once();\n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n-            .andReturn(object0)\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(listObjectsResult)\n             .once();\n     EasyMock.expect(s3Client.getObject(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n             .andReturn(object0)\n@@ -126,11 +140,11 @@ public void testGZUncompress() throws ServiceException, IOException, SegmentLoad\n   }\n \n   @Test\n-  public void testGZUncompressRetries() throws ServiceException, IOException, SegmentLoadingException\n+  public void testGZUncompressRetries() throws IOException, SegmentLoadingException\n   {\n     final String bucket = \"bucket\";\n     final String keyPrefix = \"prefix/dir/0\";\n-    final RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    final AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n     final byte[] value = bucket.getBytes(\"utf8\");\n \n     final File tmpFile = temporaryFolder.newFile(\"gzTest.gz\");\n@@ -143,25 +157,34 @@ public void testGZUncompressRetries() throws ServiceException, IOException, Segm\n \n     object0.setBucketName(bucket);\n     object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n-    object0.setDataInputStream(new FileInputStream(tmpFile));\n+    object0.getObjectMetadata().setLastModified(new Date(0));\n+    object0.setObjectContent(new FileInputStream(tmpFile));\n+\n+    final S3ObjectSummary objectSummary = new S3ObjectSummary();\n+    objectSummary.setBucketName(bucket);\n+    objectSummary.setKey(keyPrefix + \"/renames-0.gz\");\n+    objectSummary.setLastModified(new Date(0));\n+\n+    final ListObjectsV2Result listObjectsResult = new ListObjectsV2Result();\n+    listObjectsResult.setKeyCount(1);\n+    listObjectsResult.getObjectSummaries().add(objectSummary);\n \n     File tmpDir = temporaryFolder.newFolder(\"gzTestDir\");\n \n-    S3ServiceException exception = new S3ServiceException();\n+    AmazonS3Exception exception = new AmazonS3Exception(\"S3DataSegmentPullerTest\");\n     exception.setErrorCode(\"NoSuchKey\");\n-    exception.setResponseCode(404);\n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n-            .andReturn(null)\n+    exception.setStatusCode(404);\n+    EasyMock.expect(s3Client.doesObjectExist(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n+            .andReturn(true)\n             .once();\n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n-            .andReturn(object0)\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(listObjectsResult)\n             .once();\n     EasyMock.expect(s3Client.getObject(EasyMock.eq(bucket), EasyMock.eq(object0.getKey())))\n             .andThrow(exception)\n             .once();\n-    EasyMock.expect(s3Client.getObjectDetails(EasyMock.eq(object0.getBucketName()), EasyMock.eq(object0.getKey())))\n-            .andReturn(object0)\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(listObjectsResult)\n             .once();\n     EasyMock.expect(s3Client.getObject(EasyMock.eq(bucket), EasyMock.eq(object0.getKey())))\n             .andReturn(object0)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPullerTest.java",
                "sha": "8bc028a64f0a73acfb768df46baf6a575491271d",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPusherTest.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPusherTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 11,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPusherTest.java",
                "patch": "@@ -19,6 +19,14 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.model.AccessControlList;\n+import com.amazonaws.services.s3.model.CanonicalGrantee;\n+import com.amazonaws.services.s3.model.Grant;\n+import com.amazonaws.services.s3.model.Owner;\n+import com.amazonaws.services.s3.model.Permission;\n+import com.amazonaws.services.s3.model.PutObjectRequest;\n+import com.amazonaws.services.s3.model.PutObjectResult;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n@@ -31,14 +39,13 @@\n import org.easymock.Capture;\n import org.easymock.EasyMock;\n import org.easymock.IAnswer;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n \n import java.io.File;\n+import java.io.FileInputStream;\n \n /**\n  */\n@@ -65,27 +72,38 @@ public void setValue(T value)\n   @Test\n   public void testPush() throws Exception\n   {\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    Capture<S3Object> capturedS3Object = Capture.newInstance();\n+    final AccessControlList acl = new AccessControlList();\n+    acl.setOwner(new Owner(\"ownerId\", \"owner\"));\n+    acl.grantAllPermissions(new Grant(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl));\n+    EasyMock.expect(s3Client.getBucketAcl(EasyMock.eq(\"bucket\"))).andReturn(acl).once();\n+\n+    EasyMock.expect(s3Client.putObject(EasyMock.anyObject()))\n+            .andReturn(new PutObjectResult())\n+            .once();\n+\n+    EasyMock.expect(s3Client.getBucketAcl(EasyMock.eq(\"bucket\"))).andReturn(acl).once();\n+\n+    Capture<PutObjectRequest> capturedPutRequest = Capture.newInstance();\n     ValueContainer<String> capturedS3SegmentJson = new ValueContainer<>();\n-    EasyMock.expect(s3Client.putObject(EasyMock.anyString(), EasyMock.capture(capturedS3Object)))\n+    EasyMock.expect(s3Client.putObject(EasyMock.capture(capturedPutRequest)))\n             .andAnswer(\n-                new IAnswer<S3Object>()\n+                new IAnswer<PutObjectResult>()\n                 {\n                   @Override\n-                  public S3Object answer() throws Throwable\n+                  public PutObjectResult answer() throws Throwable\n                   {\n                     capturedS3SegmentJson.setValue(\n-                        IOUtils.toString(capturedS3Object.getValue().getDataInputStream(), \"utf-8\")\n+                        IOUtils.toString(new FileInputStream(capturedPutRequest.getValue().getFile()), \"utf-8\")\n                     );\n-                    return null;\n+                    return new PutObjectResult();\n                   }\n                 }\n             )\n-            .atLeastOnce();\n-    EasyMock.replay(s3Client);\n+            .once();\n \n+    EasyMock.replay(s3Client);\n \n     S3DataSegmentPusherConfig config = new S3DataSegmentPusherConfig();\n     config.setBucket(\"bucket\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3DataSegmentPusherTest.java",
                "sha": "c787ed7d900cdaa8414db2e1b3ca2b2b4763f7ed",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3TimestampVersionedDataFinderTest.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3TimestampVersionedDataFinderTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 39,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3TimestampVersionedDataFinderTest.java",
                "patch": "@@ -19,11 +19,12 @@\n \n package io.druid.storage.s3;\n \n+import com.amazonaws.services.s3.AmazonS3Client;\n+import com.amazonaws.services.s3.model.ListObjectsV2Request;\n+import com.amazonaws.services.s3.model.ListObjectsV2Result;\n+import com.amazonaws.services.s3.model.S3ObjectSummary;\n import io.druid.java.util.common.StringUtils;\n import org.easymock.EasyMock;\n-import org.jets3t.service.S3ServiceException;\n-import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n-import org.jets3t.service.model.S3Object;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -35,25 +36,31 @@\n {\n \n   @Test\n-  public void testSimpleLatestVersion() throws S3ServiceException\n+  public void testSimpleLatestVersion()\n   {\n     String bucket = \"bucket\";\n     String keyPrefix = \"prefix/dir/0\";\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    S3Object object0 = new S3Object(), object1 = new S3Object();\n+    S3ObjectSummary object0 = new S3ObjectSummary(), object1 = new S3ObjectSummary();\n \n     object0.setBucketName(bucket);\n     object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n+    object0.setLastModified(new Date(0));\n \n     object1.setBucketName(bucket);\n     object1.setKey(keyPrefix + \"/renames-1.gz\");\n-    object1.setLastModifiedDate(new Date(1));\n+    object1.setLastModified(new Date(1));\n \n-    EasyMock.expect(s3Client.listObjects(EasyMock.eq(bucket), EasyMock.anyString(), EasyMock.<String>isNull())).andReturn(\n-        new S3Object[]{object0, object1}\n-    ).once();\n+    final ListObjectsV2Result result = new ListObjectsV2Result();\n+    result.getObjectSummaries().add(object0);\n+    result.getObjectSummaries().add(object1);\n+    result.setKeyCount(2);\n+    result.setTruncated(false);\n+\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(result)\n+            .once();\n     S3TimestampVersionedDataFinder finder = new S3TimestampVersionedDataFinder(s3Client);\n \n     Pattern pattern = Pattern.compile(\"renames-[0-9]*\\\\.gz\");\n@@ -71,25 +78,19 @@ public void testSimpleLatestVersion() throws S3ServiceException\n   }\n \n   @Test\n-  public void testMissing() throws S3ServiceException\n+  public void testMissing()\n   {\n     String bucket = \"bucket\";\n     String keyPrefix = \"prefix/dir/0\";\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    S3Object object0 = new S3Object(), object1 = new S3Object();\n+    final ListObjectsV2Result result = new ListObjectsV2Result();\n+    result.setKeyCount(0);\n+    result.setTruncated(false);\n \n-    object0.setBucketName(bucket);\n-    object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n-\n-    object1.setBucketName(bucket);\n-    object1.setKey(keyPrefix + \"/renames-1.gz\");\n-    object1.setLastModifiedDate(new Date(1));\n-\n-    EasyMock.expect(s3Client.listObjects(EasyMock.eq(bucket), EasyMock.anyString(), EasyMock.<String>isNull())).andReturn(\n-        null\n-    ).once();\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(result)\n+            .once();\n     S3TimestampVersionedDataFinder finder = new S3TimestampVersionedDataFinder(s3Client);\n \n     Pattern pattern = Pattern.compile(\"renames-[0-9]*\\\\.gz\");\n@@ -105,21 +106,26 @@ public void testMissing() throws S3ServiceException\n   }\n \n   @Test\n-  public void testFindSelf() throws S3ServiceException\n+  public void testFindSelf()\n   {\n     String bucket = \"bucket\";\n     String keyPrefix = \"prefix/dir/0\";\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    S3Object object0 = new S3Object();\n+    S3ObjectSummary object0 = new S3ObjectSummary();\n \n     object0.setBucketName(bucket);\n     object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n+    object0.setLastModified(new Date(0));\n \n-    EasyMock.expect(s3Client.listObjects(EasyMock.eq(bucket), EasyMock.anyString(), EasyMock.<String>isNull())).andReturn(\n-        new S3Object[]{object0}\n-    ).once();\n+    final ListObjectsV2Result result = new ListObjectsV2Result();\n+    result.getObjectSummaries().add(object0);\n+    result.setKeyCount(1);\n+    result.setTruncated(false);\n+\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(result)\n+            .once();\n     S3TimestampVersionedDataFinder finder = new S3TimestampVersionedDataFinder(s3Client);\n \n     Pattern pattern = Pattern.compile(\"renames-[0-9]*\\\\.gz\");\n@@ -137,21 +143,26 @@ public void testFindSelf() throws S3ServiceException\n   }\n \n   @Test\n-  public void testFindExact() throws S3ServiceException\n+  public void testFindExact()\n   {\n     String bucket = \"bucket\";\n     String keyPrefix = \"prefix/dir/0\";\n-    RestS3Service s3Client = EasyMock.createStrictMock(RestS3Service.class);\n+    AmazonS3Client s3Client = EasyMock.createStrictMock(AmazonS3Client.class);\n \n-    S3Object object0 = new S3Object();\n+    S3ObjectSummary object0 = new S3ObjectSummary();\n \n     object0.setBucketName(bucket);\n     object0.setKey(keyPrefix + \"/renames-0.gz\");\n-    object0.setLastModifiedDate(new Date(0));\n+    object0.setLastModified(new Date(0));\n+\n+    final ListObjectsV2Result result = new ListObjectsV2Result();\n+    result.getObjectSummaries().add(object0);\n+    result.setKeyCount(1);\n+    result.setTruncated(false);\n \n-    EasyMock.expect(s3Client.listObjects(EasyMock.eq(bucket), EasyMock.anyString(), EasyMock.<String>isNull())).andReturn(\n-        new S3Object[]{object0}\n-    ).once();\n+    EasyMock.expect(s3Client.listObjectsV2(EasyMock.anyObject(ListObjectsV2Request.class)))\n+            .andReturn(result)\n+            .once();\n     S3TimestampVersionedDataFinder finder = new S3TimestampVersionedDataFinder(s3Client);\n \n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/S3TimestampVersionedDataFinderTest.java",
                "sha": "ce19c9b10fc79b70e3139d4455fde3e0f9b88353",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/TestAWSCredentialsProvider.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/TestAWSCredentialsProvider.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/TestAWSCredentialsProvider.java",
                "patch": "@@ -24,6 +24,8 @@\n import com.amazonaws.auth.AWSSessionCredentials;\n import com.google.common.io.Files;\n import io.druid.common.aws.AWSCredentialsConfig;\n+import io.druid.common.aws.AWSEndpointConfig;\n+import io.druid.common.aws.AWSProxyConfig;\n import io.druid.guice.AWSModule;\n import io.druid.metadata.DefaultPasswordProvider;\n import org.easymock.EasyMock;\n@@ -58,7 +60,7 @@ public void testWithFixedAWSKeys()\n     assertEquals(credentials.getAWSSecretKey(), \"secretKeySample\");\n \n     // try to create\n-    s3Module.getRestS3Service(provider);\n+    s3Module.getAmazonS3Client(provider, new AWSProxyConfig(), new AWSEndpointConfig());\n   }\n \n   @Rule\n@@ -86,6 +88,6 @@ public void testWithFileSessionCredentials() throws IOException\n     assertEquals(sessionCredentials.getSessionToken(), \"sessionTokenSample\");\n \n     // try to create\n-    s3Module.getRestS3Service(provider);\n+    s3Module.getAmazonS3Client(provider, new AWSProxyConfig(), new AWSEndpointConfig());\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/extensions-core/s3-extensions/src/test/java/io/druid/storage/s3/TestAWSCredentialsProvider.java",
                "sha": "a7716e2a5ed8c85f7863778497c30c02768df5ed",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-hadoop/pom.xml",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 17,
                "filename": "indexing-hadoop/pom.xml",
                "patch": "@@ -56,23 +56,6 @@\n             <groupId>com.google.guava</groupId>\n             <artifactId>guava</artifactId>\n         </dependency>\n-        <!-- override jets3t from hadoop-core -->\n-        <dependency>\n-            <groupId>net.java.dev.jets3t</groupId>\n-            <artifactId>jets3t</artifactId>\n-            <scope>test</scope>\n-        </dependency>\n-        <!-- override httpclient / httpcore version from jets3t -->\n-        <dependency>\n-            <groupId>org.apache.httpcomponents</groupId>\n-            <artifactId>httpclient</artifactId>\n-            <scope>test</scope>\n-        </dependency>\n-        <dependency>\n-            <groupId>org.apache.httpcomponents</groupId>\n-            <artifactId>httpcore</artifactId>\n-            <scope>test</scope>\n-        </dependency>\n         <dependency>\n             <groupId>org.apache.hadoop</groupId>\n             <artifactId>hadoop-client</artifactId>\n@@ -100,6 +83,21 @@\n         </dependency>\n \n         <!-- Tests -->\n+        <dependency>\n+            <groupId>com.amazonaws</groupId>\n+            <artifactId>aws-java-sdk-bundle</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.httpcomponents</groupId>\n+            <artifactId>httpclient</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.httpcomponents</groupId>\n+            <artifactId>httpcore</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n         <dependency>\n             <groupId>junit</groupId>\n             <artifactId>junit</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-hadoop/pom.xml",
                "sha": "3a9c7740a03306ec4e50fc11b0160b4de939fbee",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "patch": "@@ -395,7 +395,7 @@ public void run(Context context)\n     public int getPartition(LongWritable interval, BytesWritable text, int numPartitions)\n     {\n \n-      if (config.get(\"mapred.job.tracker\").equals(\"local\") || determineIntervals) {\n+      if (\"local\".equals(config.get(\"mapred.job.tracker\")) || determineIntervals) {\n         return 0;\n       } else {\n         return reducerLookup.get(interval);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "sha": "44e75805d77d0d89ff16563800e64a278bad449a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/main/java/io/druid/indexing/common/config/TaskConfig.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/config/TaskConfig.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/config/TaskConfig.java",
                "patch": "@@ -31,7 +31,7 @@\n public class TaskConfig\n {\n   public static final List<String> DEFAULT_DEFAULT_HADOOP_COORDINATES = ImmutableList.of(\n-      \"org.apache.hadoop:hadoop-client:2.7.3\"\n+      \"org.apache.hadoop:hadoop-client:2.8.3\"\n   );\n \n   private static final Period DEFAULT_DIRECTORY_LOCK_TIMEOUT = new Period(\"PT10M\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/main/java/io/druid/indexing/common/config/TaskConfig.java",
                "sha": "9152c873210915527f8e2453d3b56b45ea135a10",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/main/java/io/druid/indexing/common/task/HadoopTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/task/HadoopTask.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/HadoopTask.java",
                "patch": "@@ -35,7 +35,6 @@\n import java.io.File;\n import java.lang.reflect.InvocationTargetException;\n import java.lang.reflect.Method;\n-import java.net.MalformedURLException;\n import java.net.URISyntaxException;\n import java.net.URL;\n import java.net.URLClassLoader;\n@@ -127,7 +126,6 @@ public boolean apply(@Nullable URL input)\n    *\n    * @param toolbox The toolbox to pull the default coordinates from if not present in the task\n    * @return An isolated URLClassLoader not tied by parent chain to the ApplicationClassLoader\n-   * @throws MalformedURLException from Initialization.getClassLoaderForExtension\n    */\n   protected ClassLoader buildClassLoader(final TaskToolbox toolbox)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/main/java/io/druid/indexing/common/task/HadoopTask.java",
                "sha": "8963559e3123edff92f3666585767cdd6dc04de6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/EC2AutoScalerSerdeTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/EC2AutoScalerSerdeTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 3,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/EC2AutoScalerSerdeTest.java",
                "patch": "@@ -74,12 +74,12 @@ public Object findInjectableValue(\n         }\n     );\n \n-    final EC2AutoScaler autoScaler = objectMapper.readValue(json, EC2AutoScaler.class);\n+    final EC2AutoScaler autoScaler = (EC2AutoScaler) objectMapper.readValue(json, AutoScaler.class);\n     verifyAutoScaler(autoScaler);\n \n-    final EC2AutoScaler roundTripAutoScaler = objectMapper.readValue(\n+    final EC2AutoScaler roundTripAutoScaler = (EC2AutoScaler) objectMapper.readValue(\n         objectMapper.writeValueAsBytes(autoScaler),\n-        EC2AutoScaler.class\n+        AutoScaler.class\n     );\n     verifyAutoScaler(roundTripAutoScaler);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/EC2AutoScalerSerdeTest.java",
                "sha": "18a46365edb68b55f7c5c8b174e84bb9bbd13866",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/test/java/io/druid/indexing/overlord/setup/JavaScriptWorkerSelectStrategyTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/setup/JavaScriptWorkerSelectStrategyTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/setup/JavaScriptWorkerSelectStrategyTest.java",
                "patch": "@@ -86,7 +86,7 @@ public void testSerde() throws Exception\n         STRATEGY,\n         mapper.readValue(\n             mapper.writeValueAsString(STRATEGY),\n-            JavaScriptWorkerSelectStrategy.class\n+            WorkerSelectStrategy.class\n         )\n     );\n   }\n@@ -108,7 +108,7 @@ public void testDisabled() throws Exception\n     expectedException.expectCause(CoreMatchers.<Throwable>instanceOf(IllegalStateException.class));\n     expectedException.expectMessage(\"JavaScript is disabled\");\n \n-    mapper.readValue(strategyString, JavaScriptWorkerSelectStrategy.class);\n+    mapper.readValue(strategyString, WorkerSelectStrategy.class);\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/indexing-service/src/test/java/io/druid/indexing/overlord/setup/JavaScriptWorkerSelectStrategyTest.java",
                "sha": "0569d07f2b11386f4b03332c79dcf32b56890f4a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/pom.xml",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 85,
                "filename": "pom.xml",
                "patch": "@@ -68,8 +68,8 @@\n         <guice.version>4.1.0</guice.version>\n         <jetty.version>9.3.19.v20170502</jetty.version>\n         <jersey.version>1.19.3</jersey.version>\n-        <!-- Watch out for Hadoop compatibility when updating to >= 2.5; see https://github.com/druid-io/druid/pull/1669 -->\n-        <jackson.version>2.4.6</jackson.version>\n+        <!-- jackson 2.7.x causes injection error and 2.8.x can't be used because avatica is using 2.6.3 -->\n+        <jackson.version>2.6.7</jackson.version>\n         <log4j.version>2.5</log4j.version>\n         <!-- HttpClient has not yet been ported to Netty 4.x -->\n         <netty3.version>3.10.6.Final</netty3.version>\n@@ -78,12 +78,10 @@\n         <netty4.version>4.0.52.Final</netty4.version>\n         <slf4j.version>1.7.12</slf4j.version>\n         <!-- If compiling with different hadoop version also modify default hadoop coordinates in TaskConfig.java -->\n-        <hadoop.compile.version>2.7.3</hadoop.compile.version>\n+        <hadoop.compile.version>2.8.3</hadoop.compile.version>\n         <hive.version>2.0.0</hive.version>\n         <powermock.version>1.6.6</powermock.version>\n-        <!-- Cannot update to AWS SDK 1.11+ because of Jackson incompatibility.\n-        Need to update Druid to use Jackson 2.6+ -->\n-        <aws.sdk.version>1.10.77</aws.sdk.version>\n+        <aws.sdk.bundle.version>1.11.199</aws.sdk.bundle.version>\n         <caffeine.version>2.5.5</caffeine.version>\n         <!-- When upgrading ZK, edit docs and integration tests as well (integration-tests/docker-base/setup.sh) -->\n         <zookeeper.version>3.4.11</zookeeper.version>\n@@ -189,49 +187,8 @@\n             </dependency>\n             <dependency>\n                 <groupId>com.amazonaws</groupId>\n-                <artifactId>aws-java-sdk-ec2</artifactId>\n-                <version>${aws.sdk.version}</version>\n-                <exclusions>\n-                    <exclusion>\n-                        <groupId>javax.mail</groupId>\n-                        <artifactId>mail</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>com.fasterxml.jackson.core</groupId>\n-                        <artifactId>jackson-databind</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>com.fasterxml.jackson.core</groupId>\n-                        <artifactId>jackson-annotations</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>commons-codec</groupId>\n-                        <artifactId>commons-codec</artifactId>\n-                    </exclusion>\n-                </exclusions>\n-            </dependency>\n-            <dependency>\n-                <groupId>com.amazonaws</groupId>\n-                <artifactId>aws-java-sdk-s3</artifactId>\n-                <version>${aws.sdk.version}</version>\n-                <exclusions>\n-                    <exclusion>\n-                        <groupId>javax.mail</groupId>\n-                        <artifactId>mail</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>com.fasterxml.jackson.core</groupId>\n-                        <artifactId>jackson-databind</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>com.fasterxml.jackson.core</groupId>\n-                        <artifactId>jackson-annotations</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>commons-codec</groupId>\n-                        <artifactId>commons-codec</artifactId>\n-                    </exclusion>\n-                </exclusions>\n+                <artifactId>aws-java-sdk-bundle</artifactId>\n+                <version>${aws.sdk.bundle.version}</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.ning</groupId>\n@@ -612,49 +569,15 @@\n                 <artifactId>aether-api</artifactId>\n                 <version>0.9.0.M2</version>\n             </dependency>\n-            <dependency>\n-                <groupId>net.java.dev.jets3t</groupId>\n-                <artifactId>jets3t</artifactId>\n-                <version>0.9.4</version>\n-                <exclusions>\n-                    <exclusion>\n-                        <groupId>commons-codec</groupId>\n-                        <artifactId>commons-codec</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>commons-logging</groupId>\n-                        <artifactId>commons-logging</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>org.apache.httpcomponents</groupId>\n-                        <artifactId>httpclient</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>org.apache.httpcomponents</groupId>\n-                        <artifactId>httpcore</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>org.codehaus.jackson</groupId>\n-                        <artifactId>jackson-core-asl</artifactId>\n-                    </exclusion>\n-                    <exclusion>\n-                        <groupId>org.codehaus.jackson</groupId>\n-                        <artifactId>jackson-mapper-asl</artifactId>\n-                    </exclusion>\n-                </exclusions>\n-            </dependency>\n-            <!-- The htttpcomponents artifacts have non-matching release cadence -->\n-            <!-- Watch out for compatibility issues between aws-java-sdk 1.10.x and httpclient > 4.5.1 and\n-                 httpcore > 4.4.3; see https://github.com/druid-io/druid/issues/4456 -->\n             <dependency>\n                 <groupId>org.apache.httpcomponents</groupId>\n                 <artifactId>httpclient</artifactId>\n-                <version>4.5.1</version>\n+                <version>4.5.3</version>\n             </dependency>\n             <dependency>\n                 <groupId>org.apache.httpcomponents</groupId>\n                 <artifactId>httpcore</artifactId>\n-                <version>4.4.3</version>\n+                <version>4.4.4</version>\n             </dependency>\n             <dependency>\n                 <groupId>org.apache.hadoop</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/pom.xml",
                "sha": "870fe78d04671c0fc5c219ec29fc324364aa1620",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/groupby/orderby/DefaultLimitSpecTest.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/groupby/orderby/DefaultLimitSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 15,
                "filename": "processing/src/test/java/io/druid/query/groupby/orderby/DefaultLimitSpecTest.java",
                "patch": "@@ -74,9 +74,9 @@ public void testSerde() throws Exception\n     //defaults\n     String json = \"{\\\"type\\\": \\\"default\\\"}\";\n \n-    DefaultLimitSpec spec = mapper.readValue(\n-        mapper.writeValueAsString(mapper.readValue(json, DefaultLimitSpec.class)),\n-        DefaultLimitSpec.class\n+    DefaultLimitSpec spec = (DefaultLimitSpec) mapper.readValue(\n+        mapper.writeValueAsString(mapper.readValue(json, LimitSpec.class)),\n+        LimitSpec.class\n     );\n \n     Assert.assertEquals(\n@@ -90,9 +90,9 @@ public void testSerde() throws Exception\n            + \"  \\\"columns\\\":[{\\\"dimension\\\":\\\"d\\\",\\\"direction\\\":\\\"DESCENDING\\\", \\\"dimensionOrder\\\":\\\"numeric\\\"}],\\n\"\n            + \"  \\\"limit\\\":10\\n\"\n            + \"}\";\n-    spec = mapper.readValue(\n-        mapper.writeValueAsString(mapper.readValue(json, DefaultLimitSpec.class)),\n-        DefaultLimitSpec.class\n+    spec = (DefaultLimitSpec) mapper.readValue(\n+        mapper.writeValueAsString(mapper.readValue(json, LimitSpec.class)),\n+        LimitSpec.class\n     );\n     Assert.assertEquals(\n         new DefaultLimitSpec(ImmutableList.of(new OrderByColumnSpec(\"d\", OrderByColumnSpec.Direction.DESCENDING,\n@@ -106,9 +106,9 @@ public void testSerde() throws Exception\n            + \"  \\\"limit\\\":10\\n\"\n            + \"}\";\n \n-    spec = mapper.readValue(\n-        mapper.writeValueAsString(mapper.readValue(json, DefaultLimitSpec.class)),\n-        DefaultLimitSpec.class\n+    spec = (DefaultLimitSpec) mapper.readValue(\n+        mapper.writeValueAsString(mapper.readValue(json, LimitSpec.class)),\n+        LimitSpec.class\n     );\n \n     Assert.assertEquals(\n@@ -122,9 +122,9 @@ public void testSerde() throws Exception\n            + \"  \\\"columns\\\":[{\\\"dimension\\\":\\\"d\\\"}],\\n\"\n            + \"  \\\"limit\\\":10\\n\"\n            + \"}\";\n-    spec = mapper.readValue(\n-        mapper.writeValueAsString(mapper.readValue(json, DefaultLimitSpec.class)),\n-        DefaultLimitSpec.class\n+    spec = (DefaultLimitSpec) mapper.readValue(\n+        mapper.writeValueAsString(mapper.readValue(json, LimitSpec.class)),\n+        LimitSpec.class\n     );\n     Assert.assertEquals(\n         new DefaultLimitSpec(ImmutableList.of(new OrderByColumnSpec(\"d\", OrderByColumnSpec.Direction.ASCENDING,\n@@ -137,9 +137,9 @@ public void testSerde() throws Exception\n            + \"  \\\"columns\\\":[\\\"d\\\"],\\n\"\n            + \"  \\\"limit\\\":10\\n\"\n            + \"}\";\n-    spec = mapper.readValue(\n-        mapper.writeValueAsString(mapper.readValue(json, DefaultLimitSpec.class)),\n-        DefaultLimitSpec.class\n+    spec = (DefaultLimitSpec) mapper.readValue(\n+        mapper.writeValueAsString(mapper.readValue(json, LimitSpec.class)),\n+        LimitSpec.class\n     );\n     Assert.assertEquals(\n         new DefaultLimitSpec(ImmutableList.of(new OrderByColumnSpec(\"d\", OrderByColumnSpec.Direction.ASCENDING,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/groupby/orderby/DefaultLimitSpecTest.java",
                "sha": "3da0488d6790ceb61eba82634ebf4b612536a8e6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "patch": "@@ -104,8 +104,8 @@ public void testSerdeAlphaNumericTopNMetricSpec() throws IOException\n                        + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n                        + \"}\";\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n-    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n-    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), TopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), TopNMetricSpec.class);\n     Assert.assertEquals(expectedMetricSpec, actualMetricSpec);\n     Assert.assertEquals(expectedMetricSpec1, actualMetricSpec1);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "sha": "36562c3388aa472812210f7278a5da4f8f9cbc90",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/topn/DimensionTopNMetricSpecTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/DimensionTopNMetricSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 8,
                "filename": "processing/src/test/java/io/druid/query/topn/DimensionTopNMetricSpecTest.java",
                "patch": "@@ -44,8 +44,8 @@ public void testSerdeAlphaNumericDimensionTopNMetricSpec() throws IOException\n                        + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n                        + \"}\";\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n-    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n-    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), TopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), TopNMetricSpec.class);\n     Assert.assertEquals(expectedMetricSpec, actualMetricSpec);\n     Assert.assertEquals(expectedMetricSpec1, actualMetricSpec1);\n   }\n@@ -65,8 +65,8 @@ public void testSerdeLexicographicDimensionTopNMetricSpec() throws IOException\n                        + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n                        + \"}\";\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n-    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n-    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), TopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), TopNMetricSpec.class);\n     Assert.assertEquals(expectedMetricSpec, actualMetricSpec);\n     Assert.assertEquals(expectedMetricSpec1, actualMetricSpec1);\n   }\n@@ -86,8 +86,8 @@ public void testSerdeStrlenDimensionTopNMetricSpec() throws IOException\n                        + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n                        + \"}\";\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n-    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n-    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), TopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), TopNMetricSpec.class);\n     Assert.assertEquals(expectedMetricSpec, actualMetricSpec);\n     Assert.assertEquals(expectedMetricSpec1, actualMetricSpec1);\n   }\n@@ -107,8 +107,8 @@ public void testSerdeNumericDimensionTopNMetricSpec() throws IOException\n                        + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n                        + \"}\";\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n-    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n-    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), DimensionTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), TopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), TopNMetricSpec.class);\n     Assert.assertEquals(expectedMetricSpec, actualMetricSpec);\n     Assert.assertEquals(expectedMetricSpec1, actualMetricSpec1);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/processing/src/test/java/io/druid/query/topn/DimensionTopNMetricSpecTest.java",
                "sha": "7091ef5ee93aab3e3ef7594353a4b1436a6c7f0b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/pom.xml?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 0,
                "filename": "server/pom.xml",
                "patch": "@@ -70,6 +70,10 @@\n             <groupId>org.apache.zookeeper</groupId>\n             <artifactId>zookeeper</artifactId>\n         </dependency>\n+        <dependency>\n+            <groupId>org.apache.httpcomponents</groupId>\n+            <artifactId>httpclient</artifactId>\n+        </dependency>\n         <dependency>\n             <groupId>org.apache.curator</groupId>\n             <artifactId>curator-framework</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/pom.xml",
                "sha": "4173650b1b0ba601ba0294a7f5b10807212d0c66",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/guice/AWSModule.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/guice/AWSModule.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/guice/AWSModule.java",
                "patch": "@@ -27,6 +27,8 @@\n import com.google.inject.Provides;\n import io.druid.common.aws.AWSCredentialsConfig;\n import io.druid.common.aws.AWSCredentialsUtils;\n+import io.druid.common.aws.AWSEndpointConfig;\n+import io.druid.common.aws.AWSProxyConfig;\n \n /**\n  */\n@@ -36,6 +38,8 @@\n   public void configure(Binder binder)\n   {\n     JsonConfigProvider.bind(binder, \"druid.s3\", AWSCredentialsConfig.class);\n+    JsonConfigProvider.bind(binder, \"druid.s3.proxy\", AWSProxyConfig.class);\n+    JsonConfigProvider.bind(binder, \"druid.s3.endpoint\", AWSEndpointConfig.class);\n   }\n \n   @Provides",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/guice/AWSModule.java",
                "sha": "4127f3cb2000a47b4cad9f910d3fbe39b2367fc6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/segment/realtime/firehose/HttpFirehoseFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/firehose/HttpFirehoseFactory.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/realtime/firehose/HttpFirehoseFactory.java",
                "patch": "@@ -23,11 +23,11 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Predicate;\n+import com.google.common.net.HttpHeaders;\n import io.druid.data.input.impl.prefetch.PrefetchableTextFilesFirehoseFactory;\n import io.druid.java.util.common.CompressionUtils;\n import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.logger.Logger;\n-import org.apache.http.HttpHeaders;\n \n import java.io.IOException;\n import java.io.InputStream;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/segment/realtime/firehose/HttpFirehoseFactory.java",
                "sha": "aaab6f9dae553fc4e21d28e3ee2784b944012dee",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 3,
                "filename": "server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java",
                "patch": "@@ -58,6 +58,7 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n import javax.ws.rs.core.MediaType;\n+import javax.ws.rs.core.Response.Status;\n import java.io.IOException;\n import java.net.URI;\n import java.net.URISyntaxException;\n@@ -465,13 +466,11 @@ public void onComplete(Result result)\n                         TimeUnit.NANOSECONDS.toMillis(requestTimeNs),\n                         \"success\",\n                         success\n-                        && result.getResponse().getStatus() == javax.ws.rs.core.Response.Status.OK.getStatusCode()\n+                        && result.getResponse().getStatus() == Status.OK.getStatusCode()\n                     )\n                 )\n             )\n         );\n-\n-\n       }\n       catch (Exception e) {\n         log.error(e, \"Unable to log query [%s]!\", query);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/server/AsyncQueryForwardingServlet.java",
                "sha": "d10880fe83e5b5365b2236bea4074ce190d12d1f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/server/initialization/jetty/JettyServerModule.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/initialization/jetty/JettyServerModule.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 2,
                "filename": "server/src/main/java/io/druid/server/initialization/jetty/JettyServerModule.java",
                "patch": "@@ -60,7 +60,6 @@\n import io.druid.server.metrics.DataSourceTaskIdHolder;\n import io.druid.server.metrics.MetricsModule;\n import io.druid.server.metrics.MonitorsConfig;\n-import org.apache.http.HttpVersion;\n import org.eclipse.jetty.server.ConnectionFactory;\n import org.eclipse.jetty.server.Handler;\n import org.eclipse.jetty.server.HttpConfiguration;\n@@ -90,6 +89,7 @@\n   private static final Logger log = new Logger(JettyServerModule.class);\n \n   private static final AtomicInteger activeConnections = new AtomicInteger();\n+  private static final String HTTP_1_1_STRING = \"HTTP/1.1\";\n \n   @Override\n   protected void configureServlets()\n@@ -268,7 +268,7 @@ static Server makeAndInitializeServer(\n       httpsConfiguration.setRequestHeaderSize(config.getMaxRequestHeaderSize());\n       final ServerConnector connector = new ServerConnector(\n           server,\n-          new SslConnectionFactory(sslContextFactory, HttpVersion.HTTP_1_1.toString()),\n+          new SslConnectionFactory(sslContextFactory, HTTP_1_1_STRING),\n           new HttpConnectionFactory(httpsConfiguration)\n       );\n       connector.setPort(node.getTlsPort());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/src/main/java/io/druid/server/initialization/jetty/JettyServerModule.java",
                "sha": "1b1508b555846124c543af063ba4778fdd0e5b88",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1ad898bde29820a8ff41267457a4370395406ae5/server/src/test/java/io/druid/query/dimension/LookupDimensionSpecTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/query/dimension/LookupDimensionSpecTest.java?ref=1ad898bde29820a8ff41267457a4370395406ae5",
                "deletions": 1,
                "filename": "server/src/test/java/io/druid/query/dimension/LookupDimensionSpecTest.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.fasterxml.jackson.databind.InjectableValues;\n import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n import com.google.common.base.Strings;\n import com.google.common.collect.ImmutableMap;\n import io.druid.jackson.DefaultObjectMapper;\n@@ -70,12 +71,13 @@\n   public void testSerDesr(DimensionSpec lookupDimSpec) throws IOException\n   {\n     ObjectMapper mapper = new DefaultObjectMapper();\n+    mapper.registerSubtypes(new NamedType(LookupDimensionSpec.class, \"lookup\"));\n     InjectableValues injectableValues = new InjectableValues.Std().addValue(\n         LookupReferencesManager.class,\n         LOOKUP_REF_MANAGER\n     );\n     String serLookup = mapper.writeValueAsString(lookupDimSpec);\n-    Assert.assertEquals(lookupDimSpec, mapper.reader(LookupDimensionSpec.class).with(injectableValues).readValue(serLookup));\n+    Assert.assertEquals(lookupDimSpec, mapper.reader(DimensionSpec.class).with(injectableValues).readValue(serLookup));\n   }\n \n   private Object[] parametersForTestSerDesr()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1ad898bde29820a8ff41267457a4370395406ae5/server/src/test/java/io/druid/query/dimension/LookupDimensionSpecTest.java",
                "sha": "02c44c2972534979134c6f6ffaa6fcab42f0cd9e",
                "status": "modified"
            }
        ],
        "message": "Use the official aws-sdk instead of jet3t (#5382)\n\n* Use the official aws-sdk instead of jet3t\r\n\r\n* fix compile and serde tests\r\n\r\n* address comments and fix test\r\n\r\n* add http version string\r\n\r\n* remove redundant dependencies, fix potential NPE, and fix test\r\n\r\n* resolve TODOs\r\n\r\n* fix build\r\n\r\n* downgrade jackson version to 2.6.7\r\n\r\n* fix test\r\n\r\n* resolve the last TODO\r\n\r\n* support proxy and endpoint configurations\r\n\r\n* fix build\r\n\r\n* remove debugging log\r\n\r\n* downgrade hadoop version to 2.8.3\r\n\r\n* fix tests\r\n\r\n* remove unused log\r\n\r\n* fix it test\r\n\r\n* revert KerberosAuthenticator change\r\n\r\n* change hadoop-aws scope to provided in hdfs-storage\r\n\r\n* address comments\r\n\r\n* address comments",
        "parent": "https://github.com/apache/incubator-druid/commit/885b975c9574e3cfc4aae1cfccfd749b4dbb6e35",
        "repo": "incubator-druid",
        "unit_tests": [
            "StaticS3FirehoseFactoryTest.java",
            "S3DataSegmentArchiverTest.java",
            "S3DataSegmentMoverTest.java",
            "S3DataSegmentPullerTest.java",
            "S3DataSegmentPusherTest.java",
            "S3TaskLogsTest.java",
            "S3TimestampVersionedDataFinderTest.java",
            "DetermineHashedPartitionsJobTest.java",
            "HadoopTaskTest.java",
            "HttpFirehoseFactoryTest.java",
            "AsyncQueryForwardingServletTest.java"
        ]
    },
    "incubator-druid_1d67e39": {
        "bug_id": "incubator-druid_1d67e39",
        "commit": "https://github.com/apache/incubator-druid/commit/1d67e399492e90beebadab76df947492f3a49e97",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 2,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -762,8 +762,6 @@ public ShardSpec apply(DimPartition dimPartition)\n         log.info(\"  %s\", HadoopDruidIndexerConfig.jsonMapper.writeValueAsString(shardSpec));\n       }\n \n-      System.out.println(HadoopDruidIndexerConfig.jsonMapper.writeValueAsString(chosenShardSpecs));\n-\n       try {\n         HadoopDruidIndexerConfig.jsonMapper\n                                 .writerWithType(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "sha": "7a6856505283ce008a912b4ed0138e9fd8cce478",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/UnionDataSource.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/UnionDataSource.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/UnionDataSource.java",
                "patch": "@@ -43,6 +43,9 @@\n   public UnionDataSource(@JsonProperty(\"dataSources\") List<DataSource> dataSources)\n   {\n     Preconditions.checkNotNull(dataSources, \"datasources cannot be null for uniondatasource\");\n+    for(DataSource ds : dataSources){\n+      Preconditions.checkArgument(ds instanceof TableDataSource, \"Union DataSource only supports TableDatasource\");\n+    }\n     this.dataSources = dataSources;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/UnionDataSource.java",
                "sha": "83b9f0acbe7a0db81367ac9b7abf36527df6e9b2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/select/PagingSpec.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "patch": "@@ -40,7 +40,7 @@ public PagingSpec(\n       @JsonProperty(\"threshold\") int threshold\n   )\n   {\n-    this.pagingIdentifiers = pagingIdentifiers;\n+    this.pagingIdentifiers = pagingIdentifiers == null ? new LinkedHashMap<String, Integer>() : pagingIdentifiers;\n     this.threshold = threshold;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/select/PagingSpec.java",
                "sha": "0ad5b56635dc9831757d3418fbbf1ec3f4c2a142",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQuery.java?ref=1d67e399492e90beebadab76df947492f3a49e97",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "patch": "@@ -167,7 +167,7 @@ public TopNQuery withQuerySegmentSpec(QuerySegmentSpec querySegmentSpec)\n   public Query<Result<TopNResultValue>> withDataSource(DataSource dataSource)\n   {\n     return new TopNQuery(\n-        getDataSource(),\n+        dataSource,\n         dimensionSpec,\n         topNMetricSpec,\n         threshold,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/1d67e399492e90beebadab76df947492f3a49e97/processing/src/main/java/io/druid/query/topn/TopNQuery.java",
                "sha": "b456e487039b63835d74a6b9f6359eeae12df387",
                "status": "modified"
            }
        ],
        "message": "Fixes\n\nfix NPE in select query, calculating cache key\nremove unwanted logging\nfix ds in topNQuery",
        "parent": "https://github.com/apache/incubator-druid/commit/728a606d32ba31939e6a4d20b47ea72b2d2de49f",
        "repo": "incubator-druid",
        "unit_tests": [
            "DeterminePartitionsJobTest.java",
            "TopNQueryTest.java"
        ]
    },
    "incubator-druid_2183c7a": {
        "bug_id": "incubator-druid_2183c7a",
        "commit": "https://github.com/apache/incubator-druid/commit/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
        "file": [
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/aggregation/AggregatorUtil.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/AggregatorUtil.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/aggregation/AggregatorUtil.java",
                "patch": "@@ -20,6 +20,8 @@\n package io.druid.query.aggregation;\n \n import com.google.common.collect.Lists;\n+import com.metamx.common.ISE;\n+import com.metamx.common.Pair;\n \n import java.util.HashSet;\n import java.util.LinkedList;\n@@ -53,4 +55,31 @@\n     return rv;\n   }\n \n+  public static Pair<List<AggregatorFactory>, List<PostAggregator>> condensedAggregators(\n+      List<AggregatorFactory> aggList,\n+      List<PostAggregator> postAggList,\n+      String metric\n+  )\n+  {\n+\n+    List<PostAggregator> condensedPostAggs = AggregatorUtil.pruneDependentPostAgg(\n+        postAggList,\n+        metric\n+    );\n+    // calculate dependent aggregators for these postAgg\n+    Set<String> dependencySet = new HashSet<>();\n+    dependencySet.add(metric);\n+    for (PostAggregator postAggregator : condensedPostAggs) {\n+      dependencySet.addAll(postAggregator.getDependentFields());\n+    }\n+\n+    List<AggregatorFactory> condensedAggs = Lists.newArrayList();\n+    for (AggregatorFactory aggregatorSpec : aggList) {\n+      if (dependencySet.contains(aggregatorSpec.getName())) {\n+        condensedAggs.add(aggregatorSpec);\n+      }\n+    }\n+    return new Pair(condensedAggs, condensedPostAggs);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/aggregation/AggregatorUtil.java",
                "sha": "809a903cddd741c8df2c8e911f9b6accb00fefde",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 54,
                "filename": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "patch": "@@ -19,10 +19,11 @@\n \n package io.druid.query.topn;\n \n-import com.google.common.collect.Lists;\n import com.metamx.common.ISE;\n+import com.metamx.common.Pair;\n import io.druid.collections.StupidPool;\n import io.druid.query.aggregation.AggregatorFactory;\n+import io.druid.query.aggregation.AggregatorUtil;\n import io.druid.query.aggregation.PostAggregator;\n import io.druid.segment.Capabilities;\n import io.druid.segment.Cursor;\n@@ -64,70 +65,27 @@ public TopNParams makeInitParams(\n     return new TopNParams(dimSelector, cursor, dimSelector.getValueCardinality(), Integer.MAX_VALUE);\n   }\n \n-  @Override\n-  public TopNResultBuilder makeResultBuilder(TopNParams params)\n-  {\n-    return query.getTopNMetricSpec().getResultBuilder(\n-        params.getCursor().getTime(),\n-        query.getDimensionSpec(),\n-        query.getThreshold(),\n-        comparator,\n-        query.getAggregatorSpecs(),\n-        query.getPostAggregatorSpecs()\n-    );\n-  }\n-\n   @Override\n   public void run(\n       TopNParams params, TopNResultBuilder resultBuilder, int[] ints\n   )\n   {\n-    final TopNResultBuilder singleMetricResultBuilder = makeResultBuilder(params);\n-    final String metric;\n-    // ugly\n-    TopNMetricSpec spec = query.getTopNMetricSpec();\n-    if (spec instanceof InvertedTopNMetricSpec\n-        && ((InvertedTopNMetricSpec) spec).getDelegate() instanceof NumericTopNMetricSpec) {\n-      metric = ((NumericTopNMetricSpec) ((InvertedTopNMetricSpec) spec).getDelegate()).getMetric();\n-    } else if (spec instanceof NumericTopNMetricSpec) {\n-      metric = ((NumericTopNMetricSpec) query.getTopNMetricSpec()).getMetric();\n-    } else {\n-      throw new ISE(\"WTF?! We are in AggregateTopNMetricFirstAlgorithm with a [%s] spec\", spec.getClass().getName());\n-    }\n-\n-    // Find either the aggregator or post aggregator to do the topN over\n-    List<AggregatorFactory> condensedAggs = Lists.newArrayList();\n-    for (AggregatorFactory aggregatorSpec : query.getAggregatorSpecs()) {\n-      if (aggregatorSpec.getName().equalsIgnoreCase(metric)) {\n-        condensedAggs.add(aggregatorSpec);\n-        break;\n-      }\n-    }\n-    List<PostAggregator> condensedPostAggs = Lists.newArrayList();\n-    if (condensedAggs.isEmpty()) {\n-      for (PostAggregator postAggregator : query.getPostAggregatorSpecs()) {\n-        if (postAggregator.getName().equalsIgnoreCase(metric)) {\n-          condensedPostAggs.add(postAggregator);\n+    final String metric = query.getTopNMetricSpec().getMetricName(query.getDimensionSpec());\n+    Pair<List<AggregatorFactory>, List<PostAggregator>> condensedAggPostAggPair = AggregatorUtil.condensedAggregators(\n+        query.getAggregatorSpecs(),\n+        query.getPostAggregatorSpecs(),\n+        metric\n+    );\n \n-          // Add all dependent metrics\n-          for (AggregatorFactory aggregatorSpec : query.getAggregatorSpecs()) {\n-            if (postAggregator.getDependentFields().contains(aggregatorSpec.getName())) {\n-              condensedAggs.add(aggregatorSpec);\n-            }\n-          }\n-          break;\n-        }\n-      }\n-    }\n-    if (condensedAggs.isEmpty() && condensedPostAggs.isEmpty()) {\n+    if (condensedAggPostAggPair.lhs.isEmpty() && condensedAggPostAggPair.rhs.isEmpty()) {\n       throw new ISE(\"WTF! Can't find the metric to do topN over?\");\n     }\n-\n     // Run topN for only a single metric\n     TopNQuery singleMetricQuery = new TopNQueryBuilder().copy(query)\n-                                                        .aggregators(condensedAggs)\n-                                                        .postAggregators(condensedPostAggs)\n+                                                        .aggregators(condensedAggPostAggPair.lhs)\n+                                                        .postAggregators(condensedAggPostAggPair.rhs)\n                                                         .build();\n+    final TopNResultBuilder singleMetricResultBuilder = BaseTopNAlgorithm.makeResultBuilder(params, singleMetricQuery);\n \n     PooledTopNAlgorithm singleMetricAlgo = new PooledTopNAlgorithm(capabilities, singleMetricQuery, bufferPool);\n     PooledTopNAlgorithm.PooledTopNParams singleMetricParam = null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "sha": "254d13d581bcecb586ac65e923b9a03527287a8d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java",
                "patch": "@@ -28,6 +28,7 @@\n import io.druid.segment.DimensionSelector;\n \n import java.util.Arrays;\n+import java.util.Comparator;\n import java.util.List;\n \n /**\n@@ -230,4 +231,18 @@ public void keepOnlyN(int n)\n       return Pair.of(startIndex, endIndex);\n     }\n   }\n+\n+  public static TopNResultBuilder makeResultBuilder(TopNParams params, TopNQuery query)\n+  {\n+    Comparator comparator = query.getTopNMetricSpec()\n+                                 .getComparator(query.getAggregatorSpecs(), query.getPostAggregatorSpecs());\n+    return query.getTopNMetricSpec().getResultBuilder(\n+        params.getCursor().getTime(),\n+        query.getDimensionSpec(),\n+        query.getThreshold(),\n+        comparator,\n+        query.getAggregatorSpecs(),\n+        query.getPostAggregatorSpecs()\n+    );\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/BaseTopNAlgorithm.java",
                "sha": "9d8baceb2c98574f803b76520284eafd12d07769",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 13,
                "filename": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "patch": "@@ -56,19 +56,6 @@ public TopNParams makeInitParams(\n     return new TopNParams(dimSelector, cursor, dimSelector.getValueCardinality(), Integer.MAX_VALUE);\n   }\n \n-  @Override\n-  public TopNResultBuilder makeResultBuilder(TopNParams params)\n-  {\n-    return query.getTopNMetricSpec().getResultBuilder(\n-        params.getCursor().getTime(),\n-        query.getDimensionSpec(),\n-        query.getThreshold(),\n-        comparator,\n-        query.getAggregatorSpecs(),\n-        query.getPostAggregatorSpecs()\n-    );\n-  }\n-\n   @Override\n   protected Aggregator[][] makeDimValSelector(TopNParams params, int numProcessed, int numToProcess)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "sha": "09535a84678ad370afba4abb9e5a8e2555b4db8d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 12,
                "filename": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "patch": "@@ -114,18 +114,7 @@ public PooledTopNParams makeInitParams(\n                            .build();\n   }\n \n-  @Override\n-  public TopNResultBuilder makeResultBuilder(PooledTopNParams params)\n-  {\n-    return query.getTopNMetricSpec().getResultBuilder(\n-        params.getCursor().getTime(),\n-        query.getDimensionSpec(),\n-        query.getThreshold(),\n-        comparator,\n-        query.getAggregatorSpecs(),\n-        query.getPostAggregatorSpecs()\n-    );\n-  }\n+\n \n   @Override\n   protected int[] makeDimValSelector(PooledTopNParams params, int numProcessed, int numToProcess)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "sha": "a8e3f324467315f29fc2fa5987630700278dcd18",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "patch": "@@ -33,8 +33,6 @@\n \n   public TopNParams makeInitParams(DimensionSelector dimSelector, Cursor cursor);\n \n-  public TopNResultBuilder makeResultBuilder(Parameters params);\n-\n   public void run(\n       Parameters params,\n       TopNResultBuilder resultBuilder,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "sha": "3d5c90e2b207c6254e279e45e96dd7d5cc6976d5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNMapFn.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "patch": "@@ -24,6 +24,8 @@\n import io.druid.segment.Cursor;\n import io.druid.segment.DimensionSelector;\n \n+import java.util.Comparator;\n+\n public class TopNMapFn implements Function<Cursor, Result<TopNResultValue>>\n {\n   private final TopNQuery query;\n@@ -52,7 +54,7 @@ public TopNMapFn(\n     try {\n       params = topNAlgorithm.makeInitParams(dimSelector, cursor);\n \n-      TopNResultBuilder resultBuilder = topNAlgorithm.makeResultBuilder(params);\n+      TopNResultBuilder resultBuilder = BaseTopNAlgorithm.makeResultBuilder(params, query);\n \n       topNAlgorithm.run(params, resultBuilder, null);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "sha": "5479eeb72f8fc15c8557c11c0a142487c6a82106",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 1,
                "filename": "processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "patch": "@@ -81,7 +81,8 @@\n       \"+\",\n       Lists.newArrayList(\n           constant,\n-          new FieldAccessPostAggregator(addRowsIndexConstantMetric, addRowsIndexConstantMetric)\n+          new FieldAccessPostAggregator(addRowsIndexConstantMetric, addRowsIndexConstantMetric),\n+          new FieldAccessPostAggregator(\"rows\", \"rows\")\n       )\n   );\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "sha": "fe0e5af8c66e1ac80aaad6e359d5cb6fc675aca6",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/aggregation/AggregatorUtilTest.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/aggregation/AggregatorUtilTest.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/aggregation/AggregatorUtilTest.java",
                "patch": "@@ -19,15 +19,22 @@\n \n package io.druid.query.aggregation;\n \n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n+import com.metamx.common.Pair;\n+import io.druid.query.QueryRunnerTestHelper;\n import io.druid.query.aggregation.post.ArithmeticPostAggregator;\n import io.druid.query.aggregation.post.ConstantPostAggregator;\n import io.druid.query.aggregation.post.FieldAccessPostAggregator;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n \n+import static io.druid.query.QueryRunnerTestHelper.dependentPostAggMetric;\n+\n public class AggregatorUtilTest\n {\n \n@@ -101,4 +108,41 @@ public void testOutOfOrderPruneDependentPostAgg()\n     Assert.assertEquals(Lists.newArrayList(dependency1, aggregator), prunedAgg);\n   }\n \n+  @Test\n+  public void testCondenseAggregators()\n+  {\n+\n+    ArrayList<AggregatorFactory> aggregatorFactories = Lists.<AggregatorFactory>newArrayList(\n+        Iterables.concat(\n+            QueryRunnerTestHelper.commonAggregators,\n+            Lists.newArrayList(\n+                new MaxAggregatorFactory(\"maxIndex\", \"index\"),\n+                new MinAggregatorFactory(\"minIndex\", \"index\")\n+            )\n+        )\n+    );\n+\n+    List<PostAggregator> postAggregatorList = Arrays.<PostAggregator>asList(\n+        QueryRunnerTestHelper.addRowsIndexConstant,\n+        QueryRunnerTestHelper.dependentPostAgg\n+    );\n+    Pair<List<AggregatorFactory>, List<PostAggregator>> aggregatorsPair = AggregatorUtil.condensedAggregators(\n+        aggregatorFactories,\n+        postAggregatorList,\n+        dependentPostAggMetric\n+    );\n+    // verify aggregators\n+    Assert.assertEquals(\n+        Lists.newArrayList(QueryRunnerTestHelper.rowsCount, QueryRunnerTestHelper.indexDoubleSum),\n+        aggregatorsPair.lhs\n+    );\n+    Assert.assertEquals(\n+        Lists.newArrayList(\n+            QueryRunnerTestHelper.addRowsIndexConstant,\n+            QueryRunnerTestHelper.dependentPostAgg\n+        ), aggregatorsPair.rhs\n+    );\n+\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/aggregation/AggregatorUtilTest.java",
                "sha": "90e1f449a967a0f5c27325f9f65c1b5ce4ad3f39",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java?ref=2183c7ab08f37f2d4df93d5efd0edd750f32a9c0",
                "deletions": 3,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "patch": "@@ -1215,7 +1215,7 @@ public void testTopNDependentPostAgg() {\n                                 .put(\"rows\", 186L)\n                                 .put(\"index\", 215679.82879638672D)\n                                 .put(\"addRowsIndexConstant\", 215866.82879638672D)\n-                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 215867.82879638672D)\n+                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 216053.82879638672D)\n                                 .put(\"uniques\", QueryRunnerTestHelper.UNIQUES_2)\n                                 .put(\"maxIndex\", 1743.9217529296875D)\n                                 .put(\"minIndex\", 792.3260498046875D)\n@@ -1225,7 +1225,7 @@ public void testTopNDependentPostAgg() {\n                                 .put(\"rows\", 186L)\n                                 .put(\"index\", 192046.1060180664D)\n                                 .put(\"addRowsIndexConstant\", 192233.1060180664D)\n-                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 192234.1060180664D)\n+                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 192420.1060180664D)\n                                 .put(\"uniques\", QueryRunnerTestHelper.UNIQUES_2)\n                                 .put(\"maxIndex\", 1870.06103515625D)\n                                 .put(\"minIndex\", 545.9906005859375D)\n@@ -1235,7 +1235,7 @@ public void testTopNDependentPostAgg() {\n                                 .put(\"rows\", 837L)\n                                 .put(\"index\", 95606.57232284546D)\n                                 .put(\"addRowsIndexConstant\", 96444.57232284546D)\n-                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 96445.57232284546D)\n+                                .put(QueryRunnerTestHelper.dependentPostAggMetric, 97282.57232284546D)\n                                 .put(\"uniques\", QueryRunnerTestHelper.UNIQUES_9)\n                                 .put(\"maxIndex\", 277.2735290527344D)\n                                 .put(\"minIndex\", 59.02102279663086D)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2183c7ab08f37f2d4df93d5efd0edd750f32a9c0/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "sha": "e5a540f1e3e6c70b5a311443a872ea039e72cace",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #483 from metamx/fix-aggregatefirst-topn-algo\n\nfix NPE in aggregatrFirstTopnAlgo",
        "parent": "https://github.com/apache/incubator-druid/commit/a87213e15fa7a9616e31980242f8e57533d6c52c",
        "repo": "incubator-druid",
        "unit_tests": [
            "AggregatorUtilTest.java",
            "PooledTopNAlgorithmTest.java"
        ]
    },
    "incubator-druid_23133f3": {
        "bug_id": "incubator-druid_23133f3",
        "commit": "https://github.com/apache/incubator-druid/commit/23133f3b5b064856a11ec4c4b130f562467e0170",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/23133f3b5b064856a11ec4c4b130f562467e0170/processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/jackson/JacksonModule.java?ref=23133f3b5b064856a11ec4c4b130f562467e0170",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "patch": "@@ -49,7 +49,7 @@ public ObjectMapper jsonMapper()\n   public ObjectMapper smileMapper()\n   {\n     ObjectMapper retVal = new DefaultObjectMapper(new SmileFactory());\n-    retVal.getJsonFactory().setCodec(retVal);\n+    retVal.getFactory().setCodec(retVal);\n     return retVal;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/23133f3b5b064856a11ec4c4b130f562467e0170/processing/src/main/java/io/druid/jackson/JacksonModule.java",
                "sha": "16d19cf95441e3c36b54473929cb40850c4b67bc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/23133f3b5b064856a11ec4c4b130f562467e0170/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/DirectDruidClient.java?ref=23133f3b5b064856a11ec4c4b130f562467e0170",
                "deletions": 3,
                "filename": "server/src/main/java/io/druid/client/DirectDruidClient.java",
                "patch": "@@ -89,7 +89,7 @@ public DirectDruidClient(\n     this.httpClient = httpClient;\n     this.host = host;\n \n-    this.isSmile = this.objectMapper.getJsonFactory() instanceof SmileFactory;\n+    this.isSmile = this.objectMapper.getFactory() instanceof SmileFactory;\n     this.openConnections = new AtomicInteger();\n   }\n \n@@ -269,7 +269,7 @@ private void init()\n     {\n       if (jp == null) {\n         try {\n-          jp = objectMapper.getJsonFactory().createJsonParser(future.get());\n+          jp = objectMapper.getFactory().createParser(future.get());\n           if (jp.nextToken() != JsonToken.START_ARRAY) {\n             throw new IAE(\"Next token wasn't a START_ARRAY, was[%s]\", jp.getCurrentToken());\n           } else {\n@@ -292,7 +292,9 @@ private void init()\n     @Override\n     public void close() throws IOException\n     {\n-      jp.close();\n+      if(jp != null) {\n+        jp.close();\n+      }\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/23133f3b5b064856a11ec4c4b130f562467e0170/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "sha": "8befef5cabb3f0fe4ebc0994ea442e7677c40660",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #305 from metamx/npe-deprecated-fix\n\nRemove deprecated jackson calls and fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/00df13af0634fa08a369a5d4c60328dc353b880d",
        "repo": "incubator-druid",
        "unit_tests": [
            "DirectDruidClientTest.java"
        ]
    },
    "incubator-druid_2bfe1b6": {
        "bug_id": "incubator-druid_2bfe1b6",
        "commit": "https://github.com/apache/incubator-druid/commit/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 1,
                "filename": "extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "patch": "@@ -53,6 +53,7 @@\n import org.joda.time.Duration;\n import org.joda.time.Interval;\n \n+import javax.annotation.Nullable;\n import java.io.IOException;\n import java.util.List;\n import java.util.Map;\n@@ -239,7 +240,12 @@ public void reset(DataSourceMetadata dataSourceMetadata)\n   }\n \n   @Override\n-  public void checkpoint(int taskGroupId, DataSourceMetadata previousCheckPoint, DataSourceMetadata currentCheckPoint)\n+  public void checkpoint(\n+      @Nullable Integer taskGroupId,\n+      String baseSequenceName,\n+      DataSourceMetadata previousCheckPoint,\n+      DataSourceMetadata currentCheckPoint\n+  )\n   {\n     // do nothing\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "sha": "d499b4f0e8d2cc6e30bed2c0caee7c787858e0b2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 0,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "patch": "@@ -603,6 +603,7 @@ public void onFailure(Throwable t)\n             final CheckPointDataSourceMetadataAction checkpointAction = new CheckPointDataSourceMetadataAction(\n                 task.getDataSource(),\n                 ioConfig.getTaskGroupId(),\n+                task.getIOConfig().getBaseSequenceName(),\n                 new KafkaDataSourceMetadata(new KafkaPartitions(topic, sequenceToCheckpoint.getStartOffsets())),\n                 new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n             );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "sha": "6d424163a279f3854b293caa7155d738f0037cee",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 40,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -92,6 +92,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Map.Entry;\n import java.util.Properties;\n import java.util.Random;\n import java.util.Set;\n@@ -143,7 +144,7 @@\n    * time, there should only be up to a maximum of [taskCount] actively-reading task groups (tracked in the [taskGroups]\n    * map) + zero or more pending-completion task groups (tracked in [pendingCompletionTaskGroups]).\n    */\n-  private static class TaskGroup\n+  private class TaskGroup\n   {\n     // This specifies the partitions and starting offsets for this task group. It is set on group creation from the data\n     // in [partitionGroups] and never changes during the lifetime of this task group, which will live until a task in\n@@ -157,6 +158,7 @@\n     final Optional<DateTime> maximumMessageTime;\n     DateTime completionTimeout; // is set after signalTasksToFinish(); if not done by timeout, take corrective action\n     final TreeMap<Integer, Map<Integer, Long>> sequenceOffsets = new TreeMap<>();\n+    final String baseSequenceName;\n \n     TaskGroup(\n         ImmutableMap<Integer, Long> partitionOffsets,\n@@ -168,6 +170,7 @@\n       this.minimumMessageTime = minimumMessageTime;\n       this.maximumMessageTime = maximumMessageTime;\n       this.sequenceOffsets.put(0, partitionOffsets);\n+      this.baseSequenceName = generateSequenceName(partitionOffsets, minimumMessageTime, maximumMessageTime);\n     }\n \n     int addNewCheckpoint(Map<Integer, Long> checkpoint)\n@@ -509,23 +512,29 @@ public void reset(DataSourceMetadata dataSourceMetadata)\n   }\n \n   @Override\n-  public void checkpoint(int taskGroupId, DataSourceMetadata previousCheckpoint, DataSourceMetadata currentCheckpoint)\n+  public void checkpoint(\n+      @Nullable Integer taskGroupId,\n+      @Deprecated String baseSequenceName,\n+      DataSourceMetadata previousCheckPoint,\n+      DataSourceMetadata currentCheckPoint\n+  )\n   {\n-    Preconditions.checkNotNull(previousCheckpoint, \"previousCheckpoint\");\n-    Preconditions.checkNotNull(currentCheckpoint, \"current checkpoint cannot be null\");\n+    Preconditions.checkNotNull(previousCheckPoint, \"previousCheckpoint\");\n+    Preconditions.checkNotNull(currentCheckPoint, \"current checkpoint cannot be null\");\n     Preconditions.checkArgument(\n-        ioConfig.getTopic().equals(((KafkaDataSourceMetadata) currentCheckpoint).getKafkaPartitions().getTopic()),\n+        ioConfig.getTopic().equals(((KafkaDataSourceMetadata) currentCheckPoint).getKafkaPartitions().getTopic()),\n         \"Supervisor topic [%s] and topic in checkpoint [%s] does not match\",\n         ioConfig.getTopic(),\n-        ((KafkaDataSourceMetadata) currentCheckpoint).getKafkaPartitions().getTopic()\n+        ((KafkaDataSourceMetadata) currentCheckPoint).getKafkaPartitions().getTopic()\n     );\n \n-    log.info(\"Checkpointing [%s] for taskGroup [%s]\", currentCheckpoint, taskGroupId);\n+    log.info(\"Checkpointing [%s] for taskGroup [%s]\", currentCheckPoint, taskGroupId);\n     notices.add(\n         new CheckpointNotice(\n             taskGroupId,\n-            (KafkaDataSourceMetadata) previousCheckpoint,\n-            (KafkaDataSourceMetadata) currentCheckpoint\n+            baseSequenceName,\n+            (KafkaDataSourceMetadata) previousCheckPoint,\n+            (KafkaDataSourceMetadata) currentCheckPoint\n         )\n     );\n   }\n@@ -629,30 +638,65 @@ public void handle()\n \n   private class CheckpointNotice implements Notice\n   {\n-    final int taskGroupId;\n-    final KafkaDataSourceMetadata previousCheckpoint;\n-    final KafkaDataSourceMetadata currentCheckpoint;\n+    @Nullable private final Integer nullableTaskGroupId;\n+    @Deprecated private final String baseSequenceName;\n+    private final KafkaDataSourceMetadata previousCheckpoint;\n+    private final KafkaDataSourceMetadata currentCheckpoint;\n \n     CheckpointNotice(\n-        int taskGroupId,\n+        @Nullable Integer nullableTaskGroupId,\n+        @Deprecated String baseSequenceName,\n         KafkaDataSourceMetadata previousCheckpoint,\n         KafkaDataSourceMetadata currentCheckpoint\n     )\n     {\n-      this.taskGroupId = taskGroupId;\n+      this.baseSequenceName = baseSequenceName;\n+      this.nullableTaskGroupId = nullableTaskGroupId;\n       this.previousCheckpoint = previousCheckpoint;\n       this.currentCheckpoint = currentCheckpoint;\n     }\n \n     @Override\n     public void handle() throws ExecutionException, InterruptedException\n     {\n+      // Find taskGroupId using taskId if it's null. It can be null while rolling update.\n+      final int taskGroupId;\n+      if (nullableTaskGroupId == null) {\n+        // We search taskId in taskGroups and pendingCompletionTaskGroups sequentially. This should be fine because\n+        // 1) a taskGroup can be moved from taskGroups to pendingCompletionTaskGroups in RunNotice\n+        //    (see checkTaskDuration()).\n+        // 2) Notices are proceesed by a single thread. So, CheckpointNotice and RunNotice cannot be processed at the\n+        //    same time.\n+        final java.util.Optional<Integer> maybeGroupId = taskGroups\n+            .entrySet()\n+            .stream()\n+            .filter(entry -> {\n+              final TaskGroup taskGroup = entry.getValue();\n+              return taskGroup.baseSequenceName.equals(baseSequenceName);\n+            })\n+            .findAny()\n+            .map(Entry::getKey);\n+        taskGroupId = maybeGroupId.orElse(\n+            pendingCompletionTaskGroups\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                  final List<TaskGroup> taskGroups = entry.getValue();\n+                  return taskGroups.stream().anyMatch(group -> group.baseSequenceName.equals(baseSequenceName));\n+                })\n+                .findAny()\n+                .orElseThrow(() -> new ISE(\"Cannot find taskGroup for baseSequenceName[%s]\", baseSequenceName))\n+                .getKey()\n+        );\n+      } else {\n+        taskGroupId = nullableTaskGroupId;\n+      }\n+\n       // check for consistency\n       // if already received request for this sequenceName and dataSourceMetadata combination then return\n-\n       final TaskGroup taskGroup = taskGroups.get(taskGroupId);\n \n-      if (isValidTaskGroup(taskGroup)) {\n+      if (isValidTaskGroup(taskGroupId, taskGroup)) {\n         final TreeMap<Integer, Map<Integer, Long>> checkpoints = taskGroup.sequenceOffsets;\n \n         // check validity of previousCheckpoint\n@@ -674,20 +718,13 @@ public void handle() throws ExecutionException, InterruptedException\n           log.info(\"Already checkpointed with offsets [%s]\", checkpoints.lastEntry().getValue());\n           return;\n         }\n-        final int taskGroupId = getTaskGroupIdForPartition(\n-            currentCheckpoint.getKafkaPartitions()\n-                             .getPartitionOffsetMap()\n-                             .keySet()\n-                             .iterator()\n-                             .next()\n-        );\n         final Map<Integer, Long> newCheckpoint = checkpointTaskGroup(taskGroupId, false).get();\n         taskGroups.get(taskGroupId).addNewCheckpoint(newCheckpoint);\n         log.info(\"Handled checkpoint notice, new checkpoint is [%s] for taskGroup [%s]\", newCheckpoint, taskGroupId);\n       }\n     }\n \n-    private boolean isValidTaskGroup(@Nullable TaskGroup taskGroup)\n+    private boolean isValidTaskGroup(int taskGroupId, @Nullable TaskGroup taskGroup)\n     {\n       if (taskGroup == null) {\n         // taskGroup might be in pendingCompletionTaskGroups or partitionGroups\n@@ -886,17 +923,6 @@ String generateSequenceName(\n     return Joiner.on(\"_\").join(\"index_kafka\", dataSource, hashCode);\n   }\n \n-  @VisibleForTesting\n-  String generateSequenceName(TaskGroup taskGroup)\n-  {\n-    Preconditions.checkNotNull(taskGroup, \"taskGroup cannot be null\");\n-    return generateSequenceName(\n-        taskGroup.partitionOffsets,\n-        taskGroup.minimumMessageTime,\n-        taskGroup.maximumMessageTime\n-    );\n-  }\n-\n   private static String getRandomId()\n   {\n     final StringBuilder suffix = new StringBuilder(8);\n@@ -1774,15 +1800,14 @@ private void createKafkaTasksForGroup(int groupId, int replicas) throws JsonProc\n       endPartitions.put(partition, Long.MAX_VALUE);\n     }\n     TaskGroup group = taskGroups.get(groupId);\n-    String sequenceName = generateSequenceName(group);\n \n     Map<String, String> consumerProperties = Maps.newHashMap(ioConfig.getConsumerProperties());\n     DateTime minimumMessageTime = taskGroups.get(groupId).minimumMessageTime.orNull();\n     DateTime maximumMessageTime = taskGroups.get(groupId).maximumMessageTime.orNull();\n \n     KafkaIOConfig kafkaIOConfig = new KafkaIOConfig(\n         groupId,\n-        sequenceName,\n+        group.baseSequenceName,\n         new KafkaPartitions(ioConfig.getTopic(), startPartitions),\n         new KafkaPartitions(ioConfig.getTopic(), endPartitions),\n         consumerProperties,\n@@ -1803,10 +1828,10 @@ private void createKafkaTasksForGroup(int groupId, int replicas) throws JsonProc\n                                             .putAll(spec.getContext())\n                                             .build();\n     for (int i = 0; i < replicas; i++) {\n-      String taskId = Joiner.on(\"_\").join(sequenceName, getRandomId());\n+      String taskId = Joiner.on(\"_\").join(group.baseSequenceName, getRandomId());\n       KafkaIndexTask indexTask = new KafkaIndexTask(\n           taskId,\n-          new TaskResource(sequenceName, 1),\n+          new TaskResource(group.baseSequenceName, 1),\n           spec.getDataSchema(),\n           taskTuningConfig,\n           kafkaIOConfig,\n@@ -1936,7 +1961,10 @@ private boolean isTaskCurrent(int taskGroupId, String taskId)\n \n     String taskSequenceName = ((KafkaIndexTask) taskOptional.get()).getIOConfig().getBaseSequenceName();\n     if (taskGroups.get(taskGroupId) != null) {\n-      return generateSequenceName(taskGroups.get(taskGroupId)).equals(taskSequenceName);\n+      return Preconditions\n+          .checkNotNull(taskGroups.get(taskGroupId), \"null taskGroup for taskId[%s]\", taskGroupId)\n+          .baseSequenceName\n+          .equals(taskSequenceName);\n     } else {\n       return generateSequenceName(\n           ((KafkaIndexTask) taskOptional.get()).getIOConfig()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "8c9bb599ada44c572e42ea02232e8f4afb9d8ac1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "patch": "@@ -2060,7 +2060,8 @@ private void makeToolboxFactory() throws IOException\n           @Override\n           public boolean checkPointDataSourceMetadata(\n               String supervisorId,\n-              int taskGroupId,\n+              @Nullable Integer taskGroupId,\n+              String baseSequenceName,\n               @Nullable DataSourceMetadata previousDataSourceMetadata,\n               @Nullable DataSourceMetadata currentDataSourceMetadata\n           )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "sha": "c3cf1153671b54afde1cb1b44110221da7d2dc45",
                "status": "modified"
            },
            {
                "additions": 90,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "patch": "@@ -2104,6 +2104,7 @@ public void testCheckpointForInactiveTaskGroup()\n     supervisor.moveTaskGroupToPendingCompletion(0);\n     supervisor.checkpoint(\n         0,\n+        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n         new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n         new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n     );\n@@ -2173,6 +2174,7 @@ public void testCheckpointForUnknownTaskGroup() throws InterruptedException\n \n     supervisor.checkpoint(\n         0,\n+        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n         new KafkaDataSourceMetadata(new KafkaPartitions(topic, Collections.emptyMap())),\n         new KafkaDataSourceMetadata(new KafkaPartitions(topic, Collections.emptyMap()))\n     );\n@@ -2195,13 +2197,100 @@ public void testCheckpointForUnknownTaskGroup() throws InterruptedException\n     Assert.assertEquals(ISE.class, serviceEmitter.getExceptionClass());\n   }\n \n+  @Test(timeout = 60_000L)\n+  public void testCheckpointWithNullTaskGroupId()\n+      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n+  {\n+    supervisor = getSupervisor(1, 3, true, \"PT1S\", null, null, false);\n+    //not adding any events\n+    final Task id1 = createKafkaIndexTask(\n+        \"id1\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id2 = createKafkaIndexTask(\n+        \"id2\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id3 = createKafkaIndexTask(\n+        \"id3\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n+    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n+    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n+    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n+    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n+    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n+    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n+    expect(\n+        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n+    ).anyTimes();\n+    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n+    expect(taskClient.getStatusAsync(anyString()))\n+        .andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING))\n+        .anyTimes();\n+    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n+    checkpoints.put(0, ImmutableMap.of(0, 0L));\n+    expect(taskClient.getCheckpointsAsync(anyString(), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(3);\n+    expect(taskClient.getStartTimeAsync(anyString())).andReturn(Futures.immediateFuture(DateTimes.nowUtc())).anyTimes();\n+    expect(taskClient.pauseAsync(anyString()))\n+        .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 10L)))\n+        .anyTimes();\n+    expect(taskClient.setEndOffsetsAsync(anyString(), EasyMock.eq(ImmutableMap.of(0, 10L)), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(true))\n+        .anyTimes();\n+\n+    replayAll();\n+\n+    supervisor.start();\n+\n+    supervisor.runInternal();\n+\n+    final TreeMap<Integer, Map<Integer, Long>> newCheckpoints = new TreeMap<>();\n+    newCheckpoints.put(0, ImmutableMap.of(0, 10L));\n+    supervisor.checkpoint(\n+        null,\n+        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, newCheckpoints.get(0)))\n+    );\n+\n+    while (supervisor.getNoticesQueueSize() > 0) {\n+      Thread.sleep(100);\n+    }\n+\n+    verifyAll();\n+  }\n+\n   private void addSomeEvents(int numEventsPerPartition) throws Exception\n   {\n     try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n       for (int i = 0; i < NUM_PARTITIONS; i++) {\n         for (int j = 0; j < numEventsPerPartition; j++) {\n           kafkaProducer.send(\n-              new ProducerRecord<byte[], byte[]>(\n+              new ProducerRecord<>(\n                   topic,\n                   i,\n                   null,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "sha": "83e253ff9f1594c749bcea3de8a6f9bbf3c5de3b",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 4,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "patch": "@@ -25,22 +25,29 @@\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.DataSourceMetadata;\n \n+import javax.annotation.Nullable;\n+\n public class CheckPointDataSourceMetadataAction implements TaskAction<Boolean>\n {\n   private final String supervisorId;\n-  private final int taskGroupId;\n+  @Nullable\n+  private final Integer taskGroupId;\n+  @Deprecated\n+  private final String baseSequenceName;\n   private final DataSourceMetadata previousCheckPoint;\n   private final DataSourceMetadata currentCheckPoint;\n \n   public CheckPointDataSourceMetadataAction(\n       @JsonProperty(\"supervisorId\") String supervisorId,\n-      @JsonProperty(\"taskGroupId\") Integer taskGroupId,\n+      @JsonProperty(\"taskGroupId\") @Nullable Integer taskGroupId, // nullable for backward compatibility,\n+      @JsonProperty(\"sequenceName\") @Deprecated String baseSequenceName, // old version would use this\n       @JsonProperty(\"previousCheckPoint\") DataSourceMetadata previousCheckPoint,\n       @JsonProperty(\"currentCheckPoint\") DataSourceMetadata currentCheckPoint\n   )\n   {\n     this.supervisorId = Preconditions.checkNotNull(supervisorId, \"supervisorId\");\n-    this.taskGroupId = Preconditions.checkNotNull(taskGroupId, \"taskGroupId\");\n+    this.taskGroupId = taskGroupId;\n+    this.baseSequenceName = Preconditions.checkNotNull(baseSequenceName, \"sequenceName\");\n     this.previousCheckPoint = Preconditions.checkNotNull(previousCheckPoint, \"previousCheckPoint\");\n     this.currentCheckPoint = Preconditions.checkNotNull(currentCheckPoint, \"currentCheckPoint\");\n   }\n@@ -51,8 +58,16 @@ public String getSupervisorId()\n     return supervisorId;\n   }\n \n+  @Deprecated\n+  @JsonProperty(\"sequenceName\")\n+  public String getBaseSequenceName()\n+  {\n+    return baseSequenceName;\n+  }\n+\n+  @Nullable\n   @JsonProperty\n-  public int getTaskGroupId()\n+  public Integer getTaskGroupId()\n   {\n     return taskGroupId;\n   }\n@@ -85,6 +100,7 @@ public Boolean perform(\n     return toolbox.getSupervisorManager().checkPointDataSourceMetadata(\n         supervisorId,\n         taskGroupId,\n+        baseSequenceName,\n         previousCheckPoint,\n         currentCheckPoint\n     );\n@@ -101,6 +117,7 @@ public String toString()\n   {\n     return \"CheckPointDataSourceMetadataAction{\" +\n            \"supervisorId='\" + supervisorId + '\\'' +\n+           \", baseSequenceName='\" + baseSequenceName + '\\'' +\n            \", taskGroupId='\" + taskGroupId + '\\'' +\n            \", previousCheckPoint=\" + previousCheckPoint +\n            \", currentCheckPoint=\" + currentCheckPoint +",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "sha": "433af987be3b847dedc1a69ff2e2a8d41ea60541",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "patch": "@@ -165,7 +165,8 @@ public boolean resetSupervisor(String id, @Nullable DataSourceMetadata dataSourc\n \n   public boolean checkPointDataSourceMetadata(\n       String supervisorId,\n-      int taskGroupId,\n+      @Nullable Integer taskGroupId,\n+      String baseSequenceName,\n       DataSourceMetadata previousDataSourceMetadata,\n       DataSourceMetadata currentDataSourceMetadata\n   )\n@@ -178,7 +179,7 @@ public boolean checkPointDataSourceMetadata(\n \n       Preconditions.checkNotNull(supervisor, \"supervisor could not be found\");\n \n-      supervisor.lhs.checkpoint(taskGroupId, previousDataSourceMetadata, currentDataSourceMetadata);\n+      supervisor.lhs.checkpoint(taskGroupId, baseSequenceName, previousDataSourceMetadata, currentDataSourceMetadata);\n       return true;\n     }\n     catch (Exception e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "sha": "03cc96f5b0b2841052b9cf54d8d6f7970e89ddcb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "patch": "@@ -83,7 +83,8 @@ public void reset(DataSourceMetadata dataSourceMetadata) {}\n \n       @Override\n       public void checkpoint(\n-          int taskGroupId,\n+          @Nullable Integer taskGroupId,\n+          String baseSequenceName,\n           DataSourceMetadata previousCheckPoint,\n           DataSourceMetadata currentCheckPoint\n       )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "sha": "ccf695e41a523ea236cfb34f45c60ffc692a18f5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java?ref=2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.ImmutableMap;\n import io.druid.indexing.overlord.DataSourceMetadata;\n \n+import javax.annotation.Nullable;\n import java.util.Map;\n \n public interface Supervisor\n@@ -52,8 +53,14 @@\n    * represented by {@param currentCheckpoint} DataSourceMetadata\n    *\n    * @param taskGroupId        unique Identifier to figure out for which sequence to do checkpointing\n+   * @param baseSequenceName   baseSequenceName\n    * @param previousCheckPoint DataSourceMetadata checkpointed in previous call\n    * @param currentCheckPoint  current DataSourceMetadata to be checkpointed\n    */\n-  void checkpoint(int taskGroupId, DataSourceMetadata previousCheckPoint, DataSourceMetadata currentCheckPoint);\n+  void checkpoint(\n+      @Nullable Integer taskGroupId,\n+      @Deprecated String baseSequenceName,\n+      DataSourceMetadata previousCheckPoint,\n+      DataSourceMetadata currentCheckPoint\n+  );\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/2bfe1b6a5a666f925ea1cc1781ea55bafaeeb3c5/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "sha": "6f3c05003a802c50a5fd4fe9342e9f7ad5b667c9",
                "status": "modified"
            }
        ],
        "message": "Fix NPE for taskGroupId when rolling update (#6168)\n\n* Fix NPE for taskGroupId\r\n\r\n* missing changes\r\n\r\n* fix wrong annotation\r\n\r\n* fix potential race\r\n\r\n* keep baseSequenceName\r\n\r\n* make deprecated old param",
        "parent": "https://github.com/apache/incubator-druid/commit/78fc5b246c30c12d247b85daff0f54e67cab2ce6",
        "repo": "incubator-druid",
        "unit_tests": [
            "MaterializedViewSupervisorTest.java",
            "KafkaSupervisorTest.java",
            "SupervisorManagerTest.java"
        ]
    },
    "incubator-druid_3134aff": {
        "bug_id": "incubator-druid_3134aff",
        "commit": "https://github.com/apache/incubator-druid/commit/3134affac9e1f6fc628289f345c1f0ebfdbf6307",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/DirectDruidClient.java?ref=3134affac9e1f6fc628289f345c1f0ebfdbf6307",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/client/DirectDruidClient.java",
                "patch": "@@ -292,7 +292,9 @@ private void init()\n     @Override\n     public void close() throws IOException\n     {\n-      jp.close();\n+      if(jp != null) {\n+        jp.close();\n+      }\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server/src/main/java/io/druid/client/DirectDruidClient.java",
                "sha": "8befef5cabb3f0fe4ebc0994ea442e7677c40660",
                "status": "modified"
            }
        ],
        "message": "fix NPE in DirectDruidClient",
        "parent": "https://github.com/apache/incubator-druid/commit/d0fe70a21f716a26bbaf6dc0e38c50e535a539e7",
        "repo": "incubator-druid",
        "unit_tests": [
            "DirectDruidClientTest.java"
        ]
    },
    "incubator-druid_34be1e9": {
        "bug_id": "incubator-druid_34be1e9",
        "commit": "https://github.com/apache/incubator-druid/commit/34be1e96fa07ea65f16289602a47fa5aa2c156b3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java?ref=34be1e96fa07ea65f16289602a47fa5aa2c156b3",
                "deletions": 6,
                "filename": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "patch": "@@ -101,9 +101,7 @@ public void run(\n       dimValSelector = getDimValSelectorForTopNMetric(singleMetricParam, singleMetricResultBuilder);\n     }\n     finally {\n-      if (singleMetricParam != null) {\n-        singleMetricAlgo.cleanup(singleMetricParam);\n-      }\n+      singleMetricAlgo.cleanup(singleMetricParam);\n     }\n \n     PooledTopNAlgorithm allMetricAlgo = new PooledTopNAlgorithm(capabilities, query, bufferPool);\n@@ -118,9 +116,7 @@ public void run(\n       );\n     }\n     finally {\n-      if (allMetricsParam != null) {\n-        allMetricAlgo.cleanup(allMetricsParam);\n-      }\n+      allMetricAlgo.cleanup(allMetricsParam);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "sha": "f892e07d83967d0e89e26ad3f7ab2b08b538c28f",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java?ref=34be1e96fa07ea65f16289602a47fa5aa2c156b3",
                "deletions": 4,
                "filename": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "patch": "@@ -311,12 +311,14 @@ protected void closeAggregators(BufferAggregator[] bufferAggregators)\n   @Override\n   public void cleanup(PooledTopNParams params)\n   {\n-    ResourceHolder<ByteBuffer> resultsBufHolder = params.getResultsBufHolder();\n+    if (params != null) {\n+      ResourceHolder<ByteBuffer> resultsBufHolder = params.getResultsBufHolder();\n \n-    if (resultsBufHolder != null) {\n-      resultsBufHolder.get().clear();\n+      if (resultsBufHolder != null) {\n+        resultsBufHolder.get().clear();\n+      }\n+      CloseQuietly.close(resultsBufHolder);\n     }\n-    CloseQuietly.close(resultsBufHolder);\n   }\n \n   public static class PooledTopNParams extends TopNParams",
                "raw_url": "https://github.com/apache/incubator-druid/raw/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "sha": "bc6a61578092eac261333fad54e26cf94c024dc9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/incubator-druid/blob/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java?ref=34be1e96fa07ea65f16289602a47fa5aa2c156b3",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Druid - a distributed column store.\n+ * Copyright 2012 - 2015 Metamarkets Group Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.druid.query.topn;\n+\n+import io.druid.collections.ResourceHolder;\n+import io.druid.segment.Capabilities;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class PooledTopNAlgorithmTest\n+{\n+  @Test\n+  public void testCleanupWithNullParams()\n+  {\n+    PooledTopNAlgorithm pooledTopNAlgorithm = new PooledTopNAlgorithm(Capabilities.builder().build(), null, null);\n+    pooledTopNAlgorithm.cleanup(null);\n+  }\n+\n+  @Test\n+  public void cleanup() throws IOException\n+  {\n+    PooledTopNAlgorithm pooledTopNAlgorithm = new PooledTopNAlgorithm(Capabilities.builder().build(), null, null);\n+    PooledTopNAlgorithm.PooledTopNParams params = EasyMock.createMock(PooledTopNAlgorithm.PooledTopNParams.class);\n+    ResourceHolder<ByteBuffer> resourceHolder = EasyMock.createMock(ResourceHolder.class);\n+    EasyMock.expect(params.getResultsBufHolder()).andReturn(resourceHolder).times(1);\n+    EasyMock.expect(resourceHolder.get()).andReturn(ByteBuffer.allocate(1)).times(1);\n+    resourceHolder.close();\n+    EasyMock.expectLastCall().once();\n+    EasyMock.replay(params);\n+    EasyMock.replay(resourceHolder);\n+    pooledTopNAlgorithm.cleanup(params);\n+    EasyMock.verify(params);\n+    EasyMock.verify(resourceHolder);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/34be1e96fa07ea65f16289602a47fa5aa2c156b3/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "sha": "0e2888094d64f0e4fdc37e293ba31f5445ddad08",
                "status": "added"
            }
        ],
        "message": "fix NPE\n\nreview comments\n\nAdd test\n\nfix test for java8",
        "parent": "https://github.com/apache/incubator-druid/commit/d7562fd4d1e2dfe786dc5bb8cdd0fc0829a27e63",
        "repo": "incubator-druid",
        "unit_tests": [
            "PooledTopNAlgorithmTest.java"
        ]
    },
    "incubator-druid_35c89d2": {
        "bug_id": "incubator-druid_35c89d2",
        "commit": "https://github.com/apache/incubator-druid/commit/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/CachingQueryRunner.java?ref=35c89d29a0e31ce4bbd568bb3218b3c79386ccb4",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "patch": "@@ -84,11 +84,16 @@ public CachingQueryRunner(\n         && strategy != null\n         && cacheConfig.isPopulateCache();\n \n-    final Cache.NamedKey key = CacheUtil.computeSegmentCacheKey(\n-        segmentIdentifier,\n-        segmentDescriptor,\n-        strategy.computeCacheKey(query)\n-    );\n+    final Cache.NamedKey key;\n+    if(strategy != null && (useCache || populateCache)) {\n+      key = CacheUtil.computeSegmentCacheKey(\n+          segmentIdentifier,\n+          segmentDescriptor,\n+          strategy.computeCacheKey(query)\n+      );\n+    } else {\n+      key = null;\n+    }\n \n     if(useCache) {\n       final Function cacheFn = strategy.pullFromCache();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/35c89d29a0e31ce4bbd568bb3218b3c79386ccb4/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "sha": "d92db6415fba83fcc3e18c357f8eff913e3aac43",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #473 from metamx/fix-caching-npe\n\nfix npe in CachingQueryRunner",
        "parent": "https://github.com/apache/incubator-druid/commit/c05f16917115cf19257e7c928a14d9eeec9b005c",
        "repo": "incubator-druid",
        "unit_tests": [
            "CachingQueryRunnerTest.java"
        ]
    },
    "incubator-druid_3a0a667": {
        "bug_id": "incubator-druid_3a0a667",
        "commit": "https://github.com/apache/incubator-druid/commit/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 2,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "patch": "@@ -44,6 +44,8 @@\n import org.apache.druid.sql.calcite.planner.PlannerConfig;\n import org.apache.druid.sql.calcite.planner.PlannerFactory;\n import org.apache.druid.sql.calcite.planner.PlannerResult;\n+import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.SpecificSegmentsQuerySegmentWalker;\n import org.apache.druid.timeline.DataSegment;\n@@ -111,10 +113,12 @@ public void setup()\n         .createQueryRunnerFactoryConglomerate();\n     final QueryRunnerFactoryConglomerate conglomerate = conglomerateCloserPair.lhs;\n     final PlannerConfig plannerConfig = new PlannerConfig();\n-\n+    final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     this.walker = new SpecificSegmentsQuerySegmentWalker(conglomerate).add(dataSegment, index);\n     plannerFactory = new PlannerFactory(\n-        CalciteTests.createMockSchema(conglomerate, walker, plannerConfig),\n+        druidSchema,\n+        systemSchema,\n         CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n         CalciteTests.createOperatorTable(),\n         CalciteTests.createExprMacroTable(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "sha": "7971b549c4753339d5f434bb91c0e8f581e31c2e",
                "status": "modified"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/docs/content/querying/sql.md",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/querying/sql.md?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "docs/content/querying/sql.md",
                "patch": "@@ -477,6 +477,11 @@ plan SQL queries. This metadata is cached on broker startup and also updated per\n [SegmentMetadata queries](segmentmetadataquery.html). Background metadata refreshing is triggered by\n segments entering and exiting the cluster, and can also be throttled through configuration.\n \n+Druid exposes system information through special system tables. There are two such schemas available: Information Schema and Sys Schema.\n+Information schema provides details about table and column types. The \"sys\" schema provides information about Druid internals like segments/tasks/servers.\n+\n+## INFORMATION SCHEMA\n+\n You can access table and column metadata through JDBC using `connection.getMetaData()`, or through the\n INFORMATION_SCHEMA tables described below. For example, to retrieve metadata for the Druid\n datasource \"foo\", use the query:\n@@ -528,6 +533,101 @@ SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_\n |COLLATION_NAME||\n |JDBC_TYPE|Type code from java.sql.Types (Druid extension)|\n \n+## SYSTEM SCHEMA\n+\n+The \"sys\" schema provides visibility into Druid segments, servers and tasks.\n+For example to retrieve all segments for datasource \"wikipedia\", use the query:\n+```sql\n+SELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n+```\n+\n+### SEGMENTS table\n+Segments table provides details on all Druid segments, whether they are published yet or not.\n+\n+\n+|Column|Notes|\n+|------|-----|\n+|segment_id|Unique segment identifier|\n+|datasource|Name of datasource|\n+|start|Interval start time (in ISO 8601 format)|\n+|end|Interval end time (in ISO 8601 format)|\n+|size|Size of segment in bytes|\n+|version|Version string (generally an ISO8601 timestamp corresponding to when the segment set was first started). Higher version means the more recently created segment. Version comparing is based on string comparison.|\n+|partition_num|Partition number (an integer, unique within a datasource+interval+version; may not necessarily be contiguous)|\n+|num_replicas|Number of replicas of this segment currently being served|\n+|num_rows|Number of rows in current segment, this value could be null if unkown to broker at query time|\n+|is_published|Boolean is represented as long type where 1 = true, 0 = false. 1 represents this segment has been published to the metadata store|\n+|is_available|Boolean is represented as long type where 1 = true, 0 = false. 1 if this segment is currently being served by any server(historical or realtime)|\n+|is_realtime|Boolean is represented as long type where 1 = true, 0 = false. 1 if this segment is being served on any type of realtime tasks|\n+|payload|JSON-serialized data segment payload|\n+\n+### SERVERS table\n+Servers table lists all data servers(any server that hosts a segment). It includes both historicals and peons.\n+\n+|Column|Notes|\n+|------|-----|\n+|server|Server name in the form host:port|\n+|host|Hostname of the server|\n+|plaintext_port|Unsecured port of the server, or -1 if plaintext traffic is disabled|\n+|tls_port|TLS port of the server, or -1 if TLS is disabled|\n+|server_type|Type of Druid service. Possible values include: historical, realtime and indexer_executor(peon).|\n+|tier|Distribution tier see [druid.server.tier](#../configuration/index.html#Historical-General-Configuration)|\n+|current_size|Current size of segments in bytes on this server|\n+|max_size|Max size in bytes this server recommends to assign to segments see [druid.server.maxSize](#../configuration/index.html#Historical-General-Configuration)|\n+\n+To retrieve information about all servers, use the query:\n+```sql\n+SELECT * FROM sys.servers;\n+```\n+\n+### SERVER_SEGMENTS table\n+\n+SERVER_SEGMENTS is used to join servers with segments table\n+\n+|Column|Notes|\n+|------|-----|\n+|server|Server name in format host:port (Primary key of [servers table](#SERVERS-table))|\n+|segment_id|Segment identifier (Primary key of [segments table](#SEGMENTS-table))|\n+\n+JOIN between \"servers\" and \"segments\" can be used to query the number of segments for a specific datasource, \n+grouped by server, example query:\n+```sql\n+SELECT count(segments.segment_id) as num_segments from sys.segments as segments \n+INNER JOIN sys.server_segments as server_segments \n+ON segments.segment_id  = server_segments.segment_id \n+INNER JOIN sys.servers as servers \n+ON servers.server = server_segments.server\n+WHERE segments.datasource = 'wikipedia' \n+GROUP BY servers.server;\n+```\n+\n+### TASKS table\n+\n+The tasks table provides information about active and recently-completed indexing tasks. For more information \n+check out [ingestion tasks](#../ingestion/tasks.html)\n+\n+|Column|Notes|\n+|------|-----|\n+|task_id|Unique task identifier|\n+|type|Task type, for example this value is \"index\" for indexing tasks. See [tasks-overview](../ingestion/tasks.md)|\n+|datasource|Datasource name being indexed|\n+|created_time|Timestamp in ISO8601 format corresponding to when the ingestion task was created. Note that this value is populated for completed and waiting tasks. For running and pending tasks this value is set to 1970-01-01T00:00:00Z|\n+|queue_insertion_time|Timestamp in ISO8601 format corresponding to when this task was added to the queue on the overlord|\n+|status|Status of a task can be RUNNING, FAILED, SUCCESS|\n+|runner_status|Runner status of a completed task would be NONE, for in-progress tasks this can be RUNNING, WAITING, PENDING|\n+|duration|Time it took to finish the task in milliseconds, this value is present only for completed tasks|\n+|location|Server name where this task is running in the format host:port, this information is present only for RUNNING tasks|\n+|host|Hostname of the server where task is running|\n+|plaintext_port|Unsecured port of the server, or -1 if plaintext traffic is disabled|\n+|tls_port|TLS port of the server, or -1 if TLS is disabled|\n+|error_msg|Detailed error message in case of FAILED tasks|\n+\n+For example, to retrieve tasks information filtered by status, use the query\n+```sql\n+SELECT * FROM sys.tasks where status='FAILED';\n+```\n+\n+\n ## Server configuration\n \n The Druid SQL server is configured through the following properties on the broker.",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/docs/content/querying/sql.md",
                "sha": "e604a1cb98046947d56d5c333bdabe225b7ba5f2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/sql/QuantileSqlAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/sql/QuantileSqlAggregatorTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/sql/QuantileSqlAggregatorTest.java",
                "patch": "@@ -60,6 +60,7 @@\n import org.apache.druid.sql.calcite.planner.PlannerFactory;\n import org.apache.druid.sql.calcite.planner.PlannerResult;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTestBase;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n@@ -150,13 +151,15 @@ public void setUp() throws Exception\n \n     final PlannerConfig plannerConfig = new PlannerConfig();\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = new DruidOperatorTable(\n         ImmutableSet.of(new QuantileSqlAggregator()),\n         ImmutableSet.of()\n     );\n \n     plannerFactory = new PlannerFactory(\n         druidSchema,\n+        systemSchema,\n         CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n         operatorTable,\n         CalciteTests.createExprMacroTable(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/sql/QuantileSqlAggregatorTest.java",
                "sha": "4de50604a67bbc71e2493379fd2d82fa8be4603d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/BrokerServerView.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "patch": "@@ -45,12 +45,14 @@\n import org.apache.druid.timeline.partition.PartitionChunk;\n \n import javax.annotation.Nullable;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.function.Function;\n+import java.util.stream.Collectors;\n \n /**\n  */\n@@ -322,4 +324,12 @@ private void runTimelineCallbacks(final Function<TimelineCallback, CallbackActio\n       );\n     }\n   }\n+\n+  @Override\n+  public List<ImmutableDruidServer> getDruidServers()\n+  {\n+    return clients.values().stream()\n+                  .map(queryableDruidServer -> queryableDruidServer.getServer().toImmutableDruidServer())\n+                  .collect(Collectors.toList());\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "sha": "f4e874d8d73ead3ae6384dd039016ec3683d7321",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/DirectDruidClient.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/DirectDruidClient.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 118,
                "filename": "server/src/main/java/org/apache/druid/client/DirectDruidClient.java",
                "patch": "@@ -19,9 +19,6 @@\n \n package org.apache.druid.client;\n \n-import com.fasterxml.jackson.core.JsonParser;\n-import com.fasterxml.jackson.core.JsonToken;\n-import com.fasterxml.jackson.core.ObjectCodec;\n import com.fasterxml.jackson.databind.JavaType;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.type.TypeFactory;\n@@ -32,7 +29,6 @@\n import com.google.common.util.concurrent.FutureCallback;\n import com.google.common.util.concurrent.Futures;\n import com.google.common.util.concurrent.ListenableFuture;\n-import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.RE;\n import org.apache.druid.java.util.common.StringUtils;\n@@ -52,14 +48,12 @@\n import org.apache.druid.query.BySegmentResultValueClass;\n import org.apache.druid.query.Query;\n import org.apache.druid.query.QueryContexts;\n-import org.apache.druid.query.QueryInterruptedException;\n import org.apache.druid.query.QueryMetrics;\n import org.apache.druid.query.QueryPlus;\n import org.apache.druid.query.QueryRunner;\n import org.apache.druid.query.QueryToolChest;\n import org.apache.druid.query.QueryToolChestWarehouse;\n import org.apache.druid.query.QueryWatcher;\n-import org.apache.druid.query.ResourceLimitExceededException;\n import org.apache.druid.query.Result;\n import org.apache.druid.query.aggregation.MetricManipulatorFns;\n import org.jboss.netty.buffer.ChannelBuffer;\n@@ -71,20 +65,16 @@\n import org.joda.time.Duration;\n \n import javax.ws.rs.core.MediaType;\n-import java.io.Closeable;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.SequenceInputStream;\n import java.net.URL;\n import java.nio.charset.StandardCharsets;\n import java.util.Enumeration;\n-import java.util.Iterator;\n import java.util.Map;\n import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.CancellationException;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.Future;\n import java.util.concurrent.LinkedBlockingQueue;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n@@ -550,7 +540,7 @@ public void onFailure(Throwable t)\n           @Override\n           public JsonParserIterator<T> make()\n           {\n-            return new JsonParserIterator<T>(typeRef, future, url, query);\n+            return new JsonParserIterator<T>(typeRef, future, url, query, host, objectMapper);\n           }\n \n           @Override\n@@ -576,113 +566,6 @@ public void cleanup(JsonParserIterator<T> iterFromMake)\n     return retVal;\n   }\n \n-  private class JsonParserIterator<T> implements Iterator<T>, Closeable\n-  {\n-    private JsonParser jp;\n-    private ObjectCodec objectCodec;\n-    private final JavaType typeRef;\n-    private final Future<InputStream> future;\n-    private final Query<T> query;\n-    private final String url;\n-\n-    public JsonParserIterator(JavaType typeRef, Future<InputStream> future, String url, Query<T> query)\n-    {\n-      this.typeRef = typeRef;\n-      this.future = future;\n-      this.url = url;\n-      this.query = query;\n-      jp = null;\n-    }\n-\n-    @Override\n-    public boolean hasNext()\n-    {\n-      init();\n-\n-      if (jp.isClosed()) {\n-        return false;\n-      }\n-      if (jp.getCurrentToken() == JsonToken.END_ARRAY) {\n-        CloseQuietly.close(jp);\n-        return false;\n-      }\n-\n-      return true;\n-    }\n-\n-    @Override\n-    public T next()\n-    {\n-      init();\n-\n-      try {\n-        final T retVal = objectCodec.readValue(jp, typeRef);\n-        jp.nextToken();\n-        return retVal;\n-      }\n-      catch (IOException e) {\n-        throw Throwables.propagate(e);\n-      }\n-    }\n-\n-    @Override\n-    public void remove()\n-    {\n-      throw new UnsupportedOperationException();\n-    }\n-\n-    private void init()\n-    {\n-      if (jp == null) {\n-        try {\n-          InputStream is = future.get();\n-          if (is == null) {\n-            throw new QueryInterruptedException(\n-                new ResourceLimitExceededException(\n-                    \"query[%s] url[%s] timed out or max bytes limit reached.\",\n-                    query.getId(),\n-                    url\n-                ),\n-                host\n-            );\n-          } else {\n-            jp = objectMapper.getFactory().createParser(is);\n-          }\n-          final JsonToken nextToken = jp.nextToken();\n-          if (nextToken == JsonToken.START_OBJECT) {\n-            QueryInterruptedException cause = jp.getCodec().readValue(jp, QueryInterruptedException.class);\n-            throw new QueryInterruptedException(cause, host);\n-          } else if (nextToken != JsonToken.START_ARRAY) {\n-            throw new IAE(\"Next token wasn't a START_ARRAY, was[%s] from url [%s]\", jp.getCurrentToken(), url);\n-          } else {\n-            jp.nextToken();\n-            objectCodec = jp.getCodec();\n-          }\n-        }\n-        catch (IOException | InterruptedException | ExecutionException e) {\n-          throw new RE(\n-              e,\n-              \"Failure getting results for query[%s] url[%s] because of [%s]\",\n-              query.getId(),\n-              url,\n-              e.getMessage()\n-          );\n-        }\n-        catch (CancellationException e) {\n-          throw new QueryInterruptedException(e, host);\n-        }\n-      }\n-    }\n-\n-    @Override\n-    public void close() throws IOException\n-    {\n-      if (jp != null) {\n-        jp.close();\n-      }\n-    }\n-  }\n-\n   @Override\n   public String toString()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/DirectDruidClient.java",
                "sha": "da2fb03b819c4699b96e452b663e706489733658",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java",
                "patch": "@@ -65,6 +65,16 @@ public String getHost()\n     return metadata.getHost();\n   }\n \n+  public String getHostAndPort()\n+  {\n+    return metadata.getHostAndPort();\n+  }\n+\n+  public String getHostAndTlsPort()\n+  {\n+    return metadata.getHostAndTlsPort();\n+  }\n+\n   public long getCurrSize()\n   {\n     return currSize;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java",
                "sha": "280ed38ec3b9d7fed9633514d579b6a54ef46c04",
                "status": "modified"
            },
            {
                "additions": 160,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/JsonParserIterator.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "patch": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.client;\n+\n+import com.fasterxml.jackson.core.JsonParser;\n+import com.fasterxml.jackson.core.JsonToken;\n+import com.fasterxml.jackson.core.ObjectCodec;\n+import com.fasterxml.jackson.databind.JavaType;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Throwables;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.guava.CloseQuietly;\n+import org.apache.druid.query.Query;\n+import org.apache.druid.query.QueryInterruptedException;\n+import org.apache.druid.query.ResourceLimitExceededException;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Iterator;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+\n+public class JsonParserIterator<T> implements Iterator<T>, Closeable\n+{\n+  private JsonParser jp;\n+  private ObjectCodec objectCodec;\n+  private final JavaType typeRef;\n+  private final Future<InputStream> future;\n+  private final Query<T> query;\n+  private final String url;\n+  private final String host;\n+  private final ObjectMapper objectMapper;\n+\n+  public JsonParserIterator(\n+      JavaType typeRef,\n+      Future<InputStream> future,\n+      String url,\n+      Query<T> query,\n+      String host,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.typeRef = typeRef;\n+    this.future = future;\n+    this.url = url;\n+    this.query = query;\n+    jp = null;\n+    this.host = host;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  public boolean hasNext()\n+  {\n+    init();\n+\n+    if (jp.isClosed()) {\n+      return false;\n+    }\n+    if (jp.getCurrentToken() == JsonToken.END_ARRAY) {\n+      CloseQuietly.close(jp);\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @Override\n+  public T next()\n+  {\n+    init();\n+\n+    try {\n+      final T retVal = objectCodec.readValue(jp, typeRef);\n+      jp.nextToken();\n+      return retVal;\n+    }\n+    catch (IOException e) {\n+      throw Throwables.propagate(e);\n+    }\n+  }\n+\n+  @Override\n+  public void remove()\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private void init()\n+  {\n+    if (jp == null) {\n+      try {\n+        InputStream is = future.get();\n+        if (is == null) {\n+          throw new QueryInterruptedException(\n+              new ResourceLimitExceededException(\n+                  \"query[%s] url[%s] timed out or max bytes limit reached.\",\n+                  query.getId(),\n+                  url\n+              ),\n+              host\n+          );\n+        } else {\n+          jp = objectMapper.getFactory().createParser(is);\n+        }\n+        final JsonToken nextToken = jp.nextToken();\n+        if (nextToken == JsonToken.START_OBJECT) {\n+          QueryInterruptedException cause = jp.getCodec().readValue(jp, QueryInterruptedException.class);\n+          throw new QueryInterruptedException(cause, host);\n+        } else if (nextToken != JsonToken.START_ARRAY) {\n+          throw new IAE(\"Next token wasn't a START_ARRAY, was[%s] from url [%s]\", jp.getCurrentToken(), url);\n+        } else {\n+          jp.nextToken();\n+          objectCodec = jp.getCodec();\n+        }\n+      }\n+      catch (IOException | InterruptedException | ExecutionException e) {\n+        throw new RE(\n+            e,\n+            \"Failure getting results for query[%s] url[%s] because of [%s]\",\n+            query == null ? null : query.getId(),\n+            url,\n+            e.getMessage()\n+        );\n+      }\n+      catch (CancellationException e) {\n+        throw new QueryInterruptedException(e, host);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException\n+  {\n+    if (jp != null) {\n+      jp.close();\n+    }\n+  }\n+}\n+",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "sha": "18535c19c393cab7dca7c41ee8da9584b36a783b",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/TimelineServerView.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/TimelineServerView.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/TimelineServerView.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.druid.timeline.TimelineLookup;\n \n import javax.annotation.Nullable;\n+import java.util.List;\n import java.util.concurrent.Executor;\n \n /**\n@@ -36,6 +37,11 @@\n   @Nullable\n   TimelineLookup<String, ServerSelector> getTimeline(DataSource dataSource);\n \n+  /**\n+   * Returns a list of {@link ImmutableDruidServer}\n+   */\n+  List<ImmutableDruidServer> getDruidServers();\n+\n   <T> QueryRunner<T> getQueryRunner(DruidServer server);\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/client/TimelineServerView.java",
                "sha": "89042c6507d64099f9520f1167d5d2d6b399094c",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.base.Preconditions;\n import com.google.common.base.Throwables;\n+import com.google.common.util.concurrent.ListenableFuture;\n import org.apache.druid.client.selector.Server;\n import org.apache.druid.concurrent.LifecycleLock;\n import org.apache.druid.curator.discovery.ServerDiscoverySelector;\n@@ -133,6 +134,18 @@ public FullResponseHolder go(Request request) throws IOException, InterruptedExc\n     return go(request, new FullResponseHandler(StandardCharsets.UTF_8));\n   }\n \n+  /**\n+   * Executes the request object aimed at the leader and process the response with given handler\n+   * Note: this method doesn't do retrying on errors or handle leader changes occurred during communication\n+   */\n+  public <Intermediate, Final> ListenableFuture<Final> goAsync(\n+      final Request request,\n+      final HttpResponseHandler<Intermediate, Final> handler\n+  )\n+  {\n+    return httpClient.go(request, handler);\n+  }\n+\n   /**\n    * Executes a Request object aimed at the leader. Throws IOException if the leader cannot be located.\n    */",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java",
                "sha": "72feb9cdbbdcb1bf63424f9c4063bcf51517b110",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncer.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncer.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 22,
                "filename": "server/src/main/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncer.java",
                "patch": "@@ -34,12 +34,9 @@\n import org.apache.druid.java.util.emitter.EmittingLogger;\n import org.apache.druid.java.util.http.client.HttpClient;\n import org.apache.druid.java.util.http.client.Request;\n-import org.apache.druid.java.util.http.client.io.AppendableByteArrayInputStream;\n-import org.apache.druid.java.util.http.client.response.ClientResponse;\n-import org.apache.druid.java.util.http.client.response.InputStreamResponseHandler;\n+import org.apache.druid.server.coordinator.BytesAccumulatingResponseHandler;\n import org.jboss.netty.handler.codec.http.HttpHeaders;\n import org.jboss.netty.handler.codec.http.HttpMethod;\n-import org.jboss.netty.handler.codec.http.HttpResponse;\n import org.joda.time.Duration;\n \n import javax.servlet.http.HttpServletResponse;\n@@ -233,11 +230,11 @@ public void onSuccess(InputStream stream)\n                 }\n \n                 try {\n-                  if (responseHandler.status == HttpServletResponse.SC_NO_CONTENT) {\n+                  if (responseHandler.getStatus() == HttpServletResponse.SC_NO_CONTENT) {\n                     log.debug(\"Received NO CONTENT from server[%s]\", logIdentity);\n                     lastSuccessfulSyncTime = System.currentTimeMillis();\n                     return;\n-                  } else if (responseHandler.status != HttpServletResponse.SC_OK) {\n+                  } else if (responseHandler.getStatus() != HttpServletResponse.SC_OK) {\n                     handleFailure(new RE(\"Bad Sync Response.\"));\n                     return;\n                   }\n@@ -326,8 +323,8 @@ private void handleFailure(Throwable t)\n               String logMsg = StringUtils.nonStrictFormat(\n                   \"failed to get sync response from [%s]. Return code [%s], Reason: [%s]\",\n                   logIdentity,\n-                  responseHandler.status,\n-                  responseHandler.description\n+                  responseHandler.getStatus(),\n+                  responseHandler.getDescription()\n               );\n \n               if (incrementFailedAttemptAndCheckUnstabilityTimeout()) {\n@@ -420,20 +417,6 @@ private boolean incrementFailedAttemptAndCheckUnstabilityTimeout()\n     return false;\n   }\n \n-  private static class BytesAccumulatingResponseHandler extends InputStreamResponseHandler\n-  {\n-    private int status;\n-    private String description;\n-\n-    @Override\n-    public ClientResponse<AppendableByteArrayInputStream> handleResponse(HttpResponse response, TrafficCop trafficCop)\n-    {\n-      status = response.getStatus().getCode();\n-      description = response.getStatus().getReasonPhrase();\n-      return ClientResponse.unfinished(super.handleResponse(response, trafficCop).getObj());\n-    }\n-  }\n-\n   public interface Listener<T>\n   {\n     void fullSync(List<T> changes);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncer.java",
                "sha": "309f0efee36788866dab9369cc40799a349bbc12",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordinator/BytesAccumulatingResponseHandler.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/BytesAccumulatingResponseHandler.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/BytesAccumulatingResponseHandler.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.druid.server.coordinator;\n+\n+import org.apache.druid.java.util.http.client.io.AppendableByteArrayInputStream;\n+import org.apache.druid.java.util.http.client.response.ClientResponse;\n+import org.apache.druid.java.util.http.client.response.InputStreamResponseHandler;\n+import org.jboss.netty.handler.codec.http.HttpResponse;\n+\n+/**\n+ * An async BytesAccumulatingResponseHandler which returns unfinished response\n+ */\n+public class BytesAccumulatingResponseHandler extends InputStreamResponseHandler\n+{\n+  private int status;\n+  private String description;\n+\n+  @Override\n+  public ClientResponse<AppendableByteArrayInputStream> handleResponse(HttpResponse response, TrafficCop trafficCop)\n+  {\n+    status = response.getStatus().getCode();\n+    description = response.getStatus().getReasonPhrase();\n+    return ClientResponse.unfinished(super.handleResponse(response, trafficCop).getObj());\n+  }\n+\n+  public int getStatus()\n+  {\n+    return status;\n+  }\n+\n+  public String getDescription()\n+  {\n+    return description;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordinator/BytesAccumulatingResponseHandler.java",
                "sha": "c9e89c3c35d73aefa970723dc10ff1aeffa4800c",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 22,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java",
                "patch": "@@ -35,9 +35,6 @@\n import org.apache.druid.java.util.emitter.EmittingLogger;\n import org.apache.druid.java.util.http.client.HttpClient;\n import org.apache.druid.java.util.http.client.Request;\n-import org.apache.druid.java.util.http.client.io.AppendableByteArrayInputStream;\n-import org.apache.druid.java.util.http.client.response.ClientResponse;\n-import org.apache.druid.java.util.http.client.response.InputStreamResponseHandler;\n import org.apache.druid.server.coordination.DataSegmentChangeCallback;\n import org.apache.druid.server.coordination.DataSegmentChangeHandler;\n import org.apache.druid.server.coordination.DataSegmentChangeRequest;\n@@ -47,7 +44,6 @@\n import org.apache.druid.timeline.DataSegment;\n import org.jboss.netty.handler.codec.http.HttpHeaders;\n import org.jboss.netty.handler.codec.http.HttpMethod;\n-import org.jboss.netty.handler.codec.http.HttpResponse;\n import org.joda.time.Duration;\n \n import javax.servlet.http.HttpServletResponse;\n@@ -207,9 +203,9 @@ public void onSuccess(InputStream result)\n             {\n               boolean scheduleNextRunImmediately = true;\n               try {\n-                if (responseHandler.status == HttpServletResponse.SC_NO_CONTENT) {\n+                if (responseHandler.getStatus() == HttpServletResponse.SC_NO_CONTENT) {\n                   log.debug(\"Received NO CONTENT reseponse from [%s]\", serverId);\n-                } else if (HttpServletResponse.SC_OK == responseHandler.status) {\n+                } else if (HttpServletResponse.SC_OK == responseHandler.getStatus()) {\n                   try {\n                     List<SegmentLoadDropHandler.DataSegmentChangeRequestAndStatus> statuses = jsonMapper.readValue(\n                         result, RESPONSE_ENTITY_TYPE_REF\n@@ -260,7 +256,6 @@ public void onSuccess(InputStream result)\n             public void onFailure(Throwable t)\n             {\n               try {\n-                responseHandler.description = t.toString();\n                 logRequestFailure(t);\n               }\n               finally {\n@@ -274,8 +269,8 @@ private void logRequestFailure(Throwable t)\n                   t,\n                   \"Request[%s] Failed with status[%s]. Reason[%s].\",\n                   changeRequestURL,\n-                  responseHandler.status,\n-                  responseHandler.description\n+                  responseHandler.getStatus(),\n+                  responseHandler.getDescription()\n               );\n             }\n           },\n@@ -596,17 +591,4 @@ public DropSegmentHolder(DataSegment segment, LoadPeonCallback callback)\n     }\n   }\n \n-  private static class BytesAccumulatingResponseHandler extends InputStreamResponseHandler\n-  {\n-    private int status;\n-    private String description;\n-\n-    @Override\n-    public ClientResponse<AppendableByteArrayInputStream> handleResponse(HttpResponse response, TrafficCop trafficCop)\n-    {\n-      status = response.getStatus().getCode();\n-      description = response.getStatus().getReasonPhrase();\n-      return ClientResponse.unfinished(super.handleResponse(response, trafficCop).getObj());\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java",
                "sha": "4b4498019da77013a22cbb5dc3476b20e6eb1ff0",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/http/MetadataResource.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/MetadataResource.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/server/http/MetadataResource.java",
                "patch": "@@ -19,13 +19,17 @@\n \n package org.apache.druid.server.http;\n \n+import com.fasterxml.jackson.core.JsonFactory;\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Function;\n import com.google.common.collect.Collections2;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import com.google.inject.Inject;\n import com.sun.jersey.spi.container.ResourceFilters;\n import org.apache.druid.client.ImmutableDruidDataSource;\n+import org.apache.druid.guice.annotations.Json;\n import org.apache.druid.indexing.overlord.IndexerMetadataStorageCoordinator;\n import org.apache.druid.metadata.MetadataSegmentManager;\n import org.apache.druid.server.http.security.DatasourceResourceFilter;\n@@ -46,10 +50,12 @@\n import javax.ws.rs.core.Context;\n import javax.ws.rs.core.MediaType;\n import javax.ws.rs.core.Response;\n+import javax.ws.rs.core.StreamingOutput;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.List;\n import java.util.Set;\n+import java.util.stream.Collectors;\n \n /**\n  */\n@@ -60,19 +66,22 @@\n   private final IndexerMetadataStorageCoordinator metadataStorageCoordinator;\n   private final AuthConfig authConfig;\n   private final AuthorizerMapper authorizerMapper;\n+  private final ObjectMapper jsonMapper;\n \n   @Inject\n   public MetadataResource(\n       MetadataSegmentManager metadataSegmentManager,\n       IndexerMetadataStorageCoordinator metadataStorageCoordinator,\n       AuthConfig authConfig,\n-      AuthorizerMapper authorizerMapper\n+      AuthorizerMapper authorizerMapper,\n+      @Json ObjectMapper jsonMapper\n   )\n   {\n     this.metadataSegmentManager = metadataSegmentManager;\n     this.metadataStorageCoordinator = metadataStorageCoordinator;\n     this.authConfig = authConfig;\n     this.authorizerMapper = authorizerMapper;\n+    this.jsonMapper = jsonMapper;\n   }\n \n   @GET\n@@ -136,6 +145,39 @@ public Response getDatabaseSegmentDataSource(\n     return Response.status(Response.Status.OK).entity(dataSource).build();\n   }\n \n+  @GET\n+  @Path(\"/segments\")\n+  @Produces(MediaType.APPLICATION_JSON)\n+  public Response getDatabaseSegments(@Context final HttpServletRequest req)\n+  {\n+    final Collection<ImmutableDruidDataSource> druidDataSources = metadataSegmentManager.getInventory();\n+    final Set<DataSegment> metadataSegments = druidDataSources\n+        .stream()\n+        .flatMap(t -> t.getSegments().stream())\n+        .collect(Collectors.toSet());\n+\n+    Function<DataSegment, Iterable<ResourceAction>> raGenerator = segment -> Collections.singletonList(\n+        AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(segment.getDataSource()));\n+\n+    final Iterable<DataSegment> authorizedSegments = AuthorizationUtils.filterAuthorizedResources(\n+        req, metadataSegments, raGenerator, authorizerMapper);\n+\n+    final StreamingOutput stream = outputStream -> {\n+      final JsonFactory jsonFactory = jsonMapper.getFactory();\n+      try (final JsonGenerator jsonGenerator = jsonFactory.createGenerator(outputStream)) {\n+        jsonGenerator.writeStartArray();\n+        for (DataSegment ds : authorizedSegments) {\n+          jsonGenerator.writeObject(ds);\n+          jsonGenerator.flush();\n+        }\n+        jsonGenerator.writeEndArray();\n+      }\n+    };\n+\n+    Response.ResponseBuilder builder = Response.status(Response.Status.OK);\n+    return builder.entity(stream).build();\n+  }\n+\n   @GET\n   @Path(\"/datasources/{dataSourceName}/segments\")\n   @Produces(MediaType.APPLICATION_JSON)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/main/java/org/apache/druid/server/http/MetadataResource.java",
                "sha": "781842f4fee5cfe70b2d46be76b2927a938e93bc",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.junit.Before;\n import org.junit.Test;\n \n+import javax.annotation.Nullable;\n import java.io.IOException;\n import java.util.Collections;\n import java.util.Comparator;\n@@ -251,6 +252,13 @@ public void registerSegmentCallback(Executor exec, SegmentCallback callback)\n             return timeline;\n           }\n \n+          @Nullable\n+          @Override\n+          public List<ImmutableDruidServer> getDruidServers()\n+          {\n+            throw new UnsupportedOperationException();\n+          }\n+\n           @Override\n           public void registerTimelineCallback(final Executor exec, final TimelineCallback callback)\n           {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java",
                "sha": "9cec0227aac382ce6c478e5cffc2805b6fc6600b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "patch": "@@ -2639,6 +2639,12 @@ public void registerSegmentCallback(Executor exec, SegmentCallback callback)\n             return timeline;\n           }\n \n+          @Override\n+          public List<ImmutableDruidServer> getDruidServers()\n+          {\n+            throw new UnsupportedOperationException();\n+          }\n+\n           @Override\n           public <T> QueryRunner<T> getQueryRunner(DruidServer server)\n           {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "sha": "573c5c924658354a31f5c451c0708aa144688968",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/Calcites.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/planner/Calcites.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 2,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/planner/Calcites.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.calcite.rel.type.RelDataTypeFactory;\n import org.apache.calcite.rex.RexLiteral;\n import org.apache.calcite.rex.RexNode;\n-import org.apache.calcite.schema.Schema;\n import org.apache.calcite.schema.SchemaPlus;\n import org.apache.calcite.sql.SqlCollation;\n import org.apache.calcite.sql.SqlKind;\n@@ -46,6 +45,7 @@\n import org.apache.druid.server.security.AuthorizerMapper;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n import org.apache.druid.sql.calcite.schema.InformationSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.joda.time.DateTime;\n import org.joda.time.DateTimeZone;\n import org.joda.time.Days;\n@@ -98,11 +98,16 @@ public static Charset defaultCharset()\n     return DEFAULT_CHARSET;\n   }\n \n-  public static SchemaPlus createRootSchema(final Schema druidSchema, final AuthorizerMapper authorizerMapper)\n+  public static SchemaPlus createRootSchema(\n+      final DruidSchema druidSchema,\n+      final SystemSchema systemSchema,\n+      final AuthorizerMapper authorizerMapper\n+  )\n   {\n     final SchemaPlus rootSchema = CalciteSchema.createRootSchema(false, false).plus();\n     rootSchema.add(DruidSchema.NAME, druidSchema);\n     rootSchema.add(InformationSchema.NAME, new InformationSchema(rootSchema, authorizerMapper));\n+    rootSchema.add(SystemSchema.NAME, systemSchema);\n     return rootSchema;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/Calcites.java",
                "sha": "3601e15b66e69afeb4c9599c7e6174148f35780a",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/DruidPlanner.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/planner/DruidPlanner.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 8,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/planner/DruidPlanner.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.calcite.interpreter.BindableRel;\n import org.apache.calcite.interpreter.Bindables;\n import org.apache.calcite.linq4j.Enumerable;\n+import org.apache.calcite.linq4j.Enumerator;\n import org.apache.calcite.plan.RelOptPlanner;\n import org.apache.calcite.plan.RelOptTable;\n import org.apache.calcite.plan.RelOptUtil;\n@@ -51,6 +52,7 @@\n import org.apache.calcite.tools.ValidationException;\n import org.apache.calcite.util.Pair;\n import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n import org.apache.druid.java.util.common.guava.Sequence;\n import org.apache.druid.java.util.common.guava.Sequences;\n import org.apache.druid.server.security.Access;\n@@ -66,6 +68,7 @@\n import javax.servlet.http.HttpServletRequest;\n import java.io.Closeable;\n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Set;\n \n@@ -309,19 +312,65 @@ private PlannerResult planWithBindableConvention(\n     } else {\n       final BindableRel theRel = bindableRel;\n       final DataContext dataContext = plannerContext.createDataContext((JavaTypeFactory) planner.getTypeFactory());\n-      final Supplier<Sequence<Object[]>> resultsSupplier = new Supplier<Sequence<Object[]>>()\n-      {\n-        @Override\n-        public Sequence<Object[]> get()\n-        {\n-          final Enumerable enumerable = theRel.bind(dataContext);\n-          return Sequences.simple(enumerable);\n-        }\n+      final Supplier<Sequence<Object[]>> resultsSupplier = () -> {\n+        final Enumerable enumerable = theRel.bind(dataContext);\n+        final Enumerator enumerator = enumerable.enumerator();\n+        return Sequences.withBaggage(new BaseSequence<>(\n+            new BaseSequence.IteratorMaker<Object[], EnumeratorIterator<Object[]>>()\n+            {\n+              @Override\n+              public EnumeratorIterator make()\n+              {\n+                return new EnumeratorIterator(new Iterator<Object[]>()\n+                {\n+                  @Override\n+                  public boolean hasNext()\n+                  {\n+                    return enumerator.moveNext();\n+                  }\n+\n+                  @Override\n+                  public Object[] next()\n+                  {\n+                    return (Object[]) enumerator.current();\n+                  }\n+                });\n+              }\n+\n+              @Override\n+              public void cleanup(EnumeratorIterator iterFromMake)\n+              {\n+\n+              }\n+            }\n+        ), () -> enumerator.close());\n       };\n       return new PlannerResult(resultsSupplier, root.validatedRowType);\n     }\n   }\n \n+  private static class EnumeratorIterator<T> implements Iterator<T>\n+  {\n+    private final Iterator<T> it;\n+\n+    public EnumeratorIterator(Iterator<T> it)\n+    {\n+      this.it = it;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      return it.hasNext();\n+    }\n+\n+    @Override\n+    public T next()\n+    {\n+      return it.next();\n+    }\n+  }\n+\n   private PlannerResult planExplanation(\n       final RelNode rel,\n       final SqlExplain explain",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/DruidPlanner.java",
                "sha": "1d1c1f65b1dd3a35e09974fb2d06a74600c74716",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerFactory.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerFactory.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 1,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerFactory.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.druid.server.security.AuthorizerMapper;\n import org.apache.druid.sql.calcite.rel.QueryMaker;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n \n import java.util.Map;\n import java.util.Properties;\n@@ -57,6 +58,7 @@\n       .build();\n \n   private final DruidSchema druidSchema;\n+  private final SystemSchema systemSchema;\n   private final QueryLifecycleFactory queryLifecycleFactory;\n   private final DruidOperatorTable operatorTable;\n   private final ExprMacroTable macroTable;\n@@ -67,6 +69,7 @@\n   @Inject\n   public PlannerFactory(\n       final DruidSchema druidSchema,\n+      final SystemSchema systemSchema,\n       final QueryLifecycleFactory queryLifecycleFactory,\n       final DruidOperatorTable operatorTable,\n       final ExprMacroTable macroTable,\n@@ -76,6 +79,7 @@ public PlannerFactory(\n   )\n   {\n     this.druidSchema = druidSchema;\n+    this.systemSchema = systemSchema;\n     this.queryLifecycleFactory = queryLifecycleFactory;\n     this.operatorTable = operatorTable;\n     this.macroTable = macroTable;\n@@ -86,7 +90,11 @@ public PlannerFactory(\n \n   public DruidPlanner createPlanner(final Map<String, Object> queryContext)\n   {\n-    final SchemaPlus rootSchema = Calcites.createRootSchema(druidSchema, authorizerMapper);\n+    final SchemaPlus rootSchema = Calcites.createRootSchema(\n+        druidSchema,\n+        systemSchema,\n+        authorizerMapper\n+    );\n     final PlannerContext plannerContext = PlannerContext.create(\n         operatorTable,\n         macroTable,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerFactory.java",
                "sha": "83f90cd49e92114b6ee4bf323ae18f5fd84e8be4",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 26,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.druid.segment.column.ValueType;\n import org.apache.druid.server.QueryLifecycleFactory;\n import org.apache.druid.server.coordination.DruidServerMetadata;\n+import org.apache.druid.server.coordination.ServerType;\n import org.apache.druid.server.security.AuthenticationResult;\n import org.apache.druid.server.security.Escalator;\n import org.apache.druid.sql.calcite.planner.PlannerConfig;\n@@ -91,7 +92,6 @@\n   private static final int MAX_SEGMENTS_PER_QUERY = 15000;\n \n   private final QueryLifecycleFactory queryLifecycleFactory;\n-  private final TimelineServerView serverView;\n   private final PlannerConfig config;\n   private final ViewManager viewManager;\n   private final ExecutorService cacheExec;\n@@ -103,9 +103,10 @@\n   // Protects access to segmentSignatures, mutableSegments, segmentsNeedingRefresh, lastRefresh, isServerViewInitialized\n   private final Object lock = new Object();\n \n-  // DataSource -> Segment -> RowSignature for that segment.\n+  // DataSource -> Segment -> SegmentMetadataHolder(contains RowSignature) for that segment.\n   // Use TreeMap for segments so they are merged in deterministic order, from older to newer.\n-  private final Map<String, TreeMap<DataSegment, RowSignature>> segmentSignatures = new HashMap<>();\n+  // This data structure need to be accessed in a thread-safe way since SystemSchema accesses it\n+  private final Map<String, TreeMap<DataSegment, SegmentMetadataHolder>> segmentMetadataInfo = new HashMap<>();\n \n   // All mutable segments.\n   private final Set<DataSegment> mutableSegments = new TreeSet<>(SEGMENT_ORDER);\n@@ -134,7 +135,7 @@ public DruidSchema(\n   )\n   {\n     this.queryLifecycleFactory = Preconditions.checkNotNull(queryLifecycleFactory, \"queryLifecycleFactory\");\n-    this.serverView = Preconditions.checkNotNull(serverView, \"serverView\");\n+    Preconditions.checkNotNull(serverView, \"serverView\");\n     this.config = Preconditions.checkNotNull(config, \"config\");\n     this.viewManager = Preconditions.checkNotNull(viewManager, \"viewManager\");\n     this.cacheExec = ScheduledExecutors.fixed(1, \"DruidSchema-Cache-%d\");\n@@ -320,25 +321,39 @@ public void awaitInitialization() throws InterruptedException\n   private void addSegment(final DruidServerMetadata server, final DataSegment segment)\n   {\n     synchronized (lock) {\n-      final Map<DataSegment, RowSignature> knownSegments = segmentSignatures.get(segment.getDataSource());\n+      final Map<DataSegment, SegmentMetadataHolder> knownSegments = segmentMetadataInfo.get(segment.getDataSource());\n       if (knownSegments == null || !knownSegments.containsKey(segment)) {\n+        // segmentReplicatable is used to determine if segments are served by realtime servers or not\n+        final long isRealtime = server.segmentReplicatable() ? 0 : 1;\n+        final long isPublished = server.getType() == ServerType.HISTORICAL ? 1 : 0;\n+        SegmentMetadataHolder holder = new SegmentMetadataHolder.Builder(isPublished, 1, isRealtime, 1).build();\n         // Unknown segment.\n-        setSegmentSignature(segment, null);\n+        setSegmentSignature(segment, holder);\n         segmentsNeedingRefresh.add(segment);\n-\n         if (!server.segmentReplicatable()) {\n           log.debug(\"Added new mutable segment[%s].\", segment.getIdentifier());\n           mutableSegments.add(segment);\n         } else {\n           log.debug(\"Added new immutable segment[%s].\", segment.getIdentifier());\n         }\n-      } else if (server.segmentReplicatable()) {\n-        // If a segment shows up on a replicatable (historical) server at any point, then it must be immutable,\n-        // even if it's also available on non-replicatable (realtime) servers.\n-        mutableSegments.remove(segment);\n-        log.debug(\"Segment[%s] has become immutable.\", segment.getIdentifier());\n+      } else {\n+        if (knownSegments.containsKey(segment)) {\n+          SegmentMetadataHolder holder = knownSegments.get(segment);\n+          SegmentMetadataHolder holderWithNumReplicas = new SegmentMetadataHolder.Builder(\n+              holder.isPublished(),\n+              holder.isAvailable(),\n+              holder.isRealtime(),\n+              holder.getNumReplicas()\n+          ).withNumReplicas(holder.getNumReplicas() + 1).build();\n+          knownSegments.put(segment, holderWithNumReplicas);\n+        }\n+        if (server.segmentReplicatable()) {\n+          // If a segment shows up on a replicatable (historical) server at any point, then it must be immutable,\n+          // even if it's also available on non-replicatable (realtime) servers.\n+          mutableSegments.remove(segment);\n+          log.debug(\"Segment[%s] has become immutable.\", segment.getIdentifier());\n+        }\n       }\n-\n       if (!tables.containsKey(segment.getDataSource())) {\n         refreshImmediately = true;\n       }\n@@ -356,11 +371,11 @@ private void removeSegment(final DataSegment segment)\n       segmentsNeedingRefresh.remove(segment);\n       mutableSegments.remove(segment);\n \n-      final Map<DataSegment, RowSignature> dataSourceSegments = segmentSignatures.get(segment.getDataSource());\n+      final Map<DataSegment, SegmentMetadataHolder> dataSourceSegments = segmentMetadataInfo.get(segment.getDataSource());\n       dataSourceSegments.remove(segment);\n \n       if (dataSourceSegments.isEmpty()) {\n-        segmentSignatures.remove(segment.getDataSource());\n+        segmentMetadataInfo.remove(segment.getDataSource());\n         tables.remove(segment.getDataSource());\n         log.info(\"Removed all metadata for dataSource[%s].\", segment.getDataSource());\n       }\n@@ -431,10 +446,21 @@ private void removeSegment(final DataSegment segment)\n         if (segment == null) {\n           log.warn(\"Got analysis for segment[%s] we didn't ask for, ignoring.\", analysis.getId());\n         } else {\n-          final RowSignature rowSignature = analysisToRowSignature(analysis);\n-          log.debug(\"Segment[%s] has signature[%s].\", segment.getIdentifier(), rowSignature);\n-          setSegmentSignature(segment, rowSignature);\n-          retVal.add(segment);\n+          synchronized (lock) {\n+            final RowSignature rowSignature = analysisToRowSignature(analysis);\n+            log.debug(\"Segment[%s] has signature[%s].\", segment.getIdentifier(), rowSignature);\n+            final Map<DataSegment, SegmentMetadataHolder> dataSourceSegments = segmentMetadataInfo.get(segment.getDataSource());\n+            SegmentMetadataHolder holder = dataSourceSegments.get(segment);\n+            SegmentMetadataHolder updatedHolder = new SegmentMetadataHolder.Builder(\n+                holder.isPublished(),\n+                holder.isAvailable(),\n+                holder.isRealtime(),\n+                holder.getNumReplicas()\n+            ).withRowSignature(rowSignature).withNumRows(analysis.getNumRows()).build();\n+            dataSourceSegments.put(segment, updatedHolder);\n+            setSegmentSignature(segment, updatedHolder);\n+            retVal.add(segment);\n+          }\n         }\n \n         yielder = yielder.next(null);\n@@ -455,22 +481,23 @@ private void removeSegment(final DataSegment segment)\n     return retVal;\n   }\n \n-  private void setSegmentSignature(final DataSegment segment, final RowSignature rowSignature)\n+  private void setSegmentSignature(final DataSegment segment, final SegmentMetadataHolder segmentMetadataHolder)\n   {\n     synchronized (lock) {\n-      segmentSignatures.computeIfAbsent(segment.getDataSource(), x -> new TreeMap<>(SEGMENT_ORDER))\n-                       .put(segment, rowSignature);\n+      segmentMetadataInfo.computeIfAbsent(segment.getDataSource(), x -> new TreeMap<>(SEGMENT_ORDER))\n+                         .put(segment, segmentMetadataHolder);\n     }\n   }\n \n   private DruidTable buildDruidTable(final String dataSource)\n   {\n     synchronized (lock) {\n-      final TreeMap<DataSegment, RowSignature> segmentMap = segmentSignatures.get(dataSource);\n+      final TreeMap<DataSegment, SegmentMetadataHolder> segmentMap = segmentMetadataInfo.get(dataSource);\n       final Map<String, ValueType> columnTypes = new TreeMap<>();\n \n       if (segmentMap != null) {\n-        for (RowSignature rowSignature : segmentMap.values()) {\n+        for (SegmentMetadataHolder segmentMetadataHolder : segmentMap.values()) {\n+          final RowSignature rowSignature = segmentMetadataHolder.getRowSignature();\n           if (rowSignature != null) {\n             for (String column : rowSignature.getRowOrder()) {\n               // Newer column types should override older ones.\n@@ -520,7 +547,6 @@ private DruidTable buildDruidTable(final String dataSource)\n   private static RowSignature analysisToRowSignature(final SegmentAnalysis analysis)\n   {\n     final RowSignature.Builder rowSignatureBuilder = RowSignature.builder();\n-\n     for (Map.Entry<String, ColumnAnalysis> entry : analysis.getColumns().entrySet()) {\n       if (entry.getValue().isError()) {\n         // Skip columns with analysis errors.\n@@ -539,7 +565,17 @@ private static RowSignature analysisToRowSignature(final SegmentAnalysis analysi\n \n       rowSignatureBuilder.add(entry.getKey(), valueType);\n     }\n-\n     return rowSignatureBuilder.build();\n   }\n+\n+  public Map<DataSegment, SegmentMetadataHolder> getSegmentMetadata()\n+  {\n+    final Map<DataSegment, SegmentMetadataHolder> segmentMetadata = new HashMap<>();\n+    synchronized (lock) {\n+      for (TreeMap<DataSegment, SegmentMetadataHolder> val : segmentMetadataInfo.values()) {\n+        segmentMetadata.putAll(val);\n+      }\n+    }\n+    return segmentMetadata;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "sha": "5a64d08700c3fdfb53d4912f06c1edf293645298",
                "status": "modified"
            },
            {
                "additions": 134,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/SegmentMetadataHolder.java",
                "changes": 134,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/schema/SegmentMetadataHolder.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/schema/SegmentMetadataHolder.java",
                "patch": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.druid.sql.calcite.schema;\n+\n+import org.apache.druid.sql.calcite.table.RowSignature;\n+\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Immutable representation of RowSignature and other segment attributes needed by {@link SystemSchema.SegmentsTable}\n+ */\n+public class SegmentMetadataHolder\n+{\n+\n+  // Booleans represented as long type, where 1 = true and 0 = false\n+  // to make it easy to count number of segments which are\n+  // published, available or realtime etc.\n+  private final long isPublished;\n+  private final long isAvailable;\n+  private final long isRealtime;\n+\n+  private final long numReplicas;\n+  @Nullable\n+  private final RowSignature rowSignature;\n+  @Nullable\n+  private final Long numRows;\n+\n+  private SegmentMetadataHolder(Builder builder)\n+  {\n+    this.rowSignature = builder.rowSignature;\n+    this.isPublished = builder.isPublished;\n+    this.isAvailable = builder.isAvailable;\n+    this.isRealtime = builder.isRealtime;\n+    this.numReplicas = builder.numReplicas;\n+    this.numRows = builder.numRows;\n+  }\n+\n+  public long isPublished()\n+  {\n+    return isPublished;\n+  }\n+\n+  public long isAvailable()\n+  {\n+    return isAvailable;\n+  }\n+\n+  public long isRealtime()\n+  {\n+    return isRealtime;\n+  }\n+\n+  public long getNumReplicas()\n+  {\n+    return numReplicas;\n+  }\n+\n+  @Nullable\n+  public Long getNumRows()\n+  {\n+    return numRows;\n+  }\n+\n+  @Nullable\n+  public RowSignature getRowSignature()\n+  {\n+    return rowSignature;\n+  }\n+\n+  public static class Builder\n+  {\n+    private final long isPublished;\n+    private final long isAvailable;\n+    private final long isRealtime;\n+    private long numReplicas;\n+    @Nullable\n+    private RowSignature rowSignature;\n+    @Nullable\n+    private Long numRows;\n+\n+    public Builder(\n+        long isPublished,\n+        long isAvailable,\n+        long isRealtime,\n+        long numReplicas\n+    )\n+    {\n+      this.isPublished = isPublished;\n+      this.isAvailable = isAvailable;\n+      this.isRealtime = isRealtime;\n+      this.numReplicas = numReplicas;\n+    }\n+\n+    public Builder withRowSignature(RowSignature rowSignature)\n+    {\n+      this.rowSignature = rowSignature;\n+      return this;\n+    }\n+\n+    public Builder withNumRows(Long numRows)\n+    {\n+      this.numRows = numRows;\n+      return this;\n+    }\n+\n+    public Builder withNumReplicas(long numReplicas)\n+    {\n+      this.numReplicas = numReplicas;\n+      return this;\n+    }\n+\n+    public SegmentMetadataHolder build()\n+    {\n+      return new SegmentMetadataHolder(this);\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/SegmentMetadataHolder.java",
                "sha": "f3b13bb80957af0bc500620d893bc28ca2e08833",
                "status": "added"
            },
            {
                "additions": 658,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/SystemSchema.java",
                "changes": 658,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/schema/SystemSchema.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/schema/SystemSchema.java",
                "patch": "@@ -0,0 +1,658 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.druid.sql.calcite.schema;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.JavaType;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Function;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.util.concurrent.ListenableFuture;\n+import com.google.inject.Inject;\n+import org.apache.calcite.DataContext;\n+import org.apache.calcite.linq4j.DefaultEnumerable;\n+import org.apache.calcite.linq4j.Enumerable;\n+import org.apache.calcite.linq4j.Enumerator;\n+import org.apache.calcite.linq4j.Linq4j;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.schema.ScannableTable;\n+import org.apache.calcite.schema.Table;\n+import org.apache.calcite.schema.impl.AbstractSchema;\n+import org.apache.calcite.schema.impl.AbstractTable;\n+import org.apache.druid.client.ImmutableDruidServer;\n+import org.apache.druid.client.JsonParserIterator;\n+import org.apache.druid.client.TimelineServerView;\n+import org.apache.druid.client.coordinator.Coordinator;\n+import org.apache.druid.client.indexing.IndexingService;\n+import org.apache.druid.discovery.DruidLeaderClient;\n+import org.apache.druid.indexer.TaskStatusPlus;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.http.client.Request;\n+import org.apache.druid.segment.column.ValueType;\n+import org.apache.druid.server.coordinator.BytesAccumulatingResponseHandler;\n+import org.apache.druid.server.security.Access;\n+import org.apache.druid.server.security.Action;\n+import org.apache.druid.server.security.AuthenticationResult;\n+import org.apache.druid.server.security.AuthorizationUtils;\n+import org.apache.druid.server.security.AuthorizerMapper;\n+import org.apache.druid.server.security.ForbiddenException;\n+import org.apache.druid.server.security.Resource;\n+import org.apache.druid.server.security.ResourceAction;\n+import org.apache.druid.server.security.ResourceType;\n+import org.apache.druid.sql.calcite.planner.PlannerContext;\n+import org.apache.druid.sql.calcite.table.RowSignature;\n+import org.apache.druid.timeline.DataSegment;\n+import org.jboss.netty.handler.codec.http.HttpMethod;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+\n+public class SystemSchema extends AbstractSchema\n+{\n+  private static final Logger log = new Logger(SystemSchema.class);\n+\n+  public static final String NAME = \"sys\";\n+  private static final String SEGMENTS_TABLE = \"segments\";\n+  private static final String SERVERS_TABLE = \"servers\";\n+  private static final String SERVER_SEGMENTS_TABLE = \"server_segments\";\n+  private static final String TASKS_TABLE = \"tasks\";\n+\n+  private static final RowSignature SEGMENTS_SIGNATURE = RowSignature\n+      .builder()\n+      .add(\"segment_id\", ValueType.STRING)\n+      .add(\"datasource\", ValueType.STRING)\n+      .add(\"start\", ValueType.STRING)\n+      .add(\"end\", ValueType.STRING)\n+      .add(\"size\", ValueType.LONG)\n+      .add(\"version\", ValueType.STRING)\n+      .add(\"partition_num\", ValueType.STRING)\n+      .add(\"num_replicas\", ValueType.LONG)\n+      .add(\"num_rows\", ValueType.LONG)\n+      .add(\"is_published\", ValueType.LONG)\n+      .add(\"is_available\", ValueType.LONG)\n+      .add(\"is_realtime\", ValueType.LONG)\n+      .add(\"payload\", ValueType.STRING)\n+      .build();\n+\n+  private static final RowSignature SERVERS_SIGNATURE = RowSignature\n+      .builder()\n+      .add(\"server\", ValueType.STRING)\n+      .add(\"host\", ValueType.STRING)\n+      .add(\"plaintext_port\", ValueType.STRING)\n+      .add(\"tls_port\", ValueType.STRING)\n+      .add(\"server_type\", ValueType.STRING)\n+      .add(\"tier\", ValueType.STRING)\n+      .add(\"curr_size\", ValueType.LONG)\n+      .add(\"max_size\", ValueType.LONG)\n+      .build();\n+\n+  private static final RowSignature SERVER_SEGMENTS_SIGNATURE = RowSignature\n+      .builder()\n+      .add(\"server\", ValueType.STRING)\n+      .add(\"segment_id\", ValueType.STRING)\n+      .build();\n+\n+  private static final RowSignature TASKS_SIGNATURE = RowSignature\n+      .builder()\n+      .add(\"task_id\", ValueType.STRING)\n+      .add(\"type\", ValueType.STRING)\n+      .add(\"datasource\", ValueType.STRING)\n+      .add(\"created_time\", ValueType.STRING)\n+      .add(\"queue_insertion_time\", ValueType.STRING)\n+      .add(\"status\", ValueType.STRING)\n+      .add(\"runner_status\", ValueType.STRING)\n+      .add(\"duration\", ValueType.LONG)\n+      .add(\"location\", ValueType.STRING)\n+      .add(\"host\", ValueType.STRING)\n+      .add(\"plaintext_port\", ValueType.STRING)\n+      .add(\"tls_port\", ValueType.STRING)\n+      .add(\"error_msg\", ValueType.STRING)\n+      .build();\n+\n+  private final Map<String, Table> tableMap;\n+\n+  @Inject\n+  public SystemSchema(\n+      final DruidSchema druidSchema,\n+      final TimelineServerView serverView,\n+      final AuthorizerMapper authorizerMapper,\n+      final @Coordinator DruidLeaderClient coordinatorDruidLeaderClient,\n+      final @IndexingService DruidLeaderClient overlordDruidLeaderClient,\n+      final ObjectMapper jsonMapper\n+  )\n+  {\n+    Preconditions.checkNotNull(serverView, \"serverView\");\n+    BytesAccumulatingResponseHandler responseHandler = new BytesAccumulatingResponseHandler();\n+    this.tableMap = ImmutableMap.of(\n+        SEGMENTS_TABLE, new SegmentsTable(druidSchema, coordinatorDruidLeaderClient, jsonMapper, responseHandler, authorizerMapper),\n+        SERVERS_TABLE, new ServersTable(serverView, authorizerMapper),\n+        SERVER_SEGMENTS_TABLE, new ServerSegmentsTable(serverView, authorizerMapper),\n+        TASKS_TABLE, new TasksTable(overlordDruidLeaderClient, jsonMapper, responseHandler, authorizerMapper)\n+    );\n+  }\n+\n+  @Override\n+  public Map<String, Table> getTableMap()\n+  {\n+    return tableMap;\n+  }\n+\n+  static class SegmentsTable extends AbstractTable implements ScannableTable\n+  {\n+    private final DruidSchema druidSchema;\n+    private final DruidLeaderClient druidLeaderClient;\n+    private final ObjectMapper jsonMapper;\n+    private final BytesAccumulatingResponseHandler responseHandler;\n+    private final AuthorizerMapper authorizerMapper;\n+\n+    public SegmentsTable(\n+        DruidSchema druidSchemna,\n+        DruidLeaderClient druidLeaderClient,\n+        ObjectMapper jsonMapper,\n+        BytesAccumulatingResponseHandler responseHandler,\n+        AuthorizerMapper authorizerMapper\n+    )\n+    {\n+      this.druidSchema = druidSchemna;\n+      this.druidLeaderClient = druidLeaderClient;\n+      this.jsonMapper = jsonMapper;\n+      this.responseHandler = responseHandler;\n+      this.authorizerMapper = authorizerMapper;\n+    }\n+\n+    @Override\n+    public RelDataType getRowType(RelDataTypeFactory typeFactory)\n+    {\n+      return SEGMENTS_SIGNATURE.getRelDataType(typeFactory);\n+    }\n+\n+    @Override\n+    public TableType getJdbcTableType()\n+    {\n+      return TableType.SYSTEM_TABLE;\n+    }\n+\n+    @Override\n+    public Enumerable<Object[]> scan(DataContext root)\n+    {\n+      //get available segments from druidSchema\n+      final Map<DataSegment, SegmentMetadataHolder> availableSegmentMetadata = druidSchema.getSegmentMetadata();\n+      final Iterator<Entry<DataSegment, SegmentMetadataHolder>> availableSegmentEntries = availableSegmentMetadata.entrySet()\n+                                                                                                                  .iterator();\n+\n+      //get published segments from coordinator\n+      final JsonParserIterator<DataSegment> metadataSegments = getMetadataSegments(\n+          druidLeaderClient,\n+          jsonMapper,\n+          responseHandler\n+      );\n+\n+      Set<String> availableSegmentIds = new HashSet<>();\n+      //auth check for available segments\n+      final Iterator<Entry<DataSegment, SegmentMetadataHolder>> authorizedAvailableSegments = getAuthorizedAvailableSegments(\n+          availableSegmentEntries,\n+          root\n+      );\n+\n+      final FluentIterable<Object[]> availableSegments = FluentIterable\n+          .from(() -> authorizedAvailableSegments)\n+          .transform(val -> {\n+            try {\n+              if (!availableSegmentIds.contains(val.getKey().getIdentifier())) {\n+                availableSegmentIds.add(val.getKey().getIdentifier());\n+              }\n+              return new Object[]{\n+                  val.getKey().getIdentifier(),\n+                  val.getKey().getDataSource(),\n+                  val.getKey().getInterval().getStart(),\n+                  val.getKey().getInterval().getEnd(),\n+                  val.getKey().getSize(),\n+                  val.getKey().getVersion(),\n+                  val.getKey().getShardSpec().getPartitionNum(),\n+                  val.getValue().getNumReplicas(),\n+                  val.getValue().getNumRows(),\n+                  val.getValue().isPublished(),\n+                  val.getValue().isAvailable(),\n+                  val.getValue().isRealtime(),\n+                  jsonMapper.writeValueAsString(val.getKey())\n+              };\n+            }\n+            catch (JsonProcessingException e) {\n+              throw new RuntimeException(StringUtils.format(\n+                  \"Error getting segment payload for segment %s\",\n+                  val.getKey().getIdentifier()\n+              ), e);\n+            }\n+          });\n+\n+      //auth check for published segments\n+      final CloseableIterator<DataSegment> authorizedPublishedSegments = getAuthorizedPublishedSegments(\n+          metadataSegments,\n+          root\n+      );\n+      final FluentIterable<Object[]> publishedSegments = FluentIterable\n+          .from(() -> authorizedPublishedSegments)\n+          .transform(val -> {\n+            try {\n+              if (availableSegmentIds.contains(val.getIdentifier())) {\n+                return null;\n+              }\n+              return new Object[]{\n+                  val.getIdentifier(),\n+                  val.getDataSource(),\n+                  val.getInterval().getStart(),\n+                  val.getInterval().getEnd(),\n+                  val.getSize(),\n+                  val.getVersion(),\n+                  val.getShardSpec().getPartitionNum(),\n+                  0L,\n+                  -1L,\n+                  1L,\n+                  0L,\n+                  0L,\n+                  jsonMapper.writeValueAsString(val)\n+              };\n+            }\n+            catch (JsonProcessingException e) {\n+              throw new RuntimeException(StringUtils.format(\n+                  \"Error getting segment payload for segment %s\",\n+                  val.getIdentifier()\n+              ), e);\n+            }\n+          });\n+\n+      final Iterable<Object[]> allSegments = Iterables.unmodifiableIterable(\n+          Iterables.concat(availableSegments, publishedSegments));\n+\n+      return Linq4j.asEnumerable(allSegments).where(t -> t != null);\n+\n+    }\n+\n+    private Iterator<Entry<DataSegment, SegmentMetadataHolder>> getAuthorizedAvailableSegments(\n+        Iterator<Entry<DataSegment, SegmentMetadataHolder>> availableSegmentEntries,\n+        DataContext root\n+    )\n+    {\n+      final AuthenticationResult authenticationResult =\n+          (AuthenticationResult) root.get(PlannerContext.DATA_CTX_AUTHENTICATION_RESULT);\n+\n+      Function<Entry<DataSegment, SegmentMetadataHolder>, Iterable<ResourceAction>> raGenerator = segment -> Collections\n+          .singletonList(AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(segment.getKey().getDataSource()));\n+\n+      final Iterable<Entry<DataSegment, SegmentMetadataHolder>> authorizedSegments = AuthorizationUtils.filterAuthorizedResources(\n+          authenticationResult, () -> availableSegmentEntries, raGenerator, authorizerMapper);\n+\n+      return authorizedSegments.iterator();\n+    }\n+\n+    private CloseableIterator<DataSegment> getAuthorizedPublishedSegments(\n+        JsonParserIterator<DataSegment> it,\n+        DataContext root\n+    )\n+    {\n+      final AuthenticationResult authenticationResult =\n+          (AuthenticationResult) root.get(PlannerContext.DATA_CTX_AUTHENTICATION_RESULT);\n+\n+      Function<DataSegment, Iterable<ResourceAction>> raGenerator = segment -> Collections.singletonList(\n+          AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(segment.getDataSource()));\n+\n+      final Iterable<DataSegment> authorizedSegments = AuthorizationUtils.filterAuthorizedResources(\n+          authenticationResult, () -> it, raGenerator, authorizerMapper);\n+\n+      return wrap(authorizedSegments.iterator(), it);\n+    }\n+  }\n+\n+  // Note that coordinator must be up to get segments\n+  private static JsonParserIterator<DataSegment> getMetadataSegments(\n+      DruidLeaderClient coordinatorClient,\n+      ObjectMapper jsonMapper,\n+      BytesAccumulatingResponseHandler responseHandler\n+  )\n+  {\n+\n+    Request request;\n+    try {\n+      request = coordinatorClient.makeRequest(\n+          HttpMethod.GET,\n+          StringUtils.format(\"/druid/coordinator/v1/metadata/segments\")\n+      );\n+    }\n+    catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    ListenableFuture<InputStream> future = coordinatorClient.goAsync(\n+        request,\n+        responseHandler\n+    );\n+    final JavaType typeRef = jsonMapper.getTypeFactory().constructType(new TypeReference<DataSegment>()\n+    {\n+    });\n+    return new JsonParserIterator<>(\n+        typeRef,\n+        future,\n+        request.getUrl().toString(),\n+        null,\n+        request.getUrl().getHost(),\n+        jsonMapper\n+    );\n+  }\n+\n+  static class ServersTable extends AbstractTable implements ScannableTable\n+  {\n+    private final TimelineServerView serverView;\n+    private final AuthorizerMapper authorizerMapper;\n+\n+    public ServersTable(TimelineServerView serverView, AuthorizerMapper authorizerMapper)\n+    {\n+      this.serverView = serverView;\n+      this.authorizerMapper = authorizerMapper;\n+    }\n+\n+    @Override\n+    public RelDataType getRowType(RelDataTypeFactory typeFactory)\n+    {\n+      return SERVERS_SIGNATURE.getRelDataType(typeFactory);\n+    }\n+\n+    @Override\n+    public TableType getJdbcTableType()\n+    {\n+      return TableType.SYSTEM_TABLE;\n+    }\n+\n+    @Override\n+    public Enumerable<Object[]> scan(DataContext root)\n+    {\n+      final List<ImmutableDruidServer> druidServers = serverView.getDruidServers();\n+      final AuthenticationResult authenticationResult =\n+          (AuthenticationResult) root.get(PlannerContext.DATA_CTX_AUTHENTICATION_RESULT);\n+      final Access access = AuthorizationUtils.authorizeAllResourceActions(\n+          authenticationResult,\n+          Collections.singletonList(new ResourceAction(new Resource(\"STATE\", ResourceType.STATE), Action.READ)),\n+          authorizerMapper\n+      );\n+      if (!access.isAllowed()) {\n+        throw new ForbiddenException(\"Insufficient permission to view servers :\" + access.toString());\n+      }\n+      final FluentIterable<Object[]> results = FluentIterable\n+          .from(druidServers)\n+          .transform(val -> new Object[]{\n+              val.getHost(),\n+              val.getHost().split(\":\")[0],\n+              val.getHostAndPort() == null ? -1 : val.getHostAndPort().split(\":\")[1],\n+              val.getHostAndTlsPort() == null ? -1 : val.getHostAndTlsPort().split(\":\")[1],\n+              val.getType(),\n+              val.getTier(),\n+              val.getCurrSize(),\n+              val.getMaxSize()\n+          });\n+      return Linq4j.asEnumerable(results);\n+    }\n+  }\n+\n+  private static class ServerSegmentsTable extends AbstractTable implements ScannableTable\n+  {\n+    private final TimelineServerView serverView;\n+    final AuthorizerMapper authorizerMapper;\n+\n+    public ServerSegmentsTable(TimelineServerView serverView, AuthorizerMapper authorizerMapper)\n+    {\n+      this.serverView = serverView;\n+      this.authorizerMapper = authorizerMapper;\n+    }\n+\n+    @Override\n+    public RelDataType getRowType(RelDataTypeFactory typeFactory)\n+    {\n+      return SERVER_SEGMENTS_SIGNATURE.getRelDataType(typeFactory);\n+    }\n+\n+    @Override\n+    public TableType getJdbcTableType()\n+    {\n+      return TableType.SYSTEM_TABLE;\n+    }\n+\n+    @Override\n+    public Enumerable<Object[]> scan(DataContext root)\n+    {\n+      final List<Object[]> rows = new ArrayList<>();\n+      final List<ImmutableDruidServer> druidServers = serverView.getDruidServers();\n+      final int serverSegmentsTableSize = SERVER_SEGMENTS_SIGNATURE.getRowOrder().size();\n+      for (ImmutableDruidServer druidServer : druidServers) {\n+        final Map<String, DataSegment> segmentMap = druidServer.getSegments();\n+        for (DataSegment segment : segmentMap.values()) {\n+          Object[] row = new Object[serverSegmentsTableSize];\n+          row[0] = druidServer.getHost();\n+          row[1] = segment.getIdentifier();\n+          rows.add(row);\n+        }\n+      }\n+      return Linq4j.asEnumerable(rows);\n+    }\n+  }\n+\n+  static class TasksTable extends AbstractTable implements ScannableTable\n+  {\n+    private final DruidLeaderClient druidLeaderClient;\n+    private final ObjectMapper jsonMapper;\n+    private final BytesAccumulatingResponseHandler responseHandler;\n+    private final AuthorizerMapper authorizerMapper;\n+\n+    public TasksTable(\n+        DruidLeaderClient druidLeaderClient,\n+        ObjectMapper jsonMapper,\n+        BytesAccumulatingResponseHandler responseHandler,\n+        AuthorizerMapper authorizerMapper\n+    )\n+    {\n+      this.druidLeaderClient = druidLeaderClient;\n+      this.jsonMapper = jsonMapper;\n+      this.responseHandler = responseHandler;\n+      this.authorizerMapper = authorizerMapper;\n+    }\n+\n+    @Override\n+    public RelDataType getRowType(RelDataTypeFactory typeFactory)\n+    {\n+      return TASKS_SIGNATURE.getRelDataType(typeFactory);\n+    }\n+\n+    @Override\n+    public TableType getJdbcTableType()\n+    {\n+      return TableType.SYSTEM_TABLE;\n+    }\n+\n+    @Override\n+    public Enumerable<Object[]> scan(DataContext root)\n+    {\n+      class TasksEnumerable extends DefaultEnumerable<Object[]>\n+      {\n+        private final CloseableIterator<TaskStatusPlus> it;\n+\n+        public TasksEnumerable(JsonParserIterator<TaskStatusPlus> tasks)\n+        {\n+          this.it = getAuthorizedTasks(tasks, root);\n+        }\n+\n+        @Override\n+        public Iterator<Object[]> iterator()\n+        {\n+          throw new UnsupportedOperationException(\"Do not use iterator(), it cannot be closed.\");\n+        }\n+\n+        @Override\n+        public Enumerator<Object[]> enumerator()\n+        {\n+          return new Enumerator<Object[]>()\n+          {\n+            @Override\n+            public Object[] current()\n+            {\n+              TaskStatusPlus task = it.next();\n+              return new Object[]{task.getId(),\n+                                  task.getType(),\n+                                  task.getDataSource(),\n+                                  task.getCreatedTime(),\n+                                  task.getQueueInsertionTime(),\n+                                  task.getStatusCode(),\n+                                  task.getRunnerStatusCode(),\n+                                  task.getDuration(),\n+                                  task.getLocation().getHost() + \":\" + (task.getLocation().getTlsPort()\n+                                                                          == -1\n+                                                                          ? task.getLocation()\n+                                                                                .getPort()\n+                                                                          : task.getLocation().getTlsPort()),\n+                                  task.getLocation().getHost(),\n+                                  task.getLocation().getPort(),\n+                                  task.getLocation().getTlsPort(),\n+                                  task.getErrorMsg()};\n+            }\n+\n+            @Override\n+            public boolean moveNext()\n+            {\n+              return it.hasNext();\n+            }\n+\n+            @Override\n+            public void reset()\n+            {\n+\n+            }\n+\n+            @Override\n+            public void close()\n+            {\n+              try {\n+                it.close();\n+              }\n+              catch (IOException e) {\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+        }\n+      }\n+\n+      return new TasksEnumerable(getTasks(druidLeaderClient, jsonMapper, responseHandler));\n+    }\n+\n+    private CloseableIterator<TaskStatusPlus> getAuthorizedTasks(JsonParserIterator<TaskStatusPlus> it, DataContext root)\n+    {\n+      final AuthenticationResult authenticationResult =\n+          (AuthenticationResult) root.get(PlannerContext.DATA_CTX_AUTHENTICATION_RESULT);\n+\n+      Function<TaskStatusPlus, Iterable<ResourceAction>> raGenerator = task -> Collections.singletonList(\n+          AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(task.getDataSource()));\n+\n+      final Iterable<TaskStatusPlus> authorizedTasks = AuthorizationUtils.filterAuthorizedResources(\n+          authenticationResult, () -> it, raGenerator, authorizerMapper);\n+\n+      return wrap(authorizedTasks.iterator(), it);\n+    }\n+\n+  }\n+\n+  //Note that overlord must be up to get tasks\n+  private static JsonParserIterator<TaskStatusPlus> getTasks(\n+      DruidLeaderClient indexingServiceClient,\n+      ObjectMapper jsonMapper,\n+      BytesAccumulatingResponseHandler responseHandler\n+  )\n+  {\n+\n+    Request request;\n+    try {\n+      request = indexingServiceClient.makeRequest(\n+          HttpMethod.GET,\n+          StringUtils.format(\"/druid/indexer/v1/tasks\")\n+      );\n+    }\n+    catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    ListenableFuture<InputStream> future = indexingServiceClient.goAsync(\n+        request,\n+        responseHandler\n+    );\n+    final JavaType typeRef = jsonMapper.getTypeFactory().constructType(new TypeReference<TaskStatusPlus>()\n+    {\n+    });\n+    return new JsonParserIterator<>(\n+        typeRef,\n+        future,\n+        request.getUrl().toString(),\n+        null,\n+        request.getUrl().getHost(),\n+        jsonMapper\n+    );\n+  }\n+\n+  private static <T> CloseableIterator<T> wrap(Iterator<T> iterator, JsonParserIterator<T> it)\n+  {\n+    return new CloseableIterator<T>()\n+    {\n+      @Override\n+      public boolean hasNext()\n+      {\n+        final boolean hasNext = iterator.hasNext();\n+        if (!hasNext) {\n+          try {\n+            it.close();\n+          }\n+          catch (IOException e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+        return hasNext;\n+      }\n+\n+      @Override\n+      public T next()\n+      {\n+        return iterator.next();\n+      }\n+\n+      @Override\n+      public void close() throws IOException\n+      {\n+        it.close();\n+      }\n+    };\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/main/java/org/apache/druid/sql/calcite/schema/SystemSchema.java",
                "sha": "da34bbeeacd777b8b1b5f6b5941aaddf9d7470d7",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/avatica/DruidAvaticaHandlerTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/avatica/DruidAvaticaHandlerTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/avatica/DruidAvaticaHandlerTest.java",
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.druid.sql.calcite.planner.PlannerConfig;\n import org.apache.druid.sql.calcite.planner.PlannerFactory;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTestBase;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n@@ -153,6 +154,7 @@ public void setUp() throws Exception\n     walker = CalciteTests.createMockWalker(conglomerate, temporaryFolder.newFolder());\n     final PlannerConfig plannerConfig = new PlannerConfig();\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = CalciteTests.createOperatorTable();\n     final ExprMacroTable macroTable = CalciteTests.createExprMacroTable();\n \n@@ -177,6 +179,7 @@ public void configure(Binder binder)\n     druidMeta = new DruidMeta(\n         new PlannerFactory(\n             druidSchema,\n+            systemSchema,\n             CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n             operatorTable,\n             macroTable,\n@@ -752,12 +755,14 @@ public int getMaxRowsPerFrame()\n \n     final PlannerConfig plannerConfig = new PlannerConfig();\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = CalciteTests.createOperatorTable();\n     final ExprMacroTable macroTable = CalciteTests.createExprMacroTable();\n     final List<Meta.Frame> frames = new ArrayList<>();\n     DruidMeta smallFrameDruidMeta = new DruidMeta(\n         new PlannerFactory(\n             druidSchema,\n+            systemSchema,\n             CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n             operatorTable,\n             macroTable,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/avatica/DruidAvaticaHandlerTest.java",
                "sha": "4831421cdf8aba36c2e675e5b240abd864265ddf",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/avatica/DruidStatementTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/avatica/DruidStatementTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/avatica/DruidStatementTest.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.druid.sql.calcite.planner.PlannerConfig;\n import org.apache.druid.sql.calcite.planner.PlannerFactory;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTestBase;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n@@ -86,10 +87,12 @@ public void setUp() throws Exception\n     walker = CalciteTests.createMockWalker(conglomerate, temporaryFolder.newFolder());\n     final PlannerConfig plannerConfig = new PlannerConfig();\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = CalciteTests.createOperatorTable();\n     final ExprMacroTable macroTable = CalciteTests.createExprMacroTable();\n     plannerFactory = new PlannerFactory(\n         druidSchema,\n+        systemSchema,\n         CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n         operatorTable,\n         macroTable,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/avatica/DruidStatementTest.java",
                "sha": "77a44dc05974bbac37eecadc1340d6d39d85b840",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 2,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "patch": "@@ -106,6 +106,7 @@\n import org.apache.druid.sql.calcite.planner.PlannerResult;\n import org.apache.druid.sql.calcite.rel.CannotBuildQueryException;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTestBase;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n@@ -468,6 +469,7 @@ public void testInformationSchemaSchemata() throws Exception\n         ImmutableList.of(),\n         ImmutableList.of(\n             new Object[]{\"druid\"},\n+            new Object[]{\"sys\"},\n             new Object[]{\"INFORMATION_SCHEMA\"}\n         )\n     );\n@@ -488,7 +490,11 @@ public void testInformationSchemaTables() throws Exception\n             new Object[]{\"druid\", \"bview\", \"VIEW\"},\n             new Object[]{\"INFORMATION_SCHEMA\", \"COLUMNS\", \"SYSTEM_TABLE\"},\n             new Object[]{\"INFORMATION_SCHEMA\", \"SCHEMATA\", \"SYSTEM_TABLE\"},\n-            new Object[]{\"INFORMATION_SCHEMA\", \"TABLES\", \"SYSTEM_TABLE\"}\n+            new Object[]{\"INFORMATION_SCHEMA\", \"TABLES\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"segments\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"server_segments\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"servers\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"tasks\", \"SYSTEM_TABLE\"}\n         )\n     );\n \n@@ -507,7 +513,11 @@ public void testInformationSchemaTables() throws Exception\n             new Object[]{\"druid\", \"bview\", \"VIEW\"},\n             new Object[]{\"INFORMATION_SCHEMA\", \"COLUMNS\", \"SYSTEM_TABLE\"},\n             new Object[]{\"INFORMATION_SCHEMA\", \"SCHEMATA\", \"SYSTEM_TABLE\"},\n-            new Object[]{\"INFORMATION_SCHEMA\", \"TABLES\", \"SYSTEM_TABLE\"}\n+            new Object[]{\"INFORMATION_SCHEMA\", \"TABLES\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"segments\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"server_segments\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"servers\", \"SYSTEM_TABLE\"},\n+            new Object[]{\"sys\", \"tasks\", \"SYSTEM_TABLE\"}\n         )\n     );\n   }\n@@ -7710,11 +7720,13 @@ private void testQuery(\n   {\n     final InProcessViewManager viewManager = new InProcessViewManager(CalciteTests.TEST_AUTHENTICATOR_ESCALATOR);\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig, viewManager);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = CalciteTests.createOperatorTable();\n     final ExprMacroTable macroTable = CalciteTests.createExprMacroTable();\n \n     final PlannerFactory plannerFactory = new PlannerFactory(\n         druidSchema,\n+        systemSchema,\n         CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n         operatorTable,\n         macroTable,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "sha": "81bb09129b43efa4e5f840d0ff75f958ab9d410c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.druid.sql.calcite.planner.PlannerContext;\n import org.apache.druid.sql.calcite.planner.PlannerFactory;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.util.CalciteTestBase;\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n@@ -113,6 +114,7 @@ public void setUp() throws Exception\n \n     final PlannerConfig plannerConfig = new PlannerConfig();\n     final DruidSchema druidSchema = CalciteTests.createMockSchema(conglomerate, walker, plannerConfig);\n+    final SystemSchema systemSchema = CalciteTests.createMockSystemSchema(druidSchema, walker);\n     final DruidOperatorTable operatorTable = CalciteTests.createOperatorTable();\n     final ExprMacroTable macroTable = CalciteTests.createExprMacroTable();\n     req = EasyMock.createStrictMock(HttpServletRequest.class);\n@@ -134,6 +136,7 @@ public void setUp() throws Exception\n         JSON_MAPPER,\n         new PlannerFactory(\n             druidSchema,\n+            systemSchema,\n             CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n             operatorTable,\n             macroTable,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/http/SqlResourceTest.java",
                "sha": "59374e323bae00f959300c88e8b1d27c05c88ce1",
                "status": "modified"
            },
            {
                "additions": 632,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/schema/SystemSchemaTest.java",
                "changes": 632,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/schema/SystemSchemaTest.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/schema/SystemSchemaTest.java",
                "patch": "@@ -0,0 +1,632 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.druid.sql.calcite.schema;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.SettableFuture;\n+import org.apache.calcite.DataContext;\n+import org.apache.calcite.adapter.java.JavaTypeFactory;\n+import org.apache.calcite.jdbc.JavaTypeFactoryImpl;\n+import org.apache.calcite.linq4j.Enumerable;\n+import org.apache.calcite.linq4j.Enumerator;\n+import org.apache.calcite.linq4j.QueryProvider;\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.schema.SchemaPlus;\n+import org.apache.calcite.schema.Table;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.ImmutableDruidServer;\n+import org.apache.druid.client.TimelineServerView;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.discovery.DruidLeaderClient;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.Intervals;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.http.client.HttpClient;\n+import org.apache.druid.java.util.http.client.Request;\n+import org.apache.druid.java.util.http.client.io.AppendableByteArrayInputStream;\n+import org.apache.druid.java.util.http.client.response.FullResponseHolder;\n+import org.apache.druid.java.util.http.client.response.HttpResponseHandler;\n+import org.apache.druid.query.QueryRunnerFactoryConglomerate;\n+import org.apache.druid.query.QueryRunnerTestHelper;\n+import org.apache.druid.query.ReflectionQueryToolChestWarehouse;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.aggregation.DoubleSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.LongSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.hyperloglog.HyperUniquesAggregatorFactory;\n+import org.apache.druid.segment.IndexBuilder;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n+import org.apache.druid.segment.writeout.OffHeapMemorySegmentWriteOutMediumFactory;\n+import org.apache.druid.server.coordination.DruidServerMetadata;\n+import org.apache.druid.server.coordination.ServerType;\n+import org.apache.druid.server.coordinator.BytesAccumulatingResponseHandler;\n+import org.apache.druid.server.metrics.NoopServiceEmitter;\n+import org.apache.druid.server.security.Access;\n+import org.apache.druid.server.security.Authorizer;\n+import org.apache.druid.server.security.AuthorizerMapper;\n+import org.apache.druid.server.security.NoopEscalator;\n+import org.apache.druid.sql.calcite.planner.PlannerConfig;\n+import org.apache.druid.sql.calcite.util.CalciteTestBase;\n+import org.apache.druid.sql.calcite.util.CalciteTests;\n+import org.apache.druid.sql.calcite.util.SpecificSegmentsQuerySegmentWalker;\n+import org.apache.druid.sql.calcite.util.TestServerInventoryView;\n+import org.apache.druid.sql.calcite.view.NoopViewManager;\n+import org.apache.druid.timeline.DataSegment;\n+import org.easymock.EasyMock;\n+import org.jboss.netty.handler.codec.http.HttpMethod;\n+import org.jboss.netty.handler.codec.http.HttpResponse;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SystemSchemaTest extends CalciteTestBase\n+{\n+  private static final PlannerConfig PLANNER_CONFIG_DEFAULT = new PlannerConfig();\n+\n+  private static final List<InputRow> ROWS1 = ImmutableList.of(\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2000-01-01\", \"m1\", \"1.0\", \"dim1\", \"\")),\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2000-01-02\", \"m1\", \"2.0\", \"dim1\", \"10.1\")),\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2000-01-03\", \"m1\", \"3.0\", \"dim1\", \"2\"))\n+  );\n+\n+  private static final List<InputRow> ROWS2 = ImmutableList.of(\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2001-01-01\", \"m1\", \"4.0\", \"dim2\", ImmutableList.of(\"a\"))),\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2001-01-02\", \"m1\", \"5.0\", \"dim2\", ImmutableList.of(\"abc\"))),\n+      CalciteTests.createRow(ImmutableMap.of(\"t\", \"2001-01-03\", \"m1\", \"6.0\"))\n+  );\n+\n+  private SystemSchema schema;\n+  private SpecificSegmentsQuerySegmentWalker walker;\n+  private DruidLeaderClient client;\n+  private TimelineServerView serverView;\n+  private ObjectMapper mapper;\n+  private FullResponseHolder responseHolder;\n+  private BytesAccumulatingResponseHandler responseHandler;\n+  private Request request;\n+  private DruidSchema druidSchema;\n+  private AuthorizerMapper authMapper;\n+  private static QueryRunnerFactoryConglomerate conglomerate;\n+  private static Closer resourceCloser;\n+\n+  @Rule\n+  public TemporaryFolder temporaryFolder = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void setUpClass()\n+  {\n+    final Pair<QueryRunnerFactoryConglomerate, Closer> conglomerateCloserPair = CalciteTests\n+        .createQueryRunnerFactoryConglomerate();\n+    conglomerate = conglomerateCloserPair.lhs;\n+    resourceCloser = conglomerateCloserPair.rhs;\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    resourceCloser.close();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception\n+  {\n+    serverView = EasyMock.createNiceMock(TimelineServerView.class);\n+    client = EasyMock.createMock(DruidLeaderClient.class);\n+    mapper = TestHelper.makeJsonMapper();\n+    responseHolder = EasyMock.createMock(FullResponseHolder.class);\n+    responseHandler = EasyMock.createMockBuilder(BytesAccumulatingResponseHandler.class)\n+                              .withConstructor()\n+                              .addMockedMethod(\n+                                      \"handleResponse\",\n+                                      HttpResponse.class,\n+                                      HttpResponseHandler.TrafficCop.class\n+                                  )\n+                              .addMockedMethod(\"getStatus\")\n+                              .createMock();\n+    request = EasyMock.createMock(Request.class);\n+    authMapper = new AuthorizerMapper(null)\n+    {\n+      @Override\n+      public Authorizer getAuthorizer(String name)\n+      {\n+        return (authenticationResult, resource, action) -> new Access(true);\n+      }\n+    };\n+\n+    final File tmpDir = temporaryFolder.newFolder();\n+    final QueryableIndex index1 = IndexBuilder.create()\n+                                              .tmpDir(new File(tmpDir, \"1\"))\n+                                              .segmentWriteOutMediumFactory(OffHeapMemorySegmentWriteOutMediumFactory.instance())\n+                                              .schema(\n+                                                  new IncrementalIndexSchema.Builder()\n+                                                      .withMetrics(\n+                                                          new CountAggregatorFactory(\"cnt\"),\n+                                                          new DoubleSumAggregatorFactory(\"m1\", \"m1\"),\n+                                                          new HyperUniquesAggregatorFactory(\"unique_dim1\", \"dim1\")\n+                                                      )\n+                                                      .withRollup(false)\n+                                                      .build()\n+                                              )\n+                                              .rows(ROWS1)\n+                                              .buildMMappedIndex();\n+\n+    final QueryableIndex index2 = IndexBuilder.create()\n+                                              .tmpDir(new File(tmpDir, \"2\"))\n+                                              .segmentWriteOutMediumFactory(OffHeapMemorySegmentWriteOutMediumFactory.instance())\n+                                              .schema(\n+                                                  new IncrementalIndexSchema.Builder()\n+                                                      .withMetrics(new LongSumAggregatorFactory(\"m1\", \"m1\"))\n+                                                      .withRollup(false)\n+                                                      .build()\n+                                              )\n+                                              .rows(ROWS2)\n+                                              .buildMMappedIndex();\n+\n+    walker = new SpecificSegmentsQuerySegmentWalker(conglomerate)\n+        .add(segment1, index1)\n+        .add(segment2, index2)\n+        .add(segment2, index2)\n+        .add(segment3, index2);\n+\n+    druidSchema = new DruidSchema(\n+        CalciteTests.createMockQueryLifecycleFactory(walker, conglomerate),\n+        new TestServerInventoryView(walker.getSegments()),\n+        PLANNER_CONFIG_DEFAULT,\n+        new NoopViewManager(),\n+        new NoopEscalator()\n+    );\n+    druidSchema.start();\n+    druidSchema.awaitInitialization();\n+    schema = new SystemSchema(\n+        druidSchema,\n+        serverView,\n+        EasyMock.createStrictMock(AuthorizerMapper.class),\n+        client,\n+        client,\n+        mapper\n+    );\n+  }\n+\n+  private final DataSegment segment1 = new DataSegment(\n+      \"test1\",\n+      Intervals.of(\"2010/2011\"),\n+      \"version1\",\n+      null,\n+      ImmutableList.of(\"dim1\", \"dim2\"),\n+      ImmutableList.of(\"met1\", \"met2\"),\n+      null,\n+      1,\n+      100L,\n+      DataSegment.PruneLoadSpecHolder.DEFAULT\n+  );\n+  private final DataSegment segment2 = new DataSegment(\n+      \"test2\",\n+      Intervals.of(\"2011/2012\"),\n+      \"version2\",\n+      null,\n+      ImmutableList.of(\"dim1\", \"dim2\"),\n+      ImmutableList.of(\"met1\", \"met2\"),\n+      null,\n+      1,\n+      100L,\n+      DataSegment.PruneLoadSpecHolder.DEFAULT\n+  );\n+  private final DataSegment segment3 = new DataSegment(\n+      \"test3\",\n+      Intervals.of(\"2012/2013\"),\n+      \"version3\",\n+      null,\n+      ImmutableList.of(\"dim1\", \"dim2\"),\n+      ImmutableList.of(\"met1\", \"met2\"),\n+      null,\n+      1,\n+      100L,\n+      DataSegment.PruneLoadSpecHolder.DEFAULT\n+  );\n+  private final DataSegment segment4 = new DataSegment(\n+      \"test4\",\n+      Intervals.of(\"2017/2018\"),\n+      \"version4\",\n+      null,\n+      ImmutableList.of(\"dim1\", \"dim2\"),\n+      ImmutableList.of(\"met1\", \"met2\"),\n+      null,\n+      1,\n+      100L,\n+      DataSegment.PruneLoadSpecHolder.DEFAULT\n+  );\n+  private final DataSegment segment5 = new DataSegment(\n+      \"test5\",\n+      Intervals.of(\"2017/2018\"),\n+      \"version5\",\n+      null,\n+      ImmutableList.of(\"dim1\", \"dim2\"),\n+      ImmutableList.of(\"met1\", \"met2\"),\n+      null,\n+      1,\n+      100L,\n+      DataSegment.PruneLoadSpecHolder.DEFAULT\n+  );\n+\n+  private final HttpClient httpClient = EasyMock.createMock(HttpClient.class);\n+  private final DirectDruidClient client1 = new DirectDruidClient(\n+      new ReflectionQueryToolChestWarehouse(),\n+      QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n+      new DefaultObjectMapper(),\n+      httpClient,\n+      \"http\",\n+      \"foo\",\n+      new NoopServiceEmitter()\n+  );\n+  private final DirectDruidClient client2 = new DirectDruidClient(\n+      new ReflectionQueryToolChestWarehouse(),\n+      QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n+      new DefaultObjectMapper(),\n+      httpClient,\n+      \"http\",\n+      \"foo2\",\n+      new NoopServiceEmitter()\n+  );\n+  private final ImmutableDruidServer druidServer1 = new ImmutableDruidServer(\n+      new DruidServerMetadata(\"server1\", \"localhost:0000\", null, 5L, ServerType.REALTIME, DruidServer.DEFAULT_TIER, 0),\n+      1L,\n+      null,\n+      ImmutableMap.of(\"segment1\", segment1, \"segment2\", segment2)\n+  );\n+\n+  private final ImmutableDruidServer druidServer2 = new ImmutableDruidServer(\n+      new DruidServerMetadata(\"server2\", \"server2:1234\", null, 5L, ServerType.HISTORICAL, DruidServer.DEFAULT_TIER, 0),\n+      1L,\n+      null,\n+      ImmutableMap.of(\"segment2\", segment2, \"segment4\", segment4, \"segment5\", segment5)\n+  );\n+\n+  private final List<ImmutableDruidServer> immutableDruidServers = ImmutableList.of(druidServer1, druidServer2);\n+\n+  @Test\n+  public void testGetTableMap()\n+  {\n+    Assert.assertEquals(ImmutableSet.of(\"segments\", \"servers\", \"server_segments\", \"tasks\"), schema.getTableNames());\n+\n+    final Map<String, Table> tableMap = schema.getTableMap();\n+    Assert.assertEquals(ImmutableSet.of(\"segments\", \"servers\", \"server_segments\", \"tasks\"), tableMap.keySet());\n+    final SystemSchema.SegmentsTable segmentsTable = (SystemSchema.SegmentsTable) schema.getTableMap().get(\"segments\");\n+    final RelDataType rowType = segmentsTable.getRowType(new JavaTypeFactoryImpl());\n+    final List<RelDataTypeField> fields = rowType.getFieldList();\n+\n+    Assert.assertEquals(13, fields.size());\n+\n+    final SystemSchema.TasksTable tasksTable = (SystemSchema.TasksTable) schema.getTableMap().get(\"tasks\");\n+    final RelDataType sysRowType = tasksTable.getRowType(new JavaTypeFactoryImpl());\n+    final List<RelDataTypeField> sysFields = sysRowType.getFieldList();\n+    Assert.assertEquals(13, sysFields.size());\n+\n+    Assert.assertEquals(\"task_id\", sysFields.get(0).getName());\n+    Assert.assertEquals(SqlTypeName.VARCHAR, sysFields.get(0).getType().getSqlTypeName());\n+\n+    final SystemSchema.ServersTable serversTable = (SystemSchema.ServersTable) schema.getTableMap().get(\"servers\");\n+    final RelDataType serverRowType = serversTable.getRowType(new JavaTypeFactoryImpl());\n+    final List<RelDataTypeField> serverFields = serverRowType.getFieldList();\n+    Assert.assertEquals(8, serverFields.size());\n+    Assert.assertEquals(\"server\", serverFields.get(0).getName());\n+    Assert.assertEquals(SqlTypeName.VARCHAR, serverFields.get(0).getType().getSqlTypeName());\n+  }\n+\n+  @Test\n+  public void testSegmentsTable() throws Exception\n+  {\n+    // total segments = 6\n+    // segments 1,2,3 are published and available\n+    // segments 4,5,6  are published but unavailable\n+    // segment 3 is published but not served\n+    // segment 2 is served by 2 servers, so num_replicas=2\n+\n+    final SystemSchema.SegmentsTable segmentsTable = EasyMock.createMockBuilder(SystemSchema.SegmentsTable.class).withConstructor(\n+        druidSchema, client, mapper, responseHandler, authMapper).createMock();\n+    EasyMock.replay(segmentsTable);\n+\n+    EasyMock.expect(client.makeRequest(HttpMethod.GET, \"/druid/coordinator/v1/metadata/segments\")).andReturn(request).anyTimes();\n+    SettableFuture<InputStream> future = SettableFuture.create();\n+    EasyMock.expect(client.goAsync(request, responseHandler)).andReturn(future).once();\n+    final int ok = HttpServletResponse.SC_OK;\n+    EasyMock.expect(responseHandler.getStatus()).andReturn(ok).once();\n+\n+    EasyMock.expect(request.getUrl()).andReturn(new URL(\"http://test-host:1234/druid/coordinator/v1/metadata/segments\")).anyTimes();\n+\n+    AppendableByteArrayInputStream in = new AppendableByteArrayInputStream();\n+    //published but unavailable segments\n+    final String json = \"[{\\n\"\n+                        + \"\\t\\\"dataSource\\\": \\\"wikipedia-kafka\\\",\\n\"\n+                        + \"\\t\\\"interval\\\": \\\"2018-08-07T23:00:00.000Z/2018-08-08T00:00:00.000Z\\\",\\n\"\n+                        + \"\\t\\\"version\\\": \\\"2018-08-07T23:00:00.059Z\\\",\\n\"\n+                        + \"\\t\\\"loadSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"local\\\",\\n\"\n+                        + \"\\t\\t\\\"path\\\": \\\"/var/druid/segments/wikipedia-kafka/2018-08-07T23:00:00.000Z_2018-08-08T00:00:00.000Z/2018-08-07T23:00:00.059Z/51/1578eb79-0e44-4b41-a87b-65e40c52be53/index.zip\\\"\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"dimensions\\\": \\\"isRobot,channel,flags,isUnpatrolled,page,diffUrl,comment,isNew,isMinor,user,namespace,commentLength,deltaBucket,cityName,countryIsoCode,countryName,isAnonymous,regionIsoCode,regionName,added,deleted,delta\\\",\\n\"\n+                        + \"\\t\\\"metrics\\\": \\\"count,user_unique\\\",\\n\"\n+                        + \"\\t\\\"shardSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"none\\\",\\n\"\n+                        + \"\\t\\t\\\"partitionNum\\\": 51,\\n\"\n+                        + \"\\t\\t\\\"partitions\\\": 0\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"binaryVersion\\\": 9,\\n\"\n+                        + \"\\t\\\"size\\\": 47406,\\n\"\n+                        + \"\\t\\\"identifier\\\": \\\"wikipedia-kafka_2018-08-07T23:00:00.000Z_2018-08-08T00:00:00.000Z_2018-08-07T23:00:00.059Z_51\\\"\\n\"\n+                        + \"}, {\\n\"\n+                        + \"\\t\\\"dataSource\\\": \\\"wikipedia-kafka\\\",\\n\"\n+                        + \"\\t\\\"interval\\\": \\\"2018-08-07T18:00:00.000Z/2018-08-07T19:00:00.000Z\\\",\\n\"\n+                        + \"\\t\\\"version\\\": \\\"2018-08-07T18:00:00.117Z\\\",\\n\"\n+                        + \"\\t\\\"loadSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"local\\\",\\n\"\n+                        + \"\\t\\t\\\"path\\\": \\\"/var/druid/segments/wikipedia-kafka/2018-08-07T18:00:00.000Z_2018-08-07T19:00:00.000Z/2018-08-07T18:00:00.117Z/9/a2646827-b782-424c-9eed-e48aa448d2c5/index.zip\\\"\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"dimensions\\\": \\\"isRobot,channel,flags,isUnpatrolled,page,diffUrl,comment,isNew,isMinor,user,namespace,commentLength,deltaBucket,cityName,countryIsoCode,countryName,isAnonymous,metroCode,regionIsoCode,regionName,added,deleted,delta\\\",\\n\"\n+                        + \"\\t\\\"metrics\\\": \\\"count,user_unique\\\",\\n\"\n+                        + \"\\t\\\"shardSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"none\\\",\\n\"\n+                        + \"\\t\\t\\\"partitionNum\\\": 9,\\n\"\n+                        + \"\\t\\t\\\"partitions\\\": 0\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"binaryVersion\\\": 9,\\n\"\n+                        + \"\\t\\\"size\\\": 83846,\\n\"\n+                        + \"\\t\\\"identifier\\\": \\\"wikipedia-kafka_2018-08-07T18:00:00.000Z_2018-08-07T19:00:00.000Z_2018-08-07T18:00:00.117Z_9\\\"\\n\"\n+                        + \"}, {\\n\"\n+                        + \"\\t\\\"dataSource\\\": \\\"wikipedia-kafka\\\",\\n\"\n+                        + \"\\t\\\"interval\\\": \\\"2018-08-07T23:00:00.000Z/2018-08-08T00:00:00.000Z\\\",\\n\"\n+                        + \"\\t\\\"version\\\": \\\"2018-08-07T23:00:00.059Z\\\",\\n\"\n+                        + \"\\t\\\"loadSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"local\\\",\\n\"\n+                        + \"\\t\\t\\\"path\\\": \\\"/var/druid/segments/wikipedia-kafka/2018-08-07T23:00:00.000Z_2018-08-08T00:00:00.000Z/2018-08-07T23:00:00.059Z/50/87c5457e-c39b-4c03-9df8-e2b20b210dfc/index.zip\\\"\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"dimensions\\\": \\\"isRobot,channel,flags,isUnpatrolled,page,diffUrl,comment,isNew,isMinor,user,namespace,commentLength,deltaBucket,cityName,countryIsoCode,countryName,isAnonymous,metroCode,regionIsoCode,regionName,added,deleted,delta\\\",\\n\"\n+                        + \"\\t\\\"metrics\\\": \\\"count,user_unique\\\",\\n\"\n+                        + \"\\t\\\"shardSpec\\\": {\\n\"\n+                        + \"\\t\\t\\\"type\\\": \\\"none\\\",\\n\"\n+                        + \"\\t\\t\\\"partitionNum\\\": 50,\\n\"\n+                        + \"\\t\\t\\\"partitions\\\": 0\\n\"\n+                        + \"\\t},\\n\"\n+                        + \"\\t\\\"binaryVersion\\\": 9,\\n\"\n+                        + \"\\t\\\"size\\\": 53527,\\n\"\n+                        + \"\\t\\\"identifier\\\": \\\"wikipedia-kafka_2018-08-07T23:00:00.000Z_2018-08-08T00:00:00.000Z_2018-08-07T23:00:00.059Z_50\\\"\\n\"\n+                        + \"}]\";\n+    byte[] bytesToWrite = json.getBytes(StandardCharsets.UTF_8);\n+    in.add(bytesToWrite);\n+    in.done();\n+    future.set(in);\n+\n+    EasyMock.replay(client, request, responseHolder, responseHandler);\n+    DataContext dataContext = new DataContext()\n+    {\n+      @Override\n+      public SchemaPlus getRootSchema()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public JavaTypeFactory getTypeFactory()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public QueryProvider getQueryProvider()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public Object get(String name)\n+      {\n+        return CalciteTests.SUPER_USER_AUTH_RESULT;\n+      }\n+    };\n+    Enumerable<Object[]> rows = segmentsTable.scan(dataContext);\n+    Enumerator<Object[]> enumerator = rows.enumerator();\n+\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Object[] row2 = enumerator.current();\n+    //segment 2 is published and has 2 replicas\n+    Assert.assertEquals(1L, row2[9]);\n+    Assert.assertEquals(2L, row2[7]);\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Object[] row6 = enumerator.current();\n+    //segment 6 is published and unavailable, num_replicas is 0\n+    Assert.assertEquals(1L, row6[9]);\n+    Assert.assertEquals(0L, row6[7]);\n+    Assert.assertEquals(false, enumerator.moveNext());\n+\n+  }\n+\n+  @Test\n+  public void testServersTable()\n+  {\n+\n+    SystemSchema.ServersTable serversTable = EasyMock.createMockBuilder(SystemSchema.ServersTable.class).withConstructor(serverView, authMapper).createMock();\n+    EasyMock.replay(serversTable);\n+\n+    EasyMock.expect(serverView.getDruidServers())\n+            .andReturn(immutableDruidServers)\n+            .once();\n+    EasyMock.replay(serverView);\n+    DataContext dataContext = new DataContext()\n+    {\n+      @Override\n+      public SchemaPlus getRootSchema()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public JavaTypeFactory getTypeFactory()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public QueryProvider getQueryProvider()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public Object get(String name)\n+      {\n+        return CalciteTests.SUPER_USER_AUTH_RESULT;\n+      }\n+    };\n+    Enumerable<Object[]> rows = serversTable.scan(dataContext);\n+    Assert.assertEquals(2, rows.count());\n+    Object[] row1 = rows.first();\n+    Assert.assertEquals(\"localhost:0000\", row1[0]);\n+    Assert.assertEquals(\"realtime\", row1[4].toString());\n+    Object[] row2 = rows.last();\n+    Assert.assertEquals(\"server2:1234\", row2[0]);\n+    Assert.assertEquals(\"historical\", row2[4].toString());\n+  }\n+\n+  @Test\n+  public void testTasksTable() throws Exception\n+  {\n+\n+    SystemSchema.TasksTable tasksTable = EasyMock.createMockBuilder(SystemSchema.TasksTable.class)\n+                                                 .withConstructor(client, mapper, responseHandler, authMapper)\n+                                                 .createMock();\n+    EasyMock.replay(tasksTable);\n+    EasyMock.expect(client.makeRequest(HttpMethod.GET, \"/druid/indexer/v1/tasks\")).andReturn(request).anyTimes();\n+    SettableFuture<InputStream> future = SettableFuture.create();\n+    EasyMock.expect(client.goAsync(request, responseHandler)).andReturn(future).once();\n+    final int ok = HttpServletResponse.SC_OK;\n+    EasyMock.expect(responseHandler.getStatus()).andReturn(ok).once();\n+    EasyMock.expect(request.getUrl()).andReturn(new URL(\"http://test-host:1234/druid/indexer/v1/tasks\")).anyTimes();\n+\n+    AppendableByteArrayInputStream in = new AppendableByteArrayInputStream();\n+\n+\n+    String json = \"[{\\n\"\n+                  + \"\\t\\\"id\\\": \\\"index_wikipedia_2018-09-20T22:33:44.911Z\\\",\\n\"\n+                  + \"\\t\\\"type\\\": \\\"index\\\",\\n\"\n+                  + \"\\t\\\"createdTime\\\": \\\"2018-09-20T22:33:44.922Z\\\",\\n\"\n+                  + \"\\t\\\"queueInsertionTime\\\": \\\"1970-01-01T00:00:00.000Z\\\",\\n\"\n+                  + \"\\t\\\"statusCode\\\": \\\"FAILED\\\",\\n\"\n+                  + \"\\t\\\"runnerStatusCode\\\": \\\"NONE\\\",\\n\"\n+                  + \"\\t\\\"duration\\\": -1,\\n\"\n+                  + \"\\t\\\"location\\\": {\\n\"\n+                  + \"\\t\\t\\\"host\\\": \\\"testHost\\\",\\n\"\n+                  + \"\\t\\t\\\"port\\\": 1234,\\n\"\n+                  + \"\\t\\t\\\"tlsPort\\\": -1\\n\"\n+                  + \"\\t},\\n\"\n+                  + \"\\t\\\"dataSource\\\": \\\"wikipedia\\\",\\n\"\n+                  + \"\\t\\\"errorMsg\\\": null\\n\"\n+                  + \"}, {\\n\"\n+                  + \"\\t\\\"id\\\": \\\"index_wikipedia_2018-09-21T18:38:47.773Z\\\",\\n\"\n+                  + \"\\t\\\"type\\\": \\\"index\\\",\\n\"\n+                  + \"\\t\\\"createdTime\\\": \\\"2018-09-21T18:38:47.873Z\\\",\\n\"\n+                  + \"\\t\\\"queueInsertionTime\\\": \\\"2018-09-21T18:38:47.910Z\\\",\\n\"\n+                  + \"\\t\\\"statusCode\\\": \\\"RUNNING\\\",\\n\"\n+                  + \"\\t\\\"runnerStatusCode\\\": \\\"RUNNING\\\",\\n\"\n+                  + \"\\t\\\"duration\\\": null,\\n\"\n+                  + \"\\t\\\"location\\\": {\\n\"\n+                  + \"\\t\\t\\\"host\\\": \\\"192.168.1.6\\\",\\n\"\n+                  + \"\\t\\t\\\"port\\\": 8100,\\n\"\n+                  + \"\\t\\t\\\"tlsPort\\\": -1\\n\"\n+                  + \"\\t},\\n\"\n+                  + \"\\t\\\"dataSource\\\": \\\"wikipedia\\\",\\n\"\n+                  + \"\\t\\\"errorMsg\\\": null\\n\"\n+                  + \"}]\";\n+    byte[] bytesToWrite = json.getBytes(StandardCharsets.UTF_8);\n+    in.add(bytesToWrite);\n+    in.done();\n+    future.set(in);\n+\n+    EasyMock.replay(client, request, responseHandler);\n+    DataContext dataContext = new DataContext()\n+    {\n+      @Override\n+      public SchemaPlus getRootSchema()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public JavaTypeFactory getTypeFactory()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public QueryProvider getQueryProvider()\n+      {\n+        return null;\n+      }\n+\n+      @Override\n+      public Object get(String name)\n+      {\n+        return CalciteTests.SUPER_USER_AUTH_RESULT;\n+      }\n+    };\n+    Enumerable<Object[]> rows = tasksTable.scan(dataContext);\n+    Enumerator<Object[]> enumerator = rows.enumerator();\n+\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Object[] row1 = enumerator.current();\n+    Assert.assertEquals(\"index_wikipedia_2018-09-20T22:33:44.911Z\", row1[0].toString());\n+    Assert.assertEquals(\"FAILED\", row1[5].toString());\n+    Assert.assertEquals(\"NONE\", row1[6].toString());\n+    Assert.assertEquals(-1L, row1[7]);\n+    Assert.assertEquals(\"testHost:1234\", row1[8]);\n+\n+    Assert.assertEquals(true, enumerator.moveNext());\n+    Object[] row2 = enumerator.current();\n+    Assert.assertEquals(\"index_wikipedia_2018-09-21T18:38:47.773Z\", row2[0].toString());\n+    Assert.assertEquals(\"RUNNING\", row2[5].toString());\n+    Assert.assertEquals(\"RUNNING\", row2[6].toString());\n+    Assert.assertEquals(null, row2[7]);\n+    Assert.assertEquals(\"192.168.1.6:8100\", row2[8]);\n+\n+    Assert.assertEquals(false, enumerator.moveNext());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/schema/SystemSchemaTest.java",
                "sha": "9b9073344dc1a015b875dcb49dd31bfc5663e938",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "patch": "@@ -32,19 +32,24 @@\n import com.google.inject.Injector;\n import com.google.inject.Key;\n import com.google.inject.Module;\n+import org.apache.curator.x.discovery.ServiceProvider;\n import org.apache.druid.collections.CloseableStupidPool;\n+import org.apache.druid.curator.discovery.ServerDiscoverySelector;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n import org.apache.druid.data.input.impl.InputRowParser;\n import org.apache.druid.data.input.impl.MapInputRowParser;\n import org.apache.druid.data.input.impl.TimeAndDimsParseSpec;\n import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.discovery.DruidLeaderClient;\n+import org.apache.druid.discovery.DruidNodeDiscoveryProvider;\n import org.apache.druid.guice.ExpressionModule;\n import org.apache.druid.guice.annotations.Json;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.java.util.emitter.core.NoopEmitter;\n import org.apache.druid.java.util.emitter.service.ServiceEmitter;\n+import org.apache.druid.java.util.http.client.HttpClient;\n import org.apache.druid.math.expr.ExprMacroTable;\n import org.apache.druid.query.DefaultGenericQueryMetricsFactory;\n import org.apache.druid.query.DefaultQueryRunnerFactoryConglomerate;\n@@ -115,10 +120,12 @@\n import org.apache.druid.sql.calcite.planner.DruidOperatorTable;\n import org.apache.druid.sql.calcite.planner.PlannerConfig;\n import org.apache.druid.sql.calcite.schema.DruidSchema;\n+import org.apache.druid.sql.calcite.schema.SystemSchema;\n import org.apache.druid.sql.calcite.view.NoopViewManager;\n import org.apache.druid.sql.calcite.view.ViewManager;\n import org.apache.druid.timeline.DataSegment;\n import org.apache.druid.timeline.partition.LinearShardSpec;\n+import org.easymock.EasyMock;\n import org.joda.time.DateTime;\n import org.joda.time.chrono.ISOChronology;\n \n@@ -568,4 +575,30 @@ public static InputRow createRow(final Object t, final String dim1, final String\n         )\n     ).get(0);\n   }\n+\n+\n+  public static SystemSchema createMockSystemSchema(\n+      final DruidSchema druidSchema,\n+      final SpecificSegmentsQuerySegmentWalker walker\n+  )\n+  {\n+    final DruidLeaderClient druidLeaderClient = new DruidLeaderClient(\n+        EasyMock.createMock(HttpClient.class),\n+        EasyMock.createMock(DruidNodeDiscoveryProvider.class),\n+        \"nodetype\",\n+        \"/simple/leader\",\n+        new ServerDiscoverySelector(EasyMock.createMock(ServiceProvider.class), \"test\")\n+    )\n+    {\n+    };\n+    final SystemSchema schema = new SystemSchema(\n+        druidSchema,\n+        new TestServerInventoryView(walker.getSegments()),\n+        TEST_AUTHORIZER_MAPPER,\n+        druidLeaderClient,\n+        druidLeaderClient,\n+        getJsonMapper()\n+    );\n+    return schema;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "sha": "a83fde550e16cc95a7937b90ae77250fcd2d3867",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/util/TestServerInventoryView.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/util/TestServerInventoryView.java?ref=3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7",
                "deletions": 1,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/util/TestServerInventoryView.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.collect.ImmutableList;\n import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.ImmutableDruidServer;\n import org.apache.druid.client.TimelineServerView;\n import org.apache.druid.client.selector.ServerSelector;\n import org.apache.druid.query.DataSource;\n@@ -30,9 +31,10 @@\n import org.apache.druid.timeline.DataSegment;\n import org.apache.druid.timeline.TimelineLookup;\n \n+import javax.annotation.Nullable;\n import java.util.List;\n import java.util.concurrent.Executor;\n-\n+// this class is used for testing and benchmark\n public class TestServerInventoryView implements TimelineServerView\n {\n   private static final DruidServerMetadata DUMMY_SERVER = new DruidServerMetadata(\n@@ -57,6 +59,13 @@ public TestServerInventoryView(List<DataSegment> segments)\n     throw new UnsupportedOperationException();\n   }\n \n+  @Nullable\n+  @Override\n+  public List<ImmutableDruidServer> getDruidServers()\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n   @Override\n   public void registerSegmentCallback(Executor exec, final SegmentCallback callback)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3a0a667fe03d03d97c4e5d6cbeb3e60e28c7d4f7/sql/src/test/java/org/apache/druid/sql/calcite/util/TestServerInventoryView.java",
                "sha": "e0b6825cc738a415cb205c898a6659644cca0bdf",
                "status": "modified"
            }
        ],
        "message": "Introduce SystemSchema tables (#5989) (#6094)\n\n* Added SystemSchema with following tables (#5989)\r\n\r\n* SEGMENTS table provides details on served and published segments\r\n* SERVERS table provides details on data servers\r\n* SERVERSEGMETS table is the JOIN of SEGMENTS and SERVERS\r\n* TASKS table provides details on tasks\r\n\r\n* Add documentation for system schema\r\n\r\n* Fix static-analysis warnings\r\n\r\n* Address PR comments\r\n\r\n*Add unit tests\r\n\r\n* Fix a test\r\n\r\n* Try to fix a test\r\n\r\n* Fix a bug around replica count\r\n\r\n* rename io.druid to org.apache.druid\r\n\r\n* Major change is to make tasks and segment queries streaming\r\n\r\n* Made tasks/segments stream to calcite instead of storing it in memory\r\n* Add num_rows to segments table\r\n* Refactor JsonParserIterator\r\n* Replace with closeable iterator\r\n\r\n* Fix docs, make num_rows column nullable, some unit test changes\r\n\r\n* make num_rows column type long, allow it to be null\r\n\r\nfix a compile error after merge, add TrafficCop param to InputStreamResponseHandler\r\n\r\n* Filter null rows for segments table from Linq4j enumerable\r\n\r\n* change num_replicas datatype to long in segments table\r\n\r\n* Fix some tests and address comments\r\n\r\n* Doc updates, other PR comments\r\n\r\n* Update tests\r\n\r\n* Address comments\r\n\r\n* Add auth check\r\n* Update docs\r\n* Refactoring\r\n\r\n* Fix teamcity warning, change the getQueryableServer in TimelineServerView\r\n\r\n* Fix compilation after rebase\r\n\r\n* Use the stream API from AuthorizationUtils\r\n\r\n* Added LeaderClient interface and NoopDruidLeaderClient class\r\n\r\n* Revert \"Added LeaderClient interface and NoopDruidLeaderClient class\"\r\n\r\nThis reverts commit 100fa46e396ab0f68da6c4bef80951f6b996657e.\r\n\r\n* Make the naming consistent to server_segments for the join table\r\n\r\n* Add ForbiddenException on auth check failure\r\n* Remove static block from SystemSchema\r\n\r\n* Try to fix a test in CalciteQueryTest due to rename of server_segments\r\n\r\n* Fix the json output format in the coordinator API\r\n\r\n* Add auth check in the segments API\r\n* Add null check to avoid NPE\r\n\r\n* Use annonymous class object instead of mock for DruidLeaderClient in SqlBenchmark\r\n\r\n* Fix test failures, type long/BIGINT can be nullable\r\n\r\n* Revert long nullability to fix tests\r\n\r\n* Fix style for tests\r\n\r\n* PR comments\r\n\r\n* Address PR comments\r\n\r\n* Add the missing BytesAccumulatingResponseHandler class\r\n\r\n* Use Sequences.withBaggage in DruidPlanner\r\n\r\n* Fix docs, add comments\r\n\r\n* Close the iterator if hasNext returns false",
        "parent": "https://github.com/apache/incubator-druid/commit/e69a2f217b378b6a5dd416c791531e9b1cfb208e",
        "repo": "incubator-druid",
        "unit_tests": [
            "BrokerServerViewTest.java",
            "DirectDruidClientTest.java",
            "ImmutableDruidServerTests.java",
            "DruidLeaderClientTest.java",
            "ChangeRequestHttpSyncerTest.java",
            "HttpLoadQueuePeonTest.java",
            "CalcitesTest.java",
            "DruidSchemaTest.java",
            "SystemSchemaTest.java"
        ]
    },
    "incubator-druid_3fcaa1a": {
        "bug_id": "incubator-druid_3fcaa1a",
        "commit": "https://github.com/apache/incubator-druid/commit/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/BoundFilterBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/BoundFilterBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/BoundFilterBenchmark.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.druid.collections.bitmap.MutableBitmap;\n import org.apache.druid.collections.bitmap.RoaringBitmapFactory;\n import org.apache.druid.collections.spatial.ImmutableRTree;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.extendedset.intset.ConciseSetUtils;\n import org.apache.druid.query.filter.BitmapIndexSelector;\n import org.apache.druid.query.filter.BoundDimFilter;\n@@ -60,6 +61,10 @@\n @Measurement(iterations = 10)\n public class BoundFilterBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final int START_INT = 1_000_000_000;\n   private static final int END_INT = ConciseSetUtils.MAX_ALLOWED_INTEGER;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/BoundFilterBenchmark.java",
                "sha": "6c0b6d940ed4fc36ea966bba53d58e177427ddf9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedColumnarIntsBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedColumnarIntsBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/CompressedColumnarIntsBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.segment.data.ColumnarInts;\n import org.apache.druid.segment.data.CompressedVSizeColumnarIntsSupplier;\n@@ -49,6 +50,10 @@\n @State(Scope.Benchmark)\n public class CompressedColumnarIntsBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private IndexedInts uncompressed;\n   private IndexedInts compressed;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedColumnarIntsBenchmark.java",
                "sha": "5e283ae55d9c2ea377cca8fa25115563f0ee5715",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedVSizeColumnarMultiIntsBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedVSizeColumnarMultiIntsBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/CompressedVSizeColumnarMultiIntsBenchmark.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.base.Function;\n import com.google.common.collect.Iterables;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.segment.data.ColumnarInts;\n import org.apache.druid.segment.data.ColumnarMultiInts;\n@@ -54,6 +55,10 @@\n @State(Scope.Benchmark)\n public class CompressedVSizeColumnarMultiIntsBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private ColumnarMultiInts uncompressed;\n   private ColumnarMultiInts compressed;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/CompressedVSizeColumnarMultiIntsBenchmark.java",
                "sha": "bbf5b537c839508e2d9c30ecc1a9c2b3722a3172",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/DataSketchesHllBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/DataSketchesHllBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/DataSketchesHllBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import com.yahoo.sketches.hll.HllSketch;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.query.aggregation.BufferAggregator;\n import org.apache.druid.query.aggregation.datasketches.hll.HllSketchMergeAggregatorFactory;\n@@ -54,6 +55,10 @@\n @State(Scope.Benchmark)\n public class DataSketchesHllBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private final AggregatorFactory aggregatorFactory = new HllSketchMergeAggregatorFactory(\n       \"hll\",\n       \"hll\",",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/DataSketchesHllBenchmark.java",
                "sha": "1c0ddae58d0acc8727c70971a14050705e145b47",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/DimensionPredicateFilterBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/DimensionPredicateFilterBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/DimensionPredicateFilterBenchmark.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.druid.collections.bitmap.MutableBitmap;\n import org.apache.druid.collections.bitmap.RoaringBitmapFactory;\n import org.apache.druid.collections.spatial.ImmutableRTree;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.query.filter.BitmapIndexSelector;\n import org.apache.druid.query.filter.DruidDoublePredicate;\n import org.apache.druid.query.filter.DruidFloatPredicate;\n@@ -62,6 +63,10 @@\n @Measurement(iterations = 10)\n public class DimensionPredicateFilterBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final int START_INT = 1_000_000_000;\n \n   private static final DimensionPredicateFilter IS_EVEN = new DimensionPredicateFilter(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/DimensionPredicateFilterBenchmark.java",
                "sha": "8dd177f1e831fa9cec0bd40ca169bbff56b5861b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionAggregationBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionAggregationBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionAggregationBenchmark.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkColumnSchema;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.SegmentGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.Intervals;\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -71,6 +72,10 @@\n @OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class ExpressionAggregationBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param({\"1000000\"})\n   private int rowsPerSegment;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionAggregationBenchmark.java",
                "sha": "7d7c8c45f81a812dd4a7467f61ceb7d2ee612b5a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionSelectorBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionSelectorBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionSelectorBenchmark.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkColumnSchema;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.SegmentGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.Intervals;\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -70,6 +71,10 @@\n @OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class ExpressionSelectorBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param({\"1000000\"})\n   private int rowsPerSegment;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/ExpressionSelectorBenchmark.java",
                "sha": "d3406a469051f0254c1552d36802c5e652ac9dc1",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FilterPartitionBenchmark.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FilterPartitionBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FilterPartitionBenchmark.java",
                "patch": "@@ -102,6 +102,10 @@\n @Measurement(iterations = 25)\n public class FilterPartitionBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param({\"750000\"})\n   private int rowsPerSegment;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FilterPartitionBenchmark.java",
                "sha": "cedf85b00edf1bd43cefaba4ad68586f50f0e522",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FilteredAggregatorBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FilteredAggregatorBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FilteredAggregatorBenchmark.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.benchmark.query.QueryBenchmarkUtil;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.granularity.Granularities;\n@@ -99,6 +100,10 @@\n @Measurement(iterations = 25)\n public class FilteredAggregatorBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param({\"75000\"})\n   private int rowsPerSegment;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FilteredAggregatorBenchmark.java",
                "sha": "ec4256425297f6f596e3eb42d2671c4c30940811",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramAddBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramAddBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramAddBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import org.apache.commons.math3.distribution.NormalDistribution;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.query.aggregation.histogram.FixedBucketsHistogram;\n import org.openjdk.jmh.annotations.Benchmark;\n import org.openjdk.jmh.annotations.BenchmarkMode;\n@@ -46,6 +47,10 @@\n @OutputTimeUnit(TimeUnit.MICROSECONDS)\n public class FixedHistogramAddBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final int LOWER_LIMIT = 0;\n   private static final int UPPER_LIMIT = 100000;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramAddBenchmark.java",
                "sha": "c5010bbccef037bcfa5ca22e8a0c531375614977",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramBenchmark.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.benchmark;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.query.aggregation.histogram.FixedBucketsHistogram;\n import org.openjdk.jmh.annotations.Benchmark;\n@@ -46,6 +47,10 @@\n @OutputTimeUnit(TimeUnit.MICROSECONDS)\n public class FixedHistogramBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final Logger log = new Logger(FixedHistogramBenchmark.class);\n \n   private static final int LOWER_LIMIT = 0;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FixedHistogramBenchmark.java",
                "sha": "747b645816d471df8af1d3ac08a587ed33c72292",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import com.google.common.base.Supplier;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.FileUtils;\n import org.apache.druid.java.util.common.MappedByteBufferHandler;\n import org.apache.druid.segment.data.ColumnarFloats;\n@@ -55,6 +56,10 @@\n @OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class FloatCompressionBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param(\"floatCompress/\")\n   private static String dirPath;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmark.java",
                "sha": "1663c0eba3734accc87f6f355063d3c4a2b963ba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmarkFileGenerator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmarkFileGenerator.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmarkFileGenerator.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.druid.benchmark.datagen.BenchmarkColumnSchema;\n import org.apache.druid.benchmark.datagen.BenchmarkColumnValueGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.segment.column.ValueType;\n import org.apache.druid.segment.data.ColumnarFloatsSerializer;\n@@ -44,6 +45,10 @@\n \n public class FloatCompressionBenchmarkFileGenerator\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final Logger log = new Logger(FloatCompressionBenchmarkFileGenerator.class);\n   public static final int ROW_NUM = 5000000;\n   public static final List<CompressionStrategy> COMPRESSIONS =",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/FloatCompressionBenchmarkFileGenerator.java",
                "sha": "ef3bffc65004a58c512a64bcae126ee452f0c442",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/GenericIndexedBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/GenericIndexedBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/GenericIndexedBenchmark.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.io.Files;\n import com.google.common.primitives.Ints;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.data.GenericIndexed;\n@@ -60,6 +61,10 @@\n @State(Scope.Benchmark)\n public class GenericIndexedBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   public static final int ITERATIONS = 10000;\n \n   static final ObjectStrategy<byte[]> BYTE_ARRAY_STRATEGY = new ObjectStrategy<byte[]>()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/GenericIndexedBenchmark.java",
                "sha": "c8ea121d1ddb3dd481fbfc8a2993e3f70331ed1b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/GroupByTypeInterfaceBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/GroupByTypeInterfaceBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/GroupByTypeInterfaceBenchmark.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.druid.collections.DefaultBlockingPool;\n import org.apache.druid.collections.NonBlockingPool;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.granularity.Granularities;\n@@ -108,6 +109,10 @@\n @Measurement(iterations = 30)\n public class GroupByTypeInterfaceBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final SegmentId Q_INDEX_SEGMENT_ID = SegmentId.dummy(\"qIndex\");\n \n   @Param({\"4\"})",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/GroupByTypeInterfaceBenchmark.java",
                "sha": "06e27787aa9782b36b5e1dff6ed1f7dfe03f7f7b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/IncrementalIndexRowTypeBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/IncrementalIndexRowTypeBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/IncrementalIndexRowTypeBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.MapBasedInputRow;\n import org.apache.druid.java.util.common.StringUtils;\n@@ -48,6 +49,10 @@\n @State(Scope.Benchmark)\n public class IncrementalIndexRowTypeBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private IncrementalIndex incIndex;\n   private IncrementalIndex incFloatIndex;\n   private IncrementalIndex incStrIndex;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/IncrementalIndexRowTypeBenchmark.java",
                "sha": "0da6c06c747ca35cebbd2760ce73df9e88c6f5a6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LikeFilterBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/LikeFilterBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/LikeFilterBenchmark.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.druid.collections.bitmap.MutableBitmap;\n import org.apache.druid.collections.bitmap.RoaringBitmapFactory;\n import org.apache.druid.collections.spatial.ImmutableRTree;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.query.filter.BitmapIndexSelector;\n import org.apache.druid.query.filter.BoundDimFilter;\n import org.apache.druid.query.filter.Filter;\n@@ -62,6 +63,10 @@\n @Measurement(iterations = 10)\n public class LikeFilterBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final int START_INT = 1_000_000;\n   private static final int END_INT = 9_999_999;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LikeFilterBenchmark.java",
                "sha": "83551146eb24ff8571ba892cdc511d989254b67a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmark.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark;\n \n import com.google.common.base.Supplier;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.FileUtils;\n import org.apache.druid.java.util.common.MappedByteBufferHandler;\n import org.apache.druid.segment.data.ColumnarLongs;\n@@ -55,6 +56,10 @@\n @OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class LongCompressionBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param(\"longCompress/\")\n   private static String dirPath;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmark.java",
                "sha": "7bd057311a4d64d7f987a3177602fe7548499fcb",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmarkFileGenerator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmarkFileGenerator.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmarkFileGenerator.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.druid.benchmark.datagen.BenchmarkColumnSchema;\n import org.apache.druid.benchmark.datagen.BenchmarkColumnValueGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.segment.column.ValueType;\n import org.apache.druid.segment.data.ColumnarLongsSerializer;\n@@ -44,6 +45,10 @@\n \n public class LongCompressionBenchmarkFileGenerator\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final Logger log = new Logger(LongCompressionBenchmarkFileGenerator.class);\n   public static final int ROW_NUM = 5000000;\n   public static final List<CompressionStrategy> COMPRESSIONS =",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/LongCompressionBenchmarkFileGenerator.java",
                "sha": "31c39fe040b40ae7c53d38c11bb4d95c9e5b6dc3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/TopNTypeInterfaceBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/TopNTypeInterfaceBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/TopNTypeInterfaceBenchmark.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.benchmark.query.QueryBenchmarkUtil;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.granularity.Granularities;\n@@ -99,6 +100,10 @@\n @Measurement(iterations = 25)\n public class TopNTypeInterfaceBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final SegmentId Q_INDEX_SEGMENT_ID = SegmentId.dummy(\"qIndex\");\n   \n   @Param({\"1\"})",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/TopNTypeInterfaceBenchmark.java",
                "sha": "7ceb6998de4a32dcabc496d838eecf5e6fcd23dd",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/VSizeSerdeBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/VSizeSerdeBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/VSizeSerdeBenchmark.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.benchmark;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.FileUtils;\n import org.apache.druid.java.util.common.MappedByteBufferHandler;\n import org.apache.druid.java.util.common.logger.Logger;\n@@ -53,6 +54,10 @@\n @OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class VSizeSerdeBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final Logger log = new Logger(VSizeSerdeBenchmark.class);\n   @Param({\"500000\"})\n   private int values;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/VSizeSerdeBenchmark.java",
                "sha": "1738046205e1ec23b9f331bf04983c4b782b236b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/datagen/SegmentGenerator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/datagen/SegmentGenerator.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/datagen/SegmentGenerator.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.hash.Hashing;\n import com.google.common.io.Files;\n import org.apache.commons.io.FileUtils;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.impl.DimensionSchema;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n@@ -68,6 +69,10 @@\n   private static final String CACHE_DIR_PROPERTY = \"druid.benchmark.cacheDir\";\n   private static final String CACHE_DIR_ENV_VAR = \"DRUID_BENCHMARK_CACHE_DIR\";\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+  \n   private final File cacheDir;\n   private final boolean cleanupCacheDir;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/datagen/SegmentGenerator.java",
                "sha": "c2dd1400d9238a26f8e1dfb6ccfd2bf2a2c28196",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IncrementalIndexReadBenchmark.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IncrementalIndexReadBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IncrementalIndexReadBenchmark.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -83,6 +84,11 @@\n \n   private static final Logger log = new Logger(IncrementalIndexReadBenchmark.class);\n   private static final int RNG_SEED = 9999;\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private IncrementalIndex incIndex;\n \n   private BenchmarkSchemaInfo schemaInfo;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IncrementalIndexReadBenchmark.java",
                "sha": "f68f99394518f3530a6677056df57dbf339e3dd8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexIngestionBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexIngestionBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexIngestionBenchmark.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.query.aggregation.hyperloglog.HyperUniquesSerde;\n@@ -63,6 +64,10 @@\n   private static final Logger log = new Logger(IndexIngestionBenchmark.class);\n   private static final int RNG_SEED = 9999;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private IncrementalIndex incIndex;\n   private ArrayList<InputRow> rows;\n   private BenchmarkSchemaInfo schemaInfo;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexIngestionBenchmark.java",
                "sha": "19862f27c9483cdd745dedc8355b6c3646407ac1",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexMergeBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexMergeBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexMergeBenchmark.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.logger.Logger;\n@@ -84,6 +85,10 @@\n   private static final IndexIO INDEX_IO;\n   public static final ObjectMapper JSON_MAPPER;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private List<QueryableIndex> indexesToMerge;\n   private BenchmarkSchemaInfo schemaInfo;\n   private File tmpDir;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexMergeBenchmark.java",
                "sha": "78a642fb80d977f36296e81aa28c1cc4d8fe529a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexPersistBenchmark.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexPersistBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexPersistBenchmark.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.logger.Logger;\n@@ -69,6 +70,7 @@\n   private static final IndexIO INDEX_IO;\n \n   static {\n+    NullHandling.initializeForTests();\n     JSON_MAPPER = new DefaultObjectMapper();\n     INDEX_IO = new IndexIO(\n         JSON_MAPPER,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/IndexPersistBenchmark.java",
                "sha": "826c4095f2e841c9cb3052c49368023803678662",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/StringDimensionIndexerBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/StringDimensionIndexerBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/indexing/StringDimensionIndexerBenchmark.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.benchmark.indexing;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.impl.DimensionSchema;\n import org.apache.druid.segment.StringDimensionIndexer;\n import org.openjdk.jmh.annotations.Benchmark;\n@@ -42,6 +43,10 @@\n @Measurement(iterations = 10)\n public class StringDimensionIndexerBenchmark\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   StringDimensionIndexer indexer;\n   int[] exampleArray;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/indexing/StringDimensionIndexerBenchmark.java",
                "sha": "52fc85194e7cc2ccc773545549fa03c1194732af",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/CachingClusteredClientBenchmark.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/CachingClusteredClientBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/CachingClusteredClientBenchmark.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.druid.collections.DefaultBlockingPool;\n import org.apache.druid.collections.NonBlockingPool;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.guice.http.DruidHttpClientConfig;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.concurrent.Execs;\n@@ -149,10 +150,14 @@\n   private static final String DATA_SOURCE = \"ds\";\n \n   public static final ObjectMapper JSON_MAPPER;\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Param({\"8\", \"24\"})\n   private int numServers;\n \n-\n   @Param({\"0\", \"1\", \"4\"})\n   private int parallelism;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/CachingClusteredClientBenchmark.java",
                "sha": "6fa37a896a7288edb476d03b92a0ab54e947d5cb",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/GroupByBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/GroupByBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/GroupByBenchmark.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.druid.collections.DefaultBlockingPool;\n import org.apache.druid.collections.NonBlockingPool;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.concurrent.Execs;\n@@ -148,6 +149,10 @@\n   private static final IndexIO INDEX_IO;\n   public static final ObjectMapper JSON_MAPPER;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private File tmpDir;\n   private IncrementalIndex anIncrementalIndex;\n   private List<QueryableIndex> queryableIndexes;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/GroupByBenchmark.java",
                "sha": "d493e4c4951d50d26afa5ada75a61cf08ad75d8e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/QueryBenchmarkUtil.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/QueryBenchmarkUtil.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/QueryBenchmarkUtil.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.benchmark.query;\n \n import com.google.common.util.concurrent.ListenableFuture;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.guava.Sequence;\n import org.apache.druid.query.BySegmentQueryRunner;\n import org.apache.druid.query.FinalizeResultsQueryRunner;\n@@ -36,6 +37,10 @@\n \n public class QueryBenchmarkUtil\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   public static <T, QueryType extends Query<T>> QueryRunner<T> makeQueryRunner(\n       QueryRunnerFactory<T, QueryType> factory,\n       SegmentId segmentId,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/QueryBenchmarkUtil.java",
                "sha": "43cf6d20fa64a08274dc9ecd81fdf01dd6e21e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/ScanBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/ScanBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/ScanBenchmark.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.jackson.DefaultObjectMapper;\n@@ -125,6 +126,10 @@\n   private static final IndexMergerV9 INDEX_MERGER_V9;\n   private static final IndexIO INDEX_IO;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private List<IncrementalIndex> incIndexes;\n   private List<QueryableIndex> qIndexes;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/ScanBenchmark.java",
                "sha": "60b7712f1f2795574e0d1d3db448f6d51d9ac0dc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SearchBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/SearchBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/SearchBenchmark.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.jackson.DefaultObjectMapper;\n@@ -125,6 +126,10 @@\n   private static final IndexIO INDEX_IO;\n   public static final ObjectMapper JSON_MAPPER;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private List<IncrementalIndex> incIndexes;\n   private List<QueryableIndex> qIndexes;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SearchBenchmark.java",
                "sha": "b04f1e1dbb5177246c9c240c9234137b5eb77bb6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SerializingQueryRunner.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/SerializingQueryRunner.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/SerializingQueryRunner.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.fasterxml.jackson.core.JsonProcessingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.guava.Sequence;\n import org.apache.druid.java.util.common.guava.Sequences;\n import org.apache.druid.java.util.common.jackson.JacksonUtils;\n@@ -30,6 +31,10 @@\n \n public class SerializingQueryRunner<T> implements QueryRunner<T>\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private final ObjectMapper smileMapper;\n   private final QueryRunner<T> baseRunner;\n   private final Class<T> clazz;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SerializingQueryRunner.java",
                "sha": "cfdd30af391ad7bdcfec08f08ac48c60052b248b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.benchmark.datagen.SegmentGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -75,6 +76,7 @@\n public class SqlBenchmark\n {\n   static {\n+    NullHandling.initializeForTests();\n     Calcites.setSystemProperties();\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlBenchmark.java",
                "sha": "f51684a221a8e4d9e7d69782f20cd7ac59d6fe64",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlVsNativeBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlVsNativeBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlVsNativeBenchmark.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.benchmark.datagen.SegmentGenerator;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.Intervals;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.granularity.Granularities;\n@@ -80,6 +81,10 @@\n \n   private static final Logger log = new Logger(SqlVsNativeBenchmark.class);\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private SpecificSegmentsQuerySegmentWalker walker;\n   private PlannerFactory plannerFactory;\n   private GroupByQuery groupByQuery;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/SqlVsNativeBenchmark.java",
                "sha": "043405da1de33f5d8e9781b32814990e2388db8c",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/TimeseriesBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/TimeseriesBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/TimeseriesBenchmark.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkDataGenerator;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.Intervals;\n@@ -117,6 +118,10 @@\n   private static final IndexIO INDEX_IO;\n   public static final ObjectMapper JSON_MAPPER;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private List<IncrementalIndex> incIndexes;\n   private List<QueryableIndex> qIndexes;\n   private File tmpDir;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/TimeseriesBenchmark.java",
                "sha": "f8e66536dc4420e50b8ed72cdd34cc9ad42fd81b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/TopNBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/TopNBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/TopNBenchmark.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemaInfo;\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.concurrent.Execs;\n@@ -117,6 +118,10 @@\n   private static final IndexIO INDEX_IO;\n   public static final ObjectMapper JSON_MAPPER;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private List<IncrementalIndex> incIndexes;\n   private List<QueryableIndex> qIndexes;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/TopNBenchmark.java",
                "sha": "df4c22740418354e53194d4b48a0b9136131917d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.druid.benchmark.datagen.BenchmarkSchemas;\n import org.apache.druid.benchmark.query.QueryBenchmarkUtil;\n import org.apache.druid.collections.StupidPool;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.Intervals;\n@@ -119,6 +120,7 @@\n   protected static final Map<String, String> SCRIPT_DOUBLE_SUM = new HashMap<>();\n \n   static {\n+    NullHandling.initializeForTests();\n     SCRIPT_DOUBLE_SUM.put(\"fnAggregate\", \"function aggregate(current, a) { return current + a }\");\n     SCRIPT_DOUBLE_SUM.put(\"fnReset\", \"function reset() { return 0 }\");\n     SCRIPT_DOUBLE_SUM.put(\"fnCombine\", \"function combine(a,b) { return a + b }\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "sha": "92805ad46f05a9e13b91399ad1882c9c55ea939a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/main/java/org/apache/druid/common/config/NullHandling.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/common/config/NullHandling.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 6,
                "filename": "core/src/main/java/org/apache/druid/common/config/NullHandling.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.common.config;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Strings;\n import com.google.inject.Inject;\n \n@@ -31,8 +32,6 @@\n  */\n public class NullHandling\n {\n-  public static final String NULL_HANDLING_CONFIG_STRING = \"druid.generic.useDefaultValueForNull\";\n-\n   /**\n    * use these values to ensure that {@link NullHandling#defaultDoubleValue()},\n    * {@link NullHandling#defaultFloatValue()} , {@link NullHandling#defaultFloatValue()}\n@@ -50,15 +49,27 @@\n    * It does not take effect in all unit tests since we don't use Guice Injection.\n    */\n   @Inject\n-  private static NullValueHandlingConfig INSTANCE = new NullValueHandlingConfig(\n-      Boolean.valueOf(System.getProperty(NULL_HANDLING_CONFIG_STRING, \"true\"))\n-  );\n+  private static NullValueHandlingConfig INSTANCE;\n+\n+  /**\n+   * Many unit tests do not setup modules for this value to be injected, this method provides a manual way to initialize\n+   * {@link #INSTANCE}\n+   */\n+  @VisibleForTesting\n+  public static void initializeForTests()\n+  {\n+    INSTANCE = new NullValueHandlingConfig(null);\n+  }\n \n   /**\n    * whether nulls should be replaced with default value.\n    */\n   public static boolean replaceWithDefault()\n   {\n+    // this should only be null in a unit test context, in production this will be injected by the null handling module\n+    if (INSTANCE == null) {\n+      throw new IllegalStateException(\"NullHandling module not initialized, call NullHandling.initializeForTests()\");\n+    }\n     return INSTANCE.isUseDefaultValuesForNull();\n   }\n \n@@ -111,5 +122,4 @@ public static boolean isNullOrEquivalent(@Nullable String value)\n   {\n     return replaceWithDefault() ? Strings.isNullOrEmpty(value) : value == null;\n   }\n-\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/main/java/org/apache/druid/common/config/NullHandling.java",
                "sha": "f5c46e6de64a795032b226a7321baa81ca519e62",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/main/java/org/apache/druid/common/config/NullValueHandlingConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/common/config/NullValueHandlingConfig.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/main/java/org/apache/druid/common/config/NullValueHandlingConfig.java",
                "patch": "@@ -24,6 +24,7 @@\n \n public class NullValueHandlingConfig\n {\n+  private static final String NULL_HANDLING_CONFIG_STRING = \"druid.generic.useDefaultValueForNull\";\n \n   @JsonProperty(\"useDefaultValueForNull\")\n   private final boolean useDefaultValuesForNull;\n@@ -32,7 +33,7 @@\n   public NullValueHandlingConfig(@JsonProperty(\"useDefaultValueForNull\") Boolean useDefaultValuesForNull)\n   {\n     this.useDefaultValuesForNull = useDefaultValuesForNull == null\n-                                   ? true\n+                                   ? Boolean.valueOf(System.getProperty(NULL_HANDLING_CONFIG_STRING, \"true\"))\n                                    : useDefaultValuesForNull;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/main/java/org/apache/druid/common/config/NullValueHandlingConfig.java",
                "sha": "ae15d95969c27ce542f047bd2284a6b53f665990",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactoryTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactoryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "core/src/test/java/org/apache/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactoryTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.io.CountingOutputStream;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.FiniteFirehoseFactory;\n import org.apache.druid.data.input.Firehose;\n import org.apache.druid.data.input.InputSplit;\n@@ -94,6 +95,7 @@\n   @BeforeClass\n   public static void setup() throws IOException\n   {\n+    NullHandling.initializeForTests();\n     TEST_DIR = tempDir.newFolder();\n     for (int i = 0; i < 100; i++) {\n       try (",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactoryTest.java",
                "sha": "d3a09a02e0fc511871f8da9fdfd6e70aaa8204a6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/java/util/common/parsers/FlatTextFormatParserTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/java/util/common/parsers/FlatTextFormatParserTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/test/java/org/apache/druid/java/util/common/parsers/FlatTextFormatParserTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.parsers.AbstractFlatTextFormatParser.FlatTextFormat;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -38,7 +39,7 @@\n import java.util.stream.Collectors;\n \n @RunWith(Parameterized.class)\n-public class FlatTextFormatParserTest\n+public class FlatTextFormatParserTest extends InitializedNullHandlingTest\n {\n   @Parameters(name = \"{0}\")\n   public static Collection<Object[]> constructorFeeder()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/java/util/common/parsers/FlatTextFormatParserTest.java",
                "sha": "d9a498a026df78cd785f4519782f785315614dfc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/ApplyFunctionTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/math/expr/ApplyFunctionTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/test/java/org/apache/druid/math/expr/ApplyFunctionTest.java",
                "patch": "@@ -21,13 +21,14 @@\n \n import com.google.common.collect.ImmutableMap;\n import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n \n-public class ApplyFunctionTest\n+public class ApplyFunctionTest extends InitializedNullHandlingTest\n {\n   private Expr.ObjectBinding bindings;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/ApplyFunctionTest.java",
                "sha": "80d0410eeae14376e5c97ccf7b7b0f9ad2decb38",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/EvalTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/math/expr/EvalTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/test/java/org/apache/druid/math/expr/EvalTest.java",
                "patch": "@@ -21,12 +21,13 @@\n \n import com.google.common.collect.ImmutableMap;\n import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n /**\n  */\n-public class EvalTest\n+public class EvalTest extends InitializedNullHandlingTest\n {\n   private long evalLong(String x, Expr.ObjectBinding bindings)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/EvalTest.java",
                "sha": "732e744fd8fe2e017833b9aa0f3930083f839d4c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/FunctionTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/math/expr/FunctionTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/test/java/org/apache/druid/math/expr/FunctionTest.java",
                "patch": "@@ -21,11 +21,12 @@\n \n import com.google.common.collect.ImmutableMap;\n import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n \n-public class FunctionTest\n+public class FunctionTest extends InitializedNullHandlingTest\n {\n   private Expr.ObjectBinding bindings;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/FunctionTest.java",
                "sha": "5571096ccb4f7c550b3f16d693fe8e84215a2a72",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/ParserTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/math/expr/ParserTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "core/src/test/java/org/apache/druid/math/expr/ParserTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -32,7 +33,7 @@\n /**\n  *\n  */\n-public class ParserTest\n+public class ParserTest extends InitializedNullHandlingTest\n {\n   @Test\n   public void testSimple()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/math/expr/ParserTest.java",
                "sha": "27a9eb4abde41ad0154b7785dde8cf09cf3c475e",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/testing/InitializedNullHandlingTest.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/testing/InitializedNullHandlingTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "core/src/test/java/org/apache/druid/testing/InitializedNullHandlingTest.java",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing;\n+\n+import org.apache.druid.common.config.NullHandling;\n+\n+public class InitializedNullHandlingTest\n+{\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/core/src/test/java/org/apache/druid/testing/InitializedNullHandlingTest.java",
                "sha": "a4a737b4c75179079deb6c643d80b895dd2959fb",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountGroupByQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountGroupByQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountGroupByQueryTest.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.druid.segment.TestHelper;\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -50,7 +51,7 @@\n import java.util.Collections;\n import java.util.List;\n \n-public class DistinctCountGroupByQueryTest\n+public class DistinctCountGroupByQueryTest extends InitializedNullHandlingTest\n {\n   private GroupByQueryRunnerFactory factory;\n   private Closer resourceCloser;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountGroupByQueryTest.java",
                "sha": "08c286989f4a385085861394303f11c8d8822607",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTimeseriesQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTimeseriesQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTimeseriesQueryTest.java",
                "patch": "@@ -35,13 +35,14 @@\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.DateTime;\n import org.junit.Test;\n \n import java.util.Collections;\n import java.util.List;\n \n-public class DistinctCountTimeseriesQueryTest\n+public class DistinctCountTimeseriesQueryTest extends InitializedNullHandlingTest\n {\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTimeseriesQueryTest.java",
                "sha": "2cc0526480bf698aa14b6c043afe8506621ca057",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTopNQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTopNQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTopNQueryTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.DateTime;\n import org.junit.After;\n import org.junit.Before;\n@@ -48,7 +49,7 @@\n import java.util.List;\n import java.util.Map;\n \n-public class DistinctCountTopNQueryTest\n+public class DistinctCountTopNQueryTest extends InitializedNullHandlingTest\n {\n   private CloseableStupidPool<ByteBuffer> pool;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/distinctcount/src/test/java/org/apache/druid/query/aggregation/distinctcount/DistinctCountTopNQueryTest.java",
                "sha": "7b14fba07f118430073f03e0c1dfb7bbc5392cc9",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/materialized-view-selection/pom.xml",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/materialized-view-selection/pom.xml?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/materialized-view-selection/pom.xml",
                "patch": "@@ -107,7 +107,13 @@\n             <artifactId>guava</artifactId>\n             <scope>provided</scope>\n         </dependency>\n-\n+        <dependency>\n+            <groupId>org.apache.druid</groupId>\n+            <artifactId>druid-core</artifactId>\n+            <version>${project.parent.version}</version>\n+            <scope>test</scope>\n+            <type>test-jar</type>\n+        </dependency>\n         <dependency>\n             <groupId>org.apache.druid</groupId>\n             <artifactId>druid-processing</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/materialized-view-selection/pom.xml",
                "sha": "21346f630910c4eef6bcddce9225726cbc530ba2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/materialized-view-selection/src/test/java/org/apache/druid/query/materializedview/MaterializedViewUtilsTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/materialized-view-selection/src/test/java/org/apache/druid/query/materializedview/MaterializedViewUtilsTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/materialized-view-selection/src/test/java/org/apache/druid/query/materializedview/MaterializedViewUtilsTest.java",
                "patch": "@@ -27,14 +27,15 @@\n import org.apache.druid.query.timeseries.TimeseriesQuery;\n import org.apache.druid.query.topn.TopNQuery;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.Interval;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.util.List;\n import java.util.Set;\n \n-public class MaterializedViewUtilsTest \n+public class MaterializedViewUtilsTest extends InitializedNullHandlingTest\n {\n   private static ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n   ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/materialized-view-selection/src/test/java/org/apache/druid/query/materializedview/MaterializedViewUtilsTest.java",
                "sha": "fe00a2ce318cb766ab7fe9d4eb65c3d816d28b4a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/momentsketch/src/test/java/org/apache/druid/query/aggregation/momentsketch/aggregator/MomentsSketchAggregatorTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/momentsketch/src/test/java/org/apache/druid/query/aggregation/momentsketch/aggregator/MomentsSketchAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "extensions-contrib/momentsketch/src/test/java/org/apache/druid/query/aggregation/momentsketch/aggregator/MomentsSketchAggregatorTest.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.druid.query.groupby.GroupByQueryConfig;\n import org.apache.druid.query.groupby.GroupByQueryRunnerTest;\n import org.apache.druid.query.groupby.ResultRow;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -44,9 +45,8 @@\n import java.util.List;\n \n @RunWith(Parameterized.class)\n-public class MomentsSketchAggregatorTest\n+public class MomentsSketchAggregatorTest extends InitializedNullHandlingTest\n {\n-\n   private final AggregationTestHelper helper;\n \n   @Rule",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/momentsketch/src/test/java/org/apache/druid/query/aggregation/momentsketch/aggregator/MomentsSketchAggregatorTest.java",
                "sha": "f75de5ed24912544bd0f2fc984038e5d09e4715d",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/pom.xml",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/moving-average-query/pom.xml?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 11,
                "filename": "extensions-contrib/moving-average-query/pom.xml",
                "patch": "@@ -38,17 +38,6 @@\n   </properties>\n \n   <dependencies>\n-    <dependency>\n-      <groupId>junit</groupId>\n-      <artifactId>junit</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n-    <dependency>\n-      <groupId>com.fasterxml.jackson.dataformat</groupId>\n-      <artifactId>jackson-dataformat-yaml</artifactId>\n-      <version>${jackson.version}</version>\n-      <scope>test</scope>\n-    </dependency>\n     <dependency>\n       <groupId>org.apache.druid</groupId>\n       <artifactId>druid-core</artifactId>\n@@ -108,6 +97,25 @@\n       <scope>provided</scope>\n     </dependency>\n \n+    <!-- test -->\n+    <dependency>\n+      <groupId>org.apache.druid</groupId>\n+      <artifactId>druid-core</artifactId>\n+      <version>${project.parent.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>junit</groupId>\n+      <artifactId>junit</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.dataformat</groupId>\n+      <artifactId>jackson-dataformat-yaml</artifactId>\n+      <version>${jackson.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n     <dependency>\n       <groupId>org.hamcrest</groupId>\n       <artifactId>hamcrest-core</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/pom.xml",
                "sha": "d50c2ec0217b0721b8df06d3a6b8ae1a8c780166",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageIterableTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageIterableTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 3,
                "filename": "extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageIterableTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.query.movingaverage;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.MapBasedRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -33,6 +34,7 @@\n import org.apache.druid.query.movingaverage.averagers.AveragerFactory;\n import org.apache.druid.query.movingaverage.averagers.ConstantAveragerFactory;\n import org.apache.druid.query.movingaverage.averagers.LongMeanAveragerFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.hamcrest.CoreMatchers;\n import org.joda.time.DateTime;\n import org.joda.time.chrono.ISOChronology;\n@@ -47,7 +49,7 @@\n import java.util.List;\n import java.util.Map;\n \n-public class MovingAverageIterableTest\n+public class MovingAverageIterableTest extends InitializedNullHandlingTest\n {\n   private static final DateTime JAN_1 = new DateTime(2017, 1, 1, 0, 0, 0, 0, ISOChronology.getInstanceUTC());\n   private static final DateTime JAN_2 = new DateTime(2017, 1, 2, 0, 0, 0, 0, ISOChronology.getInstanceUTC());\n@@ -612,7 +614,7 @@ public void testMissingDaysAtBegining()\n   public void testMissingDaysInMiddle()\n   {\n     System.setProperty(\"druid.generic.useDefaultValueForNull\", \"true\");\n-\n+    NullHandling.initializeForTests();\n     Map<String, Object> event1 = new HashMap<>();\n     Map<String, Object> event2 = new HashMap<>();\n \n@@ -732,7 +734,7 @@ public void testWithFilteredAggregation()\n   public void testMissingDaysAtEnd()\n   {\n     System.setProperty(\"druid.generic.useDefaultValueForNull\", \"true\");\n-\n+    NullHandling.initializeForTests();\n     Map<String, Object> event1 = new HashMap<>();\n     Map<String, Object> event2 = new HashMap<>();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageIterableTest.java",
                "sha": "589b03d6d20e3264f6ed0c144ad953cb5e477682",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageQueryTest.java",
                "patch": "@@ -67,6 +67,7 @@\n import org.apache.druid.query.timeseries.TimeseriesResultValue;\n import org.apache.druid.server.ClientQuerySegmentWalker;\n import org.apache.druid.server.initialization.ServerConfig;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.druid.timeline.TimelineLookup;\n import org.hamcrest.core.IsInstanceOf;\n import org.joda.time.Interval;\n@@ -91,7 +92,7 @@\n  * Base class for implementing MovingAverageQuery tests\n  */\n @RunWith(Parameterized.class)\n-public class MovingAverageQueryTest\n+public class MovingAverageQueryTest extends InitializedNullHandlingTest\n {\n   private final ObjectMapper jsonMapper;\n   private final QueryToolChestWarehouse warehouse;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/MovingAverageQueryTest.java",
                "sha": "ca9e4c5ec18b2c989fc60b7c754747f2f4d20fef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/PostAveragerAggregatorCalculatorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/PostAveragerAggregatorCalculatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/PostAveragerAggregatorCalculatorTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.query.movingaverage;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.MapBasedRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.java.util.common.granularity.Granularities;\n@@ -50,6 +51,7 @@\n   public void setup()\n   {\n     System.setProperty(\"druid.generic.useDefaultValueForNull\", \"true\");\n+    NullHandling.initializeForTests();\n     MovingAverageQuery query = new MovingAverageQuery(\n         new TableDataSource(\"d\"),\n         new MultipleIntervalSegmentSpec(Collections.singletonList(new Interval(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/moving-average-query/src/test/java/org/apache/druid/query/movingaverage/PostAveragerAggregatorCalculatorTest.java",
                "sha": "280cedd1088c2df91e244dc41306c2a2921a8df4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/tdigestsketch/src/test/java/org/apache/druid/query/aggregation/tdigestsketch/TDigestSketchAggregatorTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/tdigestsketch/src/test/java/org/apache/druid/query/aggregation/tdigestsketch/TDigestSketchAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "extensions-contrib/tdigestsketch/src/test/java/org/apache/druid/query/aggregation/tdigestsketch/TDigestSketchAggregatorTest.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.druid.query.groupby.GroupByQueryConfig;\n import org.apache.druid.query.groupby.GroupByQueryRunnerTest;\n import org.apache.druid.query.groupby.ResultRow;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -41,9 +42,8 @@\n import java.util.List;\n \n @RunWith(Parameterized.class)\n-public class TDigestSketchAggregatorTest\n+public class TDigestSketchAggregatorTest extends InitializedNullHandlingTest\n {\n-\n   private final AggregationTestHelper helper;\n \n   @Rule",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/tdigestsketch/src/test/java/org/apache/druid/query/aggregation/tdigestsketch/TDigestSketchAggregatorTest.java",
                "sha": "d5577b9bcc2e4c076e2a7cbdf79bf7e05295905f",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/pom.xml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/virtual-columns/pom.xml?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-contrib/virtual-columns/pom.xml",
                "patch": "@@ -88,6 +88,13 @@\n             <artifactId>easymock</artifactId>\n             <scope>test</scope>\n         </dependency>\n+        <dependency>\n+            <groupId>org.apache.druid</groupId>\n+            <artifactId>druid-core</artifactId>\n+            <version>${project.parent.version}</version>\n+            <type>test-jar</type>\n+            <scope>test</scope>\n+        </dependency>\n         <dependency>\n             <groupId>org.apache.druid</groupId>\n             <artifactId>druid-processing</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/pom.xml",
                "sha": "0e8d3a14bd890c6aea76a4258a8a2f1f327b5059",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnGroupByTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnGroupByTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnGroupByTest.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.druid.query.groupby.strategy.GroupByStrategyV2;\n import org.apache.druid.query.spec.MultipleIntervalSegmentSpec;\n import org.apache.druid.segment.incremental.IncrementalIndex;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.druid.timeline.SegmentId;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -56,7 +57,7 @@\n import java.util.List;\n import java.util.stream.Collectors;\n \n-public class MapVirtualColumnGroupByTest\n+public class MapVirtualColumnGroupByTest extends InitializedNullHandlingTest\n {\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnGroupByTest.java",
                "sha": "d335025239fdc9ac899254178b25a91f4442770f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTestBase.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTestBase.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTestBase.java",
                "patch": "@@ -27,13 +27,14 @@\n import org.apache.druid.java.util.common.DateTimes;\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n \n import java.io.IOException;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.Map;\n \n-public class MapVirtualColumnTestBase\n+public class MapVirtualColumnTestBase extends InitializedNullHandlingTest\n {\n   static IncrementalIndex generateIndex() throws IOException\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTestBase.java",
                "sha": "4d2164d7ec213dc61e64f29c5402aa7a52308fe1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTopNTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTopNTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTopNTest.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.druid.query.topn.TopNQueryRunnerFactory;\n import org.apache.druid.query.topn.TopNResultValue;\n import org.apache.druid.segment.incremental.IncrementalIndex;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.druid.timeline.SegmentId;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -52,7 +53,7 @@\n import java.util.Collections;\n import java.util.List;\n \n-public class MapVirtualColumnTopNTest\n+public class MapVirtualColumnTopNTest extends InitializedNullHandlingTest\n {\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-contrib/virtual-columns/src/test/java/org/apache/druid/segment/MapVirtualColumnTopNTest.java",
                "sha": "345698564f35c0a806e6c0318755807949fb5a44",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/hll/HllSketchAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/hll/HllSketchAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/hll/HllSketchAggregatorTest.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.druid.query.groupby.GroupByQueryConfig;\n import org.apache.druid.query.groupby.GroupByQueryRunnerTest;\n import org.apache.druid.query.groupby.ResultRow;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -44,7 +45,7 @@\n import java.util.Map;\n \n @RunWith(Parameterized.class)\n-public class HllSketchAggregatorTest\n+public class HllSketchAggregatorTest extends InitializedNullHandlingTest\n {\n   private static final boolean ROUND = true;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/hll/HllSketchAggregatorTest.java",
                "sha": "f6a13060ce4a4ffaf9d84563ee2ab4b332eb5b89",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterAggregatorTest.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.druid.segment.DoubleColumnSelector;\n import org.apache.druid.segment.FloatColumnSelector;\n import org.apache.druid.segment.LongColumnSelector;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -55,7 +56,7 @@\n import java.util.List;\n import java.util.stream.IntStream;\n \n-public class BloomFilterAggregatorTest\n+public class BloomFilterAggregatorTest extends InitializedNullHandlingTest\n {\n   private static final String NULLISH = NullHandling.replaceWithDefault() ? \"\" : null;\n   private static final List<String[]> VALUES1 = dimensionValues(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterAggregatorTest.java",
                "sha": "5888b7d13ddea31715b111ef8aa6c0f6f7ab21c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterGroupByQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterGroupByQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterGroupByQueryTest.java",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.druid.query.groupby.ResultRow;\n import org.apache.druid.query.groupby.strategy.GroupByStrategySelector;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -51,7 +52,7 @@\n import java.util.List;\n \n @RunWith(Parameterized.class)\n-public class BloomFilterGroupByQueryTest\n+public class BloomFilterGroupByQueryTest extends InitializedNullHandlingTest\n {\n   private static final BloomFilterExtensionModule MODULE = new BloomFilterExtensionModule();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/BloomFilterGroupByQueryTest.java",
                "sha": "8b664ff3464aac95196ef95dd486a79b3506ea7a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/sql/BloomFilterSqlAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/sql/BloomFilterSqlAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/sql/BloomFilterSqlAggregatorTest.java",
                "patch": "@@ -77,6 +77,7 @@\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n import org.apache.druid.sql.calcite.util.SpecificSegmentsQuerySegmentWalker;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.druid.timeline.DataSegment;\n import org.apache.druid.timeline.partition.LinearShardSpec;\n import org.junit.After;\n@@ -92,7 +93,7 @@\n import java.util.Collections;\n import java.util.List;\n \n-public class BloomFilterSqlAggregatorTest\n+public class BloomFilterSqlAggregatorTest extends InitializedNullHandlingTest\n {\n   private static final int TEST_NUM_ENTRIES = 1000;\n   private static AuthenticationResult authenticationResult = CalciteTests.REGULAR_USER_AUTH_RESULT;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/druid-bloom-filter/src/test/java/org/apache/druid/query/aggregation/bloom/sql/BloomFilterSqlAggregatorTest.java",
                "sha": "6c63e1943bd3ceb964faa597b6a54b44e4aac653",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregationTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregationTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregationTest.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.druid.query.groupby.GroupByQueryConfig;\n import org.apache.druid.query.groupby.GroupByQueryRunnerTest;\n import org.apache.druid.query.groupby.ResultRow;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -46,7 +47,7 @@\n  *\n  */\n @RunWith(Parameterized.class)\n-public class ApproximateHistogramAggregationTest\n+public class ApproximateHistogramAggregationTest extends InitializedNullHandlingTest\n {\n   private AggregationTestHelper helper;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregationTest.java",
                "sha": "da752a256c955d7eb4349452e334d35e60812843",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregatorTest.java",
                "patch": "@@ -22,12 +22,13 @@\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.query.aggregation.BufferAggregator;\n import org.apache.druid.query.aggregation.TestFloatColumnSelector;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.nio.ByteBuffer;\n \n-public class ApproximateHistogramAggregatorTest\n+public class ApproximateHistogramAggregatorTest extends InitializedNullHandlingTest\n {\n   private void aggregateBuffer(TestFloatColumnSelector selector, BufferAggregator agg, ByteBuffer buf, int position)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramAggregatorTest.java",
                "sha": "fd41e150214bea3626b1cc6af9d539cece0f65c4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramGroupByQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramGroupByQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramGroupByQueryTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.druid.query.groupby.orderby.OrderByColumnSpec;\n import org.apache.druid.query.groupby.strategy.GroupByStrategySelector;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -50,7 +51,7 @@\n /**\n  */\n @RunWith(Parameterized.class)\n-public class ApproximateHistogramGroupByQueryTest\n+public class ApproximateHistogramGroupByQueryTest extends InitializedNullHandlingTest\n {\n   private static final Closer RESOURCE_CLOSER = Closer.create();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramGroupByQueryTest.java",
                "sha": "3e941d232b5603f5979f497b1492674c94d00656",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramPostAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramPostAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramPostAggregatorTest.java",
                "patch": "@@ -20,13 +20,14 @@\n package org.apache.druid.query.aggregation.histogram;\n \n import org.apache.druid.query.aggregation.TestFloatColumnSelector;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.util.HashMap;\n import java.util.Map;\n \n-public class ApproximateHistogramPostAggregatorTest\n+public class ApproximateHistogramPostAggregatorTest extends InitializedNullHandlingTest\n {\n   static final float[] VALUES = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramPostAggregatorTest.java",
                "sha": "045ba03dd7ece3ad7e34b4921248a122bd960c13",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTopNQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTopNQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTopNQueryTest.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.druid.query.topn.TopNQueryRunnerFactory;\n import org.apache.druid.query.topn.TopNResultValue;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.AfterClass;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -52,7 +53,7 @@\n import java.util.Map;\n \n @RunWith(Parameterized.class)\n-public class ApproximateHistogramTopNQueryTest\n+public class ApproximateHistogramTopNQueryTest extends InitializedNullHandlingTest\n {\n   private static final Closer RESOURCE_CLOSER = Closer.create();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTopNQueryTest.java",
                "sha": "d61114a06ffd7465417ea80f976744895e80b355",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramAggregationTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramAggregationTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramAggregationTest.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.druid.query.groupby.GroupByQueryConfig;\n import org.apache.druid.query.groupby.GroupByQueryRunnerTest;\n import org.apache.druid.query.groupby.ResultRow;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -46,7 +47,7 @@\n  *\n  */\n @RunWith(Parameterized.class)\n-public class FixedBucketsHistogramAggregationTest\n+public class FixedBucketsHistogramAggregationTest extends InitializedNullHandlingTest\n {\n   private AggregationTestHelper helper;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramAggregationTest.java",
                "sha": "198d62e4185dcb22321dc92e17db2d30462f4b0f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramGroupByQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramGroupByQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramGroupByQueryTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.druid.query.groupby.orderby.OrderByColumnSpec;\n import org.apache.druid.query.groupby.strategy.GroupByStrategySelector;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -50,7 +51,7 @@\n /**\n  */\n @RunWith(Parameterized.class)\n-public class FixedBucketsHistogramGroupByQueryTest\n+public class FixedBucketsHistogramGroupByQueryTest extends InitializedNullHandlingTest\n {\n   private static final Closer RESOURCE_CLOSER = Closer.create();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramGroupByQueryTest.java",
                "sha": "793dcc3b0bebe8aa24a36ce05a225ca49b1f5b16",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramTopNQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramTopNQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramTopNQueryTest.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.druid.query.topn.TopNQueryRunnerFactory;\n import org.apache.druid.query.topn.TopNResultValue;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.AfterClass;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -52,7 +53,7 @@\n import java.util.Map;\n \n @RunWith(Parameterized.class)\n-public class FixedBucketsHistogramTopNQueryTest\n+public class FixedBucketsHistogramTopNQueryTest extends InitializedNullHandlingTest\n {\n   private static final Closer RESOURCE_CLOSER = Closer.create();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramTopNQueryTest.java",
                "sha": "b493745f6b0095a2fca50810591ed8545e0fa80a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaSamplerSpecTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaSamplerSpecTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaSamplerSpecTest.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.druid.segment.TestHelper;\n import org.apache.druid.segment.indexing.DataSchema;\n import org.apache.druid.segment.indexing.granularity.UniformGranularitySpec;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n import org.junit.AfterClass;\n@@ -61,7 +62,7 @@\n import java.util.List;\n import java.util.Map;\n \n-public class KafkaSamplerSpecTest\n+public class KafkaSamplerSpecTest extends InitializedNullHandlingTest\n {\n   private static final ObjectMapper OBJECT_MAPPER = TestHelper.makeJsonMapper();\n   private static final String TOPIC = \"sampling\";",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaSamplerSpecTest.java",
                "sha": "629f0aa64f3b3ce6330c3a61274ce5dc8387258e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisSamplerSpecTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisSamplerSpecTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisSamplerSpecTest.java",
                "patch": "@@ -25,6 +25,7 @@\n import com.google.common.collect.ImmutableSet;\n import org.apache.druid.client.cache.MapCache;\n import org.apache.druid.common.aws.AWSCredentialsConfig;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.Firehose;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n import org.apache.druid.data.input.impl.FloatDimensionSchema;\n@@ -103,6 +104,10 @@\n       OBJECT_MAPPER\n   );\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private final KinesisRecordSupplier recordSupplier = mock(KinesisRecordSupplier.class);\n \n   private static List<OrderedPartitionableRecord<String, String>> generateRecords(String stream)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisSamplerSpecTest.java",
                "sha": "7f50834cf9788860174692862ab0b49b447ca11f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import com.google.inject.Injector;\n import com.google.inject.Key;\n import com.google.inject.Module;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.guice.GuiceInjectors;\n import org.apache.druid.guice.JsonConfigProvider;\n import org.apache.druid.guice.annotations.Json;\n@@ -74,6 +75,10 @@\n @PowerMockIgnore({\"javax.net.ssl.*\", \"javax.xml.*\", \"com.sun.xml.*\"})\n public class NamespaceLookupExtractorFactoryTest\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+  \n   private final ObjectMapper mapper = new DefaultObjectMapper();\n   @Rule\n   public TemporaryFolder temporaryFolder = new TemporaryFolder();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "sha": "bab8259dbcfccfadec32f1b22bd87c37558a7600",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/namespace/UriExtractionNamespaceTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/namespace/UriExtractionNamespaceTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/namespace/UriExtractionNamespaceTest.java",
                "patch": "@@ -27,6 +27,7 @@\n import com.google.inject.Binder;\n import com.google.inject.Guice;\n import com.google.inject.Module;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.guice.GuiceAnnotationIntrospector;\n import org.apache.druid.guice.GuiceInjectableValues;\n import org.apache.druid.guice.annotations.Json;\n@@ -43,6 +44,10 @@\n  */\n public class UriExtractionNamespaceTest\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   public static ObjectMapper registerTypes(\n       final ObjectMapper mapper\n   )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/query/lookup/namespace/UriExtractionNamespaceTest.java",
                "sha": "d5ac42a068c717f07e4ef8fceaee2ed3d86d4d22",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "patch": "@@ -67,6 +67,10 @@\n @RunWith(Parameterized.class)\n public class JdbcExtractionNamespaceTest\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   @Rule\n   public final TestDerbyConnector.DerbyConnectorRule derbyConnectorRule = new TestDerbyConnector.DerbyConnectorRule();\n   private static final Logger log = new Logger(JdbcExtractionNamespaceTest.class);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "sha": "84086b8f6490c0a5d895b54819327b95523d7a7c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/pom.xml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-single/pom.xml?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-single/pom.xml",
                "patch": "@@ -103,6 +103,13 @@\n       <artifactId>junit</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.druid</groupId>\n+      <artifactId>druid-core</artifactId>\n+      <version>${project.parent.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n     <dependency>\n       <groupId>org.apache.druid</groupId>\n       <artifactId>druid-server</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/pom.xml",
                "sha": "61630a2db9b80e95c0f32a2f8c8cfb1ea113c4a7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/main/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcher.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-single/src/main/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcher.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-single/src/main/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcher.java",
                "patch": "@@ -43,6 +43,10 @@\n \n public class JdbcDataFetcher implements DataFetcher<String, String>\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final Logger LOGGER = new Logger(JdbcDataFetcher.class);\n   private static final int DEFAULT_STREAMING_FETCH_SIZE = 1000;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/main/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcher.java",
                "sha": "6336de583c30b087f35e447083a57d4f52ef701e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/LoadingLookupTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/LoadingLookupTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/LoadingLookupTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.collect.ImmutableSet;\n import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.server.lookup.cache.loading.LoadingCache;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -32,7 +33,7 @@\n import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutionException;\n \n-public class LoadingLookupTest\n+public class LoadingLookupTest extends InitializedNullHandlingTest\n {\n   DataFetcher dataFetcher = EasyMock.createMock(DataFetcher.class);\n   LoadingCache lookupCache = EasyMock.createStrictMock(LoadingCache.class);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/LoadingLookupTest.java",
                "sha": "93e147dee460ce0c4fce6559c1782eb5d02e093a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/PollingLookupTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/PollingLookupTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/PollingLookupTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.druid.server.lookup.cache.polling.OffHeapPollingCache;\n import org.apache.druid.server.lookup.cache.polling.OnHeapPollingCache;\n import org.apache.druid.server.lookup.cache.polling.PollingCacheFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -45,7 +46,7 @@\n import java.util.Map;\n \n @RunWith(Parameterized.class)\n-public class PollingLookupTest\n+public class PollingLookupTest extends InitializedNullHandlingTest\n {\n   private static final Map<String, String> FIRST_LOOKUP_MAP = ImmutableMap.of(\n       \"foo\", \"bar\",",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/PollingLookupTest.java",
                "sha": "cb31eb404f05cba1f704eb3cb0a5f1a3fafcd413",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcherTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcherTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcherTest.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.druid.metadata.MetadataStorageConnectorConfig;\n import org.apache.druid.metadata.TestDerbyConnector;\n import org.apache.druid.server.lookup.DataFetcher;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -43,7 +44,7 @@\n import java.util.Map;\n \n @RunWith(Enclosed.class)\n-public class JdbcDataFetcherTest\n+public class JdbcDataFetcherTest extends InitializedNullHandlingTest\n {\n   private static final String TABLE_NAME = \"tableName\";\n   private static final String KEY_COLUMN = \"keyColumn\";",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/lookups-cached-single/src/test/java/org/apache/druid/server/lookup/jdbc/JdbcDataFetcherTest.java",
                "sha": "7fc50e4842387825709732c6056cc12c4677f937",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorCollectorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorCollectorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorCollectorTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.segment.TestFloatColumnSelector;\n import org.apache.druid.segment.TestObjectColumnSelector;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -32,7 +33,7 @@\n import java.util.Random;\n import java.util.concurrent.ThreadLocalRandom;\n \n-public class VarianceAggregatorCollectorTest\n+public class VarianceAggregatorCollectorTest extends InitializedNullHandlingTest\n {\n   private static final float[] MARKET_UPFRONT = new float[]{\n       800.0f, 800.0f, 826.0602f, 1564.6177f, 1006.4021f, 869.64374f, 809.04175f, 1458.4027f, 852.4375f, 879.9881f,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorCollectorTest.java",
                "sha": "57ab6ea86d42cebd21505b4f4b894b793f7ea6bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.query.aggregation.TestFloatColumnSelector;\n import org.apache.druid.segment.ColumnSelectorFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -31,7 +32,7 @@\n \n /**\n  */\n-public class VarianceAggregatorTest\n+public class VarianceAggregatorTest extends InitializedNullHandlingTest\n {\n   private VarianceAggregatorFactory aggFactory;\n   private ColumnSelectorFactory colSelectorFactory;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceAggregatorTest.java",
                "sha": "33967f2ed03f338984a4109cfc92851086d9aed6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.druid.query.groupby.orderby.DefaultLimitSpec;\n import org.apache.druid.query.groupby.orderby.OrderByColumnSpec;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.Period;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -52,7 +53,7 @@\n  *\n  */\n @RunWith(Parameterized.class)\n-public class VarianceGroupByQueryTest\n+public class VarianceGroupByQueryTest extends InitializedNullHandlingTest\n {\n   private final GroupByQueryConfig config;\n   private final QueryRunner<Row> runner;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "sha": "9a04aee18b82e1f3f72514a864912fd3febcce3a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.druid.query.timeseries.TimeseriesQueryRunnerTest;\n import org.apache.druid.query.timeseries.TimeseriesResultValue;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -40,7 +41,7 @@\n import java.util.stream.StreamSupport;\n \n @RunWith(Parameterized.class)\n-public class VarianceTimeseriesQueryTest\n+public class VarianceTimeseriesQueryTest extends InitializedNullHandlingTest\n {\n   @Parameterized.Parameters(name = \"{0}:descending={1}\")\n   public static Iterable<Object[]> constructorFeeder()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "sha": "9c52961f7c643f8c25de942e1643706fb1f3fa5f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTopNQueryTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTopNQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTopNQueryTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.druid.query.topn.TopNQueryRunnerTest;\n import org.apache.druid.query.topn.TopNResultValue;\n import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -47,7 +48,7 @@\n import java.util.Map;\n \n @RunWith(Parameterized.class)\n-public class VarianceTopNQueryTest\n+public class VarianceTopNQueryTest extends InitializedNullHandlingTest\n {\n   @Parameterized.Parameters(name = \"{0}\")\n   public static Iterable<Object[]> constructorFeeder()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTopNQueryTest.java",
                "sha": "8003ad1c39491a3fd3fa1daf37bb913d774edf87",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/sql/VarianceSqlAggregatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/sql/VarianceSqlAggregatorTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/sql/VarianceSqlAggregatorTest.java",
                "patch": "@@ -63,6 +63,7 @@\n import org.apache.druid.sql.calcite.util.CalciteTests;\n import org.apache.druid.sql.calcite.util.QueryLogHook;\n import org.apache.druid.sql.calcite.util.SpecificSegmentsQuerySegmentWalker;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.apache.druid.timeline.DataSegment;\n import org.apache.druid.timeline.partition.LinearShardSpec;\n import org.junit.After;\n@@ -77,7 +78,7 @@\n import java.io.IOException;\n import java.util.List;\n \n-public class VarianceSqlAggregatorTest\n+public class VarianceSqlAggregatorTest extends InitializedNullHandlingTest\n {\n   private static AuthenticationResult authenticationResult = CalciteTests.REGULAR_USER_AUTH_RESULT;\n   private static final String DATA_SOURCE = \"numfoo\";",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/sql/VarianceSqlAggregatorTest.java",
                "sha": "a061f476f993906d49cb2ab41f09e1b72242ba8c",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/FirehoseSamplerTest.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/FirehoseSamplerTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 107,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/FirehoseSamplerTest.java",
                "patch": "@@ -24,7 +24,6 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import org.apache.druid.client.cache.MapCache;\n-import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.FirehoseFactory;\n import org.apache.druid.data.input.impl.DelimitedParseSpec;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n@@ -49,6 +48,7 @@\n import org.apache.druid.segment.indexing.granularity.UniformGranularitySpec;\n import org.apache.druid.segment.transform.ExpressionTransform;\n import org.apache.druid.segment.transform.TransformSpec;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -65,18 +65,14 @@\n import java.util.stream.Collectors;\n \n @RunWith(Parameterized.class)\n-public class FirehoseSamplerTest\n+public class FirehoseSamplerTest extends InitializedNullHandlingTest\n {\n   private enum ParserType\n   {\n     MAP, STR_JSON, STR_CSV\n   }\n \n   private static final ObjectMapper OBJECT_MAPPER = TestHelper.makeJsonMapper();\n-  private static final boolean USE_DEFAULT_VALUE_FOR_NULL = Boolean.valueOf(System.getProperty(\n-      NullHandling.NULL_HANDLING_CONFIG_STRING,\n-      \"true\"\n-  ));\n \n   private static final List<Object> MAP_ROWS = ImmutableList.of(\n       ImmutableMap.of(\"t\", \"2019-04-22T12:00\", \"dim1\", \"foo\", \"met1\", \"1\"),\n@@ -251,7 +247,7 @@ public void testCSVColumnAllNull()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         str_csv_rows.get(1).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -260,13 +256,13 @@ public void testCSVColumnAllNull()\n             put(\"LastName\", \"G\");\n             put(\"Gender\", \"Male\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         str_csv_rows.get(2).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -275,13 +271,13 @@ public void testCSVColumnAllNull()\n             put(\"LastName\", \"Bryant\");\n             put(\"Gender\", \"Male\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         str_csv_rows.get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -290,13 +286,13 @@ public void testCSVColumnAllNull()\n             put(\"LastName\", \" Krystal\");\n             put(\"Gender\", \"Female\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n     Assert.assertEquals(new SamplerResponseRow(\n         str_csv_rows.get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -305,7 +301,7 @@ public void testCSVColumnAllNull()\n             put(\"LastName\", \"Jackson\");\n             put(\"Gender\", \"Male\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(3));\n@@ -329,7 +325,7 @@ public void testMissingValueTimestampSpec()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -338,13 +334,13 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"1\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(1).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -353,13 +349,13 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"2\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(2).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -368,13 +364,13 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"3\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -383,13 +379,13 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", \"4\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(3));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -398,13 +394,13 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"5\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(4));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(5).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 0L);\n@@ -413,7 +409,7 @@ public void testMissingValueTimestampSpec()\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"6\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(5));\n@@ -437,71 +433,71 @@ public void testWithTimestampSpec()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"1\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(1).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"2\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(2).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934460000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"3\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", \"4\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(3));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", \"bar\");\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"5\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(4));\n@@ -537,66 +533,66 @@ public void testWithDimensionSpec()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"1\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(1).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"2\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(2).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934460000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"3\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", \"4\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(3));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", \"5\");\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(4));\n@@ -635,71 +631,71 @@ public void testWithNoRollup()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 1L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(1).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 2L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(2).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 3L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(3));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", \"bar\");\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 5L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(4));\n@@ -738,43 +734,43 @@ public void testWithRollup()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 6L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", \"bar\");\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 5L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n@@ -816,27 +812,27 @@ public void testWithMoreRollup()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 11L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n@@ -883,27 +879,27 @@ public void testWithMoreRollupCacheReplay()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 11L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n@@ -950,43 +946,43 @@ public void testWithTransformsAutoDimensions()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 6L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo2\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", \"bar\");\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 5L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(2));\n@@ -1039,27 +1035,27 @@ public void testWithTransformsDimensionsSpec()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1PlusBar\", \"foobar\");\n             put(\"met1\", 11L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(3).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim1PlusBar\", \"foo2bar\");\n             put(\"met1\", 4L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n@@ -1102,29 +1098,29 @@ public void testWithFilter()\n \n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(0).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", null);\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 6L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(0));\n     Assert.assertEquals(new SamplerResponseRow(\n         getTestRows().get(4).toString(),\n-        replaceNullValues(new HashMap<String, Object>()\n+        new HashMap<String, Object>()\n         {\n           {\n             put(\"__time\", 1555934400000L);\n             put(\"dim2\", \"bar\");\n             put(\"dim1\", \"foo\");\n             put(\"met1\", 5L);\n           }\n-        }),\n+        },\n         null,\n         null\n     ), data.get(1));\n@@ -1194,14 +1190,6 @@ private String getUnparseableTimestampString()\n            : \"Unparseable timestamp found! Event: {t=bad_timestamp, dim1=foo, met1=6}\";\n   }\n \n-  private List<SamplerResponseRow> removeEmptyColumns(List<SamplerResponseRow> rows)\n-  {\n-    return USE_DEFAULT_VALUE_FOR_NULL\n-           ? rows\n-           : rows.stream().map(x -> x.withParsed(removeEmptyValues(x.getParsed()))).collect(Collectors.toList());\n-  }\n-\n-\n   @Nullable\n   private Map<String, Object> removeEmptyValues(Map<String, Object> data)\n   {\n@@ -1211,19 +1199,4 @@ private String getUnparseableTimestampString()\n                         .filter(x -> !(x.getValue() instanceof String) || !((String) x.getValue()).isEmpty())\n                         .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n   }\n-\n-  @Nullable\n-  private Map<String, Object> replaceNullValues(Map<String, Object> data)\n-  {\n-    return ParserType.STR_CSV.equals(parserType)\n-           ? USE_DEFAULT_VALUE_FOR_NULL\n-             ? data\n-             : data.entrySet()\n-                   .stream()\n-                   .collect(Collectors.toMap(\n-                       Map.Entry::getKey,\n-                       e -> e.getValue() == null ? \"\" : e.getValue()\n-                   ))\n-           : data;\n-  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/FirehoseSamplerTest.java",
                "sha": "3ff3b4f2ef4ca395682b3b76b7defb37164f9692",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskTestBase.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskTestBase.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskTestBase.java",
                "patch": "@@ -29,6 +29,7 @@\n import com.google.common.io.Files;\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.ListeningExecutorService;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n import org.apache.druid.data.input.impl.FloatDimensionSchema;\n import org.apache.druid.data.input.impl.JSONParseSpec;\n@@ -133,6 +134,10 @@\n   protected static final Logger LOG = new Logger(SeekableStreamIndexTaskTestBase.class);\n   protected static ListeningExecutorService taskExec;\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   protected final List<Task> runningTasks = new ArrayList<>();\n   protected final LockGranularity lockGranularity;\n   protected File directory;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskTestBase.java",
                "sha": "b13c0b4129e5069e1a66fc26a25b092428a1cca0",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/main/java/org/apache/druid/segment/NilColumnValueSelector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/segment/NilColumnValueSelector.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 6,
                "filename": "processing/src/main/java/org/apache/druid/segment/NilColumnValueSelector.java",
                "patch": "@@ -45,8 +45,8 @@ private NilColumnValueSelector()\n   }\n \n   /**\n-   * always returns 0, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is set to true,\n-   * or always throws an exception, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is\n+   * always returns 0, if {@link NullHandling#replaceWithDefault} is set to true,\n+   * or always throws an exception, if {@link NullHandling#replaceWithDefault} is\n    * set to false.\n    */\n   @Override\n@@ -56,8 +56,8 @@ public double getDouble()\n   }\n \n   /**\n-   * always returns 0.0f, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is set to true,\n-   * or always throws an exception, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is\n+   * always returns 0.0f, if {@link NullHandling#replaceWithDefault} is set to true,\n+   * or always throws an exception, if {@link NullHandling#replaceWithDefault} is\n    * set to false.\n    */\n   @Override\n@@ -67,8 +67,8 @@ public float getFloat()\n   }\n \n   /**\n-   * always returns 0L, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is set to true,\n-   * or always throws an exception, if {@link NullHandling#NULL_HANDLING_CONFIG_STRING} is\n+   * always returns 0L, if {@link NullHandling#replaceWithDefault} is set to true,\n+   * or always throws an exception, if {@link NullHandling#replaceWithDefault} is\n    * set to false.\n    */\n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/main/java/org/apache/druid/segment/NilColumnValueSelector.java",
                "sha": "98d6ee6459208ed75f8db49ccdcd4ef9c567693b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/collections/bitmap/BitmapBenchmark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/collections/bitmap/BitmapBenchmark.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/collections/bitmap/BitmapBenchmark.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.carrotsearch.junitbenchmarks.BenchmarkOptions;\n import com.carrotsearch.junitbenchmarks.BenchmarkRule;\n import com.carrotsearch.junitbenchmarks.Clock;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.extendedset.intset.ImmutableConciseSet;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -66,6 +67,10 @@\n   @Rule\n   public TestRule benchmarkRun = new BenchmarkRule();\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   protected static ImmutableConciseSet makeOffheapConcise(ImmutableConciseSet concise)\n   {\n     final byte[] bytes = concise.toBytes();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/collections/bitmap/BitmapBenchmark.java",
                "sha": "a0689b8979e7b051a39a600b92ca25fd8e337e5f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/expression/MacroTestBase.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/expression/MacroTestBase.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/query/expression/MacroTestBase.java",
                "patch": "@@ -19,10 +19,11 @@\n \n package org.apache.druid.query.expression;\n \n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Rule;\n import org.junit.rules.ExpectedException;\n \n-public abstract class MacroTestBase\n+public abstract class MacroTestBase extends InitializedNullHandlingTest\n {\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/expression/MacroTestBase.java",
                "sha": "38e607c2b8f44fbff9b5b2b1533ce9a0aaef4895",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataQueryTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataQueryTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.impl.TimestampSpec;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.Intervals;\n@@ -73,6 +74,10 @@\n @RunWith(Parameterized.class)\n public class SegmentMetadataQueryTest\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final SegmentMetadataQueryRunnerFactory FACTORY = new SegmentMetadataQueryRunnerFactory(\n       new SegmentMetadataQueryQueryToolChest(new SegmentMetadataQueryConfig()),\n       QueryRunnerTestHelper.NOOP_QUERYWATCHER",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataQueryTest.java",
                "sha": "715e3c8b89e4bb7b40d0791b7bb2d69850bc2ecf",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataUnionQueryTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataUnionQueryTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataUnionQueryTest.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.Intervals;\n import org.apache.druid.query.Druids;\n import org.apache.druid.query.QueryPlus;\n@@ -46,6 +47,10 @@\n @RunWith(Parameterized.class)\n public class SegmentMetadataUnionQueryTest\n {\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static final QueryRunnerFactory FACTORY = new SegmentMetadataQueryRunnerFactory(\n       new SegmentMetadataQueryQueryToolChest(new SegmentMetadataQueryConfig()),\n       QueryRunnerTestHelper.NOOP_QUERYWATCHER",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/query/metadata/SegmentMetadataUnionQueryTest.java",
                "sha": "76812023d11b4d5135d64ebe63df34fbe78b9155",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/IndexIOTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/IndexIOTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "processing/src/test/java/org/apache/druid/segment/IndexIOTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.data.input.MapBasedInputRow;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n import org.apache.druid.java.util.common.Intervals;\n@@ -39,6 +40,7 @@\n import org.apache.druid.segment.incremental.IncrementalIndexAdapter;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.segment.incremental.IndexSizeExceededException;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.Interval;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -61,7 +63,7 @@\n  * This is mostly a test of the validator\n  */\n @RunWith(Parameterized.class)\n-public class IndexIOTest\n+public class IndexIOTest extends InitializedNullHandlingTest\n {\n   private static Interval DEFAULT_INTERVAL = Intervals.of(\"1970-01-01/2000-01-01\");\n   private static final IndexSpec INDEX_SPEC = IndexMergerTestBase.makeIndexSpec(\n@@ -71,6 +73,10 @@\n       CompressionFactory.LongEncodingStrategy.LONGS\n   );\n \n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n   private static <T> List<T> filterByBitset(List<T> list, BitSet bitSet)\n   {\n     final ArrayList<T> outList = new ArrayList<>(bitSet.cardinality());\n@@ -85,7 +91,6 @@\n   @Parameterized.Parameters(name = \"{0}, {1}\")\n   public static Iterable<Object[]> constructionFeeder()\n   {\n-\n     final Map<String, Object> map = ImmutableMap.of();\n \n     final Map<String, Object> map00 = ImmutableMap.of(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/IndexIOTest.java",
                "sha": "2fb6bc39eaa4f34d0ba4a4a85e5cb3476cf43152",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/IndexMergerTestBase.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/IndexMergerTestBase.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "processing/src/test/java/org/apache/druid/segment/IndexMergerTestBase.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.apache.druid.segment.incremental.IncrementalIndexAdapter;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.segment.writeout.SegmentWriteOutMediumFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.Interval;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -81,9 +82,8 @@\n import java.util.Map;\n import java.util.stream.Collectors;\n \n-public class IndexMergerTestBase\n+public class IndexMergerTestBase extends InitializedNullHandlingTest\n {\n-\n   @Rule\n   public final TemporaryFolder temporaryFolder = new TemporaryFolder();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/IndexMergerTestBase.java",
                "sha": "1783f433b1b599f304eaf759075f32c10c4d729c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedStringWriterTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedStringWriterTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/data/GenericIndexedStringWriterTest.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.segment.data;\n \n import org.apache.druid.segment.writeout.OnHeapMemorySegmentWriteOutMedium;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -29,7 +30,7 @@\n import java.util.List;\n import java.util.concurrent.ThreadLocalRandom;\n \n-public class GenericIndexedStringWriterTest\n+public class GenericIndexedStringWriterTest extends InitializedNullHandlingTest\n {\n   @Test\n   public void testRandomAccess() throws IOException",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedStringWriterTest.java",
                "sha": "5b4a2d95d8e9ac821c743a7430264b03699cb177",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.segment.data;\n \n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -32,7 +33,7 @@\n \n /**\n  */\n-public class GenericIndexedTest\n+public class GenericIndexedTest extends InitializedNullHandlingTest\n {\n   @Test(expected = UnsupportedOperationException.class)\n   public void testNotSortedNoIndexOf()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java",
                "sha": "52e5087f0f3753bcdbe50f9850521fd959499370",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java",
                "patch": "@@ -66,6 +66,7 @@\n import org.apache.druid.segment.incremental.IncrementalIndex.Builder;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.segment.incremental.IndexSizeExceededException;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.Interval;\n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -92,7 +93,7 @@\n /**\n  */\n @RunWith(Parameterized.class)\n-public class IncrementalIndexTest\n+public class IncrementalIndexTest extends InitializedNullHandlingTest\n {\n   interface IndexCreator\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java",
                "sha": "11d5bdc677746bae496b7110294375c5eaa9044f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/filter/BaseFilterTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/filter/BaseFilterTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/filter/BaseFilterTest.java",
                "patch": "@@ -70,6 +70,7 @@\n import org.apache.druid.segment.writeout.OffHeapMemorySegmentWriteOutMediumFactory;\n import org.apache.druid.segment.writeout.SegmentWriteOutMediumFactory;\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -87,7 +88,7 @@\n import java.util.Map;\n import java.util.Set;\n \n-public abstract class BaseFilterTest\n+public abstract class BaseFilterTest extends InitializedNullHandlingTest\n {\n   static final VirtualColumns VIRTUAL_COLUMNS = VirtualColumns.create(\n       ImmutableList.of(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/filter/BaseFilterTest.java",
                "sha": "cdfe8644b1ef70f4001a3602c878edee6a0f37d4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexAdapterTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexAdapterTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexAdapterTest.java",
                "patch": "@@ -29,14 +29,15 @@\n import org.apache.druid.segment.data.CompressionStrategy;\n import org.apache.druid.segment.data.ConciseBitmapSerdeFactory;\n import org.apache.druid.segment.data.IncrementalIndexTest;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n import java.util.function.Function;\n \n-public class IncrementalIndexAdapterTest\n+public class IncrementalIndexAdapterTest extends InitializedNullHandlingTest\n {\n   private static final IndexSpec INDEX_SPEC = new IndexSpec(\n       new ConciseBitmapSerdeFactory(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexAdapterTest.java",
                "sha": "32a26a521c0a5e60187cd651ef1396601bf5e0c2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexMultiValueSpecTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexMultiValueSpecTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexMultiValueSpecTest.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.segment.VirtualColumns;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -38,7 +39,7 @@\n \n /**\n  */\n-public class IncrementalIndexMultiValueSpecTest\n+public class IncrementalIndexMultiValueSpecTest extends InitializedNullHandlingTest\n {\n   @Test\n   public void test() throws IndexSizeExceededException",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexMultiValueSpecTest.java",
                "sha": "dfd386f3e92ed5c6eea7b8fe53925d78765251e2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowCompTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowCompTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowCompTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.Lists;\n import org.apache.druid.data.input.MapBasedInputRow;\n import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -32,7 +33,7 @@\n \n /**\n  */\n-public class IncrementalIndexRowCompTest\n+public class IncrementalIndexRowCompTest extends InitializedNullHandlingTest\n {\n   @Test\n   public void testBasic()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowCompTest.java",
                "sha": "94e17c645e53fd79dd47d042bc0db4d00853254d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowSizeTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowSizeTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowSizeTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.Lists;\n import org.apache.druid.data.input.MapBasedInputRow;\n import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -31,7 +32,7 @@\n \n /**\n  */\n-public class IncrementalIndexRowSizeTest\n+public class IncrementalIndexRowSizeTest extends InitializedNullHandlingTest\n {\n   @Test\n   public void testIncrementalIndexRowSizeBasic()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexRowSizeTest.java",
                "sha": "afcdfd492277d96b6390b094c9ffb3a2fbf8e216",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java",
                "patch": "@@ -64,6 +64,7 @@\n import org.apache.druid.segment.data.IndexedInts;\n import org.apache.druid.segment.filter.Filters;\n import org.apache.druid.segment.filter.SelectorFilter;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n import org.junit.Assert;\n@@ -83,7 +84,7 @@\n /**\n  */\n @RunWith(Parameterized.class)\n-public class IncrementalIndexStorageAdapterTest\n+public class IncrementalIndexStorageAdapterTest extends InitializedNullHandlingTest\n {\n   interface IndexCreator\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexStorageAdapterTest.java",
                "sha": "3f3f79d0c03ad202ee0dd2c546feb062ac0f0f36",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexTest.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.druid.query.aggregation.FilteredAggregatorFactory;\n import org.apache.druid.query.filter.SelectorDimFilter;\n import org.apache.druid.segment.CloserRule;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -56,9 +57,8 @@\n /**\n  */\n @RunWith(Parameterized.class)\n-public class IncrementalIndexTest\n+public class IncrementalIndexTest extends InitializedNullHandlingTest\n {\n-\n   interface IndexCreator\n   {\n     IncrementalIndex createIndex();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/IncrementalIndexTest.java",
                "sha": "91863ff54344176ed118972853bce8106f530a32",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 2,
                "filename": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.druid.query.aggregation.LongMaxAggregatorFactory;\n import org.apache.druid.query.aggregation.LongSumAggregatorFactory;\n import org.apache.druid.query.expression.TestExprMacroTable;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -39,9 +40,8 @@\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.atomic.AtomicInteger;\n \n-public class OnheapIncrementalIndexTest\n+public class OnheapIncrementalIndexTest extends InitializedNullHandlingTest\n {\n-\n   private static final int MAX_ROWS = 100000;\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexTest.java",
                "sha": "d475a5779f437a3d878875f689de2edf28d29f90",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/virtual/ExpressionVirtualColumnTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/virtual/ExpressionVirtualColumnTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/virtual/ExpressionVirtualColumnTest.java",
                "patch": "@@ -43,12 +43,13 @@\n import org.apache.druid.segment.ColumnValueSelector;\n import org.apache.druid.segment.DimensionSelector;\n import org.apache.druid.segment.column.ValueType;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.util.Arrays;\n \n-public class ExpressionVirtualColumnTest\n+public class ExpressionVirtualColumnTest extends InitializedNullHandlingTest\n {\n   private static final InputRow ROW0 = new MapBasedInputRow(\n       DateTimes.of(\"2000-01-01T00:00:00\").getMillis(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/virtual/ExpressionVirtualColumnTest.java",
                "sha": "4675852da2b150e5c21090062f8ea61b55c96049",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/virtual/VirtualColumnsTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/virtual/VirtualColumnsTest.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/segment/virtual/VirtualColumnsTest.java",
                "patch": "@@ -49,6 +49,7 @@\n import org.apache.druid.segment.column.ValueType;\n import org.apache.druid.segment.data.IndexedInts;\n import org.apache.druid.segment.data.ZeroIndexedInts;\n+import org.apache.druid.testing.InitializedNullHandlingTest;\n import org.junit.Assert;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -58,7 +59,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n-public class VirtualColumnsTest\n+public class VirtualColumnsTest extends InitializedNullHandlingTest\n {\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/processing/src/test/java/org/apache/druid/segment/virtual/VirtualColumnsTest.java",
                "sha": "b1a9eeeab14db75dd565f8505c654dbc2046a82e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTestBase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTestBase.java?ref=3fcaa1a61b9b8b72bbff385a8c7b61cd97840345",
                "deletions": 0,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTestBase.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.sql.calcite.util;\n \n+import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.sql.calcite.planner.Calcites;\n import org.junit.BeforeClass;\n \n@@ -28,5 +29,6 @@\n   public static void setupCalciteProperties()\n   {\n     Calcites.setSystemProperties();\n+    NullHandling.initializeForTests();\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/3fcaa1a61b9b8b72bbff385a8c7b61cd97840345/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTestBase.java",
                "sha": "8c99ee52ee37ea2586728e4a9336c5f68124f75a",
                "status": "modified"
            }
        ],
        "message": "fix sql compatible null handling config work with runtime.properties (#8876)\n\n* fix sql compatible null handling config work with runtime.properties\r\n\r\n* fix npe\r\n\r\n* fix tests\r\n\r\n* add friendly error\r\n\r\n* comment, and friendlier still\r\n\r\n* fix compile\r\n\r\n* fix from merges",
        "parent": "https://github.com/apache/incubator-druid/commit/f5fbd0bea0715bc3fd3d7ac63028e168cf257712",
        "repo": "incubator-druid",
        "unit_tests": [
            "JdbcDataFetcherTest.java"
        ]
    },
    "incubator-druid_40aa6ec": {
        "bug_id": "incubator-druid_40aa6ec",
        "commit": "https://github.com/apache/incubator-druid/commit/40aa6ecaefd3becd05d685198037087fa9550f34",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/40aa6ecaefd3becd05d685198037087fa9550f34/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=40aa6ecaefd3becd05d685198037087fa9550f34",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -92,7 +92,7 @@ public void doPost(\n     final long start = System.currentTimeMillis();\n     Query query = null;\n     byte[] requestQuery = null;\n-    String queryID;\n+    String queryId;\n \n     final boolean isSmile = \"application/smile\".equals(req.getContentType());\n \n@@ -105,9 +105,10 @@ public void doPost(\n     try {\n       requestQuery = ByteStreams.toByteArray(req.getInputStream());\n       query = objectMapper.readValue(requestQuery, Query.class);\n-      queryID = query.getId();\n-      if (queryID == null) {\n-        query = query.withId(idProvider.next(query));\n+      queryId = query.getId();\n+      if (queryId == null) {\n+        queryId = idProvider.next(query);\n+        query = query.withId(queryId);\n       }\n \n       requestLogger.log(\n@@ -136,7 +137,7 @@ public void doPost(\n               .setUser6(String.valueOf(query.hasFilters()))\n               .setUser7(req.getRemoteAddr())\n               .setUser9(query.getDuration().toPeriod().toStandardMinutes().toString())\n-              .setUser10(queryID)\n+              .setUser10(queryId)\n               .build(\"request/time\", requestTime)\n       );\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/40aa6ecaefd3becd05d685198037087fa9550f34/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "97bca835ddb4274bea34cb80c9f85c64d1b84a8c",
                "status": "modified"
            }
        ],
        "message": "fix npe queryID",
        "parent": "https://github.com/apache/incubator-druid/commit/e54949ca981ca5a446991c4a1ac671e2ec94736a",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_4338af0": {
        "bug_id": "incubator-druid_4338af0",
        "commit": "https://github.com/apache/incubator-druid/commit/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=4338af0e3f2ef3f91c3a8cf78e91122c54c89e21",
                "deletions": 2,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -130,7 +130,6 @@ public Response doPost(\n   {\n     final long start = System.currentTimeMillis();\n     Query query = null;\n-    byte[] requestQuery = null;\n     String queryId = null;\n \n     final String reqContentType = req.getContentType();\n@@ -267,9 +266,10 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n       ).build();\n     }\n     catch (Exception e) {\n+      // Input stream has already been consumed by the json object mapper if query == null\n       final String queryString =\n           query == null\n-          ? (isSmile ? \"smile_unknown\" : new String(requestQuery, Charsets.UTF_8))\n+          ? \"unparsable query\"\n           : query.toString();\n \n       log.warn(e, \"Exception occurred on request [%s]\", queryString);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4338af0e3f2ef3f91c3a8cf78e91122c54c89e21/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "165904668935fc1f7379739ab147a424108c4d28",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in QueryResource on bad query",
        "parent": "https://github.com/apache/incubator-druid/commit/63df70cbe25d44a36cab6dbcd5a21894c7977f34",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_4ace65a": {
        "bug_id": "incubator-druid_4ace65a",
        "commit": "https://github.com/apache/incubator-druid/commit/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java?ref=4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "patch": "@@ -454,7 +454,7 @@ public int getPartition(BytesWritable bytesWritable, Writable value, int numPart\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       int shardNum = bytes.getInt();\n-      if (config.get(\"mapred.job.tracker\").equals(\"local\")) {\n+      if (\"local\".equals(config.get(\"mapreduce.jobtracker.address\")) || \"local\".equals(config.get(\"mapred.job.tracker\"))) {\n         return shardNum % numPartitions;\n       } else {\n         if (shardNum >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ace65a2af7c92bbd3b2fa56ebbc3cbf05e7730a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "sha": "2a07fa521590092b44910d0fc1419647bca45685",
                "status": "modified"
            }
        ],
        "message": "fix NPE in IndexGeneratorJob (#4371)\n\n* fix NPE in IndexGeneratorJob\r\n\r\n* address review comment\r\n\r\n* review comments",
        "parent": "https://github.com/apache/incubator-druid/commit/2d15215cd0953a3ffbc719752edb309c04a717de",
        "repo": "incubator-druid",
        "unit_tests": [
            "IndexGeneratorJobTest.java"
        ]
    },
    "incubator-druid_4db9e39": {
        "bug_id": "incubator-druid_4db9e39",
        "commit": "https://github.com/apache/incubator-druid/commit/4db9e39a715ec64e59c985ec534b54bcf153b2df",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4db9e39a715ec64e59c985ec534b54bcf153b2df/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java?ref=4db9e39a715ec64e59c985ec534b54bcf153b2df",
                "deletions": 2,
                "filename": "java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "patch": "@@ -19,7 +19,6 @@\n \n package io.druid.java.util.common.io.smoosh;\n \n-import com.google.common.base.Throwables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.io.Closeables;\n@@ -143,6 +142,9 @@ public void close()\n   {\n     Throwable thrown = null;\n     for (MappedByteBuffer mappedByteBuffer : buffersList) {\n+      if (mappedByteBuffer == null) {\n+        continue;\n+      }\n       try {\n         ByteBufferUtils.unmap(mappedByteBuffer);\n       }\n@@ -154,6 +156,9 @@ public void close()\n         }\n       }\n     }\n-    Throwables.propagateIfPossible(thrown);\n+    buffersList.clear();\n+    if (thrown != null) {\n+      throw new RuntimeException(thrown);\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4db9e39a715ec64e59c985ec534b54bcf153b2df/java-util/src/main/java/io/druid/java/util/common/io/smoosh/SmooshedFileMapper.java",
                "sha": "d7c284b8a83012e24e220fc3357830e4d50f15c4",
                "status": "modified"
            }
        ],
        "message": "fix NPE when buffersList contains null in SmooshedFileMapper (#5689)\n\n* fix NPE when buffersList contains null\r\n\r\n* address the comment",
        "parent": "https://github.com/apache/incubator-druid/commit/86746f82d8a5e0fa36748240009e838eb0dff6d9",
        "repo": "incubator-druid",
        "unit_tests": [
            "SmooshedFileMapperTest.java"
        ]
    },
    "incubator-druid_4ff12e4": {
        "bug_id": "incubator-druid_4ff12e4",
        "commit": "https://github.com/apache/incubator-druid/commit/4ff12e4394b8634b60a927ad4f1d4614b6b598c4",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java?ref=4ff12e4394b8634b60a927ad4f1d4614b6b598c4",
                "deletions": 5,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "patch": "@@ -300,12 +300,14 @@ public void run(Context context) throws IOException, InterruptedException\n   {\n     private final List<Interval> intervals = Lists.newArrayList();\n     protected HadoopDruidIndexerConfig config = null;\n+    private boolean determineIntervals;\n \n     @Override\n     protected void setup(Context context)\n         throws IOException, InterruptedException\n     {\n       config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n+      determineIntervals = !config.getSegmentGranularIntervals().isPresent();\n     }\n \n     @Override\n@@ -321,12 +323,20 @@ protected void reduce(\n             HyperLogLogCollector.makeCollector(ByteBuffer.wrap(value.getBytes(), 0, value.getLength()))\n         );\n       }\n-      Optional<Interval> intervalOptional = config.getGranularitySpec().bucketInterval(DateTimes.utc(key.get()));\n \n-      if (!intervalOptional.isPresent()) {\n-        throw new ISE(\"WTF?! No bucket found for timestamp: %s\", key.get());\n+      Interval interval;\n+\n+      if (determineIntervals) {\n+        interval = config.getGranularitySpec().getSegmentGranularity().bucket(DateTimes.utc(key.get()));\n+      } else {\n+        Optional<Interval> intervalOptional = config.getGranularitySpec().bucketInterval(DateTimes.utc(key.get()));\n+\n+        if (!intervalOptional.isPresent()) {\n+          throw new ISE(\"WTF?! No bucket found for timestamp: %s\", key.get());\n+        }\n+        interval = intervalOptional.get();\n       }\n-      Interval interval = intervalOptional.get();\n+\n       intervals.add(interval);\n       final Path outPath = config.makeSegmentPartitionInfoPath(interval);\n       final OutputStream out = Utils.makePathAndOutputStream(\n@@ -353,7 +363,7 @@ public void run(Context context)\n         throws IOException, InterruptedException\n     {\n       super.run(context);\n-      if (!config.getSegmentGranularIntervals().isPresent()) {\n+      if (determineIntervals) {\n         final Path outPath = config.makeIntervalInfoPath();\n         final OutputStream out = Utils.makePathAndOutputStream(\n             context, outPath, config.isOverwriteFiles()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "sha": "b3ccee6b8ae6ecae40c840dc827fd024645aa0ea",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java?ref=4ff12e4394b8634b60a927ad4f1d4614b6b598c4",
                "deletions": 7,
                "filename": "indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java",
                "patch": "@@ -29,10 +29,15 @@\n import io.druid.indexer.partitions.HashedPartitionsSpec;\n import io.druid.java.util.common.Intervals;\n import io.druid.java.util.common.granularity.Granularities;\n+import io.druid.java.util.common.granularity.Granularity;\n+import io.druid.java.util.common.granularity.PeriodGranularity;\n import io.druid.query.aggregation.AggregatorFactory;\n import io.druid.query.aggregation.DoubleSumAggregatorFactory;\n import io.druid.segment.indexing.DataSchema;\n import io.druid.segment.indexing.granularity.UniformGranularitySpec;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Interval;\n+import org.joda.time.Period;\n import org.junit.Assert;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -53,7 +58,7 @@\n   private int[] expectedNumOfShards;\n   private int errorMargin;\n \n-  @Parameterized.Parameters(name = \"File={0}, TargetPartitionSize={1}, Interval={2}, ErrorMargin={3}, NumTimeBuckets={4}, NumShards={5}\")\n+  @Parameterized.Parameters(name = \"File={0}, TargetPartitionSize={1}, Interval={2}, ErrorMargin={3}, NumTimeBuckets={4}, NumShards={5}, SegmentGranularity={6}\")\n   public static Collection<?> data()\n   {\n     int[] first = new int[1];\n@@ -73,23 +78,44 @@\n                 \"2011-04-10T00:00:00.000Z/2011-04-11T00:00:00.000Z\",\n                 0,\n                 1,\n-                first\n+                first,\n+                Granularities.DAY\n             },\n             {\n                 DetermineHashedPartitionsJobTest.class.getResource(\"/druid.test.data.with.duplicate.rows.tsv\").getPath(),\n                 100L,\n                 \"2011-04-10T00:00:00.000Z/2011-04-16T00:00:00.000Z\",\n                 0,\n                 6,\n-                second\n+                second,\n+                Granularities.DAY\n             },\n             {\n                 DetermineHashedPartitionsJobTest.class.getResource(\"/druid.test.data.with.duplicate.rows.tsv\").getPath(),\n                 1L,\n                 \"2011-04-10T00:00:00.000Z/2011-04-16T00:00:00.000Z\",\n                 0,\n                 6,\n-                third\n+                third,\n+                Granularities.DAY\n+            },\n+            {\n+                DetermineHashedPartitionsJobTest.class.getResource(\"/druid.test.data.with.duplicate.rows.tsv\").getPath(),\n+                1L,\n+                null,\n+                0,\n+                6,\n+                third,\n+                Granularities.DAY\n+            },\n+            {\n+                DetermineHashedPartitionsJobTest.class.getResource(\"/druid.test.data.with.rows.in.timezone.tsv\").getPath(),\n+                1L,\n+                null,\n+                0,\n+                1,\n+                first,\n+                new PeriodGranularity(new Period(\"P1D\"), null, DateTimeZone.forID(\"America/Los_Angeles\"))\n             }\n         }\n     );\n@@ -101,14 +127,20 @@ public DetermineHashedPartitionsJobTest(\n       String interval,\n       int errorMargin,\n       int expectedNumTimeBuckets,\n-      int[] expectedNumOfShards\n+      int[] expectedNumOfShards,\n+      Granularity segmentGranularity\n   ) throws IOException\n   {\n     this.expectedNumOfShards = expectedNumOfShards;\n     this.expectedNumTimeBuckets = expectedNumTimeBuckets;\n     this.errorMargin = errorMargin;\n     File tmpDir = Files.createTempDir();\n \n+    ImmutableList<Interval> intervals = null;\n+    if (interval != null) {\n+      intervals = ImmutableList.of(Intervals.of(interval));\n+    }\n+\n     HadoopIngestionSpec ingestionSpec = new HadoopIngestionSpec(\n         new DataSchema(\n             \"test_schema\",\n@@ -145,9 +177,9 @@ public DetermineHashedPartitionsJobTest(\n             ),\n             new AggregatorFactory[]{new DoubleSumAggregatorFactory(\"index\", \"index\")},\n             new UniformGranularitySpec(\n-                Granularities.DAY,\n+                segmentGranularity,\n                 Granularities.NONE,\n-                ImmutableList.of(Intervals.of(interval))\n+                intervals\n             ),\n             HadoopDruidIndexerConfig.JSON_MAPPER\n         ),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/test/java/io/druid/indexer/DetermineHashedPartitionsJobTest.java",
                "sha": "5ff2afdcd5ddef1433f10c1e963af0048514961c",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv?ref=4ff12e4394b8634b60a927ad4f1d4614b6b598c4",
                "deletions": 0,
                "filename": "indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv",
                "patch": "@@ -0,0 +1,16 @@\n+2011-04-10T00:00:00.000-07:00\tspot\tautomotive\tpreferred\ta\u0001preferred\t113.221448\n+2011-04-10T00:00:00.000-07:00\tspot\tautomotive\tpreferred\ta\u0001preferred\t11.221448\n+2011-04-10T00:00:00.000-07:00\tspot\tautomotive\tpreferred\ta\u0001preferred\t103.221448\n+2011-04-10T00:00:00.000-07:00\tspot\tautomotive\tpreferred\ta\u0001preferred\t53.221448\n+2011-04-10T00:00:00.000-07:00\tspot\tbusiness\tpreferred\tb\u0001preferred\t95.570457\n+2011-04-10T00:00:00.000-07:00\tspot\tentertainment\tpreferred\te\u0001preferred\t131.766616\n+2011-04-10T00:00:00.000-07:00\tspot\thealth\tpreferred\th\u0001preferred\t99.950855\n+2011-04-10T00:00:00.000-07:00\tspot\tmezzanine\tpreferred\tm\u0001preferred\t91.470524\n+2011-04-10T00:00:00.000-07:00\tspot\tnews\tpreferred\tn\u0001preferred\t99.393076\n+2011-04-10T00:00:00.000-07:00\tspot\tpremium\tpreferred\tp\u0001preferred\t123.207579\n+2011-04-10T00:00:00.000-07:00\tspot\ttechnology\tpreferred\tt\u0001preferred\t84.898691\n+2011-04-10T00:00:00.000-07:00\tspot\ttravel\tpreferred\tt\u0001preferred\t114.353962\n+2011-04-10T00:00:00.000-07:00\ttotal_market\tmezzanine\tpreferred\tm\u0001preferred\t1005.253077\n+2011-04-10T00:00:00.000-07:00\ttotal_market\tpremium\tpreferred\tp\u0001preferred\t1030.094757\n+2011-04-10T00:00:00.000-07:00\tupfront\tmezzanine\tpreferred\tm\u0001preferred\t1031.741509\n+2011-04-10T00:00:00.000-07:00\tupfront\tpremium\tpreferred\tp\u0001preferred\t775.965555\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/indexing-hadoop/src/test/resources/druid.test.data.with.rows.in.timezone.tsv",
                "sha": "f855c19d0964c1c1677b7160f577dac71a8f4cff",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java?ref=4ff12e4394b8634b60a927ad4f1d4614b6b598c4",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java",
                "patch": "@@ -97,7 +97,11 @@ public UniformGranularitySpec(\n   @Override\n   public Optional<Interval> bucketInterval(DateTime dt)\n   {\n-    return wrappedSpec.bucketInterval(dt);\n+    if (wrappedSpec == null) {\n+      return Optional.absent();\n+    } else {\n+      return wrappedSpec.bucketInterval(dt);\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/4ff12e4394b8634b60a927ad4f1d4614b6b598c4/server/src/main/java/io/druid/segment/indexing/granularity/UniformGranularitySpec.java",
                "sha": "e459877d8add355aa823efddf455ee76c14fe663",
                "status": "modified"
            }
        ],
        "message": "Hadoop indexing: Fix NPE when intervals not provided (#4686)\n\n* Fix #4647\r\n\r\n* NPE protect bucketInterval as well\r\n\r\n* Add test to verify timezone as well\r\n\r\n* Also handle case when intervals are already present\r\n\r\n* Fix checkstyle error\r\n\r\n* Use factory method instead for Datetime\r\n\r\n* Use Intervals factory method",
        "parent": "https://github.com/apache/incubator-druid/commit/716a5ec1a8bb66ca480eaf0eb8e3cc68a234d1cc",
        "repo": "incubator-druid",
        "unit_tests": [
            "DetermineHashedPartitionsJobTest.java"
        ]
    },
    "incubator-druid_50ad0e6": {
        "bug_id": "incubator-druid_50ad0e6",
        "commit": "https://github.com/apache/incubator-druid/commit/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java?ref=50ad0e64748fd4721408b8cbeb8233dd8ec85f3e",
                "deletions": 6,
                "filename": "processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "patch": "@@ -38,7 +38,11 @@ public int compare(String str1, String str2)\n     {\n       int[] pos = {0, 0};\n \n-      if (str1.length() == 0) {\n+      if (str1 == null) {\n+        return -1;\n+      } else if (str2 == null) {\n+        return 1;\n+      } else if (str1.length() == 0) {\n         return str2.length() == 0 ? 0 : -1;\n       } else if (str2.length() == 0) {\n         return 1;\n@@ -179,15 +183,12 @@ private int compareNonNumeric(String str0, String str1, int[] pos)\n     }\n   };\n \n-  private final String previousStop;\n-\n   @JsonCreator\n   public AlphaNumericTopNMetricSpec(\n       @JsonProperty(\"previousStop\") String previousStop\n   )\n   {\n     super(previousStop);\n-    this.previousStop = (previousStop == null) ? \"\" : previousStop;\n   }\n \n   @Override\n@@ -199,7 +200,7 @@ public Comparator getComparator(List<AggregatorFactory> aggregatorSpecs, List<Po\n   @Override\n   public byte[] getCacheKey()\n   {\n-    byte[] previousStopBytes = StringUtils.toUtf8(previousStop);\n+    byte[] previousStopBytes = getPreviousStop() == null ? new byte[]{} : StringUtils.toUtf8(getPreviousStop());\n \n     return ByteBuffer.allocate(1 + previousStopBytes.length)\n                      .put(CACHE_TYPE_ID)\n@@ -217,7 +218,7 @@ public Comparator getComparator(List<AggregatorFactory> aggregatorSpecs, List<Po\n   public String toString()\n   {\n     return \"AlphaNumericTopNMetricSpec{\" +\n-           \"previousStop='\" + previousStop + '\\'' +\n+           \"previousStop='\" + getPreviousStop() + '\\'' +\n            '}';\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "sha": "4daadbe8137d8744a9dfc1e1f74ba7542310b15a",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java?ref=50ad0e64748fd4721408b8cbeb8233dd8ec85f3e",
                "deletions": 1,
                "filename": "processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "patch": "@@ -17,11 +17,15 @@\n \n package io.druid.query.topn;\n \n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.jackson.DefaultObjectMapper;\n import org.junit.Test;\n \n+import java.io.IOException;\n import java.util.Comparator;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n public class AlphaNumericTopNMetricSpecTest\n {\n@@ -60,4 +64,22 @@ public void testComparator() throws Exception\n     assertTrue(comparator.compare(\"1.3\", \"1.15\") < 0);\n \n   }\n+\n+  @Test\n+  public void testSerdeAlphaNumericTopNMetricSpec() throws IOException{\n+    AlphaNumericTopNMetricSpec expectedMetricSpec = new AlphaNumericTopNMetricSpec(null);\n+    AlphaNumericTopNMetricSpec expectedMetricSpec1 = new AlphaNumericTopNMetricSpec(\"test\");\n+    String jsonSpec = \"{\\n\"\n+                      + \"    \\\"type\\\": \\\"alphaNumeric\\\"\\n\"\n+                      + \"}\";\n+    String jsonSpec1 = \"{\\n\"\n+                       + \"    \\\"type\\\": \\\"alphaNumeric\\\",\\n\"\n+                       + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n+                       + \"}\";\n+    ObjectMapper jsonMapper = new DefaultObjectMapper();\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n+    assertEquals(expectedMetricSpec, actualMetricSpec);\n+    assertEquals(expectedMetricSpec1, actualMetricSpec1);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "sha": "18c37614b2e925b56fe8a0aa0e746e86906981a1",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/incubator-druid/blob/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java?ref=50ad0e64748fd4721408b8cbeb8233dd8ec85f3e",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "patch": "@@ -2991,4 +2991,34 @@ public void testTopNOverPartialNullDimensionWithFilterOnNOTNullValue()\n     );\n     assertExpectedResults(expectedResults, query);\n   }\n+  @Test\n+  public void testAlphaNumericTopNWithNullPreviousStop(){\n+    TopNQuery query = new TopNQueryBuilder()\n+        .dataSource(QueryRunnerTestHelper.dataSource)\n+        .granularity(QueryGranularity.ALL)\n+        .dimension(QueryRunnerTestHelper.marketDimension)\n+        .metric(new AlphaNumericTopNMetricSpec(null))\n+        .threshold(2)\n+        .intervals(QueryRunnerTestHelper.secondOnly)\n+        .aggregators(Lists.<AggregatorFactory>newArrayList(QueryRunnerTestHelper.rowsCount))\n+        .build();\n+    List<Result<TopNResultValue>> expectedResults = Arrays.asList(\n+        new Result<>(\n+            new DateTime(\"2011-04-02T00:00:00.000Z\"),\n+            new TopNResultValue(\n+                Arrays.asList(\n+                    ImmutableMap.<String, Object>of(\n+                        \"market\", \"spot\",\n+                        \"rows\", 9L\n+                    ),\n+                    ImmutableMap.<String, Object>of(\n+                        \"market\", \"total_market\",\n+                        \"rows\", 2L\n+                    )\n+                )\n+            )\n+        )\n+    );\n+    TestHelper.assertExpectedResults(expectedResults, runner.run(query, new HashMap<String, Object>()));\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "sha": "621edb6ebd29410485cc3d7c600092e6348ea347",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/incubator-druid/blob/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java?ref=50ad0e64748fd4721408b8cbeb8233dd8ec85f3e",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import io.druid.query.aggregation.DoubleMaxAggregatorFactory;\n import io.druid.query.aggregation.DoubleMinAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n+import io.druid.query.dimension.LegacyDimensionSpec;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -39,6 +40,7 @@\n import static io.druid.query.QueryRunnerTestHelper.fullOnInterval;\n import static io.druid.query.QueryRunnerTestHelper.indexMetric;\n import static io.druid.query.QueryRunnerTestHelper.marketDimension;\n+import static io.druid.query.QueryRunnerTestHelper.rowsCount;\n \n public class TopNQueryTest\n {\n@@ -74,4 +76,38 @@ public void testQuerySerialization() throws IOException\n     Assert.assertEquals(query, serdeQuery);\n   }\n \n+  @Test\n+  public void testQuerySerdeWithAlphaNumericTopNMetricSpec() throws IOException{\n+    TopNQuery expectedQuery = new TopNQueryBuilder()\n+        .dataSource(dataSource)\n+        .granularity(allGran)\n+        .dimension(new LegacyDimensionSpec(marketDimension))\n+        .metric(new AlphaNumericTopNMetricSpec(null))\n+        .threshold(2)\n+        .intervals(fullOnInterval.getIntervals())\n+        .aggregators(Lists.<AggregatorFactory>newArrayList(rowsCount))\n+        .build();\n+    String jsonQuery = \"{\\n\"\n+                       + \"  \\\"queryType\\\": \\\"topN\\\",\\n\"\n+                       + \"  \\\"dataSource\\\": \\\"testing\\\",\\n\"\n+                       + \"  \\\"dimension\\\": \\\"market\\\",\\n\"\n+                       + \"  \\\"threshold\\\": 2,\\n\"\n+                       + \"  \\\"metric\\\": {\\n\"\n+                       + \"    \\\"type\\\": \\\"alphaNumeric\\\"\\n\"\n+                       + \"   },\\n\"\n+                       + \"  \\\"granularity\\\": \\\"all\\\",\\n\"\n+                       + \"  \\\"aggregations\\\": [\\n\"\n+                       + \"    {\\n\"\n+                       + \"      \\\"type\\\": \\\"count\\\",\\n\"\n+                       + \"      \\\"name\\\": \\\"rows\\\"\\n\"\n+                       + \"    }\\n\"\n+                       + \"  ],\\n\"\n+                       + \"  \\\"intervals\\\": [\\n\"\n+                       + \"    \\\"1970-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\\\"\\n\"\n+                       + \"  ]\\n\"\n+                       + \"}\";\n+    TopNQuery actualQuery = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonQuery, TopNQuery.class)), TopNQuery.class);\n+    Assert.assertEquals(expectedQuery, actualQuery);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/50ad0e64748fd4721408b8cbeb8233dd8ec85f3e/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "sha": "bb2c37b2c24c6f6e6ffdc2fbbe89d16986fd235b",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1412 from pjain1/alphaNumericTopN_NPE_fix\n\nNPE fix for TopN query with alphaNumericTopN metric spec",
        "parent": "https://github.com/apache/incubator-druid/commit/35e2fde18ef334e20a7c897658723ce7c9b28d15",
        "repo": "incubator-druid",
        "unit_tests": [
            "AlphaNumericTopNMetricSpecTest.java"
        ]
    },
    "incubator-druid_54139c6": {
        "bug_id": "incubator-druid_54139c6",
        "commit": "https://github.com/apache/incubator-druid/commit/54139c68159149017ea4ed9510346934d4e68997",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54139c68159149017ea4ed9510346934d4e68997/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java?ref=54139c68159149017ea4ed9510346934d4e68997",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "patch": "@@ -55,7 +55,7 @@ public RegisteredLookupExtractionFn(\n     this.replaceMissingValueWith = replaceMissingValueWith;\n     this.retainMissingValue = retainMissingValue;\n     this.injective = injective;\n-    this.optimize = optimize;\n+    this.optimize = optimize == null ? true : optimize;\n     this.lookup = lookup;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54139c68159149017ea4ed9510346934d4e68997/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "sha": "ab58beb25a2e174b9308d49285812dd8b67b6e32",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in registeredLookup extractionFn when \"optimize\" is not provided. (#3064)",
        "parent": "https://github.com/apache/incubator-druid/commit/2db5f49f3526ec16e7535cebdb977b65cb762583",
        "repo": "incubator-druid",
        "unit_tests": [
            "RegisteredLookupExtractionFnTest.java"
        ]
    },
    "incubator-druid_54351a5": {
        "bug_id": "incubator-druid_54351a5",
        "commit": "https://github.com/apache/incubator-druid/commit/54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
        "file": [
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/.idea/inspectionProfiles/Druid.xml",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/.idea/inspectionProfiles/Druid.xml?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": ".idea/inspectionProfiles/Druid.xml",
                "patch": "@@ -11,28 +11,35 @@\n     <inspection_tool class=\"ArrayObjectsEquals\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ArraysAsListWithZeroOrOneArgument\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"AssertWithSideEffects\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"CapturingCleaner\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CastConflictsWithInstanceof\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CastToIncompatibleInterface\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CatchMayIgnoreException\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\">\n       <option name=\"m_ignoreCatchBlocksWithComments\" value=\"false\" />\n     </inspection_tool>\n     <inspection_tool class=\"CheckValidXmlInScriptTagBody\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"ClassGetClass\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ClassNewInstance\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CollectionAddedToSelf\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ComparableImplementedButEqualsNotOverridden\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ComparatorMethodParameterNotUsed\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"ComparatorResultComparison\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CompareToUsesNonFinalVariable\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ConstantAssertCondition\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"Contract\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"CopyConstructorMissesField\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"CovariantEquals\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EmptyInitializer\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EmptyStatementBody\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\">\n       <option name=\"m_reportEmptyBlocks\" value=\"true\" />\n       <option name=\"commentsAreContent\" value=\"true\" />\n     </inspection_tool>\n+    <inspection_tool class=\"EndlessStream\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EqualsAndHashcode\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EqualsBetweenInconvertibleTypes\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"EqualsOnSuspiciousObject\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EqualsUsesNonFinalVariable\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"EqualsWhichDoesntCheckParameterClass\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"EqualsWithItself\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"FieldCanBeLocal\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\">\n       <option name=\"EXCLUDE_ANNOS\">\n@@ -72,6 +79,17 @@\n     <inspection_tool class=\"MathRandomCastToInt\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"MavenModelInspection\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"MismatchedArrayReadWrite\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"MismatchedCollectionQueryUpdate\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\">\n+      <option name=\"queryNames\">\n+        <value />\n+      </option>\n+      <option name=\"updateNames\">\n+        <value />\n+      </option>\n+      <option name=\"ignoredClasses\">\n+        <value />\n+      </option>\n+    </inspection_tool>\n     <inspection_tool class=\"MismatchedStringBuilderQueryUpdate\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"MissingOverrideAnnotation\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\">\n       <scope name=\"NonGeneratedFiles\" level=\"ERROR\" enabled=\"true\">\n@@ -93,6 +111,7 @@\n     </inspection_tool>\n     <inspection_tool class=\"ObjectEqualsNull\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"ObjectToString\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"OverwrittenKey\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"PrimitiveArrayArgumentToVariableArgMethod\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"RedundantThrows\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"RedundantTypeArguments\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n@@ -182,22 +201,38 @@\n         <constraint name=\"E\" regexp=\"java\\.lang\\.UnsupportedOperationException\" within=\"\" contains=\"\" />\n         <constraint name=\"x\" minCount=\"0\" maxCount=\"2147483647\" within=\"\" contains=\"\" />\n       </searchConfiguration>\n+      <searchConfiguration name=\"Use TypeReference&lt;List&lt;...&gt;&gt; instead\" created=\"1539884261626\" text=\"TypeReference&lt;ArrayList&lt;$E$&gt;&gt;\" recursive=\"false\" caseInsensitive=\"true\" type=\"JAVA\">\n+        <constraint name=\"__context__\" target=\"true\" within=\"\" contains=\"\" />\n+        <constraint name=\"E\" within=\"\" contains=\"\" />\n+      </searchConfiguration>\n+      <searchConfiguration name=\"Use TypeReference&lt;Map&lt;...&gt;&gt; instead\" created=\"1539884261626\" text=\"TypeReference&lt;HashMap&lt;$K$, $V$&gt;&gt;\" recursive=\"false\" caseInsensitive=\"true\" type=\"JAVA\">\n+        <constraint name=\"__context__\" target=\"true\" within=\"\" contains=\"\" />\n+        <constraint name=\"K\" within=\"\" contains=\"\" />\n+        <constraint name=\"V\" within=\"\" contains=\"\" />\n+      </searchConfiguration>\n+      <searchConfiguration name=\"Use TypeReference&lt;Set&lt;...&gt;&gt; instead\" created=\"1539884261626\" text=\"TypeReference&lt;HashSet&lt;$E$&gt;&gt;\" recursive=\"false\" caseInsensitive=\"true\" type=\"JAVA\">\n+        <constraint name=\"__context__\" target=\"true\" within=\"\" contains=\"\" />\n+        <constraint name=\"E\" within=\"\" contains=\"\" />\n+      </searchConfiguration>\n     </inspection_tool>\n     <inspection_tool class=\"SpellCheckingInspection\" enabled=\"false\" level=\"TYPO\" enabled_by_default=\"false\">\n       <option name=\"processCode\" value=\"true\" />\n       <option name=\"processLiterals\" value=\"true\" />\n       <option name=\"processComments\" value=\"true\" />\n     </inspection_tool>\n-    <inspection_tool class=\"StaticCallOnSubclass\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n-    <inspection_tool class=\"StaticFieldReferenceOnSubclass\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"StaticCallOnSubclass\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"StaticFieldReferenceOnSubclass\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"StringConcatenationInFormatCall\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"StringConcatenationInMessageFormatCall\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"StringConcatenationMissingWhitespace\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"StringEquality\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"StringEqualsCharSequence\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"StringTokenizerDelimiter\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n-    <inspection_tool class=\"SubtractionInCompareTo\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"SubtractionInCompareTo\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"SuspiciousArrayCast\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"SuspiciousArrayMethodCall\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"SuspiciousIndentAfterControlStatement\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"SuspiciousListRemoveInLoop\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n     <inspection_tool class=\"SuspiciousMethodCalls\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\">\n       <option name=\"REPORT_CONVERTIBLE_METHOD_CALLS\" value=\"true\" />\n     </inspection_tool>\n@@ -226,6 +261,11 @@\n       <option name=\"ignoreInModuleStatements\" value=\"true\" />\n     </inspection_tool>\n     <inspection_tool class=\"UnnecessaryInterfaceModifier\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\" />\n+    <inspection_tool class=\"UnusedAssignment\" enabled=\"true\" level=\"ERROR\" enabled_by_default=\"true\">\n+      <option name=\"REPORT_PREFIX_EXPRESSIONS\" value=\"true\" />\n+      <option name=\"REPORT_POSTFIX_EXPRESSIONS\" value=\"true\" />\n+      <option name=\"REPORT_REDUNDANT_INITIALIZER\" value=\"true\" />\n+    </inspection_tool>\n     <inspection_tool class=\"UnusedCatchParameter\" enabled=\"true\" level=\"WARNING\" enabled_by_default=\"true\">\n       <option name=\"m_ignoreCatchBlocksWithComments\" value=\"false\" />\n       <option name=\"m_ignoreTestCases\" value=\"false\" />",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/.idea/inspectionProfiles/Druid.xml",
                "sha": "4ada21411dcfc79b21920263041ad700ebd7685f",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 21,
                "filename": "benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "patch": "@@ -95,7 +95,6 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.HashMap;\n-import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ExecutorService;\n@@ -167,15 +166,13 @@ public int columnCacheSizeBytes()\n     INDEX_MERGER_V9 = new IndexMergerV9(JSON_MAPPER, INDEX_IO, OffHeapMemorySegmentWriteOutMediumFactory.instance());\n   }\n \n-  private static final Map<String, Map<String, Object>> SCHEMA_QUERY_MAP = new LinkedHashMap<>();\n-\n   private void setupQueries()\n   {\n     // queries for the basic schema\n-    Map<String, Object> basicQueries = new LinkedHashMap<>();\n     BenchmarkSchemaInfo basicSchema = BenchmarkSchemas.SCHEMA_MAP.get(\"basic\");\n \n-    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(Collections.singletonList(basicSchema.getDataInterval()));\n+    QuerySegmentSpec intervalSpec =\n+        new MultipleIntervalSegmentSpec(Collections.singletonList(basicSchema.getDataInterval()));\n \n     long startMillis = basicSchema.getDataInterval().getStartMillis();\n     long endMillis = basicSchema.getDataInterval().getEndMillis();\n@@ -204,9 +201,7 @@ private void setupQueries()\n       );\n       queryAggs.add(\n           new FilteredAggregatorFactory(\n-              new LongSumAggregatorFactory(\n-                  \"_cmp_sumLongSequential\", \"sumLongSequential\"\n-              ),\n+              new LongSumAggregatorFactory(\"_cmp_sumLongSequential\", \"sumLongSequential\"),\n               new IntervalDimFilter(\n                   ColumnHolder.TIME_COLUMN_NAME,\n                   Collections.singletonList(previous),\n@@ -235,8 +230,6 @@ private void setupQueries()\n           new TopNQueryQueryToolChest(new TopNQueryConfig(), QueryBenchmarkUtil.NoopIntervalChunkingQueryRunnerDecorator()),\n           QueryBenchmarkUtil.NOOP_QUERYWATCHER\n       );\n-\n-      basicQueries.put(\"topNTimeCompare\", queryBuilderA);\n     }\n     { // basic.timeseriesTimeCompare\n       List<AggregatorFactory> queryAggs = new ArrayList<>();\n@@ -265,24 +258,21 @@ private void setupQueries()\n           )\n       );\n \n-      Druids.TimeseriesQueryBuilder timeseriesQueryBuilder = Druids.newTimeseriesQueryBuilder()\n-                                                                   .dataSource(\"blah\")\n-                                                                   .granularity(Granularities.ALL)\n-                                                                   .intervals(intervalSpec)\n-                                                                   .aggregators(queryAggs)\n-                                                                   .descending(false);\n+      Druids.TimeseriesQueryBuilder timeseriesQueryBuilder = Druids\n+          .newTimeseriesQueryBuilder()\n+          .dataSource(\"blah\")\n+          .granularity(Granularities.ALL)\n+          .intervals(intervalSpec)\n+          .aggregators(queryAggs)\n+          .descending(false);\n \n       timeseriesQuery = timeseriesQueryBuilder.build();\n       timeseriesFactory = new TimeseriesQueryRunnerFactory(\n-          new TimeseriesQueryQueryToolChest(\n-              QueryBenchmarkUtil.NoopIntervalChunkingQueryRunnerDecorator()\n-          ),\n+          new TimeseriesQueryQueryToolChest(QueryBenchmarkUtil.NoopIntervalChunkingQueryRunnerDecorator()),\n           new TimeseriesQueryEngine(),\n           QueryBenchmarkUtil.NOOP_QUERYWATCHER\n       );\n     }\n-\n-    SCHEMA_QUERY_MAP.put(\"basic\", basicQueries);\n   }\n \n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/benchmarks/src/main/java/org/apache/druid/benchmark/query/timecompare/TimeCompareBenchmark.java",
                "sha": "d719fe25ff8fddea23c2bd7ba5011954d4598448",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/common/guava/Comparators.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/java/util/common/guava/Comparators.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 20,
                "filename": "core/src/main/java/org/apache/druid/java/util/common/guava/Comparators.java",
                "patch": "@@ -59,26 +59,6 @@ public int compare(@Nullable Object left, @Nullable Object right)\n     return NATURAL_NULLS_FIRST;\n   }\n \n-  /**\n-   * This is a \"reverse\" comparator.  Positive becomes negative, negative becomes positive and 0 (equal) stays the same.\n-   * This was poorly named as \"inverse\" as it's not really inverting a true/false relationship\n-   *\n-   * @param baseComp\n-   * @param <T>\n-   * @return\n-   */\n-  public static <T> Comparator<T> inverse(final Comparator<T> baseComp)\n-  {\n-    return new Comparator<T>()\n-    {\n-      @Override\n-      public int compare(T t, T t1)\n-      {\n-        return baseComp.compare(t1, t);\n-      }\n-    };\n-  }\n-\n   /**\n    * Use Guava Ordering.natural() instead\n    *",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/common/guava/Comparators.java",
                "sha": "fec9879826ac33db6daa0c6d1f5dc9e3e1098d28",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/emitter/core/Emitters.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/java/util/emitter/core/Emitters.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "core/src/main/java/org/apache/druid/java/util/emitter/core/Emitters.java",
                "patch": "@@ -50,7 +50,7 @@ public static Emitter create(\n       Lifecycle lifecycle\n   )\n   {\n-    Map<String, Object> jsonified = new HashMap<>();\n+    Map<String, Object> jsonified;\n     if (props.getProperty(LOG_EMITTER_PROP) != null) {\n       jsonified = makeLoggingMap(props);\n       jsonified.put(\"type\", \"logging\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/emitter/core/Emitters.java",
                "sha": "00d424b88fc4ac61493a3b777d6cb2cf1d11f9b7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/http/client/pool/ChannelResourceFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/java/util/http/client/pool/ChannelResourceFactory.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "core/src/main/java/org/apache/druid/java/util/http/client/pool/ChannelResourceFactory.java",
                "patch": "@@ -77,7 +77,7 @@ public ChannelResourceFactory(\n   public ChannelFuture generate(final String hostname)\n   {\n     log.debug(\"Generating: %s\", hostname);\n-    URL url = null;\n+    URL url;\n     try {\n       url = new URL(hostname);\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/java/util/http/client/pool/ChannelResourceFactory.java",
                "sha": "9c7f4c7d0aea3a27a4aa5ddbd383018a3f4e70c3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/math/expr/ExprEval.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/main/java/org/apache/druid/math/expr/ExprEval.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "core/src/main/java/org/apache/druid/math/expr/ExprEval.java",
                "patch": "@@ -52,6 +52,9 @@ public static ExprEval of(double doubleValue)\n \n   public static ExprEval of(@Nullable String stringValue)\n   {\n+    if (stringValue == null) {\n+      return StringExprEval.OF_NULL;\n+    }\n     return new StringExprEval(stringValue);\n   }\n \n@@ -180,7 +183,11 @@ public final ExprEval castTo(ExprType castTo)\n         case DOUBLE:\n           return this;\n         case LONG:\n-          return ExprEval.of(value == null ? null : asLong());\n+          if (value == null) {\n+            return ExprEval.of(null);\n+          } else {\n+            return ExprEval.of(asLong());\n+          }\n         case STRING:\n           return ExprEval.of(asString());\n       }\n@@ -218,7 +225,11 @@ public final ExprEval castTo(ExprType castTo)\n     {\n       switch (castTo) {\n         case DOUBLE:\n-          return ExprEval.of(value == null ? null : asDouble());\n+          if (value == null) {\n+            return ExprEval.of(null);\n+          } else {\n+            return ExprEval.of(asDouble());\n+          }\n         case LONG:\n           return this;\n         case STRING:\n@@ -236,6 +247,8 @@ public Expr toExpr()\n \n   private static class StringExprEval extends ExprEval<String>\n   {\n+    private static final StringExprEval OF_NULL = new StringExprEval(null);\n+\n     private Number numericVal;\n \n     private StringExprEval(@Nullable String value)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/main/java/org/apache/druid/math/expr/ExprEval.java",
                "sha": "60009122de19b0463898e080fa2b2c8a3f200269",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/collections/OrderedMergeIteratorTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/collections/OrderedMergeIteratorTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 6,
                "filename": "core/src/test/java/org/apache/druid/collections/OrderedMergeIteratorTest.java",
                "patch": "@@ -26,6 +26,7 @@\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.NoSuchElementException;\n \n@@ -170,21 +171,19 @@ public Integer next()\n   @Test(expected = NoSuchElementException.class)\n   public void testNoElementInNext()\n   {\n-    final ArrayList<Iterator<Integer>> iterators = new ArrayList<>();\n-    OrderedMergeIterator<Integer> iter = new OrderedMergeIterator<Integer>(\n+    OrderedMergeIterator<Integer> iter = new OrderedMergeIterator<>(\n         Ordering.natural(),\n-        iterators.iterator()\n+        Collections.emptyIterator()\n     );\n     iter.next();\n   }\n \n   @Test(expected = UnsupportedOperationException.class)\n   public void testRemove()\n   {\n-    final ArrayList<Iterator<Integer>> iterators = new ArrayList<>();\n-    OrderedMergeIterator<Integer> iter = new OrderedMergeIterator<Integer>(\n+    OrderedMergeIterator<Integer> iter = new OrderedMergeIterator<>(\n         Ordering.natural(),\n-        iterators.iterator()\n+        Collections.emptyIterator()\n     );\n     iter.remove();\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/collections/OrderedMergeIteratorTest.java",
                "sha": "e58b83b4e1a5d5ed8690c4c00833be5ce4747425",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/ComparatorsTest.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/java/util/common/guava/ComparatorsTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 28,
                "filename": "core/src/test/java/org/apache/druid/java/util/common/guava/ComparatorsTest.java",
                "patch": "@@ -31,34 +31,6 @@\n  */\n public class ComparatorsTest\n {\n-  @Test\n-  public void testInverse()\n-  {\n-    Comparator<Integer> normal = Comparators.comparable();\n-    Comparator<Integer> inverted = Comparators.inverse(normal);\n-\n-    Assert.assertEquals(-1, normal.compare(0, 1));\n-    Assert.assertEquals(1, normal.compare(1, 0));\n-    Assert.assertEquals(0, normal.compare(1, 1));\n-    Assert.assertEquals(1, inverted.compare(0, 1));\n-    Assert.assertEquals(-1, inverted.compare(1, 0));\n-    Assert.assertEquals(0, inverted.compare(1, 1));\n-  }\n-\n-  @Test\n-  public void testInverseOverflow()\n-  {\n-    Comparator<Integer> invertedSimpleIntegerComparator = Comparators.inverse(new Comparator<Integer>()\n-    {\n-      @Override\n-      public int compare(Integer o1, Integer o2)\n-      {\n-        return o1 - o2;\n-      }\n-    });\n-    Assert.assertTrue(invertedSimpleIntegerComparator.compare(0, Integer.MIN_VALUE) < 0);\n-  }\n-\n   @Test\n   public void testIntervalsByStartThenEnd()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/ComparatorsTest.java",
                "sha": "816e2d5a32a9dbc883462cf3ad076d4ad54a14d8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/ConcatSequenceTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/java/util/common/guava/ConcatSequenceTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "core/src/test/java/org/apache/druid/java/util/common/guava/ConcatSequenceTest.java",
                "patch": "@@ -138,7 +138,7 @@ public void cleanup(Iterator<Sequence<Integer>> iterFromMake)\n               public Integer accumulate(Integer accumulated, Integer in)\n               {\n                 Assert.assertEquals(accumulated, in);\n-                return ++accumulated;\n+                return accumulated + 1;\n               }\n             }\n         ).intValue()\n@@ -154,7 +154,7 @@ public Integer accumulate(Integer accumulated, Integer in)\n           public Integer accumulate(Integer accumulated, Integer in)\n           {\n             Assert.assertEquals(accumulated, in);\n-            return ++accumulated;\n+            return accumulated + 1;\n           }\n         }\n     );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/ConcatSequenceTest.java",
                "sha": "cf5cabe99a873f23936e49186da0a14e146db70b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/SequenceTestHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/core/src/test/java/org/apache/druid/java/util/common/guava/SequenceTestHelper.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "core/src/test/java/org/apache/druid/java/util/common/guava/SequenceTestHelper.java",
                "patch": "@@ -134,7 +134,7 @@ public static void testClosed(AtomicInteger closedCounter, Sequence<Integer> seq\n             @Override\n             public Integer accumulate(Integer accumulated, Integer in)\n             {\n-              return ++accumulated;\n+              return accumulated + 1;\n             }\n           }\n       );\n@@ -156,7 +156,7 @@ public Integer accumulate(Integer accumulated, Integer in)\n             @Override\n             public Integer accumulate(Integer accumulated, Integer in)\n             {\n-              return ++accumulated;\n+              return accumulated + 1;\n             }\n           }\n       );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/core/src/test/java/org/apache/druid/java/util/common/guava/SequenceTestHelper.java",
                "sha": "52082e3c3397814cb103e24f6bb2123bef9383ef",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/BitIterator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extendedset/src/main/java/org/apache/druid/extendedset/intset/BitIterator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extendedset/src/main/java/org/apache/druid/extendedset/intset/BitIterator.java",
                "patch": "@@ -29,7 +29,7 @@\n   private boolean literalAndZeroFill;\n   private int nextIndex = 0;\n   private int nextOffset = 0;\n-  private int next = -1;\n+  private int next;\n \n   BitIterator(ImmutableConciseSet immutableConciseSet)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/BitIterator.java",
                "sha": "e6d3c6ea6de0c72862375e5d97f9101f7d0b8c9e",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/ConciseSet.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extendedset/src/main/java/org/apache/druid/extendedset/intset/ConciseSet.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 8,
                "filename": "extendedset/src/main/java/org/apache/druid/extendedset/intset/ConciseSet.java",
                "patch": "@@ -1272,9 +1272,9 @@ public int compareTo(IntSet o)\n     final ConciseSet other = convert(o);\r\n \r\n     // the word at the end must be the same\r\n-    int res = this.last - other.last;\r\n+    int res = Integer.compare(this.last, other.last);\r\n     if (res != 0) {\r\n-      return res < 0 ? -1 : 1;\r\n+      return res;\r\n     }\r\n \r\n     // scan words from MSB to LSB\r\n@@ -1295,19 +1295,19 @@ public int compareTo(IntSet o)\n               return -1;\r\n             }\r\n             // compare two sequences of zeros\r\n-            res = getSequenceCount(otherWord) - getSequenceCount(thisWord);\r\n+            res = Integer.compare(getSequenceCount(otherWord), getSequenceCount(thisWord));\r\n             if (res != 0) {\r\n-              return res < 0 ? -1 : 1;\r\n+              return res;\r\n             }\r\n           } else {\r\n             if (isZeroSequence(otherWord)) {\r\n               // ones > zeros\r\n               return 1;\r\n             }\r\n             // compare two sequences of ones\r\n-            res = getSequenceCount(thisWord) - getSequenceCount(otherWord);\r\n+            res = Integer.compare(getSequenceCount(thisWord), getSequenceCount(otherWord));\r\n             if (res != 0) {\r\n-              return res < 0 ? -1 : 1;\r\n+              return res;\r\n             }\r\n           }\r\n           // if the sequences are the same (both zeros or both ones)\r\n@@ -1363,9 +1363,10 @@ public int compareTo(IntSet o)\n           otherWord--;\r\n         }\r\n       } else {\r\n-        res = thisWord - otherWord; // equals getLiteralBits(thisWord) - getLiteralBits(otherWord)\r\n+        // equals compare(getLiteralBits(thisWord), getLiteralBits(otherWord))\r\n+        res = Integer.compare(thisWord, otherWord);\r\n         if (res != 0) {\r\n-          return res < 0 ? -1 : 1;\r\n+          return res;\r\n         }\r\n         if (--thisIndex >= 0) {\r\n           thisWord = this.words[thisIndex];\r",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/ConciseSet.java",
                "sha": "c944195de34b8ffdfb72fb2f167e3d09ef56e831",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/ImmutableConciseSet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extendedset/src/main/java/org/apache/druid/extendedset/intset/ImmutableConciseSet.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extendedset/src/main/java/org/apache/druid/extendedset/intset/ImmutableConciseSet.java",
                "patch": "@@ -216,7 +216,7 @@ public static ImmutableConciseSet complement(ImmutableConciseSet set, int length\n     int endIndex = length - 1;\n \n     int wordsWalked = 0;\n-    int last = 0;\n+    int last;\n \n     WordIterator iter = set.newWordIterator();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extendedset/src/main/java/org/apache/druid/extendedset/intset/ImmutableConciseSet.java",
                "sha": "43591e2b59d1e6c77431353e0496c436043f90d2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/azure-extensions/src/main/java/org/apache/druid/firehose/azure/AzureBlob.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/azure-extensions/src/main/java/org/apache/druid/firehose/azure/AzureBlob.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/firehose/azure/AzureBlob.java",
                "patch": "@@ -30,11 +30,11 @@\n {\n   @JsonProperty\n   @NotNull\n-  private String container = null;\n+  private String container;\n \n   @JsonProperty\n   @NotNull\n-  private String path = null;\n+  private String path;\n \n   @JsonCreator\n   public AzureBlob(@JsonProperty(\"container\") String container, @JsonProperty(\"path\") String path)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/azure-extensions/src/main/java/org/apache/druid/firehose/azure/AzureBlob.java",
                "sha": "eb30a16d21d103445c079116e23859e3c3e0afc7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cassandra-storage/src/main/java/org/apache/druid/storage/cassandra/CassandraDataSegmentPuller.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/cassandra-storage/src/main/java/org/apache/druid/storage/cassandra/CassandraDataSegmentPuller.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-contrib/cassandra-storage/src/main/java/org/apache/druid/storage/cassandra/CassandraDataSegmentPuller.java",
                "patch": "@@ -62,9 +62,8 @@ public CassandraDataSegmentPuller(CassandraDataSegmentConfig config)\n     final File tmpFile = new File(outDir, \"index.zip\");\n     log.info(\"Pulling to temporary local cache [%s]\", tmpFile.getAbsolutePath());\n \n-    final FileUtils.FileCopyResult localResult;\n     try {\n-      localResult = RetryUtils.retry(\n+      RetryUtils.retry(\n           () -> {\n             try (OutputStream os = new FileOutputStream(tmpFile)) {\n               ChunkedStorage",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cassandra-storage/src/main/java/org/apache/druid/storage/cassandra/CassandraDataSegmentPuller.java",
                "sha": "a94b6bfd9fddb476be78dcb3583b7ea38f828172",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/firehose/cloudfiles/CloudFilesBlob.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/firehose/cloudfiles/CloudFilesBlob.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/firehose/cloudfiles/CloudFilesBlob.java",
                "patch": "@@ -29,15 +29,15 @@\n {\n   @JsonProperty\n   @NotNull\n-  private String container = null;\n+  private String container;\n \n   @JsonProperty\n   @NotNull\n-  private String path = null;\n+  private String path;\n \n   @JsonProperty\n   @NotNull\n-  private String region = null;\n+  private String region;\n \n   @JsonCreator\n   public CloudFilesBlob(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/firehose/cloudfiles/CloudFilesBlob.java",
                "sha": "edc0bbb65c061a324ff6825bf6791f46d484eeda",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/storage/cloudfiles/CloudFilesStorageDruidModule.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/storage/cloudfiles/CloudFilesStorageDruidModule.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/storage/cloudfiles/CloudFilesStorageDruidModule.java",
                "patch": "@@ -96,7 +96,7 @@ public CloudFilesApi getCloudFilesApi(final CloudFilesAccountConfig config)\n   {\n     log.info(\"Building Cloud Files Api...\");\n \n-    Iterable<com.google.inject.Module> modules = null;\n+    Iterable<com.google.inject.Module> modules;\n     if (config.getUseServiceNet()) {\n       log.info(\"Configuring Cloud Files Api to use the internal service network...\");\n       modules = ImmutableSet.of(new SLF4JLoggingModule(), new InternalUrlModule());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/cloudfiles-extensions/src/main/java/org/apache/druid/storage/cloudfiles/CloudFilesStorageDruidModule.java",
                "sha": "c54342fd32de96bf2960c1d8e5df5254578ad1ea",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/kafka-eight-simpleConsumer/src/main/java/org/apache/druid/firehose/kafka/KafkaSimpleConsumer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/kafka-eight-simpleConsumer/src/main/java/org/apache/druid/firehose/kafka/KafkaSimpleConsumer.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-contrib/kafka-eight-simpleConsumer/src/main/java/org/apache/druid/firehose/kafka/KafkaSimpleConsumer.java",
                "patch": "@@ -197,7 +197,7 @@ private long getOffset(boolean earliest) throws InterruptedException\n         )\n     );\n     OffsetRequest request = new OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientId);\n-    OffsetResponse response = null;\n+    OffsetResponse response;\n     try {\n       response = consumer.getOffsetsBefore(request);\n     }\n@@ -219,7 +219,7 @@ private long getOffset(boolean earliest) throws InterruptedException\n \n   public Iterable<BytesMessageWithOffset> fetch(long offset, int timeoutMs) throws InterruptedException\n   {\n-    FetchResponse response = null;\n+    FetchResponse response;\n     Broker previousLeader = leaderBroker;\n     while (true) {\n       ensureConsumer(previousLeader);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/kafka-eight-simpleConsumer/src/main/java/org/apache/druid/firehose/kafka/KafkaSimpleConsumer.java",
                "sha": "038fb2db90f4a048b7476fd35807712258e553be",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/materialized-view-maintenance/src/main/java/org/apache/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/materialized-view-maintenance/src/main/java/org/apache/druid/indexing/materializedview/MaterializedViewSupervisor.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-contrib/materialized-view-maintenance/src/main/java/org/apache/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "patch": "@@ -327,7 +327,8 @@ void checkSegmentsAndSubmitTasks()\n     // use max created_date of base segments as the version of derivative segments\n     Map<Interval, String> maxCreatedDate = baseSegmentsSnapshot.lhs;\n     Map<Interval, String> derivativeVersion = derivativeSegmentsSnapshot.lhs;\n-    SortedMap<Interval, String> sortedToBuildInterval = new TreeMap<>(Comparators.inverse(Comparators.intervalsByStartThenEnd()));\n+    SortedMap<Interval, String> sortedToBuildInterval =\n+        new TreeMap<>(Comparators.intervalsByStartThenEnd().reversed());\n     // find the intervals to drop and to build\n     MapDifference<Interval, String> difference = Maps.difference(maxCreatedDate, derivativeVersion);\n     Map<Interval, String> toBuildInterval = new HashMap<>(difference.entriesOnlyOnLeft());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/materialized-view-maintenance/src/main/java/org/apache/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "sha": "2aa05e39ee4854f1976411fa573b828446dd8b56",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcIndexGeneratorJobTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcIndexGeneratorJobTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-contrib/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcIndexGeneratorJobTest.java",
                "patch": "@@ -54,7 +54,7 @@\n import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n-import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.orc.CompressionKind;\n import org.apache.orc.OrcFile;\n import org.apache.orc.TypeDescription;\n@@ -225,7 +225,7 @@ public void setUp() throws Exception\n                 false,\n                 false,\n                 false,\n-                ImmutableMap.of(JobContext.NUM_REDUCES, \"0\"), //verifies that set num reducers is ignored\n+                ImmutableMap.of(MRJobConfig.NUM_REDUCES, \"0\"), //verifies that set num reducers is ignored\n                 false,\n                 true,\n                 null,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-contrib/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcIndexGeneratorJobTest.java",
                "sha": "6aa4715920a6eac9f373f5948f4e4f6fa345e822",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildAggregator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildAggregator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildAggregator.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.query.aggregation.datasketches.quantiles;\n \n+import com.yahoo.sketches.quantiles.DoublesSketch;\n import com.yahoo.sketches.quantiles.UpdateDoublesSketch;\n import org.apache.druid.query.aggregation.Aggregator;\n import org.apache.druid.segment.ColumnValueSelector;\n@@ -35,7 +36,7 @@ public DoublesSketchBuildAggregator(final ColumnValueSelector<Double> valueSelec\n   {\n     this.valueSelector = valueSelector;\n     this.size = size;\n-    sketch = UpdateDoublesSketch.builder().setK(size).build();\n+    sketch = DoublesSketch.builder().setK(size).build();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildAggregator.java",
                "sha": "bd46fc5166f9e7a6a0d1daa4f8bf9a56c7baaa27",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildBufferAggregator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildBufferAggregator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildBufferAggregator.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.query.aggregation.datasketches.quantiles;\n \n import com.yahoo.memory.WritableMemory;\n+import com.yahoo.sketches.quantiles.DoublesSketch;\n import com.yahoo.sketches.quantiles.UpdateDoublesSketch;\n import it.unimi.dsi.fastutil.ints.Int2ObjectMap;\n import it.unimi.dsi.fastutil.ints.Int2ObjectOpenHashMap;\n@@ -54,7 +55,7 @@ public synchronized void init(final ByteBuffer buffer, final int position)\n   {\n     final WritableMemory mem = getMemory(buffer);\n     final WritableMemory region = mem.writableRegion(position, maxIntermediateSize);\n-    final UpdateDoublesSketch sketch = UpdateDoublesSketch.builder().setK(size).build(region);\n+    final UpdateDoublesSketch sketch = DoublesSketch.builder().setK(size).build(region);\n     putSketch(buffer, position, sketch);\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/quantiles/DoublesSketchBuildBufferAggregator.java",
                "sha": "ead9a6aa2807e23117ec1069c94be7f3479c08e0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/theta/SketchHolder.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/theta/SketchHolder.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/theta/SketchHolder.java",
                "patch": "@@ -90,6 +90,7 @@ public int compare(Sketch o1, Sketch o2)\n \n   private static final Comparator<Memory> MEMORY_COMPARATOR = new Comparator<Memory>()\n   {\n+    @SuppressWarnings(\"SubtractionInCompareTo\")\n     @Override\n     public int compare(Memory o1, Memory o2)\n     {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/theta/SketchHolder.java",
                "sha": "1e70a7197d76a00a27b4ede5ce60a85e200c5a65",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/tuple/ArrayOfDoublesSketchToQuantilesSketchPostAggregator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/tuple/ArrayOfDoublesSketchToQuantilesSketchPostAggregator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/tuple/ArrayOfDoublesSketchToQuantilesSketchPostAggregator.java",
                "patch": "@@ -74,7 +74,7 @@ public ArrayOfDoublesSketchToQuantilesSketchPostAggregator(\n   public DoublesSketch compute(final Map<String, Object> combinedAggregators)\n   {\n     final ArrayOfDoublesSketch sketch = (ArrayOfDoublesSketch) getField().compute(combinedAggregators);\n-    final UpdateDoublesSketch qs = UpdateDoublesSketch.builder().setK(k).build();\n+    final UpdateDoublesSketch qs = DoublesSketch.builder().setK(k).build();\n     final ArrayOfDoublesSketchIterator it = sketch.iterator();\n     while (it.next()) {\n       qs.update(it.getValues()[column - 1]); // convert 1-based column number to zero-based index",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/main/java/org/apache/druid/query/aggregation/datasketches/tuple/ArrayOfDoublesSketchToQuantilesSketchPostAggregator.java",
                "sha": "220c1cd6ac389cc073b8bd461fe23b2381076836",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/quantiles/GenerateTestData.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/quantiles/GenerateTestData.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/quantiles/GenerateTestData.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package org.apache.druid.query.aggregation.datasketches.quantiles;\n \n+import com.yahoo.sketches.quantiles.DoublesSketch;\n import com.yahoo.sketches.quantiles.UpdateDoublesSketch;\n import org.apache.commons.codec.binary.Base64;\n \n@@ -44,7 +45,7 @@ public static void main(String[] args) throws Exception\n     int sequenceNumber = 0;\n     for (int i = 0; i < 20; i++) {\n       int product = rand.nextInt(10);\n-      UpdateDoublesSketch sketch = UpdateDoublesSketch.builder().build();\n+      UpdateDoublesSketch sketch = DoublesSketch.builder().build();\n       for (int j = 0; j < 20; j++) {\n         double value = rand.nextDouble();\n         buildData.write(\"2016010101\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/datasketches/src/test/java/org/apache/druid/query/aggregation/datasketches/quantiles/GenerateTestData.java",
                "sha": "ea9d32851b77c0fe14259bf407b4ac3678008996",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/DruidKerberosAuthenticationHandler.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/DruidKerberosAuthenticationHandler.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 6,
                "filename": "extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/DruidKerberosAuthenticationHandler.java",
                "patch": "@@ -143,15 +143,17 @@ public GSSManager run()\n   public AuthenticationToken authenticate(HttpServletRequest request, final HttpServletResponse response)\n       throws IOException, AuthenticationException\n   {\n-    AuthenticationToken token = null;\n-    String authorization = request.getHeader(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.AUTHORIZATION);\n+    AuthenticationToken token;\n+    String authorization = request\n+        .getHeader(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.AUTHORIZATION);\n \n-    if (authorization == null\n-        || !authorization.startsWith(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE)) {\n+    if (authorization == null ||\n+        !authorization.startsWith(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE)) {\n       return null;\n     } else {\n-      authorization = authorization.substring(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE\n-                                                  .length()).trim();\n+      authorization = authorization\n+          .substring(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE.length())\n+          .trim();\n       final Base64 base64 = new Base64(0);\n       final byte[] clientToken = base64.decode(authorization);\n       final String serverName = request.getServerName();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/DruidKerberosAuthenticationHandler.java",
                "sha": "12ebb4be4e15e1aa534402099f400778325e5cfb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/KerberosAuthenticator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/KerberosAuthenticator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/KerberosAuthenticator.java",
                "patch": "@@ -248,7 +248,7 @@ public void doFilter(ServletRequest request, ServletResponse response, FilterCha\n         if (isExcluded(path)) {\n           filterChain.doFilter(request, response);\n         } else {\n-          String clientPrincipal = null;\n+          String clientPrincipal;\n           try {\n             Cookie[] cookies = httpReq.getCookies();\n             if (cookies == null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/KerberosAuthenticator.java",
                "sha": "07b27a69930154dc7b276ea93f0e839137a4454a",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 10,
                "filename": "extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java",
                "patch": "@@ -73,30 +73,31 @@\n  */\n public class HdfsDataSegmentPusherTest\n {\n+  static TestObjectMapper objectMapper;\n+\n+  static {\n+    objectMapper = new TestObjectMapper();\n+    InjectableValues.Std injectableValues = new InjectableValues.Std();\n+    injectableValues.addValue(ObjectMapper.class, objectMapper);\n+    injectableValues.addValue(DataSegment.PruneLoadSpecHolder.class, DataSegment.PruneLoadSpecHolder.DEFAULT);\n+    objectMapper.setInjectableValues(injectableValues);\n+  }\n \n   @Rule\n   public final TemporaryFolder tempFolder = new TemporaryFolder();\n \n   @Rule\n   public final ExpectedException expectedException = ExpectedException.none();\n \n-  static TestObjectMapper objectMapper = new TestObjectMapper();\n-\n   private HdfsDataSegmentPusher hdfsDataSegmentPusher;\n+\n   @Before\n-  public void setUp() throws IOException\n+  public void setUp()\n   {\n     HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConf = new HdfsDataSegmentPusherConfig();\n     hdfsDataSegmentPusherConf.setStorageDirectory(\"path/to/\");\n     hdfsDataSegmentPusher = new HdfsDataSegmentPusher(hdfsDataSegmentPusherConf, new Configuration(true), objectMapper);\n   }\n-  static {\n-    objectMapper = new TestObjectMapper();\n-    InjectableValues.Std injectableValues = new InjectableValues.Std();\n-    injectableValues.addValue(ObjectMapper.class, objectMapper);\n-    injectableValues.addValue(DataSegment.PruneLoadSpecHolder.class, DataSegment.PruneLoadSpecHolder.DEFAULT);\n-    objectMapper.setInjectableValues(injectableValues);\n-  }\n \n   @Test\n   public void testPushWithScheme() throws Exception",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java",
                "sha": "03f45080865b63797f39406b367162846af64383",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/histogram/src/main/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogram.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/main/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogram.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "extensions-core/histogram/src/main/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogram.java",
                "patch": "@@ -363,7 +363,6 @@ public void offer(float value)\n       mergeValue = true;\n     }\n     if (deltaLeft < minDelta) {\n-      minDelta = deltaLeft;\n       minPos = insertAt - 1;\n       mergeValue = true;\n     }\n@@ -1563,7 +1562,7 @@ public double sum(final float b)\n       int i = 0;\n       int sum = 0;\n       int k = 1;\n-      long count = 0;\n+      long count;\n       while (k <= this.binCount()) {\n         count = bins[k - 1];\n         if (sum + count > s) {\n@@ -1583,7 +1582,7 @@ public double sum(final float b)\n         final double c = -2 * d;\n         final long a = bins[i] - bins[i - 1];\n         final long b = 2 * bins[i - 1];\n-        double z = 0;\n+        double z;\n         if (a == 0) {\n           z = -c / b;\n         } else {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/histogram/src/main/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogram.java",
                "sha": "0135c8de18d310e3b288ceb0323561df713fa7ef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.collect.Iterators;\n import org.apache.druid.java.util.common.StringUtils;\n import org.junit.Assert;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import java.nio.ByteBuffer;\n@@ -221,6 +222,7 @@ public void testFoldNothing2()\n   }\n \n     //@Test\n+  @Ignore\n   @SuppressWarnings(\"unused\") //TODO rewrite using JMH and move to the benchmarks module\n   public void testFoldSpeed()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/ApproximateHistogramTest.java",
                "sha": "c63ead315bbca5ec26f518ca2b4813b8aef92d84",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 5,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -34,7 +34,6 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Iterables;\n-import com.google.common.collect.Lists;\n import com.google.common.primitives.Longs;\n import com.google.common.util.concurrent.Futures;\n import com.google.common.util.concurrent.ListenableFuture;\n@@ -43,6 +42,7 @@\n import org.apache.commons.codec.digest.DigestUtils;\n import org.apache.druid.indexer.TaskLocation;\n import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexing.common.IndexTaskClient;\n import org.apache.druid.indexing.common.TaskInfoProvider;\n import org.apache.druid.indexing.common.stats.RowIngestionMetersFactory;\n import org.apache.druid.indexing.common.task.RealtimeIndexTask;\n@@ -129,7 +129,6 @@\n   private static final long INITIAL_GET_OFFSET_DELAY_MILLIS = 15000;\n   private static final long INITIAL_EMIT_LAG_METRIC_DELAY_MILLIS = 25000;\n   private static final int MAX_INITIALIZATION_RETRIES = 20;\n-  private static final CopyOnWriteArrayList EMPTY_LIST = Lists.newCopyOnWriteArrayList();\n \n   public static final String IS_INCREMENTAL_HANDOFF_SUPPORTED = \"IS_INCREMENTAL_HANDOFF_SUPPORTED\";\n \n@@ -337,7 +336,7 @@ public TaskLocation getTaskLocation(final String id)\n     this.futureTimeoutInSeconds = Math.max(\n         MINIMUM_FUTURE_TIMEOUT_IN_SECONDS,\n         tuningConfig.getChatRetries() * (tuningConfig.getHttpTimeout().getStandardSeconds()\n-                                         + KafkaIndexTaskClient.MAX_RETRY_WAIT_SECONDS)\n+                                         + IndexTaskClient.MAX_RETRY_WAIT_SECONDS)\n     );\n \n     int chatThreads = (this.tuningConfig.getChatThreads() != null\n@@ -468,6 +467,12 @@ public void stop(boolean stopGracefully)\n     }\n   }\n \n+  private boolean someTaskGroupsPendingCompletion(Integer groupId)\n+  {\n+    CopyOnWriteArrayList<TaskGroup> taskGroups = pendingCompletionTaskGroups.get(groupId);\n+    return taskGroups != null && taskGroups.size() > 0;\n+  }\n+\n   @Override\n   public SupervisorReport getStatus()\n   {\n@@ -1341,7 +1346,7 @@ private void verifyAndMergeCheckpoints(final TaskGroup taskGroup)\n                     partitionOffset.getValue() :\n                     latestOffsetsFromDb.getOrDefault(partitionOffset.getKey(), partitionOffset.getValue())\n                 ) == 0) && earliestConsistentSequenceId.compareAndSet(-1, sequenceCheckpoint.getKey())) || (\n-                pendingCompletionTaskGroups.getOrDefault(groupId, EMPTY_LIST).size() > 0\n+                someTaskGroupsPendingCompletion(groupId)\n                 && earliestConsistentSequenceId.compareAndSet(-1, taskCheckpoints.firstKey()))) {\n           final SortedMap<Integer, Map<Integer, Long>> latestCheckpoints = new TreeMap<>(\n               taskCheckpoints.tailMap(earliestConsistentSequenceId.get())\n@@ -1378,7 +1383,7 @@ private void verifyAndMergeCheckpoints(final TaskGroup taskGroup)\n     }\n \n     if ((tasksToKill.size() > 0 && tasksToKill.size() == taskGroup.tasks.size()) ||\n-        (taskGroup.tasks.size() == 0 && pendingCompletionTaskGroups.getOrDefault(groupId, EMPTY_LIST).size() == 0)) {\n+        (taskGroup.tasks.size() == 0 && !someTaskGroupsPendingCompletion(groupId))) {\n       // killing all tasks or no task left in the group ?\n       // clear state about the taskgroup so that get latest offset information is fetched from metadata store\n       log.warn(\"Clearing task group [%d] information as no valid tasks left the group\", groupId);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "6609b2412c07d98641d9b2665d7de7ec583f342d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/test/TestBroker.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/test/TestBroker.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/test/TestBroker.java",
                "patch": "@@ -29,7 +29,7 @@\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n import org.apache.kafka.common.serialization.ByteArraySerializer;\n-import org.apache.kafka.common.utils.SystemTime;\n+import org.apache.kafka.common.utils.Time;\n import scala.Some;\n import scala.collection.immutable.List$;\n \n@@ -81,7 +81,7 @@ public void start()\n \n     final KafkaConfig config = new KafkaConfig(props);\n \n-    server = new KafkaServer(config, SystemTime.SYSTEM, Some.apply(StringUtils.format(\"TestingBroker[%d]-\", id)), List$.MODULE$.empty());\n+    server = new KafkaServer(config, Time.SYSTEM, Some.apply(StringUtils.format(\"TestingBroker[%d]-\", id)), List$.MODULE$.empty());\n     server.startup();\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/test/TestBroker.java",
                "sha": "a57d22b1446ceba88cf4f418e938fdf20116cbe6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/CacheSchedulerTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/CacheSchedulerTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/CacheSchedulerTest.java",
                "patch": "@@ -311,7 +311,7 @@ public void testSimpleDelete() throws InterruptedException\n     testDelete();\n   }\n \n-  public void testDelete() throws InterruptedException\n+  private void testDelete() throws InterruptedException\n   {\n     final long period = 1_000L; // Give it some time between attempts to update\n     final UriExtractionNamespace namespace = getUriExtractionNamespace(period);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/CacheSchedulerTest.java",
                "sha": "41a861621dc436409f0a5304011bb6773fc665f1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "patch": "@@ -519,15 +519,15 @@ public void testSerde() throws IOException\n   private void waitForUpdates(long timeout, long numUpdates) throws InterruptedException\n   {\n     long startTime = System.currentTimeMillis();\n-    long pre = 0L;\n+    long pre;\n     updateLock.lockInterruptibly();\n     try {\n       pre = updates.get();\n     }\n     finally {\n       updateLock.unlock();\n     }\n-    long post = 0L;\n+    long post;\n     do {\n       // Sleep to spare a few cpu cycles\n       Thread.sleep(5);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/lookups-cached-global/src/test/java/org/apache/druid/server/lookup/namespace/cache/JdbcExtractionNamespaceTest.java",
                "sha": "fd89b7bac68daa6bd864d00aa1e7027d413a2253",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java",
                "patch": "@@ -111,7 +111,7 @@ private Descriptor getDescriptor(String descriptorFilePath)\n \n     fin = this.getClass().getClassLoader().getResourceAsStream(descriptorFilePath);\n     if (fin == null) {\n-      URL url = null;\n+      URL url;\n       try {\n         url = new URL(descriptorFilePath);\n       }\n@@ -126,7 +126,7 @@ private Descriptor getDescriptor(String descriptorFilePath)\n       }\n     }\n \n-    DynamicSchema dynamicSchema = null;\n+    DynamicSchema dynamicSchema;\n     try {\n       dynamicSchema = DynamicSchema.parseFrom(fin);\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java",
                "sha": "e47adeeeaa853341bcf188f3af31726764abdb26",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 9,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "patch": "@@ -125,12 +125,12 @@ public void testGroupBy()\n         .setDataSource(QueryRunnerTestHelper.dataSource)\n         .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n         .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n-        .setAggregatorSpecs(VarianceTestHelper.rowsCount,\n-                            VarianceTestHelper.indexVarianceAggr,\n-                            new LongSumAggregatorFactory(\"idx\", \"index\"))\n-        .setPostAggregatorSpecs(\n-            Collections.singletonList(VarianceTestHelper.stddevOfIndexPostAggr)\n+        .setAggregatorSpecs(\n+            QueryRunnerTestHelper.rowsCount,\n+            VarianceTestHelper.indexVarianceAggr,\n+            new LongSumAggregatorFactory(\"idx\", \"index\")\n         )\n+        .setPostAggregatorSpecs(Collections.singletonList(VarianceTestHelper.stddevOfIndexPostAggr))\n         .setGranularity(QueryRunnerTestHelper.dayGran)\n         .build();\n \n@@ -178,12 +178,14 @@ public void testPostAggHavingSpec()\n \n     GroupByQuery query = GroupByQuery\n         .builder()\n-        .setDataSource(VarianceTestHelper.dataSource)\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n         .setInterval(\"2011-04-02/2011-04-04\")\n         .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n-        .setAggregatorSpecs(VarianceTestHelper.rowsCount,\n-                            VarianceTestHelper.indexLongSum,\n-                            VarianceTestHelper.indexVarianceAggr)\n+        .setAggregatorSpecs(\n+            QueryRunnerTestHelper.rowsCount,\n+            QueryRunnerTestHelper.indexLongSum,\n+            VarianceTestHelper.indexVarianceAggr\n+        )\n         .setPostAggregatorSpecs(ImmutableList.of(VarianceTestHelper.stddevOfIndexPostAggr))\n         .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n         .setHavingSpec(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceGroupByQueryTest.java",
                "sha": "90f61128da6b1313e3eda755a9ee6a48842d98b1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 8,
                "filename": "extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.druid.query.Druids;\n import org.apache.druid.query.QueryPlus;\n import org.apache.druid.query.QueryRunner;\n+import org.apache.druid.query.QueryRunnerTestHelper;\n import org.apache.druid.query.Result;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.query.timeseries.TimeseriesQuery;\n@@ -59,13 +60,13 @@ public VarianceTimeseriesQueryTest(QueryRunner runner, boolean descending, List<\n   public void testTimeseriesWithNullFilterOnNonExistentDimension()\n   {\n     TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n-                                  .dataSource(VarianceTestHelper.dataSource)\n-                                  .granularity(VarianceTestHelper.dayGran)\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.dayGran)\n                                   .filters(\"bobby\", null)\n-                                  .intervals(VarianceTestHelper.firstToThird)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n                                   .aggregators(VarianceTestHelper.commonPlusVarAggregators)\n                                   .postAggregators(\n-                                      VarianceTestHelper.addRowsIndexConstant,\n+                                      QueryRunnerTestHelper.addRowsIndexConstant,\n                                       VarianceTestHelper.stddevOfIndexPostAggr\n                                   )\n                                   .descending(descending)\n@@ -75,11 +76,11 @@ public void testTimeseriesWithNullFilterOnNonExistentDimension()\n         new Result<>(\n             DateTimes.of(\"2011-04-01\"),\n             new TimeseriesResultValue(\n-                VarianceTestHelper.of(\n+                QueryRunnerTestHelper.of(\n                     \"rows\", 13L,\n                     \"index\", 6626.151596069336,\n                     \"addRowsIndexConstant\", 6640.151596069336,\n-                    \"uniques\", VarianceTestHelper.UNIQUES_9,\n+                    \"uniques\", QueryRunnerTestHelper.UNIQUES_9,\n                     \"index_var\", descending ? 368885.6897238851 : 368885.689155086,\n                     \"index_stddev\", descending ? 607.3596049490657 : 607.35960448081\n                 )\n@@ -88,11 +89,11 @@ public void testTimeseriesWithNullFilterOnNonExistentDimension()\n         new Result<>(\n             DateTimes.of(\"2011-04-02\"),\n             new TimeseriesResultValue(\n-                VarianceTestHelper.of(\n+                QueryRunnerTestHelper.of(\n                     \"rows\", 13L,\n                     \"index\", 5833.2095947265625,\n                     \"addRowsIndexConstant\", 5847.2095947265625,\n-                    \"uniques\", VarianceTestHelper.UNIQUES_9,\n+                    \"uniques\", QueryRunnerTestHelper.UNIQUES_9,\n                     \"index_var\", descending ? 259061.6037088883 : 259061.60216419376,\n                     \"index_stddev\", descending ? 508.9809463122252 : 508.98094479478675\n                 )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/extensions-core/stats/src/test/java/org/apache/druid/query/aggregation/variance/VarianceTimeseriesQueryTest.java",
                "sha": "c5dcecea9bfbf5bed0a4e6b5d249ff57ce521bf9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/hll/src/main/java/org/apache/druid/hll/HyperLogLogCollector.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/hll/src/main/java/org/apache/druid/hll/HyperLogLogCollector.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "hll/src/main/java/org/apache/druid/hll/HyperLogLogCollector.java",
                "patch": "@@ -358,7 +358,7 @@ public void add(short bucket, byte positionOf1)\n     short numNonZeroRegisters = addNibbleRegister(bucket, (byte) ((0xff & positionOf1) - registerOffset));\n     setNumNonZeroRegisters(numNonZeroRegisters);\n     if (numNonZeroRegisters == NUM_BUCKETS) {\n-      setRegisterOffset(++registerOffset);\n+      setRegisterOffset((byte) (registerOffset + 1));\n       setNumNonZeroRegisters(decrementBuckets());\n     }\n   }\n@@ -421,7 +421,7 @@ public HyperLogLogCollector fold(@Nullable HyperLogLogCollector other)\n         }\n         if (numNonZero == NUM_BUCKETS) {\n           numNonZero = decrementBuckets();\n-          setRegisterOffset(++myOffset);\n+          setRegisterOffset((byte) (myOffset + 1));\n           setNumNonZeroRegisters(numNonZero);\n         }\n       } else { // dense\n@@ -437,7 +437,7 @@ public HyperLogLogCollector fold(@Nullable HyperLogLogCollector other)\n         }\n         if (numNonZero == NUM_BUCKETS) {\n           numNonZero = decrementBuckets();\n-          setRegisterOffset(++myOffset);\n+          setRegisterOffset((byte) (myOffset + 1));\n           setNumNonZeroRegisters(numNonZero);\n         }\n       }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/hll/src/main/java/org/apache/druid/hll/HyperLogLogCollector.java",
                "sha": "5fd7df778debe3643a441ee997be0045c652c8cf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/hll/src/test/java/org/apache/druid/hll/HyperLogLogCollectorBenchmark.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/hll/src/test/java/org/apache/druid/hll/HyperLogLogCollectorBenchmark.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "hll/src/test/java/org/apache/druid/hll/HyperLogLogCollectorBenchmark.java",
                "patch": "@@ -41,6 +41,7 @@\n {\n   private final HashFunction fn = Hashing.murmur3_128();\n \n+  @SuppressWarnings(\"MismatchedQueryAndUpdateOfCollection\") // TODO understand if this field should be used or not\n   private final List<HyperLogLogCollector> collectors = new ArrayList<>();\n \n   @Param({\"true\"}) boolean targetIsDirect;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/hll/src/test/java/org/apache/druid/hll/HyperLogLogCollectorBenchmark.java",
                "sha": "938bd9c7345a931c8a5801a6a8242ba56969965a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-hadoop/src/test/java/org/apache/druid/indexer/IndexGeneratorJobTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/test/java/org/apache/druid/indexer/IndexGeneratorJobTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "indexing-hadoop/src/test/java/org/apache/druid/indexer/IndexGeneratorJobTest.java",
                "patch": "@@ -54,7 +54,7 @@\n import org.apache.hadoop.io.SequenceFile;\n import org.apache.hadoop.io.SequenceFile.Writer;\n import org.apache.hadoop.io.compress.CompressionCodec;\n-import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\n import org.joda.time.DateTime;\n import org.joda.time.DateTimeComparator;\n@@ -524,7 +524,7 @@ public void setUp() throws Exception\n                 false,\n                 false,\n                 false,\n-                ImmutableMap.of(JobContext.NUM_REDUCES, \"0\"), //verifies that set num reducers is ignored\n+                ImmutableMap.of(MRJobConfig.NUM_REDUCES, \"0\"), //verifies that set num reducers is ignored\n                 false,\n                 useCombiner,\n                 null,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-hadoop/src/test/java/org/apache/druid/indexer/IndexGeneratorJobTest.java",
                "sha": "8e191c7fba918bce8f811e2279ef76c158fa145e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java",
                "patch": "@@ -280,7 +280,7 @@ private TaskStatus runInternal(TaskToolbox toolbox) throws Exception\n         toolbox.getSegmentPusher().getPathForHadoop()\n     };\n \n-    HadoopIngestionSpec indexerSchema = null;\n+    HadoopIngestionSpec indexerSchema;\n     final ClassLoader oldLoader = Thread.currentThread().getContextClassLoader();\n     Class<?> determinePartitionsRunnerClass = determinePartitionsInnerProcessingRunner.getClass();\n     Method determinePartitionsInnerProcessingRunTask = determinePartitionsRunnerClass.getMethod(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java",
                "sha": "104123fc1002acec47e9c06d6c4c73aa5874fc8f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/org/apache/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java",
                "patch": "@@ -963,7 +963,7 @@ private void addPendingTaskToExecutor(final String taskId)\n     pendingTasksExec.execute(\n         () -> {\n           while (!Thread.interrupted() && lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS)) {\n-            ImmutableWorkerInfo immutableWorker = null;\n+            ImmutableWorkerInfo immutableWorker;\n             HttpRemoteTaskRunnerWorkItem taskItem = null;\n             try {\n               synchronized (statusLock) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/overlord/hrtr/HttpRemoteTaskRunner.java",
                "sha": "3d36000c9afaaf37b189bd1717beb008d6fa4783",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorManager.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "patch": "@@ -289,7 +289,7 @@ private boolean createAndStartSupervisorInternal(SupervisorSpec spec, boolean pe\n       metadataSupervisorManager.insert(id, spec);\n     }\n \n-    Supervisor supervisor = null;\n+    Supervisor supervisor;\n     try {\n       supervisor = spec.createSupervisor();\n       supervisor.start();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "sha": "4a8ae2f96cd4ca8e21ef2c93f095e0f3696d278a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/worker/WorkerTaskManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/org/apache/druid/indexing/worker/WorkerTaskManager.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/org/apache/druid/indexing/worker/WorkerTaskManager.java",
                "patch": "@@ -576,7 +576,7 @@ public String getTaskId()\n     @Override\n     public void handle()\n     {\n-      TaskAnnouncement announcement = null;\n+      TaskAnnouncement announcement;\n       synchronized (lock) {\n         if (runningTasks.containsKey(task.getId()) || completedTasks.containsKey(task.getId())) {\n           log.warn(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/main/java/org/apache/druid/indexing/worker/WorkerTaskManager.java",
                "sha": "561a6e1befad1b2a788a7f2c6a049266d8a8c9c8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 4,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "patch": "@@ -174,7 +174,6 @@\n     final IndexerSQLMetadataStorageCoordinator mdc = new IndexerSQLMetadataStorageCoordinator(null, null, null)\n     {\n       private final Set<DataSegment> published = new HashSet<>();\n-      private final Set<DataSegment> nuked = new HashSet<>();\n \n       @Override\n       public List<DataSegment> getUsedSegmentsForInterval(String dataSource, Interval interval)\n@@ -210,7 +209,7 @@\n       @Override\n       public void deleteSegments(Set<DataSegment> segments)\n       {\n-        nuked.addAll(segments);\n+        // do nothing\n       }\n     };\n     final LocalTaskActionClientFactory tac = new LocalTaskActionClientFactory(\n@@ -596,7 +595,6 @@ public void testGetUniqueDimensionsAndMetrics()\n     final int numSegmentsPerPartitionChunk = 5;\n     final int numPartitionChunksPerTimelineObject = 10;\n     final int numSegments = numSegmentsPerPartitionChunk * numPartitionChunksPerTimelineObject;\n-    final List<DataSegment> segments = new ArrayList<>(numSegments);\n     final Interval interval = Intervals.of(\"2017-01-01/2017-01-02\");\n     final String version = \"1\";\n \n@@ -621,7 +619,6 @@ public void testGetUniqueDimensionsAndMetrics()\n             1,\n             1\n         );\n-        segments.add(segment);\n \n         final PartitionChunk<DataSegment> partitionChunk = new NumberedPartitionChunk<>(\n             i,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "sha": "9db06583a80026f60a6cb94e25483e328c0f2ae5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentKiller.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentKiller.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 6,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentKiller.java",
                "patch": "@@ -19,20 +19,15 @@\n \n package org.apache.druid.indexing.test;\n \n-import com.google.common.collect.Sets;\n import org.apache.druid.segment.loading.DataSegmentKiller;\n import org.apache.druid.timeline.DataSegment;\n \n-import java.util.Set;\n-\n public class TestDataSegmentKiller implements DataSegmentKiller\n {\n-  private final Set<DataSegment> killedSegments = Sets.newConcurrentHashSet();\n-\n   @Override\n   public void kill(DataSegment segment)\n   {\n-    killedSegments.add(segment);\n+    // do nothing\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentKiller.java",
                "sha": "33421eb1a5cb22c46be6bfc4d5a5795b66646f68",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentPusher.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentPusher.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 5,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentPusher.java",
                "patch": "@@ -19,19 +19,15 @@\n \n package org.apache.druid.indexing.test;\n \n-import com.google.common.collect.Sets;\n import org.apache.druid.segment.loading.DataSegmentPusher;\n import org.apache.druid.timeline.DataSegment;\n \n import java.io.File;\n import java.net.URI;\n import java.util.Map;\n-import java.util.Set;\n \n public class TestDataSegmentPusher implements DataSegmentPusher\n {\n-  private final Set<DataSegment> pushedSegments = Sets.newConcurrentHashSet();\n-\n   @Deprecated\n   @Override\n   public String getPathForHadoop(String dataSource)\n@@ -48,7 +44,6 @@ public String getPathForHadoop()\n   @Override\n   public DataSegment push(File file, DataSegment segment, boolean useUniquePath)\n   {\n-    pushedSegments.add(segment);\n     return segment;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/indexing-service/src/test/java/org/apache/druid/indexing/test/TestDataSegmentPusher.java",
                "sha": "543fa76d6d7439f6749fac06577cd24a8d41df12",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 5,
                "filename": "integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java",
                "patch": "@@ -88,12 +88,12 @@ private String getLoadStatusURL()\n   // return a list of the segment dates for the specified datasource\n   public List<String> getMetadataSegments(final String dataSource)\n   {\n-    ArrayList<String> segments = null;\n+    ArrayList<String> segments;\n     try {\n       StatusResponseHolder response = makeRequest(HttpMethod.GET, getMetadataSegmentsURL(dataSource));\n \n       segments = jsonMapper.readValue(\n-          response.getContent(), new TypeReference<ArrayList<String>>()\n+          response.getContent(), new TypeReference<List<String>>()\n           {\n           }\n       );\n@@ -107,12 +107,12 @@ private String getLoadStatusURL()\n   // return a list of the segment dates for the specified datasource\n   public List<String> getSegmentIntervals(final String dataSource)\n   {\n-    ArrayList<String> segments = null;\n+    ArrayList<String> segments;\n     try {\n       StatusResponseHolder response = makeRequest(HttpMethod.GET, getIntervalsURL(dataSource));\n \n       segments = jsonMapper.readValue(\n-          response.getContent(), new TypeReference<ArrayList<String>>()\n+          response.getContent(), new TypeReference<List<String>>()\n           {\n           }\n       );\n@@ -125,7 +125,7 @@ private String getLoadStatusURL()\n \n   private Map<String, Integer> getLoadStatus()\n   {\n-    Map<String, Integer> status = null;\n+    Map<String, Integer> status;\n     try {\n       StatusResponseHolder response = makeRequest(HttpMethod.GET, getLoadStatusURL());\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/main/java/org/apache/druid/testing/clients/CoordinatorResourceTestClient.java",
                "sha": "a3a6683395b1ae0ac6b72396e267071b326754fb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java",
                "patch": "@@ -60,7 +60,7 @@ public void testHadoopIndex() throws Exception\n \n   private void loadData(String hadoopDir)\n   {\n-    String indexerSpec = \"\";\n+    String indexerSpec;\n \n     try {\n       LOG.info(\"indexerFile name: [%s]\", BATCH_TASK);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java",
                "sha": "6e6b298aedf7e66c67abc35e73ab3cacff81c4d3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITRealtimeIndexTaskTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITRealtimeIndexTaskTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITRealtimeIndexTaskTest.java",
                "patch": "@@ -100,7 +100,7 @@ void doTest()\n       TimeUnit.SECONDS.sleep(5);\n \n       // put the timestamps into the query structure\n-      String query_response_template = null;\n+      String query_response_template;\n       InputStream is = ITRealtimeIndexTaskTest.class.getResourceAsStream(getQueriesResource());\n       if (null == is) {\n         throw new ISE(\"could not open query file: %s\", getQueriesResource());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractITRealtimeIndexTaskTest.java",
                "sha": "3d7802a7a47f8d27db58bea46b87cd75b919e64c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITAppenderatorDriverRealtimeIndexTaskTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITAppenderatorDriverRealtimeIndexTaskTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITAppenderatorDriverRealtimeIndexTaskTest.java",
                "patch": "@@ -61,7 +61,7 @@ public void postEvents() throws Exception\n     final ServerDiscoverySelector eventReceiverSelector = factory.createSelector(EVENT_RECEIVER_SERVICE_NAME);\n     eventReceiverSelector.start();\n     BufferedReader reader = null;\n-    InputStreamReader isr = null;\n+    InputStreamReader isr;\n     try {\n       isr = new InputStreamReader(\n           ITRealtimeIndexTaskTest.class.getResourceAsStream(EVENT_DATA_FILE),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITAppenderatorDriverRealtimeIndexTaskTest.java",
                "sha": "bdd0df78ee3ec6e553529bbb7bc46085f14c5364",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKafkaTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKafkaTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 4,
                "filename": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKafkaTest.java",
                "patch": "@@ -197,7 +197,6 @@ public void testKafka()\n       consumerProperties.put(\"zookeeper.connection.timeout.ms\", \"15000\");\n       consumerProperties.put(\"zookeeper.sync.time.ms\", \"5000\");\n       consumerProperties.put(\"group.id\", Long.toString(System.currentTimeMillis()));\n-      consumerProperties.put(\"zookeeper.sync.time.ms\", \"5000\");\n       consumerProperties.put(\"fetch.message.max.bytes\", \"1048586\");\n       consumerProperties.put(\"auto.offset.reset\", \"smallest\");\n       consumerProperties.put(\"auto.commit.enable\", \"false\");\n@@ -249,20 +248,20 @@ public Boolean call()\n     segmentsExist = true;\n \n     // put the timestamps into the query structure\n-    String query_response_template = null;\n+    String queryResponseTemplate;\n     InputStream is = ITKafkaTest.class.getResourceAsStream(QUERIES_FILE);\n     if (null == is) {\n       throw new ISE(\"could not open query file: %s\", QUERIES_FILE);\n     }\n \n     try {\n-      query_response_template = IOUtils.toString(is, \"UTF-8\");\n+      queryResponseTemplate = IOUtils.toString(is, \"UTF-8\");\n     }\n     catch (IOException e) {\n       throw new ISE(e, \"could not read query file: %s\", QUERIES_FILE);\n     }\n \n-    String queryStr = query_response_template\n+    String queryStr = queryResponseTemplate\n         .replaceAll(\"%%DATASOURCE%%\", DATASOURCE)\n         // time boundary\n         .replace(\"%%TIMEBOUNDARY_RESPONSE_TIMESTAMP%%\", TIMESTAMP_FMT.print(dtFirst))",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKafkaTest.java",
                "sha": "4078984e854e44a76239a282645f6357469c61c5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITRealtimeIndexTaskTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITRealtimeIndexTaskTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITRealtimeIndexTaskTest.java",
                "patch": "@@ -74,7 +74,7 @@ public void postEvents() throws Exception\n     final ServerDiscoverySelector eventReceiverSelector = factory.createSelector(EVENT_RECEIVER_SERVICE_NAME);\n     eventReceiverSelector.start();\n     BufferedReader reader = null;\n-    InputStreamReader isr = null;\n+    InputStreamReader isr;\n     try {\n       isr = new InputStreamReader(\n           ITRealtimeIndexTaskTest.class.getResourceAsStream(EVENT_DATA_FILE),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/integration-tests/src/test/java/org/apache/druid/tests/indexer/ITRealtimeIndexTaskTest.java",
                "sha": "af30db3347488befd481b644a3aab166a8e16872",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/pom.xml",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 5,
                "filename": "pom.xml",
                "patch": "@@ -337,7 +337,7 @@\n             <dependency>\n                 <groupId>com.google.errorprone</groupId>\n                 <artifactId>error_prone_annotations</artifactId>\n-                <version>2.2.0</version>\n+                <version>2.3.2</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.ibm.icu</groupId>\n@@ -1265,8 +1265,8 @@\n                                 <arg>-Xep:PreconditionsInvalidPlaceholder:ERROR</arg>\n                                 <arg>-Xep:MissingOverride:ERROR</arg>\n                                 <arg>-Xep:DefaultCharset:ERROR</arg>\n+                                <arg>-Xep:QualifierOrScopeOnInjectMethod:ERROR</arg>\n \n-                                <arg>-Xep:ArgumentParameterSwap</arg>\n                                 <arg>-Xep:AssistedInjectAndInjectOnSameConstructor</arg>\n                                 <arg>-Xep:AutoFactoryAtInject</arg>\n                                 <arg>-Xep:ClassName</arg>\n@@ -1289,22 +1289,21 @@\n                                 <arg>-Xep:NumericEquality</arg>\n                                 <arg>-Xep:ParameterPackage</arg>\n                                 <arg>-Xep:ProtoStringFieldReferenceEquality</arg>\n-                                <arg>-Xep:QualifierOnMethodWithoutProvides</arg>\n                                 <arg>-Xep:UnlockMethod</arg>\n                             </compilerArgs>\n                         </configuration>\n                         <dependencies>\n                             <dependency>\n                                 <groupId>org.codehaus.plexus</groupId>\n                                 <artifactId>plexus-compiler-javac-errorprone</artifactId>\n-                                <version>2.8.1</version>\n+                                <version>2.8.5</version>\n                             </dependency>\n                             <!-- override plexus-compiler-javac-errorprone's dependency on\n                                  Error Prone with the latest version -->\n                             <dependency>\n                                 <groupId>com.google.errorprone</groupId>\n                                 <artifactId>error_prone_core</artifactId>\n-                                <version>2.0.19</version>\n+                                <version>2.3.2</version>\n                             </dependency>\n                         </dependencies>\n                     </plugin>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/pom.xml",
                "sha": "d5814ada7b3a62a7f427aa63a4e8e757eaed0df2",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/GroupByQuery.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/GroupByQuery.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 9,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/GroupByQuery.java",
                "patch": "@@ -95,11 +95,14 @@ public static Builder builder()\n \n   private final VirtualColumns virtualColumns;\n   private final LimitSpec limitSpec;\n+  @Nullable\n   private final HavingSpec havingSpec;\n+  @Nullable\n   private final DimFilter dimFilter;\n   private final List<DimensionSpec> dimensions;\n   private final List<AggregatorFactory> aggregatorSpecs;\n   private final List<PostAggregator> postAggregatorSpecs;\n+  @Nullable\n   private final List<List<String>> subtotalsSpec;\n \n   private final boolean applyLimitPushDown;\n@@ -115,9 +118,9 @@ public GroupByQuery(\n       @JsonProperty(\"dimensions\") List<DimensionSpec> dimensions,\n       @JsonProperty(\"aggregations\") List<AggregatorFactory> aggregatorSpecs,\n       @JsonProperty(\"postAggregations\") List<PostAggregator> postAggregatorSpecs,\n-      @JsonProperty(\"having\") HavingSpec havingSpec,\n+      @JsonProperty(\"having\") @Nullable HavingSpec havingSpec,\n       @JsonProperty(\"limitSpec\") LimitSpec limitSpec,\n-      @JsonProperty(\"subtotalsSpec\") List<List<String>> subtotalsSpec,\n+      @JsonProperty(\"subtotalsSpec\") @Nullable List<List<String>> subtotalsSpec,\n       @JsonProperty(\"context\") Map<String, Object> context\n   )\n   {\n@@ -168,12 +171,12 @@ private GroupByQuery(\n       final DataSource dataSource,\n       final QuerySegmentSpec querySegmentSpec,\n       final VirtualColumns virtualColumns,\n-      final DimFilter dimFilter,\n+      final @Nullable DimFilter dimFilter,\n       final Granularity granularity,\n-      final List<DimensionSpec> dimensions,\n-      final List<AggregatorFactory> aggregatorSpecs,\n-      final List<PostAggregator> postAggregatorSpecs,\n-      final HavingSpec havingSpec,\n+      final @Nullable List<DimensionSpec> dimensions,\n+      final @Nullable List<AggregatorFactory> aggregatorSpecs,\n+      final @Nullable List<PostAggregator> postAggregatorSpecs,\n+      final @Nullable HavingSpec havingSpec,\n       final LimitSpec limitSpec,\n       final @Nullable List<List<String>> subtotalsSpec,\n       final @Nullable Function<Sequence<Row>, Sequence<Row>> postProcessingFn,\n@@ -198,7 +201,7 @@ private GroupByQuery(\n     this.havingSpec = havingSpec;\n     this.limitSpec = LimitSpec.nullToNoopLimitSpec(limitSpec);\n \n-    this.subtotalsSpec = verifySubtotalsSpec(subtotalsSpec, dimensions);\n+    this.subtotalsSpec = verifySubtotalsSpec(subtotalsSpec, this.dimensions);\n \n     // Verify no duplicate names between dimensions, aggregators, and postAggregators.\n     // They will all end up in the same namespace in the returned Rows and we can't have them clobbering each other.\n@@ -211,7 +214,11 @@ private GroupByQuery(\n     this.applyLimitPushDown = determineApplyLimitPushDown();\n   }\n \n-  private List<List<String>> verifySubtotalsSpec(List<List<String>> subtotalsSpec, List<DimensionSpec> dimensions)\n+  @Nullable\n+  private List<List<String>> verifySubtotalsSpec(\n+      @Nullable List<List<String>> subtotalsSpec,\n+      List<DimensionSpec> dimensions\n+  )\n   {\n     // if subtotalsSpec exists then validate that all are subsets of dimensions spec and are in same order.\n     // For example if we had {D1, D2, D3} in dimensions spec then\n@@ -736,20 +743,37 @@ private static void verifyOutputNames(\n \n   public static class Builder\n   {\n+    @Nullable\n+    private static List<List<String>> copySubtotalSpec(@Nullable List<List<String>> subtotalsSpec)\n+    {\n+      if (subtotalsSpec == null) {\n+        return null;\n+      }\n+      return subtotalsSpec.stream().map(ArrayList::new).collect(Collectors.toList());\n+    }\n+\n     private DataSource dataSource;\n     private QuerySegmentSpec querySegmentSpec;\n     private VirtualColumns virtualColumns;\n+    @Nullable\n     private DimFilter dimFilter;\n     private Granularity granularity;\n+    @Nullable\n     private List<DimensionSpec> dimensions;\n+    @Nullable\n     private List<AggregatorFactory> aggregatorSpecs;\n+    @Nullable\n     private List<PostAggregator> postAggregatorSpecs;\n+    @Nullable\n     private HavingSpec havingSpec;\n \n     private Map<String, Object> context;\n \n+    @Nullable\n     private List<List<String>> subtotalsSpec = null;\n+    @Nullable\n     private LimitSpec limitSpec = null;\n+    @Nullable\n     private Function<Sequence<Row>, Sequence<Row>> postProcessingFn;\n     private List<OrderByColumnSpec> orderByColumnSpecs = new ArrayList<>();\n     private int limit = Integer.MAX_VALUE;\n@@ -787,6 +811,7 @@ public Builder(Builder builder)\n       postAggregatorSpecs = builder.postAggregatorSpecs;\n       havingSpec = builder.havingSpec;\n       limitSpec = builder.limitSpec;\n+      subtotalsSpec = copySubtotalSpec(builder.subtotalsSpec);\n       postProcessingFn = builder.postProcessingFn;\n       limit = builder.limit;\n       orderByColumnSpecs = new ArrayList<>(builder.orderByColumnSpecs);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/GroupByQuery.java",
                "sha": "1c7fb6c140838475f9b5c0740b8591b952a1ca5a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferArrayGrouper.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferArrayGrouper.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferArrayGrouper.java",
                "patch": "@@ -233,7 +233,7 @@ public void close()\n \n     return new CloseableIterator<Entry<Integer>>()\n     {\n-      int cur = -1;\n+      int cur;\n       boolean findNext = false;\n \n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferArrayGrouper.java",
                "sha": "4bc541f4a3c1703a2a3aef013560f7ae6d2da56a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferHashGrouper.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferHashGrouper.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferHashGrouper.java",
                "patch": "@@ -173,6 +173,7 @@ public void reset()\n     }\n \n     if (sorted) {\n+      @SuppressWarnings(\"MismatchedQueryAndUpdateOfCollection\")\n       final List<Integer> wrappedOffsets = new AbstractList<Integer>()\n       {\n         @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/BufferHashGrouper.java",
                "sha": "5f54fad913310e217eed502474bd933eecc71065",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/LimitedBufferHashGrouper.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/LimitedBufferHashGrouper.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/LimitedBufferHashGrouper.java",
                "patch": "@@ -254,6 +254,7 @@ public int getHeapIndexForOffset(int bucketOffset)\n   {\n     final int size = offsetHeap.getHeapSize();\n \n+    @SuppressWarnings(\"MismatchedQueryAndUpdateOfCollection\")\n     final List<Integer> wrappedOffsets = new AbstractList<Integer>()\n     {\n       @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/LimitedBufferHashGrouper.java",
                "sha": "c8d97eafc1b51e485fa4aa67cdc36d9e09351ba7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/SpillingGrouper.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/SpillingGrouper.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/SpillingGrouper.java",
                "patch": "@@ -72,7 +72,7 @@\n   private final List<File> dictionaryFiles = new ArrayList<>();\n   private final boolean sortHasNonGroupingFields;\n \n-  private boolean spillingAllowed = false;\n+  private boolean spillingAllowed;\n \n   public SpillingGrouper(\n       final Supplier<ByteBuffer> bufferSupplier,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/groupby/epinephelinae/SpillingGrouper.java",
                "sha": "5dce77ffcb8691d3d19ed6110880c9554578d8c0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/ordering/StringComparators.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/ordering/StringComparators.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "processing/src/main/java/org/apache/druid/query/ordering/StringComparators.java",
                "patch": "@@ -122,7 +122,7 @@ public int compare(String str1, String str2)\n         int ch1 = str1.codePointAt(pos[0]);\n         int ch2 = str2.codePointAt(pos[1]);\n \n-        int result = 0;\n+        int result;\n \n         if (isDigit(ch1)) {\n           result = isDigit(ch2) ? compareNumbers(str1, str2, pos) : -1;\n@@ -135,7 +135,7 @@ public int compare(String str1, String str2)\n         }\n       }\n \n-      return str1.length() - str2.length();\n+      return Integer.compare(str1.length(), str2.length());\n     }\n \n     private int compareNumbers(String str0, String str1, int[] pos)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/ordering/StringComparators.java",
                "sha": "dae5cdac2bdb50c2b70a9d8ee587982e11b84837",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/scan/ScanQueryLimitRowIterator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/scan/ScanQueryLimitRowIterator.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/scan/ScanQueryLimitRowIterator.java",
                "patch": "@@ -33,7 +33,7 @@\n {\n   private Yielder<ScanResultValue> yielder;\n   private String resultFormat;\n-  private long limit = 0;\n+  private long limit;\n   private long count = 0;\n \n   public ScanQueryLimitRowIterator(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/scan/ScanQueryLimitRowIterator.java",
                "sha": "b655b21a155d6e6387e931f092665dd095b3d197",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 6,
                "filename": "processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "patch": "@@ -285,22 +285,22 @@ public SearchHit apply(@Nullable Object input)\n                               @Override\n                               public SearchHit apply(@Nullable Object input)\n                               {\n-                                String dim = null;\n-                                String val = null;\n-                                Integer cnt = null;\n+                                String dim;\n+                                String val;\n+                                Integer count;\n                                 if (input instanceof Map) {\n                                   dim = outputNameMap.get((String) ((Map) input).get(\"dimension\"));\n                                   val = (String) ((Map) input).get(\"value\");\n-                                  cnt = (Integer) ((Map) input).get(\"count\");\n+                                  count = (Integer) ((Map) input).get(\"count\");\n                                 } else if (input instanceof SearchHit) {\n                                   SearchHit cached = (SearchHit) input;\n                                   dim = outputNameMap.get(cached.getDimension());\n                                   val = cached.getValue();\n-                                  cnt = cached.getCount();\n+                                  count = cached.getCount();\n                                 } else {\n                                   throw new IAE(\"Unknown format [%s]\", input.getClass());\n                                 }\n-                                return new SearchHit(dim, val, cnt);\n+                                return new SearchHit(dim, val, count);\n                               }\n                             }\n                         )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "sha": "8829184556554e53d0b36350f728a4aa81846081",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/select/SelectResultValueBuilder.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/select/SelectResultValueBuilder.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "processing/src/main/java/org/apache/druid/query/select/SelectResultValueBuilder.java",
                "patch": "@@ -23,7 +23,6 @@\n import com.google.common.collect.Maps;\n import com.google.common.collect.MinMaxPriorityQueue;\n import com.google.common.primitives.Longs;\n-import org.apache.druid.java.util.common.guava.Comparators;\n import org.apache.druid.query.Result;\n import org.joda.time.DateTime;\n \n@@ -137,7 +136,7 @@ public MergeBuilder(DateTime timestamp, PagingSpec pagingSpec, boolean descendin\n     protected Queue<EventHolder> instantiatePQueue()\n     {\n       int threshold = pagingSpec.getThreshold();\n-      return MinMaxPriorityQueue.orderedBy(descending ? Comparators.inverse(comparator) : comparator)\n+      return MinMaxPriorityQueue.orderedBy(descending ? comparator.reversed() : comparator)\n                                 .maximumSize(threshold > 0 ? threshold : Integer.MAX_VALUE)\n                                 .create();\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/select/SelectResultValueBuilder.java",
                "sha": "5f2cac5b8f92bef0a1a2d865f249013bfa7c1aaa",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "patch": "@@ -84,7 +84,7 @@ public void run(\n \n     PooledTopNAlgorithm singleMetricAlgo = new PooledTopNAlgorithm(storageAdapter, singleMetricQuery, bufferPool);\n     PooledTopNAlgorithm.PooledTopNParams singleMetricParam = null;\n-    int[] dimValSelector = null;\n+    int[] dimValSelector;\n     try {\n       singleMetricParam = singleMetricAlgo.makeInitParams(params.getSelectorPlus(), params.getCursor());\n       singleMetricAlgo.run(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "sha": "270ec164086613a6271f750d3f18db2598988e77",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/topn/InvertedTopNMetricSpec.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/topn/InvertedTopNMetricSpec.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 18,
                "filename": "processing/src/main/java/org/apache/druid/query/topn/InvertedTopNMetricSpec.java",
                "patch": "@@ -21,7 +21,6 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n-import org.apache.druid.java.util.common.guava.Comparators;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.query.aggregation.PostAggregator;\n import org.apache.druid.query.dimension.DimensionSpec;\n@@ -67,23 +66,7 @@ public Comparator getComparator(\n       final List<PostAggregator> postAggregatorSpecs\n   )\n   {\n-    return Comparators.inverse(\n-        new Comparator()\n-        {\n-          @Override\n-          public int compare(Object o1, Object o2)\n-          {\n-            // nulls last\n-            if (o1 == null) {\n-              return 1;\n-            }\n-            if (o2 == null) {\n-              return -1;\n-            }\n-            return delegate.getComparator(aggregatorSpecs, postAggregatorSpecs).compare(o1, o2);\n-          }\n-        }\n-    );\n+    return Comparator.nullsFirst(delegate.getComparator(aggregatorSpecs, postAggregatorSpecs).reversed());\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/query/topn/InvertedTopNMetricSpec.java",
                "sha": "eac69d328375ebbbcb09c0f67e4062e8209c1426",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/IndexIO.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/segment/IndexIO.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 9,
                "filename": "processing/src/main/java/org/apache/druid/segment/IndexIO.java",
                "patch": "@@ -78,7 +78,6 @@\n import java.util.Map;\n import java.util.Objects;\n import java.util.Set;\n-import java.util.TreeSet;\n \n public class IndexIO\n {\n@@ -484,14 +483,6 @@ public QueryableIndex load(File inDir, ObjectMapper mapper) throws IOException\n         }\n       }\n \n-      Set<String> colSet = new TreeSet<>();\n-      for (String dimension : index.getAvailableDimensions()) {\n-        colSet.add(dimension);\n-      }\n-      for (String metric : index.getAvailableMetrics()) {\n-        colSet.add(metric);\n-      }\n-\n       columns.put(\n           ColumnHolder.TIME_COLUMN_NAME,\n           new ColumnBuilder()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/IndexIO.java",
                "sha": "2293991c5515170f741c4db5afc7d84e5970beae",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java",
                "patch": "@@ -47,7 +47,7 @@\n   private int numInserted = 0;\n   private int numInsertedForNextFlush;\n \n-  private ByteBuffer endBuffer = null;\n+  private ByteBuffer endBuffer;\n \n   BlockLayoutColumnarLongsSerializer(\n       SegmentWriteOutMedium segmentWriteOutMedium,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java",
                "sha": "c1851c73d38669ab02032082d2326e1066d35a7e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/filter/Filters.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/segment/filter/Filters.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java",
                "patch": "@@ -397,7 +397,7 @@ public IntIterator iterator()\n         {\n           private final int bitmapIndexCardinality = bitmapIndex.getCardinality();\n           private int nextIndex = 0;\n-          private int found = -1;\n+          private int found;\n \n           {\n             found = findNextIndex();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/filter/Filters.java",
                "sha": "a81eb60a8fea438ea73b22e23198c60d692f0e3d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/filter/LikeFilter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/segment/filter/LikeFilter.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/segment/filter/LikeFilter.java",
                "patch": "@@ -174,7 +174,7 @@ public IntIterator iterator()\n         return new IntIterator()\n         {\n           int currIndex = startIndex;\n-          int found = -1;\n+          int found;\n \n           {\n             found = findNext();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/main/java/org/apache/druid/segment/filter/LikeFilter.java",
                "sha": "018102a1833a62cbe5267e8fa17cd3537bde7b35",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/aggregation/JavaScriptAggregatorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/aggregation/JavaScriptAggregatorTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "processing/src/test/java/org/apache/druid/query/aggregation/JavaScriptAggregatorTest.java",
                "patch": "@@ -322,7 +322,7 @@ public static void main(String... args)\n \n     // warmup\n     int i = 0;\n-    long t = 0;\n+    long t;\n     while (i < 10000) {\n       aggregate(selector, aggRhino);\n       ++i;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/aggregation/JavaScriptAggregatorTest.java",
                "sha": "757001b0ed33e0deb3695f4e0ad895ce50a23d53",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/select/SelectBinaryFnTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/select/SelectBinaryFnTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "processing/src/test/java/org/apache/druid/query/select/SelectBinaryFnTest.java",
                "patch": "@@ -137,9 +137,6 @@ public void testApply()\n     Assert.assertEquals(res1.getTimestamp(), merged.getTimestamp());\n \n     LinkedHashMap<String, Integer> expectedPageIds = Maps.newLinkedHashMap();\n-    expectedPageIds.put(segmentId1, 0);\n-    expectedPageIds.put(segmentId2, 0);\n-    expectedPageIds.put(segmentId2, 1);\n     expectedPageIds.put(segmentId1, 1);\n     expectedPageIds.put(segmentId2, 2);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/select/SelectBinaryFnTest.java",
                "sha": "219452e3b99551e951c4b3e3bf9ab0e218e4ea98",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryRunnerTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryRunnerTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 8,
                "filename": "processing/src/test/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryRunnerTest.java",
                "patch": "@@ -24,7 +24,6 @@\n import com.google.common.io.CharSource;\n import org.apache.commons.lang.StringUtils;\n import org.apache.druid.java.util.common.DateTimes;\n-import org.apache.druid.java.util.common.Intervals;\n import org.apache.druid.java.util.common.granularity.Granularities;\n import org.apache.druid.query.Druids;\n import org.apache.druid.query.QueryPlus;\n@@ -40,7 +39,6 @@\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n import org.apache.druid.timeline.DataSegment;\n-import org.apache.druid.timeline.TimelineObjectHolder;\n import org.apache.druid.timeline.VersionedIntervalTimeline;\n import org.apache.druid.timeline.partition.NoneShardSpec;\n import org.apache.druid.timeline.partition.SingleElementPartitionChunk;\n@@ -80,7 +78,6 @@\n   );\n   private static Segment segment0;\n   private static Segment segment1;\n-  private static List<String> segmentIdentifiers;\n \n   public TimeBoundaryQueryRunnerTest(\n       QueryRunner runner\n@@ -157,11 +154,6 @@ private QueryRunner getCustomRunner() throws IOException\n     timeline.add(index0.getInterval(), \"v1\", new SingleElementPartitionChunk(segment0));\n     timeline.add(index1.getInterval(), \"v1\", new SingleElementPartitionChunk(segment1));\n \n-    segmentIdentifiers = new ArrayList<>();\n-    for (TimelineObjectHolder<String, ?> holder : timeline.lookup(Intervals.of(\"2011-01-12/2011-01-17\"))) {\n-      segmentIdentifiers.add(makeIdentifier(holder.getInterval(), holder.getVersion()));\n-    }\n-\n     return QueryRunnerTestHelper.makeFilteringQueryRunner(timeline, factory);\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryRunnerTest.java",
                "sha": "6527f3ceefdfb3de5d4231d2c5a6805c6f26a765",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/segment/SchemalessTestSimpleTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/segment/SchemalessTestSimpleTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/segment/SchemalessTestSimpleTest.java",
                "patch": "@@ -57,6 +57,7 @@\n import org.apache.druid.query.topn.TopNResultValue;\n import org.apache.druid.segment.incremental.IncrementalIndex;\n import org.apache.druid.segment.writeout.SegmentWriteOutMediumFactory;\n+import org.junit.Ignore;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -167,6 +168,7 @@ public void testFullOnTimeseries()\n \n   //  @Test TODO: Handling of null values is inconsistent right now, need to make it all consistent and re-enable test\n   // TODO: Complain to Eric when you see this.  It shouldn't be like this...\n+  @Ignore\n   @SuppressWarnings(\"unused\")\n   public void testFullOnTopN()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/processing/src/test/java/org/apache/druid/segment/SchemalessTestSimpleTest.java",
                "sha": "a6a2330774ee83373227b2f1c382faf01a247323",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 15,
                "filename": "server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java",
                "patch": "@@ -137,7 +137,6 @@ public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedExcepti\n     checkNotNull(e);\n     checkSize(e);\n     long nanos = unit.toNanos(timeout);\n-    boolean added = false;\n     putLock.lockInterruptibly();\n     try {\n       while (currentSize.get() + getBytesSize(e) > capacity) {\n@@ -148,16 +147,12 @@ public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedExcepti\n       }\n       delegate.add(e);\n       elementAdded(e);\n-      added = true;\n     }\n     finally {\n       putLock.unlock();\n     }\n-    if (added) {\n-      signalNotEmpty();\n-    }\n-    return added;\n-\n+    signalNotEmpty();\n+    return true;\n   }\n \n   @Override\n@@ -222,12 +217,12 @@ public int drainTo(Collection<? super E> c, int maxElements)\n     if (c == this) {\n       throw new IllegalArgumentException();\n     }\n-    int n = 0;\n+    int n;\n     takeLock.lock();\n     try {\n       // elementCount.get provides visibility to first n Nodes\n       n = Math.min(maxElements, elementCount.get());\n-      if (n < 0) {\n+      if (n <= 0) {\n         return 0;\n       }\n       for (int i = 0; i < n; i++) {\n@@ -239,9 +234,7 @@ public int drainTo(Collection<? super E> c, int maxElements)\n     finally {\n       takeLock.unlock();\n     }\n-    if (n > 0) {\n-      signalNotFull();\n-    }\n+    signalNotFull();\n     return n;\n   }\n \n@@ -250,7 +243,7 @@ public boolean offer(E e)\n   {\n     checkNotNull(e);\n     checkSize(e);\n-    boolean added = false;\n+    boolean added;\n     putLock.lock();\n     try {\n       if (currentSize.get() + getBytesSize(e) > capacity) {\n@@ -274,7 +267,7 @@ public boolean offer(E e)\n   @Override\n   public E poll()\n   {\n-    E e = null;\n+    E e;\n     takeLock.lock();\n     try {\n       e = delegate.poll();\n@@ -295,7 +288,7 @@ public E poll()\n   public E poll(long timeout, TimeUnit unit) throws InterruptedException\n   {\n     long nanos = unit.toNanos(timeout);\n-    E e = null;\n+    E e;\n     takeLock.lockInterruptibly();\n     try {\n       while (elementCount.get() == 0) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java",
                "sha": "25a2da4c8995dbda54061c672147d8d1ca985b6e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java",
                "patch": "@@ -639,7 +639,7 @@ protected BasicDataSource getDatasource()\n   {\n     MetadataStorageConnectorConfig connectorConfig = getConfig();\n \n-    BasicDataSource dataSource = null;\n+    BasicDataSource dataSource;\n \n     try {\n       Properties dbcpProperties = connectorConfig.getDbcpProperties();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java",
                "sha": "f7243751a9fb47d66230c51feaccbbbc1bee7938",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java",
                "patch": "@@ -242,7 +242,7 @@ public Response addAll(\n       }\n \n       CountingInputStream countingInputStream = new CountingInputStream(in);\n-      Collection<Map<String, Object>> events = null;\n+      Collection<Map<String, Object>> events;\n       try {\n         events = objectMapper.readValue(\n             countingInputStream,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java",
                "sha": "94e2c6cd50b5f1ac090598dba83c634dd9670c9e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/coordinator/ReplicationThrottler.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/ReplicationThrottler.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/ReplicationThrottler.java",
                "patch": "@@ -154,7 +154,7 @@ public void reduceLifetime(String tier)\n         lifetime = maxLifetime;\n         lifetimes.put(tier, lifetime);\n       }\n-      lifetimes.put(tier, --lifetime);\n+      lifetimes.put(tier, lifetime - 1);\n     }\n \n     public void resetLifetime(String tier)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/coordinator/ReplicationThrottler.java",
                "sha": "a08a68c76d0218b412784830d7122c60f94fcd2e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/coordinator/SegmentReplicantLookup.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/SegmentReplicantLookup.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/SegmentReplicantLookup.java",
                "patch": "@@ -47,7 +47,7 @@ public static SegmentReplicantLookup make(DruidCluster cluster)\n           if (numReplicants == null) {\n             numReplicants = 0;\n           }\n-          segmentsInCluster.put(segment.getIdentifier(), server.getTier(), ++numReplicants);\n+          segmentsInCluster.put(segment.getIdentifier(), server.getTier(), numReplicants + 1);\n         }\n \n         // Also account for queued segments\n@@ -56,7 +56,7 @@ public static SegmentReplicantLookup make(DruidCluster cluster)\n           if (numReplicants == null) {\n             numReplicants = 0;\n           }\n-          loadingSegments.put(segment.getIdentifier(), server.getTier(), ++numReplicants);\n+          loadingSegments.put(segment.getIdentifier(), server.getTier(), numReplicants + 1);\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/coordinator/SegmentReplicantLookup.java",
                "sha": "6c122870df429e5b3d9b17c4fa1a0c15882137a3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/DatasourcesResource.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/DatasourcesResource.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "server/src/main/java/org/apache/druid/server/http/DatasourcesResource.java",
                "patch": "@@ -276,7 +276,7 @@ public Response getSegmentDataSourceIntervals(\n       return Response.noContent().build();\n     }\n \n-    final Comparator<Interval> comparator = Comparators.inverse(Comparators.intervalsByStartThenEnd());\n+    final Comparator<Interval> comparator = Comparators.intervalsByStartThenEnd().reversed();\n \n     if (full != null) {\n       final Map<Interval, Map<String, Object>> retVal = new TreeMap<>(comparator);\n@@ -342,7 +342,7 @@ public Response getSegmentDataSourceSpecificInterval(\n       return Response.noContent().build();\n     }\n \n-    final Comparator<Interval> comparator = Comparators.inverse(Comparators.intervalsByStartThenEnd());\n+    final Comparator<Interval> comparator = Comparators.intervalsByStartThenEnd().reversed();\n     if (full != null) {\n       final Map<Interval, Map<String, Object>> retVal = new TreeMap<>(comparator);\n       for (DataSegment dataSegment : dataSource.getSegments()) {\n@@ -385,7 +385,7 @@ public Response getSegmentDataSourceSpecificInterval(\n       return Response.ok(retVal).build();\n     }\n \n-    final Set<String> retVal = new TreeSet<>(Comparators.inverse(String.CASE_INSENSITIVE_ORDER));\n+    final Set<String> retVal = new TreeSet<>(String.CASE_INSENSITIVE_ORDER.reversed());\n     for (DataSegment dataSegment : dataSource.getSegments()) {\n       if (theInterval.contains(dataSegment.getInterval())) {\n         retVal.add(dataSegment.getIdentifier());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/DatasourcesResource.java",
                "sha": "77ca931eae19c53a6e0652162601ac58e06681c8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/IntervalsResource.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/IntervalsResource.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 12,
                "filename": "server/src/main/java/org/apache/druid/server/http/IntervalsResource.java",
                "patch": "@@ -70,7 +70,7 @@ public IntervalsResource(\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getIntervals(@Context final HttpServletRequest req)\n   {\n-    final Comparator<Interval> comparator = Comparators.inverse(Comparators.intervalsByStartThenEnd());\n+    final Comparator<Interval> comparator = Comparators.intervalsByStartThenEnd().reversed();\n     final Set<ImmutableDruidDataSource> datasources = InventoryViewUtils.getSecuredDataSources(\n         req,\n         serverInventoryView,\n@@ -80,11 +80,7 @@ public Response getIntervals(@Context final HttpServletRequest req)\n     final Map<Interval, Map<String, Map<String, Object>>> retVal = new TreeMap<>(comparator);\n     for (ImmutableDruidDataSource dataSource : datasources) {\n       for (DataSegment dataSegment : dataSource.getSegments()) {\n-        Map<String, Map<String, Object>> interval = retVal.get(dataSegment.getInterval());\n-        if (interval == null) {\n-          Map<String, Map<String, Object>> tmp = new HashMap<>();\n-          retVal.put(dataSegment.getInterval(), tmp);\n-        }\n+        retVal.computeIfAbsent(dataSegment.getInterval(), i -> new HashMap<>());\n         setProperties(retVal, dataSource, dataSegment);\n       }\n     }\n@@ -109,18 +105,14 @@ public Response getSpecificIntervals(\n         authorizerMapper\n     );\n \n-    final Comparator<Interval> comparator = Comparators.inverse(Comparators.intervalsByStartThenEnd());\n+    final Comparator<Interval> comparator = Comparators.intervalsByStartThenEnd().reversed();\n \n     if (full != null) {\n       final Map<Interval, Map<String, Map<String, Object>>> retVal = new TreeMap<>(comparator);\n       for (ImmutableDruidDataSource dataSource : datasources) {\n         for (DataSegment dataSegment : dataSource.getSegments()) {\n           if (theInterval.contains(dataSegment.getInterval())) {\n-            Map<String, Map<String, Object>> dataSourceInterval = retVal.get(dataSegment.getInterval());\n-            if (dataSourceInterval == null) {\n-              Map<String, Map<String, Object>> tmp = new HashMap<>();\n-              retVal.put(dataSegment.getInterval(), tmp);\n-            }\n+            retVal.computeIfAbsent(dataSegment.getInterval(), k -> new HashMap<>());\n             setProperties(retVal, dataSource, dataSegment);\n           }\n         }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/IntervalsResource.java",
                "sha": "56357944c5fd897d7247722c5e8b5f511f7ee38d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java",
                "patch": "@@ -482,7 +482,7 @@ public Response getAllNodesStatus(\n   )\n   {\n     try {\n-      Collection<String> tiers = null;\n+      Collection<String> tiers;\n       if (discover) {\n         tiers = lookupCoordinatorManager.discoverTiers();\n       } else {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java",
                "sha": "90e56941ce7dd9178003bc9d3f8677f721e62aed",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerInitUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerInitUtils.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerInitUtils.java",
                "patch": "@@ -57,7 +57,7 @@ public static void addExtensionFilters(ServletContextHandler handler, Injector i\n     for (ServletFilterHolder servletFilterHolder : extensionFilters) {\n       // Check the Filter first to guard against people who don't read the docs and return the Class even\n       // when they have an instance.\n-      FilterHolder holder = null;\n+      FilterHolder holder;\n       if (servletFilterHolder.getFilter() != null) {\n         holder = new FilterHolder(servletFilterHolder.getFilter());\n       } else if (servletFilterHolder.getFilterClass() != null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerInitUtils.java",
                "sha": "0b0234324a89cc5f414ca566de8d8f9ac7ebc6b9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 4,
                "filename": "server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java",
                "patch": "@@ -24,7 +24,6 @@\n import org.apache.druid.java.util.emitter.EmittingLogger;\n import org.apache.druid.query.QueryInterruptedException;\n import org.apache.druid.server.DruidNode;\n-import org.eclipse.jetty.server.Response;\n \n import javax.servlet.Filter;\n import javax.servlet.FilterChain;\n@@ -94,7 +93,7 @@ public void doFilter(ServletRequest servletRequest, ServletResponse servletRespo\n       );\n     }\n \n-    if (authInfoChecked != null && !authInfoChecked && response.getStatus() != Response.SC_FORBIDDEN) {\n+    if (authInfoChecked != null && !authInfoChecked && response.getStatus() != HttpServletResponse.SC_FORBIDDEN) {\n       handleAuthorizationCheckError(\n           \"Request's authorization check failed but status code was not 403.\",\n           request,\n@@ -134,7 +133,7 @@ private void handleUnauthenticatedRequest(\n     );\n     unauthorizedError.setStackTrace(new StackTraceElement[0]);\n     OutputStream out = response.getOutputStream();\n-    sendJsonError(response, Response.SC_UNAUTHORIZED, jsonMapper.writeValueAsString(unauthorizedError), out);\n+    sendJsonError(response, HttpServletResponse.SC_UNAUTHORIZED, jsonMapper.writeValueAsString(unauthorizedError), out);\n     out.close();\n     return;\n   }\n@@ -157,7 +156,7 @@ private void handleAuthorizationCheckError(\n       throw new ISE(errorMsg);\n     } else {\n       try {\n-        servletResponse.sendError(Response.SC_FORBIDDEN);\n+        servletResponse.sendError(HttpServletResponse.SC_FORBIDDEN);\n       }\n       catch (Exception e) {\n         throw new RuntimeException(e);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java",
                "sha": "9b72b19e36e7225868b20e6c995f34bdeab9227f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java",
                "patch": "@@ -23,7 +23,6 @@\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.query.QueryInterruptedException;\n import org.apache.druid.server.DruidNode;\n-import org.eclipse.jetty.server.Response;\n \n import javax.servlet.Filter;\n import javax.servlet.FilterChain;\n@@ -79,7 +78,7 @@ public void doFilter(ServletRequest request, ServletResponse response, FilterCha\n \n     AuthenticationResult result = (AuthenticationResult) request.getAttribute(AuthConfig.DRUID_AUTHENTICATION_RESULT);\n     if (authInfoChecked != null || result != null || allowUnsecured != null) {\n-      sendJsonError(httpResponse, Response.SC_FORBIDDEN, unauthorizedMessage, out);\n+      sendJsonError(httpResponse, HttpServletResponse.SC_FORBIDDEN, unauthorizedMessage, out);\n       out.close();\n       return;\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java",
                "sha": "f222e5a8a6fc44566314d5e643c2b193e969ffbd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "patch": "@@ -2875,6 +2875,9 @@ public int compareTo(DataSegment dataSegment)\n       @Override\n       public boolean equals(Object o)\n       {\n+        if (!(o instanceof DataSegment)) {\n+          return false;\n+        }\n         return baseSegment.equals(o);\n       }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "sha": "f65dc3070d8c89b609b8f0586dc131c42c3916e6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/BytesBoundedLinkedQueueTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/cache/BytesBoundedLinkedQueueTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 3,
                "filename": "server/src/test/java/org/apache/druid/client/cache/BytesBoundedLinkedQueueTest.java",
                "patch": "@@ -183,7 +183,7 @@ public void testAddedObjectExceedsCapacity() throws Exception\n     Assert.assertFalse(q.offer(new TestObject(2), delayMS, TimeUnit.MILLISECONDS));\n   }\n \n- // @Test\n+  @Test\n   public void testConcurrentOperations() throws Exception\n   {\n     final BlockingQueue<TestObject> q = getQueue(Integer.MAX_VALUE);\n@@ -240,8 +240,7 @@ public Boolean call() throws InterruptedException\n                 public Boolean call()\n                 {\n                   while (!stopTest.get()) {\n-                    System.out\n-                          .println(\"drained elements : \" + q.drainTo(new ArrayList<TestObject>(), Integer.MAX_VALUE));\n+                    q.drainTo(new ArrayList<>(), Integer.MAX_VALUE);\n                   }\n                   return true;\n                 }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/BytesBoundedLinkedQueueTest.java",
                "sha": "b01ad799311777a65555b6babe384c3770ee5f0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/HybridCacheTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/cache/HybridCacheTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "server/src/test/java/org/apache/druid/client/cache/HybridCacheTest.java",
                "patch": "@@ -172,8 +172,8 @@ public void testSanity()\n       Assert.assertEquals(Sets.newHashSet(key3), res.keySet());\n       Assert.assertArrayEquals(value3, res.get(key3));\n \n-      Assert.assertEquals(++hits, cache.getStats().getNumHits());\n-      Assert.assertEquals(++misses, cache.getStats().getNumMisses());\n+      Assert.assertEquals(hits + 1, cache.getStats().getNumHits());\n+      Assert.assertEquals(misses + 1, cache.getStats().getNumMisses());\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/HybridCacheTest.java",
                "sha": "b5d96ebe2c22f27c7beaeb5db139bda4a1564427",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java",
                "patch": "@@ -131,7 +131,7 @@ public void timePutObjects(int reps)\n \n   public long timeGetObject(int reps)\n   {\n-    byte[] bytes = null;\n+    byte[] bytes;\n     long count = 0;\n     for (int i = 0; i < reps; i++) {\n       for (int k = 0; k < objectCount; ++k) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java",
                "sha": "fda0b64c41497db1eb495d22b8f3c334868ff3a2",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumberTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumberTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 22,
                "filename": "server/src/test/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumberTest.java",
                "patch": "@@ -30,9 +30,6 @@\n import org.junit.Assert;\n import org.junit.Test;\n \n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n-\n public class AppenderatorPlumberTest\n {\n   private final AppenderatorPlumber plumber;\n@@ -94,43 +91,29 @@ public AppenderatorPlumberTest() throws Exception\n   @Test\n   public void testSimpleIngestion() throws Exception\n   {\n-\n-    final ConcurrentMap<String, String> commitMetadata = new ConcurrentHashMap<>();    \n-    \n     Appenderator appenderator = appenderatorTester.getAppenderator();\n \n     // startJob\n     Assert.assertEquals(null, plumber.startJob());\n \n     // getDataSource\n-    Assert.assertEquals(AppenderatorTester.DATASOURCE,\n-        appenderator.getDataSource());\n+    Assert.assertEquals(AppenderatorTester.DATASOURCE, appenderator.getDataSource());\n \n     InputRow[] rows = new InputRow[] {AppenderatorTest.IR(\"2000\", \"foo\", 1), \n         AppenderatorTest.IR(\"2000\", \"bar\", 2), AppenderatorTest.IR(\"2000\", \"qux\", 4)};\n     // add\n-    commitMetadata.put(\"x\", \"1\");\n-    Assert.assertEquals(\n-        1,\n-        plumber.add(rows[0], null).getRowCount());\n+    Assert.assertEquals(1, plumber.add(rows[0], null).getRowCount());\n \n-    commitMetadata.put(\"x\", \"2\");\n-    Assert.assertEquals(\n-        2,\n-        plumber.add(rows[1], null).getRowCount());\n+    Assert.assertEquals(2, plumber.add(rows[1], null).getRowCount());\n \n-    commitMetadata.put(\"x\", \"3\");\n-    Assert.assertEquals(\n-        3,\n-        plumber.add(rows[2], null).getRowCount());\n+    Assert.assertEquals(3, plumber.add(rows[2], null).getRowCount());\n \n     \n     Assert.assertEquals(1, plumber.getSegmentsView().size());\n     \n     SegmentIdentifier si = plumber.getSegmentsView().values().toArray(new SegmentIdentifier[0])[0];\n     \n-    Assert.assertEquals(3,\n-        appenderator.getRowCount(si));\n+    Assert.assertEquals(3, appenderator.getRowCount(si));\n \n     appenderator.clear();    \n     Assert.assertTrue(appenderator.getSegments().isEmpty());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumberTest.java",
                "sha": "b5f37a6cca3ce7d25f213301d7a40a476457ad98",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "patch": "@@ -307,14 +307,13 @@ public void testMoveSegment() throws Exception\n     DataSegment segmentToMove = sourceSegments.get(2);\n \n     List<String> sourceSegKeys = new ArrayList<>();\n-    List<String> destSegKeys = new ArrayList<>();\n \n     for (DataSegment segment : sourceSegments) {\n       sourceSegKeys.add(announceBatchSegmentsForServer(source, ImmutableSet.of(segment), zkPathsConfig, jsonMapper));\n     }\n \n     for (DataSegment segment : destinationSegments) {\n-      destSegKeys.add(announceBatchSegmentsForServer(dest, ImmutableSet.of(segment), zkPathsConfig, jsonMapper));\n+      announceBatchSegmentsForServer(dest, ImmutableSet.of(segment), zkPathsConfig, jsonMapper);\n     }\n \n     Assert.assertTrue(timing.forWaiting().awaitLatch(segmentViewInitLatch));",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "sha": "2b4a8c5c75edc7a00b8d7427d20bd6210c0d2357",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/server/router/JavaScriptTieredBrokerSelectorStrategyTest.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/router/JavaScriptTieredBrokerSelectorStrategyTest.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/router/JavaScriptTieredBrokerSelectorStrategyTest.java",
                "patch": "@@ -94,7 +94,6 @@ public void testDisabled() throws Exception\n   public void testGetBrokerServiceName()\n   {\n     final LinkedHashMap<String, String> tierBrokerMap = new LinkedHashMap<>();\n-    tierBrokerMap.put(\"fast\", \"druid/fastBroker\");\n     tierBrokerMap.put(\"fast\", \"druid/broker\");\n     tierBrokerMap.put(\"slow\", \"druid/slowBroker\");\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/server/src/test/java/org/apache/druid/server/router/JavaScriptTieredBrokerSelectorStrategyTest.java",
                "sha": "277f56d6d835ea50e24e60e5171226eeeb9b7e7e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/CoordinatorJettyServerInitializer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CoordinatorJettyServerInitializer.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "services/src/main/java/org/apache/druid/cli/CoordinatorJettyServerInitializer.java",
                "patch": "@@ -117,7 +117,6 @@ public void initialize(Server server, Injector injector)\n     final ObjectMapper jsonMapper = injector.getInstance(Key.get(ObjectMapper.class, Json.class));\n     final AuthenticatorMapper authenticatorMapper = injector.getInstance(AuthenticatorMapper.class);\n \n-    List<Authenticator> authenticators = null;\n     AuthenticationUtils.addSecuritySanityCheckFilter(root, jsonMapper);\n \n     // perform no-op authorization for these resources\n@@ -128,7 +127,7 @@ public void initialize(Server server, Injector injector)\n       AuthenticationUtils.addNoopAuthorizationFilters(root, CliOverlord.UNSECURED_PATHS);\n     }\n \n-    authenticators = authenticatorMapper.getAuthenticatorChain();\n+    List<Authenticator> authenticators = authenticatorMapper.getAuthenticatorChain();\n     AuthenticationUtils.addAuthenticationFilterChain(root, authenticators);\n \n     AuthenticationUtils.addAllowOptionsFilter(root, authConfig.isAllowUnauthenticatedHttpOptions());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/CoordinatorJettyServerInitializer.java",
                "sha": "64deddd89cec3d7b735a734e704cfe1dcf5ffc56",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/PullDependencies.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/PullDependencies.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 0,
                "filename": "services/src/main/java/org/apache/druid/cli/PullDependencies.java",
                "patch": "@@ -73,6 +73,7 @@\n {\n   private static final Logger log = new Logger(PullDependencies.class);\n \n+  @SuppressWarnings(\"MismatchedQueryAndUpdateOfCollection\")\n   private static final Set<String> exclusions = new HashSet<>(\n       /*\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/PullDependencies.java",
                "sha": "885e100104ae0d686a348bad8050e2e9b1553329",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/QueryJettyServerInitializer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/QueryJettyServerInitializer.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 2,
                "filename": "services/src/main/java/org/apache/druid/cli/QueryJettyServerInitializer.java",
                "patch": "@@ -93,14 +93,13 @@ public void initialize(Server server, Injector injector)\n     final ObjectMapper jsonMapper = injector.getInstance(Key.get(ObjectMapper.class, Json.class));\n     final AuthenticatorMapper authenticatorMapper = injector.getInstance(AuthenticatorMapper.class);\n \n-    List<Authenticator> authenticators = null;\n     AuthenticationUtils.addSecuritySanityCheckFilter(root, jsonMapper);\n \n     // perform no-op authorization for these resources\n     AuthenticationUtils.addNoopAuthorizationFilters(root, UNSECURED_PATHS);\n     AuthenticationUtils.addNoopAuthorizationFilters(root, authConfig.getUnsecuredPaths());\n \n-    authenticators = authenticatorMapper.getAuthenticatorChain();\n+    List<Authenticator> authenticators = authenticatorMapper.getAuthenticatorChain();\n     AuthenticationUtils.addAuthenticationFilterChain(root, authenticators);\n \n     AuthenticationUtils.addAllowOptionsFilter(root, authConfig.isAllowUnauthenticatedHttpOptions());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/services/src/main/java/org/apache/druid/cli/QueryJettyServerInitializer.java",
                "sha": "2c9260241bc043fec3aca93e84fc9d3de14919d9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/sql/src/main/java/org/apache/druid/sql/avatica/DruidConnection.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/avatica/DruidConnection.java?ref=54351a5c75d8902d1d4c2eb2f6590bcf126348d4",
                "deletions": 12,
                "filename": "sql/src/main/java/org/apache/druid/sql/avatica/DruidConnection.java",
                "patch": "@@ -20,7 +20,6 @@\n package org.apache.druid.sql.avatica;\n \n import com.google.common.base.Preconditions;\n-import com.google.common.base.Predicate;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSortedMap;\n@@ -29,7 +28,6 @@\n import org.apache.druid.java.util.common.ISE;\n import org.apache.druid.java.util.common.logger.Logger;\n \n-import javax.annotation.Nullable;\n import javax.annotation.concurrent.GuardedBy;\n import java.util.HashMap;\n import java.util.Map;\n@@ -85,17 +83,9 @@ public DruidStatement createStatement()\n \n       // remove sensitive fields from the context, only the connection's context needs to have authentication\n       // credentials\n-      Map<String, Object> sanitizedContext = new HashMap<>();\n-      sanitizedContext = Maps.filterEntries(\n+      Map<String, Object> sanitizedContext = Maps.filterEntries(\n           context,\n-          new Predicate<Map.Entry<String, Object>>()\n-          {\n-            @Override\n-            public boolean apply(@Nullable Map.Entry<String, Object> input)\n-            {\n-              return !SENSITIVE_CONTEXT_FIELDS.contains(input.getKey());\n-            }\n-          }\n+          e -> !SENSITIVE_CONTEXT_FIELDS.contains(e.getKey())\n       );\n \n       final DruidStatement statement = new DruidStatement(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/54351a5c75d8902d1d4c2eb2f6590bcf126348d4/sql/src/main/java/org/apache/druid/sql/avatica/DruidConnection.java",
                "sha": "fb93a698cbba360a9db571d8e6a13ee240d2f207",
                "status": "modified"
            }
        ],
        "message": "Fix various bugs; Enable more IntelliJ inspections and update error-prone (#6490)\n\n* Fix various bugs; Enable more IntelliJ inspections and update error-prone\r\n\r\n* Fix NPE\r\n\r\n* Fix inspections\r\n\r\n* Remove unused imports",
        "parent": "https://github.com/apache/incubator-druid/commit/bcb754d066cc8a140d4c67ae7d51a31ec8cc7554",
        "repo": "incubator-druid",
        "unit_tests": [
            "ComparatorsTest.java",
            "ImmutableConciseSetTest.java",
            "MaterializedViewSupervisorTest.java",
            "ArrayOfDoublesSketchToQuantilesSketchPostAggregatorTest.java",
            "ApproximateHistogramTest.java",
            "KafkaSupervisorTest.java",
            "ProtobufInputRowParserTest.java",
            "HyperLogLogCollectorTest.java",
            "HttpRemoteTaskRunnerTest.java",
            "SupervisorManagerTest.java",
            "WorkerTaskManagerTest.java",
            "GroupByQueryTest.java",
            "BufferArrayGrouperTest.java",
            "BufferHashGrouperTest.java",
            "LimitedBufferHashGrouperTest.java",
            "StringComparatorsTest.java",
            "ScanQueryLimitRowIteratorTest.java",
            "SearchQueryQueryToolChestTest.java",
            "IndexIOTest.java",
            "FiltersTest.java",
            "LikeFilterTest.java",
            "BytesBoundedLinkedQueueTest.java",
            "SQLMetadataConnectorTest.java",
            "IntervalsResourceTest.java",
            "LookupCoordinatorResourceTest.java",
            "PreResponseAuthorizationCheckFilterTest.java",
            "SecuritySanityCheckFilterTest.java",
            "PullDependenciesTest.java"
        ]
    },
    "incubator-druid_57ee691": {
        "bug_id": "incubator-druid_57ee691",
        "commit": "https://github.com/apache/incubator-druid/commit/57ee69111f33df272cc5c063d8035d15ee51db06",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/57ee69111f33df272cc5c063d8035d15ee51db06/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java?ref=57ee69111f33df272cc5c063d8035d15ee51db06",
                "deletions": 7,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "patch": "@@ -162,13 +162,16 @@ public SearchQueryRunner(Segment segment)\n             while (!cursor.isDone()) {\n               for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n                 final DimensionSelector selector = entry.getValue();\n-                final IndexedInts vals = selector.getRow();\n-                for (int i = 0; i < vals.size(); ++i) {\n-                  final String dimVal = selector.lookupName(vals.get(i));\n-                  if (searchQuerySpec.accept(dimVal)) {\n-                    set.add(new SearchHit(entry.getKey(), dimVal));\n-                    if (set.size() >= limit) {\n-                      return set;\n+\n+                if (selector != null) {\n+                  final IndexedInts vals = selector.getRow();\n+                  for (int i = 0; i < vals.size(); ++i) {\n+                    final String dimVal = selector.lookupName(vals.get(i));\n+                    if (searchQuerySpec.accept(dimVal)) {\n+                      set.add(new SearchHit(entry.getKey(), dimVal));\n+                      if (set.size() >= limit) {\n+                        return set;\n+                      }\n                     }\n                   }\n                 }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/57ee69111f33df272cc5c063d8035d15ee51db06/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "sha": "878151bfb64c461a266c7d6abfdd6cb7e748c86e",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/incubator-druid/blob/57ee69111f33df272cc5c063d8035d15ee51db06/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java?ref=57ee69111f33df272cc5c063d8035d15ee51db06",
                "deletions": 3,
                "filename": "processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "patch": "@@ -115,10 +115,10 @@ public void testSearchWithDimensionQuality()\n     Map<String, Set<String>> expectedResults = new HashMap<String, Set<String>>();\n     expectedResults.put(\n         QueryRunnerTestHelper.qualityDimension, new HashSet<String>(\n-        Arrays.asList(\n-            \"automotive\", \"mezzanine\", \"travel\", \"health\", \"entertainment\"\n+            Arrays.asList(\n+                \"automotive\", \"mezzanine\", \"travel\", \"health\", \"entertainment\"\n+            )\n         )\n-    )\n     );\n \n     checkSearchQuery(\n@@ -366,6 +366,24 @@ public void testSearchWithFilterEmptyResults()\n     );\n   }\n \n+\n+  @Test\n+  public void testSearchNonExistingDimension()\n+  {\n+    Map<String, Set<String>> expectedResults = Maps.newTreeMap(String.CASE_INSENSITIVE_ORDER);\n+\n+    checkSearchQuery(\n+        Druids.newSearchQueryBuilder()\n+              .dataSource(QueryRunnerTestHelper.dataSource)\n+              .granularity(QueryRunnerTestHelper.allGran)\n+              .intervals(QueryRunnerTestHelper.fullOnInterval)\n+              .dimensions(\"does_not_exist\")\n+              .query(\"a\")\n+              .build(),\n+        expectedResults\n+    );\n+  }\n+\n   private void checkSearchQuery(SearchQuery searchQuery, Map<String, Set<String>> expectedResults)\n   {\n     Iterable<Result<SearchResultValue>> results = Sequences.toList(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/57ee69111f33df272cc5c063d8035d15ee51db06/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "sha": "e5abe9b5d7fbfec5130b1f64e966ff54aa66b354",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #767 from metamx/fix-search\n\nFix NPE when searching  real-time with a non-existing dimension",
        "parent": "https://github.com/apache/incubator-druid/commit/e367f76824080eac457f26fb580214c7e35af744",
        "repo": "incubator-druid",
        "unit_tests": [
            "SearchQueryRunnerTest.java"
        ]
    },
    "incubator-druid_5998de7": {
        "bug_id": "incubator-druid_5998de7",
        "commit": "https://github.com/apache/incubator-druid/commit/5998de7d5b328fbdec86ec917daffb153e80f8e5",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5998de7d5b328fbdec86ec917daffb153e80f8e5/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java?ref=5998de7d5b328fbdec86ec917daffb153e80f8e5",
                "deletions": 3,
                "filename": "processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "patch": "@@ -295,8 +295,10 @@ public static SegmentAnalysis mergeAnalyses(\n       // Merge each aggregator individually, ignoring nulls\n       for (SegmentAnalysis analysis : ImmutableList.of(arg1, arg2)) {\n         if (analysis.getAggregators() != null) {\n-          for (AggregatorFactory aggregator : analysis.getAggregators().values()) {\n-            AggregatorFactory merged = aggregators.get(aggregator.getName());\n+          for (Map.Entry<String, AggregatorFactory> entry : analysis.getAggregators().entrySet()) {\n+            final String aggregatorName = entry.getKey();\n+            final AggregatorFactory aggregator = entry.getValue();\n+            AggregatorFactory merged = aggregators.get(aggregatorName);\n             if (merged != null) {\n               try {\n                 merged = merged.getMergingFactory(aggregator);\n@@ -307,7 +309,7 @@ public static SegmentAnalysis mergeAnalyses(\n             } else {\n               merged = aggregator;\n             }\n-            aggregators.put(aggregator.getName(), merged);\n+            aggregators.put(aggregatorName, merged);\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5998de7d5b328fbdec86ec917daffb153e80f8e5/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "sha": "80bf4d9aee3cb83fdd700c4b0794c799e8b974bc",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5998de7d5b328fbdec86ec917daffb153e80f8e5/processing/src/test/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChestTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChestTest.java?ref=5998de7d5b328fbdec86ec917daffb153e80f8e5",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChestTest.java",
                "patch": "@@ -244,6 +244,15 @@ public void testMergeAggregatorsConflict()\n     expectedLenient.put(\"baz\", new LongMaxAggregatorFactory(\"baz\", \"baz\"));\n     Assert.assertNull(mergeStrict(analysis1, analysis2).getAggregators());\n     Assert.assertEquals(expectedLenient, mergeLenient(analysis1, analysis2).getAggregators());\n+\n+    // Simulate multi-level merge\n+    Assert.assertEquals(\n+        expectedLenient,\n+        mergeLenient(\n+            mergeLenient(analysis1, analysis2),\n+            mergeLenient(analysis1, analysis2)\n+        ).getAggregators()\n+    );\n   }\n \n   private static SegmentAnalysis mergeStrict(SegmentAnalysis analysis1, SegmentAnalysis analysis2)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5998de7d5b328fbdec86ec917daffb153e80f8e5/processing/src/test/java/io/druid/query/metadata/SegmentMetadataQueryQueryToolChestTest.java",
                "sha": "e19657b92ce05f0bf0fa5b967d41db9b4f5a1637",
                "status": "modified"
            }
        ],
        "message": "Fix lenient merging of conflicting aggregators. (#3113)\n\nThis should have marked the conflicting aggregator as null, but instead it\r\nthrew an NPE for the entire query.",
        "parent": "https://github.com/apache/incubator-druid/commit/37c8a8f186344c238b96a8ba9c2a33da7524ac42",
        "repo": "incubator-druid",
        "unit_tests": [
            "SegmentMetadataQueryQueryToolChestTest.java"
        ]
    },
    "incubator-druid_5ecd909": {
        "bug_id": "incubator-druid_5ecd909",
        "commit": "https://github.com/apache/incubator-druid/commit/5ecd909f7c04527904cf3ec5b2575c3c6f69c210",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5ecd909f7c04527904cf3ec5b2575c3c6f69c210/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=5ecd909f7c04527904cf3ec5b2575c3c6f69c210",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -84,6 +84,11 @@ public ChainedExecutionQueryRunner(\n   {\n     final int priority = Integer.parseInt(query.getContextValue(\"priority\", \"0\"));\n \n+    if (Iterables.isEmpty(queryables)) {\n+      log.warn(\"No queryables found.\");\n+      return Sequences.empty();\n+    }\n+\n     return new BaseSequence<T, Iterator<T>>(\n         new BaseSequence.IteratorMaker<T, Iterator<T>>()\n         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5ecd909f7c04527904cf3ec5b2575c3c6f69c210/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "316c8d8675e6bbdf20ede3166551b892b55ec3d6",
                "status": "modified"
            }
        ],
        "message": "reduce NPEs in CQE",
        "parent": "https://github.com/apache/incubator-druid/commit/84233238b1845011d6097dd040093e12342f747e",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java"
        ]
    },
    "incubator-druid_5fd4b79": {
        "bug_id": "incubator-druid_5fd4b79",
        "commit": "https://github.com/apache/incubator-druid/commit/5fd4b79373c271267050b66fcd0088a8add9f609",
        "file": [
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5fd4b79373c271267050b66fcd0088a8add9f609/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java?ref=5fd4b79373c271267050b66fcd0088a8add9f609",
                "deletions": 29,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "patch": "@@ -325,11 +325,17 @@ public String getVersion(final Interval interval)\n       // Delay firehose connection to avoid claiming input resources while the plumber is starting up.\n       final FirehoseFactory firehoseFactory = spec.getIOConfig().getFirehoseFactory();\n       final boolean firehoseDrainableByClosing = isFirehoseDrainableByClosing(firehoseFactory);\n-      firehose = firehoseFactory.connect(spec.getDataSchema().getParser());\n-      committerSupplier = Committers.supplierFromFirehose(firehose);\n+\n+      // Skip connecting firehose if we've been stopped before we got started.\n+      synchronized (this) {\n+        if (!gracefullyStopped) {\n+          firehose = firehoseFactory.connect(spec.getDataSchema().getParser());\n+          committerSupplier = Committers.supplierFromFirehose(firehose);\n+        }\n+      }\n \n       // Time to read data!\n-      while ((!gracefullyStopped || firehoseDrainableByClosing) && firehose.hasMore()) {\n+      while (firehose != null && (!gracefullyStopped || firehoseDrainableByClosing) && firehose.hasMore()) {\n         final InputRow inputRow;\n \n         try {\n@@ -366,33 +372,35 @@ public String getVersion(final Interval interval)\n     finally {\n       if (normalExit) {\n         try {\n-          // Always want to persist.\n-          log.info(\"Persisting remaining data.\");\n-\n-          final Committer committer = committerSupplier.get();\n-          final CountDownLatch persistLatch = new CountDownLatch(1);\n-          plumber.persist(\n-              new Committer()\n-              {\n-                @Override\n-                public Object getMetadata()\n-                {\n-                  return committer.getMetadata();\n-                }\n-\n-                @Override\n-                public void run()\n+          // Persist if we had actually started.\n+          if (firehose != null) {\n+            log.info(\"Persisting remaining data.\");\n+\n+            final Committer committer = committerSupplier.get();\n+            final CountDownLatch persistLatch = new CountDownLatch(1);\n+            plumber.persist(\n+                new Committer()\n                 {\n-                  try {\n-                    committer.run();\n+                  @Override\n+                  public Object getMetadata()\n+                  {\n+                    return committer.getMetadata();\n                   }\n-                  finally {\n-                    persistLatch.countDown();\n+\n+                  @Override\n+                  public void run()\n+                  {\n+                    try {\n+                      committer.run();\n+                    }\n+                    finally {\n+                      persistLatch.countDown();\n+                    }\n                   }\n                 }\n-              }\n-          );\n-          persistLatch.await();\n+            );\n+            persistLatch.await();\n+          }\n \n           if (gracefullyStopped) {\n             log.info(\"Gracefully stopping.\");\n@@ -420,8 +428,9 @@ public void run()\n           throw e;\n         }\n         finally {\n-          // firehose will be non-null since normalExit is true\n-          CloseQuietly.close(firehose);\n+          if (firehose != null) {\n+            CloseQuietly.close(firehose);\n+          }\n           toolbox.getMonitorScheduler().removeMonitor(metricsMonitor);\n         }\n       }\n@@ -444,7 +453,9 @@ public void stopGracefully()\n       synchronized (this) {\n         if (!gracefullyStopped) {\n           gracefullyStopped = true;\n-          if (finishingJob) {\n+          if (firehose == null) {\n+            log.info(\"stopGracefully: Firehose not started yet, so nothing to stop.\");\n+          } else if (finishingJob) {\n             log.info(\"stopGracefully: Interrupting finishJob.\");\n             runThread.interrupt();\n           } else if (isFirehoseDrainableByClosing(spec.getIOConfig().getFirehoseFactory())) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5fd4b79373c271267050b66fcd0088a8add9f609/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "sha": "24ab73adb32910a2e924e1140523bbee7436f9fc",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/5fd4b79373c271267050b66fcd0088a8add9f609/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java?ref=5fd4b79373c271267050b66fcd0088a8add9f609",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "patch": "@@ -536,6 +536,22 @@ public void testRestoreCorruptData() throws Exception\n     }\n   }\n \n+  @Test(timeout = 10000L)\n+  public void testStopBeforeStarting() throws Exception\n+  {\n+    final File directory = tempFolder.newFolder();\n+    final RealtimeIndexTask task1 = makeRealtimeTask(null);\n+\n+    task1.stopGracefully();\n+    final TestIndexerMetadataStorageCoordinator mdc = new TestIndexerMetadataStorageCoordinator();\n+    final TaskToolbox taskToolbox = makeToolbox(task1, mdc, directory);\n+    final ListenableFuture<TaskStatus> statusFuture = runTask(task1, taskToolbox);\n+\n+    // Wait for the task to finish.\n+    final TaskStatus taskStatus = statusFuture.get();\n+    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+  }\n+\n   private ListenableFuture<TaskStatus> runTask(final Task task, final TaskToolbox toolbox)\n   {\n     return taskExec.submit(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/5fd4b79373c271267050b66fcd0088a8add9f609/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "sha": "37914a8df42a530f363346439778effc36950d97",
                "status": "modified"
            }
        ],
        "message": "RealtimeIndexTask: Fix NPE caused by calling stopGracefully before a firehose had been connected.",
        "parent": "https://github.com/apache/incubator-druid/commit/c4fde52160773dca18bad002589957880b0aeeb3",
        "repo": "incubator-druid",
        "unit_tests": [
            "RealtimeIndexTaskTest.java"
        ]
    },
    "incubator-druid_6171e07": {
        "bug_id": "incubator-druid_6171e07",
        "commit": "https://github.com/apache/incubator-druid/commit/6171e078c8fdd2250e331f1c225db2b4e3be8809",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6171e078c8fdd2250e331f1c225db2b4e3be8809/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java?ref=6171e078c8fdd2250e331f1c225db2b4e3be8809",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "patch": "@@ -26,10 +26,10 @@\n import com.google.common.base.Strings;\n import com.metamx.common.StringUtils;\n import io.druid.query.extraction.ExtractionFn;\n+import io.druid.query.filter.DimFilterCacheHelper;\n import io.druid.query.lookup.LookupExtractionFn;\n import io.druid.query.lookup.LookupExtractor;\n import io.druid.query.lookup.LookupReferencesManager;\n-import io.druid.query.filter.DimFilterCacheHelper;\n import io.druid.segment.DimensionSelector;\n \n import javax.annotation.Nullable;\n@@ -129,18 +129,18 @@ public ExtractionFn getExtractionFn()\n     final LookupExtractor lookupExtractor = Strings.isNullOrEmpty(name)\n                                             ? this.lookup\n                                             : Preconditions.checkNotNull(\n-                                                this.lookupReferencesManager.get(name).get(),\n-                                                \"can not find lookup with name [%s]\",\n+                                                lookupReferencesManager.get(name),\n+                                                \"Lookup [%s] not found\",\n                                                 name\n-                                            );\n+                                            ).get();\n+\n     return new LookupExtractionFn(\n         lookupExtractor,\n         retainMissingValue,\n         replaceMissingValueWith,\n         lookupExtractor.isOneToOne(),\n         optimize\n     );\n-\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6171e078c8fdd2250e331f1c225db2b4e3be8809/processing/src/main/java/io/druid/query/dimension/LookupDimensionSpec.java",
                "sha": "06d9eb91c8c6222ae9d3a3b0d3a2037b87668a97",
                "status": "modified"
            }
        ],
        "message": "Improve NPE message in LookupDimensionSpec when lookup does not exist. (#3065)\n\nThe message used to be empty, which made things hard to debug.",
        "parent": "https://github.com/apache/incubator-druid/commit/603fbbcc2009ba1e1cff66669df266a525d2aeb7",
        "repo": "incubator-druid",
        "unit_tests": [
            "LookupDimensionSpecTest.java"
        ]
    },
    "incubator-druid_65c71a4": {
        "bug_id": "incubator-druid_65c71a4",
        "commit": "https://github.com/apache/incubator-druid/commit/65c71a40bb11a3273deec638dd1077bf816bea17",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/65c71a40bb11a3273deec638dd1077bf816bea17/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryQueryToolChest.java?ref=65c71a40bb11a3273deec638dd1077bf816bea17",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "patch": "@@ -265,8 +265,12 @@ public Object apply(@Nullable final Result<TimeseriesResultValue> input)\n         final Map<String, Object> values = Maps.newHashMap();\n         final TimeseriesResultValue holder = result.getValue();\n         if (calculatePostAggs) {\n+          // put non finalized aggregators for calculating dependent post Aggregators\n+          for (AggregatorFactory agg : query.getAggregatorSpecs()) {\n+            values.put(agg.getName(), holder.getMetric(agg.getName()));\n+          }\n           for (PostAggregator postAgg : query.getPostAggregatorSpecs()) {\n-            values.put(postAgg.getName(), postAgg.compute(holder.getBaseObject()));\n+            values.put(postAgg.getName(), postAgg.compute(values));\n           }\n         }\n         for (AggregatorFactory agg : query.getAggregatorSpecs()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/65c71a40bb11a3273deec638dd1077bf816bea17/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "sha": "482e92adc3e5f49f22dacf2e8ee560c5251234dd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/65c71a40bb11a3273deec638dd1077bf816bea17/processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java?ref=65c71a40bb11a3273deec638dd1077bf816bea17",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java",
                "patch": "@@ -208,13 +208,17 @@ public TopNQueryQueryToolChest(\n                   public Map<String, Object> apply(DimensionAndMetricValueExtractor input)\n                   {\n                     final Map<String, Object> values = Maps.newHashMap();\n-                    // compute all post aggs\n+                    // put non finalized aggregators for calculating dependent post Aggregators\n+                    for (AggregatorFactory agg : query.getAggregatorSpecs()) {\n+                      values.put(agg.getName(), fn.manipulate(agg, input.getMetric(agg.getName())));\n+                    }\n+\n                     for (PostAggregator postAgg : query.getPostAggregatorSpecs()) {\n                       Object calculatedPostAgg = input.getMetric(postAgg.getName());\n                       if (calculatedPostAgg != null) {\n                         values.put(postAgg.getName(), calculatedPostAgg);\n                       } else {\n-                        values.put(postAgg.getName(), postAgg.compute(input.getBaseObject()));\n+                        values.put(postAgg.getName(), postAgg.compute(values));\n                       }\n                     }\n                     for (AggregatorFactory agg : query.getAggregatorSpecs()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/65c71a40bb11a3273deec638dd1077bf816bea17/processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java",
                "sha": "4e7cc10b347c4e4db4c1196505ac400e5f218781",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/incubator-druid/blob/65c71a40bb11a3273deec638dd1077bf816bea17/server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "changes": 117,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/client/CachingClusteredClientTest.java?ref=65c71a40bb11a3273deec638dd1077bf816bea17",
                "deletions": 15,
                "filename": "server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "patch": "@@ -61,6 +61,7 @@\n import io.druid.query.aggregation.LongSumAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n import io.druid.query.aggregation.post.ArithmeticPostAggregator;\n+import io.druid.query.aggregation.post.ConstantPostAggregator;\n import io.druid.query.aggregation.post.FieldAccessPostAggregator;\n import io.druid.query.filter.DimFilter;\n import io.druid.query.search.SearchQueryQueryToolChest;\n@@ -145,6 +146,22 @@\n               new FieldAccessPostAggregator(\"imps\", \"imps\"),\n               new FieldAccessPostAggregator(\"rows\", \"rows\")\n           )\n+      ),\n+      new ArithmeticPostAggregator(\n+          \"avg_imps_per_row_double\",\n+          \"*\",\n+          Arrays.<PostAggregator>asList(\n+              new FieldAccessPostAggregator(\"avg_imps_per_row\", \"avg_imps_per_row\"),\n+              new ConstantPostAggregator(\"constant\", 2, 2 )\n+          )\n+      ),\n+      new ArithmeticPostAggregator(\n+          \"avg_imps_per_row_half\",\n+          \"/\",\n+          Arrays.<PostAggregator>asList(\n+              new FieldAccessPostAggregator(\"avg_imps_per_row\", \"avg_imps_per_row\"),\n+              new ConstantPostAggregator(\"constant\", 2, 2 )\n+          )\n       )\n   );\n   private static final List<AggregatorFactory> RENAMED_AGGS = Arrays.asList(\n@@ -567,6 +584,72 @@ public void testTopNCachingEmptyResults() throws Exception\n     );\n   }\n \n+  public  void testTopNOnPostAggMetricCaching(){\n+    final TopNQueryBuilder builder = new TopNQueryBuilder()\n+        .dataSource(DATA_SOURCE)\n+        .dimension(TOP_DIM)\n+        .metric(\"avg_imps_per_row_double\")\n+        .threshold(3)\n+        .intervals(SEG_SPEC)\n+        .filters(DIM_FILTER)\n+        .granularity(GRANULARITY)\n+        .aggregators(AGGS)\n+        .postAggregators(POST_AGGS)\n+        .context(CONTEXT);\n+\n+    QueryRunner runner = new FinalizeResultsQueryRunner(client, new TopNQueryQueryToolChest(new TopNQueryConfig()));\n+    testQueryCaching(\n+        runner,\n+        builder.build(),\n+        new Interval(\"2011-01-01/2011-01-02\"),\n+        makeTopNResults(),\n+\n+        new Interval(\"2011-01-02/2011-01-03\"),\n+        makeTopNResults(),\n+\n+        new Interval(\"2011-01-05/2011-01-10\"),\n+        makeTopNResults(\n+            new DateTime(\"2011-01-05\"), \"a\", 50, 4994, \"b\", 50, 4993, \"c\", 50, 4992,\n+            new DateTime(\"2011-01-06\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-07\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-08\"), \"a\", 50, 4988, \"b\", 50, 4987, \"c\", 50, 4986,\n+            new DateTime(\"2011-01-09\"), \"a\", 50, 4985, \"b\", 50, 4984, \"c\", 50, 4983\n+        ),\n+\n+        new Interval(\"2011-01-05/2011-01-10\"),\n+        makeTopNResults(\n+            new DateTime(\"2011-01-05T01\"), \"a\", 50, 4994, \"b\", 50, 4993, \"c\", 50, 4992,\n+            new DateTime(\"2011-01-06T01\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-07T01\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-08T01\"), \"a\", 50, 4988, \"b\", 50, 4987, \"c\", 50, 4986,\n+            new DateTime(\"2011-01-09T01\"), \"a\", 50, 4985, \"b\", 50, 4984, \"c\", 50, 4983\n+        )\n+    );\n+\n+\n+    TestHelper.assertExpectedResults(\n+        makeRenamedTopNResults(\n+            new DateTime(\"2011-01-05\"), \"a\", 50, 4994, \"b\", 50, 4993, \"c\", 50, 4992,\n+            new DateTime(\"2011-01-05T01\"), \"a\", 50, 4994, \"b\", 50, 4993, \"c\", 50, 4992,\n+            new DateTime(\"2011-01-06\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-06T01\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-07\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-07T01\"), \"a\", 50, 4991, \"b\", 50, 4990, \"c\", 50, 4989,\n+            new DateTime(\"2011-01-08\"), \"a\", 50, 4988, \"b\", 50, 4987, \"c\", 50, 4986,\n+            new DateTime(\"2011-01-08T01\"), \"a\", 50, 4988, \"b\", 50, 4987, \"c\", 50, 4986,\n+            new DateTime(\"2011-01-09\"), \"a\", 50, 4985, \"b\", 50, 4984, \"c\", 50, 4983,\n+            new DateTime(\"2011-01-09T01\"), \"a\", 50, 4985, \"b\", 50, 4984, \"c\", 50, 4983\n+        ),\n+        runner.run(\n+            builder.intervals(\"2011-01-01/2011-01-10\")\n+                   .metric(\"avg_imps_per_row_double\")\n+                   .aggregators(RENAMED_AGGS)\n+                   .postAggregators(RENAMED_POST_AGGS)\n+                   .build()\n+        )\n+    );\n+  }\n+\n   @Test\n   public void testSearchCaching() throws Exception\n   {\n@@ -1007,20 +1090,22 @@ public void run()\n \n     List<Result<TimeseriesResultValue>> retVal = Lists.newArrayListWithCapacity(objects.length / 3);\n     for (int i = 0; i < objects.length; i += 3) {\n+      double avg_impr = ((Number) objects[i + 2]).doubleValue() / ((Number) objects[i + 1]).doubleValue();\n       retVal.add(\n           new Result<>(\n               (DateTime) objects[i],\n               new TimeseriesResultValue(\n-                  ImmutableMap.of(\n-                      \"rows\", objects[i + 1],\n-                      \"imps\", objects[i + 2],\n-                      \"impers\", objects[i + 2],\n-                      \"avg_imps_per_row\",\n-                      ((Number) objects[i + 2]).doubleValue() / ((Number) objects[i + 1]).doubleValue()\n+                  ImmutableMap.<String, Object>builder()\n+                      .put(\"rows\", objects[i + 1])\n+                      .put(\"imps\", objects[i + 2])\n+                      .put(\"impers\", objects[i + 2])\n+                      .put(\"avg_imps_per_row\",avg_impr)\n+                      .put(\"avg_imps_per_row_half\",avg_impr / 2)\n+                      .put(\"avg_imps_per_row_double\",avg_impr * 2)\n+                      .build()\n                   )\n               )\n-          )\n-      );\n+          );\n     }\n     return retVal;\n   }\n@@ -1099,13 +1184,15 @@ public void run()\n         final double imps = ((Number) objects[index + 2]).doubleValue();\n         final double rows = ((Number) objects[index + 1]).doubleValue();\n         values.add(\n-            ImmutableMap.of(\n-                TOP_DIM, objects[index],\n-                \"rows\", rows,\n-                \"imps\", imps,\n-                \"impers\", imps,\n-                \"avg_imps_per_row\", imps / rows\n-            )\n+            ImmutableMap.<String, Object>builder()\n+                .put(TOP_DIM, objects[index])\n+                .put(\"rows\", rows)\n+                .put(\"imps\", imps)\n+                .put(\"impers\", imps)\n+                .put(\"avg_imps_per_row\", imps / rows)\n+                .put(\"avg_imps_per_row_double\", ((imps * 2) / rows))\n+                .put(\"avg_imps_per_row_half\", (imps / (rows * 2)))\n+                .build()\n         );\n         index += 3;\n       }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/65c71a40bb11a3273deec638dd1077bf816bea17/server/src/test/java/io/druid/client/CachingClusteredClientTest.java",
                "sha": "58f37f4d112cb696fa101a942fade91381139f1c",
                "status": "modified"
            }
        ],
        "message": "fix more issues and add test\n\n1) Add test for caching and calculation of dependent post-aggs for\nTimeSeries and TopN\n2) Fix more NPEs while calculating dependent postAggs, passing\nbaseHolder was wrong as it does not contain post-aggs during computing\ndependent ones.",
        "parent": "https://github.com/apache/incubator-druid/commit/3fb42251d90cea242b43413842bd32be8042cd4f",
        "repo": "incubator-druid",
        "unit_tests": [
            "TimeseriesQueryQueryToolChestTest.java",
            "TopNQueryQueryToolChestTest.java"
        ]
    },
    "incubator-druid_674f940": {
        "bug_id": "incubator-druid_674f940",
        "commit": "https://github.com/apache/incubator-druid/commit/674f94083e381f302fab9e43f15fb940a3b3934c",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/incubator-druid/blob/674f94083e381f302fab9e43f15fb940a3b3934c/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=674f94083e381f302fab9e43f15fb940a3b3934c",
                "deletions": 4,
                "filename": "extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.inject.Inject;\n import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n+import com.metamx.common.StringUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n import io.druid.segment.loading.SegmentLoadingException;\n@@ -133,18 +134,33 @@ public Void call() throws Exception\n                     s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n                   log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\", s3Bucket, s3Path);\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n+                  final String copyMsg = StringUtils.safeFormat(\n+                      \"[s3://%s/%s] to [s3://%s/%s]\", s3Bucket,\n                       s3Path,\n                       targetS3Bucket,\n                       targetS3Path\n                   );\n+                  log.info(\n+                      \"Moving file %s\",\n+                      copyMsg\n+                  );\n                   final S3Object target = new S3Object(targetS3Path);\n                   if (!config.getDisableAcl()) {\n                     target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n+                  final Map<String, Object> copyResult = s3Client.moveObject(\n+                      s3Bucket,\n+                      s3Path,\n+                      targetS3Bucket,\n+                      target,\n+                      false\n+                  );\n+                  if (copyResult != null && copyResult.containsKey(\"DeleteException\")) {\n+                    log.error(\"Error Deleting data after copy %s: %s\", copyMsg, copyResult);\n+                    // Maybe retry deleting here?\n+                  } else {\n+                    log.debug(\"Finished moving file %s\", copyMsg);\n+                  }\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/674f94083e381f302fab9e43f15fb940a3b3934c/extensions-core/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "6726569d056463b96037be02948bc6b87341c9f6",
                "status": "modified"
            }
        ],
        "message": "Add more logging around failed S3DataSegmentMover DeleteExceptions (#3104)\n\n* Add more logging around failed S3DataSegmentMover DeleteExceptions\r\n\r\n* Fix test NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/6c2fd75e4645613cf26b9e1f654fc7d1d7691458",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    },
    "incubator-druid_677e24b": {
        "bug_id": "incubator-druid_677e24b",
        "commit": "https://github.com/apache/incubator-druid/commit/677e24b76027d833da2987416a5439f474faff7a",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/677e24b76027d833da2987416a5439f474faff7a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java?ref=677e24b76027d833da2987416a5439f474faff7a",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "patch": "@@ -883,7 +883,10 @@ public void onFailure(Throwable t)\n       if (chatHandlerProvider.isPresent()) {\n         chatHandlerProvider.get().unregister(getId());\n       }\n-      publishExecService.shutdownNow();\n+\n+      if (publishExecService != null) {\n+        publishExecService.shutdownNow();\n+      }\n \n       toolbox.getDruidNodeAnnouncer().unannounce(discoveryDruidNode);\n       toolbox.getDataSegmentServerAnnouncer().unannounce();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/677e24b76027d833da2987416a5439f474faff7a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java",
                "sha": "6a17872d7230dbf1a21cf6a416c4c06730a79211",
                "status": "modified"
            }
        ],
        "message": "prevent NPE from supressing actual exception (#5146)",
        "parent": "https://github.com/apache/incubator-druid/commit/64848c7ebfd5b95c528c0890ef47990de4e0a8f3",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaIndexTaskTest.java"
        ]
    },
    "incubator-druid_67f4bba": {
        "bug_id": "incubator-druid_67f4bba",
        "commit": "https://github.com/apache/incubator-druid/commit/67f4bbae74b45af24146551a3a6208475e3d8f34",
        "file": [
            {
                "additions": 149,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 257,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 108,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -130,146 +130,148 @@ public IncrementalIndex(\n     int currAggSize = 0;\n     for (int i = 0; i < metrics.length; i++) {\n       final AggregatorFactory agg = metrics[i];\n-      aggs[i] = agg.factorizeBuffered(\n-          new ColumnSelectorFactory()\n-          {\n-            @Override\n-            public TimestampColumnSelector makeTimestampColumnSelector()\n-            {\n-              return new TimestampColumnSelector()\n+      aggs[i] = new ThreadSafeAggregator(\n+          agg.factorizeBuffered(\n+              new ColumnSelectorFactory()\n               {\n                 @Override\n-                public long getTimestamp()\n+                public TimestampColumnSelector makeTimestampColumnSelector()\n                 {\n-                  return in.get().getTimestampFromEpoch();\n+                  return new TimestampColumnSelector()\n+                  {\n+                    @Override\n+                    public long getTimestamp()\n+                    {\n+                      return in.get().getTimestampFromEpoch();\n+                    }\n+                  };\n                 }\n-              };\n-            }\n \n-            @Override\n-            public FloatColumnSelector makeFloatColumnSelector(String columnName)\n-            {\n-              final String metricName = columnName.toLowerCase();\n-              return new FloatColumnSelector()\n-              {\n                 @Override\n-                public float get()\n+                public FloatColumnSelector makeFloatColumnSelector(String columnName)\n                 {\n-                  return in.get().getFloatMetric(metricName);\n+                  final String metricName = columnName.toLowerCase();\n+                  return new FloatColumnSelector()\n+                  {\n+                    @Override\n+                    public float get()\n+                    {\n+                      return in.get().getFloatMetric(metricName);\n+                    }\n+                  };\n                 }\n-              };\n-            }\n \n-            @Override\n-            public ObjectColumnSelector makeObjectColumnSelector(String column)\n-            {\n-              final String typeName = agg.getTypeName();\n-              final String columnName = column.toLowerCase();\n-\n-              final ObjectColumnSelector<Object> rawColumnSelector = new ObjectColumnSelector<Object>()\n-              {\n                 @Override\n-                public Class classOfObject()\n+                public ObjectColumnSelector makeObjectColumnSelector(String column)\n                 {\n-                  return Object.class;\n-                }\n+                  final String typeName = agg.getTypeName();\n+                  final String columnName = column.toLowerCase();\n \n-                @Override\n-                public Object get()\n-                {\n-                  return in.get().getRaw(columnName);\n-                }\n-              };\n+                  final ObjectColumnSelector<Object> rawColumnSelector = new ObjectColumnSelector<Object>()\n+                  {\n+                    @Override\n+                    public Class classOfObject()\n+                    {\n+                      return Object.class;\n+                    }\n \n-              if (!deserializeComplexMetrics) {\n-                return rawColumnSelector;\n-              } else {\n-                if (typeName.equals(\"float\")) {\n-                  return rawColumnSelector;\n-                }\n+                    @Override\n+                    public Object get()\n+                    {\n+                      return in.get().getRaw(columnName);\n+                    }\n+                  };\n \n-                final ComplexMetricSerde serde = ComplexMetrics.getSerdeForType(typeName);\n-                if (serde == null) {\n-                  throw new ISE(\"Don't know how to handle type[%s]\", typeName);\n-                }\n+                  if (!deserializeComplexMetrics) {\n+                    return rawColumnSelector;\n+                  } else {\n+                    if (typeName.equals(\"float\")) {\n+                      return rawColumnSelector;\n+                    }\n \n-                final ComplexMetricExtractor extractor = serde.getExtractor();\n-                return new ObjectColumnSelector()\n-                {\n-                  @Override\n-                  public Class classOfObject()\n-                  {\n-                    return extractor.extractedClass();\n-                  }\n+                    final ComplexMetricSerde serde = ComplexMetrics.getSerdeForType(typeName);\n+                    if (serde == null) {\n+                      throw new ISE(\"Don't know how to handle type[%s]\", typeName);\n+                    }\n \n-                  @Override\n-                  public Object get()\n-                  {\n-                    return extractor.extractValue(in.get(), columnName);\n+                    final ComplexMetricExtractor extractor = serde.getExtractor();\n+                    return new ObjectColumnSelector()\n+                    {\n+                      @Override\n+                      public Class classOfObject()\n+                      {\n+                        return extractor.extractedClass();\n+                      }\n+\n+                      @Override\n+                      public Object get()\n+                      {\n+                        return extractor.extractValue(in.get(), columnName);\n+                      }\n+                    };\n                   }\n-                };\n-              }\n-            }\n+                }\n \n-            @Override\n-            public DimensionSelector makeDimensionSelector(final String dimension)\n-            {\n-              final String dimensionName = dimension.toLowerCase();\n-              return new DimensionSelector()\n-              {\n                 @Override\n-                public IndexedInts getRow()\n+                public DimensionSelector makeDimensionSelector(final String dimension)\n                 {\n-                  final List<String> dimensionValues = in.get().getDimension(dimensionName);\n-                  final ArrayList<Integer> vals = Lists.newArrayList();\n-                  if (dimensionValues != null) {\n-                    for (int i = 0; i < dimensionValues.size(); ++i) {\n-                      vals.add(i);\n+                  final String dimensionName = dimension.toLowerCase();\n+                  return new DimensionSelector()\n+                  {\n+                    @Override\n+                    public IndexedInts getRow()\n+                    {\n+                      final List<String> dimensionValues = in.get().getDimension(dimensionName);\n+                      final ArrayList<Integer> vals = Lists.newArrayList();\n+                      if (dimensionValues != null) {\n+                        for (int i = 0; i < dimensionValues.size(); ++i) {\n+                          vals.add(i);\n+                        }\n+                      }\n+\n+                      return new IndexedInts()\n+                      {\n+                        @Override\n+                        public int size()\n+                        {\n+                          return vals.size();\n+                        }\n+\n+                        @Override\n+                        public int get(int index)\n+                        {\n+                          return vals.get(index);\n+                        }\n+\n+                        @Override\n+                        public Iterator<Integer> iterator()\n+                        {\n+                          return vals.iterator();\n+                        }\n+                      };\n                     }\n-                  }\n \n-                  return new IndexedInts()\n-                  {\n                     @Override\n-                    public int size()\n+                    public int getValueCardinality()\n                     {\n-                      return vals.size();\n+                      throw new UnsupportedOperationException(\"value cardinality is unknown in incremental index\");\n                     }\n \n                     @Override\n-                    public int get(int index)\n+                    public String lookupName(int id)\n                     {\n-                      return vals.get(index);\n+                      return in.get().getDimension(dimensionName).get(id);\n                     }\n \n                     @Override\n-                    public Iterator<Integer> iterator()\n+                    public int lookupId(String name)\n                     {\n-                      return vals.iterator();\n+                      return in.get().getDimension(dimensionName).indexOf(name);\n                     }\n                   };\n                 }\n-\n-                @Override\n-                public int getValueCardinality()\n-                {\n-                  throw new UnsupportedOperationException(\"value cardinality is unknown in incremental index\");\n-                }\n-\n-                @Override\n-                public String lookupName(int id)\n-                {\n-                  return in.get().getDimension(dimensionName).get(id);\n-                }\n-\n-                @Override\n-                public int lookupId(String name)\n-                {\n-                  return in.get().getDimension(dimensionName).indexOf(name);\n-                }\n-              };\n-            }\n-          }\n+              }\n+          )\n       );\n       aggPositionOffsets[i] = currAggSize;\n       currAggSize += agg.getMaxIntermediateSize();\n@@ -458,9 +460,7 @@ public int add(InputRow row)\n     in.set(row);\n     int rowOffset = facts.get(key);\n     for (int i = 0; i < aggs.length; i++) {\n-      synchronized (aggs[i]) {\n         aggs[i].aggregate(bufferHolder.get(), getMetricPosition(rowOffset, i));\n-      }\n     }\n     in.set(null);\n     return numEntries.get();\n@@ -473,7 +473,7 @@ public boolean isEmpty()\n \n   /**\n    *\n-   * @return true if the underlying buffer for IncrementalIndex is full and cannot accomodate more rows.\n+   * @return true if the underlying buffer for IncrementalIndex is full and cannot accommodate more rows.\n    */\n   public boolean isFull()\n   {\n@@ -861,4 +861,45 @@ private void assertSorted()\n       }\n     }\n   }\n+\n+  private static class ThreadSafeAggregator implements BufferAggregator\n+  {\n+\n+    private final BufferAggregator delegate;\n+\n+    public ThreadSafeAggregator(BufferAggregator delegate)\n+    {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public synchronized void init(ByteBuffer buf, int position)\n+    {\n+      delegate.init(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized void aggregate(ByteBuffer buf, int position)\n+    {\n+      delegate.aggregate(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized Object get(ByteBuffer buf, int position)\n+    {\n+      return delegate.get(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized float getFloat(ByteBuffer buf, int position)\n+    {\n+      return delegate.getFloat(buf, position);\n+    }\n+\n+    @Override\n+    public synchronized void close()\n+    {\n+      delegate.close();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "124035e612f6326e6f05d97a17450a131227d481",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "patch": "@@ -36,7 +36,7 @@\n  */\n public class RealtimeTuningConfig implements TuningConfig\n {\n-  private static final int defaultBufferSize = 512 * 1024 * 1024;\n+  private static final int defaultBufferSize = 256 * 1024 * 1024;\n   private static final Period defaultIntermediatePersistPeriod = new Period(\"PT10M\");\n   private static final Period defaultWindowPeriod = new Period(\"PT10M\");\n   private static final File defaultBasePersistDirectory = Files.createTempDir();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/indexing/RealtimeTuningConfig.java",
                "sha": "52df7cafbe4d54075906a502b55e3b570281d02a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java?ref=67f4bbae74b45af24146551a3a6208475e3d8f34",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "patch": "@@ -136,7 +136,7 @@ public boolean isEmpty()\n   public boolean isFull()\n   {\n     synchronized (currHydrant){\n-      return currHydrant.getIndex().isFull();\n+      return currHydrant != null && currHydrant.getIndex().isFull();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/67f4bbae74b45af24146551a3a6208475e3d8f34/server/src/main/java/io/druid/segment/realtime/plumber/Sink.java",
                "sha": "033152015206c92bcc9c9635f9695817a37d70f7",
                "status": "modified"
            }
        ],
        "message": "fixes from review comments\n\nfix sync of aggs,\nfix NPE in sink.isFull,\nRealtimeTuningConfig lower the bufferSize to 256m",
        "parent": "https://github.com/apache/incubator-druid/commit/d64879ccca9f2b472d60c5db897f003eb83580cf",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexTest.java",
            "RealtimeTuningConfigTest.java",
            "SinkTest.java"
        ]
    },
    "incubator-druid_69a8723": {
        "bug_id": "incubator-druid_69a8723",
        "commit": "https://github.com/apache/incubator-druid/commit/69a8723f1851a32c557670f2d0b7c9beb29a78ab",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/69a8723f1851a32c557670f2d0b7c9beb29a78ab/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/CachingQueryRunner.java?ref=69a8723f1851a32c557670f2d0b7c9beb29a78ab",
                "deletions": 5,
                "filename": "server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "patch": "@@ -84,11 +84,16 @@ public CachingQueryRunner(\n         && strategy != null\n         && cacheConfig.isPopulateCache();\n \n-    final Cache.NamedKey key = CacheUtil.computeSegmentCacheKey(\n-        segmentIdentifier,\n-        segmentDescriptor,\n-        strategy.computeCacheKey(query)\n-    );\n+    final Cache.NamedKey key;\n+    if(strategy != null && (useCache || populateCache)) {\n+      key = CacheUtil.computeSegmentCacheKey(\n+          segmentIdentifier,\n+          segmentDescriptor,\n+          strategy.computeCacheKey(query)\n+      );\n+    } else {\n+      key = null;\n+    }\n \n     if(useCache) {\n       final Function cacheFn = strategy.pullFromCache();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/69a8723f1851a32c557670f2d0b7c9beb29a78ab/server/src/main/java/io/druid/client/CachingQueryRunner.java",
                "sha": "d92db6415fba83fcc3e18c357f8eff913e3aac43",
                "status": "modified"
            }
        ],
        "message": "fix npe in CachingQueryRunner",
        "parent": "https://github.com/apache/incubator-druid/commit/c05f16917115cf19257e7c928a14d9eeec9b005c",
        "repo": "incubator-druid",
        "unit_tests": [
            "CachingQueryRunnerTest.java"
        ]
    },
    "incubator-druid_6c8a024": {
        "bug_id": "incubator-druid_6c8a024",
        "commit": "https://github.com/apache/incubator-druid/commit/6c8a024b84fbff79fe12778252b9208bbc6a2e5a",
        "file": [
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java?ref=6c8a024b84fbff79fe12778252b9208bbc6a2e5a",
                "deletions": 29,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "patch": "@@ -325,11 +325,17 @@ public String getVersion(final Interval interval)\n       // Delay firehose connection to avoid claiming input resources while the plumber is starting up.\n       final FirehoseFactory firehoseFactory = spec.getIOConfig().getFirehoseFactory();\n       final boolean firehoseDrainableByClosing = isFirehoseDrainableByClosing(firehoseFactory);\n-      firehose = firehoseFactory.connect(spec.getDataSchema().getParser());\n-      committerSupplier = Committers.supplierFromFirehose(firehose);\n+\n+      // Skip connecting firehose if we've been stopped before we got started.\n+      synchronized (this) {\n+        if (!gracefullyStopped) {\n+          firehose = firehoseFactory.connect(spec.getDataSchema().getParser());\n+          committerSupplier = Committers.supplierFromFirehose(firehose);\n+        }\n+      }\n \n       // Time to read data!\n-      while ((!gracefullyStopped || firehoseDrainableByClosing) && firehose.hasMore()) {\n+      while (firehose != null && (!gracefullyStopped || firehoseDrainableByClosing) && firehose.hasMore()) {\n         final InputRow inputRow;\n \n         try {\n@@ -366,33 +372,35 @@ public String getVersion(final Interval interval)\n     finally {\n       if (normalExit) {\n         try {\n-          // Always want to persist.\n-          log.info(\"Persisting remaining data.\");\n-\n-          final Committer committer = committerSupplier.get();\n-          final CountDownLatch persistLatch = new CountDownLatch(1);\n-          plumber.persist(\n-              new Committer()\n-              {\n-                @Override\n-                public Object getMetadata()\n-                {\n-                  return committer.getMetadata();\n-                }\n-\n-                @Override\n-                public void run()\n+          // Persist if we had actually started.\n+          if (firehose != null) {\n+            log.info(\"Persisting remaining data.\");\n+\n+            final Committer committer = committerSupplier.get();\n+            final CountDownLatch persistLatch = new CountDownLatch(1);\n+            plumber.persist(\n+                new Committer()\n                 {\n-                  try {\n-                    committer.run();\n+                  @Override\n+                  public Object getMetadata()\n+                  {\n+                    return committer.getMetadata();\n                   }\n-                  finally {\n-                    persistLatch.countDown();\n+\n+                  @Override\n+                  public void run()\n+                  {\n+                    try {\n+                      committer.run();\n+                    }\n+                    finally {\n+                      persistLatch.countDown();\n+                    }\n                   }\n                 }\n-              }\n-          );\n-          persistLatch.await();\n+            );\n+            persistLatch.await();\n+          }\n \n           if (gracefullyStopped) {\n             log.info(\"Gracefully stopping.\");\n@@ -420,8 +428,9 @@ public void run()\n           throw e;\n         }\n         finally {\n-          // firehose will be non-null since normalExit is true\n-          CloseQuietly.close(firehose);\n+          if (firehose != null) {\n+            CloseQuietly.close(firehose);\n+          }\n           toolbox.getMonitorScheduler().removeMonitor(metricsMonitor);\n         }\n       }\n@@ -444,7 +453,9 @@ public void stopGracefully()\n       synchronized (this) {\n         if (!gracefullyStopped) {\n           gracefullyStopped = true;\n-          if (finishingJob) {\n+          if (firehose == null) {\n+            log.info(\"stopGracefully: Firehose not started yet, so nothing to stop.\");\n+          } else if (finishingJob) {\n             log.info(\"stopGracefully: Interrupting finishJob.\");\n             runThread.interrupt();\n           } else if (isFirehoseDrainableByClosing(spec.getIOConfig().getFirehoseFactory())) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/main/java/io/druid/indexing/common/task/RealtimeIndexTask.java",
                "sha": "24ab73adb32910a2e924e1140523bbee7436f9fc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java?ref=6c8a024b84fbff79fe12778252b9208bbc6a2e5a",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "patch": "@@ -144,7 +144,7 @@ public void stop()\n              .addData(\"taskId\", task.getId())\n              .addData(\"dataSource\", task.getDataSource())\n              .emit();\n-          log.warn(e, \"Graceful shutdown of task[%s] aborted with exception.\");\n+          log.warn(e, \"Graceful shutdown of task[%s] aborted with exception.\", task.getId());\n           error = true;\n         }\n       } else {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "sha": "f7ae62f40430e63a8ec96fae2511de108ef3ccc6",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java?ref=6c8a024b84fbff79fe12778252b9208bbc6a2e5a",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "patch": "@@ -536,6 +536,22 @@ public void testRestoreCorruptData() throws Exception\n     }\n   }\n \n+  @Test(timeout = 10000L)\n+  public void testStopBeforeStarting() throws Exception\n+  {\n+    final File directory = tempFolder.newFolder();\n+    final RealtimeIndexTask task1 = makeRealtimeTask(null);\n+\n+    task1.stopGracefully();\n+    final TestIndexerMetadataStorageCoordinator mdc = new TestIndexerMetadataStorageCoordinator();\n+    final TaskToolbox taskToolbox = makeToolbox(task1, mdc, directory);\n+    final ListenableFuture<TaskStatus> statusFuture = runTask(task1, taskToolbox);\n+\n+    // Wait for the task to finish.\n+    final TaskStatus taskStatus = statusFuture.get();\n+    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+  }\n+\n   private ListenableFuture<TaskStatus> runTask(final Task task, final TaskToolbox toolbox)\n   {\n     return taskExec.submit(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6c8a024b84fbff79fe12778252b9208bbc6a2e5a/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "sha": "37914a8df42a530f363346439778effc36950d97",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #2357 from gianm/restorestuff\n\nFix NPE caused by stopping a RealtimeIndexTask before firehose connects",
        "parent": "https://github.com/apache/incubator-druid/commit/867d6e84b3a7b335aeb1a04c5419abd36be84919",
        "repo": "incubator-druid",
        "unit_tests": [
            "RealtimeIndexTaskTest.java"
        ]
    },
    "incubator-druid_6da2272": {
        "bug_id": "incubator-druid_6da2272",
        "commit": "https://github.com/apache/incubator-druid/commit/6da22720765af90e191340cbe7685bb5a27a08f8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java?ref=6da22720765af90e191340cbe7685bb5a27a08f8",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "patch": "@@ -65,7 +65,7 @@ public TopNParams makeInitParams(\n   }\n \n   @Override\n-  public TopNResultBuilder makeResultBuilder(TopNParams params)\n+  public TopNResultBuilder makeResultBuilder(TopNParams params, TopNQuery query)\n   {\n     return query.getTopNMetricSpec().getResultBuilder(\n         params.getCursor().getTime(),\n@@ -82,7 +82,6 @@ public void run(\n       TopNParams params, TopNResultBuilder resultBuilder, int[] ints\n   )\n   {\n-    final TopNResultBuilder singleMetricResultBuilder = makeResultBuilder(params);\n     final String metric;\n     // ugly\n     TopNMetricSpec spec = query.getTopNMetricSpec();\n@@ -128,6 +127,7 @@ public void run(\n                                                         .aggregators(condensedAggs)\n                                                         .postAggregators(condensedPostAggs)\n                                                         .build();\n+    final TopNResultBuilder singleMetricResultBuilder = makeResultBuilder(params, singleMetricQuery);\n \n     PooledTopNAlgorithm singleMetricAlgo = new PooledTopNAlgorithm(capabilities, singleMetricQuery, bufferPool);\n     PooledTopNAlgorithm.PooledTopNParams singleMetricParam = null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "sha": "c826ce2333bbd295e535bb050da579ed584f350f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java?ref=6da22720765af90e191340cbe7685bb5a27a08f8",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "patch": "@@ -57,7 +57,7 @@ public TopNParams makeInitParams(\n   }\n \n   @Override\n-  public TopNResultBuilder makeResultBuilder(TopNParams params)\n+  public TopNResultBuilder makeResultBuilder(TopNParams params, TopNQuery query)\n   {\n     return query.getTopNMetricSpec().getResultBuilder(\n         params.getCursor().getTime(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/DimExtractionTopNAlgorithm.java",
                "sha": "547d5e25ca4001a9c6d6ec8b517ec113be1f862b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java?ref=6da22720765af90e191340cbe7685bb5a27a08f8",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "patch": "@@ -115,7 +115,7 @@ public PooledTopNParams makeInitParams(\n   }\n \n   @Override\n-  public TopNResultBuilder makeResultBuilder(PooledTopNParams params)\n+  public TopNResultBuilder makeResultBuilder(PooledTopNParams params, TopNQuery query)\n   {\n     return query.getTopNMetricSpec().getResultBuilder(\n         params.getCursor().getTime(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "sha": "50e5f6e7a9553c95ca91058a50109d5dae4b36fb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java?ref=6da22720765af90e191340cbe7685bb5a27a08f8",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "patch": "@@ -33,7 +33,7 @@\n \n   public TopNParams makeInitParams(DimensionSelector dimSelector, Cursor cursor);\n \n-  public TopNResultBuilder makeResultBuilder(Parameters params);\n+  public TopNResultBuilder makeResultBuilder(Parameters params, TopNQuery query);\n \n   public void run(\n       Parameters params,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/TopNAlgorithm.java",
                "sha": "49496e8f0808a7a9f015c74a3729bea20a0a2836",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNMapFn.java?ref=6da22720765af90e191340cbe7685bb5a27a08f8",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "patch": "@@ -52,7 +52,7 @@ public TopNMapFn(\n     try {\n       params = topNAlgorithm.makeInitParams(dimSelector, cursor);\n \n-      TopNResultBuilder resultBuilder = topNAlgorithm.makeResultBuilder(params);\n+      TopNResultBuilder resultBuilder = topNAlgorithm.makeResultBuilder(params, query);\n \n       topNAlgorithm.run(params, resultBuilder, null);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/6da22720765af90e191340cbe7685bb5a27a08f8/processing/src/main/java/io/druid/query/topn/TopNMapFn.java",
                "sha": "c51b63cd5397a4daa79a34c74b364328f542dd10",
                "status": "modified"
            }
        ],
        "message": "fix NPE in aggregatrFirstTopnAlgo",
        "parent": "https://github.com/apache/incubator-druid/commit/6893282a65338aed71253eae448369a068924bd0",
        "repo": "incubator-druid",
        "unit_tests": [
            "PooledTopNAlgorithmTest.java"
        ]
    },
    "incubator-druid_7239f56": {
        "bug_id": "incubator-druid_7239f56",
        "commit": "https://github.com/apache/incubator-druid/commit/7239f56131d6bff3e9a1484c0f71960288d4ba1b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java?ref=7239f56131d6bff3e9a1484c0f71960288d4ba1b",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "patch": "@@ -968,7 +968,7 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event)\n                             announcement.getTaskType(),\n                             zkWorker.getWorker(),\n                             TaskLocation.unknown(),\n-                            runningTasks.get(taskId).getDataSource()\n+                            announcement.getTaskDataSource()\n                         );\n                         final RemoteTaskRunnerWorkItem existingItem = runningTasks.putIfAbsent(\n                             taskId,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "sha": "8b06b195157b95f20fb194d3df4cb036ebbedfad",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/WorkerHolder.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/WorkerHolder.java?ref=7239f56131d6bff3e9a1484c0f71960288d4ba1b",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/WorkerHolder.java",
                "patch": "@@ -387,7 +387,8 @@ public void fullSync(List<WorkerHistoryItem> changes)\n                 announcement.getTaskType(),\n                 announcement.getTaskResource(),\n                 TaskStatus.failure(announcement.getTaskId()),\n-                announcement.getTaskLocation()\n+                announcement.getTaskLocation(),\n+                announcement.getTaskDataSource()\n             ));\n           }\n         }\n@@ -423,7 +424,8 @@ public void deltaSync(List<WorkerHistoryItem> changes)\n                   announcement.getTaskType(),\n                   announcement.getTaskResource(),\n                   TaskStatus.failure(announcement.getTaskId()),\n-                  announcement.getTaskLocation()\n+                  announcement.getTaskLocation(),\n+                  announcement.getTaskDataSource()\n               ));\n             }\n           } else if (change instanceof WorkerHistoryItem.Metadata) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/overlord/hrtr/WorkerHolder.java",
                "sha": "7adc43419d9e30950b1b3b2076da55f4ba2b7b38",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java?ref=7239f56131d6bff3e9a1484c0f71960288d4ba1b",
                "deletions": 5,
                "filename": "indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "patch": "@@ -28,6 +28,8 @@\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.common.task.TaskResource;\n \n+import javax.annotation.Nullable;\n+\n /**\n  * Used by workers to announce the status of tasks they are currently running. This class is immutable.\n  */\n@@ -38,21 +40,25 @@\n   private final TaskResource taskResource;\n   private final TaskLocation taskLocation;\n \n+  @Nullable\n+  private final String taskDataSource; // nullable for backward compatibility\n+\n   public static TaskAnnouncement create(Task task, TaskStatus status, TaskLocation location)\n   {\n-    return create(task.getId(), task.getType(), task.getTaskResource(), status, location);\n+    return create(task.getId(), task.getType(), task.getTaskResource(), status, location, task.getDataSource());\n   }\n \n   public static TaskAnnouncement create(\n       String taskId,\n       String taskType,\n       TaskResource resource,\n       TaskStatus status,\n-      TaskLocation location\n+      TaskLocation location,\n+      String taskDataSource\n   )\n   {\n     Preconditions.checkArgument(status.getId().equals(taskId), \"task id == status id\");\n-    return new TaskAnnouncement(null, taskType, null, status, resource, location);\n+    return new TaskAnnouncement(null, taskType, null, status, resource, location, taskDataSource);\n   }\n \n   @JsonCreator\n@@ -62,7 +68,8 @@ private TaskAnnouncement(\n       @JsonProperty(\"status\") TaskState status,\n       @JsonProperty(\"taskStatus\") TaskStatus taskStatus,\n       @JsonProperty(\"taskResource\") TaskResource taskResource,\n-      @JsonProperty(\"taskLocation\") TaskLocation taskLocation\n+      @JsonProperty(\"taskLocation\") TaskLocation taskLocation,\n+      @JsonProperty(\"taskDataSource\") String taskDataSource\n   )\n   {\n     this.taskType = taskType;\n@@ -74,6 +81,7 @@ private TaskAnnouncement(\n     }\n     this.taskResource = taskResource == null ? new TaskResource(this.taskStatus.getId(), 1) : taskResource;\n     this.taskLocation = taskLocation == null ? TaskLocation.unknown() : taskLocation;\n+    this.taskDataSource = taskDataSource;\n   }\n \n   @JsonProperty(\"id\")\n@@ -112,13 +120,21 @@ public TaskLocation getTaskLocation()\n     return taskLocation;\n   }\n \n+  @JsonProperty(\"taskDataSource\")\n+  public String getTaskDataSource()\n+  {\n+    return taskDataSource;\n+  }\n+\n   @Override\n   public String toString()\n   {\n     return \"TaskAnnouncement{\" +\n-           \"taskStatus=\" + taskStatus +\n+           \"taskType=\" + taskType +\n+           \", taskStatus=\" + taskStatus +\n            \", taskResource=\" + taskResource +\n            \", taskLocation=\" + taskLocation +\n+           \", taskDataSource=\" + taskDataSource +\n            '}';\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "sha": "7ab4cc6129e36e047fe30609be2c06e10b598740",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java?ref=7239f56131d6bff3e9a1484c0f71960288d4ba1b",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "patch": "@@ -136,7 +136,8 @@ private void cleanupStaleAnnouncements() throws Exception\n                     announcement.getTaskType(),\n                     announcement.getTaskResource(),\n                     completionStatus,\n-                    TaskLocation.unknown()\n+                    TaskLocation.unknown(),\n+                    announcement.getTaskDataSource()\n                 )\n             );\n           }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "sha": "d5ff7d3854cb8784460544e119941b707a662275",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java?ref=7239f56131d6bff3e9a1484c0f71960288d4ba1b",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "patch": "@@ -447,6 +447,33 @@ public void testWorkerDisabled() throws Exception\n     Assert.assertEquals(\"\", Iterables.getOnlyElement(remoteTaskRunner.getWorkers()).getWorker().getVersion());\n   }\n \n+  @Test\n+  public void testRestartRemoteTaskRunner() throws Exception\n+  {\n+    doSetup();\n+    remoteTaskRunner.run(task);\n+\n+    Assert.assertTrue(taskAnnounced(task.getId()));\n+    mockWorkerRunningTask(task);\n+    Assert.assertTrue(workerRunningTask(task.getId()));\n+\n+    remoteTaskRunner.stop();\n+    makeRemoteTaskRunner(new TestRemoteTaskRunnerConfig(new Period(\"PT5S\")));\n+    final RemoteTaskRunnerWorkItem newWorkItem = remoteTaskRunner\n+        .getKnownTasks()\n+        .stream()\n+        .filter(workItem -> workItem.getTaskId().equals(task.getId()))\n+        .findFirst()\n+        .orElse(null);\n+    final ListenableFuture<TaskStatus> result = newWorkItem.getResult();\n+\n+    mockWorkerCompleteSuccessfulTask(task);\n+    Assert.assertTrue(workerCompletedTask(result));\n+\n+    Assert.assertEquals(task.getId(), result.get().getId());\n+    Assert.assertEquals(TaskState.SUCCESS, result.get().getStatusCode());\n+  }\n+\n   private void doSetup() throws Exception\n   {\n     makeWorker();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7239f56131d6bff3e9a1484c0f71960288d4ba1b/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "sha": "a5e230e0c135f6d73bed05a11ef8928a8f3f0e94",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in RemoteTaskRunner when some tasks in ZooKeeper but not in Overlord (#5511)\n\n* Fix NPE in RemoteTaskRunner when some tasks in ZooKeeper but not in Overlord\r\n\r\n* revert unnecessary change",
        "parent": "https://github.com/apache/incubator-druid/commit/f0a94f50350cadc271730db916e22d254b6f3e90",
        "repo": "incubator-druid",
        "unit_tests": [
            "RemoteTaskRunnerTest.java",
            "WorkerHolderTest.java",
            "TaskAnnouncementTest.java",
            "WorkerTaskMonitorTest.java"
        ]
    },
    "incubator-druid_76cb06a": {
        "bug_id": "incubator-druid_76cb06a",
        "commit": "https://github.com/apache/incubator-druid/commit/76cb06a8d8161d29d985ef048b89e6a82b489058",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/pom.xml",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-extraction-namespace/pom.xml?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/kafka-extraction-namespace/pom.xml",
                "patch": "@@ -103,5 +103,15 @@\n       <artifactId>easymock</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.powermock</groupId>\n+      <artifactId>powermock-module-junit4</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.powermock</groupId>\n+      <artifactId>powermock-api-easymock</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n </project>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/pom.xml",
                "sha": "0443490549b0a3db78716fbbcd5887eeaa4cb02e",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 7,
                "filename": "extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java",
                "patch": "@@ -31,13 +31,13 @@\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.ListeningExecutorService;\n import com.google.common.util.concurrent.MoreExecutors;\n-\n import io.druid.concurrent.Execs;\n import io.druid.java.util.common.IAE;\n import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.query.extraction.MapLookupExtractor;\n+import io.druid.server.lookup.namespace.cache.CacheHandler;\n import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n import kafka.consumer.ConsumerConfig;\n import kafka.consumer.KafkaStream;\n@@ -82,6 +82,7 @@ public String fromBytes(byte[] bytes)\n   private final String factoryId;\n   private final AtomicReference<Map<String, String>> mapRef = new AtomicReference<>(null);\n   private final AtomicBoolean started = new AtomicBoolean(false);\n+  private CacheHandler cacheHandler;\n \n   private volatile ConsumerConnector consumerConnector;\n   private volatile ListenableFuture<?> future = null;\n@@ -182,7 +183,8 @@ public boolean start()\n       kafkaProperties.setProperty(\"group.id\", factoryId);\n       final String topic = getKafkaTopic();\n       LOG.debug(\"About to listen to topic [%s] with group.id [%s]\", topic, factoryId);\n-      final Map<String, String> map = cacheManager.getCacheMap(factoryId);\n+      cacheHandler = cacheManager.createCache();\n+      final Map<String, String> map = cacheHandler.getCache();\n       mapRef.set(map);\n       // Enable publish-subscribe\n       kafkaProperties.setProperty(\"auto.offset.reset\", \"smallest\");\n@@ -280,7 +282,7 @@ public void onFailure(Throwable t)\n           LOG.warn(\"Could not cancel kafka listening thread\");\n         }\n         LOG.error(e, \"Failed to start kafka extraction factory\");\n-        cacheManager.delete(factoryId);\n+        cacheHandler.close();\n         return false;\n       }\n \n@@ -319,10 +321,7 @@ public boolean close()\n           return false;\n         }\n       }\n-      if (!cacheManager.delete(factoryId)) {\n-        LOG.error(\"Error removing [%s] for topic [%s] from cache\", factoryId, getKafkaTopic());\n-        return false;\n-      }\n+      cacheHandler.close();\n       return true;\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/src/main/java/io/druid/query/lookup/KafkaLookupExtractorFactory.java",
                "sha": "da6e8edf47357a591da923f7b11582c2909953f1",
                "status": "modified"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/src/test/java/io/druid/query/lookup/KafkaLookupExtractorFactoryTest.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-extraction-namespace/src/test/java/io/druid/query/lookup/KafkaLookupExtractorFactoryTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 79,
                "filename": "extensions-core/kafka-extraction-namespace/src/test/java/io/druid/query/lookup/KafkaLookupExtractorFactoryTest.java",
                "patch": "@@ -26,9 +26,9 @@\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n-\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.StringUtils;\n+import io.druid.server.lookup.namespace.cache.CacheHandler;\n import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n import kafka.consumer.ConsumerIterator;\n import kafka.consumer.KafkaStream;\n@@ -41,6 +41,11 @@\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n@@ -53,14 +58,30 @@\n \n import static io.druid.query.lookup.KafkaLookupExtractorFactory.DEFAULT_STRING_DECODER;\n \n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({\n+    NamespaceExtractionCacheManager.class,\n+    CacheHandler.class\n+})\n+@PowerMockIgnore({\n+    \"javax.management.*\",\n+    \"javax.net.ssl.*\",\n+    \"org.apache.logging.*\",\n+    \"org.slf4j.*\",\n+    \"com.sun.*\",\n+    \"javax.script.*\",\n+    \"jdk.*\"\n+})\n public class KafkaLookupExtractorFactoryTest\n {\n   private static final String TOPIC = \"some_topic\";\n   private static final Map<String, String> DEFAULT_PROPERTIES = ImmutableMap.of(\n       \"some.property\", \"some.value\"\n   );\n   private final ObjectMapper mapper = new DefaultObjectMapper();\n-  final NamespaceExtractionCacheManager cacheManager = EasyMock.createStrictMock(NamespaceExtractionCacheManager.class);\n+  private final NamespaceExtractionCacheManager cacheManager = PowerMock.createStrictMock(NamespaceExtractionCacheManager.class);\n+  private final CacheHandler cacheHandler = PowerMock.createStrictMock(CacheHandler.class);\n+\n \n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();\n@@ -258,9 +279,9 @@ public void testStopWithoutStart()\n   @Test\n   public void testStartStop()\n   {\n-    final KafkaStream<String, String> kafkaStream = EasyMock.createStrictMock(KafkaStream.class);\n-    final ConsumerIterator<String, String> consumerIterator = EasyMock.createStrictMock(ConsumerIterator.class);\n-    final ConsumerConnector consumerConnector = EasyMock.createStrictMock(ConsumerConnector.class);\n+    final KafkaStream<String, String> kafkaStream = PowerMock.createStrictMock(KafkaStream.class);\n+    final ConsumerIterator<String, String> consumerIterator = PowerMock.createStrictMock(ConsumerIterator.class);\n+    final ConsumerConnector consumerConnector = PowerMock.createStrictMock(ConsumerConnector.class);\n     EasyMock.expect(consumerConnector.createMessageStreamsByFilter(\n         EasyMock.anyObject(TopicFilter.class),\n         EasyMock.anyInt(),\n@@ -270,10 +291,12 @@ public void testStartStop()\n     )).andReturn(ImmutableList.of(kafkaStream)).once();\n     EasyMock.expect(kafkaStream.iterator()).andReturn(consumerIterator).anyTimes();\n     EasyMock.expect(consumerIterator.hasNext()).andAnswer(getBlockingAnswer()).anyTimes();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n+    EasyMock.expect(cacheManager.createCache())\n+            .andReturn(cacheHandler)\n             .once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(true).once();\n+    EasyMock.expect(cacheHandler.getCache()).andReturn(new ConcurrentHashMap<String, String>()).once();\n+    cacheHandler.close();\n+    EasyMock.expectLastCall();\n \n     final AtomicBoolean threadWasInterrupted = new AtomicBoolean(false);\n     consumerConnector.shutdown();\n@@ -286,7 +309,7 @@ public Object answer() throws Throwable {\n       }\n     }).times(2);\n \n-    EasyMock.replay(cacheManager, kafkaStream, consumerConnector, consumerIterator);\n+    PowerMock.replay(cacheManager, cacheHandler, kafkaStream, consumerConnector, consumerIterator);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n@@ -307,18 +330,20 @@ ConsumerConnector buildConnector(Properties properties)\n     Assert.assertTrue(factory.getFuture().isDone());\n     Assert.assertFalse(threadWasInterrupted.get());\n \n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager, cacheHandler);\n   }\n \n \n   @Test\n   public void testStartFailsFromTimeout() throws Exception\n   {\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n+    EasyMock.expect(cacheManager.createCache())\n+            .andReturn(cacheHandler)\n             .once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    EasyMock.expect(cacheHandler.getCache()).andReturn(new ConcurrentHashMap<String, String>()).once();\n+    cacheHandler.close();\n+    EasyMock.expectLastCall();\n+    PowerMock.replay(cacheManager, cacheHandler);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n@@ -343,56 +368,15 @@ ConsumerConnector buildConnector(Properties properties)\n     Assert.assertFalse(factory.start());\n     Assert.assertTrue(factory.getFuture().isDone());\n     Assert.assertTrue(factory.getFuture().isCancelled());\n-    EasyMock.verify(cacheManager);\n-  }\n-\n-  @Test\n-  public void testStopDeleteError()\n-  {\n-    final KafkaStream<String, String> kafkaStream = EasyMock.createStrictMock(KafkaStream.class);\n-    final ConsumerIterator<String, String> consumerIterator = EasyMock.createStrictMock(ConsumerIterator.class);\n-    final ConsumerConnector consumerConnector = EasyMock.createStrictMock(ConsumerConnector.class);\n-    EasyMock.expect(consumerConnector.createMessageStreamsByFilter(\n-        EasyMock.anyObject(TopicFilter.class),\n-        EasyMock.anyInt(),\n-        EasyMock.eq(\n-            DEFAULT_STRING_DECODER),\n-        EasyMock.eq(DEFAULT_STRING_DECODER)\n-    )).andReturn(ImmutableList.of(kafkaStream)).once();\n-    EasyMock.expect(kafkaStream.iterator()).andReturn(consumerIterator).anyTimes();\n-    EasyMock.expect(consumerIterator.hasNext()).andAnswer(getBlockingAnswer()).anyTimes();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n-            .once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(false).once();\n-    consumerConnector.shutdown();\n-    EasyMock.expectLastCall().anyTimes();\n-\n-    EasyMock.replay(cacheManager, kafkaStream, consumerConnector, consumerIterator);\n-    final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n-        cacheManager,\n-        TOPIC,\n-        ImmutableMap.of(\"zookeeper.connect\", \"localhost\")\n-    )\n-    {\n-      @Override\n-      ConsumerConnector buildConnector(Properties properties)\n-      {\n-        return consumerConnector;\n-      }\n-    };\n-    Assert.assertTrue(factory.start());\n-    Assert.assertFalse(factory.close());\n-    EasyMock.verify(cacheManager, kafkaStream, consumerConnector, consumerIterator);\n+    PowerMock.verify(cacheManager, cacheHandler);\n   }\n \n-\n   @Test\n   public void testStartStopStart()\n   {\n-    final KafkaStream<String, String> kafkaStream = EasyMock.createStrictMock(KafkaStream.class);\n-    final ConsumerIterator<String, String> consumerIterator = EasyMock.createStrictMock(ConsumerIterator.class);\n-    final ConsumerConnector consumerConnector = EasyMock.createStrictMock(ConsumerConnector.class);\n+    final KafkaStream<String, String> kafkaStream = PowerMock.createStrictMock(KafkaStream.class);\n+    final ConsumerIterator<String, String> consumerIterator = PowerMock.createStrictMock(ConsumerIterator.class);\n+    final ConsumerConnector consumerConnector = PowerMock.createStrictMock(ConsumerConnector.class);\n     EasyMock.expect(consumerConnector.createMessageStreamsByFilter(\n         EasyMock.anyObject(TopicFilter.class),\n         EasyMock.anyInt(),\n@@ -402,13 +386,15 @@ public void testStartStopStart()\n     )).andReturn(ImmutableList.of(kafkaStream)).once();\n     EasyMock.expect(kafkaStream.iterator()).andReturn(consumerIterator).anyTimes();\n     EasyMock.expect(consumerIterator.hasNext()).andAnswer(getBlockingAnswer()).anyTimes();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n+    EasyMock.expect(cacheManager.createCache())\n+            .andReturn(cacheHandler)\n             .once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(true).once();\n+    EasyMock.expect(cacheHandler.getCache()).andReturn(new ConcurrentHashMap<String, String>()).once();\n+    cacheHandler.close();\n+    EasyMock.expectLastCall().once();\n     consumerConnector.shutdown();\n     EasyMock.expectLastCall().times(2);\n-    EasyMock.replay(cacheManager, kafkaStream, consumerConnector, consumerIterator);\n+    PowerMock.replay(cacheManager, cacheHandler, kafkaStream, consumerConnector, consumerIterator);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n@@ -424,15 +410,15 @@ ConsumerConnector buildConnector(Properties properties)\n     Assert.assertTrue(factory.start());\n     Assert.assertTrue(factory.close());\n     Assert.assertFalse(factory.start());\n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager, cacheHandler);\n   }\n \n   @Test\n   public void testStartStartStop()\n   {\n-    final KafkaStream<String, String> kafkaStream = EasyMock.createStrictMock(KafkaStream.class);\n-    final ConsumerIterator<String, String> consumerIterator = EasyMock.createStrictMock(ConsumerIterator.class);\n-    final ConsumerConnector consumerConnector = EasyMock.createStrictMock(ConsumerConnector.class);\n+    final KafkaStream<String, String> kafkaStream = PowerMock.createStrictMock(KafkaStream.class);\n+    final ConsumerIterator<String, String> consumerIterator = PowerMock.createStrictMock(ConsumerIterator.class);\n+    final ConsumerConnector consumerConnector = PowerMock.createStrictMock(ConsumerConnector.class);\n     EasyMock.expect(consumerConnector.createMessageStreamsByFilter(\n         EasyMock.anyObject(TopicFilter.class),\n         EasyMock.anyInt(),\n@@ -442,13 +428,15 @@ public void testStartStartStop()\n     )).andReturn(ImmutableList.of(kafkaStream)).once();\n     EasyMock.expect(kafkaStream.iterator()).andReturn(consumerIterator).anyTimes();\n     EasyMock.expect(consumerIterator.hasNext()).andAnswer(getBlockingAnswer()).anyTimes();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n+    EasyMock.expect(cacheManager.createCache())\n+            .andReturn(cacheHandler)\n             .once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(true).once();\n+    EasyMock.expect(cacheHandler.getCache()).andReturn(new ConcurrentHashMap<String, String>()).once();\n+    cacheHandler.close();\n+    EasyMock.expectLastCall().once();\n     consumerConnector.shutdown();\n     EasyMock.expectLastCall().times(3);\n-    EasyMock.replay(cacheManager, kafkaStream, consumerConnector, consumerIterator);\n+    PowerMock.replay(cacheManager, cacheHandler, kafkaStream, consumerConnector, consumerIterator);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n@@ -467,54 +455,54 @@ ConsumerConnector buildConnector(Properties properties)\n     Assert.assertTrue(factory.start());\n     Assert.assertTrue(factory.close());\n     Assert.assertTrue(factory.close());\n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager, cacheHandler);\n   }\n \n   @Test\n   public void testStartFailsOnMissingConnect()\n   {\n     expectedException.expectMessage(\"zookeeper.connect required property\");\n-    EasyMock.replay(cacheManager);\n+    PowerMock.replay(cacheManager);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n         ImmutableMap.<String, String>of()\n     );\n     Assert.assertTrue(factory.start());\n     Assert.assertTrue(factory.close());\n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager);\n   }\n \n   @Test\n   public void testStartFailsOnGroupID()\n   {\n     expectedException.expectMessage(\n         \"Cannot set kafka property [group.id]. Property is randomly generated for you. Found\");\n-    EasyMock.replay(cacheManager);\n+    PowerMock.replay(cacheManager);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n         ImmutableMap.of(\"group.id\", \"make me fail\")\n     );\n     Assert.assertTrue(factory.start());\n     Assert.assertTrue(factory.close());\n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager);\n   }\n \n   @Test\n   public void testStartFailsOnAutoOffset()\n   {\n     expectedException.expectMessage(\n         \"Cannot set kafka property [auto.offset.reset]. Property will be forced to [smallest]. Found \");\n-    EasyMock.replay(cacheManager);\n+    PowerMock.replay(cacheManager);\n     final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(\n         cacheManager,\n         TOPIC,\n         ImmutableMap.of(\"auto.offset.reset\", \"make me fail\")\n     );\n     Assert.assertTrue(factory.start());\n     Assert.assertTrue(factory.close());\n-    EasyMock.verify(cacheManager);\n+    PowerMock.verify(cacheManager);\n   }\n \n   @Test\n@@ -531,7 +519,7 @@ public void testFailsGetNotStarted()\n   @Test\n   public void testSerDe() throws Exception\n   {\n-    final NamespaceExtractionCacheManager cacheManager = EasyMock.createStrictMock(NamespaceExtractionCacheManager.class);\n+    final NamespaceExtractionCacheManager cacheManager = PowerMock.createStrictMock(NamespaceExtractionCacheManager.class);\n     final String kafkaTopic = \"some_topic\";\n     final Map<String, String> kafkaProperties = ImmutableMap.of(\"some_key\", \"some_value\");\n     final long connectTimeout = 999;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/kafka-extraction-namespace/src/test/java/io/druid/query/lookup/KafkaLookupExtractorFactoryTest.java",
                "sha": "ee3bf1bdf051796eeaefc3112b2ad8d6aaf25b21",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/pom.xml",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/pom.xml?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/pom.xml",
                "patch": "@@ -82,5 +82,15 @@\n       <artifactId>easymock</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.powermock</groupId>\n+      <artifactId>powermock-module-junit4</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.powermock</groupId>\n+      <artifactId>powermock-api-easymock</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n </project>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/pom.xml",
                "sha": "1cd3365d7e9a09ab9f024006fb755e0818fb1976",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 52,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java",
                "patch": "@@ -32,7 +32,7 @@\n import io.druid.java.util.common.logger.Logger;\n import io.druid.query.extraction.MapLookupExtractor;\n import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n \n import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n@@ -54,9 +54,9 @@\n     CLASS_CACHE_KEY = ByteBuffer.allocate(keyUtf8.length + 1).put(keyUtf8).put((byte) 0xFF).array();\n   }\n \n-  private volatile boolean started = false;\n+  CacheScheduler.Entry entry = null;\n   private final ReadWriteLock startStopSync = new ReentrantReadWriteLock();\n-  private final NamespaceExtractionCacheManager manager;\n+  private final CacheScheduler cacheScheduler;\n   private final LookupIntrospectHandler lookupIntrospectHandler;\n   private final ExtractionNamespace extractionNamespace;\n   private final long firstCacheTimeout;\n@@ -69,7 +69,7 @@ public NamespaceLookupExtractorFactory(\n       @JsonProperty(\"extractionNamespace\") ExtractionNamespace extractionNamespace,\n       @JsonProperty(\"firstCacheTimeout\") long firstCacheTimeout,\n       @JsonProperty(\"injective\") boolean injective,\n-      @JacksonInject final NamespaceExtractionCacheManager manager\n+      @JacksonInject final CacheScheduler cacheScheduler\n   )\n   {\n     this.extractionNamespace = Preconditions.checkNotNull(\n@@ -79,18 +79,18 @@ public NamespaceLookupExtractorFactory(\n     this.firstCacheTimeout = firstCacheTimeout;\n     Preconditions.checkArgument(this.firstCacheTimeout >= 0);\n     this.injective = injective;\n-    this.manager = manager;\n+    this.cacheScheduler = cacheScheduler;\n     this.extractorID = String.format(\"namespace-factory-%s-%s\", extractionNamespace, UUID.randomUUID().toString());\n-    this.lookupIntrospectHandler = new NamespaceLookupIntrospectHandler(this, manager, extractorID);\n+    this.lookupIntrospectHandler = new NamespaceLookupIntrospectHandler(this);\n   }\n \n   @VisibleForTesting\n   public NamespaceLookupExtractorFactory(\n       ExtractionNamespace extractionNamespace,\n-      NamespaceExtractionCacheManager manager\n+      CacheScheduler cacheScheduler\n   )\n   {\n-    this(extractionNamespace, 60000, false, manager);\n+    this(extractionNamespace, 60000, false, cacheScheduler);\n   }\n \n   @Override\n@@ -99,32 +99,29 @@ public boolean start()\n     final Lock writeLock = startStopSync.writeLock();\n     try {\n       writeLock.lockInterruptibly();\n-    }\n-    catch (InterruptedException e) {\n-      throw Throwables.propagate(e);\n-    }\n-    try {\n-      if (started) {\n-        LOG.warn(\"Already started! [%s]\", extractorID);\n-        return true;\n-      }\n-      if (firstCacheTimeout > 0) {\n-        if (!manager.scheduleAndWait(extractorID, extractionNamespace, firstCacheTimeout)) {\n-          LOG.error(\"Failed to schedule and wait for lookup [%s]\", extractorID);\n-          return false;\n+      try {\n+        if (entry != null) {\n+          LOG.warn(\"Already started! [%s]\", extractorID);\n+          return true;\n         }\n-      } else {\n-        if (!manager.scheduleOrUpdate(extractorID, extractionNamespace)) {\n-          LOG.error(\"Failed to schedule lookup [%s]\", extractorID);\n-          return false;\n+        if (firstCacheTimeout > 0) {\n+          entry = cacheScheduler.scheduleAndWait(extractionNamespace, firstCacheTimeout);\n+          if (entry == null) {\n+            LOG.error(\"Failed to schedule and wait for lookup [%s]\", extractorID);\n+            return false;\n+          }\n+        } else {\n+          entry = cacheScheduler.schedule(extractionNamespace);\n         }\n+        LOG.debug(\"NamespaceLookupExtractorFactory[%s] started\", extractorID);\n+        return true;\n+      }\n+      finally {\n+        writeLock.unlock();\n       }\n-      LOG.debug(\"NamespaceLookupExtractorFactory[%s] started\", extractorID);\n-      started = true;\n-      return true;\n     }\n-    finally {\n-      writeLock.unlock();\n+    catch (InterruptedException e) {\n+      throw Throwables.propagate(e);\n     }\n   }\n \n@@ -139,12 +136,13 @@ public boolean close()\n       throw Throwables.propagate(e);\n     }\n     try {\n-      if (!started) {\n+      if (entry == null) {\n         LOG.warn(\"Not started! [%s]\", extractorID);\n         return true;\n       }\n-      started = false;\n-      return manager.checkedDelete(extractorID);\n+      entry.close();\n+      entry = null;\n+      return true;\n     }\n     finally {\n       writeLock.unlock();\n@@ -191,7 +189,7 @@ public boolean isInjective()\n     return injective;\n   }\n \n-  // Grab the latest snapshot from the cache manager\n+  // Grab the latest snapshot from the CacheScheduler's entry\n   @Override\n   public LookupExtractor get()\n   {\n@@ -203,26 +201,17 @@ public LookupExtractor get()\n       throw Throwables.propagate(e);\n     }\n     try {\n-      if (!started) {\n+      if (entry == null) {\n         throw new ISE(\"Factory [%s] not started\", extractorID);\n       }\n-      String preVersion = null, postVersion = null;\n-      Map<String, String> map = null;\n-      // Make sure we absolutely know what version of map we grabbed (for caching purposes)\n-      do {\n-        preVersion = manager.getVersion(extractorID);\n-        if (preVersion == null) {\n-          throw new ISE(\"Namespace vanished for [%s]\", extractorID);\n-        }\n-        map = manager.getCacheMap(extractorID);\n-        postVersion = manager.getVersion(extractorID);\n-        if (postVersion == null) {\n-          // We lost some horrible race... make sure we clean up\n-          manager.delete(extractorID);\n-          throw new ISE(\"Lookup [%s] is deleting\", extractorID);\n-        }\n-      } while (!preVersion.equals(postVersion));\n-      final byte[] v = StringUtils.toUtf8(postVersion);\n+      final CacheScheduler.CacheState cacheState = entry.getCacheState();\n+      if (cacheState instanceof CacheScheduler.NoCache) {\n+        final String noCacheReason = ((CacheScheduler.NoCache) cacheState).name();\n+        throw new ISE(\"%s: %s, extractorID = %s\", entry, noCacheReason, extractorID);\n+      }\n+      CacheScheduler.VersionedCache versionedCache = (CacheScheduler.VersionedCache) cacheState;\n+      Map<String, String> map = versionedCache.getCache();\n+      final byte[] v = StringUtils.toUtf8(versionedCache.getVersion());\n       final byte[] id = StringUtils.toUtf8(extractorID);\n       return new MapLookupExtractor(map, isInjective())\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupExtractorFactory.java",
                "sha": "208baeaa3cb7071510fd0cc534f155402988c296",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupIntrospectHandler.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupIntrospectHandler.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 13,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupIntrospectHandler.java",
                "patch": "@@ -24,7 +24,7 @@\n import io.druid.common.utils.ServletResourceUtils;\n import io.druid.java.util.common.ISE;\n import io.druid.query.extraction.MapLookupExtractor;\n-import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n \n import javax.ws.rs.GET;\n import javax.ws.rs.Path;\n@@ -36,17 +36,12 @@\n public class NamespaceLookupIntrospectHandler implements LookupIntrospectHandler\n {\n   private final NamespaceLookupExtractorFactory factory;\n-  private final String extractorID;\n-  private final NamespaceExtractionCacheManager manager;\n-  public NamespaceLookupIntrospectHandler(\n-      NamespaceLookupExtractorFactory factory,\n-      NamespaceExtractionCacheManager manager,\n-      String extractorID\n-  ) {\n+\n+  public NamespaceLookupIntrospectHandler(NamespaceLookupExtractorFactory factory)\n+  {\n     this.factory = factory;\n-    this.extractorID = extractorID;\n-    this.manager = manager;\n   }\n+\n   @GET\n   @Path(\"/keys\")\n   @Produces(MediaType.APPLICATION_JSON)\n@@ -78,11 +73,11 @@ public Response getValues()\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getVersion()\n   {\n-    final String version = manager.getVersion(extractorID);\n-    if (null == version) {\n-      // Handle race between delete and this method being called\n+    final CacheScheduler.CacheState cacheState = factory.entry.getCacheState();\n+    if (cacheState instanceof CacheScheduler.NoCache) {\n       return Response.status(Response.Status.NOT_FOUND).build();\n     } else {\n+      String version = ((CacheScheduler.VersionedCache) cacheState).getVersion();\n       return Response.ok(ImmutableMap.of(\"version\", version)).build();\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/NamespaceLookupIntrospectHandler.java",
                "sha": "9df6973019311f3780989920182284856e009a01",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/namespace/ExtractionNamespaceCacheFactory.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/namespace/ExtractionNamespaceCacheFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 19,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/namespace/ExtractionNamespaceCacheFactory.java",
                "patch": "@@ -19,32 +19,35 @@\n \n package io.druid.query.lookup.namespace;\n \n-import java.util.Map;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n+\n+import javax.annotation.Nullable;\n \n /**\n  *\n  */\n public interface ExtractionNamespaceCacheFactory<T extends ExtractionNamespace>\n {\n   /**\n-   * This function is called once if `ExtractionNamespace.getUpdateMs() == 0`, or every update if\n-   * `ExtractionNamespace.getUpdateMs() > 0`\n-   * For ExtractionNamespace which have the NamespaceExtractionCacheManager handle regular updates, this function\n-   * is used to populate the namespace cache each time.\n-   * For ExtractionNamespace implementations which do not have regular updates, this function can be used to\n-   * initialize resources.\n-   * If the returned version is the same as what is passed in as lastVersion, then no swap takes place, and the swap\n-   * is discarded.\n-   *\n-   * @param id                  The ID of ExtractionNamespace\n-   * @param extractionNamespace The ExtractionNamespace for which to populate data.\n-   * @param lastVersion         The version which was last cached\n-   * @param swap                The temporary Map into which data may be placed and will be \"swapped\" with the proper\n-   *                            namespace Map in NamespaceExtractionCacheManager. Implementations which cannot offer\n-   *                            a swappable cache of the data may ignore this but must make sure `buildFn(...)` returns\n-   *                            a proper Function.\n+   * If the lookup source, encapsulated by this {@code ExtractionNamespaceCacheFactory}, has data newer than identified\n+   * by the given {@code lastVersion} (which is null at the first run of this method, or the version from the previous\n+   * run), this method creates a new {@code CacheScheduler.VersionedCache} with {@link\n+   * CacheScheduler#createVersionedCache}, called on the given {@code scheduler}, with the version string identifying\n+   * the current version of lookup source, populates the created {@code VersionedCache} and returns it. If the lookup\n+   * source is up-to-date, this methods returns null.\n    *\n-   * @return return the (new) version string used in the populating\n+   * @param namespace The ExtractionNamespace for which to populate data.\n+   * @param id An object uniquely corresponding to the {@link CacheScheduler.Entry}, for which this populateCache()\n+   *           method is called. Also it has the same toString() representation, that is useful for logging\n+   * @param lastVersion The version which was last cached\n+   * @param scheduler Should be used only to call {@link CacheScheduler#createVersionedCache}.\n+   * @return the new cache along with the new version, or null if the last version is up-to-date.\n    */\n-  String populateCache(String id, T extractionNamespace, String lastVersion, Map<String, String> swap) throws Exception;\n+  @Nullable\n+  CacheScheduler.VersionedCache populateCache(\n+      T namespace,\n+      CacheScheduler.EntryImpl<T> id,\n+      String lastVersion,\n+      CacheScheduler scheduler\n+  ) throws Exception;\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/query/lookup/namespace/ExtractionNamespaceCacheFactory.java",
                "sha": "2f423b68fa07ff2b6f9829dfdc15e134f5ebf30c",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/JDBCExtractionNamespaceCacheFactory.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/JDBCExtractionNamespaceCacheFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 19,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/JDBCExtractionNamespaceCacheFactory.java",
                "patch": "@@ -24,13 +24,15 @@\n import io.druid.java.util.common.logger.Logger;\n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import io.druid.query.lookup.namespace.JDBCExtractionNamespace;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n import org.skife.jdbi.v2.DBI;\n import org.skife.jdbi.v2.Handle;\n import org.skife.jdbi.v2.StatementContext;\n import org.skife.jdbi.v2.tweak.HandleCallback;\n import org.skife.jdbi.v2.tweak.ResultSetMapper;\n import org.skife.jdbi.v2.util.TimestampMapper;\n \n+import javax.annotation.Nullable;\n import java.sql.ResultSet;\n import java.sql.SQLException;\n import java.sql.Timestamp;\n@@ -42,32 +44,34 @@\n /**\n  *\n  */\n-public class JDBCExtractionNamespaceCacheFactory\n+public final class JDBCExtractionNamespaceCacheFactory\n     implements ExtractionNamespaceCacheFactory<JDBCExtractionNamespace>\n {\n   private static final Logger LOG = new Logger(JDBCExtractionNamespaceCacheFactory.class);\n-  private final ConcurrentMap<String, DBI> dbiCache = new ConcurrentHashMap<>();\n+  private final ConcurrentMap<CacheScheduler.EntryImpl<JDBCExtractionNamespace>, DBI> dbiCache =\n+      new ConcurrentHashMap<>();\n \n   @Override\n-  public String populateCache(\n-      final String id,\n+  @Nullable\n+  public CacheScheduler.VersionedCache populateCache(\n       final JDBCExtractionNamespace namespace,\n+      final CacheScheduler.EntryImpl<JDBCExtractionNamespace> entryId,\n       final String lastVersion,\n-      final Map<String, String> cache\n-  ) throws Exception\n+      final CacheScheduler scheduler\n+  )\n   {\n     final long lastCheck = lastVersion == null ? JodaUtils.MIN_INSTANT : Long.parseLong(lastVersion);\n-    final Long lastDBUpdate = lastUpdates(id, namespace);\n+    final Long lastDBUpdate = lastUpdates(entryId, namespace);\n     if (lastDBUpdate != null && lastDBUpdate <= lastCheck) {\n-      return lastVersion;\n+      return null;\n     }\n     final long dbQueryStart = System.currentTimeMillis();\n-    final DBI dbi = ensureDBI(id, namespace);\n+    final DBI dbi = ensureDBI(entryId, namespace);\n     final String table = namespace.getTable();\n     final String valueColumn = namespace.getValueColumn();\n     final String keyColumn = namespace.getKeyColumn();\n \n-    LOG.debug(\"Updating [%s]\", id);\n+    LOG.debug(\"Updating %s\", entryId);\n     final List<Pair<String, String>> pairs = dbi.withHandle(\n         new HandleCallback<List<Pair<String, String>>>()\n         {\n@@ -102,20 +106,35 @@ public String populateCache(\n           }\n         }\n     );\n-    for (Pair<String, String> pair : pairs) {\n-      cache.put(pair.lhs, pair.rhs);\n-    }\n-    LOG.info(\"Finished loading %d values for namespace[%s]\", cache.size(), id);\n+    final String newVersion;\n     if (lastDBUpdate != null) {\n-      return lastDBUpdate.toString();\n+      newVersion = lastDBUpdate.toString();\n     } else {\n-      return String.format(\"%d\", dbQueryStart);\n+      newVersion = String.format(\"%d\", dbQueryStart);\n+    }\n+    final CacheScheduler.VersionedCache versionedCache = scheduler.createVersionedCache(entryId, newVersion);\n+    try {\n+      final Map<String, String> cache = versionedCache.getCache();\n+      for (Pair<String, String> pair : pairs) {\n+        cache.put(pair.lhs, pair.rhs);\n+      }\n+      LOG.info(\"Finished loading %d values for %s\", cache.size(), entryId);\n+      return versionedCache;\n+    }\n+    catch (Throwable t) {\n+      try {\n+        versionedCache.close();\n+      }\n+      catch (Exception e) {\n+        t.addSuppressed(e);\n+      }\n+      throw t;\n     }\n   }\n \n-  private DBI ensureDBI(String id, JDBCExtractionNamespace namespace)\n+  private DBI ensureDBI(CacheScheduler.EntryImpl<JDBCExtractionNamespace> id, JDBCExtractionNamespace namespace)\n   {\n-    final String key = id;\n+    final CacheScheduler.EntryImpl<JDBCExtractionNamespace> key = id;\n     DBI dbi = null;\n     if (dbiCache.containsKey(key)) {\n       dbi = dbiCache.get(key);\n@@ -132,7 +151,7 @@ private DBI ensureDBI(String id, JDBCExtractionNamespace namespace)\n     return dbi;\n   }\n \n-  private Long lastUpdates(String id, JDBCExtractionNamespace namespace)\n+  private Long lastUpdates(CacheScheduler.EntryImpl<JDBCExtractionNamespace> id, JDBCExtractionNamespace namespace)\n   {\n     final DBI dbi = ensureDBI(id, namespace);\n     final String table = namespace.getTable();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/JDBCExtractionNamespaceCacheFactory.java",
                "sha": "403bbaa4112e2bbf10fcb0311422460c3d405b7d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 1,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "patch": "@@ -30,8 +30,8 @@\n import io.druid.guice.PolyBind;\n import io.druid.initialization.DruidModule;\n import io.druid.query.lookup.NamespaceLookupExtractorFactory;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n+import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.JDBCExtractionNamespace;\n import io.druid.query.lookup.namespace.StaticMapExtractionNamespace;\n import io.druid.query.lookup.namespace.URIExtractionNamespace;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "sha": "72dbbd2a9ec5833078d53515cfb6888dacd33ddd",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactory.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 13,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactory.java",
                "patch": "@@ -21,32 +21,45 @@\n \n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import io.druid.query.lookup.namespace.StaticMapExtractionNamespace;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n \n-import java.util.Map;\n+import javax.annotation.Nullable;\n import java.util.UUID;\n \n-public class StaticMapExtractionNamespaceCacheFactory\n-    implements ExtractionNamespaceCacheFactory<StaticMapExtractionNamespace>\n+public final class StaticMapExtractionNamespaceCacheFactory implements ExtractionNamespaceCacheFactory<StaticMapExtractionNamespace>\n {\n   private final String version = UUID.randomUUID().toString();\n \n   @Override\n-  public String populateCache(\n-      final String id,\n-      final StaticMapExtractionNamespace extractionNamespace,\n+  @Nullable\n+  public CacheScheduler.VersionedCache populateCache(\n+      final StaticMapExtractionNamespace namespace,\n+      final CacheScheduler.EntryImpl<StaticMapExtractionNamespace> id,\n       final String lastVersion,\n-      final Map<String, String> swap\n+      final CacheScheduler scheduler\n   )\n   {\n     if (lastVersion != null) {\n-      // Throwing AssertionError, because NamespaceExtractionCacheManager doesn't suppress Errors and will stop trying\n-      // to update the cache periodically.\n+      // Throwing AssertionError, because CacheScheduler doesn't suppress Errors and will stop trying to update\n+      // the cache periodically.\n       throw new AssertionError(\n-          \"StaticMapExtractionNamespaceCacheFactory could only be configured for a namespace which is scheduled \" +\n-          \"to be updated once, not periodically. Last version: `\" + lastVersion + \"`\");\n+          \"StaticMapExtractionNamespaceCacheFactory could only be configured for a namespace which is scheduled \"\n+          + \"to be updated once, not periodically. Last version: `\" + lastVersion + \"`\");\n+    }\n+    CacheScheduler.VersionedCache versionedCache = scheduler.createVersionedCache(id, version);\n+    try {\n+      versionedCache.getCache().putAll(namespace.getMap());\n+      return versionedCache;\n+    }\n+    catch (Throwable t) {\n+      try {\n+        versionedCache.close();\n+      }\n+      catch (Exception e) {\n+        t.addSuppressed(e);\n+      }\n+      throw t;\n     }\n-    swap.putAll(extractionNamespace.getMap());\n-    return version;\n   }\n \n   String getVersion()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactory.java",
                "sha": "74a8ccd7a65b123df05fea10729ab3a41c78df52",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactory.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactory.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 20,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactory.java",
                "patch": "@@ -30,6 +30,7 @@\n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import io.druid.query.lookup.namespace.URIExtractionNamespace;\n import io.druid.segment.loading.URIDataPuller;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n \n import javax.annotation.Nullable;\n import java.io.FileNotFoundException;\n@@ -43,7 +44,7 @@\n /**\n  *\n  */\n-public class URIExtractionNamespaceCacheFactory implements ExtractionNamespaceCacheFactory<URIExtractionNamespace>\n+public final class URIExtractionNamespaceCacheFactory implements ExtractionNamespaceCacheFactory<URIExtractionNamespace>\n {\n   private static final int DEFAULT_NUM_RETRIES = 3;\n   private static final Logger log = new Logger(URIExtractionNamespaceCacheFactory.class);\n@@ -58,11 +59,12 @@ public URIExtractionNamespaceCacheFactory(\n   }\n \n   @Override\n-  public String populateCache(\n-      final String id,\n+  @Nullable\n+  public CacheScheduler.VersionedCache populateCache(\n       final URIExtractionNamespace extractionNamespace,\n+      final CacheScheduler.EntryImpl<URIExtractionNamespace> entryId,\n       @Nullable final String lastVersion,\n-      final Map<String, String> cache\n+      final CacheScheduler scheduler\n   ) throws Exception\n   {\n     final boolean doSearch = extractionNamespace.getUriPrefix() != null;\n@@ -114,23 +116,23 @@ public String populateCache(\n     final String uriPath = uri.getPath();\n \n     return RetryUtils.retry(\n-        new Callable<String>()\n+        new Callable<CacheScheduler.VersionedCache>()\n         {\n           @Override\n-          public String call() throws Exception\n+          public CacheScheduler.VersionedCache call() throws Exception\n           {\n             final String version = puller.getVersion(uri);\n             try {\n               // Important to call equals() against version because lastVersion could be null\n               if (version.equals(lastVersion)) {\n                 log.debug(\n-                    \"URI [%s] for namespace [%s] has the same last modified time [%s] as the last cached. \" +\n+                    \"URI [%s] for [%s] has the same last modified time [%s] as the last cached. \" +\n                     \"Skipping \",\n                     uri.toString(),\n-                    id,\n+                    entryId,\n                     version\n                 );\n-                return lastVersion;\n+                return null;\n               }\n             }\n             catch (NumberFormatException ex) {\n@@ -158,17 +160,30 @@ public InputStream openStream() throws IOException\n                 }\n               };\n             }\n-            final MapPopulator.PopulateResult populateResult = new MapPopulator<>(\n-                extractionNamespace.getNamespaceParseSpec()\n-                                   .getParser()\n-            ).populate(source, cache);\n-            log.info(\n-                \"Finished loading %,d values from %,d lines for namespace [%s]\",\n-                populateResult.getEntries(),\n-                populateResult.getLines(),\n-                id\n-            );\n-            return version;\n+\n+            CacheScheduler.VersionedCache versionedCache = scheduler.createVersionedCache(entryId, version);\n+            try {\n+              final MapPopulator.PopulateResult populateResult = new MapPopulator<>(\n+                  extractionNamespace.getNamespaceParseSpec()\n+                                     .getParser()\n+              ).populate(source, versionedCache.getCache());\n+              log.info(\n+                  \"Finished loading %,d values from %,d lines for [%s]\",\n+                  populateResult.getEntries(),\n+                  populateResult.getLines(),\n+                  entryId\n+              );\n+              return versionedCache;\n+            }\n+            catch (Throwable t) {\n+              try {\n+                versionedCache.close();\n+              }\n+              catch (Exception e) {\n+                t.addSuppressed(e);\n+              }\n+              throw t;\n+            }\n           }\n         },\n         puller.shouldRetryPredicate(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactory.java",
                "sha": "3778421c1737c7169ecbbd3e2f4dfced797dd0a0",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheHandler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheHandler.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheHandler.java",
                "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.server.lookup.namespace.cache;\n+\n+import io.druid.java.util.common.logger.Logger;\n+\n+import java.util.concurrent.ConcurrentMap;\n+\n+public final class CacheHandler implements AutoCloseable\n+{\n+  private static final Logger log = new Logger(CacheHandler.class);\n+\n+  private final NamespaceExtractionCacheManager cacheManager;\n+  private final ConcurrentMap<String, String> cache;\n+  final Object id;\n+\n+  CacheHandler(NamespaceExtractionCacheManager cacheManager, ConcurrentMap<String, String> cache, Object id)\n+  {\n+    log.debug(\"Creating %s\", super.toString());\n+    this.cacheManager = cacheManager;\n+    this.cache = cache;\n+    this.id = id;\n+  }\n+\n+  public ConcurrentMap<String, String> getCache()\n+  {\n+    return cache;\n+  }\n+\n+  public void close()\n+  {\n+    cacheManager.disposeCache(this);\n+    // Log statement after disposeCache(), because logging may fail (e. g. in shutdown hooks)\n+    log.debug(\"Closed %s\", super.toString());\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheHandler.java",
                "sha": "c249661eb63282ba91094620cdbf491884b9ad8d",
                "status": "added"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheProxy.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheProxy.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheProxy.java",
                "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.server.lookup.namespace.cache;\n+\n+import com.google.common.collect.ForwardingConcurrentMap;\n+\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * Used in {@link OffHeapNamespaceExtractionCacheManager#createCache()}\n+ */\n+final class CacheProxy extends ForwardingConcurrentMap<String, String>\n+{\n+  private final ConcurrentMap<String, String> delegate;\n+\n+  CacheProxy(ConcurrentMap<String, String> delegate)\n+  {\n+    this.delegate = delegate;\n+  }\n+\n+  @Override\n+  protected ConcurrentMap<String, String> delegate()\n+  {\n+    return delegate;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheProxy.java",
                "sha": "c56c8f74121e648bc8b3e0f942242117fdcc5d2f",
                "status": "added"
            },
            {
                "additions": 510,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheScheduler.java",
                "changes": 510,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheScheduler.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheScheduler.java",
                "patch": "@@ -0,0 +1,510 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.server.lookup.namespace.cache;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Throwables;\n+import com.google.inject.Inject;\n+import com.metamx.emitter.service.ServiceEmitter;\n+import com.metamx.emitter.service.ServiceMetricEvent;\n+import io.druid.java.util.common.ISE;\n+import io.druid.java.util.common.logger.Logger;\n+import io.druid.query.lookup.namespace.ExtractionNamespace;\n+import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n+import sun.misc.Cleaner;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Usage:\n+ * <pre>{@code\n+ * CacheScheduler.Entry entry = cacheScheduler.schedule(namespace); // or scheduleAndWait(namespace, timeout)\n+ * CacheState cacheState = entry.getCacheState();\n+ * // cacheState could be either NoCache or VersionedCache.\n+ * if (cacheState instanceof NoCache) {\n+ *   // the cache is not yet created, or already closed\n+ * } else if (cacheState instanceof VersionedCache) {\n+ *   Map<String, String> cache = ((VersionedCache) cacheState).getCache(); // use the cache\n+ *   // Although VersionedCache implements AutoCloseable, versionedCache shouldn't be manually closed\n+ *   // when obtained from entry.getCacheState(). If the namespace updates should be ceased completely,\n+ *   // entry.close() (see below) should be called, it will close the last VersionedCache itself.\n+ *   // On scheduled updates, outdated VersionedCaches are also closed automatically.\n+ * }\n+ * ...\n+ * entry.close(); // close the last VersionedCache and unschedule future updates\n+ * }</pre>\n+ */\n+public final class CacheScheduler\n+{\n+  private static final Logger log = new Logger(CacheScheduler.class);\n+\n+  public final class Entry<T extends ExtractionNamespace> implements AutoCloseable\n+  {\n+    private final EntryImpl<T> impl;\n+\n+    private Entry(final T namespace, final ExtractionNamespaceCacheFactory<T> cachePopulator)\n+    {\n+      impl = new EntryImpl<>(namespace, this, cachePopulator);\n+    }\n+\n+    /**\n+     * Returns the last cache state, either {@link NoCache} or {@link VersionedCache}.\n+     */\n+    public CacheState getCacheState()\n+    {\n+      return impl.cacheStateHolder.get();\n+    }\n+\n+    /**\n+     * @return the entry's cache if it is already initialized and not yet closed\n+     * @throws IllegalStateException if the entry's cache is not yet initialized, or {@link #close()} has\n+     * already been called\n+     */\n+    public Map<String, String> getCache()\n+    {\n+      CacheState cacheState = getCacheState();\n+      if (cacheState instanceof VersionedCache) {\n+        return ((VersionedCache) cacheState).getCache();\n+      } else {\n+        throw new ISE(\"Cannot get cache: %s\", cacheState);\n+      }\n+    }\n+\n+    @VisibleForTesting\n+    Future<?> getUpdaterFuture()\n+    {\n+      return impl.updaterFuture;\n+    }\n+\n+    public void awaitTotalUpdates(int totalUpdates) throws InterruptedException\n+    {\n+      impl.updateCounter.awaitTotalUpdates(totalUpdates);\n+    }\n+\n+    void awaitNextUpdates(int nextUpdates) throws InterruptedException\n+    {\n+      impl.updateCounter.awaitNextUpdates(nextUpdates);\n+    }\n+\n+    /**\n+     * Close the last {@link #getCacheState()}, if it is {@link VersionedCache}, and unschedule future updates.\n+     */\n+    @Override\n+    public void close()\n+    {\n+      impl.close();\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+      return impl.toString();\n+    }\n+  }\n+\n+  /**\n+   * This class effectively contains the whole state and most of the logic of {@link Entry}, need to be a separate class\n+   * because the Entry must not be referenced from the runnable executed in {@link #cacheManager}'s ExecutorService,\n+   * that would be a leak preventing the Entry to be collected by GC, and therefore {@link #entryCleaner} to be run by\n+   * the JVM. Also, {@link #entryCleaner} must not reference the Entry through it's Runnable hunk.\n+   */\n+  public class EntryImpl<T extends ExtractionNamespace> implements AutoCloseable {\n+\n+    private final T namespace;\n+    private final String asString;\n+    private final AtomicReference<CacheState> cacheStateHolder = new AtomicReference<CacheState>(NoCache.CACHE_NOT_INITIALIZED);\n+    private final Future<?> updaterFuture;\n+    private final Cleaner entryCleaner;\n+    private final ExtractionNamespaceCacheFactory<T> cachePopulator;\n+    private final UpdateCounter updateCounter = new UpdateCounter();\n+    private final CountDownLatch startLatch = new CountDownLatch(1);\n+\n+    private EntryImpl(final T namespace, final Entry<T> entry, final ExtractionNamespaceCacheFactory<T> cachePopulator)\n+    {\n+      try {\n+        this.namespace = namespace;\n+        this.asString = String.format(\"namespace [%s] : %s\", namespace, super.toString());\n+        this.updaterFuture = schedule(namespace);\n+        this.entryCleaner = createCleaner(entry);\n+        this.cachePopulator = cachePopulator;\n+        activeEntries.incrementAndGet();\n+      }\n+      finally {\n+        startLatch.countDown();\n+      }\n+    }\n+\n+    private Cleaner createCleaner(Entry<T> entry)\n+    {\n+      return Cleaner.create(entry, new Runnable()\n+      {\n+        @Override\n+        public void run()\n+        {\n+          closeFromCleaner();\n+        }\n+      });\n+    }\n+\n+    private Future<?> schedule(final T namespace)\n+    {\n+      final long updateMs = namespace.getPollMs();\n+      Runnable command = new Runnable()\n+      {\n+        @Override\n+        public void run()\n+        {\n+          updateCache();\n+        }\n+      };\n+      if (updateMs > 0) {\n+        return cacheManager.scheduledExecutorService().scheduleAtFixedRate(command, 0, updateMs, TimeUnit.MILLISECONDS);\n+      } else {\n+        return cacheManager.scheduledExecutorService().schedule(command, 0, TimeUnit.MILLISECONDS);\n+      }\n+    }\n+\n+    private void updateCache()\n+    {\n+      try {\n+        // Ensures visibility of the whole EntryImpl's state (fields and their state).\n+        startLatch.await();\n+        CacheState currentCacheState = cacheStateHolder.get();\n+        if (!Thread.currentThread().isInterrupted() && currentCacheState != NoCache.ENTRY_CLOSED) {\n+          final String currentVersion = currentVersionOrNull(currentCacheState);\n+          tryUpdateCache(currentVersion);\n+        }\n+      }\n+      catch (Throwable t) {\n+        try {\n+          close();\n+        }\n+        catch (Exception e) {\n+          t.addSuppressed(e);\n+        }\n+        if (Thread.currentThread().isInterrupted() || t instanceof InterruptedException || t instanceof Error) {\n+          throw Throwables.propagate(t);\n+        }\n+      }\n+    }\n+\n+    private void tryUpdateCache(String currentVersion) throws Exception\n+    {\n+      boolean updatedCacheSuccessfully = false;\n+      VersionedCache newVersionedCache = null;\n+      try {\n+        newVersionedCache = cachePopulator.populateCache(namespace, this, currentVersion, CacheScheduler.this\n+        );\n+        if (newVersionedCache != null) {\n+          CacheState previousCacheState = swapCacheState(newVersionedCache);\n+          if (previousCacheState != NoCache.ENTRY_CLOSED) {\n+            updatedCacheSuccessfully = true;\n+            if (previousCacheState instanceof VersionedCache) {\n+              ((VersionedCache) previousCacheState).close();\n+            }\n+            log.debug(\"%s: the cache was successfully updated\", this);\n+          } else {\n+            newVersionedCache.close();\n+            log.debug(\"%s was closed while the cache was being updated, discarding the update\", this);\n+          }\n+        } else {\n+          log.debug(\"%s: Version `%s` not updated, the cache is not updated\", this, currentVersion);\n+        }\n+      }\n+      catch (Throwable t) {\n+        try {\n+          if (newVersionedCache != null && !updatedCacheSuccessfully) {\n+            newVersionedCache.close();\n+          }\n+          log.error(t, \"Failed to update %s\", this);\n+        }\n+        catch (Exception e) {\n+          t.addSuppressed(e);\n+        }\n+        if (Thread.currentThread().isInterrupted() || t instanceof InterruptedException || t instanceof Error) {\n+          // propagate to the catch block in updateCache()\n+          throw t;\n+        }\n+      }\n+    }\n+\n+    private String currentVersionOrNull(CacheState currentCacheState)\n+    {\n+      if (currentCacheState instanceof VersionedCache) {\n+        return ((VersionedCache) currentCacheState).version;\n+      } else {\n+        return null;\n+      }\n+    }\n+\n+    private CacheState swapCacheState(VersionedCache newVersionedCache)\n+    {\n+      CacheState lastCacheState;\n+      // CAS loop\n+      do {\n+        lastCacheState = cacheStateHolder.get();\n+        if (lastCacheState == NoCache.ENTRY_CLOSED) {\n+          return lastCacheState;\n+        }\n+      } while (!cacheStateHolder.compareAndSet(lastCacheState, newVersionedCache));\n+      updateCounter.update();\n+      return lastCacheState;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+      if (!doClose(true)) {\n+        log.error(\"Cache for %s has already been closed\", this);\n+      }\n+      // This Cleaner.clean() call effectively just removes the Cleaner from the internal linked list of all cleaners.\n+      // It will delegate to closeFromCleaner() which will be a no-op because cacheStateHolder is already set to\n+      // ENTRY_CLOSED.\n+      entryCleaner.clean();\n+    }\n+\n+    private void closeFromCleaner()\n+    {\n+      try {\n+        if (doClose(false)) {\n+          log.error(\"Entry.close() was not called, closed resources by the JVM\");\n+        }\n+      }\n+      catch (Throwable t) {\n+        try {\n+          log.error(t, \"Error while closing %s\", this);\n+        }\n+        catch (Exception e) {\n+          t.addSuppressed(e);\n+        }\n+        Throwables.propagateIfInstanceOf(t, Error.class);\n+        // Must not throw exceptions in the cleaner thread, run by the JVM.\n+      }\n+    }\n+\n+    /**\n+     * @param calledManually true if called manually from {@link #close()}, false if called by the JVM via Cleaner\n+     * @return true if successfully closed, false if has already closed before\n+     */\n+    private boolean doClose(boolean calledManually)\n+    {\n+      CacheState lastCacheState = cacheStateHolder.getAndSet(NoCache.ENTRY_CLOSED);\n+      if (lastCacheState != NoCache.ENTRY_CLOSED) {\n+        try {\n+          log.info(\"Closing %s\", this);\n+          logExecutionError();\n+        }\n+        // Logging (above) is not the main goal of the closing process, so try to cancel the updaterFuture even if\n+        // logging failed for whatever reason.\n+        finally {\n+          activeEntries.decrementAndGet();\n+          updaterFuture.cancel(true);\n+          // If calledManually = false, i. e. called by the JVM via Cleaner.clean(), let the JVM close cache itself\n+          // via it's own Cleaner as well, when the cache becomes unreachable. Because when somebody forgets to call\n+          // entry.close(), it may be harmful to forcibly close the cache, which could still be used, at some\n+          // non-deterministic point of time. Cleaners are introduced to mitigate possible errors, not to escalate them.\n+          if (calledManually && lastCacheState instanceof VersionedCache) {\n+            ((VersionedCache) lastCacheState).cacheHandler.close();\n+          }\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+\n+    private void logExecutionError()\n+    {\n+      if (updaterFuture.isDone()) {\n+        try {\n+          updaterFuture.get();\n+        }\n+        catch (ExecutionException ee) {\n+          log.error(ee.getCause(), \"Error in %s\", this);\n+        }\n+        catch (CancellationException ce) {\n+          log.error(ce, \"Future for %s has already been cancelled\", this);\n+        }\n+        catch (InterruptedException ie) {\n+          Thread.currentThread().interrupt();\n+          throw new RuntimeException(ie);\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+      return asString;\n+    }\n+  }\n+\n+  public interface CacheState\n+  {}\n+\n+  public enum NoCache implements CacheState\n+  {\n+    CACHE_NOT_INITIALIZED,\n+    ENTRY_CLOSED\n+  }\n+\n+  public final class VersionedCache implements CacheState, AutoCloseable\n+  {\n+    final String entryId;\n+    final CacheHandler cacheHandler;\n+    final String version;\n+\n+    private VersionedCache(String entryId, String version)\n+    {\n+      this.entryId = entryId;\n+      this.cacheHandler = cacheManager.createCache();\n+      this.version = version;\n+    }\n+\n+    public Map<String, String> getCache()\n+    {\n+      return cacheHandler.getCache();\n+    }\n+\n+    public String getVersion()\n+    {\n+      return version;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+      cacheHandler.close();\n+      // Log statement after cacheHandler.close(), because logging may fail (e. g. in shutdown hooks)\n+      log.debug(\"Closed version [%s] of %s\", version, entryId);\n+    }\n+  }\n+\n+  private final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespacePopulatorMap;\n+  private final NamespaceExtractionCacheManager cacheManager;\n+  private final AtomicLong updatesStarted = new AtomicLong(0);\n+  private final AtomicInteger activeEntries = new AtomicInteger();\n+\n+  @Inject\n+  public CacheScheduler(\n+      final ServiceEmitter serviceEmitter,\n+      final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespacePopulatorMap,\n+      NamespaceExtractionCacheManager cacheManager\n+  )\n+  {\n+    this.namespacePopulatorMap = namespacePopulatorMap;\n+    this.cacheManager = cacheManager;\n+    cacheManager.scheduledExecutorService().scheduleAtFixedRate(\n+        new Runnable()\n+        {\n+          long priorUpdatesStarted = 0L;\n+\n+          @Override\n+          public void run()\n+          {\n+            try {\n+              final long tasks = updatesStarted.get();\n+              serviceEmitter.emit(\n+                  ServiceMetricEvent.builder()\n+                                    .build(\"namespace/deltaTasksStarted\", tasks - priorUpdatesStarted)\n+              );\n+              priorUpdatesStarted = tasks;\n+            }\n+            catch (Exception e) {\n+              log.error(e, \"Error emitting namespace stats\");\n+              if (Thread.currentThread().isInterrupted()) {\n+                throw Throwables.propagate(e);\n+              }\n+            }\n+          }\n+        },\n+        1,\n+        10, TimeUnit.MINUTES\n+    );\n+  }\n+\n+  /**\n+   * This method should be used from {@link ExtractionNamespaceCacheFactory#populateCache} implementations, to obtain\n+   * a {@link VersionedCache} to be returned.\n+   *\n+   * @param entryId an object uniquely corresponding to the {@link CacheScheduler.Entry}, for which VersionedCache is\n+   *                created\n+   * @param version version, associated with the cache\n+   */\n+  public VersionedCache createVersionedCache(@Nullable EntryImpl<? extends ExtractionNamespace> entryId, String version)\n+  {\n+    updatesStarted.incrementAndGet();\n+    return new VersionedCache(String.valueOf(entryId), version);\n+  }\n+\n+  @VisibleForTesting\n+  long updatesStarted()\n+  {\n+    return updatesStarted.get();\n+  }\n+\n+  @VisibleForTesting\n+  public long getActiveEntries()\n+  {\n+    return activeEntries.get();\n+  }\n+\n+  @Nullable\n+  public Entry scheduleAndWait(ExtractionNamespace namespace, long waitForFirstRunMs) throws InterruptedException\n+  {\n+    final Entry entry = schedule(namespace);\n+    log.debug(\"Scheduled new %s\", entry);\n+    boolean success = false;\n+    try {\n+      success = entry.impl.updateCounter.awaitFirstUpdate(waitForFirstRunMs, TimeUnit.MILLISECONDS);\n+      if (success) {\n+        return entry;\n+      } else {\n+        return null;\n+      }\n+    }\n+    finally {\n+      if (!success) {\n+        // ExecutionException's cause is logged in entry.close()\n+        entry.close();\n+        log.error(\"CacheScheduler[%s] - problem during start or waiting for the first run\", entry);\n+      }\n+    }\n+  }\n+\n+  public <T extends ExtractionNamespace> Entry schedule(final T namespace)\n+  {\n+    final ExtractionNamespaceCacheFactory<T> populator =\n+        (ExtractionNamespaceCacheFactory<T>) namespacePopulatorMap.get(namespace.getClass());\n+    if (populator == null) {\n+      throw new ISE(\"Cannot find populator for namespace [%s]\", namespace);\n+    }\n+    return new Entry<>(namespace, populator);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/CacheScheduler.java",
                "sha": "a33346f0db4d2e7862fb6a1df000ce6c4fe65378",
                "status": "added"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManager.java",
                "changes": 444,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManager.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 410,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManager.java",
                "patch": "@@ -19,105 +19,49 @@\n \n package io.druid.server.lookup.namespace.cache;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Throwables;\n-import com.google.common.util.concurrent.FutureCallback;\n-import com.google.common.util.concurrent.Futures;\n-import com.google.common.util.concurrent.ListenableFuture;\n-import com.google.common.util.concurrent.ListeningScheduledExecutorService;\n-import com.google.common.util.concurrent.MoreExecutors;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import com.metamx.emitter.service.ServiceEmitter;\n-import com.metamx.emitter.service.ServiceMetricEvent;\n-import io.druid.java.util.common.IAE;\n-import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.concurrent.ExecutorServices;\n import io.druid.java.util.common.lifecycle.Lifecycle;\n import io.druid.java.util.common.logger.Logger;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n \n-import javax.annotation.concurrent.GuardedBy;\n-import java.util.Collection;\n-import java.util.Map;\n-import java.util.UUID;\n-import java.util.concurrent.CancellationException;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n \n /**\n- *\n+ * Usage:\n+ * <pre>{@code\n+ * CacheHandler cacheHandler = namespaceExtractionCacheManager.createCache();\n+ * Map<String, String> cache == cacheHandler.cache; // use this cache\n+ * ...\n+ * cacheHandler.close();\n+ * }</pre>\n  */\n public abstract class NamespaceExtractionCacheManager\n {\n-  protected static class NamespaceImplData\n-  {\n-    public NamespaceImplData(\n-        final ListenableFuture<?> future,\n-        final ExtractionNamespace namespace,\n-        final String name\n-    )\n-    {\n-      this.future = future;\n-      this.namespace = namespace;\n-      this.name = name;\n-    }\n-\n-    final ListenableFuture<?> future;\n-    final ExtractionNamespace namespace;\n-    final String name;\n-    final Object changeLock = new Object();\n-    final AtomicBoolean enabled = new AtomicBoolean(false);\n-    final CountDownLatch firstRun = new CountDownLatch(1);\n-    volatile String latestVersion = null;\n-  }\n-\n   private static final Logger log = new Logger(NamespaceExtractionCacheManager.class);\n-  private final ListeningScheduledExecutorService listeningScheduledExecutorService;\n-  protected final ConcurrentMap<String, NamespaceImplData> implData = new ConcurrentHashMap<>();\n-  protected final AtomicLong tasksStarted = new AtomicLong(0);\n-  protected final ServiceEmitter serviceEmitter;\n-  private final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespaceFunctionFactoryMap;\n \n-  public NamespaceExtractionCacheManager(\n-      Lifecycle lifecycle,\n-      final ServiceEmitter serviceEmitter,\n-      final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespaceFunctionFactoryMap\n-  )\n-  {\n-    this.listeningScheduledExecutorService = MoreExecutors.listeningDecorator(\n-        Executors.newScheduledThreadPool(\n-            1,\n-            new ThreadFactoryBuilder()\n-                .setDaemon(true)\n-                .setNameFormat(\"NamespaceExtractionCacheManager-%d\")\n-                .setPriority(Thread.MIN_PRIORITY)\n-                .build()\n-        )\n+  private final ScheduledThreadPoolExecutor scheduledExecutorService;\n+\n+  public NamespaceExtractionCacheManager(final Lifecycle lifecycle, final ServiceEmitter serviceEmitter) {\n+    this.scheduledExecutorService = new ScheduledThreadPoolExecutor(\n+        1,\n+        new ThreadFactoryBuilder()\n+            .setDaemon(true)\n+            .setNameFormat(\"NamespaceExtractionCacheManager-%d\")\n+            .setPriority(Thread.MIN_PRIORITY)\n+            .build()\n     );\n-    ExecutorServices.manageLifecycle(lifecycle, listeningScheduledExecutorService);\n-    this.serviceEmitter = serviceEmitter;\n-    this.namespaceFunctionFactoryMap = namespaceFunctionFactoryMap;\n-    listeningScheduledExecutorService.scheduleAtFixedRate(\n+    ExecutorServices.manageLifecycle(lifecycle, scheduledExecutorService);\n+    scheduledExecutorService.scheduleAtFixedRate(\n         new Runnable()\n         {\n-          long priorTasksStarted = 0L;\n-\n           @Override\n           public void run()\n           {\n             try {\n-              final long tasks = tasksStarted.get();\n-              serviceEmitter.emit(\n-                  ServiceMetricEvent.builder()\n-                                    .build(\"namespace/deltaTasksStarted\", tasks - priorTasksStarted)\n-              );\n-              priorTasksStarted = tasks;\n               monitor(serviceEmitter);\n             }\n             catch (Exception e) {\n@@ -134,347 +78,27 @@ public void run()\n   }\n \n   /**\n-   * Optional monitoring for overriding classes. `super.monitor` does *NOT* need to be called by overriding methods\n-   *\n-   * @param serviceEmitter The emitter to emit to\n+   * Return {@link ScheduledThreadPoolExecutor} rather than more generic {@link\n+   * java.util.concurrent.ScheduledExecutorService}, because the former guarantees that periodic runs of scheduled\n+   * tasks see all \"local\" changes from the previous runs, via happens-before (see {@link ScheduledThreadPoolExecutor}'s\n+   * class-level Javadoc).\n    */\n-  protected void monitor(ServiceEmitter serviceEmitter)\n-  {\n-    // Noop by default\n-  }\n-\n-  protected boolean waitForServiceToEnd(long time, TimeUnit unit) throws InterruptedException\n-  {\n-    return listeningScheduledExecutorService.awaitTermination(time, unit);\n-  }\n-\n-\n-  protected void updateNamespace(final String id, final String cacheId, final String newVersion)\n-  {\n-    final NamespaceImplData namespaceDatum = implData.get(id);\n-    if (namespaceDatum == null) {\n-      // was removed\n-      return;\n-    }\n-    try {\n-      if (!namespaceDatum.enabled.get()) {\n-        // skip because it was disabled\n-        return;\n-      }\n-      synchronized (namespaceDatum.enabled) {\n-        if (!namespaceDatum.enabled.get()) {\n-          return;\n-        }\n-        swapAndClearCache(id, cacheId);\n-        namespaceDatum.latestVersion = newVersion;\n-      }\n-    }\n-    finally {\n-      namespaceDatum.firstRun.countDown();\n-    }\n-  }\n-\n-  // return value means actually delete or not\n-  public boolean checkedDelete(\n-      String namespaceName\n-  )\n+  final ScheduledThreadPoolExecutor scheduledExecutorService()\n   {\n-    final NamespaceImplData implDatum = implData.get(namespaceName);\n-    if (implDatum == null) {\n-      // Delete but we don't have it?\n-      log.wtf(\"Asked to delete something I just lost [%s]\", namespaceName);\n-      return false;\n-    }\n-    return delete(namespaceName);\n+    return scheduledExecutorService;\n   }\n \n-  // return value means actually schedule or not\n-  public boolean scheduleOrUpdate(\n-      final String id,\n-      ExtractionNamespace namespace\n-  )\n+  @VisibleForTesting\n+  boolean waitForServiceToEnd(long time, TimeUnit unit) throws InterruptedException\n   {\n-    final NamespaceImplData implDatum = implData.get(id);\n-    if (implDatum == null) {\n-      // New, probably\n-      schedule(id, namespace);\n-      return true;\n-    }\n-    if (!implDatum.enabled.get()) {\n-      // Race condition. Someone else disabled it first, go ahead and reschedule\n-      schedule(id, namespace);\n-      return true;\n-    }\n-\n-    // Live one. Check if it needs updated\n-    if (implDatum.namespace.equals(namespace)) {\n-      // skip if no update\n-      return false;\n-    }\n-    if (log.isDebugEnabled()) {\n-      log.debug(\"Namespace [%s] needs updated to [%s]\", implDatum.namespace, namespace);\n-    }\n-    // Ensure it is not changing state right now.\n-    synchronized (implDatum.changeLock) {\n-      removeNamespaceLocalMetadata(implDatum);\n-    }\n-    schedule(id, namespace);\n-    return true;\n+    return scheduledExecutorService.awaitTermination(time, unit);\n   }\n \n-  public boolean scheduleAndWait(\n-      final String id,\n-      ExtractionNamespace namespace,\n-      long waitForFirstRun\n-  )\n-  {\n-    if (scheduleOrUpdate(id, namespace)) {\n-      log.debug(\"Scheduled new namespace [%s]: %s\", id, namespace);\n-    } else {\n-      log.debug(\"Namespace [%s] already running: %s\", id, namespace);\n-    }\n+  public abstract CacheHandler createCache();\n \n-    final NamespaceImplData namespaceImplData = implData.get(id);\n-    if (namespaceImplData == null) {\n-      log.warn(\"NamespaceLookupExtractorFactory[%s] - deleted during start\", id);\n-      return false;\n-    }\n-\n-    boolean success = false;\n-    try {\n-      success = namespaceImplData.firstRun.await(waitForFirstRun, TimeUnit.MILLISECONDS);\n-    }\n-    catch (InterruptedException e) {\n-      log.error(e, \"NamespaceLookupExtractorFactory[%s] - interrupted during start\", id);\n-    }\n-    if (!success) {\n-      delete(id);\n-    }\n-    return success;\n-  }\n-\n-  @GuardedBy(\"implDatum.changeLock\")\n-  private void cancelFuture(final NamespaceImplData implDatum)\n-  {\n-    final CountDownLatch latch = new CountDownLatch(1);\n-    final ListenableFuture<?> future = implDatum.future;\n-    Futures.addCallback(\n-        future, new FutureCallback<Object>()\n-        {\n-          @Override\n-          public void onSuccess(Object result)\n-          {\n-            latch.countDown();\n-          }\n+  abstract void disposeCache(CacheHandler cacheHandler);\n \n-          @Override\n-          public void onFailure(Throwable t)\n-          {\n-            // Expect CancellationException\n-            latch.countDown();\n-            if (!(t instanceof CancellationException)) {\n-              log.error(t, \"Error in namespace [%s]\", implDatum.name);\n-            }\n-          }\n-        }\n-    );\n-    future.cancel(true);\n-    try {\n-      latch.await();\n-    }\n-    catch (InterruptedException e) {\n-      Thread.currentThread().interrupt();\n-      throw Throwables.propagate(e);\n-    }\n-  }\n+  abstract int cacheCount();\n \n-  // Not thread safe\n-  @GuardedBy(\"implDatum.changeLock\")\n-  private boolean removeNamespaceLocalMetadata(final NamespaceImplData implDatum)\n-  {\n-    if (implDatum == null) {\n-      return false;\n-    }\n-    // \"Leader\" election for doing the deletion\n-    if (!implDatum.enabled.compareAndSet(true, false)) {\n-      return false;\n-    }\n-    if (!implDatum.future.isDone()) {\n-      cancelFuture(implDatum);\n-    }\n-    return implData.remove(implDatum.name, implDatum);\n-  }\n-\n-  // Optimistic scheduling of updates to a namespace.\n-  public <T extends ExtractionNamespace> ListenableFuture<?> schedule(final String id, final T namespace)\n-  {\n-    final ExtractionNamespaceCacheFactory<T> factory = (ExtractionNamespaceCacheFactory<T>)\n-        namespaceFunctionFactoryMap.get(namespace.getClass());\n-    if (factory == null) {\n-      throw new ISE(\"Cannot find factory for namespace [%s]\", namespace);\n-    }\n-    final String cacheId = String.format(\"namespace-cache-%s-%s\", id, UUID.randomUUID().toString());\n-    return schedule(id, namespace, factory, cacheId);\n-  }\n-\n-  // For testing purposes this is protected\n-  protected <T extends ExtractionNamespace> ListenableFuture<?> schedule(\n-      final String id,\n-      final T namespace,\n-      final ExtractionNamespaceCacheFactory<T> factory,\n-      final String cacheId\n-  )\n-  {\n-    log.debug(\"Trying to update namespace [%s]\", id);\n-    final NamespaceImplData implDatum = implData.get(id);\n-    if (implDatum != null) {\n-      synchronized (implDatum.changeLock) {\n-        if (implDatum.enabled.get()) {\n-          // We also check at the end of the function, but fail fast here\n-          throw new IAE(\"Namespace [%s] already exists! Leaving prior running\", namespace.toString());\n-        }\n-      }\n-    }\n-    final long updateMs = namespace.getPollMs();\n-    final CountDownLatch startLatch = new CountDownLatch(1);\n-    // Must be set before leader election occurs or else runnable will fail\n-    final AtomicReference<NamespaceImplData> implDataAtomicReference = new AtomicReference<>(null);\n-\n-    final Runnable command = new Runnable()\n-    {\n-      @Override\n-      public void run()\n-      {\n-        try {\n-          startLatch.await(); // wait for \"election\" to leadership or cancellation\n-          if (!Thread.currentThread().isInterrupted()) {\n-            final NamespaceImplData implData = implDataAtomicReference.get();\n-            if (implData == null) {\n-              // should never happen\n-              throw new NullPointerException(String.format(\"No data for namespace [%s]\", id));\n-            }\n-            final Map<String, String> cache = getCacheMap(cacheId);\n-            final String preVersion = implData.latestVersion;\n-\n-            tasksStarted.incrementAndGet();\n-            final String newVersion = factory.populateCache(id, namespace, preVersion, cache);\n-            if (newVersion.equals(preVersion)) {\n-              log.debug(\"Version `%s` already exists, skipping updating cache\", preVersion);\n-            } else {\n-              updateNamespace(id, cacheId, newVersion);\n-              log.debug(\"Namespace [%s] successfully updated\", id);\n-            }\n-          }\n-        }\n-        catch (Throwable t) {\n-          try {\n-            delete(cacheId);\n-            if (t instanceof InterruptedException) {\n-              log.debug(t, \"Namespace [%s] cancelled\", id);\n-            } else {\n-              log.error(t, \"Failed update namespace [%s]\", namespace);\n-            }\n-          }\n-          catch (Exception e) {\n-            t.addSuppressed(e);\n-          }\n-          if (Thread.currentThread().isInterrupted() || (t instanceof Error)) {\n-            throw Throwables.propagate(t);\n-          }\n-        }\n-      }\n-    };\n-\n-    ListenableFuture<?> future;\n-    try {\n-      if (updateMs > 0) {\n-        future = listeningScheduledExecutorService.scheduleAtFixedRate(command, 0, updateMs, TimeUnit.MILLISECONDS);\n-      } else {\n-        future = listeningScheduledExecutorService.schedule(command, 0, TimeUnit.MILLISECONDS);\n-      }\n-\n-      // Do not need to synchronize here as we haven't set enabled to true yet, and haven't released startLatch\n-      final NamespaceImplData me = new NamespaceImplData(future, namespace, id);\n-      implDataAtomicReference.set(me);\n-      final NamespaceImplData other = implData.putIfAbsent(id, me);\n-      if (other != null) {\n-        if (!future.isDone() && !future.cancel(true)) {\n-          log.warn(\"Unable to cancel future for namespace[%s] on race loss\", id);\n-        }\n-        throw new IAE(\"Namespace [%s] already exists! Leaving prior running\", namespace);\n-      } else {\n-        if (!me.enabled.compareAndSet(false, true)) {\n-          log.wtf(\"How did someone enable this before ME?\");\n-        }\n-        log.debug(\"I own namespace [%s]\", id);\n-        return future;\n-      }\n-    }\n-    finally {\n-      startLatch.countDown();\n-    }\n-  }\n-\n-  /**\n-   * This method is expected to swap the cacheKey into the active namespace, and leave future requests for new cacheKey available. getCacheMap(cacheKey) should return empty data after this call.\n-   *\n-   * @param namespaceKey The namespace to swap the cache into\n-   * @param cacheKey     The cacheKey that contains the data of interest\n-   *\n-   * @return true if old data was cleared. False if no old data was found\n-   */\n-  protected abstract boolean swapAndClearCache(String namespaceKey, String cacheKey);\n-\n-  /**\n-   * Return a ConcurrentMap with the specified ID (either namespace's name or a cache key ID)\n-   *\n-   * @param namespaceOrCacheKey Either a namespace or cache key should be acceptable here.\n-   *\n-   * @return A ConcurrentMap<String, String> that is backed by the impl which implements this method.\n-   */\n-  public abstract ConcurrentMap<String, String> getCacheMap(String namespaceOrCacheKey);\n-\n-  /**\n-   * Clears out resources used by the namespace such as threads. Implementations may override this and call super.delete(...) if they have resources of their own which need cleared.\n-   *\n-   * @param ns The namespace to be deleted\n-   *\n-   * @return True if a deletion occurred, false if no deletion occurred.\n-   *\n-   * @throws ISE if there is an error cancelling the namespace's future task\n-   */\n-  public boolean delete(final String ns)\n-  {\n-    final NamespaceImplData implDatum = implData.get(ns);\n-    if (implDatum == null) {\n-      log.debug(\"Found no running cache for [%s]\", ns);\n-      return false;\n-    }\n-    synchronized (implDatum.changeLock) {\n-      if (removeNamespaceLocalMetadata(implDatum)) {\n-        log.info(\"Deleted namespace [%s]\", ns);\n-        return true;\n-      } else {\n-        log.debug(\"Did not delete namespace [%s]\", ns);\n-        return false;\n-      }\n-    }\n-  }\n-\n-  public String getVersion(String namespace)\n-  {\n-    if (namespace == null) {\n-      return null;\n-    }\n-    final NamespaceImplData implDatum = implData.get(namespace);\n-    if (implDatum == null) {\n-      return null;\n-    }\n-    return implDatum.latestVersion;\n-  }\n-\n-  public Collection<String> getKnownIDs()\n-  {\n-    return implData.keySet();\n-  }\n+  abstract void monitor(ServiceEmitter serviceEmitter);\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManager.java",
                "sha": "c7d3fd5c0ac62ca44ce2d62026ea1a8ea4546173",
                "status": "modified"
            },
            {
                "additions": 129,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java",
                "changes": 200,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 71,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java",
                "patch": "@@ -19,47 +19,122 @@\n \n package io.druid.server.lookup.namespace.cache;\n \n-import com.google.common.base.Preconditions;\n import com.google.common.base.Throwables;\n-import com.google.common.util.concurrent.Striped;\n import com.google.inject.Inject;\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n-\n import io.druid.java.util.common.lifecycle.Lifecycle;\n import io.druid.java.util.common.logger.Logger;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import org.mapdb.DB;\n import org.mapdb.DBMaker;\n+import org.mapdb.HTreeMap;\n+import sun.misc.Cleaner;\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.Map;\n-import java.util.UUID;\n-import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n \n /**\n  *\n  */\n public class OffHeapNamespaceExtractionCacheManager extends NamespaceExtractionCacheManager\n {\n   private static final Logger log = new Logger(OffHeapNamespaceExtractionCacheManager.class);\n+\n+  private class MapDbCacheDisposer implements Runnable\n+  {\n+    final String mapDbKey;\n+    /**\n+     * Manages the race between dispose via {@link #disposeManually()} and automatic dispose by the JVM via\n+     * {@link #run()}.\n+     *\n+     * <p>In case of actual race, we don't wait in those methods until the other one, which manages to switch this flag\n+     * first, completes. This could result into the situation that neither one completes, if the JVM is shutting down\n+     * and the thread from which {@link Cleaner#clean()} (delegating to {@link #run()}) is called started the disposal\n+     * operation, then more deterministic shutdown hook / lifecycle.stop(), which may call {@link #disposeManually()}\n+     * completed early, and then the whole process shuts down before {@link Cleaner#clean()} completes, because shutdown\n+     * is not blocked by it. However this should be harmless because anyway we remove the whole MapDB's file in\n+     * lifecycle.stop() (see {@link OffHeapNamespaceExtractionCacheManager#OffHeapNamespaceExtractionCacheManager}).\n+     * However if we persist off-heap DB between JVM runs, this decision should be revised.\n+     */\n+    final AtomicBoolean disposed = new AtomicBoolean(false);\n+\n+    private MapDbCacheDisposer(String mapDbKey)\n+    {\n+      this.mapDbKey = mapDbKey;\n+    }\n+\n+    /**\n+     * To be called by the JVM via {@link Cleaner#clean()}. The only difference from {@link #disposeManually()} is\n+     * exception treatment.\n+     */\n+    @Override\n+    public void run()\n+    {\n+      if (disposed.compareAndSet(false, true)) {\n+        try {\n+          doDispose();\n+          // Log statement goes after doDispose(), because logging may fail (e. g. if we are in shutdownHooks).\n+          log.error(\"OffHeapNamespaceExtractionCacheManager.disposeCache() was not called, disposed resources by the JVM\");\n+        }\n+        catch (Throwable t) {\n+          try {\n+            log.error(t, \"Error while deleting key %s from MapDb\", mapDbKey);\n+          }\n+          catch (Exception e) {\n+            t.addSuppressed(e);\n+          }\n+          Throwables.propagateIfInstanceOf(t, Error.class);\n+          // Must not throw exceptions in the cleaner thread, run by the JVM.\n+        }\n+      }\n+    }\n+\n+    /**\n+     * To be called from {@link #disposeCache(CacheHandler)}. The only difference from {@link #run()} is exception\n+     * treatment, disposeManually() lefts all exceptions thrown in DB.delete() to the caller.\n+     */\n+    void disposeManually()\n+    {\n+      if (disposed.compareAndSet(false, true)) {\n+        // TODO: resolve what happens here if query is actively going on\n+        doDispose();\n+      }\n+    }\n+\n+    private void doDispose()\n+    {\n+      if (!mmapDB.isClosed()) {\n+        mmapDB.delete(mapDbKey);\n+      }\n+      cacheCount.decrementAndGet();\n+    }\n+  }\n+\n+  private static class MapDbCacheDisposerAndCleaner\n+  {\n+    final MapDbCacheDisposer cacheDisposer;\n+    final Cleaner cleaner;\n+\n+    private MapDbCacheDisposerAndCleaner(MapDbCacheDisposer cacheDisposer, Cleaner cleaner)\n+    {\n+      this.cacheDisposer = cacheDisposer;\n+      this.cleaner = cleaner;\n+    }\n+  }\n+\n   private final DB mmapDB;\n-  private ConcurrentMap<String, String> currentNamespaceCache = new ConcurrentHashMap<>();\n-  private Striped<Lock> nsLocks = Striped.lazyWeakLock(1024); // Needed to make sure delete() doesn't do weird things\n   private final File tmpFile;\n+  private AtomicLong mapDbKeyCounter = new AtomicLong(0);\n+  private AtomicInteger cacheCount = new AtomicInteger(0);\n \n   @Inject\n-  public OffHeapNamespaceExtractionCacheManager(\n-      Lifecycle lifecycle,\n-      ServiceEmitter emitter,\n-      final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespaceFunctionFactoryMap\n-  )\n+  public OffHeapNamespaceExtractionCacheManager(Lifecycle lifecycle, ServiceEmitter serviceEmitter)\n   {\n-    super(lifecycle, emitter, namespaceFunctionFactoryMap);\n+    super(lifecycle, serviceEmitter);\n     try {\n       tmpFile = File.createTempFile(\"druidMapDB\", getClass().getCanonicalName());\n       log.info(\"Using file [%s] for mapDB off heap namespace cache\", tmpFile.getAbsolutePath());\n@@ -107,75 +182,58 @@ public synchronized void stop()\n   }\n \n   @Override\n-  protected boolean swapAndClearCache(String namespaceKey, String cacheKey)\n+  public CacheHandler createCache()\n   {\n-    final Lock lock = nsLocks.get(namespaceKey);\n-    lock.lock();\n-    try {\n-      Preconditions.checkArgument(mmapDB.exists(cacheKey), \"Namespace [%s] does not exist\", cacheKey);\n-\n-      final String swapCacheKey = UUID.randomUUID().toString();\n-      mmapDB.rename(cacheKey, swapCacheKey);\n-\n-      final String priorCache = currentNamespaceCache.put(namespaceKey, swapCacheKey);\n-      if (priorCache != null) {\n-        // TODO: resolve what happens here if query is actively going on\n-        mmapDB.delete(priorCache);\n-        return true;\n-      } else {\n-        return false;\n+    ConcurrentMap<String, String> cache;\n+    String mapDbKey;\n+    // This loop will succeed because 2^64 cache maps couldn't exist in memory simultaneously\n+    while (true) {\n+      mapDbKey = Long.toString(mapDbKeyCounter.getAndIncrement());\n+      try {\n+        HTreeMap<String, String> hTreeMap = mmapDB.createHashMap(mapDbKey).make();\n+        // Access MapDB's HTreeMap and create a cleaner via proxy, because there is no 100% confidence that there are\n+        // no memory leaks in MapDB and in OffHeapCacheManager. Otherwise JVM will never be able to clean the cleaner\n+        // and dispose leaked cache.\n+        cache = new CacheProxy(hTreeMap);\n+        cacheCount.incrementAndGet();\n+        break;\n+      }\n+      catch (IllegalArgumentException e) {\n+        // failed to create a map, the key exists, go to the next iteration\n       }\n     }\n-    finally {\n-      lock.unlock();\n-    }\n+    MapDbCacheDisposer cacheDisposer = new MapDbCacheDisposer(mapDbKey);\n+    // Cleaner is \"the second level of defence\". Normally all users of createCache() must call disposeCache() with\n+    // the returned CacheHandler instance manually. But if they don't do this for whatever reason, JVM will cleanup\n+    // the cache itself.\n+    Cleaner cleaner = Cleaner.create(cache, cacheDisposer);\n+    MapDbCacheDisposerAndCleaner disposerAndCleaner = new MapDbCacheDisposerAndCleaner(\n+        cacheDisposer,\n+        cleaner\n+    );\n+    return new CacheHandler(this, cache, disposerAndCleaner);\n   }\n \n   @Override\n-  public boolean delete(final String namespaceKey)\n+  void disposeCache(CacheHandler cacheHandler)\n   {\n-    // `super.delete` has a synchronization in it, don't call it in the lock.\n-    if (!super.delete(namespaceKey)) {\n-      return false;\n-    }\n-    final Lock lock = nsLocks.get(namespaceKey);\n-    lock.lock();\n-    try {\n-      final String mmapDBkey = currentNamespaceCache.remove(namespaceKey);\n-      if (mmapDBkey == null) {\n-        return false;\n-      }\n-      final long pre = tmpFile.length();\n-      mmapDB.delete(mmapDBkey);\n-      log.debug(\"MapDB file size: pre %d  post %d\", pre, tmpFile.length());\n-      return true;\n-    }\n-    finally {\n-      lock.unlock();\n-    }\n+    MapDbCacheDisposerAndCleaner disposerAndCleaner = (MapDbCacheDisposerAndCleaner) cacheHandler.id;\n+    disposerAndCleaner.cacheDisposer.disposeManually();\n+    // This clean() call effectively just removes the Cleaner from the internal linked list of all cleaners.\n+    // The thunk.run() will be a no-op because cacheDisposer.disposed is already set to true.\n+    disposerAndCleaner.cleaner.clean();\n   }\n \n   @Override\n-  public ConcurrentMap<String, String> getCacheMap(String namespaceKey)\n+  int cacheCount()\n   {\n-    final Lock lock = nsLocks.get(namespaceKey);\n-    lock.lock();\n-    try {\n-      String mapDBKey = currentNamespaceCache.get(namespaceKey);\n-      if (mapDBKey == null) {\n-        // Not something created by swapAndClearCache\n-        mapDBKey = namespaceKey;\n-      }\n-      return mmapDB.createHashMap(mapDBKey).makeOrGet();\n-    }\n-    finally {\n-      lock.unlock();\n-    }\n+    return cacheCount.get();\n   }\n \n   @Override\n-  protected void monitor(ServiceEmitter serviceEmitter)\n+  void monitor(ServiceEmitter serviceEmitter)\n   {\n+    serviceEmitter.emit(ServiceMetricEvent.builder().build(\"namespace/cache/count\", cacheCount()));\n     serviceEmitter.emit(ServiceMetricEvent.builder().build(\"namespace/cache/diskSize\", tmpFile.length()));\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManager.java",
                "sha": "41675def39556b35fa372fdf5d18eca2361ea582",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OnHeapNamespaceExtractionCacheManager.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OnHeapNamespaceExtractionCacheManager.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 60,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OnHeapNamespaceExtractionCacheManager.java",
                "patch": "@@ -20,110 +20,104 @@\n package io.druid.server.lookup.namespace.cache;\n \n import com.google.common.primitives.Chars;\n-import com.google.common.util.concurrent.Striped;\n import com.google.inject.Inject;\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n-\n-import io.druid.java.util.common.IAE;\n+import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.lifecycle.Lifecycle;\n import io.druid.java.util.common.logger.Logger;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n \n+import java.lang.ref.WeakReference;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.locks.Lock;\n \n /**\n  *\n  */\n public class OnHeapNamespaceExtractionCacheManager extends NamespaceExtractionCacheManager\n {\n   private static final Logger LOG = new Logger(OnHeapNamespaceExtractionCacheManager.class);\n-  private final ConcurrentMap<String, ConcurrentMap<String, String>> mapMap = new ConcurrentHashMap<>();\n-  private final Striped<Lock> nsLocks = Striped.lock(32);\n+\n+  /**\n+   * Weak collection of caches is \"the second level of defence\". Normally all users of {@link #createCache()} must call\n+   * {@link CacheHandler#close()} on the returned CacheHandler instance manually. But if they don't do this for\n+   * whatever reason, JVM will cleanup the cache itself.\n+   *\n+   * <p>{@link WeakReference} doesn't override Object's identity equals() and hashCode(), so effectively this map plays\n+   * like concurrent {@link java.util.IdentityHashMap}.\n+   */\n+  private final ConcurrentHashMap<WeakReference<ConcurrentMap<String, String>>, Boolean> caches =\n+      new ConcurrentHashMap<>();\n \n   @Inject\n-  public OnHeapNamespaceExtractionCacheManager(\n-      final Lifecycle lifecycle,\n-      final ServiceEmitter emitter,\n-      final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespaceFunctionFactoryMap\n-  )\n+  public OnHeapNamespaceExtractionCacheManager(Lifecycle lifecycle, ServiceEmitter serviceEmitter)\n   {\n-    super(lifecycle, emitter, namespaceFunctionFactoryMap);\n+    super(lifecycle, serviceEmitter);\n   }\n \n-  @Override\n-  protected boolean swapAndClearCache(String namespaceKey, String cacheKey)\n+  private void expungeCollectedCaches()\n   {\n-    final Lock lock = nsLocks.get(namespaceKey);\n-    lock.lock();\n-    try {\n-      ConcurrentMap<String, String> cacheMap = mapMap.get(cacheKey);\n-      if (cacheMap == null) {\n-        throw new IAE(\"Extraction Cache [%s] does not exist\", cacheKey);\n+    for (Iterator<WeakReference<ConcurrentMap<String, String>>> iterator = caches.keySet().iterator();\n+         iterator.hasNext(); ) {\n+      WeakReference<?> cacheRef = iterator.next();\n+      if (cacheRef.get() == null) {\n+        // This may not necessarily mean leak of CacheHandler, because disposeCache() may be called concurrently with\n+        // this iteration, and cacheHandler (hence the cache) could be already claimed by the GC. That is why we emit\n+        // no warning here. Also, \"soft leak\" (close() not called, but the objects becomes unreachable and claimed by\n+        // the GC) of on-heap cache is effectively harmless and logging may be useful here only for identifying bugs in\n+        // the code which uses NamespaceExtractionCacheManager, if there are plans to switch to\n+        // OffHeapNamespaceExtractionCacheManager. However in OffHeapNamespaceExtractionCacheManager CacheHandler leaks\n+        // are identified and logged better than in this class.\n+        iterator.remove();\n       }\n-      ConcurrentMap<String, String> prior = mapMap.put(namespaceKey, cacheMap);\n-      mapMap.remove(cacheKey);\n-      if (prior != null) {\n-        // Old map will get GC'd when it is not used anymore\n-        return true;\n-      } else {\n-        return false;\n-      }\n-    }\n-    finally {\n-      lock.unlock();\n     }\n   }\n \n   @Override\n-  public ConcurrentMap<String, String> getCacheMap(String namespaceOrCacheKey)\n+  public CacheHandler createCache()\n   {\n-    ConcurrentMap<String, String> map = mapMap.get(namespaceOrCacheKey);\n-    if (map == null) {\n-      mapMap.putIfAbsent(namespaceOrCacheKey, new ConcurrentHashMap<String, String>());\n-      map = mapMap.get(namespaceOrCacheKey);\n-    }\n-    return map;\n+    ConcurrentMap<String, String> cache = new ConcurrentHashMap<>();\n+    WeakReference<ConcurrentMap<String, String>> cacheRef = new WeakReference<>(cache);\n+    expungeCollectedCaches();\n+    caches.put(cacheRef, true);\n+    return new CacheHandler(this, cache, cacheRef);\n   }\n \n   @Override\n-  public boolean delete(final String namespaceKey)\n+  void disposeCache(CacheHandler cacheHandler)\n   {\n-    // `super.delete` has a synchronization in it, don't call it in the lock.\n-    if (!super.delete(namespaceKey)) {\n-      return false;\n-    }\n-    final Lock lock = nsLocks.get(namespaceKey);\n-    lock.lock();\n-    try {\n-      return mapMap.remove(namespaceKey) != null;\n-    }\n-    finally {\n-      lock.unlock();\n+    if (!(cacheHandler.id instanceof WeakReference)) {\n+      throw new ISE(\"Expected WeakReference, got: %s\", cacheHandler.id);\n     }\n+    caches.remove(cacheHandler.id);\n+  }\n+\n+  @Override\n+  int cacheCount()\n+  {\n+    expungeCollectedCaches();\n+    return caches.size();\n   }\n \n   @Override\n-  protected void monitor(ServiceEmitter serviceEmitter)\n+  void monitor(ServiceEmitter serviceEmitter)\n   {\n     long numEntries = 0;\n     long size = 0;\n-    for (Map.Entry<String, ConcurrentMap<String, String>> entry : mapMap.entrySet()) {\n-      final ConcurrentMap<String, String> map = entry.getValue();\n-      if (map == null) {\n-        LOG.debug(\"missing cache key for reporting [%s]\", entry.getKey());\n+    expungeCollectedCaches();\n+    for (WeakReference<ConcurrentMap<String, String>> cacheRef : caches.keySet()) {\n+      final Map<String, String> cache = cacheRef.get();\n+      if (cache == null) {\n         continue;\n       }\n-      numEntries += map.size();\n-      for (Map.Entry<String, String> sEntry : map.entrySet()) {\n+      numEntries += cache.size();\n+      for (Map.Entry<String, String> sEntry : cache.entrySet()) {\n         final String key = sEntry.getKey();\n         final String value = sEntry.getValue();\n         if (key == null || value == null) {\n-          LOG.debug(\"Missing entries for cache key [%s]\", entry.getKey());\n+          LOG.debug(\"Missing entries for cache key\");\n           continue;\n         }\n         size += key.length() + value.length();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/OnHeapNamespaceExtractionCacheManager.java",
                "sha": "e69754b01c1ecb8622f8b15607bb3a66b1b50ac0",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/UpdateCounter.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/UpdateCounter.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/UpdateCounter.java",
                "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.server.lookup.namespace.cache;\n+\n+import java.util.concurrent.Phaser;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+final class UpdateCounter\n+{\n+  private final Phaser phaser = new Phaser(1);\n+\n+  void update()\n+  {\n+    phaser.arrive();\n+  }\n+\n+  void awaitTotalUpdates(int totalUpdates) throws InterruptedException\n+  {\n+    int currentUpdates = phaser.getPhase();\n+    while (totalUpdates - currentUpdates > 0) { // overflow-aware\n+      currentUpdates = phaser.awaitAdvanceInterruptibly(currentUpdates);\n+    }\n+  }\n+\n+  void awaitNextUpdates(int nextUpdates) throws InterruptedException\n+  {\n+    awaitTotalUpdates(phaser.getPhase() + nextUpdates);\n+  }\n+\n+  boolean awaitFirstUpdate(long timeout, TimeUnit unit) throws InterruptedException\n+  {\n+    try {\n+      phaser.awaitAdvanceInterruptibly(0, timeout, unit);\n+      return true;\n+    }\n+    catch (TimeoutException e) {\n+      return false;\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/UpdateCounter.java",
                "sha": "46da921842836e3e8da2c368dbe039e7560b1b09",
                "status": "added"
            },
            {
                "additions": 145,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "changes": 362,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 217,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "patch": "@@ -43,20 +43,34 @@\n import io.druid.query.lookup.namespace.URIExtractionNamespace;\n import io.druid.server.DruidNode;\n import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n import org.easymock.EasyMock;\n+import org.easymock.IExpectationSetters;\n import org.joda.time.Period;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import javax.ws.rs.core.Response;\n import java.util.HashMap;\n import java.util.Map;\n-import java.util.concurrent.ConcurrentHashMap;\n \n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({\n+    NamespaceExtractionCacheManager.class,\n+    CacheScheduler.class,\n+    CacheScheduler.VersionedCache.class,\n+    CacheScheduler.Entry.class\n+})\n+@PowerMockIgnore(\"javax.net.ssl.*\")\n public class NamespaceLookupExtractorFactoryTest\n {\n   private final ObjectMapper mapper = new DefaultObjectMapper();\n@@ -65,7 +79,10 @@\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();\n \n-  private final NamespaceExtractionCacheManager cacheManager = EasyMock.createStrictMock(NamespaceExtractionCacheManager.class);\n+  private final CacheScheduler scheduler = PowerMock.createStrictMock(CacheScheduler.class);\n+  private final CacheScheduler.Entry entry = PowerMock.createStrictMock(CacheScheduler.Entry.class);\n+  private final CacheScheduler.VersionedCache versionedCache =\n+      PowerMock.createStrictMock(CacheScheduler.VersionedCache.class);\n \n   @Before\n   public void setUp()\n@@ -78,8 +95,8 @@ public Object findInjectableValue(\n               Object valueId, DeserializationContext ctxt, BeanProperty forProperty, Object beanInstance\n           )\n           {\n-            if (\"io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager\".equals(valueId)) {\n-              return cacheManager;\n+            if (\"io.druid.server.lookup.namespace.cache.CacheScheduler\".equals(valueId)) {\n+              return scheduler;\n             }\n             return null;\n           }\n@@ -100,7 +117,7 @@ public void testSimpleSerde() throws Exception\n     );\n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         uriExtractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertEquals(\n         uriExtractionNamespace,\n@@ -129,23 +146,17 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(\n-        cacheManager.checkedDelete(EasyMock.anyString())\n-    ).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    expectScheduleAndWaitOnce(extractionNamespace);\n+    expectEntryCloseOnce();\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n   @Test\n@@ -159,28 +170,29 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleOrUpdate(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace)\n-    )).andReturn(true).once();\n-    EasyMock.expect(\n-        cacheManager.checkedDelete(EasyMock.anyString())\n-    ).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    EasyMock.expect(scheduler.schedule(EasyMock.eq(extractionNamespace))).andReturn(entry).once();\n+    expectEntryCloseOnce();\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n         0,\n         false,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n+  }\n+\n+  private void expectEntryCloseOnce()\n+  {\n+    entry.close();\n+    EasyMock.expectLastCall().once();\n   }\n \n   @Test\n-  public void testStartReturnsImmediatelyAndFails()\n+  public void testStartReturnsImmediatelyAndFails() throws InterruptedException\n   {\n     final ExtractionNamespace extractionNamespace = new ExtractionNamespace()\n     {\n@@ -190,20 +202,18 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleOrUpdate(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace)\n-    )).andReturn(false).once();\n-    EasyMock.replay(cacheManager);\n+    EasyMock.expect(scheduler.scheduleAndWait(EasyMock.eq(extractionNamespace), EasyMock.eq(1L)))\n+            .andReturn(null).once();\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        0,\n+        1,\n         false,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertFalse(namespaceLookupExtractorFactory.start());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n   @Test\n@@ -217,24 +227,18 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(\n-        cacheManager.checkedDelete(EasyMock.anyString())\n-    ).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    expectScheduleAndWaitOnce(extractionNamespace);\n+    expectEntryCloseOnce();\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n   @Test\n@@ -248,20 +252,16 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    expectScheduleAndWaitOnce(extractionNamespace);\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n \n@@ -276,30 +276,28 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"0\").once();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n-            .once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"0\").once();\n-    EasyMock.expect(\n-        cacheManager.checkedDelete(EasyMock.anyString())\n-    ).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    expectScheduleAndWaitOnce(extractionNamespace);\n+    expectEntryGetCacheStateOnce(versionedCache);\n+    expectEmptyCache();\n+    expectVersionOnce(\"0\");\n+    expectEntryCloseOnce();\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     final LookupExtractor extractor = namespaceLookupExtractorFactory.get();\n     Assert.assertNull(extractor.apply(\"foo\"));\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n+  }\n+\n+  private void expectEmptyCache()\n+  {\n+    EasyMock.expect(entry.getCache()).andReturn(new HashMap<String, String>()).anyTimes();\n+    EasyMock.expect(versionedCache.getCache()).andReturn(new HashMap<String, String>()).anyTimes();\n   }\n \n \n@@ -314,22 +312,14 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"0\").once();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n-            .once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(null).once();\n-    EasyMock.expect(cacheManager.delete(EasyMock.anyString())).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n+    expectScheduleAndWaitOnce(extractionNamespace);\n+    expectEntryGetCacheStateOnce(CacheScheduler.NoCache.ENTRY_CLOSED);\n+\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(namespaceLookupExtractorFactory.start());\n     try {\n@@ -340,91 +330,40 @@ public long getPollMs()\n       // NOOP\n     }\n \n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n-\n-  @Test\n-  public void testSimpleStartRacyGetDuringUpdate()\n+  private void expectEntryGetCacheStateOnce(final CacheScheduler.CacheState versionedCache)\n   {\n-    final ExtractionNamespace extractionNamespace = new ExtractionNamespace()\n-    {\n-      @Override\n-      public long getPollMs()\n-      {\n-        return 0;\n-      }\n-    };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"0\").once();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>(ImmutableMap.of(\"foo\", \"bar\")))\n-            .once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"1\").once();\n-\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"2\").once();\n-    EasyMock.expect(cacheManager.getCacheMap(EasyMock.anyString()))\n-            .andReturn(new ConcurrentHashMap<String, String>())\n-            .once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(\"2\").once();\n-    EasyMock.expect(cacheManager.checkedDelete(EasyMock.anyString())).andReturn(true).once();\n-    EasyMock.replay(cacheManager);\n-\n-    final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n-        extractionNamespace,\n-        cacheManager\n-    );\n-    Assert.assertTrue(namespaceLookupExtractorFactory.start());\n-    final LookupExtractor extractor = namespaceLookupExtractorFactory.get();\n-    Assert.assertNull(extractor.apply(\"foo\"));\n-    Assert.assertNotNull(extractor.getCacheKey());\n-    Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    EasyMock.expect(entry.getCacheState()).andReturn(versionedCache).once();\n   }\n \n+  private IExpectationSetters<String> expectVersionOnce(String version)\n+  {\n+    return EasyMock.expect(versionedCache.getVersion()).andReturn(version).once();\n+  }\n \n-  @Test\n-  public void testSimpleStartRacyGetAfterDelete()\n+  private void expectFooBarCache()\n   {\n-    final ExtractionNamespace extractionNamespace = new ExtractionNamespace()\n-    {\n-      @Override\n-      public long getPollMs()\n-      {\n-        return 0;\n-      }\n-    };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(true).once();\n-    EasyMock.expect(cacheManager.getVersion(EasyMock.anyString())).andReturn(null).once();\n-    EasyMock.replay(cacheManager);\n+    EasyMock.expect(versionedCache.getCache()).andReturn(new HashMap<>(ImmutableMap.of(\"foo\", \"bar\"))).once();\n+  }\n \n-    final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n-        extractionNamespace,\n-        cacheManager\n-    );\n-    Assert.assertTrue(namespaceLookupExtractorFactory.start());\n+  private void expectScheduleAndWaitOnce(ExtractionNamespace extractionNamespace)\n+  {\n     try {\n-      namespaceLookupExtractorFactory.get();\n-      Assert.fail(\"Should have thrown ISE\");\n+      EasyMock.expect(scheduler.scheduleAndWait(\n+          EasyMock.eq(extractionNamespace),\n+          EasyMock.eq(60000L)\n+      )).andReturn(entry).once();\n     }\n-    catch (ISE ise) {\n-      // NOOP\n+    catch (InterruptedException e) {\n+      throw new AssertionError(e);\n     }\n-\n-    EasyMock.verify(cacheManager);\n   }\n \n \n   @Test\n-  public void testSartFailsToSchedule()\n+  public void testStartFailsToSchedule()\n   {\n     final ExtractionNamespace extractionNamespace = new ExtractionNamespace()\n     {\n@@ -434,43 +373,47 @@ public long getPollMs()\n         return 0;\n       }\n     };\n-    EasyMock.expect(cacheManager.scheduleAndWait(\n-        EasyMock.anyString(),\n-        EasyMock.eq(extractionNamespace),\n-        EasyMock.eq(60000L)\n-    )).andReturn(false).once();\n-    EasyMock.replay(cacheManager);\n+    try {\n+      EasyMock.expect(scheduler.scheduleAndWait(\n+          EasyMock.eq(extractionNamespace),\n+          EasyMock.eq(60000L)\n+      )).andReturn(null).once();\n+    }\n+    catch (InterruptedException e) {\n+      throw new AssertionError(e);\n+    }\n+    mockReplay();\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertFalse(namespaceLookupExtractorFactory.start());\n     // true because it never fully started\n     Assert.assertTrue(namespaceLookupExtractorFactory.close());\n-    EasyMock.verify(cacheManager);\n+    mockVerify();\n   }\n \n   @Test\n   public void testReplaces()\n   {\n-    final ExtractionNamespace en1 = EasyMock.createStrictMock(ExtractionNamespace.class), en2 = EasyMock.createStrictMock(\n+    final ExtractionNamespace en1 = PowerMock.createStrictMock(ExtractionNamespace.class), en2 = PowerMock.createStrictMock(\n         ExtractionNamespace.class);\n-    EasyMock.replay(en1, en2);\n+    PowerMock.replay(en1, en2);\n     final NamespaceLookupExtractorFactory f1 = new NamespaceLookupExtractorFactory(\n         en1,\n-        cacheManager\n-    ), f2 = new NamespaceLookupExtractorFactory(en2, cacheManager), f1b = new NamespaceLookupExtractorFactory(\n+        scheduler\n+    ), f2 = new NamespaceLookupExtractorFactory(en2, scheduler), f1b = new NamespaceLookupExtractorFactory(\n         en1,\n-        cacheManager\n+        scheduler\n     );\n     Assert.assertTrue(f1.replaces(f2));\n     Assert.assertTrue(f2.replaces(f1));\n     Assert.assertFalse(f1.replaces(f1b));\n     Assert.assertFalse(f1b.replaces(f1));\n     Assert.assertFalse(f1.replaces(f1));\n     Assert.assertTrue(f1.replaces(EasyMock.createNiceMock(LookupExtractorFactory.class)));\n-    EasyMock.verify(en1, en2);\n+    PowerMock.verify(en1, en2);\n   }\n \n   @Test(expected = ISE.class)\n@@ -487,7 +430,7 @@ public long getPollMs()\n \n     final NamespaceLookupExtractorFactory namespaceLookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        cacheManager\n+        scheduler\n     );\n \n     namespaceLookupExtractorFactory.get();\n@@ -584,78 +527,63 @@ public void configure(Binder binder)\n   @Test\n   public void testExceptionalIntrospectionHandler() throws Exception\n   {\n-    final NamespaceExtractionCacheManager manager = EasyMock.createStrictMock(NamespaceExtractionCacheManager.class);\n-    final ExtractionNamespace extractionNamespace = EasyMock.createStrictMock(ExtractionNamespace.class);\n-    EasyMock.expect(manager.scheduleAndWait(EasyMock.anyString(), EasyMock.eq(extractionNamespace), EasyMock.anyLong()))\n-            .andReturn(true)\n+    final ExtractionNamespace extractionNamespace = PowerMock.createStrictMock(ExtractionNamespace.class);\n+    EasyMock.expect(scheduler.scheduleAndWait(EasyMock.eq(extractionNamespace), EasyMock.anyLong()))\n+            .andReturn(entry)\n             .once();\n-    EasyMock.replay(manager);\n+    mockReplay();\n     final LookupExtractorFactory lookupExtractorFactory = new NamespaceLookupExtractorFactory(\n         extractionNamespace,\n-        manager\n+        scheduler\n     );\n     Assert.assertTrue(lookupExtractorFactory.start());\n \n     final LookupIntrospectHandler handler = lookupExtractorFactory.getIntrospectHandler();\n     Assert.assertNotNull(handler);\n     final Class<? extends LookupIntrospectHandler> clazz = handler.getClass();\n \n-    synchronized (manager) {\n-      EasyMock.verify(manager);\n-      EasyMock.reset(manager);\n-      EasyMock.expect(manager.getVersion(EasyMock.anyString())).andReturn(null).once();\n-      EasyMock.replay(manager);\n-    }\n+    mockVerify();\n+    mockReset();\n+    EasyMock.expect(entry.getCacheState()).andReturn(CacheScheduler.NoCache.CACHE_NOT_INITIALIZED).once();\n+    mockReplay();\n+\n     final Response response = (Response) clazz.getMethod(\"getVersion\").invoke(handler);\n     Assert.assertEquals(404, response.getStatus());\n \n+    validateNotFound(\"getKeys\", handler, clazz);\n+    validateNotFound(\"getValues\", handler, clazz);\n+    validateNotFound(\"getMap\", handler, clazz);\n+    mockVerify();\n+  }\n \n-    validateCode(\n-        new ISE(\"some exception\"),\n-        404,\n-        \"getKeys\",\n-        handler,\n-        manager,\n-        clazz\n-    );\n-\n-    validateCode(\n-        new ISE(\"some exception\"),\n-        404,\n-        \"getValues\",\n-        handler,\n-        manager,\n-        clazz\n-    );\n+  private void mockReplay()\n+  {\n+    PowerMock.replay(scheduler, entry, versionedCache);\n+  }\n \n-    validateCode(\n-        new ISE(\"some exception\"),\n-        404,\n-        \"getMap\",\n-        handler,\n-        manager,\n-        clazz\n-    );\n+  private void mockReset()\n+  {\n+    PowerMock.reset(scheduler, entry, versionedCache);\n+  }\n \n-    EasyMock.verify(manager);\n+  private void mockVerify()\n+  {\n+    PowerMock.verify(scheduler, entry, versionedCache);\n   }\n \n-  void validateCode(\n-      Throwable thrown,\n-      int expectedCode,\n+  private void validateNotFound(\n       String method,\n       LookupIntrospectHandler handler,\n-      NamespaceExtractionCacheManager manager,\n       Class<? extends LookupIntrospectHandler> clazz\n   ) throws Exception\n   {\n-    synchronized (manager) {\n-      EasyMock.verify(manager);\n-      EasyMock.reset(manager);\n-      EasyMock.expect(manager.getVersion(EasyMock.anyString())).andThrow(thrown).once();\n-      EasyMock.replay(manager);\n-    }\n+    mockVerify();\n+    mockReset();\n+    expectEntryGetCacheStateOnce(versionedCache);\n+    expectEmptyCache();\n+    EasyMock.expect(versionedCache.getVersion()).andThrow(new ISE(\"some exception\")).once();\n+    mockReplay();\n     final Response response = (Response) clazz.getMethod(method).invoke(handler);\n-    Assert.assertEquals(expectedCode, response.getStatus());\n+    Assert.assertEquals(404, response.getStatus());\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/query/lookup/NamespaceLookupExtractorFactoryTest.java",
                "sha": "30a19eaffda0dc6fe0dc6191b9591182cad4053c",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/NamespacedExtractorModuleTest.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/NamespacedExtractorModuleTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 35,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/NamespacedExtractorModuleTest.java",
                "patch": "@@ -21,31 +21,29 @@\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.collect.ImmutableMap;\n-\n import io.druid.data.SearchableVersionedDataFinder;\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.lifecycle.Lifecycle;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n+import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.JDBCExtractionNamespace;\n import io.druid.query.lookup.namespace.URIExtractionNamespace;\n import io.druid.query.lookup.namespace.URIExtractionNamespaceTest;\n import io.druid.segment.loading.LocalFileTimestampVersionFinder;\n-import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n import io.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager;\n import io.druid.server.metrics.NoopServiceEmitter;\n import org.joda.time.Period;\n-import org.junit.AfterClass;\n+import org.junit.After;\n import org.junit.Assert;\n-import org.junit.BeforeClass;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n \n import java.io.File;\n import java.io.FileWriter;\n import java.io.OutputStreamWriter;\n-import java.util.HashMap;\n import java.util.Map;\n \n /**\n@@ -54,14 +52,14 @@\n public class NamespacedExtractorModuleTest\n {\n   private static final ObjectMapper mapper = URIExtractionNamespaceTest.registerTypes(new DefaultObjectMapper());\n-  private static NamespaceExtractionCacheManager cacheManager;\n-  private static Lifecycle lifecycle;\n+  private CacheScheduler scheduler;\n+  private Lifecycle lifecycle;\n \n   @Rule\n   public final TemporaryFolder temporaryFolder = new TemporaryFolder();\n \n-  @BeforeClass\n-  public static void setUpStatic() throws Exception\n+  @Before\n+  public void setUp() throws Exception\n   {\n     final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> factoryMap =\n         ImmutableMap.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>of(\n@@ -75,11 +73,17 @@ public static void setUpStatic() throws Exception\n             JDBCExtractionNamespace.class, new JDBCExtractionNamespaceCacheFactory()\n         );\n     lifecycle = new Lifecycle();\n-    cacheManager = new OnHeapNamespaceExtractionCacheManager(lifecycle, new NoopServiceEmitter(), factoryMap);\n+    lifecycle.start();\n+    NoopServiceEmitter noopServiceEmitter = new NoopServiceEmitter();\n+    scheduler = new CacheScheduler(\n+        noopServiceEmitter,\n+        factoryMap,\n+        new OnHeapNamespaceExtractionCacheManager(lifecycle, noopServiceEmitter)\n+    );\n   }\n \n-  @AfterClass\n-  public static void tearDownStatic() throws Exception\n+  @After\n+  public void tearDown() throws Exception\n   {\n     lifecycle.stop();\n   }\n@@ -94,7 +98,6 @@ public void testNewTask() throws Exception\n     final URIExtractionNamespaceCacheFactory factory = new URIExtractionNamespaceCacheFactory(\n         ImmutableMap.<String, SearchableVersionedDataFinder>of(\"file\", new LocalFileTimestampVersionFinder())\n     );\n-    final String namespaceID = \"ns\";\n     final URIExtractionNamespace namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n@@ -104,11 +107,11 @@ public void testNewTask() throws Exception\n         new Period(0),\n         null\n     );\n-    Map<String, String> map = new HashMap<>();\n-    factory.populateCache(namespaceID, namespace, null, map);\n+    CacheScheduler.VersionedCache versionedCache = factory.populateCache(namespace, null, null, scheduler);\n+    Assert.assertNotNull(versionedCache);\n+    Map<String, String> map = versionedCache.getCache();\n     Assert.assertEquals(\"bar\", map.get(\"foo\"));\n     Assert.assertEquals(null, map.get(\"baz\"));\n-    cacheManager.delete(namespaceID);\n   }\n \n   @Test\n@@ -118,22 +121,18 @@ public void testListNamespaces() throws Exception\n     try (OutputStreamWriter out = new FileWriter(tmpFile)) {\n       out.write(mapper.writeValueAsString(ImmutableMap.<String, String>of(\"foo\", \"bar\")));\n     }\n-    final String namespaceID = \"ns\";\n     final URIExtractionNamespace namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n         new URIExtractionNamespace.ObjectMapperFlatDataParser(URIExtractionNamespaceTest.registerTypes(new DefaultObjectMapper())),\n         new Period(0),\n         null\n     );\n-    Assert.assertTrue(cacheManager.scheduleAndWait(namespaceID, namespace, 1_000));\n-    Assert.assertArrayEquals(cacheManager.getKnownIDs().toArray(), new Object[]{namespaceID});\n-    Assert.assertTrue(cacheManager.delete(namespaceID));\n-  }\n-\n-  private static boolean noNamespaces(NamespaceExtractionCacheManager manager)\n-  {\n-    return manager.getKnownIDs().isEmpty();\n+    try (CacheScheduler.Entry entry = scheduler.scheduleAndWait(namespace, 1_000)) {\n+      Assert.assertNotNull(entry);\n+      entry.awaitTotalUpdates(1);\n+      Assert.assertEquals(1, scheduler.getActiveEntries());\n+    }\n   }\n \n   @Test//(timeout = 10_000)\n@@ -143,7 +142,6 @@ public void testDeleteNamespaces() throws Exception\n     try (OutputStreamWriter out = new FileWriter(tmpFile)) {\n       out.write(mapper.writeValueAsString(ImmutableMap.<String, String>of(\"foo\", \"bar\")));\n     }\n-    final String namespaceID = \"ns\";\n     final URIExtractionNamespace namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n@@ -153,8 +151,9 @@ public void testDeleteNamespaces() throws Exception\n         new Period(0),\n         null\n     );\n-    Assert.assertTrue(cacheManager.scheduleAndWait(namespaceID, namespace, 1_000));\n-    Assert.assertTrue(cacheManager.delete(namespaceID));\n+    try (CacheScheduler.Entry entry = scheduler.scheduleAndWait(namespace, 1_000)) {\n+      Assert.assertNotNull(entry);\n+    }\n   }\n \n   @Test\n@@ -164,7 +163,6 @@ public void testNewUpdate() throws Exception\n     try (OutputStreamWriter out = new FileWriter(tmpFile)) {\n       out.write(mapper.writeValueAsString(ImmutableMap.<String, String>of(\"foo\", \"bar\")));\n     }\n-    final String namespaceID = \"ns\";\n     final URIExtractionNamespace namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n@@ -174,10 +172,11 @@ public void testNewUpdate() throws Exception\n         new Period(0),\n         null\n     );\n-    Assert.assertTrue(noNamespaces(cacheManager));\n-    Assert.assertTrue(cacheManager.scheduleAndWait(namespaceID, namespace, 10_000));\n-    Assert.assertArrayEquals(cacheManager.getKnownIDs().toArray(), new Object[]{namespaceID});\n-\n-    Assert.assertTrue(cacheManager.delete(namespaceID));\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n+    try (CacheScheduler.Entry entry = scheduler.scheduleAndWait(namespace, 10_000)) {\n+      Assert.assertNotNull(entry);\n+      entry.awaitTotalUpdates(1);\n+      Assert.assertEquals(1, scheduler.getActiveEntries());\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/NamespacedExtractorModuleTest.java",
                "sha": "4dcb409484db26f054cee18a548710941ee16c0b",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactoryTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactoryTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 7,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactoryTest.java",
                "patch": "@@ -20,33 +20,64 @@\n package io.druid.server.lookup.namespace;\n \n import com.google.common.collect.ImmutableMap;\n+import io.druid.java.util.common.lifecycle.Lifecycle;\n+import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n+import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.StaticMapExtractionNamespace;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n+import io.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager;\n+import io.druid.server.metrics.NoopServiceEmitter;\n+import org.junit.After;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.HashMap;\n+import java.util.Collections;\n import java.util.Map;\n \n public class StaticMapExtractionNamespaceCacheFactoryTest\n {\n   private static final Map<String, String> MAP = ImmutableMap.<String, String>builder().put(\"foo\", \"bar\").build();\n \n+  private Lifecycle lifecycle;\n+  private CacheScheduler scheduler;\n+\n+  @Before\n+  public void setup() throws Exception\n+  {\n+    lifecycle = new Lifecycle();\n+    lifecycle.start();\n+    NoopServiceEmitter noopServiceEmitter = new NoopServiceEmitter();\n+    scheduler = new CacheScheduler(\n+        noopServiceEmitter,\n+        Collections.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>emptyMap(),\n+        new OnHeapNamespaceExtractionCacheManager(lifecycle, noopServiceEmitter)\n+    );\n+  }\n+\n+  @After\n+  public void tearDown()\n+  {\n+    lifecycle.stop();\n+  }\n+\n   @Test\n   public void testSimplePopulator() throws Exception\n   {\n     final StaticMapExtractionNamespaceCacheFactory factory = new StaticMapExtractionNamespaceCacheFactory();\n     final StaticMapExtractionNamespace namespace = new StaticMapExtractionNamespace(MAP);\n-    final Map<String, String> cache = new HashMap<>();\n-    Assert.assertEquals(factory.getVersion(), factory.populateCache(null, namespace, null, cache));\n-    Assert.assertEquals(MAP, cache);\n+    CacheScheduler.VersionedCache versionedCache = factory.populateCache(namespace, null, null, scheduler);\n+    Assert.assertNotNull(versionedCache);\n+    Assert.assertEquals(factory.getVersion(), versionedCache.getVersion());\n+    Assert.assertEquals(MAP, versionedCache.getCache());\n+\n   }\n \n   @Test(expected = AssertionError.class)\n-  public void testNonNullLastVersionCausesAssertionError() throws Exception\n+  public void testNonNullLastVersionCausesAssertionError()\n   {\n     final StaticMapExtractionNamespaceCacheFactory factory = new StaticMapExtractionNamespaceCacheFactory();\n     final StaticMapExtractionNamespace namespace = new StaticMapExtractionNamespace(MAP);\n-    final Map<String, String> cache = new HashMap<>();\n-    Assert.assertNull(factory.populateCache(null, namespace, factory.getVersion(), cache));\n+    factory.populateCache(namespace, null, factory.getVersion(), scheduler);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/StaticMapExtractionNamespaceCacheFactoryTest.java",
                "sha": "ccfc5f38d2d99d1694b877bc5c3810334e59191d",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactoryTest.java",
                "changes": 171,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactoryTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 89,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactoryTest.java",
                "patch": "@@ -24,7 +24,6 @@\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n-import com.metamx.emitter.service.ServiceEmitter;\n import io.druid.data.SearchableVersionedDataFinder;\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.IAE;\n@@ -35,8 +34,9 @@\n import io.druid.query.lookup.namespace.URIExtractionNamespace;\n import io.druid.query.lookup.namespace.URIExtractionNamespaceTest;\n import io.druid.segment.loading.LocalFileTimestampVersionFinder;\n+import io.druid.server.lookup.namespace.cache.CacheScheduler;\n import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager;\n-import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagersTest;\n+import io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagerExecutorsTest;\n import io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager;\n import io.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager;\n import io.druid.server.metrics.NoopServiceEmitter;\n@@ -58,8 +58,6 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n-import java.lang.reflect.Constructor;\n-import java.lang.reflect.InvocationTargetException;\n import java.net.URI;\n import java.net.URISyntaxException;\n import java.nio.file.Files;\n@@ -69,8 +67,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n+import java.util.UUID;\n import java.util.concurrent.ExecutionException;\n import java.util.regex.Pattern;\n import java.util.zip.GZIPOutputStream;\n@@ -176,17 +173,23 @@ public void close() throws IOException\n         }\n     );\n \n-    final List<Constructor<? extends NamespaceExtractionCacheManager>> cacheConstructors = ImmutableList.<Constructor<? extends NamespaceExtractionCacheManager>>of(\n-        OnHeapNamespaceExtractionCacheManager.class.getConstructor(\n-            Lifecycle.class,\n-            ServiceEmitter.class,\n-            Map.class\n-        ),\n-        OffHeapNamespaceExtractionCacheManager.class.getConstructor(\n-            Lifecycle.class,\n-            ServiceEmitter.class,\n-            Map.class\n-        )\n+    final List<Function<Lifecycle, NamespaceExtractionCacheManager>> cacheManagerCreators = ImmutableList.of(\n+        new Function<Lifecycle, NamespaceExtractionCacheManager>()\n+        {\n+          @Override\n+          public NamespaceExtractionCacheManager apply(Lifecycle lifecycle)\n+          {\n+            return new OnHeapNamespaceExtractionCacheManager(lifecycle, new NoopServiceEmitter());\n+          }\n+        },\n+        new Function<Lifecycle, NamespaceExtractionCacheManager>()\n+        {\n+          @Override\n+          public NamespaceExtractionCacheManager apply(Lifecycle lifecycle)\n+          {\n+            return new OffHeapNamespaceExtractionCacheManager(lifecycle, new NoopServiceEmitter());\n+          }\n+        }\n     );\n     return new Iterable<Object[]>()\n     {\n@@ -196,46 +199,24 @@ public void close() throws IOException\n         return new Iterator<Object[]>()\n         {\n           Iterator<Object[]> compressionIt = compressionParams.iterator();\n-          Iterator<Constructor<? extends NamespaceExtractionCacheManager>> cacheConstructorIt = cacheConstructors.iterator();\n+          Iterator<Function<Lifecycle, NamespaceExtractionCacheManager>> cacheManagerCreatorsIt =\n+              cacheManagerCreators.iterator();\n           Object[] compressions = compressionIt.next();\n \n           @Override\n           public boolean hasNext()\n           {\n-            return compressionIt.hasNext() || cacheConstructorIt.hasNext();\n+            return compressionIt.hasNext() || cacheManagerCreatorsIt.hasNext();\n           }\n \n           @Override\n           public Object[] next()\n           {\n-            if (cacheConstructorIt.hasNext()) {\n-              Constructor<? extends NamespaceExtractionCacheManager> constructor = cacheConstructorIt.next();\n-              final NamespaceExtractionCacheManager manager;\n-              try {\n-                manager = constructor.newInstance(\n-                    new Lifecycle(),\n-                    new NoopServiceEmitter(),\n-                    new HashMap<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>()\n-                );\n-              }\n-              catch (Exception e) {\n-                throw Throwables.propagate(e);\n-              }\n-              ConcurrentHashMap<String, Function<String, String>> fnCache = new ConcurrentHashMap<String, Function<String, String>>();\n-              try {\n-                return new Object[]{\n-                    String.format(\n-                        \"[%s]:[%s]\",\n-                        compressions[0],\n-                        manager.getClass().getCanonicalName()\n-                    ), compressions[0], compressions[1], constructor\n-                };\n-              }\n-              catch (Exception e) {\n-                throw Throwables.propagate(e);\n-              }\n+            if (cacheManagerCreatorsIt.hasNext()) {\n+              Function<Lifecycle, NamespaceExtractionCacheManager> cacheManagerCreator = cacheManagerCreatorsIt.next();\n+              return new Object[]{ compressions[0], compressions[1], cacheManagerCreator };\n             } else {\n-              cacheConstructorIt = cacheConstructors.iterator();\n+              cacheManagerCreatorsIt = cacheManagerCreators.iterator();\n               compressions = compressionIt.next();\n               return next();\n             }\n@@ -252,20 +233,19 @@ public void remove()\n   }\n \n   public URIExtractionNamespaceCacheFactoryTest(\n-      String friendlyName,\n       String suffix,\n       Function<File, OutputStream> outStreamSupplier,\n-      Constructor<? extends NamespaceExtractionCacheManager> cacheManagerConstructor\n-  ) throws IllegalAccessException, InvocationTargetException, InstantiationException\n+      Function<Lifecycle, NamespaceExtractionCacheManager> cacheManagerCreator\n+  ) throws Exception\n   {\n     final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> namespaceFunctionFactoryMap = new HashMap<>();\n     this.suffix = suffix;\n     this.outStreamSupplier = outStreamSupplier;\n     this.lifecycle = new Lifecycle();\n-    this.manager = cacheManagerConstructor.newInstance(\n-        lifecycle,\n+    this.scheduler = new CacheScheduler(\n         new NoopServiceEmitter(),\n-        namespaceFunctionFactoryMap\n+        namespaceFunctionFactoryMap,\n+        cacheManagerCreator.apply(lifecycle)\n     );\n     namespaceFunctionFactoryMap.put(\n         URIExtractionNamespace.class,\n@@ -280,12 +260,11 @@ public URIExtractionNamespaceCacheFactoryTest(\n   private final String suffix;\n   private final Function<File, OutputStream> outStreamSupplier;\n   private Lifecycle lifecycle;\n-  private NamespaceExtractionCacheManager manager;\n+  private CacheScheduler scheduler;\n   private File tmpFile;\n   private File tmpFileParent;\n-  private URIExtractionNamespaceCacheFactory factory;\n+  private URIExtractionNamespaceCacheFactory populator;\n   private URIExtractionNamespace namespace;\n-  private String id;\n \n   @Before\n   public void setUp() throws Exception\n@@ -310,7 +289,7 @@ public void setUp() throws Exception\n         )));\n       }\n     }\n-    factory = new URIExtractionNamespaceCacheFactory(FINDERS);\n+    populator = new URIExtractionNamespaceCacheFactory(FINDERS);\n     namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n@@ -320,7 +299,6 @@ public void setUp() throws Exception\n         new Period(0),\n         null\n     );\n-    id = \"ns\";\n   }\n \n   @After\n@@ -332,17 +310,17 @@ public void tearDown()\n   @Test\n   public void simpleTest() throws IOException, ExecutionException, InterruptedException\n   {\n-    Assert.assertTrue(manager.getKnownIDs().isEmpty());\n-    NamespaceExtractionCacheManagersTest.waitFor(manager.schedule(id, namespace));\n-    Map<String, String> map = manager.getCacheMap(id);\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n+    CacheScheduler.Entry entry = scheduler.schedule(namespace);\n+    NamespaceExtractionCacheManagerExecutorsTest.waitFor(entry);\n+    Map<String, String> map = entry.getCache();\n     Assert.assertEquals(\"bar\", map.get(\"foo\"));\n     Assert.assertEquals(null, map.get(\"baz\"));\n   }\n \n   @Test\n   public void simpleTestRegex() throws IOException, ExecutionException, InterruptedException\n   {\n-    String regexID = \"regex\";\n     final URIExtractionNamespace namespace = new URIExtractionNamespace(\n         null,\n         Paths.get(this.namespace.getUri()).getParent().toUri(),\n@@ -351,9 +329,9 @@ public void simpleTestRegex() throws IOException, ExecutionException, Interrupte\n         Period.millis((int) this.namespace.getPollMs()),\n         null\n     );\n-    Assert.assertTrue(!manager.getKnownIDs().contains(regexID));\n-    NamespaceExtractionCacheManagersTest.waitFor(manager.schedule(regexID, namespace));\n-    Map<String, String> map = manager.getCacheMap(regexID);\n+    CacheScheduler.Entry entry = scheduler.schedule(namespace);\n+    NamespaceExtractionCacheManagerExecutorsTest.waitFor(entry);\n+    Map<String, String> map = entry.getCache();\n     Assert.assertNotNull(map);\n     Assert.assertEquals(\"bar\", map.get(\"foo\"));\n     Assert.assertEquals(null, map.get(\"baz\"));\n@@ -363,10 +341,8 @@ public void simpleTestRegex() throws IOException, ExecutionException, Interrupte\n   public void simplePileONamespacesTest() throws InterruptedException\n   {\n     final int size = 128;\n-    List<String> ids = new ArrayList<>(size);\n+    List<CacheScheduler.Entry> entries = new ArrayList<>(size);\n     for (int i = 0; i < size; ++i) {\n-      String id = String.format(\"%d-ns-%d\", i << 10, i);\n-      ids.add(id);\n       URIExtractionNamespace namespace = new URIExtractionNamespace(\n           tmpFile.toURI(),\n           null, null,\n@@ -377,35 +353,34 @@ public void simplePileONamespacesTest() throws InterruptedException\n           null\n       );\n \n-      Assert.assertFalse(manager.getKnownIDs().contains(id));\n-      NamespaceExtractionCacheManagersTest.waitFor(manager.schedule(id, namespace));\n+      CacheScheduler.Entry entry = scheduler.schedule(namespace);\n+      entries.add(entry);\n+      NamespaceExtractionCacheManagerExecutorsTest.waitFor(entry);\n     }\n \n-    for (String id : ids) {\n-      final Map<String, String> map = manager.getCacheMap(id);\n+    for (CacheScheduler.Entry entry : entries) {\n+      final Map<String, String> map = entry.getCache();\n       Assert.assertEquals(\"bar\", map.get(\"foo\"));\n       Assert.assertEquals(null, map.get(\"baz\"));\n-      manager.delete(id);\n+      entry.close();\n     }\n-    Assert.assertTrue(manager.getKnownIDs().isEmpty());\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n   }\n \n   @Test\n   public void testLoadOnlyOnce() throws Exception\n   {\n-    Assert.assertTrue(manager.getKnownIDs().isEmpty());\n-\n-    ConcurrentMap<String, String> map = new ConcurrentHashMap<>();\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n \n-    String v = factory.populateCache(id, namespace, null, map);\n+    CacheScheduler.VersionedCache versionedCache = populator.populateCache(namespace, null, null, scheduler);\n+    Assert.assertNotNull(versionedCache);\n+    Map<String, String> map = versionedCache.getCache();\n     Assert.assertEquals(\"bar\", map.get(\"foo\"));\n     Assert.assertEquals(null, map.get(\"baz\"));\n-    Assert.assertNotNull(v);\n+    String version = versionedCache.getVersion();\n+    Assert.assertNotNull(version);\n \n-    String v2 = factory.populateCache(id, namespace, v, map);\n-    Assert.assertEquals(v, v2);\n-    Assert.assertEquals(\"bar\", map.get(\"foo\"));\n-    Assert.assertEquals(null, map.get(\"baz\"));\n+    Assert.assertNull(populator.populateCache(namespace, null, version, scheduler));\n   }\n \n   @Test(expected = FileNotFoundException.class)\n@@ -419,14 +394,12 @@ public void testMissing() throws Exception\n         null\n     );\n     Assert.assertTrue(new File(namespace.getUri()).delete());\n-    ConcurrentMap<String, String> map = new ConcurrentHashMap<>();\n-    factory.populateCache(id, badNamespace, null, map);\n+    populator.populateCache(badNamespace, null, null, scheduler);\n   }\n \n   @Test(expected = FileNotFoundException.class)\n   public void testMissingRegex() throws Exception\n   {\n-    String badId = \"bad\";\n     URIExtractionNamespace badNamespace = new URIExtractionNamespace(\n         null,\n         Paths.get(namespace.getUri()).getParent().toUri(),\n@@ -436,8 +409,7 @@ public void testMissingRegex() throws Exception\n         null\n     );\n     Assert.assertTrue(new File(namespace.getUri()).delete());\n-    ConcurrentMap<String, String> map = new ConcurrentHashMap<>();\n-    factory.populateCache(badId, badNamespace, null, map);\n+    populator.populateCache(badNamespace, null, null, scheduler);\n   }\n \n   @Test(expected = IAE.class)\n@@ -525,7 +497,28 @@ public void testWeirdSchemaOnExactURI() throws Exception\n         Period.millis((int) namespace.getPollMs()),\n         null\n     );\n-    final Map<String, String> map = new HashMap<>();\n-    Assert.assertNotNull(factory.populateCache(id, extractionNamespace, null, map));\n+    Assert.assertNotNull(populator.populateCache(extractionNamespace, null, null, scheduler));\n+  }\n+\n+  @Test(timeout = 10_000)\n+  public void testDeleteOnScheduleFail() throws Exception\n+  {\n+    Assert.assertNull(scheduler.scheduleAndWait(\n+        new URIExtractionNamespace(\n+            new URI(\"file://tmp/I_DONT_REALLY_EXIST\" +\n+                    UUID.randomUUID().toString()),\n+            null,\n+            null,\n+            new URIExtractionNamespace.JSONFlatDataParser(\n+                new DefaultObjectMapper(),\n+                \"key\",\n+                \"val\"\n+            ),\n+            Period.millis(10000),\n+            null\n+        ),\n+        500\n+    ));\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/URIExtractionNamespaceCacheFactoryTest.java",
                "sha": "5699ccde71ddb16009b414512bbed867ad7e9727",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/JDBCExtractionNamespaceTest.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/JDBCExtractionNamespaceTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 59,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/JDBCExtractionNamespaceTest.java",
                "patch": "@@ -68,7 +68,6 @@\n   @Rule\n   public final TestDerbyConnector.DerbyConnectorRule derbyConnectorRule = new TestDerbyConnector.DerbyConnectorRule();\n   private static final Logger log = new Logger(JDBCExtractionNamespaceTest.class);\n-  private static final String namespace = \"testNamespace\";\n   private static final String tableName = \"abstractDbRenameTest\";\n   private static final String keyName = \"keyName\";\n   private static final String valName = \"valName\";\n@@ -97,18 +96,23 @@ public JDBCExtractionNamespaceTest(\n   }\n \n   private final String tsColumn;\n-  private OnHeapNamespaceExtractionCacheManager extractionCacheManager;\n-  private final Lifecycle lifecycle = new Lifecycle();\n-  private final AtomicLong updates = new AtomicLong(0L);\n-  private final Lock updateLock = new ReentrantLock(true);\n-  private final Closer closer = Closer.create();\n-  private final ListeningExecutorService setupTeardownService =\n-      MoreExecutors.listeningDecorator(Execs.multiThreaded(2, \"JDBCExtractionNamespaceTeardown--%s\"));\n+  private CacheScheduler scheduler;\n+  private Lifecycle lifecycle;\n+  private AtomicLong updates;\n+  private Lock updateLock;\n+  private Closer closer;\n+  private ListeningExecutorService setupTeardownService;\n   private Handle handleRef = null;\n \n   @Before\n   public void setup() throws Exception\n   {\n+    lifecycle = new Lifecycle();\n+    updates = new AtomicLong(0L);\n+    updateLock = new ReentrantLock(true);\n+    closer = Closer.create();\n+    setupTeardownService =\n+        MoreExecutors.listeningDecorator(Execs.multiThreaded(2, \"JDBCExtractionNamespaceTeardown--%s\"));\n     final ListenableFuture<Handle> setupFuture = setupTeardownService.submit(\n         new Callable<Handle>()\n         {\n@@ -164,15 +168,10 @@ public void close() throws IOException\n               @Override\n               public void close() throws IOException\n               {\n-                if (extractionCacheManager == null) {\n+                if (scheduler == null) {\n                   return;\n                 }\n-                final NamespaceExtractionCacheManager.NamespaceImplData implData = extractionCacheManager.implData.get(\n-                    namespace);\n-                if (implData != null && implData.future != null) {\n-                  implData.future.cancel(true);\n-                  Assert.assertTrue(implData.future.isDone());\n-                }\n+                Assert.assertEquals(0, scheduler.getActiveEntries());\n               }\n             });\n             for (Map.Entry<String, String> entry : renames.entrySet()) {\n@@ -185,31 +184,28 @@ public void close() throws IOException\n               }\n             }\n \n-            extractionCacheManager = new OnHeapNamespaceExtractionCacheManager(\n-                lifecycle,\n-                new NoopServiceEmitter(),\n+            NoopServiceEmitter noopServiceEmitter = new NoopServiceEmitter();\n+            scheduler = new CacheScheduler(\n+                noopServiceEmitter,\n                 ImmutableMap.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>of(\n                     JDBCExtractionNamespace.class,\n-                    new JDBCExtractionNamespaceCacheFactory()\n+                    new ExtractionNamespaceCacheFactory<JDBCExtractionNamespace>()\n                     {\n+                      private final JDBCExtractionNamespaceCacheFactory delegate =\n+                          new JDBCExtractionNamespaceCacheFactory();\n                       @Override\n-                      public String populateCache(\n-                          final String id,\n+                      public CacheScheduler.VersionedCache populateCache(\n                           final JDBCExtractionNamespace namespace,\n+                          final CacheScheduler.EntryImpl<JDBCExtractionNamespace> id,\n                           final String lastVersion,\n-                          final Map<String, String> cache\n-                      ) throws Exception\n+                          final CacheScheduler scheduler\n+                      ) throws InterruptedException\n                       {\n                         updateLock.lockInterruptibly();\n                         try {\n                           log.debug(\"Running cache populator\");\n                           try {\n-                            return super.populateCache(\n-                                id,\n-                                namespace,\n-                                lastVersion,\n-                                cache\n-                            );\n+                            return delegate.populateCache(namespace, id, lastVersion, scheduler);\n                           }\n                           finally {\n                             updates.incrementAndGet();\n@@ -220,7 +216,8 @@ public String populateCache(\n                         }\n                       }\n                     }\n-                )\n+                ),\n+                new OnHeapNamespaceExtractionCacheManager(lifecycle, noopServiceEmitter)\n             );\n             try {\n               lifecycle.start();\n@@ -367,46 +364,44 @@ public void testMapping()\n         tsColumn,\n         new Period(0)\n     );\n-    NamespaceExtractionCacheManagersTest.waitFor(extractionCacheManager.schedule(namespace, extractionNamespace));\n-    final Map<String, String> map = extractionCacheManager.getCacheMap(namespace);\n+    try (CacheScheduler.Entry entry = scheduler.schedule(extractionNamespace)) {\n+      NamespaceExtractionCacheManagerExecutorsTest.waitFor(entry);\n+      final Map<String, String> map = entry.getCache();\n \n-    for (Map.Entry<String, String> entry : renames.entrySet()) {\n-      String key = entry.getKey();\n-      String val = entry.getValue();\n-      Assert.assertEquals(\"non-null check\", Strings.emptyToNull(val), Strings.emptyToNull(map.get(key)));\n+      for (Map.Entry<String, String> e : renames.entrySet()) {\n+        String key = e.getKey();\n+        String val = e.getValue();\n+        Assert.assertEquals(\"non-null check\", Strings.emptyToNull(val), Strings.emptyToNull(map.get(key)));\n+      }\n+      Assert.assertEquals(\"null check\", null, map.get(\"baz\"));\n     }\n-    Assert.assertEquals(\"null check\", null, map.get(\"baz\"));\n   }\n \n   @Test(timeout = 10_000L)\n   public void testSkipOld()\n       throws NoSuchFieldException, IllegalAccessException, ExecutionException, InterruptedException\n   {\n-    final JDBCExtractionNamespace extractionNamespace = ensureNamespace();\n-\n-    assertUpdated(namespace, \"foo\", \"bar\");\n-\n-    if (tsColumn != null) {\n-      insertValues(handleRef, \"foo\", \"baz\", \"1900-01-01 00:00:00\");\n+    try (final CacheScheduler.Entry entry = ensureEntry()) {\n+      assertUpdated(entry, \"foo\", \"bar\");\n+      if (tsColumn != null) {\n+        insertValues(handleRef, \"foo\", \"baz\", \"1900-01-01 00:00:00\");\n+      }\n+      assertUpdated(entry, \"foo\", \"bar\");\n     }\n-\n-    assertUpdated(namespace, \"foo\", \"bar\");\n   }\n \n   @Test(timeout = 60_000L)\n   public void testFindNew()\n       throws NoSuchFieldException, IllegalAccessException, ExecutionException, InterruptedException\n   {\n-    final JDBCExtractionNamespace extractionNamespace = ensureNamespace();\n-\n-    assertUpdated(namespace, \"foo\", \"bar\");\n-\n-    insertValues(handleRef, \"foo\", \"baz\", \"2900-01-01 00:00:00\");\n-\n-    assertUpdated(namespace, \"foo\", \"baz\");\n+    try (final CacheScheduler.Entry entry = ensureEntry()) {\n+      assertUpdated(entry, \"foo\", \"bar\");\n+      insertValues(handleRef, \"foo\", \"baz\", \"2900-01-01 00:00:00\");\n+      assertUpdated(entry, \"foo\", \"baz\");\n+    }\n   }\n \n-  private JDBCExtractionNamespace ensureNamespace()\n+  private CacheScheduler.Entry ensureEntry()\n       throws NoSuchFieldException, IllegalAccessException, InterruptedException\n   {\n     final JDBCExtractionNamespace extractionNamespace = new JDBCExtractionNamespace(\n@@ -417,16 +412,16 @@ private JDBCExtractionNamespace ensureNamespace()\n         tsColumn,\n         new Period(10)\n     );\n-    extractionCacheManager.schedule(namespace, extractionNamespace);\n+    CacheScheduler.Entry entry = scheduler.schedule(extractionNamespace);\n \n     waitForUpdates(1_000L, 2L);\n \n     Assert.assertEquals(\n         \"sanity check not correct\",\n         \"bar\",\n-        extractionCacheManager.getCacheMap(namespace).get(\"foo\")\n+        entry.getCache().get(\"foo\")\n     );\n-    return extractionNamespace;\n+    return entry;\n   }\n \n   private void waitForUpdates(long timeout, long numUpdates) throws InterruptedException\n@@ -456,16 +451,16 @@ private void waitForUpdates(long timeout, long numUpdates) throws InterruptedExc\n     } while (post < pre + numUpdates);\n   }\n \n-  private void assertUpdated(String namespace, String key, String expected) throws InterruptedException\n+  private void assertUpdated(CacheScheduler.Entry entry, String key, String expected) throws InterruptedException\n   {\n     waitForUpdates(1_000L, 2L);\n \n-    Map<String, String> map = extractionCacheManager.getCacheMap(namespace);\n+    Map<String, String> map = entry.getCache();\n \n     // rely on test timeout to break out of this loop\n     while (!expected.equals(map.get(key))) {\n       Thread.sleep(100);\n-      map = extractionCacheManager.getCacheMap(namespace);\n+      map = entry.getCache();\n     }\n \n     Assert.assertEquals(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/JDBCExtractionNamespaceTest.java",
                "sha": "702142f0798f73d8b3fb080c12f08226aa9abb5e",
                "status": "modified"
            },
            {
                "additions": 179,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagerExecutorsTest.java",
                "changes": 422,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagerExecutorsTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 243,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagerExecutorsTest.java",
                "patch": "@@ -20,22 +20,18 @@\n package io.druid.server.lookup.namespace.cache;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Function;\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.ListeningExecutorService;\n-import com.google.common.util.concurrent.ListeningScheduledExecutorService;\n import com.google.common.util.concurrent.MoreExecutors;\n import io.druid.concurrent.Execs;\n-import io.druid.data.SearchableVersionedDataFinder;\n-import io.druid.java.util.common.IAE;\n import io.druid.java.util.common.lifecycle.Lifecycle;\n import io.druid.query.lookup.namespace.ExtractionNamespace;\n import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import io.druid.query.lookup.namespace.URIExtractionNamespace;\n import io.druid.query.lookup.namespace.URIExtractionNamespaceTest;\n-import io.druid.segment.loading.LocalFileTimestampVersionFinder;\n-import io.druid.server.lookup.namespace.URIExtractionNamespaceCacheFactory;\n import io.druid.server.metrics.NoopServiceEmitter;\n import org.joda.time.Period;\n import org.junit.After;\n@@ -44,91 +40,118 @@\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n \n+import javax.annotation.Nullable;\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n-import java.lang.reflect.Field;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.Map;\n import java.util.concurrent.CancellationException;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicLong;\n \n /**\n  *\n  */\n+@RunWith(Parameterized.class)\n public class NamespaceExtractionCacheManagerExecutorsTest\n {\n+  public static final Function<Lifecycle, NamespaceExtractionCacheManager> CREATE_ON_HEAP_CACHE_MANAGER =\n+      new Function<Lifecycle, NamespaceExtractionCacheManager>()\n+      {\n+        @Nullable\n+        @Override\n+        public NamespaceExtractionCacheManager apply(@Nullable Lifecycle lifecycle)\n+        {\n+          return new OnHeapNamespaceExtractionCacheManager(lifecycle, new NoopServiceEmitter());\n+        }\n+      };\n+  public static final Function<Lifecycle, NamespaceExtractionCacheManager> CREATE_OFF_HEAP_CACHE_MANAGER =\n+      new Function<Lifecycle, NamespaceExtractionCacheManager>()\n+      {\n+        @Nullable\n+        @Override\n+        public NamespaceExtractionCacheManager apply(@Nullable Lifecycle lifecycle)\n+        {\n+          return new OffHeapNamespaceExtractionCacheManager(lifecycle, new NoopServiceEmitter());\n+        }\n+      };\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> data()\n+  {\n+    return Arrays.asList(new Object[][]{{CREATE_ON_HEAP_CACHE_MANAGER}});\n+  }\n+\n+  public static void waitFor(CacheScheduler.Entry entry) throws InterruptedException\n+  {\n+    entry.awaitTotalUpdates(1);\n+  }\n+\n+\n   private static final String KEY = \"foo\";\n   private static final String VALUE = \"bar\";\n+\n   @Rule\n   public final TemporaryFolder temporaryFolder = new TemporaryFolder();\n+  private final Function<Lifecycle, NamespaceExtractionCacheManager> createCacheManager;\n   private Lifecycle lifecycle;\n-  private NamespaceExtractionCacheManager manager;\n+  private NamespaceExtractionCacheManager cacheManager;\n+  private CacheScheduler scheduler;\n   private File tmpFile;\n-  private final ConcurrentMap<String, Object> cacheUpdateAlerts = new ConcurrentHashMap<>();\n \n-  private final AtomicLong numRuns = new AtomicLong(0L);\n+  public NamespaceExtractionCacheManagerExecutorsTest(\n+      Function<Lifecycle, NamespaceExtractionCacheManager> createCacheManager\n+  )\n+  {\n+    this.createCacheManager = createCacheManager;\n+  }\n \n   @Before\n   public void setUp() throws Exception\n   {\n-    final Path tmpDir = temporaryFolder.newFolder().toPath();\n     lifecycle = new Lifecycle();\n-    // Lifecycle stop is used to shut down executors. Start does nothing, so it's ok to call it here.\n     lifecycle.start();\n-    final URIExtractionNamespaceCacheFactory factory = new URIExtractionNamespaceCacheFactory(\n-        ImmutableMap.<String, SearchableVersionedDataFinder>of(\"file\", new LocalFileTimestampVersionFinder())\n-    )\n+    cacheManager = createCacheManager.apply(lifecycle);\n+    final Path tmpDir = temporaryFolder.newFolder().toPath();\n+    final ExtractionNamespaceCacheFactory<URIExtractionNamespace> cachePopulator = new\n+        ExtractionNamespaceCacheFactory<URIExtractionNamespace>()\n     {\n       @Override\n-      public String populateCache(\n-          final String id,\n+      public CacheScheduler.VersionedCache populateCache(\n           final URIExtractionNamespace extractionNamespace,\n+          final CacheScheduler.EntryImpl<URIExtractionNamespace> id,\n           final String lastVersion,\n-          final Map<String, String> cache\n-      ) throws Exception\n+          final CacheScheduler scheduler\n+      ) throws InterruptedException\n       {\n-        // Don't actually read off disk because TravisCI doesn't like that\n-        cache.put(KEY,VALUE);\n         Thread.sleep(2);// To make absolutely sure there is a unique currentTimeMillis\n-        return Long.toString(System.currentTimeMillis());\n+        String version = Long.toString(System.currentTimeMillis());\n+        CacheScheduler.VersionedCache versionedCache = scheduler.createVersionedCache(id, version);\n+        // Don't actually read off disk because TravisCI doesn't like that\n+        versionedCache.getCache().put(KEY, VALUE);\n+        return versionedCache;\n       }\n     };\n-    manager = new OnHeapNamespaceExtractionCacheManager(\n-        lifecycle, new NoopServiceEmitter(),\n+    scheduler = new CacheScheduler(\n+        new NoopServiceEmitter(),\n         ImmutableMap.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>of(\n             URIExtractionNamespace.class,\n-            factory\n-        )\n-    )\n-    {\n-      @Override\n-      protected void updateNamespace(final String id, final String cacheId, final String newVersion)\n-      {\n-        cacheUpdateAlerts.putIfAbsent(id, new Object());\n-        final Object cacheUpdateAlerter = cacheUpdateAlerts.get(id);\n-        synchronized (cacheUpdateAlerter) {\n-          try {\n-            super.updateNamespace(id, cacheId, newVersion);\n-            numRuns.incrementAndGet();\n-          }\n-          finally {\n-            cacheUpdateAlerter.notifyAll();\n-          }\n-        }\n-      }\n-    };\n+            cachePopulator\n+        ),\n+        cacheManager\n+    );\n     tmpFile = Files.createTempFile(tmpDir, \"druidTestURIExtractionNS\", \".dat\").toFile();\n     try (OutputStream ostream = new FileOutputStream(tmpFile)) {\n       try (OutputStreamWriter out = new OutputStreamWriter(ostream)) {\n@@ -146,35 +169,9 @@ public void tearDown()\n     lifecycle.stop();\n   }\n \n-  @Test(expected = IAE.class)\n-  public void testDoubleSubmission()\n-  {\n-    final String namespaceID = \"ns\";\n-    URIExtractionNamespace namespace = new URIExtractionNamespace(\n-        tmpFile.toURI(),\n-        null, null,\n-        new URIExtractionNamespace.ObjectMapperFlatDataParser(\n-            URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n-        ),\n-        new Period(0),\n-        null\n-    );\n-    final ListenableFuture<?> future = manager.schedule(namespaceID, namespace);\n-    Assert.assertFalse(future.isDone());\n-    Assert.assertFalse(future.isCancelled());\n-    try {\n-      manager.schedule(namespaceID, namespace).cancel(true);\n-    }\n-    finally {\n-      future.cancel(true);\n-    }\n-  }\n-\n-\n-  @Test(timeout = 60_000)\n+  @Test(timeout = 10_000)\n   public void testSimpleSubmission() throws ExecutionException, InterruptedException\n   {\n-    final String namespaceID = \"ns\";\n     URIExtractionNamespace namespace = new URIExtractionNamespace(\n         tmpFile.toURI(),\n         null, null,\n@@ -184,70 +181,48 @@ public void testSimpleSubmission() throws ExecutionException, InterruptedExcepti\n         new Period(0),\n         null\n     );\n-    NamespaceExtractionCacheManagersTest.waitFor(manager.schedule(namespaceID, namespace));\n+    CacheScheduler.Entry entry = scheduler.schedule(namespace);\n+    waitFor(entry);\n+    Map<String, String> cache = entry.getCache();\n+    Assert.assertNull(cache.put(\"key\", \"val\"));\n+    Assert.assertEquals(\"val\", cache.get(\"key\"));\n   }\n \n-  @Test(timeout = 60_000)\n-  public void testRepeatSubmission() throws ExecutionException, InterruptedException\n+  @Test(timeout = 10_000)\n+  public void testPeriodicUpdatesScheduled() throws ExecutionException, InterruptedException\n   {\n     final int repeatCount = 5;\n     final long delay = 5;\n-    final long totalRunCount;\n-    final long start;\n-    final String namespaceID = \"ns\";\n     try {\n-      final URIExtractionNamespace namespace = new URIExtractionNamespace(\n-          tmpFile.toURI(),\n-          null, null,\n-          new URIExtractionNamespace.ObjectMapperFlatDataParser(\n-              URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n-          ),\n-          new Period(delay),\n-          null\n-      );\n-      cacheUpdateAlerts.putIfAbsent(namespaceID, new Object());\n-      start = System.currentTimeMillis();\n-      ListenableFuture<?> future = manager.schedule(namespaceID, namespace);\n-\n-      Assert.assertFalse(future.isDone());\n-      Assert.assertFalse(future.isCancelled());\n-\n-      final long preRunCount;\n-      final Object cacheUpdateAlerter = cacheUpdateAlerts.get(namespaceID);\n-      synchronized (cacheUpdateAlerter) {\n-        preRunCount = numRuns.get();\n-      }\n-      for (; ; ) {\n-        synchronized (cacheUpdateAlerter) {\n-          if (numRuns.get() - preRunCount >= repeatCount) {\n-            break;\n-          } else {\n-            cacheUpdateAlerter.wait();\n-          }\n-        }\n+      final URIExtractionNamespace namespace = getUriExtractionNamespace(delay);\n+      final long start = System.currentTimeMillis();\n+      try (CacheScheduler.Entry entry = scheduler.schedule(namespace)) {\n+\n+        Assert.assertFalse(entry.getUpdaterFuture().isDone());\n+        Assert.assertFalse(entry.getUpdaterFuture().isCancelled());\n+\n+        entry.awaitTotalUpdates(repeatCount);\n+\n+        long minEnd = start + ((repeatCount - 1) * delay);\n+        long end = System.currentTimeMillis();\n+        Assert.assertTrue(\n+            String.format(\n+                \"Didn't wait long enough between runs. Expected more than %d was %d\",\n+                minEnd - start,\n+                end - start\n+            ), minEnd <= end\n+        );\n       }\n-\n-      long minEnd = start + ((repeatCount - 1) * delay);\n-      long end = System.currentTimeMillis();\n-      Assert.assertTrue(\n-          String.format(\n-              \"Didn't wait long enough between runs. Expected more than %d was %d\",\n-              minEnd - start,\n-              end - start\n-          ), minEnd <= end\n-      );\n     }\n     finally {\n       lifecycle.stop();\n+      cacheManager.waitForServiceToEnd(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n     }\n-\n-    totalRunCount = numRuns.get();\n-    Thread.sleep(delay * 10);\n-    Assert.assertEquals(totalRunCount, numRuns.get(), 1);\n+    checkNoMoreRunning();\n   }\n \n \n-  @Test(timeout = 600_000) // This is very fast when run locally. Speed on Travis completely depends on noisy neighbors.\n+  @Test(timeout = 10_000) // This is very fast when run locally. Speed on Travis completely depends on noisy neighbors.\n   public void testConcurrentAddDelete() throws ExecutionException, InterruptedException, TimeoutException\n   {\n     final int threads = 10;\n@@ -261,7 +236,6 @@ public void testConcurrentAddDelete() throws ExecutionException, InterruptedExce\n     final CountDownLatch latch = new CountDownLatch(threads);\n     Collection<ListenableFuture<?>> futures = new ArrayList<>();\n     for (int i = 0; i < threads; ++i) {\n-      final int ii = i;\n       futures.add(\n           executorService.submit(\n               new Runnable()\n@@ -275,7 +249,12 @@ public void run()\n                       throw new RuntimeException(new TimeoutException(\"Took too long to wait for more tasks\"));\n                     }\n                     for (int j = 0; j < deletesPerThread; ++j) {\n-                      testDelete(String.format(\"ns-%d-%d\", ii, j));\n+                      try {\n+                        testDelete();\n+                      }\n+                      catch (Exception e) {\n+                        throw Throwables.propagate(e);\n+                      }\n                     }\n                   }\n                   catch (InterruptedException e) {\n@@ -310,74 +289,32 @@ public void run()\n       }\n     }\n     finally {\n-      executorService.shutdownNow();\n+      executorService.shutdown();\n+      executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n     }\n     checkNoMoreRunning();\n   }\n \n-  @Test(timeout = 60_000L)\n-  public void testSimpleDelete() throws InterruptedException\n+  @Test(timeout = 10_000L)\n+  public void testSimpleDelete() throws InterruptedException, TimeoutException, ExecutionException\n   {\n-    testDelete(\"someNamespace\");\n+    testDelete();\n   }\n \n-  public void testDelete(final String ns)\n-      throws InterruptedException\n+  public void testDelete()\n+      throws InterruptedException, TimeoutException, ExecutionException\n   {\n-    cacheUpdateAlerts.putIfAbsent(ns, new Object());\n-    final Object cacheUpdateAlerter = cacheUpdateAlerts.get(ns);\n-\n     final long period = 1_000L;// Give it some time between attempts to update\n-    final URIExtractionNamespace namespace = new URIExtractionNamespace(\n-        tmpFile.toURI(),\n-        null, null,\n-        new URIExtractionNamespace.ObjectMapperFlatDataParser(\n-            URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n-        ),\n-        new Period(period),\n-        null\n-    );\n-    Assert.assertTrue(manager.scheduleAndWait(ns, namespace, 10_000));\n-    final ListenableFuture<?> future = manager.implData.get(ns).future;\n+    final URIExtractionNamespace namespace = getUriExtractionNamespace(period);\n+    CacheScheduler.Entry entry = scheduler.scheduleAndWait(namespace, 10_000);\n+    Assert.assertNotNull(entry);\n+    final Future<?> future = entry.getUpdaterFuture();\n     Assert.assertFalse(future.isCancelled());\n     Assert.assertFalse(future.isDone());\n+    entry.awaitTotalUpdates(1);\n \n-    long start = 0L;\n-\n-    final long timeout = 45_000L;\n-    do {\n-      synchronized (cacheUpdateAlerter) {\n-        if (!manager.implData.containsKey(ns)) {\n-          cacheUpdateAlerter.wait(10_000);\n-        }\n-      }\n-      if (future.isDone()) {\n-        try {\n-          // Bubble up the exception\n-          Assert.assertNull(future.get());\n-          Assert.fail(\"Task finished\");\n-        }\n-        catch (ExecutionException e) {\n-          throw Throwables.propagate(e);\n-        }\n-      }\n-      if (!manager.implData.containsKey(ns) && System.currentTimeMillis() - start > timeout) {\n-        throw new RuntimeException(\n-            new TimeoutException(\n-                String.format(\n-                    \"Namespace took too long to appear in cache for %s\",\n-                    namespace\n-                )\n-            )\n-        );\n-      }\n-    } while (!manager.implData.containsKey(ns) || !manager.implData.get(ns).enabled.get());\n-\n-    Assert.assertEquals(VALUE, manager.getCacheMap(ns).get(KEY));\n-\n-    Assert.assertTrue(manager.implData.containsKey(ns));\n-\n-    Assert.assertTrue(manager.delete(ns));\n+    Assert.assertEquals(VALUE, entry.getCache().get(KEY));\n+    entry.close();\n \n     try {\n       Assert.assertNull(future.get());\n@@ -391,105 +328,104 @@ public void testDelete(final String ns)\n       }\n     }\n \n-    Assert.assertFalse(manager.implData.containsKey(ns));\n     Assert.assertTrue(future.isCancelled());\n     Assert.assertTrue(future.isDone());\n   }\n \n-  @Test(timeout = 60_000)\n+  private URIExtractionNamespace getUriExtractionNamespace(long period)\n+  {\n+    return new URIExtractionNamespace(\n+        tmpFile.toURI(),\n+        null, null,\n+        new URIExtractionNamespace.ObjectMapperFlatDataParser(\n+            URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n+        ),\n+        new Period(period),\n+        null\n+    );\n+  }\n+\n+  @Test(timeout = 10_000)\n   public void testShutdown()\n       throws NoSuchFieldException, IllegalAccessException, InterruptedException, ExecutionException\n   {\n     final long period = 5L;\n-    final ListenableFuture future;\n-    long prior = 0;\n-    final String namespaceID = \"ns\";\n     try {\n \n-      final URIExtractionNamespace namespace = new URIExtractionNamespace(\n-          tmpFile.toURI(),\n-          null, null,\n-          new URIExtractionNamespace.ObjectMapperFlatDataParser(\n-              URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n-          ),\n-          new Period(period),\n-          null\n-      );\n-      cacheUpdateAlerts.putIfAbsent(namespaceID, new Object());\n-\n-      future = manager.schedule(namespaceID, namespace);\n+      final URIExtractionNamespace namespace = getUriExtractionNamespace(period);\n \n-      final Object cacheUpdateAlerter = cacheUpdateAlerts.get(namespaceID);\n-      synchronized (cacheUpdateAlerter) {\n-        cacheUpdateAlerter.wait();\n-      }\n+      try (CacheScheduler.Entry entry = scheduler.schedule(namespace)) {\n+        final Future<?> future = entry.getUpdaterFuture();\n+        entry.awaitNextUpdates(1);\n \n-      Assert.assertFalse(future.isCancelled());\n-      Assert.assertFalse(future.isDone());\n+        Assert.assertFalse(future.isCancelled());\n+        Assert.assertFalse(future.isDone());\n \n-      synchronized (cacheUpdateAlerter) {\n-        prior = numRuns.get();\n-        cacheUpdateAlerter.wait();\n+        final long prior = scheduler.updatesStarted();\n+        entry.awaitNextUpdates(1);\n+        Assert.assertTrue(scheduler.updatesStarted() > prior);\n       }\n-      Assert.assertTrue(numRuns.get() > prior);\n     }\n     finally {\n       lifecycle.stop();\n     }\n-    while (!manager.waitForServiceToEnd(1_000, TimeUnit.MILLISECONDS)) {\n+    while (!cacheManager.waitForServiceToEnd(1_000, TimeUnit.MILLISECONDS)) {\n     }\n \n     checkNoMoreRunning();\n \n-    Field execField = NamespaceExtractionCacheManager.class.getDeclaredField(\"listeningScheduledExecutorService\");\n-    execField.setAccessible(true);\n-    Assert.assertTrue(((ListeningScheduledExecutorService) execField.get(manager)).isShutdown());\n-    Assert.assertTrue(((ListeningScheduledExecutorService) execField.get(manager)).isTerminated());\n+    Assert.assertTrue(cacheManager.scheduledExecutorService().isShutdown());\n+    Assert.assertTrue(cacheManager.scheduledExecutorService().isTerminated());\n   }\n \n-  @Test(timeout = 60_000)\n-  public void testRunCount()\n-      throws InterruptedException, ExecutionException\n+  @Test(timeout = 10_000)\n+  public void testRunCount() throws InterruptedException, ExecutionException\n   {\n-    final long numWaits = 5;\n-    final ListenableFuture<?> future;\n-    final String namespaceID = \"ns\";\n+    final int numWaits = 5;\n     try {\n-      final URIExtractionNamespace namespace = new URIExtractionNamespace(\n-          tmpFile.toURI(),\n-          null, null,\n-          new URIExtractionNamespace.ObjectMapperFlatDataParser(\n-              URIExtractionNamespaceTest.registerTypes(new ObjectMapper())\n-          ),\n-          new Period(5L),\n-          null\n-      );\n-\n-      cacheUpdateAlerts.putIfAbsent(namespaceID, new Object());\n-      future = manager.schedule(namespaceID, namespace);\n-      Assert.assertFalse(future.isDone());\n-\n-      final Object cacheUpdateAlerter = cacheUpdateAlerts.get(namespaceID);\n-      for (int i = 0; i < numWaits; ++i) {\n-        synchronized (cacheUpdateAlerter) {\n-          cacheUpdateAlerter.wait();\n-        }\n+      final URIExtractionNamespace namespace = getUriExtractionNamespace((long) 5);\n+      try (CacheScheduler.Entry entry = scheduler.schedule(namespace)) {\n+        final Future<?> future = entry.getUpdaterFuture();\n+        entry.awaitNextUpdates(numWaits);\n+        Assert.assertFalse(future.isDone());\n       }\n-      Assert.assertFalse(future.isDone());\n     }\n     finally {\n       lifecycle.stop();\n     }\n-    while (!manager.waitForServiceToEnd(1_000, TimeUnit.MILLISECONDS)) {\n+    while (!cacheManager.waitForServiceToEnd(1_000, TimeUnit.MILLISECONDS)) {\n     }\n-    Assert.assertTrue(numRuns.get() >= numWaits);\n+    Assert.assertTrue(scheduler.updatesStarted() >= numWaits);\n     checkNoMoreRunning();\n   }\n \n+  /**\n+   * Tests that even if entry.close() wasn't called, the scheduled task is cancelled when the entry becomes\n+   * unreachable.\n+   */\n+  @Test(timeout = 60_000)\n+  public void testEntryCloseForgotten() throws InterruptedException\n+  {\n+    scheduleDanglingEntry();\n+    Assert.assertEquals(1, scheduler.getActiveEntries());\n+    while (scheduler.getActiveEntries() > 0) {\n+      System.gc();\n+      Thread.sleep(1000);\n+    }\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n+  }\n+\n+  private void scheduleDanglingEntry() throws InterruptedException\n+  {\n+    CacheScheduler.Entry entry = scheduler.schedule(getUriExtractionNamespace(5));\n+    entry.awaitTotalUpdates(1);\n+  }\n+\n   private void checkNoMoreRunning() throws InterruptedException\n   {\n-    final long pre = numRuns.get();\n+    Assert.assertEquals(0, scheduler.getActiveEntries());\n+    final long pre = scheduler.updatesStarted();\n     Thread.sleep(100L);\n-    Assert.assertEquals(pre, numRuns.get(), 1); // since we don't synchronize here we might have an extra increment\n+    Assert.assertEquals(pre, scheduler.updatesStarted());\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagerExecutorsTest.java",
                "sha": "f3904d9570bc48013f856f59a57d627e392616a5",
                "status": "modified"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagersTest.java",
                "changes": 230,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagersTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 154,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagersTest.java",
                "patch": "@@ -19,196 +19,118 @@\n \n package io.druid.server.lookup.namespace.cache;\n \n+import com.google.common.base.Function;\n import com.google.common.base.Throwables;\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.ImmutableMap;\n-import com.google.common.collect.ImmutableSet;\n-import com.google.common.collect.Lists;\n-\n-import io.druid.data.SearchableVersionedDataFinder;\n-import io.druid.jackson.DefaultObjectMapper;\n+import com.google.common.util.concurrent.Futures;\n+import com.google.common.util.concurrent.ListenableFuture;\n+import com.google.common.util.concurrent.ListeningExecutorService;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import io.druid.concurrent.Execs;\n import io.druid.java.util.common.lifecycle.Lifecycle;\n-import io.druid.java.util.common.logger.Logger;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n-import io.druid.query.lookup.namespace.URIExtractionNamespace;\n-import io.druid.segment.loading.LocalFileTimestampVersionFinder;\n-import io.druid.server.lookup.namespace.URIExtractionNamespaceCacheFactory;\n-import io.druid.server.metrics.NoopServiceEmitter;\n-import org.joda.time.Period;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import java.net.URI;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collection;\n-import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n-import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.Future;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n-/**\n- *\n- */\n @RunWith(Parameterized.class)\n public class NamespaceExtractionCacheManagersTest\n {\n-  private static final Logger log = new Logger(NamespaceExtractionCacheManagersTest.class);\n-  private static final Lifecycle lifecycle = new Lifecycle();\n-  private static final Map<String, SearchableVersionedDataFinder> PULLERS = ImmutableMap.<String, SearchableVersionedDataFinder>of(\n-      \"file\",\n-      new LocalFileTimestampVersionFinder()\n-  );\n-  private static final Map<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>> CACHE_FACTORIES = ImmutableMap.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>of(\n-      URIExtractionNamespace.class, new URIExtractionNamespaceCacheFactory(PULLERS)\n-  );\n-\n-  @Parameterized.Parameters(name = \"{0}\")\n-  public static Collection<Object[]> getParameters()\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> data()\n   {\n-    ArrayList<Object[]> params = new ArrayList<>();\n-    params.add(\n-        new Object[]{\n-            new OffHeapNamespaceExtractionCacheManager(\n-                lifecycle,\n-                new NoopServiceEmitter(),\n-                CACHE_FACTORIES\n-            )\n-        }\n-    );\n-    params.add(\n-        new Object[]{\n-            new OnHeapNamespaceExtractionCacheManager(\n-                lifecycle,\n-                new NoopServiceEmitter(),\n-                CACHE_FACTORIES\n-            )\n-        }\n-    );\n-    return params;\n+    return Arrays.asList(new Object[][]{\n+        {NamespaceExtractionCacheManagerExecutorsTest.CREATE_ON_HEAP_CACHE_MANAGER},\n+        {NamespaceExtractionCacheManagerExecutorsTest.CREATE_OFF_HEAP_CACHE_MANAGER}\n+    });\n   }\n \n-  private final NamespaceExtractionCacheManager extractionCacheManager;\n+  private final Function<Lifecycle, NamespaceExtractionCacheManager> createCacheManager;\n+  private Lifecycle lifecycle;\n+  private NamespaceExtractionCacheManager manager;\n \n-  public NamespaceExtractionCacheManagersTest(\n-      NamespaceExtractionCacheManager extractionCacheManager\n-  )\n+  public NamespaceExtractionCacheManagersTest(Function<Lifecycle, NamespaceExtractionCacheManager> createCacheManager)\n   {\n-    this.extractionCacheManager = extractionCacheManager;\n-  }\n \n-  private static final List<String> nsList = ImmutableList.<String>of(\"testNs\", \"test.ns\", \"//tes-tn!s\");\n+    this.createCacheManager = createCacheManager;\n+  }\n \n   @Before\n-  public void setup()\n+  public void setUp() throws Exception\n   {\n-    // prepopulate caches\n-    for (String ns : nsList) {\n-      final ConcurrentMap<String, String> map = extractionCacheManager.getCacheMap(ns);\n-      map.put(\"oldNameSeed1\", \"oldNameSeed2\");\n-    }\n+    lifecycle = new Lifecycle();\n+    lifecycle.start();\n+    manager = createCacheManager.apply(lifecycle);\n   }\n \n-  @Test\n-  public void testSimpleCacheCreate()\n+  @Test(timeout = 30000L)\n+  public void testRacyCreation() throws Exception\n   {\n-    for (String ns : nsList) {\n-      ConcurrentMap<String, String> map = extractionCacheManager.getCacheMap(ns);\n-      map.put(\"key\", \"val\");\n-      Assert.assertEquals(\"val\", map.get(\"key\"));\n-      Assert.assertEquals(\"val\", extractionCacheManager.getCacheMap(ns).get(\"key\"));\n+    final int concurrentThreads = 10;\n+    final ListeningExecutorService service = MoreExecutors.listeningDecorator(Execs.multiThreaded(\n+        concurrentThreads,\n+        \"offheaptest-%s\"\n+    ));\n+    final List<ListenableFuture<?>> futures = new ArrayList<>();\n+    final CountDownLatch thunder = new CountDownLatch(1);\n+    try {\n+      for (int i = 0; i < concurrentThreads; ++i) {\n+        futures.add(service.submit(\n+            new Runnable()\n+            {\n+              @Override\n+              public void run()\n+              {\n+                try {\n+                  thunder.await();\n+                }\n+                catch (InterruptedException e) {\n+                  throw Throwables.propagate(e);\n+                }\n+                for (int i = 0; i < 1000; ++i) {\n+                  CacheHandler cacheHandler = manager.createCache();\n+                  cacheHandler.close();\n+                }\n+              }\n+            }\n+        ));\n+      }\n+      thunder.countDown();\n+      Futures.allAsList(futures).get();\n     }\n-  }\n-\n-  @Test\n-  public void testSimpleCacheSwap()\n-  {\n-    for (String ns : nsList) {\n-      ConcurrentMap<String, String> map = extractionCacheManager.getCacheMap(ns + \"old_cache\");\n-      map.put(\"key\", \"val\");\n-      extractionCacheManager.swapAndClearCache(ns, ns + \"old_cache\");\n-      Assert.assertEquals(\"val\", map.get(\"key\"));\n-      Assert.assertEquals(\"val\", extractionCacheManager.getCacheMap(ns).get(\"key\"));\n-\n-      ConcurrentMap<String, String> map2 = extractionCacheManager.getCacheMap(ns + \"cache\");\n-      map2.put(\"key\", \"val2\");\n-      Assert.assertTrue(extractionCacheManager.swapAndClearCache(ns, ns + \"cache\"));\n-      Assert.assertEquals(\"val2\", map2.get(\"key\"));\n-      Assert.assertEquals(\"val2\", extractionCacheManager.getCacheMap(ns).get(\"key\"));\n+    finally {\n+      service.shutdown();\n+      service.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n     }\n-  }\n \n-  @Test(expected = IllegalArgumentException.class)\n-  public void testMissingCacheThrowsIAE()\n-  {\n-    for (String ns : nsList) {\n-      ConcurrentMap<String, String> map = extractionCacheManager.getCacheMap(ns);\n-      map.put(\"key\", \"val\");\n-      Assert.assertEquals(\"val\", map.get(\"key\"));\n-      Assert.assertEquals(\"val\", extractionCacheManager.getCacheMap(ns).get(\"key\"));\n-      Assert.assertFalse(extractionCacheManager.swapAndClearCache(ns, \"I don't exist\"));\n-    }\n+    Assert.assertEquals(0, manager.cacheCount());\n   }\n \n-  @Test\n-  public void testCacheList()\n+  /**\n+   * Tests that even if CacheHandler.close() wasn't called, the cache is cleaned up when it becomes unreachable.\n+   */\n+  @Test(timeout = 60_000)\n+  public void testCacheCloseForgotten() throws InterruptedException\n   {\n-    List<String> nsList = new ArrayList<String>(NamespaceExtractionCacheManagersTest.nsList);\n-    for (String ns : nsList) {\n-      extractionCacheManager.implData.put(ns, new NamespaceExtractionCacheManager.NamespaceImplData(null, null, null));\n+    Assert.assertEquals(0, manager.cacheCount());\n+    createDanglingCache();\n+    Assert.assertEquals(1, manager.cacheCount());\n+    while (manager.cacheCount() > 0) {\n+      System.gc();\n+      Thread.sleep(1000);\n     }\n-    List<String> retvalList = Lists.newArrayList(extractionCacheManager.getKnownIDs());\n-    Collections.sort(nsList);\n-    Collections.sort(retvalList);\n-    Assert.assertArrayEquals(nsList.toArray(), retvalList.toArray());\n+    Assert.assertEquals(0, manager.cacheCount());\n   }\n \n-  @Test\n-  public void testNoDeleteNonexistant()\n+  private void createDanglingCache()\n   {\n-    Assert.assertFalse(extractionCacheManager.delete(\"I don't exist\"));\n-  }\n-\n-  @Test\n-  public void testDeleteOnScheduleFail() throws Exception\n-  {\n-    final String id = \"SOME_ID\";\n-    Assert.assertFalse(extractionCacheManager.scheduleAndWait(\n-        id,\n-        new URIExtractionNamespace(\n-            new URI(\"file://tmp/I_DONT_REALLY_EXIST\" +\n-                    UUID.randomUUID().toString()),\n-            null,\n-            null,\n-            new URIExtractionNamespace.JSONFlatDataParser(\n-                new DefaultObjectMapper(),\n-                \"key\",\n-                \"val\"\n-            ),\n-            Period.millis(10000),\n-            null\n-        ),\n-        500\n-    ));\n-    Assert.assertEquals(ImmutableSet.copyOf(nsList), extractionCacheManager.getKnownIDs());\n-  }\n-\n-  public static void waitFor(Future<?> future) throws InterruptedException\n-  {\n-    while (!future.isDone()) {\n-      try {\n-        future.get();\n-      }\n-      catch (ExecutionException e) {\n-        log.error(e.getCause(), \"Error waiting\");\n-        throw Throwables.propagate(e.getCause());\n-      }\n-    }\n+    manager.createCache();\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/NamespaceExtractionCacheManagersTest.java",
                "sha": "472bb437b248590a124da78d58bb940d05c97cdf",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManagerTest.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManagerTest.java?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 80,
                "filename": "extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManagerTest.java",
                "patch": "@@ -19,38 +19,21 @@\n \n package io.druid.server.lookup.namespace.cache;\n \n-import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.ImmutableMap;\n-import com.google.common.util.concurrent.Futures;\n-import com.google.common.util.concurrent.ListenableFuture;\n-import com.google.common.util.concurrent.ListeningExecutorService;\n-import com.google.common.util.concurrent.MoreExecutors;\n import com.google.inject.Binder;\n import com.google.inject.Injector;\n import com.google.inject.Key;\n import com.google.inject.Module;\n-import com.metamx.emitter.service.ServiceEmitter;\n-import io.druid.concurrent.Execs;\n import io.druid.guice.GuiceInjectors;\n import io.druid.guice.JsonConfigProvider;\n import io.druid.guice.annotations.Self;\n import io.druid.initialization.Initialization;\n-import io.druid.java.util.common.lifecycle.Lifecycle;\n-import io.druid.query.lookup.namespace.ExtractionNamespace;\n-import io.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory;\n import io.druid.server.DruidNode;\n import io.druid.server.lookup.namespace.NamespaceExtractionModule;\n-import io.druid.server.metrics.NoopServiceEmitter;\n import org.junit.Assert;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n-import java.util.List;\n import java.util.Properties;\n-import java.util.Random;\n-import java.util.UUID;\n-import java.util.concurrent.CountDownLatch;\n \n public class OffHeapNamespaceExtractionCacheManagerTest\n {\n@@ -78,67 +61,4 @@ public void configure(Binder binder)\n     final NamespaceExtractionCacheManager manager = injector.getInstance(NamespaceExtractionCacheManager.class);\n     Assert.assertEquals(OffHeapNamespaceExtractionCacheManager.class, manager.getClass());\n   }\n-\n-  @Test(timeout = 30000L)\n-  public void testRacyCreation() throws Exception\n-  {\n-    final int concurrentThreads = 100;\n-    final Lifecycle lifecycle = new Lifecycle();\n-    final ServiceEmitter emitter = new NoopServiceEmitter();\n-    final OffHeapNamespaceExtractionCacheManager manager = new OffHeapNamespaceExtractionCacheManager(\n-        lifecycle,\n-        emitter,\n-        ImmutableMap.<Class<? extends ExtractionNamespace>, ExtractionNamespaceCacheFactory<?>>of()\n-    );\n-    final ListeningExecutorService service = MoreExecutors.listeningDecorator(Execs.multiThreaded(\n-        concurrentThreads,\n-        \"offheaptest-%s\"\n-    ));\n-    final List<ListenableFuture<?>> futures = new ArrayList<>();\n-    final CountDownLatch thunder = new CountDownLatch(1);\n-    final List<String> namespaceIds = new ArrayList<>();\n-    for (int i = 0; i < 5; ++i) {\n-      final String namespace = \"namespace-\" + UUID.randomUUID().toString();\n-      final String cacheKey = \"initial-cache-\" + namespace;\n-      namespaceIds.add(namespace);\n-      manager.getCacheMap(cacheKey).put(\"foo\", \"bar\");\n-      Assert.assertFalse(manager.swapAndClearCache(namespace, cacheKey));\n-    }\n-    final Random random = new Random(3748218904L);\n-    try {\n-      for (int i = 0; i < concurrentThreads; ++i) {\n-        final int j = i;\n-        final String namespace = namespaceIds.get(random.nextInt(namespaceIds.size()));\n-        futures.add(service.submit(\n-            new Runnable()\n-            {\n-              @Override\n-              public void run()\n-              {\n-                try {\n-                  thunder.await();\n-                }\n-                catch (InterruptedException e) {\n-                  throw Throwables.propagate(e);\n-                }\n-                for (int i = 0; i < 1000; ++i) {\n-                  final String cacheKey = String.format(\"%s-%d-key-%d\", namespace, j, i);\n-                  manager.getCacheMap(cacheKey).put(\"foo\", \"bar\" + Integer.toString(i));\n-                  Assert.assertTrue(manager.swapAndClearCache(namespace, cacheKey));\n-                }\n-              }\n-            }\n-        ));\n-      }\n-      thunder.countDown();\n-      Futures.allAsList(futures).get();\n-    }\n-    finally {\n-      service.shutdownNow();\n-    }\n-\n-    for (final String namespace : namespaceIds) {\n-      Assert.assertEquals(ImmutableMap.of(\"foo\", \"bar999\"), manager.getCacheMap(namespace));\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/extensions-core/lookups-cached-global/src/test/java/io/druid/server/lookup/namespace/cache/OffHeapNamespaceExtractionCacheManagerTest.java",
                "sha": "0af60778d9f0a810abe54d71251be862518922ed",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-druid/blob/76cb06a8d8161d29d985ef048b89e6a82b489058/pom.xml",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=76cb06a8d8161d29d985ef048b89e6a82b489058",
                "deletions": 0,
                "filename": "pom.xml",
                "patch": "@@ -73,6 +73,7 @@\n         <!-- If compiling with different hadoop version also modify default hadoop coordinates in TaskConfig.java -->\n         <hadoop.compile.version>2.3.0</hadoop.compile.version>\n         <hive.version>2.0.0</hive.version>\n+        <powermock.version>1.6.6</powermock.version>\n     </properties>\n \n     <modules>\n@@ -637,6 +638,18 @@\n                 <version>4.11</version>\n                 <scope>test</scope>\n             </dependency>\n+            <dependency>\n+                <groupId>org.powermock</groupId>\n+                <artifactId>powermock-module-junit4</artifactId>\n+                <version>${powermock.version}</version>\n+                <scope>test</scope>\n+            </dependency>\n+            <dependency>\n+                <groupId>org.powermock</groupId>\n+                <artifactId>powermock-api-easymock</artifactId>\n+                <version>${powermock.version}</version>\n+                <scope>test</scope>\n+            </dependency>\n             <dependency>\n                 <groupId>org.slf4j</groupId>\n                 <artifactId>slf4j-simple</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/76cb06a8d8161d29d985ef048b89e6a82b489058/pom.xml",
                "sha": "1ee39ee942f2584366f2cd740c00e7c45b03df32",
                "status": "modified"
            }
        ],
        "message": "Lookup cache refactoring (the main part of #3667) (#3697)\n\n* Lookup cache refactoring (the main part of druid-io/druid#3667)\r\n\r\n* Use PowerMock's static methods in NamespaceLookupExtractorFactoryTest\r\n\r\n* Fix KafkaLookupExtractorFactoryTest\r\n\r\n* Use VisibleForTesting annotation instead of Javadoc comment\r\n\r\n* Create a NamespaceExtractionCacheManager separately for each test in NamespaceExtractionCacheManagersTest\r\n\r\n* Rename CacheScheduler.NoCache.ENTRY_DISPOSED to ENTRY_CLOSED\r\n\r\n* Reduce visibility of NamespaceExtractionCacheManager.cacheCount() and monitor() implementations, and don't run NamespaceExtractionCacheManagerExecutorsTest with off-heap cache (it didn't before)\r\n\r\n* In NamespaceLookupExtractorFactory, use safer idiom to check if CacheState is NoCache or VersionedCache\r\n\r\n* More logging in CacheHandler constructor and close(), VersionedCache.close()\r\n\r\n* PR comments addressed\r\n\r\n* Make CacheScheduler.EntryImpl AutoCloseable, avoid 'dispose' verb in comments, logging and naming in CacheScheduler in favor of 'close'\r\n\r\n* More Javadoc comments to CacheScheduler\r\n\r\n* Fix NPE\r\n\r\n* Remove logging in OnHeapNamespaceExtractionCacheManager.expungeCollectedCaches()\r\n\r\n* Make NamespaceExtractionCacheManagersTest.testRacyCreation() to have similar load to what it be before the refactoring\r\n\r\n* Unwrap NamespaceExtractionCacheManager.scheduledExecutorService from unneeded MoreExecutors.listeningDecorator() and specify that this is ScheduledThreadPoolExecutor, which ensures happens-before between periodic runs of the tasks\r\n\r\n* More comments on MapDbCacheDisposer.disposed\r\n\r\n* Replace concat with Long.toString()\r\n\r\n* Comment on why NamespaceExtractionCacheManager.scheduledExecutorService() returns ScheduledThreadPoolExecutor\r\n\r\n* Place logging statements in VersionedCache.close() and CacheHandler.close() after actual closing logic, because logging may fail\r\n\r\n* Make JDBCExtractionNamespaceCacheFactory and StaticMapExtractionNamespaceCacheFactory to try to close newly created VersionedCache if population has failed, as it is done already in URIExtractionNamespaceCacheFactory\r\n\r\n* Don't close the whole CacheScheduler.Entry, if the cache update task failed\r\n\r\n* Replace AtomicLong updateCounter and firstRunLatch with Phaser-based UpdateCounter in CacheScheduler.EntryImpl",
        "parent": "https://github.com/apache/incubator-druid/commit/0e5bd8b4d421c24e036e4104752f0e40c6d6a66a",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaLookupExtractorFactoryTest.java",
            "NamespaceLookupExtractorFactoryTest.java",
            "CacheSchedulerTest.java",
            "OffHeapNamespaceExtractionCacheManagerTest.java",
            "OnHeapNamespaceExtractionCacheManagerTest.java"
        ]
    },
    "incubator-druid_7781820": {
        "bug_id": "incubator-druid_7781820",
        "commit": "https://github.com/apache/incubator-druid/commit/7781820dead727e84f1b61a0e6c219ec115862c1",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7781820dead727e84f1b61a0e6c219ec115862c1/processing/src/main/java/org/apache/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/ChainedExecutionQueryRunner.java?ref=7781820dead727e84f1b61a0e6c219ec115862c1",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -163,7 +163,7 @@ public ChainedExecutionQueryRunner(\n               throw new QueryInterruptedException(e);\n             }\n             catch (TimeoutException e) {\n-              log.info(\"Query timeout, cancelling pending results for query id [%s]\", query.getId());\n+              log.warn(\"Query timeout, cancelling pending results for query id [%s]\", query.getId());\n               futures.cancel(true);\n               throw new QueryInterruptedException(e);\n             }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7781820dead727e84f1b61a0e6c219ec115862c1/processing/src/main/java/org/apache/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "0990c9825179caecfa2eeca65535d8577c486fe2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7781820dead727e84f1b61a0e6c219ec115862c1/processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java?ref=7781820dead727e84f1b61a0e6c219ec115862c1",
                "deletions": 3,
                "filename": "processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.util.concurrent.CancellationException;\n import java.util.concurrent.TimeoutException;\n \n@@ -57,10 +58,10 @@\n \n   @JsonCreator\n   public QueryInterruptedException(\n-      @JsonProperty(\"error\") String errorCode,\n+      @JsonProperty(\"error\") @Nullable String errorCode,\n       @JsonProperty(\"errorMessage\") String errorMessage,\n-      @JsonProperty(\"errorClass\") String errorClass,\n-      @JsonProperty(\"host\") String host\n+      @JsonProperty(\"errorClass\") @Nullable String errorClass,\n+      @JsonProperty(\"host\") @Nullable String host\n   )\n   {\n     super(errorMessage);\n@@ -88,6 +89,7 @@ public QueryInterruptedException(Throwable cause, String host)\n     this.host = host;\n   }\n \n+  @Nullable\n   @JsonProperty(\"error\")\n   public String getErrorCode()\n   {\n@@ -144,6 +146,7 @@ private static String getErrorCodeFromThrowable(Throwable e)\n     }\n   }\n \n+  @Nullable\n   private static String getErrorClassFromThrowable(Throwable e)\n   {\n     if (e instanceof QueryInterruptedException) {\n@@ -155,6 +158,7 @@ private static String getErrorClassFromThrowable(Throwable e)\n     }\n   }\n \n+  @Nullable\n   private static String getHostFromThrowable(Throwable e)\n   {\n     if (e instanceof QueryInterruptedException) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7781820dead727e84f1b61a0e6c219ec115862c1/processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java",
                "sha": "bc2ffeb4c325c31e070d99e782f21e6d0de4f4d7",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7781820dead727e84f1b61a0e6c219ec115862c1/server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "changes": 90,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/JsonParserIterator.java?ref=7781820dead727e84f1b61a0e6c219ec115862c1",
                "deletions": 35,
                "filename": "server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "patch": "@@ -26,12 +26,15 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.guava.CloseQuietly;\n+import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.query.Query;\n import org.apache.druid.query.QueryInterruptedException;\n import org.apache.druid.query.ResourceLimitExceededException;\n import org.apache.druid.server.coordinator.BytesAccumulatingResponseHandler;\n \n+import javax.annotation.Nullable;\n import javax.servlet.http.HttpServletResponse;\n import java.io.Closeable;\n import java.io.IOException;\n@@ -40,24 +43,30 @@\n import java.util.concurrent.CancellationException;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n \n public class JsonParserIterator<T> implements Iterator<T>, Closeable\n {\n+  private static final Logger LOG = new Logger(JsonParserIterator.class);\n+\n   private JsonParser jp;\n   private ObjectCodec objectCodec;\n   private final JavaType typeRef;\n   private final Future<InputStream> future;\n-  private final Query<T> query;\n   private final String url;\n   private final String host;\n   private final ObjectMapper objectMapper;\n   private final BytesAccumulatingResponseHandler responseHandler;\n+  private final boolean hasTimeout;\n+  private final long timeoutAt;\n+  private final String queryId;\n \n   public JsonParserIterator(\n       JavaType typeRef,\n       Future<InputStream> future,\n       String url,\n-      Query<T> query,\n+      @Nullable Query<T> query,\n       String host,\n       ObjectMapper objectMapper,\n       BytesAccumulatingResponseHandler responseHandler\n@@ -66,11 +75,18 @@ public JsonParserIterator(\n     this.typeRef = typeRef;\n     this.future = future;\n     this.url = url;\n-    this.query = query;\n-    jp = null;\n+    if (query != null) {\n+      this.timeoutAt = query.<Long>getContextValue(DirectDruidClient.QUERY_FAIL_TIME, -1L);\n+      this.queryId = query.getId();\n+    } else {\n+      this.timeoutAt = -1;\n+      this.queryId = null;\n+    }\n+    this.jp = null;\n     this.host = host;\n     this.objectMapper = objectMapper;\n     this.responseHandler = responseHandler;\n+    this.hasTimeout = timeoutAt > -1;\n   }\n \n   @Override\n@@ -114,49 +130,47 @@ private void init()\n   {\n     if (jp == null) {\n       try {\n-        InputStream is = future.get();\n+        long timeLeftMillis = timeoutAt - System.currentTimeMillis();\n+        if (hasTimeout && timeLeftMillis < 1) {\n+          throw new TimeoutException(StringUtils.format(\"url[%s] timed out\", url));\n+        }\n+        InputStream is = hasTimeout\n+                         ? future.get(timeLeftMillis, TimeUnit.MILLISECONDS)\n+                         : future.get();\n         if (responseHandler != null && responseHandler.getStatus() != HttpServletResponse.SC_OK) {\n-          throw new RE(\n-              \"Unexpected response status [%s] description [%s] from request url [%s]\",\n-              responseHandler.getStatus(),\n-              responseHandler.getDescription(),\n-              url\n+          interruptQuery(\n+              new RE(\n+                  \"Unexpected response status [%s] description [%s] from request url[%s]\",\n+                  responseHandler.getStatus(),\n+                  responseHandler.getDescription(),\n+                  url\n+              )\n           );\n         }\n-        if (is == null) {\n-          throw new QueryInterruptedException(\n+        if (is != null) {\n+          jp = objectMapper.getFactory().createParser(is);\n+        } else {\n+          interruptQuery(\n               new ResourceLimitExceededException(\n-                  \"query[%s] url[%s] timed out or max bytes limit reached.\",\n-                  query.getId(),\n+                  \"url[%s] timed out or max bytes limit reached.\",\n                   url\n-              ),\n-              host\n+              )\n           );\n-        } else {\n-          jp = objectMapper.getFactory().createParser(is);\n         }\n         final JsonToken nextToken = jp.nextToken();\n-        if (nextToken == JsonToken.START_OBJECT) {\n-          QueryInterruptedException cause = jp.getCodec().readValue(jp, QueryInterruptedException.class);\n-          throw new QueryInterruptedException(cause, host);\n-        } else if (nextToken != JsonToken.START_ARRAY) {\n-          throw new IAE(\"Next token wasn't a START_ARRAY, was[%s] from url [%s]\", jp.getCurrentToken(), url);\n-        } else {\n+        if (nextToken == JsonToken.START_ARRAY) {\n           jp.nextToken();\n           objectCodec = jp.getCodec();\n+        } else if (nextToken == JsonToken.START_OBJECT) {\n+          interruptQuery(jp.getCodec().readValue(jp, QueryInterruptedException.class));\n+        } else {\n+          interruptQuery(\n+              new IAE(\"Next token wasn't a START_ARRAY, was[%s] from url[%s]\", jp.getCurrentToken(), url)\n+          );\n         }\n       }\n-      catch (IOException | InterruptedException | ExecutionException e) {\n-        throw new RE(\n-            e,\n-            \"Failure getting results for query[%s] url[%s] because of [%s]\",\n-            query == null ? null : query.getId(),\n-            url,\n-            e.getMessage()\n-        );\n-      }\n-      catch (CancellationException e) {\n-        throw new QueryInterruptedException(e, host);\n+      catch (IOException | InterruptedException | ExecutionException | CancellationException | TimeoutException e) {\n+        interruptQuery(e);\n       }\n     }\n   }\n@@ -168,5 +182,11 @@ public void close() throws IOException\n       jp.close();\n     }\n   }\n+\n+  private void interruptQuery(Exception cause)\n+  {\n+    LOG.warn(cause, \"Query [%s] to host [%s] interrupted\", queryId, host);\n+    throw new QueryInterruptedException(cause, host);\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7781820dead727e84f1b61a0e6c219ec115862c1/server/src/main/java/org/apache/druid/client/JsonParserIterator.java",
                "sha": "9478f246764f42b3755c7bdd9bdf56bf32fc63f8",
                "status": "modified"
            },
            {
                "additions": 185,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7781820dead727e84f1b61a0e6c219ec115862c1/server/src/test/java/org/apache/druid/client/DirectDruidClientTest.java",
                "changes": 307,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/DirectDruidClientTest.java?ref=7781820dead727e84f1b61a0e6c219ec115862c1",
                "deletions": 122,
                "filename": "server/src/test/java/org/apache/druid/client/DirectDruidClientTest.java",
                "patch": "@@ -54,22 +54,77 @@\n import org.jboss.netty.handler.timeout.ReadTimeoutException;\n import org.joda.time.Duration;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Test;\n \n import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n import java.io.InputStream;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n import java.net.URL;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n \n public class DirectDruidClientTest\n {\n+  private final String hostName = \"localhost:8080\";\n+\n+  private final DataSegment dataSegment = new DataSegment(\n+      \"test\",\n+      Intervals.of(\"2013-01-01/2013-01-02\"),\n+      DateTimes.of(\"2013-01-01\").toString(),\n+      new HashMap<>(),\n+      new ArrayList<>(),\n+      new ArrayList<>(),\n+      NoneShardSpec.instance(),\n+      0,\n+      0L\n+  );\n+  private ServerSelector serverSelector;\n+\n+  private HttpClient httpClient;\n+  private DirectDruidClient client;\n+  private QueryableDruidServer queryableDruidServer;\n+\n+  @Before\n+  public void setup()\n+  {\n+    httpClient = EasyMock.createMock(HttpClient.class);\n+    serverSelector = new ServerSelector(\n+      dataSegment,\n+      new HighestPriorityTierSelectorStrategy(new ConnectionCountServerSelectorStrategy())\n+    );\n+    client = new DirectDruidClient(\n+        new ReflectionQueryToolChestWarehouse(),\n+        QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n+        new DefaultObjectMapper(),\n+        httpClient,\n+        \"http\",\n+        hostName,\n+        new NoopServiceEmitter()\n+    );\n+    queryableDruidServer = new QueryableDruidServer(\n+        new DruidServer(\n+            \"test1\",\n+            \"localhost\",\n+            null,\n+            0,\n+            ServerType.HISTORICAL,\n+            DruidServer.DEFAULT_TIER,\n+            0\n+        ),\n+        client\n+    );\n+    serverSelector.addServerAndUpdateSegment(queryableDruidServer, serverSelector.getSegment());\n+  }\n+\n+\n   @Test\n   public void testRun() throws Exception\n   {\n-    HttpClient httpClient = EasyMock.createMock(HttpClient.class);\n-    final URL url = new URL(\"http://foo/druid/v2/\");\n+    final URL url = new URL(StringUtils.format(\"http://%s/druid/v2/\", hostName));\n \n     SettableFuture<InputStream> futureResult = SettableFuture.create();\n     Capture<Request> capturedRequest = EasyMock.newCapture();\n@@ -106,30 +161,6 @@ public void testRun() throws Exception\n \n     EasyMock.replay(httpClient);\n \n-    final ServerSelector serverSelector = new ServerSelector(\n-        new DataSegment(\n-            \"test\",\n-            Intervals.of(\"2013-01-01/2013-01-02\"),\n-            DateTimes.of(\"2013-01-01\").toString(),\n-            new HashMap<>(),\n-            new ArrayList<>(),\n-            new ArrayList<>(),\n-            NoneShardSpec.instance(),\n-            0,\n-            0L\n-        ),\n-        new HighestPriorityTierSelectorStrategy(new ConnectionCountServerSelectorStrategy())\n-    );\n-\n-    DirectDruidClient client1 = new DirectDruidClient(\n-        new ReflectionQueryToolChestWarehouse(),\n-        QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n-        new DefaultObjectMapper(),\n-        httpClient,\n-        \"http\",\n-        \"foo\",\n-        new NoopServiceEmitter()\n-    );\n     DirectDruidClient client2 = new DirectDruidClient(\n         new ReflectionQueryToolChestWarehouse(),\n         QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n@@ -140,37 +171,40 @@ public void testRun() throws Exception\n         new NoopServiceEmitter()\n     );\n \n-    QueryableDruidServer queryableDruidServer1 = new QueryableDruidServer(\n-        new DruidServer(\"test1\", \"localhost\", null, 0, ServerType.REALTIME, DruidServer.DEFAULT_TIER, 0),\n-        client1\n-    );\n-    serverSelector.addServerAndUpdateSegment(queryableDruidServer1, serverSelector.getSegment());\n     QueryableDruidServer queryableDruidServer2 = new QueryableDruidServer(\n-        new DruidServer(\"test1\", \"localhost\", null, 0, ServerType.HISTORICAL, DruidServer.DEFAULT_TIER, 0),\n+        new DruidServer(\n+            \"test1\",\n+            \"localhost\",\n+            null,\n+            0,\n+            ServerType.HISTORICAL,\n+            DruidServer.DEFAULT_TIER,\n+            0\n+        ),\n         client2\n     );\n     serverSelector.addServerAndUpdateSegment(queryableDruidServer2, serverSelector.getSegment());\n \n     TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder().dataSource(\"test\").build();\n     query = query.withOverriddenContext(ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, Long.MAX_VALUE));\n-    Sequence s1 = client1.run(QueryPlus.wrap(query));\n+    Sequence s1 = client.run(QueryPlus.wrap(query));\n     Assert.assertTrue(capturedRequest.hasCaptured());\n     Assert.assertEquals(url, capturedRequest.getValue().getUrl());\n     Assert.assertEquals(HttpMethod.POST, capturedRequest.getValue().getMethod());\n-    Assert.assertEquals(1, client1.getNumOpenConnections());\n+    Assert.assertEquals(1, client.getNumOpenConnections());\n \n     // simulate read timeout\n-    client1.run(QueryPlus.wrap(query));\n-    Assert.assertEquals(2, client1.getNumOpenConnections());\n+    client.run(QueryPlus.wrap(query));\n+    Assert.assertEquals(2, client.getNumOpenConnections());\n     futureException.setException(new ReadTimeoutException());\n-    Assert.assertEquals(1, client1.getNumOpenConnections());\n+    Assert.assertEquals(1, client.getNumOpenConnections());\n \n     // subsequent connections should work\n-    client1.run(QueryPlus.wrap(query));\n-    client1.run(QueryPlus.wrap(query));\n-    client1.run(QueryPlus.wrap(query));\n+    client.run(QueryPlus.wrap(query));\n+    client.run(QueryPlus.wrap(query));\n+    client.run(QueryPlus.wrap(query));\n \n-    Assert.assertTrue(client1.getNumOpenConnections() == 4);\n+    Assert.assertTrue(client.getNumOpenConnections() == 4);\n \n     // produce result for first connection\n     futureResult.set(\n@@ -181,23 +215,21 @@ public void testRun() throws Exception\n     List<Result> results = s1.toList();\n     Assert.assertEquals(1, results.size());\n     Assert.assertEquals(DateTimes.of(\"2014-01-01T01:02:03Z\"), results.get(0).getTimestamp());\n-    Assert.assertEquals(3, client1.getNumOpenConnections());\n+    Assert.assertEquals(3, client.getNumOpenConnections());\n \n     client2.run(QueryPlus.wrap(query));\n     client2.run(QueryPlus.wrap(query));\n \n-    Assert.assertTrue(client2.getNumOpenConnections() == 2);\n+    Assert.assertEquals(2, client2.getNumOpenConnections());\n \n-    Assert.assertTrue(serverSelector.pick() == queryableDruidServer2);\n+    Assert.assertEquals(serverSelector.pick(), queryableDruidServer2);\n \n     EasyMock.verify(httpClient);\n   }\n \n   @Test\n   public void testCancel()\n   {\n-    HttpClient httpClient = EasyMock.createStrictMock(HttpClient.class);\n-\n     Capture<Request> capturedRequest = EasyMock.newCapture();\n     ListenableFuture<Object> cancelledFuture = Futures.immediateCancelledFuture();\n     SettableFuture<Object> cancellationFuture = SettableFuture.create();\n@@ -224,43 +256,13 @@ public void testCancel()\n \n     EasyMock.replay(httpClient);\n \n-    final ServerSelector serverSelector = new ServerSelector(\n-        new DataSegment(\n-            \"test\",\n-            Intervals.of(\"2013-01-01/2013-01-02\"),\n-            DateTimes.of(\"2013-01-01\").toString(),\n-            new HashMap<>(),\n-            new ArrayList<>(),\n-            new ArrayList<>(),\n-            NoneShardSpec.instance(),\n-            0,\n-            0L\n-        ),\n-        new HighestPriorityTierSelectorStrategy(new ConnectionCountServerSelectorStrategy())\n-    );\n-\n-    DirectDruidClient client1 = new DirectDruidClient(\n-        new ReflectionQueryToolChestWarehouse(),\n-        QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n-        new DefaultObjectMapper(),\n-        httpClient,\n-        \"http\",\n-        \"foo\",\n-        new NoopServiceEmitter()\n-    );\n-\n-    QueryableDruidServer queryableDruidServer1 = new QueryableDruidServer(\n-        new DruidServer(\"test1\", \"localhost\", null, 0, ServerType.HISTORICAL, DruidServer.DEFAULT_TIER, 0),\n-        client1\n-    );\n-    serverSelector.addServerAndUpdateSegment(queryableDruidServer1, serverSelector.getSegment());\n \n     TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder().dataSource(\"test\").build();\n     query = query.withOverriddenContext(ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, Long.MAX_VALUE));\n     cancellationFuture.set(new StatusResponseHolder(HttpResponseStatus.OK, new StringBuilder(\"cancelled\")));\n-    Sequence results = client1.run(QueryPlus.wrap(query));\n+    Sequence results = client.run(QueryPlus.wrap(query));\n     Assert.assertEquals(HttpMethod.DELETE, capturedRequest.getValue().getMethod());\n-    Assert.assertEquals(0, client1.getNumOpenConnections());\n+    Assert.assertEquals(0, client.getNumOpenConnections());\n \n \n     QueryInterruptedException exception = null;\n@@ -278,63 +280,31 @@ public void testCancel()\n   @Test\n   public void testQueryInterruptionExceptionLogMessage()\n   {\n-    HttpClient httpClient = EasyMock.createMock(HttpClient.class);\n     SettableFuture<Object> interruptionFuture = SettableFuture.create();\n     Capture<Request> capturedRequest = EasyMock.newCapture();\n-    String hostName = \"localhost:8080\";\n-    EasyMock.expect(\n-        httpClient.go(\n-            EasyMock.capture(capturedRequest),\n-            EasyMock.<HttpResponseHandler>anyObject(),\n-            EasyMock.anyObject(Duration.class)\n+    final String hostName = \"localhost:8080\";\n+    EasyMock\n+        .expect(\n+            httpClient.go(\n+                EasyMock.capture(capturedRequest),\n+                EasyMock.<HttpResponseHandler>anyObject(),\n+                EasyMock.anyObject(Duration.class)\n+            )\n         )\n-    )\n-            .andReturn(interruptionFuture)\n-            .anyTimes();\n+        .andReturn(interruptionFuture)\n+        .anyTimes();\n \n     EasyMock.replay(httpClient);\n \n-    DataSegment dataSegment = new DataSegment(\n-        \"test\",\n-        Intervals.of(\"2013-01-01/2013-01-02\"),\n-        DateTimes.of(\"2013-01-01\").toString(),\n-        new HashMap<>(),\n-        new ArrayList<>(),\n-        new ArrayList<>(),\n-        NoneShardSpec.instance(),\n-        0,\n-        0L\n-    );\n-    final ServerSelector serverSelector = new ServerSelector(\n-        dataSegment,\n-        new HighestPriorityTierSelectorStrategy(new ConnectionCountServerSelectorStrategy())\n-    );\n-\n-    DirectDruidClient client1 = new DirectDruidClient(\n-        new ReflectionQueryToolChestWarehouse(),\n-        QueryRunnerTestHelper.NOOP_QUERYWATCHER,\n-        new DefaultObjectMapper(),\n-        httpClient,\n-        \"http\",\n-        hostName,\n-        new NoopServiceEmitter()\n-    );\n-\n-    QueryableDruidServer queryableDruidServer = new QueryableDruidServer(\n-        new DruidServer(\"test1\", hostName, null, 0, ServerType.HISTORICAL, DruidServer.DEFAULT_TIER, 0),\n-        client1\n-    );\n-\n-    serverSelector.addServerAndUpdateSegment(queryableDruidServer, dataSegment);\n-\n+    // test error\n     TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder().dataSource(\"test\").build();\n     query = query.withOverriddenContext(ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, Long.MAX_VALUE));\n     interruptionFuture.set(\n         new ByteArrayInputStream(\n             StringUtils.toUtf8(\"{\\\"error\\\":\\\"testing1\\\",\\\"errorMessage\\\":\\\"testing2\\\"}\")\n         )\n     );\n-    Sequence results = client1.run(QueryPlus.wrap(query));\n+    Sequence results = client.run(QueryPlus.wrap(query));\n \n     QueryInterruptedException actualException = null;\n     try {\n@@ -349,4 +319,97 @@ public void testQueryInterruptionExceptionLogMessage()\n     Assert.assertEquals(hostName, actualException.getHost());\n     EasyMock.verify(httpClient);\n   }\n+\n+  @Test\n+  public void testQueryTimeoutBeforeFuture() throws IOException, InterruptedException\n+  {\n+    SettableFuture<Object> timeoutFuture = SettableFuture.create();\n+    Capture<Request> capturedRequest = EasyMock.newCapture();\n+    final String queryId = \"timeout-before-future\";\n+\n+    EasyMock\n+        .expect(\n+            httpClient.go(\n+                EasyMock.capture(capturedRequest),\n+                EasyMock.<HttpResponseHandler>anyObject(),\n+                EasyMock.anyObject(Duration.class)\n+            )\n+        )\n+        .andReturn(timeoutFuture)\n+        .anyTimes();\n+\n+    EasyMock.replay(httpClient);\n+\n+    TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder().dataSource(\"test\").build();\n+    query = query.withOverriddenContext(\n+        ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, System.currentTimeMillis() + 250, \"queryId\", queryId)\n+    );\n+\n+    Sequence results = client.run(QueryPlus.wrap(query));\n+\n+    // incomplete result set\n+    PipedInputStream in = new PipedInputStream();\n+    final PipedOutputStream out = new PipedOutputStream(in);\n+    timeoutFuture.set(\n+        in\n+    );\n+\n+    QueryInterruptedException actualException = null;\n+    try {\n+      out.write(StringUtils.toUtf8(\"[{\\\"timestamp\\\":\\\"2014-01-01T01:02:03Z\\\"}\"));\n+      Thread.sleep(250);\n+      out.write(StringUtils.toUtf8(\"]\"));\n+      out.close();\n+      results.toList();\n+    }\n+    catch (QueryInterruptedException e) {\n+      actualException = e;\n+    }\n+    Assert.assertNotNull(actualException);\n+    Assert.assertEquals(\"Query timeout\", actualException.getErrorCode());\n+    Assert.assertEquals(\"url[http://localhost:8080/druid/v2/] timed out\", actualException.getMessage());\n+    Assert.assertEquals(hostName, actualException.getHost());\n+    EasyMock.verify(httpClient);\n+  }\n+\n+  @Test\n+  public void testQueryTimeoutFromFuture()\n+  {\n+    SettableFuture<Object> noFuture = SettableFuture.create();\n+    Capture<Request> capturedRequest = EasyMock.newCapture();\n+    final String queryId = \"never-ending-future\";\n+\n+    EasyMock\n+        .expect(\n+            httpClient.go(\n+                EasyMock.capture(capturedRequest),\n+                EasyMock.<HttpResponseHandler>anyObject(),\n+                EasyMock.anyObject(Duration.class)\n+            )\n+        )\n+        .andReturn(noFuture)\n+        .anyTimes();\n+\n+    EasyMock.replay(httpClient);\n+\n+    TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder().dataSource(\"test\").build();\n+    query = query.withOverriddenContext(\n+        ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, System.currentTimeMillis() + 500, \"queryId\", queryId)\n+    );\n+\n+    Sequence results = client.run(QueryPlus.wrap(query));\n+\n+    QueryInterruptedException actualException = null;\n+    try {\n+      results.toList();\n+    }\n+    catch (QueryInterruptedException e) {\n+      actualException = e;\n+    }\n+    Assert.assertNotNull(actualException);\n+    Assert.assertEquals(\"Query timeout\", actualException.getErrorCode());\n+    Assert.assertEquals(\"Timeout waiting for task.\", actualException.getMessage());\n+    Assert.assertEquals(hostName, actualException.getHost());\n+    EasyMock.verify(httpClient);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7781820dead727e84f1b61a0e6c219ec115862c1/server/src/test/java/org/apache/druid/client/DirectDruidClientTest.java",
                "sha": "caa549a5b146d543004449eb48218d97177b0fe3",
                "status": "modified"
            }
        ],
        "message": "JsonParserIterator.init future timeout (#8550)\n\n* add timeout support for JsonParserIterator init future\r\n\r\n* add queryId\r\n\r\n* should be less than 1\r\n\r\n* fix\r\n\r\n* fix npe\r\n\r\n* fix lgtm\r\n\r\n* adjust exception, nullable\r\n\r\n* fix test\r\n\r\n* refactor\r\n\r\n* revert queryId change\r\n\r\n* add log.warn to tie exception to json parser iterator",
        "parent": "https://github.com/apache/incubator-druid/commit/7f2b6577ef19f18523e8353336ad496e8dc4a270",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java",
            "QueryInterruptedExceptionTest.java"
        ]
    },
    "incubator-druid_799c66d": {
        "bug_id": "incubator-druid_799c66d",
        "commit": "https://github.com/apache/incubator-druid/commit/799c66d9ac74019e6da64f4cd31118b9ad12b38e",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/799c66d9ac74019e6da64f4cd31118b9ad12b38e/docs/content/querying/scan-query.md",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/querying/scan-query.md?ref=799c66d9ac74019e6da64f4cd31118b9ad12b38e",
                "deletions": 5,
                "filename": "docs/content/querying/scan-query.md",
                "patch": "@@ -63,7 +63,7 @@ The following are the main parameters for Scan queries:\n |limit|How many rows to return. If not specified, all rows will be returned.|no|\n |order|The ordering of returned rows based on timestamp.  \"ascending\", \"descending\", and \"none\" (default) are supported.  Currently, \"ascending\" and \"descending\" are only supported for queries where the limit is less than `druid.query.scan.maxRowsQueuedForOrdering`.  Scan queries that are either legacy mode or have a limit greater than `druid.query.scan.maxRowsQueuedForOrdering` will not be time-ordered and default to a order of \"none\".|none|\n |legacy|Return results consistent with the legacy \"scan-query\" contrib extension. Defaults to the value set by `druid.query.scan.legacy`, which in turn defaults to false. See [Legacy mode](#legacy-mode) for details.|no|\n-|context|An additional JSON Object which can be used to specify certain flags.|no|\n+|context|An additional JSON Object which can be used to specify certain flags (see the Query Context Properties section below).|no|\n \n ## Example results\n \n@@ -179,7 +179,9 @@ decompression and decoding buffers for each.  The `druid.query.scan.maxSegmentPa\n from this by capping the number of partitions opened at any times when time ordering is used.\n \n Both `druid.query.scan.maxRowsQueuedForOrdering` and `druid.query.scan.maxSegmentPartitionsOrderedInMemory` are \n-configurable and can be tuned based on hardware specs and number of dimensions being queried.\n+configurable and can be tuned based on hardware specs and number of dimensions being queried.  These config properties\n+can also be overridden using the `maxRowsQueuedForOrdering` and `maxSegmentPartitionsOrderedInMemory` properties in \n+the query context (see the Query Context Properties section).\n   \n ## Legacy mode\n \n@@ -198,8 +200,27 @@ is complete.\n \n ## Configuration Properties\n \n+Configuration properties:\n+\n+|property|description|values|default|\n+|--------|-----------|------|-------|\n+|druid.query.scan.maxRowsQueuedForOrdering|The maximum number of rows returned when time ordering is used|An integer in [1, 2147483647]|100000|\n+|druid.query.scan.maxSegmentPartitionsOrderedInMemory|The maximum number of segments scanned per historical when time ordering is used|An integer in [1, 2147483647]|50|\n+|druid.query.scan.legacy|Whether legacy mode should be turned on for Scan queries|true or false|false|\n+\n+\n+## Query Context Properties\n+\n |property|description|values|default|\n |--------|-----------|------|-------|\n-|druid.query.scan.maxRowsQueuedForOrdering|The maximum number of rows returned when time ordering is used|An integer in [0, 2147483647]|100000|\n-|druid.query.scan.maxSegmentPartitionsOrderedInMemory|The maximum number of segments scanned per historical when time ordering is used|An integer in [0, 2147483647]|50|\n-|druid.query.scan.legacy|Whether legacy mode should be turned on for Scan queries|true or false|false|\n\\ No newline at end of file\n+|maxRowsQueuedForOrdering|The maximum number of rows returned when time ordering is used.  Overrides the identically named config.|An integer in [1, 2147483647]|`druid.query.scan.maxRowsQueuedForOrdering`|\n+|maxSegmentPartitionsOrderedInMemory|The maximum number of segments scanned per historical when time ordering is used.  Overrides the identically named config.|An integer in [1, 2147483647]|`druid.query.scan.maxSegmentPartitionsOrderedInMemory`|\n+\n+Sample query context JSON object:\n+\n+```json\n+{\n+  \"maxRowsQueuedForOrdering\": 100001,\n+  \"maxSegmentPartitionsOrderedInMemory\": 100\t\n+}\n+```",
                "raw_url": "https://github.com/apache/incubator-druid/raw/799c66d9ac74019e6da64f4cd31118b9ad12b38e/docs/content/querying/scan-query.md",
                "sha": "7ba5c6097d9fd170789fcc1d642c856521f10a9b",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/incubator-druid/blob/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQuery.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/scan/ScanQuery.java?ref=799c66d9ac74019e6da64f4cd31118b9ad12b38e",
                "deletions": 3,
                "filename": "processing/src/main/java/org/apache/druid/query/scan/ScanQuery.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.query.scan;\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonValue;\n import com.google.common.base.Preconditions;\n@@ -34,6 +35,7 @@\n import org.apache.druid.query.spec.QuerySegmentSpec;\n import org.apache.druid.segment.VirtualColumns;\n \n+import javax.annotation.Nullable;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n@@ -112,6 +114,8 @@ public static Order fromString(String name)\n   private final List<String> columns;\n   private final Boolean legacy;\n   private final Order order;\n+  private final Integer maxRowsQueuedForOrdering;\n+  private final Integer maxSegmentPartitionsOrderedInMemory;\n \n   @JsonCreator\n   public ScanQuery(\n@@ -132,13 +136,43 @@ public ScanQuery(\n     this.virtualColumns = VirtualColumns.nullToEmpty(virtualColumns);\n     this.resultFormat = (resultFormat == null) ? ResultFormat.RESULT_FORMAT_LIST : resultFormat;\n     this.batchSize = (batchSize == 0) ? 4096 * 5 : batchSize;\n+    Preconditions.checkArgument(\n+        this.batchSize > 0,\n+        \"batchSize must be greater than 0\"\n+    );\n     this.limit = (limit == 0) ? Long.MAX_VALUE : limit;\n-    Preconditions.checkArgument(this.batchSize > 0, \"batchSize must be greater than 0\");\n-    Preconditions.checkArgument(this.limit > 0, \"limit must be greater than 0\");\n+    Preconditions.checkArgument(\n+        this.limit > 0,\n+        \"limit must be greater than 0\"\n+    );\n     this.dimFilter = dimFilter;\n     this.columns = columns;\n     this.legacy = legacy;\n-    this.order = order == null ? Order.NONE : order;\n+    this.order = (order == null) ? Order.NONE : order;\n+    this.maxRowsQueuedForOrdering = validateAndGetMaxRowsQueuedForOrdering();\n+    this.maxSegmentPartitionsOrderedInMemory = validateAndGetMaxSegmentPartitionsOrderedInMemory();\n+  }\n+\n+  private Integer validateAndGetMaxRowsQueuedForOrdering()\n+  {\n+    final Integer maxRowsQueuedForOrdering =\n+        getContextValue(ScanQueryConfig.CTX_KEY_MAX_ROWS_QUEUED_FOR_ORDERING, null);\n+    Preconditions.checkArgument(\n+        maxRowsQueuedForOrdering == null || maxRowsQueuedForOrdering > 0,\n+        \"maxRowsQueuedForOrdering must be greater than 0\"\n+    );\n+    return maxRowsQueuedForOrdering;\n+  }\n+\n+  private Integer validateAndGetMaxSegmentPartitionsOrderedInMemory()\n+  {\n+    final Integer maxSegmentPartitionsOrderedInMemory =\n+        getContextValue(ScanQueryConfig.CTX_KEY_MAX_SEGMENT_PARTITIONS_FOR_ORDERING, null);\n+    Preconditions.checkArgument(\n+        maxSegmentPartitionsOrderedInMemory == null || maxSegmentPartitionsOrderedInMemory > 0,\n+        \"maxRowsQueuedForOrdering must be greater than 0\"\n+    );\n+    return maxSegmentPartitionsOrderedInMemory;\n   }\n \n   @JsonProperty\n@@ -171,6 +205,20 @@ public Order getOrder()\n     return order;\n   }\n \n+  @Nullable\n+  @JsonIgnore\n+  public Integer getMaxRowsQueuedForOrdering()\n+  {\n+    return maxRowsQueuedForOrdering;\n+  }\n+\n+  @Nullable\n+  @JsonIgnore\n+  public Integer getMaxSegmentPartitionsOrderedInMemory()\n+  {\n+    return maxSegmentPartitionsOrderedInMemory;\n+  }\n+\n   @Override\n   public boolean hasFilters()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQuery.java",
                "sha": "2b6fc82b71d3a426de84567133733d69ec993687",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQueryConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/scan/ScanQueryConfig.java?ref=799c66d9ac74019e6da64f4cd31118b9ad12b38e",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/scan/ScanQueryConfig.java",
                "patch": "@@ -25,6 +25,9 @@\n \n public class ScanQueryConfig\n {\n+  public static final String CTX_KEY_MAX_ROWS_QUEUED_FOR_ORDERING = \"maxRowsQueuedForOrdering\";\n+  public static final String CTX_KEY_MAX_SEGMENT_PARTITIONS_FOR_ORDERING = \"maxSegmentPartitionsOrderedInMemory\";\n+\n   @JsonProperty\n   private boolean legacy = false;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQueryConfig.java",
                "sha": "b926972826090890e1e79360931878830bde84bc",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQueryRunnerFactory.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/scan/ScanQueryRunnerFactory.java?ref=799c66d9ac74019e6da64f4cd31118b9ad12b38e",
                "deletions": 3,
                "filename": "processing/src/main/java/org/apache/druid/query/scan/ScanQueryRunnerFactory.java",
                "patch": "@@ -128,8 +128,10 @@ public ScanQueryRunnerFactory(\n           descriptorsOrdered = Lists.reverse(descriptorsOrdered);\n           queryRunnersOrdered = Lists.reverse(queryRunnersOrdered);\n         }\n-\n-        if (query.getLimit() <= scanQueryConfig.getMaxRowsQueuedForOrdering()) {\n+        int maxRowsQueuedForOrdering = (query.getMaxRowsQueuedForOrdering() == null\n+                                        ? scanQueryConfig.getMaxRowsQueuedForOrdering()\n+                                        : query.getMaxRowsQueuedForOrdering());\n+        if (query.getLimit() <= maxRowsQueuedForOrdering) {\n           // Use priority queue strategy\n           return priorityQueueSortAndLimit(\n               Sequences.concat(Sequences.map(\n@@ -172,7 +174,10 @@ public ScanQueryRunnerFactory(\n                                          .max(Comparator.comparing(Integer::valueOf))\n                                          .get();\n \n-          if (maxNumPartitionsInSegment <= scanQueryConfig.getMaxSegmentPartitionsOrderedInMemory()) {\n+          int segmentPartitionLimit = (query.getMaxSegmentPartitionsOrderedInMemory() == null\n+                                       ? scanQueryConfig.getMaxSegmentPartitionsOrderedInMemory()\n+                                       : query.getMaxSegmentPartitionsOrderedInMemory());\n+          if (maxNumPartitionsInSegment <= segmentPartitionLimit) {\n             // Use n-way merge strategy\n \n             // Create a list of grouped runner lists (i.e. each sublist/\"runner group\" corresponds to an interval) ->",
                "raw_url": "https://github.com/apache/incubator-druid/raw/799c66d9ac74019e6da64f4cd31118b9ad12b38e/processing/src/main/java/org/apache/druid/query/scan/ScanQueryRunnerFactory.java",
                "sha": "5f49a66694488eb59e6ff6a8368a10a656e5089b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/799c66d9ac74019e6da64f4cd31118b9ad12b38e/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java?ref=799c66d9ac74019e6da64f4cd31118b9ad12b38e",
                "deletions": 1,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "patch": "@@ -85,7 +85,6 @@\n \n public class CalciteQueryTest extends BaseCalciteQueryTest\n {\n-\n   @Test\n   public void testSelectConstantExpression() throws Exception\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/799c66d9ac74019e6da64f4cd31118b9ad12b38e/sql/src/test/java/org/apache/druid/sql/calcite/CalciteQueryTest.java",
                "sha": "2a9049dd902e167added20be4f12b01def2a0ef6",
                "status": "modified"
            }
        ],
        "message": "Allow max rows and max segments for time-ordered scans to be overridden using the scan query JSON spec (#7413)\n\n* Initial changes\r\n\r\n* Fixed NPEs\r\n\r\n* Fixed failing spec test\r\n\r\n* Fixed failing Calcite test\r\n\r\n* Move configs to context\r\n\r\n* Validated and added docs\r\n\r\n* fixed weird indentation\r\n\r\n* Update default context vals in doc\r\n\r\n* Fixed allowable values",
        "parent": "https://github.com/apache/incubator-druid/commit/7778f29781cb5a4d96e09e2f0bd67570ffc7f38a",
        "repo": "incubator-druid",
        "unit_tests": [
            "ScanQueryTest.java",
            "ScanQueryConfigTest.java",
            "ScanQueryRunnerFactoryTest.java"
        ]
    },
    "incubator-druid_7a09cde": {
        "bug_id": "incubator-druid_7a09cde",
        "commit": "https://github.com/apache/incubator-druid/commit/7a09cde4de1953eee75c5033e863cfde8f94d6c1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/docs/content/configuration/index.md",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/configuration/index.md?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 0,
                "filename": "docs/content/configuration/index.md",
                "patch": "@@ -1258,6 +1258,7 @@ The Druid SQL server is configured through the following properties on the broke\n |`druid.sql.avatica.maxStatementsPerConnection`|Maximum number of simultaneous open statements per Avatica client connection.|1|\n |`druid.sql.avatica.connectionIdleTimeout`|Avatica client connection idle timeout.|PT5M|\n |`druid.sql.http.enable`|Whether to enable JSON over HTTP querying at `/druid/v2/sql/`.|true|\n+|`druid.sql.planner.awaitInitializationOnStart`|Boolean|Whether the the Broker will wait for its SQL metadata view to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also `druid.broker.segment.awaitInitializationOnStart`, a related setting.|true|\n |`druid.sql.planner.maxQueryCount`|Maximum number of queries to issue, including nested queries. Set to 1 to disable sub-queries, or set to 0 for unlimited.|8|\n |`druid.sql.planner.maxSemiJoinRowsInMemory`|Maximum number of rows to keep in memory for executing two-stage semi-join queries like `SELECT * FROM Employee WHERE DeptName IN (SELECT DeptName FROM Dept)`.|100000|\n |`druid.sql.planner.maxTopNLimit`|Maximum threshold for a [TopN query](../querying/topnquery.html). Higher limits will be planned as [GroupBy queries](../querying/groupbyquery.html) instead.|100000|\n@@ -1291,6 +1292,7 @@ See [cache configuration](#cache-configuration) for how to configure cache setti\n |`druid.announcer.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n |`druid.broker.segment.watchedTiers`|List of strings|Broker watches the segment announcements from nodes serving segments to build cache of which node is serving which segments, this configuration allows to only consider segments being served from a whitelist of tiers. By default, Broker would consider all tiers. This can be used to partition your dataSources in specific historical tiers and configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n |`druid.broker.segment.watchedDataSources`|List of strings|Broker watches the segment announcements from nodes serving segments to build cache of which node is serving which segments, this configuration allows to only consider segments being served from a whitelist of dataSources. By default, Broker would consider all datasources. This can be used to configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n+|`druid.broker.segment.awaitInitializationOnStart`|Boolean|Whether the the Broker will wait for its view of segments to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also `druid.sql.planner.awaitInitializationOnStart`, a related setting.|true|\n \n ## Historical\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/docs/content/configuration/index.md",
                "sha": "92942a068cfcd58aff53b3667136f289dcdf1d86",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/server/src/main/java/org/apache/druid/client/BrokerSegmentWatcherConfig.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/BrokerSegmentWatcherConfig.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/BrokerSegmentWatcherConfig.java",
                "patch": "@@ -33,6 +33,9 @@\n   @JsonProperty\n   private Set<String> watchedDataSources = null;\n \n+  @JsonProperty\n+  private boolean awaitInitializationOnStart = true;\n+\n   public Set<String> getWatchedTiers()\n   {\n     return watchedTiers;\n@@ -42,4 +45,9 @@\n   {\n     return watchedDataSources;\n   }\n+\n+  public boolean isAwaitInitializationOnStart()\n+  {\n+    return awaitInitializationOnStart;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/server/src/main/java/org/apache/druid/client/BrokerSegmentWatcherConfig.java",
                "sha": "0abb45673631c86111e7d931644286a396f35ea3",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/BrokerServerView.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 11,
                "filename": "server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "patch": "@@ -27,10 +27,12 @@\n import org.apache.druid.client.selector.QueryableDruidServer;\n import org.apache.druid.client.selector.ServerSelector;\n import org.apache.druid.client.selector.TierSelectorStrategy;\n+import org.apache.druid.guice.ManageLifecycle;\n import org.apache.druid.guice.annotations.EscalatedClient;\n import org.apache.druid.guice.annotations.Smile;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.concurrent.Execs;\n+import org.apache.druid.java.util.common.lifecycle.LifecycleStart;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.java.util.emitter.service.ServiceEmitter;\n import org.apache.druid.java.util.http.client.HttpClient;\n@@ -49,13 +51,15 @@\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n \n /**\n  */\n+@ManageLifecycle\n public class BrokerServerView implements TimelineServerView\n {\n   private static final Logger log = new Logger(BrokerServerView.class);\n@@ -74,19 +78,20 @@\n   private final FilteredServerInventoryView baseView;\n   private final TierSelectorStrategy tierSelectorStrategy;\n   private final ServiceEmitter emitter;\n+  private final BrokerSegmentWatcherConfig segmentWatcherConfig;\n   private final Predicate<Pair<DruidServerMetadata, DataSegment>> segmentFilter;\n \n-  private volatile boolean initialized = false;\n+  private final CountDownLatch initialized = new CountDownLatch(1);\n \n   @Inject\n   public BrokerServerView(\n-      QueryToolChestWarehouse warehouse,\n-      QueryWatcher queryWatcher,\n-      @Smile ObjectMapper smileMapper,\n-      @EscalatedClient HttpClient httpClient,\n-      FilteredServerInventoryView baseView,\n-      TierSelectorStrategy tierSelectorStrategy,\n-      ServiceEmitter emitter,\n+      final QueryToolChestWarehouse warehouse,\n+      final QueryWatcher queryWatcher,\n+      final @Smile ObjectMapper smileMapper,\n+      final @EscalatedClient HttpClient httpClient,\n+      final FilteredServerInventoryView baseView,\n+      final TierSelectorStrategy tierSelectorStrategy,\n+      final ServiceEmitter emitter,\n       final BrokerSegmentWatcherConfig segmentWatcherConfig\n   )\n   {\n@@ -97,6 +102,7 @@ public BrokerServerView(\n     this.baseView = baseView;\n     this.tierSelectorStrategy = tierSelectorStrategy;\n     this.emitter = emitter;\n+    this.segmentWatcherConfig = segmentWatcherConfig;\n     this.clients = new ConcurrentHashMap<>();\n     this.selectors = new HashMap<>();\n     this.timelines = new HashMap<>();\n@@ -143,7 +149,7 @@ public boolean apply(\n           @Override\n           public CallbackAction segmentViewInitialized()\n           {\n-            initialized = true;\n+            initialized.countDown();\n             runTimelineCallbacks(TimelineCallback::timelineInitialized);\n             return ServerView.CallbackAction.CONTINUE;\n           }\n@@ -165,9 +171,25 @@ public CallbackAction segmentViewInitialized()\n     );\n   }\n \n+  @LifecycleStart\n+  public void start() throws InterruptedException\n+  {\n+    if (segmentWatcherConfig.isAwaitInitializationOnStart()) {\n+      final long startMillis = System.currentTimeMillis();\n+      log.info(\"%s waiting for initialization.\", getClass().getSimpleName());\n+      awaitInitialization();\n+      log.info(\"%s initialized in [%,d] ms.\", getClass().getSimpleName(), System.currentTimeMillis() - startMillis);\n+    }\n+  }\n+\n   public boolean isInitialized()\n   {\n-    return initialized;\n+    return initialized.getCount() == 0;\n+  }\n+\n+  public void awaitInitialization() throws InterruptedException\n+  {\n+    initialized.await();\n   }\n \n   private QueryableDruidServer addServer(DruidServer server)\n@@ -183,7 +205,15 @@ private QueryableDruidServer addServer(DruidServer server)\n \n   private DirectDruidClient makeDirectClient(DruidServer server)\n   {\n-    return new DirectDruidClient(warehouse, queryWatcher, smileMapper, httpClient, server.getScheme(), server.getHost(), emitter);\n+    return new DirectDruidClient(\n+        warehouse,\n+        queryWatcher,\n+        smileMapper,\n+        httpClient,\n+        server.getScheme(),\n+        server.getHost(),\n+        emitter\n+    );\n   }\n \n   private QueryableDruidServer removeServer(DruidServer server)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/server/src/main/java/org/apache/druid/client/BrokerServerView.java",
                "sha": "0b28a1b9ebb63cd25ca1b6a327d1d4c21dd8391e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f12a1aa9938b1a1b187e400c279661080470654a/server/src/main/java/org/apache/druid/server/coordination/broker/DruidBroker.java",
                "changes": 90,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordination/broker/DruidBroker.java?ref=f12a1aa9938b1a1b187e400c279661080470654a",
                "deletions": 90,
                "filename": "server/src/main/java/org/apache/druid/server/coordination/broker/DruidBroker.java",
                "patch": "@@ -1,90 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.druid.server.coordination.broker;\n-\n-import com.google.common.base.Predicates;\n-import com.google.common.util.concurrent.MoreExecutors;\n-import com.google.inject.Inject;\n-import org.apache.druid.client.FilteredServerInventoryView;\n-import org.apache.druid.client.ServerView;\n-import org.apache.druid.curator.discovery.ServiceAnnouncer;\n-import org.apache.druid.guice.ManageLifecycle;\n-import org.apache.druid.guice.annotations.Self;\n-import org.apache.druid.java.util.common.lifecycle.LifecycleStart;\n-import org.apache.druid.java.util.common.lifecycle.LifecycleStop;\n-import org.apache.druid.server.DruidNode;\n-\n-@ManageLifecycle\n-public class DruidBroker\n-{\n-  private final DruidNode self;\n-  private final ServiceAnnouncer serviceAnnouncer;\n-\n-  private volatile boolean started = false;\n-\n-  @Inject\n-  public DruidBroker(\n-      final FilteredServerInventoryView serverInventoryView,\n-      final @Self DruidNode self,\n-      final ServiceAnnouncer serviceAnnouncer\n-  )\n-  {\n-    this.self = self;\n-    this.serviceAnnouncer = serviceAnnouncer;\n-\n-    serverInventoryView.registerSegmentCallback(\n-        MoreExecutors.sameThreadExecutor(),\n-        new ServerView.BaseSegmentCallback()\n-        {\n-          @Override\n-          public ServerView.CallbackAction segmentViewInitialized()\n-          {\n-            serviceAnnouncer.announce(self);\n-            return ServerView.CallbackAction.UNREGISTER;\n-          }\n-        },\n-        // We are not interested in any segment callbacks except view initialization\n-        Predicates.alwaysFalse()\n-    );\n-  }\n-\n-  @LifecycleStart\n-  public void start()\n-  {\n-    synchronized (self) {\n-      if (started) {\n-        return;\n-      }\n-      started = true;\n-    }\n-  }\n-\n-  @LifecycleStop\n-  public void stop()\n-  {\n-    synchronized (self) {\n-      if (!started) {\n-        return;\n-      }\n-      serviceAnnouncer.unannounce(self);\n-      started = false;\n-    }\n-  }\n-}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f12a1aa9938b1a1b187e400c279661080470654a/server/src/main/java/org/apache/druid/server/coordination/broker/DruidBroker.java",
                "sha": "41e6d9ad0b5cd87e4731d29423985d8eea277bdf",
                "status": "removed"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliBroker.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliBroker.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 9,
                "filename": "services/src/main/java/org/apache/druid/cli/CliBroker.java",
                "patch": "@@ -20,7 +20,6 @@\n package org.apache.druid.cli;\n \n import com.google.common.collect.ImmutableList;\n-import com.google.inject.Key;\n import com.google.inject.Module;\n import com.google.inject.name.Names;\n import io.airlift.airline.Command;\n@@ -51,7 +50,6 @@\n import org.apache.druid.server.BrokerQueryResource;\n import org.apache.druid.server.ClientInfoResource;\n import org.apache.druid.server.ClientQuerySegmentWalker;\n-import org.apache.druid.server.coordination.broker.DruidBroker;\n import org.apache.druid.server.http.BrokerResource;\n import org.apache.druid.server.initialization.jetty.JettyServerInitializer;\n import org.apache.druid.server.metrics.MetricsModule;\n@@ -94,7 +92,7 @@ public CliBroker()\n           binder.bindConstant().annotatedWith(PruneLoadSpec.class).to(true);\n \n           binder.bind(CachingClusteredClient.class).in(LazySingleton.class);\n-          binder.bind(BrokerServerView.class).in(LazySingleton.class);\n+          LifecycleModule.register(binder, BrokerServerView.class);\n           binder.bind(TimelineServerView.class).to(BrokerServerView.class).in(LazySingleton.class);\n \n           JsonConfigProvider.bind(binder, \"druid.broker.cache\", CacheConfig.class);\n@@ -117,19 +115,21 @@ public CliBroker()\n           Jerseys.addResource(binder, ClientInfoResource.class);\n \n           LifecycleModule.register(binder, BrokerQueryResource.class);\n-          LifecycleModule.register(binder, DruidBroker.class);\n \n           Jerseys.addResource(binder, HttpServerInventoryViewResource.class);\n \n           MetricsModule.register(binder, CacheMonitor.class);\n \n           LifecycleModule.register(binder, Server.class);\n \n-          binder\n-              .bind(DiscoverySideEffectsProvider.Child.class)\n-              .toProvider(new DiscoverySideEffectsProvider(NodeType.BROKER, ImmutableList.of(LookupNodeService.class)))\n-              .in(LazySingleton.class);\n-          LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class));\n+\n+          bindAnnouncer(\n+              binder,\n+              DiscoverySideEffectsProvider.builder(NodeType.BROKER)\n+                                          .serviceClasses(ImmutableList.of(LookupNodeService.class))\n+                                          .useLegacyAnnouncer(true)\n+                                          .build()\n+          );\n         },\n         new LookupModule(),\n         new SqlModule()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliBroker.java",
                "sha": "982223aabd266a8b383743f223640f15f9fed6fa",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliCoordinator.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 8,
                "filename": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "patch": "@@ -21,10 +21,8 @@\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Predicates;\n-import com.google.common.collect.ImmutableList;\n import com.google.inject.Binder;\n import com.google.inject.Inject;\n-import com.google.inject.Key;\n import com.google.inject.Module;\n import com.google.inject.Provides;\n import com.google.inject.name.Names;\n@@ -217,12 +215,11 @@ public void configure(Binder binder)\n                 DruidCoordinatorCleanupPendingSegments.class\n             );\n \n-            binder\n-                .bind(DiscoverySideEffectsProvider.Child.class)\n-                .annotatedWith(Coordinator.class)\n-                .toProvider(new DiscoverySideEffectsProvider(NodeType.COORDINATOR, ImmutableList.of()))\n-                .in(LazySingleton.class);\n-            LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class, Coordinator.class));\n+            bindAnnouncer(\n+                binder,\n+                Coordinator.class,\n+                DiscoverySideEffectsProvider.builder(NodeType.COORDINATOR).build()\n+            );\n           }\n \n           @Provides",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "sha": "7f36e0cb606536a08af77ee61a44c17d42de841b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliHistorical.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliHistorical.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 12,
                "filename": "services/src/main/java/org/apache/druid/cli/CliHistorical.java",
                "patch": "@@ -20,13 +20,11 @@\n package org.apache.druid.cli;\n \n import com.google.common.collect.ImmutableList;\n-import com.google.inject.Key;\n import com.google.inject.Module;\n import com.google.inject.name.Names;\n import io.airlift.airline.Command;\n import org.apache.druid.client.cache.CacheConfig;\n import org.apache.druid.client.cache.CacheMonitor;\n-import org.apache.druid.discovery.DataNodeService;\n import org.apache.druid.discovery.LookupNodeService;\n import org.apache.druid.discovery.NodeType;\n import org.apache.druid.guice.CacheModule;\n@@ -103,16 +101,12 @@ public CliHistorical()\n           binder.install(new CacheModule());\n           MetricsModule.register(binder, CacheMonitor.class);\n \n-          binder\n-              .bind(DiscoverySideEffectsProvider.Child.class)\n-              .toProvider(\n-                  new DiscoverySideEffectsProvider(\n-                      NodeType.HISTORICAL,\n-                      ImmutableList.of(DataNodeService.class, LookupNodeService.class)\n-                  )\n-              )\n-              .in(LazySingleton.class);\n-          LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class));\n+          bindAnnouncer(\n+              binder,\n+              DiscoverySideEffectsProvider.builder(NodeType.HISTORICAL)\n+                                          .serviceClasses(ImmutableList.of(LookupNodeService.class))\n+                                          .build()\n+          );\n         },\n         new LookupModule()\n     );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliHistorical.java",
                "sha": "d8432d3d55c888e113178ecdc8daf5850b6eb8e3",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliMiddleManager.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliMiddleManager.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 8,
                "filename": "services/src/main/java/org/apache/druid/cli/CliMiddleManager.java",
                "patch": "@@ -100,7 +100,7 @@ public void configure(Binder binder)\n             binder.bind(ForkingTaskRunner.class).in(LazySingleton.class);\n \n             binder.bind(IndexingServiceClient.class).toProvider(Providers.of(null));\n-            binder.bind(new TypeLiteral<IndexTaskClientFactory<ParallelIndexTaskClient>>(){})\n+            binder.bind(new TypeLiteral<IndexTaskClientFactory<ParallelIndexTaskClient>>() {})\n                   .toProvider(Providers.of(null));\n             binder.bind(ChatHandlerProvider.class).toProvider(Providers.of(null));\n             PolyBind.createChoice(\n@@ -130,13 +130,12 @@ public void configure(Binder binder)\n \n             LifecycleModule.register(binder, Server.class);\n \n-            binder\n-                .bind(DiscoverySideEffectsProvider.Child.class)\n-                .toProvider(\n-                    new DiscoverySideEffectsProvider(NodeType.MIDDLE_MANAGER, ImmutableList.of(WorkerNodeService.class))\n-                )\n-                .in(LazySingleton.class);\n-            LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class));\n+            bindAnnouncer(\n+                binder,\n+                DiscoverySideEffectsProvider.builder(NodeType.MIDDLE_MANAGER)\n+                                            .serviceClasses(ImmutableList.of(WorkerNodeService.class))\n+                                            .build()\n+            );\n           }\n \n           @Provides",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliMiddleManager.java",
                "sha": "25c4e13e32dc473dc18a24bf2e1b1575e45dd173",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliOverlord.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliOverlord.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 9,
                "filename": "services/src/main/java/org/apache/druid/cli/CliOverlord.java",
                "patch": "@@ -194,7 +194,7 @@ public void configure(Binder binder)\n             binder.bind(SupervisorManager.class).in(LazySingleton.class);\n \n             binder.bind(IndexingServiceClient.class).to(HttpIndexingServiceClient.class).in(LazySingleton.class);\n-            binder.bind(new TypeLiteral<IndexTaskClientFactory<ParallelIndexTaskClient>>(){})\n+            binder.bind(new TypeLiteral<IndexTaskClientFactory<ParallelIndexTaskClient>>() {})\n                   .toProvider(Providers.of(null));\n             binder.bind(ChatHandlerProvider.class).toProvider(Providers.of(null));\n \n@@ -237,12 +237,11 @@ public void configure(Binder binder)\n               LifecycleModule.register(binder, Server.class);\n             }\n \n-            binder\n-                .bind(DiscoverySideEffectsProvider.Child.class)\n-                .annotatedWith(IndexingService.class)\n-                .toProvider(new DiscoverySideEffectsProvider(NodeType.OVERLORD, ImmutableList.of()))\n-                .in(LazySingleton.class);\n-            LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class, IndexingService.class));\n+            bindAnnouncer(\n+                binder,\n+                IndexingService.class,\n+                DiscoverySideEffectsProvider.builder(NodeType.OVERLORD).build()\n+            );\n           }\n \n           private void configureTaskStorage(Binder binder)\n@@ -284,10 +283,14 @@ private void configureRunners(Binder binder)\n             biddy.addBinding(\"local\").to(ForkingTaskRunnerFactory.class);\n             binder.bind(ForkingTaskRunnerFactory.class).in(LazySingleton.class);\n \n-            biddy.addBinding(RemoteTaskRunnerFactory.TYPE_NAME).to(RemoteTaskRunnerFactory.class).in(LazySingleton.class);\n+            biddy.addBinding(RemoteTaskRunnerFactory.TYPE_NAME)\n+                 .to(RemoteTaskRunnerFactory.class)\n+                 .in(LazySingleton.class);\n             binder.bind(RemoteTaskRunnerFactory.class).in(LazySingleton.class);\n \n-            biddy.addBinding(HttpRemoteTaskRunnerFactory.TYPE_NAME).to(HttpRemoteTaskRunnerFactory.class).in(LazySingleton.class);\n+            biddy.addBinding(HttpRemoteTaskRunnerFactory.TYPE_NAME)\n+                 .to(HttpRemoteTaskRunnerFactory.class)\n+                 .in(LazySingleton.class);\n             binder.bind(HttpRemoteTaskRunnerFactory.class).in(LazySingleton.class);\n \n             JacksonConfigProvider.bind(binder, WorkerBehaviorConfig.CONFIG_KEY, WorkerBehaviorConfig.class, null);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliOverlord.java",
                "sha": "1cd3954936af577543276435a1e10194a48b6b95",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliRouter.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliRouter.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 6,
                "filename": "services/src/main/java/org/apache/druid/cli/CliRouter.java",
                "patch": "@@ -21,7 +21,6 @@\n \n import com.google.common.collect.ImmutableList;\n import com.google.inject.Binder;\n-import com.google.inject.Key;\n import com.google.inject.Module;\n import com.google.inject.TypeLiteral;\n import com.google.inject.name.Names;\n@@ -114,11 +113,10 @@ public void configure(Binder binder)\n             LifecycleModule.register(binder, Server.class);\n             DiscoveryModule.register(binder, Self.class);\n \n-            binder\n-                .bind(DiscoverySideEffectsProvider.Child.class)\n-                .toProvider(new DiscoverySideEffectsProvider(NodeType.ROUTER, ImmutableList.of()))\n-                .in(LazySingleton.class);\n-            LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class));\n+            bindAnnouncer(\n+                binder,\n+                DiscoverySideEffectsProvider.builder(NodeType.ROUTER).build()\n+            );\n           }\n         },\n         new LookupModule()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/CliRouter.java",
                "sha": "64b588736499b8b8b6e81137e405fb080a7d32af",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/ServerRunnable.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/ServerRunnable.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 2,
                "filename": "services/src/main/java/org/apache/druid/cli/ServerRunnable.java",
                "patch": "@@ -20,19 +20,26 @@\n package org.apache.druid.cli;\n \n import com.google.common.base.Throwables;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Binder;\n import com.google.inject.Inject;\n import com.google.inject.Injector;\n+import com.google.inject.Key;\n import com.google.inject.Provider;\n+import org.apache.druid.curator.discovery.ServiceAnnouncer;\n import org.apache.druid.discovery.DiscoveryDruidNode;\n import org.apache.druid.discovery.DruidNodeAnnouncer;\n import org.apache.druid.discovery.DruidService;\n import org.apache.druid.discovery.NodeType;\n+import org.apache.druid.guice.LazySingleton;\n+import org.apache.druid.guice.LifecycleModule;\n import org.apache.druid.guice.annotations.Self;\n import org.apache.druid.java.util.common.lifecycle.Lifecycle;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.server.DruidNode;\n \n+import java.lang.annotation.Annotation;\n import java.util.List;\n \n /**\n@@ -58,6 +65,32 @@ public void run()\n     }\n   }\n \n+  public static void bindAnnouncer(\n+      final Binder binder,\n+      final DiscoverySideEffectsProvider provider\n+  )\n+  {\n+    binder.bind(DiscoverySideEffectsProvider.Child.class)\n+          .toProvider(provider)\n+          .in(LazySingleton.class);\n+\n+    LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class));\n+  }\n+\n+  public static void bindAnnouncer(\n+      final Binder binder,\n+      final Class<? extends Annotation> annotation,\n+      final DiscoverySideEffectsProvider provider\n+  )\n+  {\n+    binder.bind(DiscoverySideEffectsProvider.Child.class)\n+          .annotatedWith(annotation)\n+          .toProvider(provider)\n+          .in(LazySingleton.class);\n+\n+    LifecycleModule.registerKey(binder, Key.get(DiscoverySideEffectsProvider.Child.class, annotation));\n+  }\n+\n   /**\n    * This is a helper class used by CliXXX classes to announce {@link DiscoveryDruidNode}\n    * as part of {@link Lifecycle.Stage#LAST}.\n@@ -66,12 +99,50 @@ public void run()\n   {\n     public static class Child {}\n \n-    @Inject @Self\n+    public static class Builder\n+    {\n+      private NodeType nodeType;\n+      private List<Class<? extends DruidService>> serviceClasses = ImmutableList.of();\n+      private boolean useLegacyAnnouncer;\n+\n+      public Builder(final NodeType nodeType)\n+      {\n+        this.nodeType = nodeType;\n+      }\n+\n+      public Builder serviceClasses(final List<Class<? extends DruidService>> serviceClasses)\n+      {\n+        this.serviceClasses = serviceClasses;\n+        return this;\n+      }\n+\n+      public Builder useLegacyAnnouncer(final boolean useLegacyAnnouncer)\n+      {\n+        this.useLegacyAnnouncer = useLegacyAnnouncer;\n+        return this;\n+      }\n+\n+      public DiscoverySideEffectsProvider build()\n+      {\n+        return new DiscoverySideEffectsProvider(nodeType, serviceClasses, useLegacyAnnouncer);\n+      }\n+    }\n+\n+    public static Builder builder(final NodeType nodeType)\n+    {\n+      return new Builder(nodeType);\n+    }\n+\n+    @Inject\n+    @Self\n     private DruidNode druidNode;\n \n     @Inject\n     private DruidNodeAnnouncer announcer;\n \n+    @Inject\n+    private ServiceAnnouncer legacyAnnouncer;\n+\n     @Inject\n     private Lifecycle lifecycle;\n \n@@ -80,11 +151,17 @@ public void run()\n \n     private final NodeType nodeType;\n     private final List<Class<? extends DruidService>> serviceClasses;\n+    private final boolean useLegacyAnnouncer;\n \n-    public DiscoverySideEffectsProvider(NodeType nodeType, List<Class<? extends DruidService>> serviceClasses)\n+    private DiscoverySideEffectsProvider(\n+        final NodeType nodeType,\n+        final List<Class<? extends DruidService>> serviceClasses,\n+        final boolean useLegacyAnnouncer\n+    )\n     {\n       this.nodeType = nodeType;\n       this.serviceClasses = serviceClasses;\n+      this.useLegacyAnnouncer = useLegacyAnnouncer;\n     }\n \n     @Override\n@@ -105,11 +182,21 @@ public Child get()\n             public void start()\n             {\n               announcer.announce(discoveryDruidNode);\n+\n+              if (useLegacyAnnouncer) {\n+                legacyAnnouncer.announce(discoveryDruidNode.getDruidNode());\n+              }\n             }\n \n             @Override\n             public void stop()\n             {\n+              // Reverse order vs. start().\n+\n+              if (useLegacyAnnouncer) {\n+                legacyAnnouncer.unannounce(discoveryDruidNode.getDruidNode());\n+              }\n+\n               announcer.unannounce(discoveryDruidNode);\n             }\n           },",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/services/src/main/java/org/apache/druid/cli/ServerRunnable.java",
                "sha": "77d409cb6e780366f4dbcf6244ff3cdfad49672e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerConfig.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerConfig.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 0,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerConfig.java",
                "patch": "@@ -60,6 +60,9 @@\n   @JsonProperty\n   private boolean requireTimeCondition = false;\n \n+  @JsonProperty\n+  private boolean awaitInitializationOnStart = true;\n+\n   @JsonProperty\n   private DateTimeZone sqlTimeZone = DateTimeZone.UTC;\n \n@@ -113,6 +116,11 @@ public DateTimeZone getSqlTimeZone()\n     return sqlTimeZone;\n   }\n \n+  public boolean isAwaitInitializationOnStart()\n+  {\n+    return awaitInitializationOnStart;\n+  }\n+\n   public PlannerConfig withOverrides(final Map<String, Object> context)\n   {\n     if (context == null) {\n@@ -142,6 +150,7 @@ public PlannerConfig withOverrides(final Map<String, Object> context)\n     );\n     newConfig.requireTimeCondition = isRequireTimeCondition();\n     newConfig.sqlTimeZone = getSqlTimeZone();\n+    newConfig.awaitInitializationOnStart = isAwaitInitializationOnStart();\n     return newConfig;\n   }\n \n@@ -181,6 +190,7 @@ public boolean equals(final Object o)\n            useApproximateTopN == that.useApproximateTopN &&\n            useFallback == that.useFallback &&\n            requireTimeCondition == that.requireTimeCondition &&\n+           awaitInitializationOnStart == that.awaitInitializationOnStart &&\n            Objects.equals(metadataRefreshPeriod, that.metadataRefreshPeriod) &&\n            Objects.equals(sqlTimeZone, that.sqlTimeZone);\n   }\n@@ -199,6 +209,7 @@ public int hashCode()\n         useApproximateTopN,\n         useFallback,\n         requireTimeCondition,\n+        awaitInitializationOnStart,\n         sqlTimeZone\n     );\n   }\n@@ -216,6 +227,7 @@ public String toString()\n            \", useApproximateTopN=\" + useApproximateTopN +\n            \", useFallback=\" + useFallback +\n            \", requireTimeCondition=\" + requireTimeCondition +\n+           \", awaitInitializationOnStart=\" + awaitInitializationOnStart +\n            \", sqlTimeZone=\" + sqlTimeZone +\n            '}';\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/main/java/org/apache/druid/sql/calcite/planner/PlannerConfig.java",
                "sha": "fe9e72ffb4beeeba74418e9d30cc6777f0b3ca1f",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 6,
                "filename": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "patch": "@@ -19,7 +19,6 @@\n \n package org.apache.druid.sql.calcite.schema;\n \n-import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableMultimap;\n@@ -98,7 +97,7 @@\n   private final ConcurrentMap<String, DruidTable> tables;\n \n   // For awaitInitialization.\n-  private final CountDownLatch initializationLatch = new CountDownLatch(1);\n+  private final CountDownLatch initialized = new CountDownLatch(1);\n \n   // Protects access to segmentSignatures, mutableSegments, segmentsNeedingRefresh, lastRefresh, isServerViewInitialized\n   private final Object lock = new Object();\n@@ -175,7 +174,7 @@ public DruidSchema(\n   }\n \n   @LifecycleStart\n-  public void start()\n+  public void start() throws InterruptedException\n   {\n     cacheExec.submit(\n         new Runnable()\n@@ -254,7 +253,7 @@ public void run()\n                     }\n                   }\n \n-                  initializationLatch.countDown();\n+                  initialized.countDown();\n                 }\n                 catch (InterruptedException e) {\n                   // Fall through.\n@@ -288,6 +287,13 @@ public void run()\n           }\n         }\n     );\n+\n+    if (config.isAwaitInitializationOnStart()) {\n+      final long startMillis = System.currentTimeMillis();\n+      log.info(\"%s waiting for initialization.\", getClass().getSimpleName());\n+      awaitInitialization();\n+      log.info(\"%s initialized in [%,d] ms.\", getClass().getSimpleName(), System.currentTimeMillis() - startMillis);\n+    }\n   }\n \n   @LifecycleStop\n@@ -296,10 +302,9 @@ public void stop()\n     cacheExec.shutdownNow();\n   }\n \n-  @VisibleForTesting\n   public void awaitInitialization() throws InterruptedException\n   {\n-    initializationLatch.await();\n+    initialized.await();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java",
                "sha": "40dcb56ee840fe4c74dbaa408471d51462cad2af",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java?ref=7a09cde4de1953eee75c5033e863cfde8f94d6c1",
                "deletions": 1,
                "filename": "sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "patch": "@@ -566,8 +566,8 @@ public static DruidSchema createMockSchema(\n         TEST_AUTHENTICATOR_ESCALATOR\n     );\n \n-    schema.start();\n     try {\n+      schema.start();\n       schema.awaitInitialization();\n     }\n     catch (InterruptedException e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7a09cde4de1953eee75c5033e863cfde8f94d6c1/sql/src/test/java/org/apache/druid/sql/calcite/util/CalciteTests.java",
                "sha": "488a3558dda43bb46146276de7c01ad1c86209f6",
                "status": "modified"
            }
        ],
        "message": "Broker: Await initialization before finishing startup. (#6742)\n\n* Broker: Await initialization before finishing startup.\r\n\r\nIn particular, hold off on announcing the service and starting the\r\nHTTP server until the server view and SQL metadata cache are finished\r\ninitializing. This closes a window of time where a Broker could return\r\npartial results shortly after startup.\r\n\r\nAs part of this, some simplification of server-lifecycle service\r\nannouncements. This helps ensure that the two different kinds of\r\nannouncements we do (legacy and new-style) stay in sync.\r\n\r\n* Remove unused imports.\r\n\r\n* Fix NPE in ServerRunnable.",
        "parent": "https://github.com/apache/incubator-druid/commit/f12a1aa9938b1a1b187e400c279661080470654a",
        "repo": "incubator-druid",
        "unit_tests": [
            "BrokerSegmentWatcherConfigTest.java",
            "BrokerServerViewTest.java",
            "DruidSchemaTest.java"
        ]
    },
    "incubator-druid_7bafb71": {
        "bug_id": "incubator-druid_7bafb71",
        "commit": "https://github.com/apache/incubator-druid/commit/7bafb718416a7345f3d6490877b04794165ed1b4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7bafb718416a7345f3d6490877b04794165ed1b4/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java?ref=7bafb718416a7345f3d6490877b04794165ed1b4",
                "deletions": 25,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "patch": "@@ -90,19 +90,7 @@ public TopNBinaryFn(\n     TopNResultValue arg2Vals = arg2.getValue();\n \n     for (DimensionAndMetricValueExtractor arg1Val : arg1Vals) {\n-      final String dimensionValue = arg1Val.getStringDimensionValue(dimension);\n-      Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n-      retVal.put(dimension, dimensionValue);\n-\n-      for (AggregatorFactory factory : aggregations) {\n-        final String metricName = factory.getName();\n-        retVal.put(metricName, arg1Val.getMetric(metricName));\n-      }\n-      for (PostAggregator postAgg : postAggregations) {\n-        retVal.put(postAgg.getName(), postAgg.compute(retVal));\n-      }\n-\n-      retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n+      retVals.put(arg1Val.getStringDimensionValue(dimension), arg1Val);\n     }\n     for (DimensionAndMetricValueExtractor arg2Val : arg2Vals) {\n       final String dimensionValue = arg2Val.getStringDimensionValue(dimension);\n@@ -124,18 +112,7 @@ public TopNBinaryFn(\n \n         retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n       } else {\n-        Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n-        retVal.put(dimension, dimensionValue);\n-\n-        for (AggregatorFactory factory : aggregations) {\n-          final String metricName = factory.getName();\n-          retVal.put(metricName, arg2Val.getMetric(metricName));\n-        }\n-        for (PostAggregator postAgg : postAggregations) {\n-          retVal.put(postAgg.getName(), postAgg.compute(retVal));\n-        }\n-\n-        retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n+        retVals.put(dimensionValue, arg2Val);\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7bafb718416a7345f3d6490877b04794165ed1b4/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "sha": "4c02da447aae7eb67c7b4d435de97c27d062eb0c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7bafb718416a7345f3d6490877b04794165ed1b4/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java?ref=7bafb718416a7345f3d6490877b04794165ed1b4",
                "deletions": 10,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "patch": "@@ -324,9 +324,9 @@ public void testMergeByPostAgg()\n                     \"testdim\", \"2\"\n                 ),\n                 ImmutableMap.<String, Object>of(\n-                    \"rows\", 4L,\n-                    \"index\", 5L,\n-                    \"testdim\", \"other\"\n+                    \"rows\", 0L,\n+                    \"index\", 1L,\n+                    \"testdim\", \"3\"\n                 )\n             )\n         )\n@@ -336,12 +336,6 @@ public void testMergeByPostAgg()\n         currTime,\n         new TopNResultValue(\n             ImmutableList.<Map<String, Object>>of(\n-                ImmutableMap.<String, Object>of(\n-                    \"testdim\", \"other\",\n-                    \"rows\", 4L,\n-                    \"index\", 5L,\n-                    \"addrowsindexconstant\", 10.0\n-                ),\n                 ImmutableMap.<String, Object>of(\n                     \"testdim\", \"1\",\n                     \"rows\", 3L,\n@@ -363,7 +357,7 @@ public void testMergeByPostAgg()\n         QueryGranularity.ALL,\n         new DefaultDimensionSpec(\"testdim\", null),\n         new NumericTopNMetricSpec(\"addrowsindexconstant\"),\n-        3,\n+        2,\n         aggregatorFactories,\n         postAggregators\n     ).apply(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7bafb718416a7345f3d6490877b04794165ed1b4/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "sha": "cb3089d639775d010b93f761f0ac9c43a6227a67",
                "status": "modified"
            }
        ],
        "message": "Revert \"fix npe in topNBinaryFn with post aggs\"\n\nThis reverts commit bbedde3418ba0a83a1777681d549b3ff1295b000.",
        "parent": "https://github.com/apache/incubator-druid/commit/357cd66f667ec0f0902975497fa82c75ff114970",
        "repo": "incubator-druid",
        "unit_tests": [
            "TopNBinaryFnTest.java"
        ]
    },
    "incubator-druid_7c4054a": {
        "bug_id": "incubator-druid_7c4054a",
        "commit": "https://github.com/apache/incubator-druid/commit/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java?ref=7c4054aaa3464ab2add5c1fcf7e318b9bff0f123",
                "deletions": 6,
                "filename": "processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "patch": "@@ -101,9 +101,7 @@ public void run(\n       dimValSelector = getDimValSelectorForTopNMetric(singleMetricParam, singleMetricResultBuilder);\n     }\n     finally {\n-      if (singleMetricParam != null) {\n-        singleMetricAlgo.cleanup(singleMetricParam);\n-      }\n+      singleMetricAlgo.cleanup(singleMetricParam);\n     }\n \n     PooledTopNAlgorithm allMetricAlgo = new PooledTopNAlgorithm(capabilities, query, bufferPool);\n@@ -118,9 +116,7 @@ public void run(\n       );\n     }\n     finally {\n-      if (allMetricsParam != null) {\n-        allMetricAlgo.cleanup(allMetricsParam);\n-      }\n+      allMetricAlgo.cleanup(allMetricsParam);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/main/java/io/druid/query/topn/AggregateTopNMetricFirstAlgorithm.java",
                "sha": "f892e07d83967d0e89e26ad3f7ab2b08b538c28f",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java?ref=7c4054aaa3464ab2add5c1fcf7e318b9bff0f123",
                "deletions": 4,
                "filename": "processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "patch": "@@ -311,12 +311,14 @@ protected void closeAggregators(BufferAggregator[] bufferAggregators)\n   @Override\n   public void cleanup(PooledTopNParams params)\n   {\n-    ResourceHolder<ByteBuffer> resultsBufHolder = params.getResultsBufHolder();\n+    if (params != null) {\n+      ResourceHolder<ByteBuffer> resultsBufHolder = params.getResultsBufHolder();\n \n-    if (resultsBufHolder != null) {\n-      resultsBufHolder.get().clear();\n+      if (resultsBufHolder != null) {\n+        resultsBufHolder.get().clear();\n+      }\n+      CloseQuietly.close(resultsBufHolder);\n     }\n-    CloseQuietly.close(resultsBufHolder);\n   }\n \n   public static class PooledTopNParams extends TopNParams",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/main/java/io/druid/query/topn/PooledTopNAlgorithm.java",
                "sha": "bc6a61578092eac261333fad54e26cf94c024dc9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java?ref=7c4054aaa3464ab2add5c1fcf7e318b9bff0f123",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Druid - a distributed column store.\n+ * Copyright 2012 - 2015 Metamarkets Group Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.druid.query.topn;\n+\n+import io.druid.collections.ResourceHolder;\n+import io.druid.segment.Capabilities;\n+import org.easymock.EasyMock;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class PooledTopNAlgorithmTest\n+{\n+  @Test\n+  public void testCleanupWithNullParams()\n+  {\n+    PooledTopNAlgorithm pooledTopNAlgorithm = new PooledTopNAlgorithm(Capabilities.builder().build(), null, null);\n+    pooledTopNAlgorithm.cleanup(null);\n+  }\n+\n+  @Test\n+  public void cleanup() throws IOException\n+  {\n+    PooledTopNAlgorithm pooledTopNAlgorithm = new PooledTopNAlgorithm(Capabilities.builder().build(), null, null);\n+    PooledTopNAlgorithm.PooledTopNParams params = EasyMock.createMock(PooledTopNAlgorithm.PooledTopNParams.class);\n+    ResourceHolder<ByteBuffer> resourceHolder = EasyMock.createMock(ResourceHolder.class);\n+    EasyMock.expect(params.getResultsBufHolder()).andReturn(resourceHolder).times(1);\n+    EasyMock.expect(resourceHolder.get()).andReturn(ByteBuffer.allocate(1)).times(1);\n+    resourceHolder.close();\n+    EasyMock.expectLastCall().once();\n+    EasyMock.replay(params);\n+    EasyMock.replay(resourceHolder);\n+    pooledTopNAlgorithm.cleanup(params);\n+    EasyMock.verify(params);\n+    EasyMock.verify(resourceHolder);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7c4054aaa3464ab2add5c1fcf7e318b9bff0f123/processing/src/test/java/io/druid/query/topn/PooledTopNAlgorithmTest.java",
                "sha": "0e2888094d64f0e4fdc37e293ba31f5445ddad08",
                "status": "added"
            }
        ],
        "message": "Merge pull request #1343 from metamx/fix-npe\n\nfix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/7608bf5799e9ab4f01d28a30ec08bad9a4b83731",
        "repo": "incubator-druid",
        "unit_tests": [
            "PooledTopNAlgorithmTest.java"
        ]
    },
    "incubator-druid_7ce05d5": {
        "bug_id": "incubator-druid_7ce05d5",
        "commit": "https://github.com/apache/incubator-druid/commit/7ce05d58bc96aa447310bd5b75aad9b6e4616ec1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7ce05d58bc96aa447310bd5b75aad9b6e4616ec1/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java?ref=7ce05d58bc96aa447310bd5b75aad9b6e4616ec1",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "patch": "@@ -20,6 +20,7 @@\n package io.druid.query.search;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Strings;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n@@ -41,6 +42,7 @@\n import io.druid.segment.DimensionSelector;\n import io.druid.segment.FloatColumnSelector;\n import io.druid.segment.LongColumnSelector;\n+import io.druid.segment.NullDimensionSelector;\n import io.druid.segment.Segment;\n import io.druid.segment.column.ColumnCapabilities;\n import io.druid.segment.column.ValueType;\n@@ -126,12 +128,12 @@ public void updateSearchResultSet(\n         final Object2IntRBTreeMap<SearchHit> set\n     )\n     {\n-      if (selector != null) {\n+      if (selector != null && !(selector instanceof NullDimensionSelector)) {\n         final IndexedInts vals = selector.getRow();\n         for (int i = 0; i < vals.size(); ++i) {\n           final String dimVal = selector.lookupName(vals.get(i));\n           if (searchQuerySpec.accept(dimVal)) {\n-            set.addTo(new SearchHit(outputName, dimVal), 1);\n+            set.addTo(new SearchHit(outputName, Strings.nullToEmpty(dimVal)), 1);\n             if (set.size() >= limit) {\n               return;\n             }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7ce05d58bc96aa447310bd5b75aad9b6e4616ec1/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "sha": "cde2e4567dbccd77a5c699065620104d5c2c3849",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7ce05d58bc96aa447310bd5b75aad9b6e4616ec1/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java?ref=7ce05d58bc96aa447310bd5b75aad9b6e4616ec1",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "patch": "@@ -22,15 +22,19 @@\n import com.google.common.base.Suppliers;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Lists;\n+import io.druid.data.input.MapBasedInputRow;\n+import io.druid.granularity.QueryGranularities;\n import io.druid.java.util.common.guava.Sequence;\n import io.druid.java.util.common.guava.Sequences;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.js.JavaScriptConfig;\n import io.druid.query.Druids;\n import io.druid.query.Query;\n import io.druid.query.QueryRunner;\n+import io.druid.query.QueryRunnerFactory;\n import io.druid.query.QueryRunnerTestHelper;\n import io.druid.query.Result;\n+import io.druid.query.aggregation.Aggregator;\n import io.druid.query.dimension.DefaultDimensionSpec;\n import io.druid.query.dimension.ExtractionDimensionSpec;\n import io.druid.query.extraction.ExtractionFn;\n@@ -49,9 +53,14 @@\n import io.druid.query.search.search.SearchQueryConfig;\n import io.druid.query.search.search.SearchSortSpec;\n import io.druid.query.spec.MultipleIntervalSegmentSpec;\n+import io.druid.segment.QueryableIndexSegment;\n import io.druid.segment.TestHelper;\n+import io.druid.segment.TestIndex;\n import io.druid.segment.column.Column;\n import io.druid.segment.column.ValueType;\n+import io.druid.segment.incremental.IncrementalIndex;\n+import io.druid.segment.incremental.IncrementalIndexSchema;\n+import io.druid.segment.incremental.OnheapIncrementalIndex;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n import org.junit.Assert;\n@@ -732,6 +741,70 @@ public void testSearchOnFloatColumnWithExFn()\n     checkSearchQuery(searchQuery, expectedHits);\n   }\n \n+  @Test\n+  public void testSearchWithNullValueInDimension() throws Exception\n+  {\n+    IncrementalIndex<Aggregator> index = new OnheapIncrementalIndex(\n+        new IncrementalIndexSchema.Builder()\n+            .withQueryGranularity(QueryGranularities.NONE)\n+            .withMinTimestamp(new DateTime(\"2011-01-12T00:00:00.000Z\").getMillis()).build(),\n+        true,\n+        10\n+    );\n+    index.add(\n+        new MapBasedInputRow(\n+            1481871600000L,\n+            Arrays.asList(\"name\", \"host\"),\n+            ImmutableMap.<String, Object>of(\"name\", \"name1\", \"host\", \"host\")\n+        )\n+    );\n+    index.add(\n+        new MapBasedInputRow(\n+            1481871670000L,\n+            Arrays.asList(\"name\", \"table\"),\n+            ImmutableMap.<String, Object>of(\"name\", \"name2\", \"table\", \"table\")\n+        )\n+    );\n+\n+    SearchQuery searchQuery = Druids.newSearchQueryBuilder()\n+                                    .dimensions(\n+                                        new DefaultDimensionSpec(\"table\", \"table\")\n+                                    )\n+                                    .dataSource(QueryRunnerTestHelper.dataSource)\n+                                    .granularity(QueryRunnerTestHelper.allGran)\n+                                    .intervals(QueryRunnerTestHelper.fullOnInterval)\n+                                    // simulate when cardinality is big enough to fallback to cursorOnly strategy\n+                                    .context(ImmutableMap.<String, Object>of(\"searchStrategy\", \"cursorOnly\"))\n+                                    .build();\n+\n+    QueryRunnerFactory factory = new SearchQueryRunnerFactory(\n+        selector,\n+        toolChest,\n+        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n+    );\n+    QueryRunner runner = factory.createRunner(new QueryableIndexSegment(\"asdf\", TestIndex.persistRealtimeAndLoadMMapped(index)));\n+    List<SearchHit> expectedHits = Lists.newLinkedList();\n+    expectedHits.add(new SearchHit(\"table\", \"table\", 1));\n+    expectedHits.add(new SearchHit(\"table\", \"\", 1));\n+    checkSearchQuery(searchQuery, runner, expectedHits);\n+  }\n+\n+  @Test\n+  public void testSearchWithNotExistedDimension() throws Exception\n+  {\n+    SearchQuery searchQuery = Druids.newSearchQueryBuilder()\n+                                    .dimensions(\n+                                        new DefaultDimensionSpec(\"asdf\", \"asdf\")\n+                                    )\n+                                    .dataSource(QueryRunnerTestHelper.dataSource)\n+                                    .granularity(QueryRunnerTestHelper.allGran)\n+                                    .intervals(QueryRunnerTestHelper.fullOnInterval)\n+                                    .build();\n+\n+    List<SearchHit> noHit = Lists.newLinkedList();\n+    checkSearchQuery(searchQuery, noHit);\n+  }\n+\n   private void checkSearchQuery(Query searchQuery, List<SearchHit> expectedResults)\n   {\n     checkSearchQuery(searchQuery, runner, expectedResults);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7ce05d58bc96aa447310bd5b75aad9b6e4616ec1/processing/src/test/java/io/druid/query/search/SearchQueryRunnerTest.java",
                "sha": "2086b339593293ca94b57dc36f62da0907522e23",
                "status": "modified"
            }
        ],
        "message": "fix NPE in search query when dimension contains null value (#3968)\n\n* fix NPE when dimension contains null value in search query\r\n\r\n* add ut\r\n\r\n* search with not existed dimension should always return empty result",
        "parent": "https://github.com/apache/incubator-druid/commit/ebd100cbb01e442c6d34eda7983270ba943115c2",
        "repo": "incubator-druid",
        "unit_tests": [
            "SearchQueryRunnerTest.java"
        ]
    },
    "incubator-druid_7f0319a": {
        "bug_id": "incubator-druid_7f0319a",
        "commit": "https://github.com/apache/incubator-druid/commit/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579/server/src/main/java/io/druid/server/ClientInfoResource.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/ClientInfoResource.java?ref=7f0319a8ae4dc30c2e21a3ace14384eedf3fc579",
                "deletions": 4,
                "filename": "server/src/main/java/io/druid/server/ClientInfoResource.java",
                "patch": "@@ -101,7 +101,12 @@ public ClientInfoResource(\n       @QueryParam(\"interval\") String interval\n   )\n   {\n-    List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final Set<String> dims = Sets.newHashSet();\n+\n+    if (segments == null || segments.isEmpty()) {\n+      return dims;\n+    }\n \n     Interval theInterval;\n     if (interval == null || interval.isEmpty()) {\n@@ -111,7 +116,6 @@ public ClientInfoResource(\n       theInterval = new Interval(interval);\n     }\n \n-    Set<String> dims = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n       if (theInterval.overlaps(segment.getInterval())) {\n         dims.addAll(segment.getDimensions());\n@@ -129,8 +133,13 @@ public ClientInfoResource(\n       @QueryParam(\"interval\") String interval\n   )\n   {\n-    List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n+    final Set<String> metrics = Sets.newHashSet();\n \n+    if (segments == null || segments.isEmpty()) {\n+      return metrics;\n+    }\n+    \n     Interval theInterval;\n     if (interval == null || interval.isEmpty()) {\n       DateTime now = new DateTime();\n@@ -139,7 +148,6 @@ public ClientInfoResource(\n       theInterval = new Interval(interval);\n     }\n \n-    Set<String> metrics = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n       if (theInterval.overlaps(segment.getInterval())) {\n         metrics.addAll(segment.getMetrics());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/7f0319a8ae4dc30c2e21a3ace14384eedf3fc579/server/src/main/java/io/druid/server/ClientInfoResource.java",
                "sha": "cbd253525bb44c40fa9bde38c7d3e8c8d88b383f",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/174ff66a880e2381bd2c93735265242e3c958b98",
        "repo": "incubator-druid",
        "unit_tests": [
            "ClientInfoResourceTest.java"
        ]
    },
    "incubator-druid_8172bdf": {
        "bug_id": "incubator-druid_8172bdf",
        "commit": "https://github.com/apache/incubator-druid/commit/8172bdff1cf88861270c1961e64cb929906c98df",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8172bdff1cf88861270c1961e64cb929906c98df/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java?ref=8172bdff1cf88861270c1961e64cb929906c98df",
                "deletions": 18,
                "filename": "server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "patch": "@@ -165,16 +165,15 @@ public Boolean withHandle(Handle handle) throws Exception\n           @Override\n           public Optional<EntryType> withHandle(Handle handle) throws Exception\n           {\n+            byte[] res = handle.createQuery(\n+                String.format(\"SELECT payload FROM %s WHERE id = :id\", entryTable)\n+            )\n+                               .bind(\"id\", entryId)\n+                               .map(ByteArrayMapper.FIRST)\n+                               .first();\n+\n             return Optional.fromNullable(\n-                jsonMapper.<EntryType>readValue(\n-                    handle.createQuery(\n-                        String.format(\"SELECT payload FROM %s WHERE id = :id\", entryTable)\n-                    )\n-                          .bind(\"id\", entryId)\n-                          .map(ByteArrayMapper.FIRST)\n-                          .first(),\n-                    entryType\n-                )\n+                res == null ? null : jsonMapper.<EntryType>readValue(res, entryType)\n             );\n           }\n         }\n@@ -190,16 +189,15 @@ public Boolean withHandle(Handle handle) throws Exception\n           @Override\n           public Optional<StatusType> withHandle(Handle handle) throws Exception\n           {\n+            byte[] res = handle.createQuery(\n+                String.format(\"SELECT status_payload FROM %s WHERE id = :id\", entryTable)\n+            )\n+                               .bind(\"id\", entryId)\n+                               .map(ByteArrayMapper.FIRST)\n+                               .first();\n+\n             return Optional.fromNullable(\n-                jsonMapper.<StatusType>readValue(\n-                    handle.createQuery(\n-                        String.format(\"SELECT status_payload FROM %s WHERE id = :id\", entryTable)\n-                    )\n-                          .bind(\"id\", entryId)\n-                          .map(ByteArrayMapper.FIRST)\n-                          .first(),\n-                    statusType\n-                )\n+                res == null ? null : jsonMapper.<StatusType>readValue(res, statusType)\n             );\n           }\n         }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8172bdff1cf88861270c1961e64cb929906c98df/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "sha": "271495790c6d9022c93332db1fb0c61924fdfe19",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8172bdff1cf88861270c1961e64cb929906c98df/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java?ref=8172bdff1cf88861270c1961e64cb929906c98df",
                "deletions": 0,
                "filename": "server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "patch": "@@ -118,8 +118,12 @@ public void testEntryAndStatus() throws Exception\n         handler.getEntry(entryId)\n     );\n \n+    Assert.assertEquals(Optional.absent(), handler.getEntry(\"non_exist_entry\"));\n+\n     Assert.assertEquals(Optional.absent(), handler.getStatus(entryId));\n \n+    Assert.assertEquals(Optional.absent(), handler.getStatus(\"non_exist_entry\"));\n+\n     Assert.assertTrue(handler.setStatus(entryId, true, status1));\n \n     Assert.assertEquals(\n@@ -179,6 +183,11 @@ public void testLogs() throws Exception\n \n     handler.insert(entryId, new DateTime(\"2014-01-01\"), \"test\", entry, true, status);\n \n+    Assert.assertEquals(\n+        ImmutableList.of(),\n+        handler.getLogs(\"non_exist_entry\")\n+    );\n+\n     Assert.assertEquals(\n         ImmutableMap.of(),\n         handler.getLocks(entryId)\n@@ -206,6 +215,11 @@ public void testLocks() throws Exception\n \n     handler.insert(entryId, new DateTime(\"2014-01-01\"), \"test\", entry, true, status);\n \n+    Assert.assertEquals(\n+        ImmutableMap.<Long, Map<String, Integer>>of(),\n+        handler.getLocks(\"non_exist_entry\")\n+    );\n+\n     Assert.assertEquals(\n         ImmutableMap.<Long, Map<String, Integer>>of(),\n         handler.getLocks(entryId)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8172bdff1cf88861270c1961e64cb929906c98df/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "sha": "e9f0788184990e8909bc74f8d07cadd48220888c",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1468 from guobingkun/fix_npe_for_storage_action_handler\n\nFix npe thrown from SQLMetadataStorageActionHandler",
        "parent": "https://github.com/apache/incubator-druid/commit/f883ff2dabbd88e06a7e03c4b4750df5a642a98c",
        "repo": "incubator-druid",
        "unit_tests": [
            "SQLMetadataStorageActionHandlerTest.java"
        ]
    },
    "incubator-druid_8799d46": {
        "bug_id": "incubator-druid_8799d46",
        "commit": "https://github.com/apache/incubator-druid/commit/8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java?ref=8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
                "deletions": 1,
                "filename": "api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "patch": "@@ -28,7 +28,6 @@\n   public static final long DEFAULT_MAX_CACHE_CAPACITY_BYTES = 1024 * 1024 * 1024; // 1GB\n   public static final long DEFAULT_MAX_FETCH_CAPACITY_BYTES = 1024 * 1024 * 1024; // 1GB\n   public static final long DEFAULT_FETCH_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(60);\n-  public static final int DEFAULT_MAX_FETCH_RETRY = 3;\n \n   // A roughly max size of total fetched objects, but the actual fetched size can be bigger. The reason is our current\n   // client implementations for cloud storages like s3 don't support range scan yet, so we must download the whole file",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchConfig.java",
                "sha": "a1c5fe47a76ccb46c298377bb674c756690166d5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java?ref=8799d46fe9fcbaf47a6b37396ae5af08fa97f930",
                "deletions": 1,
                "filename": "api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "patch": "@@ -91,6 +91,8 @@\n {\n   private static final Logger LOG = new Logger(PrefetchableTextFilesFirehoseFactory.class);\n \n+  public static final int DEFAULT_MAX_FETCH_RETRY = 3;\n+\n   private final CacheManager<T> cacheManager;\n   private final PrefetchConfig prefetchConfig;\n \n@@ -114,7 +116,7 @@ public PrefetchableTextFilesFirehoseFactory(\n     this.cacheManager = new CacheManager<>(\n         prefetchConfig.getMaxCacheCapacityBytes()\n     );\n-    this.maxFetchRetry = maxFetchRetry;\n+    this.maxFetchRetry = maxFetchRetry == null ? DEFAULT_MAX_FETCH_RETRY : maxFetchRetry;\n   }\n \n   @JsonProperty",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8799d46fe9fcbaf47a6b37396ae5af08fa97f930/api/src/main/java/io/druid/data/input/impl/prefetch/PrefetchableTextFilesFirehoseFactory.java",
                "sha": "370262c05937c650b21cc67af9b31dccabe36403",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in PrefetchableTextFilesFirehoseFactory (#5802)",
        "parent": "https://github.com/apache/incubator-druid/commit/bc0ff251a3dcdd3284e92441778805c9d3ac8201",
        "repo": "incubator-druid",
        "unit_tests": [
            "PrefetchableTextFilesFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_8fd39c6": {
        "bug_id": "incubator-druid_8fd39c6",
        "commit": "https://github.com/apache/incubator-druid/commit/8fd39c63d5df8053567c867a8edd0a027c70f2f5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -74,7 +74,7 @@\n             <dependency>\n                 <groupId>com.metamx</groupId>\n                 <artifactId>emitter</artifactId>\n-                <version>0.2.10</version>\n+                <version>0.2.11</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.metamx</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/pom.xml",
                "sha": "c0c9f69a4b0d6a35510b353467ad9dfe14215e95",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -35,8 +35,10 @@\n import java.util.Arrays;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n /**\n@@ -110,7 +112,11 @@ public ChainedExecutionQueryRunner(\n                                   if (input == null) {\n                                     throw new ISE(\"Input is null?! How is this possible?!\");\n                                   }\n-                                  return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                  Sequence<T> result = input.run(query);\n+                                  if (result == null) {\n+                                    throw new ISE(\"Got a null result! Segments are missing!\");\n+                                  }\n+                                  return Sequences.toList(result, Lists.<T>newArrayList());\n                                 }\n                                 catch (Exception e) {\n                                   log.error(e, \"Exception with one of the sequences!\");\n@@ -156,4 +162,20 @@ public void cleanup(Iterator<T> tIterator)\n         }\n     );\n   }\n+\n+  public static void main(String[] args) throws Exception\n+  {\n+    ExecutorService foo = Executors.newFixedThreadPool(1);\n+    Future test = foo.submit(\n+        new Callable<List>()\n+        {\n+          @Override\n+          public List call() throws Exception\n+          {\n+            throw new ISE(\"\");\n+          }\n+        }\n+    );\n+    System.out.println(Lists.newArrayList(test));\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "f849768d6719b3805a2d10758cf97ebdf91f5d91",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 10,
                "filename": "processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "patch": "@@ -70,7 +70,7 @@\n   private final StupidPool<ByteBuffer> intermediateResultsBufferPool;\n \n   @Inject\n-  public GroupByQueryEngine (\n+  public GroupByQueryEngine(\n       Supplier<GroupByQueryConfig> config,\n       @Global StupidPool<ByteBuffer> intermediateResultsBufferPool\n   )\n@@ -81,6 +81,12 @@ public GroupByQueryEngine (\n \n   public Sequence<Row> process(final GroupByQuery query, StorageAdapter storageAdapter)\n   {\n+    if (storageAdapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> intervals = query.getQuerySegmentSpec().getIntervals();\n     if (intervals.size() != 1) {\n       throw new IAE(\"Should only have one interval, got[%s]\", intervals);\n@@ -187,8 +193,7 @@ public int getNumRows()\n           ByteBuffer newKey = key.duplicate();\n           newKey.putInt(dimSelector.getValueCardinality());\n           unaggregatedBuffers = updateValues(newKey, dims.subList(1, dims.size()));\n-        }\n-        else {\n+        } else {\n           for (Integer dimValue : row) {\n             ByteBuffer newKey = key.duplicate();\n             newKey.putInt(dimValue);\n@@ -202,8 +207,7 @@ public int getNumRows()\n           retVal.addAll(unaggregatedBuffers);\n         }\n         return retVal;\n-      }\n-      else {\n+      } else {\n         key.clear();\n         Integer position = positions.get(key);\n         int[] increments = positionMaintainer.getIncrements();\n@@ -267,8 +271,7 @@ public Integer getNext()\n     {\n       if (nextVal > max) {\n         return null;\n-      }\n-      else {\n+      } else {\n         int retVal = (int) nextVal;\n         nextVal += increment;\n         return retVal;\n@@ -402,7 +405,7 @@ public Row apply(@Nullable Map.Entry<ByteBuffer, Integer> input)\n                     final DimExtractionFn fn = dimensionSpecs.get(i).getDimExtractionFn();\n                     final int dimVal = keyBuffer.getInt();\n                     if (dimSelector.getValueCardinality() != dimVal) {\n-                      if(fn != null) {\n+                      if (fn != null) {\n                         theEvent.put(dimNames.get(i), fn.apply(dimSelector.lookupName(dimVal)));\n                       } else {\n                         theEvent.put(dimNames.get(i), dimSelector.lookupName(dimVal));\n@@ -434,9 +437,10 @@ public void remove()\n       throw new UnsupportedOperationException();\n     }\n \n-    public void close() {\n+    public void close()\n+    {\n       // cleanup\n-      for(BufferAggregator agg : aggregators) {\n+      for (BufferAggregator agg : aggregators) {\n         agg.close();\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "sha": "1c75f1390f17561fc5df2d2a388dff3fce03b5ab",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 35,
                "filename": "processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "patch": "@@ -104,47 +104,47 @@\n   )\n   {\n     return new ConcatQueryRunner<SegmentAnalysis>(\n-            Sequences.map(\n-                Sequences.simple(queryRunners),\n-                new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+        Sequences.map(\n+            Sequences.simple(queryRunners),\n+            new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+            {\n+              @Override\n+              public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+              {\n+                return new QueryRunner<SegmentAnalysis>()\n                 {\n                   @Override\n-                  public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+                  public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n                   {\n-                    return new QueryRunner<SegmentAnalysis>()\n-                    {\n-                      @Override\n-                      public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n-                      {\n \n-                        Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n-                            new Callable<Sequence<SegmentAnalysis>>()\n-                            {\n-                              @Override\n-                              public Sequence<SegmentAnalysis> call() throws Exception\n-                              {\n-                                return new ExecutorExecutingSequence<SegmentAnalysis>(\n-                                    input.run(query),\n-                                    queryExecutor\n-                                );\n-                              }\n-                            }\n-                        );\n-                        try {\n-                          return future.get();\n+                    Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n+                        new Callable<Sequence<SegmentAnalysis>>()\n+                        {\n+                          @Override\n+                          public Sequence<SegmentAnalysis> call() throws Exception\n+                          {\n+                            return new ExecutorExecutingSequence<SegmentAnalysis>(\n+                                input.run(query),\n+                                queryExecutor\n+                            );\n+                          }\n                         }\n-                        catch (InterruptedException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                        catch (ExecutionException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                      }\n-                    };\n+                    );\n+                    try {\n+                      return future.get();\n+                    }\n+                    catch (InterruptedException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n+                    catch (ExecutionException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n                   }\n-                }\n-            )\n-        );\n+                };\n+              }\n+            }\n+        )\n+    );\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "sha": "c5b64c2d46ac1af275c5a42451015e56b30b5ca5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "patch": "@@ -32,7 +32,6 @@\n \n public class SegmentMetadataQuery extends BaseQuery<SegmentAnalysis>\n {\n-\n   private final ColumnIncluderator toInclude;\n   private final boolean merge;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "sha": "c6d6ecca0bde4d22a13a50f3b2ebdaccb84a3ffa",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 38,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "patch": "@@ -55,7 +55,7 @@\n import java.util.TreeSet;\n \n /**\n-*/\n+ */\n public class SearchQueryRunner implements QueryRunner<Result<SearchResultValue>>\n {\n   private static final EmittingLogger log = new EmittingLogger(SearchQueryRunner.class);\n@@ -99,12 +99,10 @@ public SearchQueryRunner(Segment segment)\n           ConciseSet set = new ConciseSet();\n           set.add(0);\n           baseFilter = ImmutableConciseSet.newImmutableFromMutable(set);\n-        }\n-        else {\n+        } else {\n           baseFilter = ImmutableConciseSet.complement(new ImmutableConciseSet(), index.getNumRows());\n         }\n-      }\n-      else {\n+      } else {\n         baseFilter = filter.goConcise(new ColumnSelectorBitmapIndexSelector(index));\n       }\n \n@@ -133,49 +131,52 @@ public SearchQueryRunner(Segment segment)\n     }\n \n     final StorageAdapter adapter = segment.asStorageAdapter();\n-    if (adapter != null) {\n-      Iterable<String> dimsToSearch;\n-      if (dimensions == null || dimensions.isEmpty()) {\n-        dimsToSearch = adapter.getAvailableDimensions();\n-      } else {\n-        dimsToSearch = dimensions;\n-      }\n \n-      final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n+    if (adapter == null) {\n+      log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n+         .addData(\"segment\", segment.getIdentifier())\n+         .addData(\"query\", query).emit();\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n \n-      final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n-      for (Cursor cursor : cursors) {\n-        Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n-        for (String dim : dimsToSearch) {\n-          dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n-        }\n+    Iterable<String> dimsToSearch;\n+    if (dimensions == null || dimensions.isEmpty()) {\n+      dimsToSearch = adapter.getAvailableDimensions();\n+    } else {\n+      dimsToSearch = dimensions;\n+    }\n+\n+    final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n \n-        while (!cursor.isDone()) {\n-          for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n-            final DimensionSelector selector = entry.getValue();\n-            final IndexedInts vals = selector.getRow();\n-            for (int i = 0; i < vals.size(); ++i) {\n-              final String dimVal = selector.lookupName(vals.get(i));\n-              if (searchQuerySpec.accept(dimVal)) {\n-                retVal.add(new SearchHit(entry.getKey(), dimVal));\n-                if (retVal.size() >= limit) {\n-                  return makeReturnResult(limit, retVal);\n-                }\n+    final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n+    for (Cursor cursor : cursors) {\n+      Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n+      for (String dim : dimsToSearch) {\n+        dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n+      }\n+\n+      while (!cursor.isDone()) {\n+        for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n+          final DimensionSelector selector = entry.getValue();\n+          final IndexedInts vals = selector.getRow();\n+          for (int i = 0; i < vals.size(); ++i) {\n+            final String dimVal = selector.lookupName(vals.get(i));\n+            if (searchQuerySpec.accept(dimVal)) {\n+              retVal.add(new SearchHit(entry.getKey(), dimVal));\n+              if (retVal.size() >= limit) {\n+                return makeReturnResult(limit, retVal);\n               }\n             }\n           }\n-\n-          cursor.advance();\n         }\n-      }\n \n-      return makeReturnResult(limit, retVal);\n+        cursor.advance();\n+      }\n     }\n \n-    log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n-       .addData(\"segment\", segment.getIdentifier())\n-       .addData(\"query\", query);\n-    return Sequences.empty();\n+    return makeReturnResult(limit, retVal);\n   }\n \n   private Sequence<Result<SearchResultValue>> makeReturnResult(int limit, TreeSet<SearchHit> retVal)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "sha": "4070a274b4fe374a6d2e582e0d8c4631beb43050",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Function;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -54,6 +55,12 @@\n           {\n             final StorageAdapter adapter = segment.asStorageAdapter();\n \n+            if (adapter == null) {\n+              throw new ISE(\n+                  \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+              );\n+            }\n+\n             final Iterable<String> dims;\n             if (query.getDimensions() == null || query.getDimensions().isEmpty()) {\n               dims = adapter.getAvailableDimensions();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "sha": "f5bc7aba043c425bffb6d40885d082a07136ffcd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "patch": "@@ -87,6 +87,12 @@ public TimeBoundaryQueryRunner(Segment segment)\n             @Override\n             public Iterator<Result<TimeBoundaryResultValue>> make()\n             {\n+              if (adapter == null) {\n+                throw new ISE(\n+                    \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+                );\n+              }\n+\n               return legacyQuery.buildResult(\n                   adapter.getInterval().getStart(),\n                   adapter.getMinTime(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "sha": "16e9ae832fae74154c2ad1380350931455320343",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "patch": "@@ -20,6 +20,7 @@\n package io.druid.query.timeseries;\n \n import com.google.common.base.Function;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -40,6 +41,12 @@\n {\n   public Sequence<Result<TimeseriesResultValue>> process(final TimeseriesQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     return new BaseSequence<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>(\n         new BaseSequence.IteratorMaker<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>()\n         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "sha": "6ab42477890e9791a42cc60e4066be26f02d033e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java?ref=8fd39c63d5df8053567c867a8edd0a027c70f2f5",
                "deletions": 12,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "patch": "@@ -21,7 +21,7 @@\n \n import com.google.common.base.Function;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.FunctionalIterable;\n import com.metamx.common.logger.Logger;\n import io.druid.collections.StupidPool;\n@@ -53,6 +53,12 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n \n   public Iterable<Result<TopNResultValue>> query(final TopNQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> queryIntervals = query.getQuerySegmentSpec().getIntervals();\n     final Filter filter = Filters.convertDimensionFilters(query.getDimensionsFilter());\n     final QueryGranularity granularity = query.getGranularity();\n@@ -62,10 +68,6 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n         queryIntervals.size() == 1, \"Can only handle a single interval, got[%s]\", queryIntervals\n     );\n \n-    if (mapFn == null) {\n-      return Lists.newArrayList();\n-    }\n-\n     return FunctionalIterable\n         .create(adapter.makeCursors(filter, queryIntervals.get(0), granularity))\n         .transform(\n@@ -84,13 +86,6 @@ public Cursor apply(Cursor input)\n \n   private Function<Cursor, Result<TopNResultValue>> getMapFn(TopNQuery query, final StorageAdapter adapter)\n   {\n-    if (adapter == null) {\n-      log.warn(\n-          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped. Returning empty results.\"\n-      );\n-      return null;\n-    }\n-\n     final Capabilities capabilities = adapter.getCapabilities();\n     final int cardinality = adapter.getDimensionCardinality(query.getDimensionSpec().getDimension());\n     int numBytesPerRecord = 0;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/8fd39c63d5df8053567c867a8edd0a027c70f2f5/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "sha": "1f3a8892733b2f3f2fc9a84a5eb92bade1224721",
                "status": "modified"
            }
        ],
        "message": "Better handle null adapters and NPEs in the CQE",
        "parent": "https://github.com/apache/incubator-druid/commit/560045ddd2a37ed96703d31623e059248c62b944",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java",
            "SegmentMetadataQueryTest.java",
            "SearchQueryRunnerTest.java"
        ]
    },
    "incubator-druid_9199d61": {
        "bug_id": "incubator-druid_9199d61",
        "commit": "https://github.com/apache/incubator-druid/commit/9199d61389bf215e41387908372068a6d4da46c2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskLocation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/indexer/TaskLocation.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "api/src/main/java/io/druid/indexer/TaskLocation.java",
                "patch": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package io.druid.indexing.common;\n+package io.druid.indexer;\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;",
                "previous_filename": "indexing-service/src/main/java/io/druid/indexing/common/TaskLocation.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskLocation.java",
                "sha": "70fd0b2e3ceac2c627ce411e975b7c48136ecea6",
                "status": "renamed"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskState.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/indexer/TaskState.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "api/src/main/java/io/druid/indexer/TaskState.java",
                "patch": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.indexer;\n+\n+public enum TaskState\n+{\n+  RUNNING,\n+  SUCCESS,\n+  FAILED;\n+\n+  public boolean isRunnable()\n+  {\n+    return this == RUNNING;\n+  }\n+\n+  public boolean isComplete()\n+  {\n+    return this != RUNNING;\n+  }\n+\n+  public boolean isSuccess()\n+  {\n+    return this == SUCCESS;\n+  }\n+\n+  public boolean isFailure()\n+  {\n+    return this == FAILED;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskState.java",
                "sha": "c4f54a1f716f468c0f97f6ef4a8439003202c6e0",
                "status": "added"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskStatusPlus.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/indexer/TaskStatusPlus.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "api/src/main/java/io/druid/indexer/TaskStatusPlus.java",
                "patch": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.indexer;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.base.Preconditions;\n+import org.joda.time.DateTime;\n+\n+import javax.annotation.Nullable;\n+\n+public class TaskStatusPlus\n+{\n+  private final String id;\n+  private final DateTime createdTime;\n+  private final DateTime queueInsertionTime;\n+  private final TaskState state;\n+  private final Long duration;\n+  private final TaskLocation location;\n+\n+  @JsonCreator\n+  public TaskStatusPlus(\n+      @JsonProperty(\"id\") String id,\n+      @JsonProperty(\"createdTime\") DateTime createdTime,\n+      @JsonProperty(\"queueInsertionTime\") DateTime queueInsertionTime,\n+      @JsonProperty(\"state\") @Nullable TaskState state,\n+      @JsonProperty(\"duration\") @Nullable Long duration,\n+      @JsonProperty(\"location\") TaskLocation location\n+  )\n+  {\n+    if (state != null && state.isComplete()) {\n+      Preconditions.checkNotNull(duration, \"duration\");\n+    }\n+    this.id = Preconditions.checkNotNull(id, \"id\");\n+    this.createdTime = Preconditions.checkNotNull(createdTime, \"createdTime\");\n+    this.queueInsertionTime = Preconditions.checkNotNull(queueInsertionTime, \"queueInsertionTime\");\n+    this.state = state;\n+    this.duration = duration;\n+    this.location = Preconditions.checkNotNull(location, \"location\");\n+  }\n+\n+  @JsonProperty\n+  public String getId()\n+  {\n+    return id;\n+  }\n+\n+  @JsonProperty\n+  public DateTime getCreatedTime()\n+  {\n+    return createdTime;\n+  }\n+\n+  @JsonProperty\n+  public DateTime getQueueInsertionTime()\n+  {\n+    return queueInsertionTime;\n+  }\n+\n+  @JsonProperty\n+  public TaskState getState()\n+  {\n+    return state;\n+  }\n+\n+  @JsonProperty\n+  public Long getDuration()\n+  {\n+    return duration;\n+  }\n+\n+  @JsonProperty\n+  public TaskLocation getLocation()\n+  {\n+    return location;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/api/src/main/java/io/druid/indexer/TaskStatusPlus.java",
                "sha": "a45a9a7865a9ac92d58f58353f6c120b0bedba38",
                "status": "added"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/common/src/main/java/io/druid/metadata/MetadataStorageActionHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/common/src/main/java/io/druid/metadata/MetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "common/src/main/java/io/druid/metadata/MetadataStorageActionHandler.java",
                "patch": "@@ -85,13 +85,27 @@ void insert(\n    */\n   List<Pair<EntryType, StatusType>> getActiveEntriesWithStatus();\n \n+  default List<StatusType> getInactiveStatusesSince(DateTime timestamp)\n+  {\n+    return getInactiveStatusesSince(timestamp, null);\n+  }\n+\n   /**\n-   * Return all statuses for inactive entries created on or later than the given timestamp\n+   * Return up to {@code maxNumStatuses} statuses for inactive entries created on or later than the given timestamp\n    *\n    * @param timestamp timestamp\n+   * @param maxNumStatuses maxNumStatuses\n    * @return list of statuses\n    */\n-  List<StatusType> getInactiveStatusesSince(DateTime timestamp);\n+  List<StatusType> getInactiveStatusesSince(DateTime timestamp, @Nullable Integer maxNumStatuses);\n+\n+  /**\n+   * Return createdDate and dataSource for the given id\n+   *\n+   * @return a pair of createdDate and dataSource or null if an entry for the given id is not found\n+   */\n+  @Nullable\n+  Pair<DateTime, String> getCreatedDateAndDataSource(String entryId);\n \n   /**\n    * Add a lock to the given entry",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/common/src/main/java/io/druid/metadata/MetadataStorageActionHandler.java",
                "sha": "dc12c2d97cdcfff92c1d73ef20f7b9b3305d26a2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "patch": "@@ -26,10 +26,12 @@\n import io.druid.guice.PolyBind;\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n+import io.druid.metadata.MetadataStorageActionHandlerFactory;\n import io.druid.metadata.MetadataStorageConnector;\n import io.druid.metadata.MetadataStorageProvider;\n import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n+import io.druid.metadata.SQLServerMetadataStorageActionHandlerFactory;\n \n import java.util.List;\n \n@@ -72,5 +74,10 @@ public void configure(Binder binder)\n         .addBinding(TYPE)\n         .to(SQLServerConnector.class)\n         .in(LazySingleton.class);\n+\n+    PolyBind.optionBinder(binder, Key.get(MetadataStorageActionHandlerFactory.class))\n+            .addBinding(TYPE)\n+            .to(SQLServerMetadataStorageActionHandlerFactory.class)\n+            .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "sha": "a3839f4534d92b198871bbb51ecc8ffc908e4f98",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTaskClient.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTaskClient.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTaskClient.java",
                "patch": "@@ -38,7 +38,7 @@\n import io.druid.indexing.common.RetryPolicyConfig;\n import io.druid.indexing.common.RetryPolicyFactory;\n import io.druid.indexing.common.TaskInfoProvider;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.java.util.common.IAE;\n import io.druid.java.util.common.IOE;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTaskClient.java",
                "sha": "30a06de8c17d6bc9485a7e01d33deaacaac3ec6f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -47,7 +47,7 @@\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n import io.druid.indexing.common.TaskInfoProvider;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.common.task.TaskResource;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "d0b8ea55c241ff3e35651030097400dd0ac516c7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskClientTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskClientTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskClientTest.java",
                "patch": "@@ -32,7 +32,7 @@\n import com.metamx.http.client.response.FullResponseHandler;\n import com.metamx.http.client.response.FullResponseHolder;\n import io.druid.indexing.common.TaskInfoProvider;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.DateTimes;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskClientTest.java",
                "sha": "8d328aadbad22f2d5272cea6c55c8354bf1bb306",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 27,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "patch": "@@ -51,6 +51,7 @@\n import io.druid.discovery.DataNodeService;\n import io.druid.discovery.DruidNodeAnnouncer;\n import io.druid.discovery.LookupNodeService;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.SegmentLoaderFactory;\n import io.druid.indexing.common.TaskLock;\n import io.druid.indexing.common.TaskStatus;\n@@ -85,9 +86,9 @@\n import io.druid.java.util.common.parsers.JSONPathFieldSpec;\n import io.druid.java.util.common.parsers.JSONPathSpec;\n import io.druid.math.expr.ExprMacroTable;\n+import io.druid.metadata.DerbyMetadataStorageActionHandlerFactory;\n import io.druid.metadata.EntryExistsException;\n import io.druid.metadata.IndexerSQLMetadataStorageCoordinator;\n-import io.druid.metadata.SQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.TestDerbyConnector;\n import io.druid.query.DefaultQueryRunnerFactoryConglomerate;\n import io.druid.query.Druids;\n@@ -115,8 +116,6 @@\n import io.druid.segment.TestHelper;\n import io.druid.segment.column.DictionaryEncodedColumn;\n import io.druid.segment.indexing.DataSchema;\n-import io.druid.segment.transform.ExpressionTransform;\n-import io.druid.segment.transform.TransformSpec;\n import io.druid.segment.indexing.granularity.UniformGranularitySpec;\n import io.druid.segment.loading.DataSegmentPusher;\n import io.druid.segment.loading.LocalDataSegmentPusher;\n@@ -127,6 +126,8 @@\n import io.druid.segment.realtime.appenderator.AppenderatorImpl;\n import io.druid.segment.realtime.plumber.SegmentHandoffNotifier;\n import io.druid.segment.realtime.plumber.SegmentHandoffNotifierFactory;\n+import io.druid.segment.transform.ExpressionTransform;\n+import io.druid.segment.transform.TransformSpec;\n import io.druid.server.DruidNode;\n import io.druid.server.coordination.DataSegmentServerAnnouncer;\n import io.druid.server.coordination.ServerType;\n@@ -356,7 +357,7 @@ public void testRunAfterDataInserted() throws Exception\n     final ListenableFuture<TaskStatus> future = runTask(task);\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -410,7 +411,7 @@ public void testRunBeforeDataInserted() throws Exception\n     }\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -478,7 +479,7 @@ public void testIncrementalHandOff() throws Exception\n     Assert.assertTrue(checkpoint1.getPartitionOffsetMap().equals(currentOffsets) || checkpoint2.getPartitionOffsetMap()\n                                                                                                .equals(currentOffsets));\n     task.setEndOffsets(currentOffsets, true, false);\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     Assert.assertEquals(1, checkpointRequestsHash.size());\n     Assert.assertTrue(checkpointRequestsHash.contains(\n@@ -554,7 +555,7 @@ public void testRunWithMinimumMessageTime() throws Exception\n     }\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -608,7 +609,7 @@ public void testRunWithMaximumMessageTime() throws Exception\n     }\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -672,7 +673,7 @@ public void testRunWithTransformSpec() throws Exception\n     }\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(1, task.getFireDepartmentMetrics().processed());\n@@ -720,7 +721,7 @@ public void testRunOnNothing() throws Exception\n     final ListenableFuture<TaskStatus> future = runTask(task);\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(0, task.getFireDepartmentMetrics().processed());\n@@ -761,7 +762,7 @@ public void testHandoffConditionTimeoutWhenHandoffOccurs() throws Exception\n     final ListenableFuture<TaskStatus> future = runTask(task);\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -814,7 +815,7 @@ public void testHandoffConditionTimeoutWhenHandoffDoesNotOccur() throws Exceptio\n \n     // Wait for task to exit\n     Assert.assertEquals(\n-        isIncrementalHandoffSupported ? TaskStatus.Status.SUCCESS : TaskStatus.Status.FAILED,\n+        isIncrementalHandoffSupported ? TaskState.SUCCESS : TaskState.FAILED,\n         future.get().getStatusCode()\n     );\n \n@@ -867,7 +868,7 @@ public void testReportParseExceptions() throws Exception\n     final ListenableFuture<TaskStatus> future = runTask(task);\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.FAILED, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.FAILED, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task.getFireDepartmentMetrics().processed());\n@@ -922,8 +923,8 @@ public void testRunReplicas() throws Exception\n     }\n \n     // Wait for tasks to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future1.get().getStatusCode());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future2.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task1.getFireDepartmentMetrics().processed());\n@@ -988,11 +989,11 @@ public void testRunConflicting() throws Exception\n \n     // Run first task\n     final ListenableFuture<TaskStatus> future1 = runTask(task1);\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future1.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n \n     // Run second task\n     final ListenableFuture<TaskStatus> future2 = runTask(task2);\n-    Assert.assertEquals(TaskStatus.Status.FAILED, future2.get().getStatusCode());\n+    Assert.assertEquals(TaskState.FAILED, future2.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task1.getFireDepartmentMetrics().processed());\n@@ -1057,7 +1058,7 @@ public void testRunConflictingWithoutTransactions() throws Exception\n \n     // Run first task\n     final ListenableFuture<TaskStatus> future1 = runTask(task1);\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future1.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n \n     // Check published segments & metadata\n     SegmentDescriptor desc1 = SD(task1, \"2010/P1D\", 0);\n@@ -1067,7 +1068,7 @@ public void testRunConflictingWithoutTransactions() throws Exception\n \n     // Run second task\n     final ListenableFuture<TaskStatus> future2 = runTask(task2);\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future2.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task1.getFireDepartmentMetrics().processed());\n@@ -1119,7 +1120,7 @@ public void testRunOneTaskTwoPartitions() throws Exception\n     }\n \n     // Wait for tasks to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(5, task.getFireDepartmentMetrics().processed());\n@@ -1198,8 +1199,8 @@ public void testRunTwoTasksTwoPartitions() throws Exception\n     }\n \n     // Wait for tasks to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future1.get().getStatusCode());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future2.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(3, task1.getFireDepartmentMetrics().processed());\n@@ -1262,7 +1263,7 @@ public void testRestore() throws Exception\n     task1.stopGracefully();\n     unlockAppenderatorBasePersistDirForTask(task1);\n \n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future1.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n \n     // Start a new task\n     final KafkaIndexTask task2 = createTask(\n@@ -1290,7 +1291,7 @@ public void testRestore() throws Exception\n     }\n \n     // Wait for task to exit\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future2.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(2, task1.getFireDepartmentMetrics().processed());\n@@ -1376,7 +1377,7 @@ public void testRunWithPauseAndResume() throws Exception\n \n     task.resume();\n \n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n     Assert.assertEquals(task.getEndOffsets(), task.getCurrentOffsets());\n \n     // Check metrics\n@@ -1462,7 +1463,7 @@ public void testRunAndPauseAfterReadWithModifiedEndOffsets() throws Exception\n \n     task.resume();\n \n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     // Check metrics\n     Assert.assertEquals(4, task.getFireDepartmentMetrics().processed());\n@@ -1725,7 +1726,7 @@ private void makeToolboxFactory() throws IOException\n     taskStorage = new MetadataTaskStorage(\n         derbyConnector,\n         new TaskStorageConfig(null),\n-        new SQLMetadataStorageActionHandlerFactory(\n+        new DerbyMetadataStorageActionHandlerFactory(\n             derbyConnector,\n             derby.metadataTablesConfigSupplier().get(),\n             objectMapper",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "sha": "298d1490f88268158979a48c68d3cb46d56d6fab",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "patch": "@@ -35,7 +35,7 @@\n import io.druid.data.input.impl.StringInputRowParser;\n import io.druid.data.input.impl.TimestampSpec;\n import io.druid.indexing.common.TaskInfoProvider;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.RealtimeIndexTask;\n import io.druid.indexing.common.task.Task;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "sha": "29041e27de58e4fd1959a1fb2b3cd5891f6e7961",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "patch": "@@ -27,8 +27,10 @@\n import io.druid.guice.PolyBind;\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n+import io.druid.metadata.MetadataStorageActionHandlerFactory;\n import io.druid.metadata.MetadataStorageConnector;\n import io.druid.metadata.MetadataStorageProvider;\n+import io.druid.metadata.MySQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n \n@@ -71,5 +73,10 @@ public void configure(Binder binder)\n         .addBinding(TYPE)\n         .to(MySQLConnector.class)\n         .in(LazySingleton.class);\n+\n+    PolyBind.optionBinder(binder, Key.get(MetadataStorageActionHandlerFactory.class))\n+            .addBinding(TYPE)\n+            .to(MySQLMetadataStorageActionHandlerFactory.class)\n+            .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "sha": "211d5ca0b0e43a05bc7529abbeafccb951476be5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "patch": "@@ -27,9 +27,11 @@\n import io.druid.guice.PolyBind;\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n+import io.druid.metadata.MetadataStorageActionHandlerFactory;\n import io.druid.metadata.MetadataStorageConnector;\n import io.druid.metadata.MetadataStorageProvider;\n import io.druid.metadata.NoopMetadataStorageProvider;\n+import io.druid.metadata.PostgreSQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.SQLMetadataConnector;\n \n import java.util.List;\n@@ -72,5 +74,10 @@ public void configure(Binder binder)\n         .addBinding(TYPE)\n         .to(PostgreSQLConnector.class)\n         .in(LazySingleton.class);\n+\n+    PolyBind.optionBinder(binder, Key.get(MetadataStorageActionHandlerFactory.class))\n+            .addBinding(TYPE)\n+            .to(PostgreSQLMetadataStorageActionHandlerFactory.class)\n+            .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "sha": "8c777b7070d23b9bd4e155a4850bae1d4680358d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/TaskInfoProvider.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/TaskInfoProvider.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/TaskInfoProvider.java",
                "patch": "@@ -20,6 +20,7 @@\n package io.druid.indexing.common;\n \n import com.google.common.base.Optional;\n+import io.druid.indexer.TaskLocation;\n \n public interface TaskInfoProvider\n {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/TaskInfoProvider.java",
                "sha": "5dbe96418498b6acf65fb4c0c7c7b913f7ba0f31",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/TaskStatus.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/TaskStatus.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 18,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/TaskStatus.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Objects;\n import com.google.common.base.Preconditions;\n+import io.druid.indexer.TaskState;\n \n /**\n  * Represents the status of a task from the perspective of the coordinator. The task may be ongoing\n@@ -33,41 +34,34 @@\n  */\n public class TaskStatus\n {\n-  public enum Status\n-  {\n-    RUNNING,\n-    SUCCESS,\n-    FAILED\n-  }\n-\n   public static TaskStatus running(String taskId)\n   {\n-    return new TaskStatus(taskId, Status.RUNNING, -1);\n+    return new TaskStatus(taskId, TaskState.RUNNING, -1);\n   }\n \n   public static TaskStatus success(String taskId)\n   {\n-    return new TaskStatus(taskId, Status.SUCCESS, -1);\n+    return new TaskStatus(taskId, TaskState.SUCCESS, -1);\n   }\n \n   public static TaskStatus failure(String taskId)\n   {\n-    return new TaskStatus(taskId, Status.FAILED, -1);\n+    return new TaskStatus(taskId, TaskState.FAILED, -1);\n   }\n \n-  public static TaskStatus fromCode(String taskId, Status code)\n+  public static TaskStatus fromCode(String taskId, TaskState code)\n   {\n     return new TaskStatus(taskId, code, -1);\n   }\n \n   private final String id;\n-  private final Status status;\n+  private final TaskState status;\n   private final long duration;\n \n   @JsonCreator\n-  private TaskStatus(\n+  protected TaskStatus(\n       @JsonProperty(\"id\") String id,\n-      @JsonProperty(\"status\") Status status,\n+      @JsonProperty(\"status\") TaskState status,\n       @JsonProperty(\"duration\") long duration\n   )\n   {\n@@ -87,7 +81,7 @@ public String getId()\n   }\n \n   @JsonProperty(\"status\")\n-  public Status getStatusCode()\n+  public TaskState getStatusCode()\n   {\n     return status;\n   }\n@@ -107,7 +101,7 @@ public long getDuration()\n   @JsonIgnore\n   public boolean isRunnable()\n   {\n-    return status == Status.RUNNING;\n+    return status == TaskState.RUNNING;\n   }\n \n   /**\n@@ -130,7 +124,7 @@ public boolean isComplete()\n   @JsonIgnore\n   public boolean isSuccess()\n   {\n-    return status == Status.SUCCESS;\n+    return status == TaskState.SUCCESS;\n   }\n \n   /**\n@@ -142,7 +136,7 @@ public boolean isSuccess()\n   @JsonIgnore\n   public boolean isFailure()\n   {\n-    return status == Status.FAILED;\n+    return status == TaskState.FAILED;\n   }\n \n   public TaskStatus withDuration(long _duration)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/TaskStatus.java",
                "sha": "e3535957d58aaf9072498ea21ccd0f49cbe92541",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentAllocateAction.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentAllocateAction.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 15,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentAllocateAction.java",
                "patch": "@@ -166,13 +166,12 @@ public SegmentIdentifier perform(\n       );\n \n       final SegmentIdentifier identifier = usedSegmentsForRow.isEmpty() ?\n-                                           tryAllocateFirstSegment(toolbox, task, rowInterval, skipSegmentLineageCheck) :\n+                                           tryAllocateFirstSegment(toolbox, task, rowInterval) :\n                                            tryAllocateSubsequentSegment(\n                                                toolbox,\n                                                task,\n                                                rowInterval,\n-                                               usedSegmentsForRow.iterator().next(),\n-                                               skipSegmentLineageCheck\n+                                               usedSegmentsForRow.iterator().next()\n                                            );\n       if (identifier != null) {\n         return identifier;\n@@ -212,12 +211,8 @@ public SegmentIdentifier perform(\n     }\n   }\n \n-  private SegmentIdentifier tryAllocateFirstSegment(\n-      TaskActionToolbox toolbox,\n-      Task task,\n-      Interval rowInterval,\n-      boolean skipSegmentLineageCheck\n-  ) throws IOException\n+  private SegmentIdentifier tryAllocateFirstSegment(TaskActionToolbox toolbox, Task task, Interval rowInterval)\n+      throws IOException\n   {\n     // No existing segments for this row, but there might still be nearby ones that conflict with our preferred\n     // segment granularity. Try that first, and then progressively smaller ones if it fails.\n@@ -227,7 +222,7 @@ private SegmentIdentifier tryAllocateFirstSegment(\n                                                    .collect(Collectors.toList());\n     for (Interval tryInterval : tryIntervals) {\n       if (tryInterval.contains(rowInterval)) {\n-        final SegmentIdentifier identifier = tryAllocate(toolbox, task, tryInterval, rowInterval, false, skipSegmentLineageCheck);\n+        final SegmentIdentifier identifier = tryAllocate(toolbox, task, tryInterval, rowInterval, false);\n         if (identifier != null) {\n           return identifier;\n         }\n@@ -240,8 +235,7 @@ private SegmentIdentifier tryAllocateSubsequentSegment(\n       TaskActionToolbox toolbox,\n       Task task,\n       Interval rowInterval,\n-      DataSegment usedSegment,\n-      boolean skipSegmentLineageCheck\n+      DataSegment usedSegment\n   ) throws IOException\n   {\n     // Existing segment(s) exist for this row; use the interval of the first one.\n@@ -251,7 +245,7 @@ private SegmentIdentifier tryAllocateSubsequentSegment(\n     } else {\n       // If segment allocation failed here, it is highly likely an unrecoverable error. We log here for easier\n       // debugging.\n-      return tryAllocate(toolbox, task, usedSegment.getInterval(), rowInterval, true, skipSegmentLineageCheck);\n+      return tryAllocate(toolbox, task, usedSegment.getInterval(), rowInterval, true);\n     }\n   }\n \n@@ -260,8 +254,7 @@ private SegmentIdentifier tryAllocate(\n       Task task,\n       Interval tryInterval,\n       Interval rowInterval,\n-      boolean logOnFail,\n-      boolean skipSegmentLineageCheck\n+      boolean logOnFail\n   ) throws IOException\n   {\n     log.debug(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/common/actions/SegmentAllocateAction.java",
                "sha": "bc61d23045b830fc029da3253a3ee6fb28267daf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "patch": "@@ -44,7 +44,7 @@\n import com.metamx.emitter.EmittingLogger;\n import io.druid.java.util.common.concurrent.Execs;\n import io.druid.guice.annotations.Self;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.config.TaskConfig;\n import io.druid.indexing.common.task.Task;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "sha": "ee103420207da0085cf5810715e67c55dac69f2e",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/HeapMemoryTaskStorage.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/HeapMemoryTaskStorage.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 17,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/HeapMemoryTaskStorage.java",
                "patch": "@@ -24,7 +24,6 @@\n import com.google.common.collect.ArrayListMultimap;\n import com.google.common.collect.HashMultimap;\n import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Multimap;\n import com.google.common.collect.Ordering;\n@@ -35,13 +34,16 @@\n import io.druid.indexing.common.config.TaskStorageConfig;\n import io.druid.indexing.common.task.Task;\n import io.druid.java.util.common.DateTimes;\n+import io.druid.java.util.common.Pair;\n import io.druid.java.util.common.logger.Logger;\n import io.druid.metadata.EntryExistsException;\n import org.joda.time.DateTime;\n \n+import javax.annotation.Nullable;\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.Collectors;\n \n /**\n  * Implements an in-heap TaskStorage facility, with no persistence across restarts. This class is not\n@@ -84,7 +86,7 @@ public void insert(Task task, TaskStatus status) throws EntryExistsException\n       }\n \n       log.info(\"Inserting task %s with status: %s\", task.getId(), status);\n-      tasks.put(task.getId(), new TaskStuff(task, status, DateTimes.nowUtc()));\n+      tasks.put(task.getId(), new TaskStuff(task, status, DateTimes.nowUtc(), task.getDataSource()));\n     }\n     finally {\n       giant.unlock();\n@@ -166,13 +168,11 @@ public void setStatus(TaskStatus status)\n   }\n \n   @Override\n-  public List<TaskStatus> getRecentlyFinishedTaskStatuses()\n+  public List<TaskStatus> getRecentlyFinishedTaskStatuses(@Nullable Integer maxTaskStatuses)\n   {\n     giant.lock();\n \n     try {\n-      final List<TaskStatus> returns = Lists.newArrayList();\n-      final long recent = System.currentTimeMillis() - config.getRecentlyFinishedThreshold().getMillis();\n       final Ordering<TaskStuff> createdDateDesc = new Ordering<TaskStuff>()\n       {\n         @Override\n@@ -181,12 +181,61 @@ public int compare(TaskStuff a, TaskStuff b)\n           return a.getCreatedDate().compareTo(b.getCreatedDate());\n         }\n       }.reverse();\n-      for (final TaskStuff taskStuff : createdDateDesc.sortedCopy(tasks.values())) {\n-        if (taskStuff.getStatus().isComplete() && taskStuff.getCreatedDate().getMillis() > recent) {\n-          returns.add(taskStuff.getStatus());\n-        }\n-      }\n-      return returns;\n+\n+      return maxTaskStatuses == null ?\n+             getRecentlyFinishedTaskStatusesSince(\n+                 System.currentTimeMillis() - config.getRecentlyFinishedThreshold().getMillis(),\n+                 createdDateDesc\n+             ) :\n+             getNRecentlyFinishedTaskStatuses(maxTaskStatuses, createdDateDesc);\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n+  }\n+\n+  private List<TaskStatus> getRecentlyFinishedTaskStatusesSince(long start, Ordering<TaskStuff> createdDateDesc)\n+  {\n+    giant.lock();\n+\n+    try {\n+      return createdDateDesc\n+          .sortedCopy(tasks.values())\n+          .stream()\n+          .filter(taskStuff -> taskStuff.getStatus().isComplete() && taskStuff.getCreatedDate().getMillis() > start)\n+          .map(TaskStuff::getStatus)\n+          .collect(Collectors.toList());\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n+  }\n+\n+  private List<TaskStatus> getNRecentlyFinishedTaskStatuses(int n, Ordering<TaskStuff> createdDateDesc)\n+  {\n+    giant.lock();\n+\n+    try {\n+      return createdDateDesc.sortedCopy(tasks.values())\n+                            .stream()\n+                            .limit(n)\n+                            .map(TaskStuff::getStatus)\n+                            .collect(Collectors.toList());\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n+  }\n+\n+  @Nullable\n+  @Override\n+  public Pair<DateTime, String> getCreatedDateTimeAndDataSource(String taskId)\n+  {\n+    giant.lock();\n+\n+    try {\n+      final TaskStuff taskStuff = tasks.get(taskId);\n+      return taskStuff == null ? null : Pair.of(taskStuff.getCreatedDate(), taskStuff.getDataSource());\n     }\n     finally {\n       giant.unlock();\n@@ -287,16 +336,16 @@ public void removeLock(final String taskid, final TaskLock taskLock)\n     final Task task;\n     final TaskStatus status;\n     final DateTime createdDate;\n+    final String dataSource;\n \n-    private TaskStuff(Task task, TaskStatus status, DateTime createdDate)\n+    private TaskStuff(Task task, TaskStatus status, DateTime createdDate, String dataSource)\n     {\n-      Preconditions.checkNotNull(task);\n-      Preconditions.checkNotNull(status);\n       Preconditions.checkArgument(task.getId().equals(status.getId()));\n \n-      this.task = task;\n-      this.status = status;\n+      this.task = Preconditions.checkNotNull(task, \"task\");\n+      this.status = Preconditions.checkNotNull(status, \"status\");\n       this.createdDate = Preconditions.checkNotNull(createdDate, \"createdDate\");\n+      this.dataSource = Preconditions.checkNotNull(dataSource, \"dataSource\");\n     }\n \n     public Task getTask()\n@@ -314,9 +363,14 @@ public DateTime getCreatedDate()\n       return createdDate;\n     }\n \n+    public String getDataSource()\n+    {\n+      return dataSource;\n+    }\n+\n     private TaskStuff withStatus(TaskStatus _status)\n     {\n-      return new TaskStuff(task, _status, createdDate);\n+      return new TaskStuff(task, _status, createdDate, dataSource);\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/HeapMemoryTaskStorage.java",
                "sha": "bdc718bd85f1be2df2ab9c63ce13411c3762433a",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapter.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapter.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapter.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.indexing.overlord;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.inject.Inject;\n+import io.druid.java.util.common.DateTimes;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import java.util.Comparator;\n+import java.util.Optional;\n+\n+public class IndexerMetadataStorageAdapter\n+{\n+  private final TaskStorageQueryAdapter taskStorageQueryAdapter;\n+  private final IndexerMetadataStorageCoordinator indexerMetadataStorageCoordinator;\n+\n+  @Inject\n+  public IndexerMetadataStorageAdapter(\n+      TaskStorageQueryAdapter taskStorageQueryAdapter,\n+      IndexerMetadataStorageCoordinator indexerMetadataStorageCoordinator\n+  )\n+  {\n+    this.taskStorageQueryAdapter = taskStorageQueryAdapter;\n+    this.indexerMetadataStorageCoordinator = indexerMetadataStorageCoordinator;\n+  }\n+\n+  public int deletePendingSegments(String dataSource, Interval deleteInterval)\n+  {\n+    // Check the given interval overlaps the interval(minCreatedDateOfActiveTasks, MAX)\n+    final Optional<DateTime> minCreatedDateOfActiveTasks = taskStorageQueryAdapter\n+        .getActiveTasks()\n+        .stream()\n+        .map(task -> Preconditions.checkNotNull(\n+            taskStorageQueryAdapter.getCreatedTime(task.getId()),\n+            \"Can't find the createdTime for task[%s]\",\n+            task.getId()\n+        ))\n+        .min(Comparator.naturalOrder());\n+\n+    final Interval activeTaskInterval = new Interval(\n+        minCreatedDateOfActiveTasks.orElse(DateTimes.MAX),\n+        DateTimes.MAX\n+    );\n+\n+    Preconditions.checkArgument(\n+        !deleteInterval.overlaps(activeTaskInterval),\n+        \"Cannot delete pendingSegments because there is at least one active task created at %s\",\n+        activeTaskInterval.getStart()\n+    );\n+\n+    return indexerMetadataStorageCoordinator.deletePendingSegments(dataSource, deleteInterval);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapter.java",
                "sha": "dc7ea173083867dda01d13c02c4301702baccf7b",
                "status": "added"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/MetadataTaskStorage.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/MetadataTaskStorage.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 14,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/MetadataTaskStorage.java",
                "patch": "@@ -50,6 +50,7 @@\n import javax.annotation.Nullable;\n import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n \n public class MetadataTaskStorage implements TaskStorage\n {\n@@ -212,25 +213,27 @@ public Task apply(@Nullable Pair<Task, TaskStatus> input)\n   }\n \n   @Override\n-  public List<TaskStatus> getRecentlyFinishedTaskStatuses()\n+  public List<TaskStatus> getRecentlyFinishedTaskStatuses(@Nullable Integer maxTaskStatuses)\n   {\n-    final DateTime start = DateTimes.nowUtc().minus(config.getRecentlyFinishedThreshold());\n-\n     return ImmutableList.copyOf(\n-        Iterables.filter(\n-            handler.getInactiveStatusesSince(start),\n-            new Predicate<TaskStatus>()\n-            {\n-              @Override\n-              public boolean apply(TaskStatus status)\n-              {\n-                return status.isComplete();\n-              }\n-            }\n-        )\n+        handler\n+            .getInactiveStatusesSince(\n+                DateTimes.nowUtc().minus(config.getRecentlyFinishedThreshold()),\n+                maxTaskStatuses\n+            )\n+            .stream()\n+            .filter(TaskStatus::isComplete)\n+            .collect(Collectors.toList())\n     );\n   }\n \n+  @Nullable\n+  @Override\n+  public Pair<DateTime, String> getCreatedDateTimeAndDataSource(String taskId)\n+  {\n+    return handler.getCreatedDateAndDataSource(taskId);\n+  }\n+\n   @Override\n   public void addLock(final String taskid, final TaskLock taskLock)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/MetadataTaskStorage.java",
                "sha": "479b92e92f38e994c188566673974ffe44cfc2db",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "patch": "@@ -53,7 +53,7 @@\n import io.druid.concurrent.LifecycleLock;\n import io.druid.curator.CuratorUtils;\n import io.druid.curator.cache.PathChildrenCacheFactory;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.autoscaling.ProvisioningService;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "sha": "ca052d320aaa4e11362fc6e9874452d7a2883a37",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunnerWorkItem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunnerWorkItem.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunnerWorkItem.java",
                "patch": "@@ -20,7 +20,7 @@\n package io.druid.indexing.overlord;\n \n import com.google.common.util.concurrent.SettableFuture;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.worker.Worker;\n import org.joda.time.DateTime;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunnerWorkItem.java",
                "sha": "3fecb075d81f478db07729d965bfec005e4e2bf4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerListener.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerListener.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerListener.java",
                "patch": "@@ -19,7 +19,7 @@\n \n package io.druid.indexing.overlord;\n \n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n \n import java.util.concurrent.Executor;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerListener.java",
                "sha": "79180411bd0d89137a57651f8b77e7023e1056b4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerUtils.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerUtils.java",
                "patch": "@@ -20,7 +20,7 @@\n package io.druid.indexing.overlord;\n \n import com.metamx.emitter.EmittingLogger;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.java.util.common.Pair;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerUtils.java",
                "sha": "2a6d917d4d7315166ae1a30f679727dbafd4db4f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerWorkItem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerWorkItem.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerWorkItem.java",
                "patch": "@@ -22,7 +22,7 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.util.concurrent.ListenableFuture;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.java.util.common.DateTimes;\n import org.joda.time.DateTime;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskRunnerWorkItem.java",
                "sha": "2ec4a147333102c78e9964a2004cef276e221a34",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorage.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorage.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 4,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorage.java",
                "patch": "@@ -24,8 +24,11 @@\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.actions.TaskAction;\n import io.druid.indexing.common.task.Task;\n+import io.druid.java.util.common.Pair;\n import io.druid.metadata.EntryExistsException;\n+import org.joda.time.DateTime;\n \n+import javax.annotation.Nullable;\n import java.util.List;\n \n public interface TaskStorage\n@@ -119,13 +122,16 @@\n   List<Task> getActiveTasks();\n \n   /**\n-   * Returns a list of recently finished task statuses as stored in the storage facility. No particular order\n-   * is guaranteed, but implementations are encouraged to return tasks in descending order of creation. No particular\n-   * standard of \"recent\" is guaranteed, and in fact, this method is permitted to simply return nothing.\n+   * Returns up to {@code maxTaskStatuses} statuses of recently finished tasks as stored in the storage facility. No\n+   * particular order is guaranteed, but implementations are encouraged to return tasks in descending order of creation.\n+   * No particular standard of \"recent\" is guaranteed, and in fact, this method is permitted to simply return nothing.\n    *\n    * @return list of recently finished tasks\n    */\n-  List<TaskStatus> getRecentlyFinishedTaskStatuses();\n+  List<TaskStatus> getRecentlyFinishedTaskStatuses(@Nullable Integer maxTaskStatuses);\n+\n+  @Nullable\n+  Pair<DateTime, String> getCreatedDateTimeAndDataSource(String taskId);\n \n   /**\n    * Returns a list of locks for a particular task.",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorage.java",
                "sha": "16e553fec688427893d4a8a5c26aece65028bf9d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorageQueryAdapter.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorageQueryAdapter.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorageQueryAdapter.java",
                "patch": "@@ -27,8 +27,11 @@\n import io.druid.indexing.common.actions.SegmentTransactionalInsertAction;\n import io.druid.indexing.common.actions.TaskAction;\n import io.druid.indexing.common.task.Task;\n+import io.druid.java.util.common.Pair;\n import io.druid.timeline.DataSegment;\n+import org.joda.time.DateTime;\n \n+import javax.annotation.Nullable;\n import java.util.List;\n import java.util.Set;\n \n@@ -50,9 +53,16 @@ public TaskStorageQueryAdapter(TaskStorage storage)\n     return storage.getActiveTasks();\n   }\n \n-  public List<TaskStatus> getRecentlyFinishedTaskStatuses()\n+  public List<TaskStatus> getRecentlyFinishedTaskStatuses(@Nullable Integer maxTaskStatuses)\n   {\n-    return storage.getRecentlyFinishedTaskStatuses();\n+    return storage.getRecentlyFinishedTaskStatuses(maxTaskStatuses);\n+  }\n+\n+  @Nullable\n+  public DateTime getCreatedTime(String taskId)\n+  {\n+    final Pair<DateTime, String> pair = storage.getCreatedDateTimeAndDataSource(taskId);\n+    return pair == null ? null : pair.lhs;\n   }\n \n   public Optional<Task> getTask(final String taskid)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/TaskStorageQueryAdapter.java",
                "sha": "469a5e7bff04db372f6d8992a5d126ef47fe1775",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "patch": "@@ -35,7 +35,7 @@\n import io.druid.java.util.common.concurrent.Execs;\n import io.druid.concurrent.TaskThreadPriority;\n import io.druid.guice.annotations.Self;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TaskToolbox;\n import io.druid.indexing.common.TaskToolboxFactory;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/ThreadPoolTaskRunner.java",
                "sha": "b7d7008b093777b04efb7114f5dd1c3c137a2f66",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/http/OverlordResource.java",
                "changes": 147,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/http/OverlordResource.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 78,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/http/OverlordResource.java",
                "patch": "@@ -19,9 +19,9 @@\n \n package io.druid.indexing.overlord.http;\n \n-import com.fasterxml.jackson.annotation.JsonValue;\n import com.google.common.base.Function;\n import com.google.common.base.Optional;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n@@ -35,11 +35,13 @@\n import io.druid.audit.AuditInfo;\n import io.druid.audit.AuditManager;\n import io.druid.common.config.JacksonConfigManager;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n+import io.druid.indexer.TaskStatusPlus;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.actions.TaskActionClient;\n import io.druid.indexing.common.actions.TaskActionHolder;\n import io.druid.indexing.common.task.Task;\n+import io.druid.indexing.overlord.IndexerMetadataStorageAdapter;\n import io.druid.indexing.overlord.TaskMaster;\n import io.druid.indexing.overlord.TaskQueue;\n import io.druid.indexing.overlord.TaskRunner;\n@@ -58,19 +60,19 @@\n import io.druid.server.http.security.StateResourceFilter;\n import io.druid.server.security.Access;\n import io.druid.server.security.Action;\n-import io.druid.server.security.AuthorizerMapper;\n import io.druid.server.security.AuthorizationUtils;\n+import io.druid.server.security.AuthorizerMapper;\n import io.druid.server.security.ForbiddenException;\n import io.druid.server.security.Resource;\n import io.druid.server.security.ResourceAction;\n import io.druid.server.security.ResourceType;\n import io.druid.tasklogs.TaskLogStreamer;\n import io.druid.timeline.DataSegment;\n-import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n import javax.servlet.http.HttpServletRequest;\n import javax.ws.rs.Consumes;\n+import javax.ws.rs.DELETE;\n import javax.ws.rs.DefaultValue;\n import javax.ws.rs.GET;\n import javax.ws.rs.HeaderParam;\n@@ -83,12 +85,14 @@\n import javax.ws.rs.core.Context;\n import javax.ws.rs.core.MediaType;\n import javax.ws.rs.core.Response;\n+import javax.ws.rs.core.Response.Status;\n import java.io.IOException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n \n /**\n  */\n@@ -99,6 +103,7 @@\n \n   private final TaskMaster taskMaster;\n   private final TaskStorageQueryAdapter taskStorageQueryAdapter;\n+  private final IndexerMetadataStorageAdapter indexerMetadataStorageAdapter;\n   private final TaskLogStreamer taskLogStreamer;\n   private final JacksonConfigManager configManager;\n   private final AuditManager auditManager;\n@@ -110,14 +115,16 @@\n   public OverlordResource(\n       TaskMaster taskMaster,\n       TaskStorageQueryAdapter taskStorageQueryAdapter,\n+      IndexerMetadataStorageAdapter indexerMetadataStorageAdapter,\n       TaskLogStreamer taskLogStreamer,\n       JacksonConfigManager configManager,\n       AuditManager auditManager,\n       AuthorizerMapper authorizerMapper\n-  ) throws Exception\n+  )\n   {\n     this.taskMaster = taskMaster;\n     this.taskStorageQueryAdapter = taskStorageQueryAdapter;\n+    this.indexerMetadataStorageAdapter = indexerMetadataStorageAdapter;\n     this.taskLogStreamer = taskLogStreamer;\n     this.configManager = configManager;\n     this.auditManager = auditManager;\n@@ -146,7 +153,7 @@ public Response taskPost(\n     );\n \n     if (!authResult.isAllowed()) {\n-      throw new ForbiddenException(authResult.toString());\n+      throw new ForbiddenException(authResult.getMessage());\n     }\n \n     return asLeaderWith(\n@@ -402,7 +409,7 @@ public String apply(final TaskRunnerWorkItem workItem)\n                     // Would be nice to include the real created date, but the TaskStorage API doesn't yet allow it.\n                     new TaskRunnerWorkItem(\n                         task.getId(),\n-                        SettableFuture.<TaskStatus>create(),\n+                        SettableFuture.create(),\n                         DateTimes.EPOCH,\n                         DateTimes.EPOCH\n                     )\n@@ -459,7 +466,10 @@ public Response getRunningTasks(@Context final HttpServletRequest req)\n   @GET\n   @Path(\"/completeTasks\")\n   @Produces(MediaType.APPLICATION_JSON)\n-  public Response getCompleteTasks(@Context final HttpServletRequest req)\n+  public Response getCompleteTasks(\n+      @QueryParam(\"n\") final Integer maxTaskStatuses,\n+      @Context final HttpServletRequest req\n+  )\n   {\n     Function<TaskStatus, Iterable<ResourceAction>> raGenerator = taskStatus -> {\n       final String taskId = taskStatus.getId();\n@@ -483,33 +493,60 @@ public Response getCompleteTasks(@Context final HttpServletRequest req)\n     final List<TaskStatus> recentlyFinishedTasks = Lists.newArrayList(\n         AuthorizationUtils.filterAuthorizedResources(\n             req,\n-            taskStorageQueryAdapter.getRecentlyFinishedTaskStatuses(),\n+            taskStorageQueryAdapter.getRecentlyFinishedTaskStatuses(maxTaskStatuses),\n             raGenerator,\n             authorizerMapper\n         )\n     );\n \n-    final List<TaskResponseObject> completeTasks = Lists.transform(\n-        recentlyFinishedTasks,\n-        new Function<TaskStatus, TaskResponseObject>()\n-        {\n-          @Override\n-          public TaskResponseObject apply(TaskStatus taskStatus)\n-          {\n-            // Would be nice to include the real created date, but the TaskStorage API doesn't yet allow it.\n-            return new TaskResponseObject(\n-                taskStatus.getId(),\n-                DateTimes.EPOCH,\n-                DateTimes.EPOCH,\n-                Optional.of(taskStatus),\n-                TaskLocation.unknown()\n-            );\n-          }\n-        }\n-    );\n+    final List<TaskStatusPlus> completeTasks = recentlyFinishedTasks\n+        .stream()\n+        .map(status -> new TaskStatusPlus(\n+            status.getId(),\n+            taskStorageQueryAdapter.getCreatedTime(status.getId()),\n+            // Would be nice to include the real queue insertion time, but the TaskStorage API doesn't yet allow it.\n+            DateTimes.EPOCH,\n+            status.getStatusCode(),\n+            status.getDuration(),\n+            TaskLocation.unknown())\n+        )\n+        .collect(Collectors.toList());\n+\n     return Response.ok(completeTasks).build();\n   }\n \n+  @DELETE\n+  @Path(\"/pendingSegments/{dataSource}\")\n+  @Produces(MediaType.APPLICATION_JSON)\n+  public Response killPendingSegments(\n+      @PathParam(\"dataSource\") String dataSource,\n+      @QueryParam(\"interval\") String deleteIntervalString,\n+      @Context HttpServletRequest request\n+  )\n+  {\n+    final Interval deleteInterval = Intervals.of(deleteIntervalString);\n+    // check auth for dataSource\n+    final Access authResult = AuthorizationUtils.authorizeAllResourceActions(\n+        request,\n+        ImmutableList.of(\n+            new ResourceAction(new Resource(dataSource, ResourceType.DATASOURCE), Action.READ),\n+            new ResourceAction(new Resource(dataSource, ResourceType.DATASOURCE), Action.WRITE)\n+        ),\n+        authorizerMapper\n+    );\n+\n+    if (!authResult.isAllowed()) {\n+      throw new ForbiddenException(authResult.getMessage());\n+    }\n+\n+    if (taskMaster.isLeader()) {\n+      final int numDeleted = indexerMetadataStorageAdapter.deletePendingSegments(dataSource, deleteInterval);\n+      return Response.ok().entity(ImmutableMap.of(\"numDeleted\", numDeleted)).build();\n+    } else {\n+      return Response.status(Status.SERVICE_UNAVAILABLE).build();\n+    }\n+  }\n+\n   @GET\n   @Path(\"/workers\")\n   @Produces(MediaType.APPLICATION_JSON)\n@@ -595,16 +632,17 @@ public Response apply(TaskRunner taskRunner)\n             return Response.ok(\n                 Lists.transform(\n                     Lists.newArrayList(fn.apply(taskRunner)),\n-                    new Function<TaskRunnerWorkItem, TaskResponseObject>()\n+                    new Function<TaskRunnerWorkItem, TaskStatusPlus>()\n                     {\n                       @Override\n-                      public TaskResponseObject apply(TaskRunnerWorkItem workItem)\n+                      public TaskStatusPlus apply(TaskRunnerWorkItem workItem)\n                       {\n-                        return new TaskResponseObject(\n+                        return new TaskStatusPlus(\n                             workItem.getTaskId(),\n                             workItem.getCreatedTime(),\n                             workItem.getQueueInsertionTime(),\n-                            Optional.<TaskStatus>absent(),\n+                            null,\n+                            null,\n                             workItem.getLocation()\n                         );\n                       }\n@@ -671,51 +709,4 @@ public TaskResponseObject apply(TaskRunnerWorkItem workItem)\n         )\n     );\n   }\n-\n-  static class TaskResponseObject\n-  {\n-    private final String id;\n-    private final DateTime createdTime;\n-    private final DateTime queueInsertionTime;\n-    private final Optional<TaskStatus> status;\n-    private final TaskLocation location;\n-\n-    private TaskResponseObject(\n-        String id,\n-        DateTime createdTime,\n-        DateTime queueInsertionTime,\n-        Optional<TaskStatus> status,\n-        TaskLocation location\n-    )\n-    {\n-      this.id = id;\n-      this.createdTime = createdTime;\n-      this.queueInsertionTime = queueInsertionTime;\n-      this.status = status;\n-      this.location = location;\n-    }\n-\n-    @JsonValue\n-    public Map<String, Object> toJson()\n-    {\n-      final Map<String, Object> data = Maps.newLinkedHashMap();\n-      data.put(\"id\", id);\n-      if (createdTime.getMillis() > 0) {\n-        data.put(\"createdTime\", createdTime);\n-      }\n-      if (queueInsertionTime.getMillis() > 0) {\n-        data.put(\"queueInsertionTime\", queueInsertionTime);\n-      }\n-      if (status.isPresent()) {\n-        data.put(\"statusCode\", status.get().getStatusCode().toString());\n-        if (status.get().isComplete()) {\n-          data.put(\"duration\", status.get().getDuration());\n-        }\n-      }\n-      if (location != null) {\n-        data.put(\"location\", location);\n-      }\n-      return data;\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/overlord/http/OverlordResource.java",
                "sha": "4fa507e164b1b557186216226daf6f055833788a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 3,
                "filename": "indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "patch": "@@ -22,7 +22,8 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.common.task.TaskResource;\n@@ -50,7 +51,7 @@ public static TaskAnnouncement create(String taskId, TaskResource resource, Task\n   @JsonCreator\n   private TaskAnnouncement(\n       @JsonProperty(\"id\") String taskId,\n-      @JsonProperty(\"status\") TaskStatus.Status status,\n+      @JsonProperty(\"status\") TaskState status,\n       @JsonProperty(\"taskStatus\") TaskStatus taskStatus,\n       @JsonProperty(\"taskResource\") TaskResource taskResource,\n       @JsonProperty(\"taskLocation\") TaskLocation taskLocation\n@@ -77,7 +78,7 @@ public String getTaskId()\n   // Can be removed when backwards compat is no longer needed\n   @JsonProperty(\"status\")\n   @Deprecated\n-  public TaskStatus.Status getStatus()\n+  public TaskState getStatus()\n   {\n     return taskStatus.getStatusCode();\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/worker/TaskAnnouncement.java",
                "sha": "8c3cd661b2682babcb3943829e86c1ddbb67bb9a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "patch": "@@ -28,7 +28,7 @@\n import com.google.inject.Inject;\n import com.metamx.emitter.EmittingLogger;\n import io.druid.java.util.common.concurrent.Execs;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.TaskRunner;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "sha": "040dbf3500ee46fe0a0d1969e8b2b2a16b454147",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 8,
                "filename": "indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "patch": "@@ -47,6 +47,7 @@\n import io.druid.discovery.DataNodeService;\n import io.druid.discovery.DruidNodeAnnouncer;\n import io.druid.discovery.LookupNodeService;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.SegmentLoaderFactory;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TaskToolbox;\n@@ -374,7 +375,7 @@ public void testBasics() throws Exception\n \n     // Wait for the task to finish.\n     final TaskStatus taskStatus = statusFuture.get();\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n   }\n \n   @Test(timeout = 60_000L)\n@@ -445,7 +446,7 @@ public void testTransformSpec() throws Exception\n \n     // Wait for the task to finish.\n     final TaskStatus taskStatus = statusFuture.get();\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n   }\n \n   @Test(timeout = 60_000L)\n@@ -572,7 +573,7 @@ public void testNoReportParseExceptions() throws Exception\n \n     // Wait for the task to finish.\n     final TaskStatus taskStatus = statusFuture.get();\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n   }\n \n   @Test(timeout = 60_000L)\n@@ -606,7 +607,7 @@ public void testRestore() throws Exception\n \n       // Wait for the task to finish. The status doesn't really matter, but we'll check it anyway.\n       final TaskStatus taskStatus = statusFuture.get();\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n \n       // Nothing should be published.\n       Assert.assertEquals(Sets.newHashSet(), mdc.getPublished());\n@@ -665,7 +666,7 @@ public void testRestore() throws Exception\n \n       // Wait for the task to finish.\n       final TaskStatus taskStatus = statusFuture.get();\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n     }\n   }\n \n@@ -760,7 +761,7 @@ public void testRestoreAfterHandoffAttemptDuringShutdown() throws Exception\n \n       // Wait for the task to finish.\n       final TaskStatus taskStatus = statusFuture.get();\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n     }\n   }\n \n@@ -794,7 +795,7 @@ public void testRestoreCorruptData() throws Exception\n \n       // Wait for the task to finish. The status doesn't really matter, but we'll check it anyway.\n       final TaskStatus taskStatus = statusFuture.get();\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n \n       // Nothing should be published.\n       Assert.assertEquals(Sets.newHashSet(), mdc.getPublished());\n@@ -846,7 +847,7 @@ public void testStopBeforeStarting() throws Exception\n \n     // Wait for the task to finish.\n     final TaskStatus taskStatus = statusFuture.get();\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskStatus.getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, taskStatus.getStatusCode());\n   }\n \n   private ListenableFuture<TaskStatus> runTask(final Task task, final TaskToolbox toolbox)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/common/task/RealtimeIndexTaskTest.java",
                "sha": "521bba77a3605412ffbc4a28d5b90eaf6497010f",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapterTest.java",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapterTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapterTest.java",
                "patch": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.indexing.overlord;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.druid.indexing.common.task.NoopTask;\n+import io.druid.java.util.common.DateTimes;\n+import io.druid.java.util.common.Intervals;\n+import org.easymock.EasyMock;\n+import org.hamcrest.CoreMatchers;\n+import org.joda.time.Interval;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+public class IndexerMetadataStorageAdapterTest\n+{\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private TaskStorageQueryAdapter taskStorageQueryAdapter;\n+  private IndexerMetadataStorageCoordinator indexerMetadataStorageCoordinator;\n+  private IndexerMetadataStorageAdapter indexerMetadataStorageAdapter;\n+\n+  @Before\n+  public void setup()\n+  {\n+    indexerMetadataStorageCoordinator = EasyMock.strictMock(IndexerMetadataStorageCoordinator.class);\n+    taskStorageQueryAdapter = EasyMock.strictMock(TaskStorageQueryAdapter.class);\n+    indexerMetadataStorageAdapter = new IndexerMetadataStorageAdapter(\n+        taskStorageQueryAdapter,\n+        indexerMetadataStorageCoordinator\n+    );\n+  }\n+\n+  @Test\n+  public void testDeletePendingSegments()\n+  {\n+    EasyMock.expect(taskStorageQueryAdapter.getActiveTasks())\n+            .andReturn(ImmutableList.of(NoopTask.create(\"id1\", 0), NoopTask.create(\"id2\", 0)));\n+    EasyMock.expect(taskStorageQueryAdapter.getCreatedTime(EasyMock.eq(\"id1\")))\n+            .andReturn(DateTimes.of(\"2017-12-01\"));\n+    EasyMock.expect(taskStorageQueryAdapter.getCreatedTime(EasyMock.eq(\"id2\")))\n+            .andReturn(DateTimes.of(\"2017-12-02\"));\n+\n+    final Interval deleteInterval = Intervals.of(\"2017-01-01/2017-12-01\");\n+    EasyMock\n+        .expect(\n+            indexerMetadataStorageCoordinator.deletePendingSegments(EasyMock.anyString(), EasyMock.eq(deleteInterval))\n+        )\n+        .andReturn(10);\n+    EasyMock.replay(taskStorageQueryAdapter, indexerMetadataStorageCoordinator);\n+\n+    Assert.assertEquals(10, indexerMetadataStorageAdapter.deletePendingSegments(\"dataSource\", deleteInterval));\n+  }\n+\n+  @Test\n+  public void testDeletePendingSegmentsOfRunningTasks()\n+  {\n+    EasyMock.expect(taskStorageQueryAdapter.getActiveTasks())\n+            .andReturn(ImmutableList.of(NoopTask.create(\"id1\", 0), NoopTask.create(\"id2\", 0)));\n+    EasyMock.expect(taskStorageQueryAdapter.getCreatedTime(EasyMock.eq(\"id1\")))\n+            .andReturn(DateTimes.of(\"2017-11-01\"));\n+    EasyMock.expect(taskStorageQueryAdapter.getCreatedTime(EasyMock.eq(\"id2\")))\n+            .andReturn(DateTimes.of(\"2017-12-02\"));\n+\n+    final Interval deleteInterval = Intervals.of(\"2017-01-01/2017-12-01\");\n+    EasyMock\n+        .expect(\n+            indexerMetadataStorageCoordinator.deletePendingSegments(EasyMock.anyString(), EasyMock.eq(deleteInterval))\n+        )\n+        .andReturn(10);\n+    EasyMock.replay(taskStorageQueryAdapter, indexerMetadataStorageCoordinator);\n+\n+    expectedException.expect(CoreMatchers.instanceOf(IllegalArgumentException.class));\n+    expectedException.expectMessage(\"Cannot delete pendingSegments because there is at least one active task created\");\n+    indexerMetadataStorageAdapter.deletePendingSegments(\"dataSource\", deleteInterval);\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/IndexerMetadataStorageAdapterTest.java",
                "sha": "872c85e5b816adce668fb528d6cf276f6eb945f9",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerRunPendingTasksConcurrencyTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerRunPendingTasksConcurrencyTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 9,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerRunPendingTasksConcurrencyTest.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.util.concurrent.ListenableFuture;\n \n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestTasks;\n import io.druid.indexing.common.task.Task;\n@@ -94,8 +95,8 @@ public int getPendingTasksRunnerNumThreads()\n     //simulate completion of task0 and task1\n     mockWorkerRunningAndCompletionSuccessfulTasks(tasks[0], tasks[1]);\n \n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, results[0].get().getStatusCode());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, results[1].get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, results[0].get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, results[1].get().getStatusCode());\n \n     // now both threads race to run the last 3 tasks. task2 and task3 are being assigned\n     waitForBothWorkersToHaveUnackedTasks();\n@@ -104,20 +105,20 @@ public int getPendingTasksRunnerNumThreads()\n         && remoteTaskRunner.getWorkersWithUnacknowledgedTask().containsValue(tasks[3].getId())) {\n       remoteTaskRunner.shutdown(\"task4\");\n       mockWorkerRunningAndCompletionSuccessfulTasks(tasks[3], tasks[2]);\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[3].get().getStatusCode());\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[2].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[3].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[2].get().getStatusCode());\n     } else if (remoteTaskRunner.getWorkersWithUnacknowledgedTask().containsValue(tasks[3].getId())\n                && remoteTaskRunner.getWorkersWithUnacknowledgedTask().containsValue(tasks[4].getId())) {\n       remoteTaskRunner.shutdown(\"task2\");\n       mockWorkerRunningAndCompletionSuccessfulTasks(tasks[4], tasks[3]);\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[4].get().getStatusCode());\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[3].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[4].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[3].get().getStatusCode());\n     } else if (remoteTaskRunner.getWorkersWithUnacknowledgedTask().containsValue(tasks[4].getId())\n                && remoteTaskRunner.getWorkersWithUnacknowledgedTask().containsValue(tasks[2].getId())) {\n       remoteTaskRunner.shutdown(\"task3\");\n       mockWorkerRunningAndCompletionSuccessfulTasks(tasks[4], tasks[2]);\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[4].get().getStatusCode());\n-      Assert.assertEquals(TaskStatus.Status.SUCCESS, results[2].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[4].get().getStatusCode());\n+      Assert.assertEquals(TaskState.SUCCESS, results[2].get().getStatusCode());\n     } else {\n       throw new ISE(\"two out of three tasks 2,3 and 4 must be waiting for ack.\");\n     }\n@@ -133,7 +134,7 @@ public int getPendingTasksRunnerNumThreads()\n       rtrTestUtils.mockWorkerRunningTask(\"worker1\", tasks[5]);\n       rtrTestUtils.mockWorkerCompleteSuccessfulTask(\"worker1\", tasks[5]);\n     }\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, results[5].get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, results[5].get().getStatusCode());\n   }\n \n   private void mockWorkerRunningAndCompletionSuccessfulTasks(Task t1, Task t2) throws Exception",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerRunPendingTasksConcurrencyTest.java",
                "sha": "c016065284e0bf71ef4d1d4dd0515e64a07b5223",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 9,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import com.google.common.util.concurrent.ListenableFuture;\n import com.metamx.emitter.EmittingLogger;\n import com.metamx.emitter.service.ServiceEmitter;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.IndexingServiceCondition;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestRealtimeTask;\n@@ -105,7 +106,7 @@ public void testRun() throws Exception\n     Assert.assertTrue(workerCompletedTask(result));\n \n     Assert.assertEquals(task.getId(), result.get().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, result.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, result.get().getStatusCode());\n   }\n \n   @Test\n@@ -131,7 +132,7 @@ public void testRunExistingTaskThatHasntStartedRunning() throws Exception\n     Assert.assertTrue(workerCompletedTask(result));\n \n     Assert.assertEquals(task.getId(), result.get().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, result.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, result.get().getStatusCode());\n   }\n \n   @Test\n@@ -152,7 +153,7 @@ public void testRunExistingTaskThatHasStartedRunning() throws Exception\n     Assert.assertTrue(workerCompletedTask(result));\n \n     Assert.assertEquals(task.getId(), result.get().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, result.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, result.get().getStatusCode());\n   }\n \n   @Test\n@@ -314,7 +315,7 @@ public void testStatusRemoved() throws Exception\n \n     TaskStatus status = future.get(TIMEOUT_SECONDS, TimeUnit.SECONDS);\n \n-    Assert.assertEquals(status.getStatusCode(), TaskStatus.Status.FAILED);\n+    Assert.assertEquals(status.getStatusCode(), TaskState.FAILED);\n   }\n \n   @Test\n@@ -367,7 +368,7 @@ public void testRunWithTaskComplete() throws Exception\n \n     TaskStatus status = future.get(TIMEOUT_SECONDS, TimeUnit.SECONDS);\n \n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, status.getStatusCode());\n   }\n \n   @Test\n@@ -385,7 +386,7 @@ public void testWorkerRemoved() throws Exception\n \n     TaskStatus status = future.get(TIMEOUT_SECONDS, TimeUnit.SECONDS);\n \n-    Assert.assertEquals(TaskStatus.Status.FAILED, status.getStatusCode());\n+    Assert.assertEquals(TaskState.FAILED, status.getStatusCode());\n     RemoteTaskRunnerConfig config = remoteTaskRunner.getRemoteTaskRunnerConfig();\n     Assert.assertTrue(\n         TestUtils.conditionValid(\n@@ -421,7 +422,7 @@ public void testWorkerDisabled() throws Exception\n     mockWorkerCompleteSuccessfulTask(task);\n     Assert.assertTrue(workerCompletedTask(result));\n     Assert.assertEquals(task.getId(), result.get().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, result.get().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, result.get().getStatusCode());\n \n     // Confirm RTR thinks the worker is disabled.\n     Assert.assertEquals(\"\", Iterables.getOnlyElement(remoteTaskRunner.getWorkers()).getWorker().getVersion());\n@@ -595,8 +596,8 @@ public boolean isValid()\n \n     mockWorkerCompleteSuccessfulTask(task);\n     TaskStatus status = future.get(TIMEOUT_SECONDS, TimeUnit.SECONDS);\n-    Assert.assertEquals(status.getStatusCode(), TaskStatus.Status.SUCCESS);\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(status.getStatusCode(), TaskState.SUCCESS);\n+    Assert.assertEquals(TaskState.SUCCESS, status.getStatusCode());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTest.java",
                "sha": "c2a199f872d3ba399d4a15b194e29fa5d2d5eeb9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "patch": "@@ -28,7 +28,7 @@\n import io.druid.curator.PotentiallyGzippedCompressionProvider;\n import io.druid.curator.cache.PathChildrenCacheFactory;\n import io.druid.indexing.common.IndexingServiceCondition;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestUtils;\n import io.druid.indexing.common.task.Task;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "sha": "435a5b4a6fc707299612a125a34edd35c15f0f65",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 13,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java",
                "patch": "@@ -47,6 +47,7 @@\n import io.druid.discovery.DataNodeService;\n import io.druid.discovery.DruidNodeAnnouncer;\n import io.druid.discovery.LookupNodeService;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.SegmentLoaderFactory;\n import io.druid.indexing.common.TaskLock;\n import io.druid.indexing.common.TaskStatus;\n@@ -81,7 +82,7 @@\n import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.granularity.Granularities;\n import io.druid.java.util.common.guava.Comparators;\n-import io.druid.metadata.SQLMetadataStorageActionHandlerFactory;\n+import io.druid.metadata.DerbyMetadataStorageActionHandlerFactory;\n import io.druid.metadata.TestDerbyConnector;\n import io.druid.query.QueryRunnerFactoryConglomerate;\n import io.druid.query.SegmentDescriptor;\n@@ -406,7 +407,7 @@ private TaskStorage setUpTaskStorage()\n         taskStorage = new MetadataTaskStorage(\n             testDerbyConnector,\n             new TaskStorageConfig(null),\n-            new SQLMetadataStorageActionHandlerFactory(\n+            new DerbyMetadataStorageActionHandlerFactory(\n                 testDerbyConnector,\n                 derbyConnectorRule.metadataTablesConfigSupplier().get(),\n                 mapper\n@@ -683,8 +684,8 @@ public void testIndexTask() throws Exception\n     final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n     final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n-    Assert.assertEquals(\"merged statusCode\", TaskStatus.Status.SUCCESS, mergedStatus.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, mergedStatus.getStatusCode());\n     Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n     Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n@@ -735,7 +736,7 @@ public void testIndexTaskFailure() throws Exception\n \n     final TaskStatus status = runTask(indexTask);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.FAILED, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.FAILED, status.getStatusCode());\n     Assert.assertEquals(\"num segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -803,7 +804,7 @@ public DataSegment apply(String input)\n     final Task killTask = new KillTask(null, \"test_kill_task\", Intervals.of(\"2011-04-01/P4D\"), null);\n \n     final TaskStatus status = runTask(killTask);\n-    Assert.assertEquals(\"merged statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"merged statusCode\", TaskState.SUCCESS, status.getStatusCode());\n     Assert.assertEquals(\"num segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 3, mdc.getNuked().size());\n     Assert.assertTrue(\n@@ -824,7 +825,7 @@ public void testRealtimeishTask() throws Exception\n     final Task rtishTask = new RealtimeishTask();\n     final TaskStatus status = runTask(rtishTask);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n     Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -838,7 +839,7 @@ public void testNoopTask() throws Exception\n     );\n     final TaskStatus status = runTask(noopTask);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n     Assert.assertEquals(\"num segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -852,7 +853,7 @@ public void testNeverReadyTask() throws Exception\n     );\n     final TaskStatus status = runTask(neverReadyTask);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.FAILED, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.FAILED, status.getStatusCode());\n     Assert.assertEquals(\"num segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -896,7 +897,7 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n \n     final TaskStatus status = runTask(task);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n     Assert.assertEquals(\"segments published\", 1, mdc.getPublished().size());\n     Assert.assertEquals(\"segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -930,7 +931,7 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n \n     final TaskStatus status = runTask(task);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.FAILED, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.FAILED, status.getStatusCode());\n     Assert.assertEquals(\"segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -964,7 +965,7 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n \n     final TaskStatus status = runTask(task);\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.FAILED, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.FAILED, status.getStatusCode());\n     Assert.assertEquals(\"segments published\", 0, mdc.getPublished().size());\n     Assert.assertEquals(\"segments nuked\", 0, mdc.getNuked().size());\n   }\n@@ -1116,7 +1117,7 @@ public void testResumeTasks() throws Exception\n     final List<DataSegment> publishedSegments = byIntervalOrdering.sortedCopy(mdc.getPublished());\n     final List<DataSegment> loggedSegments = byIntervalOrdering.sortedCopy(tsqa.getInsertedSegments(indexTask.getId()));\n \n-    Assert.assertEquals(\"statusCode\", TaskStatus.Status.SUCCESS, status.getStatusCode());\n+    Assert.assertEquals(\"statusCode\", TaskState.SUCCESS, status.getStatusCode());\n     Assert.assertEquals(\"segments logged vs published\", loggedSegments, publishedSegments);\n     Assert.assertEquals(\"num segments published\", 2, mdc.getPublished().size());\n     Assert.assertEquals(\"num segments nuked\", 0, mdc.getNuked().size());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLifecycleTest.java",
                "sha": "e408bd28c2db700e9eadf51a220775e25aff22a9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockBoxConcurrencyTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockBoxConcurrencyTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockBoxConcurrencyTest.java",
                "patch": "@@ -29,8 +29,8 @@\n import io.druid.indexing.common.task.Task;\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.Intervals;\n+import io.druid.metadata.DerbyMetadataStorageActionHandlerFactory;\n import io.druid.metadata.EntryExistsException;\n-import io.druid.metadata.SQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.TestDerbyConnector;\n import org.joda.time.Interval;\n import org.junit.After;\n@@ -65,7 +65,7 @@ public void setup()\n     taskStorage = new MetadataTaskStorage(\n         derbyConnector,\n         new TaskStorageConfig(null),\n-        new SQLMetadataStorageActionHandlerFactory(\n+        new DerbyMetadataStorageActionHandlerFactory(\n             derbyConnector,\n             derby.metadataTablesConfigSupplier().get(),\n             objectMapper",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockBoxConcurrencyTest.java",
                "sha": "e84a118ae0687eb5bbc9aa596022a2d1bf817b6c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockboxTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockboxTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockboxTest.java",
                "patch": "@@ -33,8 +33,8 @@\n import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.Intervals;\n import io.druid.java.util.common.StringUtils;\n+import io.druid.metadata.DerbyMetadataStorageActionHandlerFactory;\n import io.druid.metadata.EntryExistsException;\n-import io.druid.metadata.SQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.TestDerbyConnector;\n import org.easymock.EasyMock;\n import org.joda.time.Interval;\n@@ -74,7 +74,7 @@ public void setup()\n     taskStorage = new MetadataTaskStorage(\n         derbyConnector,\n         new TaskStorageConfig(null),\n-        new SQLMetadataStorageActionHandlerFactory(\n+        new DerbyMetadataStorageActionHandlerFactory(\n             derbyConnector,\n             derby.metadataTablesConfigSupplier().get(),\n             objectMapper",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/TaskLockboxTest.java",
                "sha": "11266b60def0ca77eb89153064efab88fbe2de3b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/PendingTaskBasedProvisioningStrategyTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/PendingTaskBasedProvisioningStrategyTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/PendingTaskBasedProvisioningStrategyTest.java",
                "patch": "@@ -28,7 +28,7 @@\n import com.metamx.emitter.service.ServiceEventBuilder;\n import io.druid.common.guava.DSuppliers;\n import io.druid.java.util.common.concurrent.Execs;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestTasks;\n import io.druid.indexing.common.task.NoopTask;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/PendingTaskBasedProvisioningStrategyTest.java",
                "sha": "e6afde2581833cb6d84b3638a443f165e0660030",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/SimpleProvisioningStrategyTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/SimpleProvisioningStrategyTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/SimpleProvisioningStrategyTest.java",
                "patch": "@@ -29,7 +29,7 @@\n import com.metamx.emitter.service.ServiceEventBuilder;\n import io.druid.common.guava.DSuppliers;\n import io.druid.java.util.common.concurrent.Execs;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestTasks;\n import io.druid.indexing.common.task.NoopTask;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/SimpleProvisioningStrategyTest.java",
                "sha": "f917d359c64a0237b6acf549543b2d0a60af8288",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordResourceTest.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordResourceTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 30,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordResourceTest.java",
                "patch": "@@ -25,17 +25,20 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Lists;\n import com.google.common.util.concurrent.ListenableFuture;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n+import io.druid.indexer.TaskStatusPlus;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TaskToolbox;\n import io.druid.indexing.common.actions.TaskActionClient;\n import io.druid.indexing.common.task.AbstractTask;\n import io.druid.indexing.common.task.NoopTask;\n import io.druid.indexing.common.task.Task;\n+import io.druid.indexing.overlord.IndexerMetadataStorageAdapter;\n import io.druid.indexing.overlord.TaskMaster;\n import io.druid.indexing.overlord.TaskRunner;\n import io.druid.indexing.overlord.TaskRunnerWorkItem;\n import io.druid.indexing.overlord.TaskStorageQueryAdapter;\n+import io.druid.java.util.common.DateTimes;\n import io.druid.server.security.Access;\n import io.druid.server.security.Action;\n import io.druid.server.security.AuthConfig;\n@@ -45,6 +48,7 @@\n import io.druid.server.security.ForbiddenException;\n import io.druid.server.security.Resource;\n import org.easymock.EasyMock;\n+import org.joda.time.Interval;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -56,12 +60,14 @@\n import javax.ws.rs.core.Response;\n import java.util.Collection;\n import java.util.List;\n+import java.util.Map;\n \n public class OverlordResourceTest\n {\n   private OverlordResource overlordResource;\n   private TaskMaster taskMaster;\n-  private TaskStorageQueryAdapter tsqa;\n+  private TaskStorageQueryAdapter taskStorageQueryAdapter;\n+  private IndexerMetadataStorageAdapter indexerMetadataStorageAdapter;\n   private HttpServletRequest req;\n   private TaskRunner taskRunner;\n \n@@ -73,7 +79,8 @@ public void setUp() throws Exception\n   {\n     taskRunner = EasyMock.createMock(TaskRunner.class);\n     taskMaster = EasyMock.createStrictMock(TaskMaster.class);\n-    tsqa = EasyMock.createStrictMock(TaskStorageQueryAdapter.class);\n+    taskStorageQueryAdapter = EasyMock.createStrictMock(TaskStorageQueryAdapter.class);\n+    indexerMetadataStorageAdapter = EasyMock.createStrictMock(IndexerMetadataStorageAdapter.class);\n     req = EasyMock.createStrictMock(HttpServletRequest.class);\n \n     EasyMock.expect(taskMaster.getTaskRunner()).andReturn(\n@@ -102,7 +109,8 @@ public Access authorize(AuthenticationResult authenticationResult, Resource reso\n \n     overlordResource = new OverlordResource(\n         taskMaster,\n-        tsqa,\n+        taskStorageQueryAdapter,\n+        indexerMetadataStorageAdapter,\n         null,\n         null,\n         null,\n@@ -130,7 +138,7 @@ public void expectAuthorizationTokenCheck()\n   public void testLeader()\n   {\n     EasyMock.expect(taskMaster.getCurrentLeader()).andReturn(\"boz\").once();\n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n \n     final Response response = overlordResource.getLeader();\n     Assert.assertEquals(\"boz\", response.getEntity());\n@@ -142,7 +150,7 @@ public void testIsLeader()\n   {\n     EasyMock.expect(taskMaster.isLeader()).andReturn(true).once();\n     EasyMock.expect(taskMaster.isLeader()).andReturn(false).once();\n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n \n     // true\n     final Response response1 = overlordResource.isLeader();\n@@ -160,7 +168,7 @@ public void testSecuredGetWaitingTask() throws Exception\n   {\n     expectAuthorizationTokenCheck();\n \n-    EasyMock.expect(tsqa.getActiveTasks()).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getActiveTasks()).andReturn(\n         ImmutableList.of(\n             getTaskWithIdAndDatasource(\"id_1\", \"allow\"),\n             getTaskWithIdAndDatasource(\"id_2\", \"allow\"),\n@@ -176,12 +184,12 @@ public void testSecuredGetWaitingTask() throws Exception\n         )\n     );\n \n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n \n-    List<OverlordResource.TaskResponseObject> responseObjects = (List) overlordResource.getWaitingTasks(req)\n-                                                                                       .getEntity();\n+    List<TaskStatusPlus> responseObjects = (List<TaskStatusPlus>) overlordResource.getWaitingTasks(req)\n+                                                                                  .getEntity();\n     Assert.assertEquals(1, responseObjects.size());\n-    Assert.assertEquals(\"id_2\", responseObjects.get(0).toJson().get(\"id\"));\n+    Assert.assertEquals(\"id_2\", responseObjects.get(0).getId());\n   }\n \n   @Test\n@@ -190,7 +198,7 @@ public void testSecuredGetCompleteTasks()\n     expectAuthorizationTokenCheck();\n \n     List<String> tasksIds = ImmutableList.of(\"id_1\", \"id_2\", \"id_3\");\n-    EasyMock.expect(tsqa.getRecentlyFinishedTaskStatuses()).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getRecentlyFinishedTaskStatuses(null)).andReturn(\n         Lists.transform(\n             tasksIds,\n             new Function<String, TaskStatus>()\n@@ -204,23 +212,26 @@ public TaskStatus apply(String input)\n         )\n     ).once();\n \n-    EasyMock.expect(tsqa.getTask(tasksIds.get(0))).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getTask(tasksIds.get(0))).andReturn(\n         Optional.of(getTaskWithIdAndDatasource(tasksIds.get(0), \"deny\"))\n     ).once();\n-    EasyMock.expect(tsqa.getTask(tasksIds.get(1))).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getTask(tasksIds.get(1))).andReturn(\n         Optional.of(getTaskWithIdAndDatasource(tasksIds.get(1), \"allow\"))\n     ).once();\n-    EasyMock.expect(tsqa.getTask(tasksIds.get(2))).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getTask(tasksIds.get(2))).andReturn(\n         Optional.of(getTaskWithIdAndDatasource(tasksIds.get(2), \"allow\"))\n     ).once();\n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.expect(taskStorageQueryAdapter.getCreatedTime(EasyMock.anyString()))\n+            .andReturn(DateTimes.EPOCH)\n+            .anyTimes();\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n \n-    List<OverlordResource.TaskResponseObject> responseObjects = (List) overlordResource.getCompleteTasks(req)\n-                                                                                       .getEntity();\n+    List<TaskStatusPlus> responseObjects = (List) overlordResource.getCompleteTasks(null, req)\n+                                                                  .getEntity();\n \n     Assert.assertEquals(2, responseObjects.size());\n-    Assert.assertEquals(tasksIds.get(1), responseObjects.get(0).toJson().get(\"id\"));\n-    Assert.assertEquals(tasksIds.get(2), responseObjects.get(1).toJson().get(\"id\"));\n+    Assert.assertEquals(tasksIds.get(1), responseObjects.get(0).getId());\n+    Assert.assertEquals(tasksIds.get(2), responseObjects.get(1).getId());\n   }\n \n   @Test\n@@ -235,38 +246,55 @@ public void testSecuredGetRunningTasks()\n             new MockTaskRunnerWorkItem(tasksIds.get(1), null)\n         )\n     );\n-    EasyMock.expect(tsqa.getTask(tasksIds.get(0))).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getTask(tasksIds.get(0))).andReturn(\n         Optional.of(getTaskWithIdAndDatasource(tasksIds.get(0), \"deny\"))\n     ).once();\n-    EasyMock.expect(tsqa.getTask(tasksIds.get(1))).andReturn(\n+    EasyMock.expect(taskStorageQueryAdapter.getTask(tasksIds.get(1))).andReturn(\n         Optional.of(getTaskWithIdAndDatasource(tasksIds.get(1), \"allow\"))\n     ).once();\n \n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n \n-    List<OverlordResource.TaskResponseObject> responseObjects = (List) overlordResource.getRunningTasks(req)\n-                                                                                       .getEntity();\n+    List<TaskStatusPlus> responseObjects = (List) overlordResource.getRunningTasks(req)\n+                                                                  .getEntity();\n \n     Assert.assertEquals(1, responseObjects.size());\n-    Assert.assertEquals(tasksIds.get(1), responseObjects.get(0).toJson().get(\"id\"));\n+    Assert.assertEquals(tasksIds.get(1), responseObjects.get(0).getId());\n   }\n \n   @Test\n   public void testSecuredTaskPost()\n   {\n     expectedException.expect(ForbiddenException.class);\n-    expectedException.expectMessage(\"Allowed:false, Message:\");\n     expectAuthorizationTokenCheck();\n \n-    EasyMock.replay(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n     Task task = NoopTask.create();\n     overlordResource.taskPost(task, req);\n   }\n \n+  @Test\n+  public void testKillPendingSegments()\n+  {\n+    expectAuthorizationTokenCheck();\n+\n+    EasyMock.expect(taskMaster.isLeader()).andReturn(true);\n+    EasyMock\n+        .expect(indexerMetadataStorageAdapter.deletePendingSegments(EasyMock.eq(\"allow\"), EasyMock.anyObject(Interval.class)))\n+        .andReturn(2);\n+\n+    EasyMock.replay(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n+\n+    final Map<String, Integer> response = (Map<String, Integer>) overlordResource\n+        .killPendingSegments(\"allow\", new Interval(DateTimes.MIN, DateTimes.nowUtc()).toString(), req)\n+        .getEntity();\n+    Assert.assertEquals(2, response.get(\"numDeleted\").intValue());\n+  }\n+\n   @After\n   public void tearDown()\n   {\n-    EasyMock.verify(taskRunner, taskMaster, tsqa, req);\n+    EasyMock.verify(taskRunner, taskMaster, taskStorageQueryAdapter, indexerMetadataStorageAdapter, req);\n   }\n \n   private Task getTaskWithIdAndDatasource(String id, String datasource)\n@@ -306,7 +334,7 @@ public MockTaskRunnerWorkItem(\n     @Override\n     public TaskLocation getLocation()\n     {\n-      return null;\n+      return TaskLocation.unknown();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordResourceTest.java",
                "sha": "78242e8f860339b3e3f59bcfce80f411cefc9cef",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordTest.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 13,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordTest.java",
                "patch": "@@ -28,17 +28,19 @@\n import com.google.common.util.concurrent.MoreExecutors;\n import com.metamx.emitter.EmittingLogger;\n import com.metamx.emitter.service.ServiceEmitter;\n-import io.druid.java.util.common.concurrent.Execs;\n import io.druid.curator.PotentiallyGzippedCompressionProvider;\n import io.druid.curator.discovery.NoopServiceAnnouncer;\n import io.druid.discovery.DruidLeaderSelector;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n+import io.druid.indexer.TaskState;\n+import io.druid.indexer.TaskStatusPlus;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.actions.TaskActionClientFactory;\n import io.druid.indexing.common.config.TaskStorageConfig;\n import io.druid.indexing.common.task.NoopTask;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.HeapMemoryTaskStorage;\n+import io.druid.indexing.overlord.IndexerMetadataStorageAdapter;\n import io.druid.indexing.overlord.TaskLockbox;\n import io.druid.indexing.overlord.TaskMaster;\n import io.druid.indexing.overlord.TaskRunner;\n@@ -52,11 +54,10 @@\n import io.druid.indexing.overlord.helpers.OverlordHelperManager;\n import io.druid.indexing.overlord.supervisor.SupervisorManager;\n import io.druid.java.util.common.Pair;\n+import io.druid.java.util.common.concurrent.Execs;\n import io.druid.java.util.common.guava.CloseQuietly;\n import io.druid.server.DruidNode;\n import io.druid.server.coordinator.CoordinatorOverlordServiceConfig;\n-import io.druid.server.initialization.IndexerZkConfig;\n-import io.druid.server.initialization.ZkPathsConfig;\n import io.druid.server.metrics.NoopServiceEmitter;\n import io.druid.server.security.AuthConfig;\n import io.druid.server.security.AuthTestUtils;\n@@ -162,7 +163,6 @@ public void setUp() throws Exception\n     taskCompletionCountDownLatches[0] = new CountDownLatch(1);\n     taskCompletionCountDownLatches[1] = new CountDownLatch(1);\n     announcementLatch = new CountDownLatch(1);\n-    IndexerZkConfig indexerZkConfig = new IndexerZkConfig(new ZkPathsConfig(), null, null, null, null);\n     setupServerAndCurator();\n     curator.start();\n     curator.blockUntilConnected();\n@@ -211,10 +211,12 @@ public void testOverlordRun() throws Exception\n     }\n     Assert.assertEquals(taskMaster.getCurrentLeader(), druidNode.getHostAndPort());\n \n+    final TaskStorageQueryAdapter taskStorageQueryAdapter = new TaskStorageQueryAdapter(taskStorage);\n     // Test Overlord resource stuff\n     overlordResource = new OverlordResource(\n         taskMaster,\n-        new TaskStorageQueryAdapter(taskStorage),\n+        taskStorageQueryAdapter,\n+        new IndexerMetadataStorageAdapter(taskStorageQueryAdapter, null),\n         null,\n         null,\n         null,\n@@ -252,7 +254,7 @@ public void testOverlordRun() throws Exception\n     // Simulate completion of task_0\n     taskCompletionCountDownLatches[Integer.parseInt(taskId_0)].countDown();\n     // Wait for taskQueue to handle success status of task_0\n-    waitForTaskStatus(taskId_0, TaskStatus.Status.SUCCESS);\n+    waitForTaskStatus(taskId_0, TaskState.SUCCESS);\n \n     // Manually insert task in taskStorage\n     // Verifies sync from storage\n@@ -265,19 +267,22 @@ public void testOverlordRun() throws Exception\n     response = overlordResource.getRunningTasks(req);\n     // 1 task that was manually inserted should be in running state\n     Assert.assertEquals(1, (((List) response.getEntity()).size()));\n-    final OverlordResource.TaskResponseObject taskResponseObject = ((List<OverlordResource.TaskResponseObject>) response\n+    final TaskStatusPlus taskResponseObject = ((List<TaskStatusPlus>) response\n         .getEntity()).get(0);\n-    Assert.assertEquals(taskId_1, taskResponseObject.toJson().get(\"id\"));\n-    Assert.assertEquals(TASK_LOCATION, taskResponseObject.toJson().get(\"location\"));\n+    Assert.assertEquals(taskId_1, taskResponseObject.getId());\n+    Assert.assertEquals(TASK_LOCATION, taskResponseObject.getLocation());\n \n     // Simulate completion of task_1\n     taskCompletionCountDownLatches[Integer.parseInt(taskId_1)].countDown();\n     // Wait for taskQueue to handle success status of task_1\n-    waitForTaskStatus(taskId_1, TaskStatus.Status.SUCCESS);\n+    waitForTaskStatus(taskId_1, TaskState.SUCCESS);\n \n     // should return number of tasks which are not in running state\n-    response = overlordResource.getCompleteTasks(req);\n+    response = overlordResource.getCompleteTasks(null, req);\n     Assert.assertEquals(2, (((List) response.getEntity()).size()));\n+\n+    response = overlordResource.getCompleteTasks(1, req);\n+    Assert.assertEquals(1, (((List) response.getEntity()).size()));\n     taskMaster.stop();\n     Assert.assertFalse(taskMaster.isLeader());\n     EasyMock.verify(taskLockbox, taskActionClientFactory);\n@@ -287,7 +292,7 @@ public void testOverlordRun() throws Exception\n    * These method will not timeout until the condition is met so calling method should ensure timeout\n    * This method also assumes that the task with given taskId is present\n    * */\n-  private void waitForTaskStatus(String taskId, TaskStatus.Status status) throws InterruptedException\n+  private void waitForTaskStatus(String taskId, TaskState status) throws InterruptedException\n   {\n     while (true) {\n       Response response = overlordResource.getTaskStatus(taskId);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/overlord/http/OverlordTest.java",
                "sha": "b88556cf4fc3d2b386a7357ca4650e23bb72cf2d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java",
                "patch": "@@ -123,6 +123,12 @@ public SegmentIdentifier allocatePendingSegment(\n     throw new UnsupportedOperationException();\n   }\n \n+  @Override\n+  public int deletePendingSegments(String dataSource, Interval deleteInterval)\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n   @Override\n   public void deleteSegments(Set<DataSegment> segments)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java",
                "sha": "a89a3315b929e2cd15125df625c2389c6aa38416",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/worker/TaskAnnouncementTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/worker/TaskAnnouncementTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "indexing-service/src/test/java/io/druid/indexing/worker/TaskAnnouncementTest.java",
                "patch": "@@ -20,7 +20,7 @@\n package io.druid.indexing.worker;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n-import io.druid.indexing.common.TaskLocation;\n+import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TestUtils;\n import io.druid.indexing.common.task.RealtimeIndexTask;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/worker/TaskAnnouncementTest.java",
                "sha": "e344c05fe3346224653751f5cc246e66f743c7f4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/worker/WorkerTaskMonitorTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/worker/WorkerTaskMonitorTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 4,
                "filename": "indexing-service/src/test/java/io/druid/indexing/worker/WorkerTaskMonitorTest.java",
                "patch": "@@ -25,9 +25,9 @@\n import com.google.common.collect.Lists;\n import com.google.common.io.Files;\n import io.druid.curator.PotentiallyGzippedCompressionProvider;\n+import io.druid.indexer.TaskState;\n import io.druid.indexing.common.IndexingServiceCondition;\n import io.druid.indexing.common.SegmentLoaderFactory;\n-import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TaskToolboxFactory;\n import io.druid.indexing.common.TestRealtimeTask;\n import io.druid.indexing.common.TestTasks;\n@@ -263,7 +263,7 @@ public boolean isValid()\n     );\n \n     Assert.assertEquals(task.getId(), taskAnnouncement.getTaskStatus().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, taskAnnouncement.getTaskStatus().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, taskAnnouncement.getTaskStatus().getStatusCode());\n   }\n \n   @Test(timeout = 30_000L)\n@@ -299,7 +299,7 @@ public boolean isValid()\n     List<TaskAnnouncement> announcements = workerCuratorCoordinator.getAnnouncements();\n     Assert.assertEquals(1, announcements.size());\n     Assert.assertEquals(task.getId(), announcements.get(0).getTaskStatus().getId());\n-    Assert.assertEquals(TaskStatus.Status.SUCCESS, announcements.get(0).getTaskStatus().getStatusCode());\n+    Assert.assertEquals(TaskState.SUCCESS, announcements.get(0).getTaskStatus().getStatusCode());\n     Assert.assertEquals(DUMMY_NODE.getHost(), announcements.get(0).getTaskLocation().getHost());\n     Assert.assertEquals(DUMMY_NODE.getPlaintextPort(), announcements.get(0).getTaskLocation().getPort());\n   }\n@@ -337,7 +337,7 @@ public boolean isValid()\n     List<TaskAnnouncement> announcements = workerCuratorCoordinator.getAnnouncements();\n     Assert.assertEquals(1, announcements.size());\n     Assert.assertEquals(task.getId(), announcements.get(0).getTaskStatus().getId());\n-    Assert.assertEquals(TaskStatus.Status.FAILED, announcements.get(0).getTaskStatus().getStatusCode());\n+    Assert.assertEquals(TaskState.FAILED, announcements.get(0).getTaskStatus().getStatusCode());\n   }\n \n   @Test(timeout = 30_000L)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/indexing-service/src/test/java/io/druid/indexing/worker/WorkerTaskMonitorTest.java",
                "sha": "c34a2c6be849089f04b0d5e0922b37b535476560",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/integration-tests/src/main/java/io/druid/testing/clients/OverlordResourceTestClient.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/integration-tests/src/main/java/io/druid/testing/clients/OverlordResourceTestClient.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 6,
                "filename": "integration-tests/src/main/java/io/druid/testing/clients/OverlordResourceTestClient.java",
                "patch": "@@ -29,7 +29,7 @@\n import com.metamx.http.client.Request;\n import com.metamx.http.client.response.StatusResponseHandler;\n import com.metamx.http.client.response.StatusResponseHolder;\n-import io.druid.indexing.common.TaskStatus;\n+import io.druid.indexer.TaskState;\n import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.RetryUtils;\n import io.druid.java.util.common.StringUtils;\n@@ -117,7 +117,7 @@ public String call() throws Exception\n     }\n   }\n \n-  public TaskStatus.Status getTaskStatus(String taskID)\n+  public TaskState getTaskStatus(String taskID)\n   {\n     try {\n       StatusResponseHolder response = makeRequest(\n@@ -135,7 +135,7 @@ public String call() throws Exception\n       );\n       //TODO: figure out a better way to parse the response...\n       String status = (String) ((Map) responseData.get(\"status\")).get(\"status\");\n-      return TaskStatus.Status.valueOf(status);\n+      return TaskState.valueOf(status);\n     }\n     catch (Exception e) {\n       throw Throwables.propagate(e);\n@@ -189,11 +189,11 @@ public void waitUntilTaskCompletes(final String taskID, final int millisEach, fi\n           @Override\n           public Boolean call() throws Exception\n           {\n-            TaskStatus.Status status = getTaskStatus(taskID);\n-            if (status == TaskStatus.Status.FAILED) {\n+            TaskState status = getTaskStatus(taskID);\n+            if (status == TaskState.FAILED) {\n               throw new ISE(\"Indexer task FAILED\");\n             }\n-            return status == TaskStatus.Status.SUCCESS;\n+            return status == TaskState.SUCCESS;\n           }\n         },\n         true,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/integration-tests/src/main/java/io/druid/testing/clients/OverlordResourceTestClient.java",
                "sha": "f47d97e4f5361b1956e72e0b2cd0e0711cfa2085",
                "status": "modified"
            },
            {
                "additions": 87,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/client/indexing/IndexingServiceClient.java",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/client/indexing/IndexingServiceClient.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/client/indexing/IndexingServiceClient.java",
                "patch": "@@ -19,18 +19,31 @@\n \n package io.druid.client.indexing;\n \n+import com.fasterxml.jackson.core.type.TypeReference;\n import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n import com.google.common.base.Throwables;\n import com.google.inject.Inject;\n+import com.metamx.http.client.response.FullResponseHolder;\n import io.druid.discovery.DruidLeaderClient;\n+import io.druid.indexer.TaskStatusPlus;\n+import io.druid.java.util.common.DateTimes;\n import io.druid.java.util.common.IAE;\n+import io.druid.java.util.common.ISE;\n+import io.druid.java.util.common.StringUtils;\n+import io.druid.java.util.common.jackson.JacksonUtils;\n import io.druid.timeline.DataSegment;\n import org.jboss.netty.handler.codec.http.HttpMethod;\n+import org.jboss.netty.handler.codec.http.HttpResponseStatus;\n+import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n+import javax.annotation.Nullable;\n import javax.ws.rs.core.MediaType;\n+import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n public class IndexingServiceClient\n {\n@@ -80,6 +93,80 @@ public void upgradeSegments(String dataSource, Interval interval)\n     runQuery(new ClientConversionQuery(dataSource, interval));\n   }\n \n+  public List<TaskStatusPlus> getRunningTasks()\n+  {\n+    return getTasks(\"runningTasks\");\n+  }\n+\n+  public List<TaskStatusPlus> getPendingTasks()\n+  {\n+    return getTasks(\"pendingTasks\");\n+  }\n+\n+  public List<TaskStatusPlus> getWaitingTasks()\n+  {\n+    return getTasks(\"waitingTasks\");\n+  }\n+\n+  private List<TaskStatusPlus> getTasks(String endpointSuffix)\n+  {\n+    try {\n+      final FullResponseHolder responseHolder = druidLeaderClient.go(\n+          druidLeaderClient.makeRequest(HttpMethod.GET, StringUtils.format(\"/druid/indexer/v1/%s\", endpointSuffix))\n+      );\n+\n+      if (!responseHolder.getStatus().equals(HttpResponseStatus.OK)) {\n+        throw new ISE(\"Error while fetching the status of the last complete task\");\n+      }\n+\n+      return jsonMapper.readValue(\n+          responseHolder.getContent(),\n+          new TypeReference<List<TaskStatusPlus>>()\n+          {\n+          }\n+      );\n+    }\n+    catch (IOException | InterruptedException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Nullable\n+  public TaskStatusPlus getLastCompleteTask()\n+  {\n+    final List<TaskStatusPlus> completeTaskStatuses = getTasks(\"completeTasks?n=1\");\n+    return completeTaskStatuses.isEmpty() ? null : completeTaskStatuses.get(0);\n+  }\n+\n+  public int killPendingSegments(String dataSource, DateTime end)\n+  {\n+    final String endPoint = StringUtils.format(\n+        \"/druid/indexer/v1/pendingSegments/%s?interval=%s\",\n+        dataSource,\n+        new Interval(DateTimes.MIN, end)\n+    );\n+    try {\n+      final FullResponseHolder responseHolder = druidLeaderClient.go(\n+          druidLeaderClient.makeRequest(HttpMethod.DELETE, endPoint)\n+      );\n+\n+      if (!responseHolder.getStatus().equals(HttpResponseStatus.OK)) {\n+        throw new ISE(\"Error while killing pendingSegments of dataSource[%s] created until [%s]\", dataSource, end);\n+      }\n+\n+      final Map<String, Object> resultMap = jsonMapper.readValue(\n+          responseHolder.getContent(),\n+          JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT\n+      );\n+\n+      final Object numDeletedObject = resultMap.get(\"numDeleted\");\n+      return (Integer) Preconditions.checkNotNull(numDeletedObject, \"numDeletedObject\");\n+    }\n+    catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n   private void runQuery(Object queryObject)\n   {\n     try {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/client/indexing/IndexingServiceClient.java",
                "sha": "39aa1e037df30da7e639ef618f5099b3c04a1878",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 6,
                "filename": "server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "patch": "@@ -44,7 +44,6 @@\n import io.druid.metadata.SQLMetadataSegmentManagerProvider;\n import io.druid.metadata.SQLMetadataSegmentPublisher;\n import io.druid.metadata.SQLMetadataSegmentPublisherProvider;\n-import io.druid.metadata.SQLMetadataStorageActionHandlerFactory;\n import io.druid.metadata.SQLMetadataSupervisorManager;\n import io.druid.server.audit.AuditManagerProvider;\n import io.druid.server.audit.SQLAuditManager;\n@@ -121,11 +120,6 @@ public void configure(Binder binder)\n             .to(SQLMetadataSegmentPublisherProvider.class)\n             .in(LazySingleton.class);\n \n-    PolyBind.optionBinder(binder, Key.get(MetadataStorageActionHandlerFactory.class))\n-            .addBinding(type)\n-            .to(SQLMetadataStorageActionHandlerFactory.class)\n-            .in(LazySingleton.class);\n-\n     PolyBind.optionBinder(binder, Key.get(IndexerMetadataStorageCoordinator.class))\n             .addBinding(type)\n             .to(IndexerSQLMetadataStorageCoordinator.class)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "sha": "5eef6967afe37a9a120b30c802f9d346eac208a7",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java",
                "patch": "@@ -96,6 +96,17 @@ SegmentIdentifier allocatePendingSegment(\n       boolean skipSegmentLineageCheck\n   ) throws IOException;\n \n+  /**\n+   * Delete pending segments created in the given interval for the given dataSource from the pending segments table.\n+   * The {@code created_date} field of the pending segments table is checked to find segments to be deleted.\n+   *\n+   * @param dataSource     dataSource\n+   * @param deleteInterval interval to check the {@code created_date} of pendingSegments\n+   *\n+   * @return number of deleted pending segments\n+   */\n+  int deletePendingSegments(String dataSource, Interval deleteInterval);\n+\n   /**\n    * Attempts to insert a set of segments to the metadata storage. Returns the set of segments actually added (segments\n    * with identifiers already in the metadata storage will not be added).\n@@ -121,6 +132,9 @@ SegmentPublishResult announceHistoricalSegments(\n       DataSourceMetadata endMetadata\n   ) throws IOException;\n \n+  /**\n+   * Read dataSource metadata. Returns null if there is no metadata.\n+   */\n   DataSourceMetadata getDataSourceMetadata(String dataSource);\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java",
                "sha": "17917d009e846d74ab7fafe3ef759c4a4b1c2495",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandler.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandler.java",
                "patch": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.java.util.common.StringUtils;\n+import org.joda.time.DateTime;\n+import org.skife.jdbi.v2.Handle;\n+import org.skife.jdbi.v2.Query;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+\n+public class DerbyMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+    extends SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+{\n+  DerbyMetadataStorageActionHandler(\n+      SQLMetadataConnector connector,\n+      ObjectMapper jsonMapper,\n+      MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> types,\n+      String entryTypeName,\n+      String entryTable,\n+      String logTable,\n+      String lockTable\n+  )\n+  {\n+    super(connector, jsonMapper, types, entryTypeName, entryTable, logTable, lockTable);\n+  }\n+\n+  @Override\n+  protected Query<Map<String, Object>> createInactiveStatusesSinceQuery(\n+      Handle handle, DateTime timestamp, @Nullable Integer maxNumStatuses\n+  )\n+  {\n+    String sql = StringUtils.format(\n+        \"SELECT \"\n+        + \"  id, \"\n+        + \"  status_payload \"\n+        + \"FROM \"\n+        + \"  %s \"\n+        + \"WHERE \"\n+        + \"  active = FALSE AND created_date >= :start \"\n+        + \"ORDER BY created_date DESC\",\n+        getEntryTable()\n+    );\n+\n+    if (maxNumStatuses != null) {\n+      sql += \" FETCH FIRST :n ROWS ONLY\";\n+    }\n+\n+    Query<Map<String, Object>> query = handle.createQuery(sql).bind(\"start\", timestamp.toString());\n+\n+    if (maxNumStatuses != null) {\n+      query = query.bind(\"n\", maxNumStatuses);\n+    }\n+    return query;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandler.java",
                "sha": "62acf4965ee457ea778f7ca79ac32d414d56ea51",
                "status": "added"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandlerFactory.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandlerFactory.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandlerFactory.java",
                "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.inject.Inject;\n+\n+public class DerbyMetadataStorageActionHandlerFactory extends SQLMetadataStorageActionHandlerFactory\n+{\n+  @Inject\n+  public DerbyMetadataStorageActionHandlerFactory(\n+      SQLMetadataConnector connector,\n+      MetadataStorageTablesConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    super(connector, config, jsonMapper);\n+  }\n+\n+  @Override\n+  public <A, B, C, D> MetadataStorageActionHandler<A, B, C, D> create(\n+      final String entryType,\n+      MetadataStorageActionHandlerTypes<A, B, C, D> payloadTypes\n+  )\n+  {\n+    return new DerbyMetadataStorageActionHandler<>(\n+        connector,\n+        jsonMapper,\n+        payloadTypes,\n+        entryType,\n+        config.getEntryTable(entryType),\n+        config.getLogTable(entryType),\n+        config.getLockTable(entryType)\n+    );\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/DerbyMetadataStorageActionHandlerFactory.java",
                "sha": "ba38cec1528febd6142f975084c43f6578b14f46",
                "status": "added"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "patch": "@@ -387,7 +387,7 @@ public SegmentIdentifier allocatePendingSegment(\n       final Interval interval,\n       final String maxVersion,\n       final boolean skipSegmentLineageCheck\n-  ) throws IOException\n+  )\n   {\n     Preconditions.checkNotNull(dataSource, \"dataSource\");\n     Preconditions.checkNotNull(sequenceName, \"sequenceName\");\n@@ -614,6 +614,24 @@ public SegmentIdentifier inTransaction(Handle handle, TransactionStatus transact\n     );\n   }\n \n+  @Override\n+  public int deletePendingSegments(String dataSource, Interval deleteInterval)\n+  {\n+    return connector.getDBI().inTransaction(\n+        (handle, status) -> handle\n+            .createStatement(\n+                StringUtils.format(\n+                    \"delete from %s where datasource = :dataSource and created_date >= :start and created_date < :end\",\n+                    dbTables.getPendingSegmentsTable()\n+                )\n+            )\n+            .bind(\"dataSource\", dataSource)\n+            .bind(\"start\", deleteInterval.getStart().toString())\n+            .bind(\"end\", deleteInterval.getEnd().toString())\n+            .execute()\n+    );\n+  }\n+\n   /**\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "sha": "52067ab960c4754241daad74361802cc2a9c2609",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandler.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandler.java",
                "patch": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.java.util.common.StringUtils;\n+import org.joda.time.DateTime;\n+import org.skife.jdbi.v2.Handle;\n+import org.skife.jdbi.v2.Query;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+\n+public class MySQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+    extends SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+{\n+  MySQLMetadataStorageActionHandler(\n+      SQLMetadataConnector connector,\n+      ObjectMapper jsonMapper,\n+      MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> types,\n+      String entryTypeName,\n+      String entryTable,\n+      String logTable,\n+      String lockTable\n+  )\n+  {\n+    super(connector, jsonMapper, types, entryTypeName, entryTable, logTable, lockTable);\n+  }\n+\n+  @Override\n+  protected Query<Map<String, Object>> createInactiveStatusesSinceQuery(\n+      Handle handle, DateTime timestamp, @Nullable Integer maxNumStatuses\n+  )\n+  {\n+    String sql = StringUtils.format(\n+        \"SELECT \"\n+        + \"  id, \"\n+        + \"  status_payload \"\n+        + \"FROM \"\n+        + \"  %s \"\n+        + \"WHERE \"\n+        + \"  active = FALSE AND created_date >= :start \"\n+        + \"ORDER BY created_date DESC\",\n+        getEntryTable()\n+    );\n+\n+    if (maxNumStatuses != null) {\n+      sql += \" LIMIT :n\";\n+    }\n+\n+    Query<Map<String, Object>> query = handle.createQuery(sql).bind(\"start\", timestamp.toString());\n+\n+    if (maxNumStatuses != null) {\n+      query = query.bind(\"n\", maxNumStatuses);\n+    }\n+    return query;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandler.java",
                "sha": "770d980983c8e256e47d2217e8fc6c8405b5e713",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandlerFactory.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandlerFactory.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandlerFactory.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.inject.Inject;\n+\n+public class MySQLMetadataStorageActionHandlerFactory extends SQLMetadataStorageActionHandlerFactory\n+{\n+  @Inject\n+  public MySQLMetadataStorageActionHandlerFactory(\n+      SQLMetadataConnector connector,\n+      MetadataStorageTablesConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    super(connector, config, jsonMapper);\n+  }\n+\n+  @Override\n+  public <EntryType, StatusType, LogType, LockType> MetadataStorageActionHandler<EntryType, StatusType, LogType, LockType> create(\n+      String entryType, MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> payloadTypes\n+  )\n+  {\n+    return new MySQLMetadataStorageActionHandler<>(\n+        connector,\n+        jsonMapper,\n+        payloadTypes,\n+        entryType,\n+        config.getEntryTable(entryType),\n+        config.getLogTable(entryType),\n+        config.getLockTable(entryType)\n+    );\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/MySQLMetadataStorageActionHandlerFactory.java",
                "sha": "0f1742c2d9a7fe3ec606dd15124c25161b9ad63b",
                "status": "added"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandler.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandler.java",
                "patch": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.java.util.common.StringUtils;\n+import org.joda.time.DateTime;\n+import org.skife.jdbi.v2.Handle;\n+import org.skife.jdbi.v2.Query;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+\n+public class PostgreSQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+    extends SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+{\n+  public PostgreSQLMetadataStorageActionHandler(\n+      SQLMetadataConnector connector,\n+      ObjectMapper jsonMapper,\n+      MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> types,\n+      String entryTypeName,\n+      String entryTable,\n+      String logTable,\n+      String lockTable\n+  )\n+  {\n+    super(connector, jsonMapper, types, entryTypeName, entryTable, logTable, lockTable);\n+  }\n+\n+  @Override\n+  protected Query<Map<String, Object>> createInactiveStatusesSinceQuery(\n+      Handle handle,\n+      DateTime timestamp,\n+      @Nullable Integer maxNumStatuses\n+  )\n+  {\n+    String sql = StringUtils.format(\n+        \"SELECT \"\n+        + \"  id, \"\n+        + \"  status_payload \"\n+        + \"FROM \"\n+        + \"  %s \"\n+        + \"WHERE \"\n+        + \"  active = FALSE AND created_date >= :start \"\n+        + \"ORDER BY created_date DESC\",\n+        getEntryTable()\n+    );\n+\n+    if (maxNumStatuses != null) {\n+      sql += \" LIMIT :n\";\n+    }\n+\n+    Query<Map<String, Object>> query = handle.createQuery(sql).bind(\"start\", timestamp.toString());\n+\n+    if (maxNumStatuses != null) {\n+      query = query.bind(\"n\", maxNumStatuses);\n+    }\n+    return query;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandler.java",
                "sha": "d1a2dfb0af3c94a71b20c5d6213be1f6c1fe936a",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandlerFactory.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandlerFactory.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandlerFactory.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.inject.Inject;\n+\n+public class PostgreSQLMetadataStorageActionHandlerFactory extends SQLMetadataStorageActionHandlerFactory\n+{\n+  @Inject\n+  public PostgreSQLMetadataStorageActionHandlerFactory(\n+      SQLMetadataConnector connector,\n+      MetadataStorageTablesConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    super(connector, config, jsonMapper);\n+  }\n+\n+  @Override\n+  public <EntryType, StatusType, LogType, LockType> MetadataStorageActionHandler<EntryType, StatusType, LogType, LockType> create(\n+      String entryType, MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> payloadTypes\n+  )\n+  {\n+    return new PostgreSQLMetadataStorageActionHandler<>(\n+        connector,\n+        jsonMapper,\n+        payloadTypes,\n+        entryType,\n+        config.getEntryTable(entryType),\n+        config.getLogTable(entryType),\n+        config.getLockTable(entryType)\n+    );\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/PostgreSQLMetadataStorageActionHandlerFactory.java",
                "sha": "6a802761707cc60d960767316e90f62a91366f66",
                "status": "added"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 36,
                "filename": "server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "patch": "@@ -28,12 +28,14 @@\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.metamx.emitter.EmittingLogger;\n+import io.druid.java.util.common.DateTimes;\n import io.druid.java.util.common.Pair;\n import io.druid.java.util.common.StringUtils;\n import org.joda.time.DateTime;\n import org.skife.jdbi.v2.FoldController;\n import org.skife.jdbi.v2.Folder3;\n import org.skife.jdbi.v2.Handle;\n+import org.skife.jdbi.v2.Query;\n import org.skife.jdbi.v2.StatementContext;\n import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n import org.skife.jdbi.v2.exceptions.StatementException;\n@@ -49,7 +51,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n \n-public class SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+public abstract class SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n     implements MetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n {\n   private static final EmittingLogger log = new EmittingLogger(SQLMetadataStorageActionHandler.class);\n@@ -88,6 +90,26 @@ public SQLMetadataStorageActionHandler(\n     this.lockTable = lockTable;\n   }\n \n+  protected SQLMetadataConnector getConnector()\n+  {\n+    return connector;\n+  }\n+\n+  protected ObjectMapper getJsonMapper()\n+  {\n+    return jsonMapper;\n+  }\n+\n+  protected TypeReference getStatusType()\n+  {\n+    return statusType;\n+  }\n+\n+  protected String getEntryTable()\n+  {\n+    return entryTable;\n+  }\n+\n   @Override\n   public void insert(\n       final String id,\n@@ -268,47 +290,61 @@ public Boolean withHandle(Handle handle) throws Exception\n   }\n \n   @Override\n-  public List<StatusType> getInactiveStatusesSince(final DateTime timestamp)\n+  public List<StatusType> getInactiveStatusesSince(DateTime timestamp, @Nullable Integer maxNumStatuses)\n   {\n-    return connector.retryWithHandle(\n-        new HandleCallback<List<StatusType>>()\n-        {\n-          @Override\n-          public List<StatusType> withHandle(Handle handle) throws Exception\n-          {\n-            return handle\n-                .createQuery(\n-                    StringUtils.format(\n-                        \"SELECT id, status_payload FROM %s WHERE active = FALSE AND created_date >= :start ORDER BY created_date DESC\",\n-                        entryTable\n-                    )\n-                ).bind(\"start\", timestamp.toString())\n-                .map(\n-                    new ResultSetMapper<StatusType>()\n-                    {\n-                      @Override\n-                      public StatusType map(int index, ResultSet r, StatementContext ctx) throws SQLException\n-                      {\n-                        try {\n-                          return jsonMapper.readValue(\n-                              r.getBytes(\"status_payload\"),\n-                              statusType\n-                          );\n-                        }\n-                        catch (IOException e) {\n-                          log.makeAlert(e, \"Failed to parse status payload\")\n-                             .addData(\"entry\", r.getString(\"id\"))\n-                             .emit();\n-                          throw new SQLException(e);\n-                        }\n-                      }\n+    return getConnector().retryWithHandle(\n+        handle -> {\n+          final Query<Map<String, Object>> query = createInactiveStatusesSinceQuery(handle, timestamp, maxNumStatuses);\n+\n+          return query\n+              .map(\n+                  (ResultSetMapper<StatusType>) (index, r, ctx) -> {\n+                    try {\n+                      return getJsonMapper().readValue(\n+                          r.getBytes(\"status_payload\"),\n+                          getStatusType()\n+                      );\n                     }\n-                ).list();\n-          }\n+                    catch (IOException e) {\n+                      log.makeAlert(e, \"Failed to parse status payload\")\n+                         .addData(\"entry\", r.getString(\"id\"))\n+                         .emit();\n+                      throw new SQLException(e);\n+                    }\n+                  }\n+              ).list();\n         }\n     );\n   }\n \n+  protected abstract Query<Map<String, Object>> createInactiveStatusesSinceQuery(\n+      Handle handle,\n+      DateTime timestamp,\n+      @Nullable Integer maxNumStatuses\n+  );\n+\n+  @Override\n+  @Nullable\n+  public Pair<DateTime, String> getCreatedDateAndDataSource(String entryId)\n+  {\n+    return connector.retryWithHandle(\n+        handle -> handle\n+        .createQuery(\n+            StringUtils.format(\n+                \"SELECT created_date, datasource FROM %s WHERE id = :entryId\",\n+                entryTable\n+            )\n+        )\n+        .bind(\"entryId\", entryId)\n+        .map(\n+            (index, resultSet, ctx) -> Pair.of(\n+                DateTimes.of(resultSet.getString(\"created_date\")), resultSet.getString(\"datasource\")\n+            )\n+        )\n+        .first()\n+    );\n+  }\n+\n   @Override\n   public boolean addLock(final String entryId, final LockType lock)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "sha": "90b508f05052819d3efa1f9c1aebf0b6595613fc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandlerFactory.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandlerFactory.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 21,
                "filename": "server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandlerFactory.java",
                "patch": "@@ -22,11 +22,11 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.inject.Inject;\n \n-public class SQLMetadataStorageActionHandlerFactory implements MetadataStorageActionHandlerFactory\n+public abstract class SQLMetadataStorageActionHandlerFactory implements MetadataStorageActionHandlerFactory\n {\n-  private final SQLMetadataConnector connector;\n-  private final MetadataStorageTablesConfig config;\n-  private final ObjectMapper jsonMapper;\n+  protected final SQLMetadataConnector connector;\n+  protected final MetadataStorageTablesConfig config;\n+  protected final ObjectMapper jsonMapper;\n \n   @Inject\n   public SQLMetadataStorageActionHandlerFactory(\n@@ -39,21 +39,4 @@ public SQLMetadataStorageActionHandlerFactory(\n     this.config = config;\n     this.jsonMapper = jsonMapper;\n   }\n-\n-  @Override\n-  public <A, B, C, D> MetadataStorageActionHandler<A, B, C, D> create(\n-      final String entryType,\n-      MetadataStorageActionHandlerTypes<A, B, C, D> payloadTypes\n-  )\n-  {\n-    return new SQLMetadataStorageActionHandler<>(\n-        connector,\n-        jsonMapper,\n-        payloadTypes,\n-        entryType,\n-        config.getEntryTable(entryType),\n-        config.getLogTable(entryType),\n-        config.getLockTable(entryType)\n-    );\n-  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandlerFactory.java",
                "sha": "bdbe7aa5d6e4507349d2886392f283f64c7030cc",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandler.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandler.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandler.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.java.util.common.StringUtils;\n+import org.joda.time.DateTime;\n+import org.skife.jdbi.v2.Handle;\n+import org.skife.jdbi.v2.Query;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+\n+public class SQLServerMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+    extends SQLMetadataStorageActionHandler<EntryType, StatusType, LogType, LockType>\n+{\n+  public SQLServerMetadataStorageActionHandler(\n+      SQLMetadataConnector connector,\n+      ObjectMapper jsonMapper,\n+      MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> types,\n+      String entryTypeName,\n+      String entryTable,\n+      String logTable,\n+      String lockTable\n+  )\n+  {\n+    super(connector, jsonMapper, types, entryTypeName, entryTable, logTable, lockTable);\n+  }\n+\n+  @Override\n+  protected Query<Map<String, Object>> createInactiveStatusesSinceQuery(\n+      Handle handle, DateTime timestamp, @Nullable Integer maxNumStatuses\n+  )\n+  {\n+    String sql = maxNumStatuses == null ? \"SELECT \" : \"SELECT TOP :n \";\n+\n+    sql += StringUtils.format(\n+        \"    id, \"\n+        + \"  status_payload \"\n+        + \"FROM \"\n+        + \"  %s \"\n+        + \"WHERE \"\n+        + \"  active = FALSE AND created_date >= :start \"\n+        + \"ORDER BY created_date DESC\",\n+        getEntryTable()\n+    );\n+\n+    Query<Map<String, Object>> query = handle.createQuery(sql).bind(\"start\", timestamp.toString());\n+\n+    if (maxNumStatuses != null) {\n+      query = query.bind(\"n\", maxNumStatuses);\n+    }\n+    return query;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandler.java",
                "sha": "199adff5656f540ed54d6c07175fe994d987cd92",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandlerFactory.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandlerFactory.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandlerFactory.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.inject.Inject;\n+\n+public class SQLServerMetadataStorageActionHandlerFactory extends SQLMetadataStorageActionHandlerFactory\n+{\n+  @Inject\n+  public SQLServerMetadataStorageActionHandlerFactory(\n+      SQLMetadataConnector connector,\n+      MetadataStorageTablesConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    super(connector, config, jsonMapper);\n+  }\n+\n+  @Override\n+  public <EntryType, StatusType, LogType, LockType> MetadataStorageActionHandler<EntryType, StatusType, LogType, LockType> create(\n+      String entryType, MetadataStorageActionHandlerTypes<EntryType, StatusType, LogType, LockType> payloadTypes\n+  )\n+  {\n+    return new SQLServerMetadataStorageActionHandler<>(\n+        connector,\n+        jsonMapper,\n+        payloadTypes,\n+        entryType,\n+        config.getEntryTable(entryType),\n+        config.getLogTable(entryType),\n+        config.getLockTable(entryType)\n+    );\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/SQLServerMetadataStorageActionHandlerFactory.java",
                "sha": "0704fea3428741600f335a8442d98c6e024995bb",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/storage/derby/DerbyMetadataStorageDruidModule.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/storage/derby/DerbyMetadataStorageDruidModule.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "server/src/main/java/io/druid/metadata/storage/derby/DerbyMetadataStorageDruidModule.java",
                "patch": "@@ -24,21 +24,23 @@\n import io.druid.guice.LazySingleton;\n import io.druid.guice.PolyBind;\n import io.druid.guice.SQLMetadataStorageDruidModule;\n+import io.druid.metadata.DerbyMetadataStorageActionHandlerFactory;\n import io.druid.metadata.MetadataStorage;\n+import io.druid.metadata.MetadataStorageActionHandlerFactory;\n import io.druid.metadata.MetadataStorageConnector;\n import io.druid.metadata.MetadataStorageProvider;\n import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n \n public class DerbyMetadataStorageDruidModule extends SQLMetadataStorageDruidModule\n {\n+  public static final String TYPE = \"derby\";\n+\n   public DerbyMetadataStorageDruidModule()\n   {\n     super(TYPE);\n   }\n \n-  public static final String TYPE = \"derby\";\n-\n   @Override\n   public void configure(Binder binder)\n   {\n@@ -61,5 +63,10 @@ public void configure(Binder binder)\n             .addBinding(TYPE)\n             .to(DerbyConnector.class)\n             .in(LazySingleton.class);\n+\n+    PolyBind.optionBinder(binder, Key.get(MetadataStorageActionHandlerFactory.class))\n+            .addBinding(TYPE)\n+            .to(DerbyMetadataStorageActionHandlerFactory.class)\n+            .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/metadata/storage/derby/DerbyMetadataStorageDruidModule.java",
                "sha": "850d7d688372fc0e802bb17168ee60f004c256f8",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/coordinator/CoordinatorDynamicConfig.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 24,
                "filename": "server/src/main/java/io/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "patch": "@@ -25,6 +25,7 @@\n \n import java.util.Collection;\n import java.util.HashSet;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n \n@@ -42,6 +43,9 @@\n   private final boolean emitBalancingStats;\n   private final boolean killAllDataSources;\n   private final Set<String> killDataSourceWhitelist;\n+\n+  // The pending segments of the dataSources in this list are not killed.\n+  private final Set<String> killPendingSegmentsSkipList;\n   /**\n    * The maximum number of segments that could be queued for loading to any given server.\n    * Default values is 0 with the meaning of \"unbounded\" (any number of\n@@ -65,6 +69,7 @@ public CoordinatorDynamicConfig(\n       // coordinator console can not send array of strings in the update request.\n       // See https://github.com/druid-io/druid/issues/3055\n       @JsonProperty(\"killDataSourceWhitelist\") Object killDataSourceWhitelist,\n+      @JsonProperty(\"killPendingSegmentsSkipList\") Object killPendingSegmentsSkipList,\n       @JsonProperty(\"killAllDataSources\") boolean killAllDataSources,\n       @JsonProperty(\"maxSegmentsInNodeLoadingQueue\") int maxSegmentsInNodeLoadingQueue\n   )\n@@ -78,18 +83,19 @@ public CoordinatorDynamicConfig(\n     this.balancerComputeThreads = Math.max(balancerComputeThreads, 1);\n     this.emitBalancingStats = emitBalancingStats;\n     this.killAllDataSources = killAllDataSources;\n-    this.killDataSourceWhitelist = parseKillDataSourceWhitelist(killDataSourceWhitelist);\n+    this.killDataSourceWhitelist = parseJsonStringOrArray(killDataSourceWhitelist);\n+    this.killPendingSegmentsSkipList = parseJsonStringOrArray(killPendingSegmentsSkipList);\n     this.maxSegmentsInNodeLoadingQueue = maxSegmentsInNodeLoadingQueue;\n \n     if (this.killAllDataSources && !this.killDataSourceWhitelist.isEmpty()) {\n       throw new IAE(\"can't have killAllDataSources and non-empty killDataSourceWhitelist\");\n     }\n   }\n \n-  private Set<String> parseKillDataSourceWhitelist(Object killDataSourceWhitelist)\n+  private static Set<String> parseJsonStringOrArray(Object jsonStringOrArray)\n   {\n-    if (killDataSourceWhitelist instanceof String) {\n-      String[] list = ((String) killDataSourceWhitelist).split(\",\");\n+    if (jsonStringOrArray instanceof String) {\n+      String[] list = ((String) jsonStringOrArray).split(\",\");\n       Set<String> result = new HashSet<>();\n       for (String item : list) {\n         String trimmed = item.trim();\n@@ -98,8 +104,8 @@ public CoordinatorDynamicConfig(\n         }\n       }\n       return result;\n-    } else if (killDataSourceWhitelist instanceof Collection) {\n-      return ImmutableSet.copyOf(((Collection) killDataSourceWhitelist));\n+    } else if (jsonStringOrArray instanceof Collection) {\n+      return ImmutableSet.copyOf(((Collection) jsonStringOrArray));\n     } else {\n       return ImmutableSet.of();\n     }\n@@ -159,6 +165,12 @@ public int getBalancerComputeThreads()\n     return killDataSourceWhitelist;\n   }\n \n+  @JsonProperty\n+  public Set<String> getKillPendingSegmentsSkipList()\n+  {\n+    return killPendingSegmentsSkipList;\n+  }\n+\n   @JsonProperty\n   public boolean isKillAllDataSources()\n   {\n@@ -184,6 +196,7 @@ public String toString()\n            \", balancerComputeThreads=\" + balancerComputeThreads +\n            \", emitBalancingStats=\" + emitBalancingStats +\n            \", killDataSourceWhitelist=\" + killDataSourceWhitelist +\n+           \", killPendingSegmentsSkipList=\" + killPendingSegmentsSkipList +\n            \", killAllDataSources=\" + killAllDataSources +\n            \", maxSegmentsInNodeLoadingQueue=\" + maxSegmentsInNodeLoadingQueue +\n            '}';\n@@ -231,27 +244,30 @@ public boolean equals(Object o)\n     if (maxSegmentsInNodeLoadingQueue != that.maxSegmentsInNodeLoadingQueue) {\n       return false;\n     }\n-    return !(killDataSourceWhitelist != null\n-             ? !killDataSourceWhitelist.equals(that.killDataSourceWhitelist)\n-             : that.killDataSourceWhitelist != null);\n+    if (!Objects.equals(killDataSourceWhitelist, that.killDataSourceWhitelist)) {\n+      return false;\n+    }\n \n+    return Objects.equals(killPendingSegmentsSkipList, that.killPendingSegmentsSkipList);\n   }\n \n   @Override\n   public int hashCode()\n   {\n-    int result = (int) (millisToWaitBeforeDeleting ^ (millisToWaitBeforeDeleting >>> 32));\n-    result = 31 * result + (int) (mergeBytesLimit ^ (mergeBytesLimit >>> 32));\n-    result = 31 * result + mergeSegmentsLimit;\n-    result = 31 * result + maxSegmentsToMove;\n-    result = 31 * result + replicantLifetime;\n-    result = 31 * result + replicationThrottleLimit;\n-    result = 31 * result + balancerComputeThreads;\n-    result = 31 * result + (emitBalancingStats ? 1 : 0);\n-    result = 31 * result + (killAllDataSources ? 1 : 0);\n-    result = 31 * result + (killDataSourceWhitelist != null ? killDataSourceWhitelist.hashCode() : 0);\n-    result = 31 * result + maxSegmentsInNodeLoadingQueue;\n-    return result;\n+    return Objects.hash(\n+        millisToWaitBeforeDeleting,\n+        mergeBytesLimit,\n+        mergeSegmentsLimit,\n+        maxSegmentsToMove,\n+        replicantLifetime,\n+        replicationThrottleLimit,\n+        balancerComputeThreads,\n+        emitBalancingStats,\n+        killAllDataSources,\n+        maxSegmentsInNodeLoadingQueue,\n+        killDataSourceWhitelist,\n+        killPendingSegmentsSkipList\n+    );\n   }\n \n   public static Builder builder()\n@@ -281,6 +297,7 @@ public static Builder builder()\n     private Boolean emitBalancingStats;\n     private Integer balancerComputeThreads;\n     private Object killDataSourceWhitelist;\n+    private Object killPendingSegmentsSkipList;\n     private Boolean killAllDataSources;\n     private Integer maxSegmentsInNodeLoadingQueue;\n \n@@ -299,6 +316,7 @@ public Builder(\n         @JsonProperty(\"balancerComputeThreads\") Integer balancerComputeThreads,\n         @JsonProperty(\"emitBalancingStats\") Boolean emitBalancingStats,\n         @JsonProperty(\"killDataSourceWhitelist\") Object killDataSourceWhitelist,\n+        @JsonProperty(\"killPendingSegmentsSkipList\") Object killPendingSegmentsSkipList,\n         @JsonProperty(\"killAllDataSources\") Boolean killAllDataSources,\n         @JsonProperty(\"maxSegmentsInNodeLoadingQueue\") Integer maxSegmentsInNodeLoadingQueue\n     )\n@@ -313,6 +331,7 @@ public Builder(\n       this.emitBalancingStats = emitBalancingStats;\n       this.killAllDataSources = killAllDataSources;\n       this.killDataSourceWhitelist = killDataSourceWhitelist;\n+      this.killPendingSegmentsSkipList = killPendingSegmentsSkipList;\n       this.maxSegmentsInNodeLoadingQueue = maxSegmentsInNodeLoadingQueue;\n     }\n \n@@ -382,9 +401,6 @@ public Builder withMaxSegmentsInNodeLoadingQueue(int maxSegmentsInNodeLoadingQue\n       return this;\n     }\n \n-\n-\n-\n     public CoordinatorDynamicConfig build()\n     {\n       return new CoordinatorDynamicConfig(\n@@ -397,6 +413,7 @@ public CoordinatorDynamicConfig build()\n           balancerComputeThreads == null ? DEFAULT_BALANCER_COMPUTE_THREADS : balancerComputeThreads,\n           emitBalancingStats == null ? DEFAULT_EMIT_BALANCING_STATS : emitBalancingStats,\n           killDataSourceWhitelist,\n+          killPendingSegmentsSkipList,\n           killAllDataSources == null ? DEFAULT_KILL_ALL_DATA_SOURCES : killAllDataSources,\n           maxSegmentsInNodeLoadingQueue == null ? DEFAULT_MAX_SEGMENTS_IN_NODE_LOADING_QUEUE : maxSegmentsInNodeLoadingQueue\n       );\n@@ -414,6 +431,7 @@ public CoordinatorDynamicConfig build(CoordinatorDynamicConfig defaults)\n           balancerComputeThreads == null ? defaults.getBalancerComputeThreads() : balancerComputeThreads,\n           emitBalancingStats == null ? defaults.emitBalancingStats() : emitBalancingStats,\n           killDataSourceWhitelist == null ? defaults.getKillDataSourceWhitelist() : killDataSourceWhitelist,\n+          killPendingSegmentsSkipList == null ? defaults.getKillPendingSegmentsSkipList() : killPendingSegmentsSkipList,\n           killAllDataSources == null ? defaults.isKillAllDataSources() : killAllDataSources,\n           maxSegmentsInNodeLoadingQueue == null ? defaults.getMaxSegmentsInNodeLoadingQueue() : maxSegmentsInNodeLoadingQueue\n       );",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "sha": "65bea6a5848c7d398fd0b79bf5758379aa2460b5",
                "status": "modified"
            },
            {
                "additions": 104,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorCleanupPendingSegments.java",
                "changes": 104,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorCleanupPendingSegments.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/server/coordinator/DruidCoordinatorCleanupPendingSegments.java",
                "patch": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package io.druid.server.coordinator;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.inject.Inject;\n+import io.druid.client.ImmutableDruidDataSource;\n+import io.druid.client.indexing.IndexingServiceClient;\n+import io.druid.indexer.TaskStatusPlus;\n+import io.druid.java.util.common.DateTimes;\n+import io.druid.java.util.common.guava.Comparators;\n+import io.druid.java.util.common.logger.Logger;\n+import io.druid.server.coordinator.helper.DruidCoordinatorHelper;\n+import org.joda.time.DateTime;\n+import org.joda.time.Period;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class DruidCoordinatorCleanupPendingSegments implements DruidCoordinatorHelper\n+{\n+  private static final Logger log = new Logger(DruidCoordinatorCleanupPendingSegments.class);\n+  private static final Period KEEP_PENDING_SEGMENTS_OFFSET = new Period(\"P1D\");\n+\n+  private final IndexingServiceClient indexingServiceClient;\n+\n+  @Inject\n+  public DruidCoordinatorCleanupPendingSegments(IndexingServiceClient indexingServiceClient)\n+  {\n+    this.indexingServiceClient = indexingServiceClient;\n+  }\n+\n+  @Override\n+  public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n+  {\n+    final List<DateTime> createdTimes = new ArrayList<>();\n+    createdTimes.add(\n+        indexingServiceClient\n+            .getRunningTasks()\n+            .stream()\n+            .map(TaskStatusPlus::getCreatedTime)\n+            .min(Comparators.naturalNullsFirst())\n+            .orElse(DateTimes.nowUtc()) // If there is no running tasks, this returns the current time.\n+    );\n+    createdTimes.add(\n+        indexingServiceClient\n+            .getPendingTasks()\n+            .stream()\n+            .map(TaskStatusPlus::getCreatedTime)\n+            .min(Comparators.naturalNullsFirst())\n+            .orElse(DateTimes.nowUtc()) // If there is no pending tasks, this returns the current time.\n+    );\n+    createdTimes.add(\n+        indexingServiceClient\n+            .getWaitingTasks()\n+            .stream()\n+            .map(TaskStatusPlus::getCreatedTime)\n+            .min(Comparators.naturalNullsFirst())\n+            .orElse(DateTimes.nowUtc()) // If there is no waiting tasks, this returns the current time.\n+    );\n+\n+    final TaskStatusPlus completeTaskStatus = indexingServiceClient.getLastCompleteTask();\n+    if (completeTaskStatus != null) {\n+      createdTimes.add(completeTaskStatus.getCreatedTime());\n+    }\n+    createdTimes.sort(Comparators.naturalNullsFirst());\n+\n+    // There should be at least one createdTime because the current time is added to the 'createdTimes' list if there\n+    // is no running/pending/waiting tasks.\n+    Preconditions.checkState(!createdTimes.isEmpty(), \"Failed to gather createdTimes of tasks\");\n+\n+    // If there is no running/pending/waiting/complete tasks, pendingSegmentsCleanupEndTime is\n+    // (DateTimes.nowUtc() - KEEP_PENDING_SEGMENTS_OFFSET).\n+    final DateTime pendingSegmentsCleanupEndTime = createdTimes.get(0).minus(KEEP_PENDING_SEGMENTS_OFFSET);\n+    for (ImmutableDruidDataSource dataSource : params.getDataSources()) {\n+      if (!params.getCoordinatorDynamicConfig().getKillPendingSegmentsSkipList().contains(dataSource.getName())) {\n+        log.info(\n+            \"Killed [%d] pendingSegments created until [%s] for dataSource[%s]\",\n+            indexingServiceClient.killPendingSegments(dataSource.getName(), pendingSegmentsCleanupEndTime),\n+            pendingSegmentsCleanupEndTime,\n+            dataSource\n+        );\n+      }\n+    }\n+    return params;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorCleanupPendingSegments.java",
                "sha": "40af4260324e8c835b18ba6ef6cea4a4dd1b0172",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorConfig.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorConfig.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/server/coordinator/DruidCoordinatorConfig.java",
                "patch": "@@ -57,6 +57,12 @@ public boolean isKillSegments()\n     return false;\n   }\n \n+  @Config(\"druid.coordinator.kill.pendingSegments.on\")\n+  public boolean isKillPendingSegments()\n+  {\n+    return false;\n+  }\n+\n   @Config(\"druid.coordinator.kill.period\")\n   @Default(\"P1D\")\n   public abstract Duration getCoordinatorKillPeriod();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/coordinator/DruidCoordinatorConfig.java",
                "sha": "5976b5d59c7cec73285b198ff112dcec8b35d1bd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/security/Access.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/security/Access.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 4,
                "filename": "server/src/main/java/io/druid/server/security/Access.java",
                "patch": "@@ -26,7 +26,7 @@\n   public final static Access OK = new Access(true);\n \n   private final boolean allowed;\n-  private String message;\n+  private final String message;\n \n   public Access(boolean allowed)\n   {\n@@ -44,10 +44,9 @@ public boolean isAllowed()\n     return allowed;\n   }\n \n-  public Access setMessage(String message)\n+  public String getMessage()\n   {\n-    this.message = message;\n-    return this;\n+    return message;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/main/java/io/druid/server/security/Access.java",
                "sha": "bb76b12de8f535b15dfb1f72f0fed478068bf0d4",
                "status": "modified"
            },
            {
                "additions": 111,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "server/src/test/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java",
                "patch": "@@ -26,18 +26,22 @@\n import io.druid.indexing.overlord.DataSourceMetadata;\n import io.druid.indexing.overlord.ObjectMetadata;\n import io.druid.indexing.overlord.SegmentPublishResult;\n+import io.druid.java.util.common.DateTimes;\n import io.druid.java.util.common.Intervals;\n import io.druid.java.util.common.StringUtils;\n import io.druid.segment.TestHelper;\n+import io.druid.segment.realtime.appenderator.SegmentIdentifier;\n import io.druid.timeline.DataSegment;\n import io.druid.timeline.partition.LinearShardSpec;\n import io.druid.timeline.partition.NoneShardSpec;\n import io.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.DateTime;\n import org.joda.time.Interval;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n import org.skife.jdbi.v2.Handle;\n import org.skife.jdbi.v2.tweak.HandleCallback;\n import org.skife.jdbi.v2.util.StringMapper;\n@@ -53,7 +57,11 @@\n   @Rule\n   public final TestDerbyConnector.DerbyConnectorRule derbyConnectorRule = new TestDerbyConnector.DerbyConnectorRule();\n \n+  @Rule\n+  public final ExpectedException expectedException = ExpectedException.none();\n+\n   private final ObjectMapper mapper = TestHelper.makeJsonMapper();\n+\n   private final DataSegment defaultSegment = new DataSegment(\n       \"fooDataSource\",\n       Intervals.of(\"2015-01-01T00Z/2015-01-02T00Z\"),\n@@ -172,10 +180,11 @@\n   public void setUp()\n   {\n     derbyConnector = derbyConnectorRule.getConnector();\n-    mapper.registerSubtypes(LinearShardSpec.class);\n+    mapper.registerSubtypes(LinearShardSpec.class, NumberedShardSpec.class);\n     derbyConnector.createDataSourceTable();\n     derbyConnector.createTaskTables();\n     derbyConnector.createSegmentTable();\n+    derbyConnector.createPendingSegmentsTable();\n     metadataUpdateCounter.set(0);\n     coordinator = new IndexerSQLMetadataStorageCoordinator(\n         mapper,\n@@ -826,4 +835,105 @@ private void additionalNumberedShardTest(Set<DataSegment> segments) throws IOExc\n     // Should not update dataSource metadata.\n     Assert.assertEquals(0, metadataUpdateCounter.get());\n   }\n+\n+  @Test\n+  public void testAllocatePendingSegment() throws IOException\n+  {\n+    final String dataSource = \"ds\";\n+    final Interval interval = Intervals.of(\"2017-01-01/2017-02-01\");\n+    final SegmentIdentifier identifier = coordinator.allocatePendingSegment(\n+        dataSource,\n+        \"seq\",\n+        null,\n+        interval,\n+        \"version\",\n+        false\n+    );\n+\n+    Assert.assertEquals(\"ds_2017-01-01T00:00:00.000Z_2017-02-01T00:00:00.000Z_version\", identifier.toString());\n+\n+    final SegmentIdentifier identifier1 = coordinator.allocatePendingSegment(\n+        dataSource,\n+        \"seq\",\n+        identifier.toString(),\n+        interval,\n+        identifier.getVersion(),\n+        false\n+    );\n+\n+    Assert.assertEquals(\"ds_2017-01-01T00:00:00.000Z_2017-02-01T00:00:00.000Z_version_1\", identifier1.toString());\n+\n+    final SegmentIdentifier identifier2 = coordinator.allocatePendingSegment(\n+        dataSource,\n+        \"seq\",\n+        identifier1.toString(),\n+        interval,\n+        identifier1.getVersion(),\n+        false\n+    );\n+\n+    Assert.assertEquals(\"ds_2017-01-01T00:00:00.000Z_2017-02-01T00:00:00.000Z_version_2\", identifier2.toString());\n+\n+    final SegmentIdentifier identifier3 = coordinator.allocatePendingSegment(\n+        dataSource,\n+        \"seq\",\n+        identifier1.toString(),\n+        interval,\n+        identifier1.getVersion(),\n+        false\n+    );\n+\n+    Assert.assertEquals(\"ds_2017-01-01T00:00:00.000Z_2017-02-01T00:00:00.000Z_version_2\", identifier3.toString());\n+    Assert.assertEquals(identifier2, identifier3);\n+\n+    final SegmentIdentifier identifier4 = coordinator.allocatePendingSegment(\n+        dataSource,\n+        \"seq1\",\n+        null,\n+        interval,\n+        \"version\",\n+        false\n+    );\n+\n+    Assert.assertEquals(\"ds_2017-01-01T00:00:00.000Z_2017-02-01T00:00:00.000Z_version_3\", identifier4.toString());\n+  }\n+\n+  @Test\n+  public void testDeletePendingSegment() throws IOException, InterruptedException\n+  {\n+    final String dataSource = \"ds\";\n+    final Interval interval = Intervals.of(\"2017-01-01/2017-02-01\");\n+    String prevSegmentId = null;\n+\n+    final DateTime begin = DateTimes.nowUtc();\n+\n+    for (int i = 0; i < 10; i++) {\n+      final SegmentIdentifier identifier = coordinator.allocatePendingSegment(\n+          dataSource,\n+          \"seq\",\n+          prevSegmentId,\n+          interval,\n+          \"version\",\n+          false\n+      );\n+      prevSegmentId = identifier.toString();\n+    }\n+    Thread.sleep(100);\n+\n+    final DateTime secondBegin = DateTimes.nowUtc();\n+    for (int i = 0; i < 5; i++) {\n+      final SegmentIdentifier identifier = coordinator.allocatePendingSegment(\n+          dataSource,\n+          \"seq\",\n+          prevSegmentId,\n+          interval,\n+          \"version\",\n+          false\n+      );\n+      prevSegmentId = identifier.toString();\n+    }\n+\n+    final int numDeleted = coordinator.deletePendingSegments(dataSource, new Interval(begin, secondBegin));\n+    Assert.assertEquals(10, numDeleted);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java",
                "sha": "8254c1e7e764590c60124dc50bcda05b997fe399",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 3,
                "filename": "server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "patch": "@@ -28,6 +28,7 @@\n import io.druid.jackson.DefaultObjectMapper;\n import io.druid.java.util.common.DateTimes;\n import io.druid.java.util.common.Pair;\n+import io.druid.java.util.common.StringUtils;\n import io.druid.java.util.common.jackson.JacksonUtils;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -36,6 +37,7 @@\n import org.junit.rules.ExpectedException;\n \n import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n \n public class SQLMetadataStorageActionHandlerTest\n@@ -59,13 +61,11 @@ public void setUp() throws Exception\n     final String logTable = \"logs\";\n     final String lockTable = \"locks\";\n \n-\n     connector.createEntryTable(entryTable);\n     connector.createLockTable(lockTable, entryType);\n     connector.createLogTable(logTable, entryType);\n \n-\n-    handler = new SQLMetadataStorageActionHandler<>(\n+    handler = new DerbyMetadataStorageActionHandler<>(\n         connector,\n         jsonMapper,\n         new MetadataStorageActionHandlerTypes<Map<String, Integer>, Map<String, Integer>, Map<String, String>, Map<String, Integer>>()\n@@ -179,6 +179,44 @@ public void testEntryAndStatus() throws Exception\n     );\n   }\n \n+  @Test\n+  public void testGetRecentStatuses() throws EntryExistsException\n+  {\n+    for (int i = 1; i < 11; i++) {\n+      final String entryId = \"abcd_\" + i;\n+      final Map<String, Integer> entry = ImmutableMap.of(\"a\", i);\n+      final Map<String, Integer> status = ImmutableMap.of(\"count\", i * 10);\n+\n+      handler.insert(entryId, DateTimes.of(StringUtils.format(\"2014-01-%02d\", i)), \"test\", entry, false, status);\n+    }\n+\n+    final List<Map<String, Integer>> statuses = handler.getInactiveStatusesSince(DateTimes.of(\"2014-01-01\"), 7);\n+    Assert.assertEquals(7, statuses.size());\n+    int i = 10;\n+    for (Map<String, Integer> status : statuses) {\n+      Assert.assertEquals(ImmutableMap.of(\"count\", i-- * 10), status);\n+    }\n+  }\n+\n+  @Test\n+  public void testGetRecentStatuses2() throws EntryExistsException\n+  {\n+    for (int i = 1; i < 6; i++) {\n+      final String entryId = \"abcd_\" + i;\n+      final Map<String, Integer> entry = ImmutableMap.of(\"a\", i);\n+      final Map<String, Integer> status = ImmutableMap.of(\"count\", i * 10);\n+\n+      handler.insert(entryId, DateTimes.of(StringUtils.format(\"2014-01-%02d\", i)), \"test\", entry, false, status);\n+    }\n+\n+    final List<Map<String, Integer>> statuses = handler.getInactiveStatusesSince(DateTimes.of(\"2014-01-01\"), 10);\n+    Assert.assertEquals(5, statuses.size());\n+    int i = 5;\n+    for (Map<String, Integer> status : statuses) {\n+      Assert.assertEquals(ImmutableMap.of(\"count\", i-- * 10), status);\n+    }\n+  }\n+\n   @Test(timeout = 10_000L)\n   public void testRepeatInsert() throws Exception\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "sha": "1dbdabef177f685e789c22c9ab7104ad1cc295ae",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/server/coordinator/DruidCoordinatorConfigTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/server/coordinator/DruidCoordinatorConfigTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "server/src/test/java/io/druid/server/coordinator/DruidCoordinatorConfigTest.java",
                "patch": "@@ -19,13 +19,12 @@\n \n package io.druid.server.coordinator;\n \n+import io.druid.java.util.common.config.Config;\n import org.joda.time.Duration;\n import org.junit.Assert;\n import org.junit.Test;\n import org.skife.config.ConfigurationObjectFactory;\n \n-import io.druid.java.util.common.config.Config;\n-\n import java.util.Properties;\n \n /**\n@@ -46,6 +45,7 @@ public void testDeserialization() throws Exception\n     Assert.assertFalse(config.isMergeSegments());\n     Assert.assertFalse(config.isConvertSegments());\n     Assert.assertFalse(config.isKillSegments());\n+    Assert.assertFalse(config.isKillPendingSegments());\n     Assert.assertEquals(86400000, config.getCoordinatorKillPeriod().getMillis());\n     Assert.assertEquals(-1000, config.getCoordinatorKillDurationToRetain().getMillis());\n     Assert.assertEquals(0, config.getCoordinatorKillMaxSegments());\n@@ -64,6 +64,7 @@ public void testDeserialization() throws Exception\n     props.setProperty(\"druid.coordinator.kill.period\", \"PT1s\");\n     props.setProperty(\"druid.coordinator.kill.durationToRetain\", \"PT1s\");\n     props.setProperty(\"druid.coordinator.kill.maxSegments\", \"10000\");\n+    props.setProperty(\"druid.coordinator.kill.pendingSegments.on\", \"true\");\n     props.setProperty(\"druid.coordinator.load.timeout\", \"PT1s\");\n     props.setProperty(\"druid.coordinator.console.static\", \"test\");\n     props.setProperty(\"druid.coordinator.loadqueuepeon.repeatDelay\", \"PT0.100s\");\n@@ -77,6 +78,7 @@ public void testDeserialization() throws Exception\n     Assert.assertTrue(config.isMergeSegments());\n     Assert.assertTrue(config.isConvertSegments());\n     Assert.assertTrue(config.isKillSegments());\n+    Assert.assertTrue(config.isKillPendingSegments());\n     Assert.assertEquals(new Duration(\"PT1s\"), config.getCoordinatorKillPeriod());\n     Assert.assertEquals(new Duration(\"PT1s\"), config.getCoordinatorKillDurationToRetain());\n     Assert.assertEquals(10000, config.getCoordinatorKillMaxSegments());",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/server/coordinator/DruidCoordinatorConfigTest.java",
                "sha": "87752595ba88a46facf1090a8d11e34567eb3b1d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/server/http/CoordinatorDynamicConfigTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/server/http/CoordinatorDynamicConfigTest.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 1,
                "filename": "server/src/test/java/io/druid/server/http/CoordinatorDynamicConfigTest.java",
                "patch": "@@ -183,7 +183,8 @@ public void testUpdate()\n \n     Assert.assertEquals(\n         current,\n-        new CoordinatorDynamicConfig.Builder(null, null, null, null, null, null, null, null, null, null, null).build(current)\n+        new CoordinatorDynamicConfig.Builder(null, null, null, null, null, null, null, null, null, null, null, null)\n+            .build(current)\n     );\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/server/src/test/java/io/druid/server/http/CoordinatorDynamicConfigTest.java",
                "sha": "094e426308156863d739179cdf286a380675bd8d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/services/src/main/java/io/druid/cli/CliCoordinator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/io/druid/cli/CliCoordinator.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 0,
                "filename": "services/src/main/java/io/druid/cli/CliCoordinator.java",
                "patch": "@@ -57,6 +57,7 @@\n import io.druid.server.audit.AuditManagerProvider;\n import io.druid.server.coordinator.BalancerStrategyFactory;\n import io.druid.server.coordinator.DruidCoordinator;\n+import io.druid.server.coordinator.DruidCoordinatorCleanupPendingSegments;\n import io.druid.server.coordinator.DruidCoordinatorConfig;\n import io.druid.server.coordinator.LoadQueueTaskMaster;\n import io.druid.server.coordinator.helper.DruidCoordinatorHelper;\n@@ -207,6 +208,10 @@ public void configure(Binder binder)\n                 \"druid.coordinator.kill.on\",\n                 Predicates.equalTo(\"true\"),\n                 DruidCoordinatorSegmentKiller.class\n+            ).addConditionBinding(\n+                \"druid.coordinator.kill.pendingSegments.on\",\n+                Predicates.equalTo(\"true\"),\n+                DruidCoordinatorCleanupPendingSegments.class\n             );\n \n             binder.bind(DiscoverySideEffectsProvider.Child.class).annotatedWith(Coordinator.class).toProvider(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/services/src/main/java/io/druid/cli/CliCoordinator.java",
                "sha": "af706fdeb4a33920d82e40e4efcc53806c4f8930",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9199d61389bf215e41387908372068a6d4da46c2/services/src/main/java/io/druid/cli/CliOverlord.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/io/druid/cli/CliOverlord.java?ref=9199d61389bf215e41387908372068a6d4da46c2",
                "deletions": 2,
                "filename": "services/src/main/java/io/druid/cli/CliOverlord.java",
                "patch": "@@ -58,6 +58,7 @@\n import io.druid.indexing.common.tasklogs.TaskRunnerTaskLogStreamer;\n import io.druid.indexing.overlord.ForkingTaskRunnerFactory;\n import io.druid.indexing.overlord.HeapMemoryTaskStorage;\n+import io.druid.indexing.overlord.IndexerMetadataStorageAdapter;\n import io.druid.indexing.overlord.MetadataTaskStorage;\n import io.druid.indexing.overlord.RemoteTaskRunnerFactory;\n import io.druid.indexing.overlord.TaskLockbox;\n@@ -89,7 +90,6 @@\n import io.druid.server.http.RedirectInfo;\n import io.druid.server.initialization.jetty.JettyServerInitUtils;\n import io.druid.server.initialization.jetty.JettyServerInitializer;\n-import io.druid.server.security.AuthConfig;\n import io.druid.server.security.AuthenticationUtils;\n import io.druid.server.security.Authenticator;\n import io.druid.server.security.AuthenticatorMapper;\n@@ -176,6 +176,7 @@ public void configure(Binder binder)\n             binder.bind(TaskActionToolbox.class).in(LazySingleton.class);\n             binder.bind(TaskLockbox.class).in(LazySingleton.class);\n             binder.bind(TaskStorageQueryAdapter.class).in(LazySingleton.class);\n+            binder.bind(IndexerMetadataStorageAdapter.class).in(LazySingleton.class);\n             binder.bind(SupervisorManager.class).in(LazySingleton.class);\n \n             binder.bind(ChatHandlerProvider.class).toProvider(Providers.<ChatHandlerProvider>of(null));\n@@ -316,7 +317,6 @@ public void initialize(Server server, Injector injector)\n           )\n       );\n \n-      final AuthConfig authConfig = injector.getInstance(AuthConfig.class);\n       final ObjectMapper jsonMapper = injector.getInstance(Key.get(ObjectMapper.class, Json.class));\n       final AuthenticatorMapper authenticatorMapper = injector.getInstance(AuthenticatorMapper.class);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9199d61389bf215e41387908372068a6d4da46c2/services/src/main/java/io/druid/cli/CliOverlord.java",
                "sha": "ae075abd4b84524ce8895c5917a42ea8bb86aa69",
                "status": "modified"
            }
        ],
        "message": "Automatic pendingSegments cleanup (#5149)\n\n* PendingSegments cleanup\r\n\r\n* fix build\r\n\r\n* address comments\r\n\r\n* address comments\r\n\r\n* fix potential npe\r\n\r\n* address comments\r\n\r\n* fix build\r\n\r\n* fix test\r\n\r\n* fix test",
        "parent": "https://github.com/apache/incubator-druid/commit/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f",
        "repo": "incubator-druid",
        "unit_tests": [
            "TaskStatusPlusTest.java",
            "KafkaIndexTaskClientTest.java",
            "KafkaSupervisorTest.java",
            "TaskStatusTest.java",
            "SegmentAllocateActionTest.java",
            "ForkingTaskRunnerTest.java",
            "IndexerMetadataStorageAdapterTest.java",
            "RemoteTaskRunnerTest.java",
            "TaskRunnerUtilsTest.java",
            "OverlordResourceTest.java",
            "TaskAnnouncementTest.java",
            "WorkerTaskMonitorTest.java",
            "TestIndexerMetadataStorageCoordinator.java",
            "IndexerSQLMetadataStorageCoordinatorTest.java",
            "SQLMetadataStorageActionHandlerTest.java",
            "CoordinatorDynamicConfigTest.java",
            "DruidCoordinatorConfigTest.java",
            "TestDruidCoordinatorConfig.java"
        ]
    },
    "incubator-druid_925c104": {
        "bug_id": "incubator-druid_925c104",
        "commit": "https://github.com/apache/incubator-druid/commit/925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/client/src/main/java/com/metamx/druid/QueryableNode.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/QueryableNode.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 3,
                "filename": "client/src/main/java/com/metamx/druid/QueryableNode.java",
                "patch": "@@ -52,7 +52,7 @@\n import org.I0Itec.zkclient.ZkClient;\n \n \n-\n+import org.joda.time.Duration;\n import org.mortbay.jetty.Server;\n import org.skife.config.ConfigurationObjectFactory;\n \n@@ -333,7 +333,10 @@ private void initializeEmitter()\n   {\n     if (emitter == null) {\n       final HttpClient httpClient = HttpClientInit.createClient(\n-          HttpClientConfig.builder().withNumConnections(1).build(), lifecycle\n+          HttpClientConfig.builder()\n+                          .withNumConnections(1)\n+                          .withReadTimeout(new Duration(PropUtils.getProperty(props, \"druid.emitter.timeOut\")))\n+                          .build(), lifecycle\n       );\n \n       setEmitter(\n@@ -358,7 +361,7 @@ protected void init() throws Exception\n   @LifecycleStart\n   public synchronized void start() throws Exception\n   {\n-    if (! initialized) {\n+    if (!initialized) {\n       init();\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/client/src/main/java/com/metamx/druid/QueryableNode.java",
                "sha": "2cf45630529d39c79bece0ab0a495f60bd9c2cee",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/merger/src/main/java/com/metamx/druid/merger/coordinator/http/IndexerCoordinatorNode.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/coordinator/http/IndexerCoordinatorNode.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 9,
                "filename": "merger/src/main/java/com/metamx/druid/merger/coordinator/http/IndexerCoordinatorNode.java",
                "patch": "@@ -116,6 +116,7 @@\n import org.jets3t.service.S3ServiceException;\n import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n import org.jets3t.service.security.AWSCredentials;\n+import org.joda.time.Duration;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.servlet.Context;\n import org.mortbay.jetty.servlet.DefaultServlet;\n@@ -314,10 +315,12 @@ public void doInit() throws Exception\n     final Context staticContext = new Context(server, \"/static\", Context.SESSIONS);\n     staticContext.addServlet(new ServletHolder(new DefaultServlet()), \"/*\");\n \n-    ResourceCollection resourceCollection = new ResourceCollection(new String[] {\n-        IndexerCoordinatorNode.class.getClassLoader().getResource(\"static\").toExternalForm(),\n-        IndexerCoordinatorNode.class.getClassLoader().getResource(\"indexer_static\").toExternalForm()\n-    });\n+    ResourceCollection resourceCollection = new ResourceCollection(\n+        new String[]{\n+            IndexerCoordinatorNode.class.getClassLoader().getResource(\"static\").toExternalForm(),\n+            IndexerCoordinatorNode.class.getClassLoader().getResource(\"indexer_static\").toExternalForm()\n+        }\n+    );\n     staticContext.setBaseResource(resourceCollection);\n \n     // TODO -- Need a QueryServlet and some kind of QuerySegmentWalker if we want to support querying tasks\n@@ -448,7 +451,14 @@ private void initializeEmitter()\n   {\n     if (emitter == null) {\n       final HttpClient httpClient = HttpClientInit.createClient(\n-          HttpClientConfig.builder().withNumConnections(1).build(), lifecycle\n+          HttpClientConfig.builder().withNumConnections(1).withReadTimeout(\n+              new Duration(\n+                  PropUtils.getProperty(\n+                      props,\n+                      \"druid.emitter.timeOut\"\n+                  )\n+              )\n+          ).build(), lifecycle\n       );\n \n       emitter = new ServiceEmitter(\n@@ -602,7 +612,11 @@ public void initializeTaskStorage()\n         taskStorage = new HeapMemoryTaskStorage();\n       } else if (config.getStorageImpl().equals(\"db\")) {\n         final IndexerDbConnectorConfig dbConnectorConfig = configFactory.build(IndexerDbConnectorConfig.class);\n-        taskStorage = new DbTaskStorage(getJsonMapper(), dbConnectorConfig, new DbConnector(dbConnectorConfig).getDBI());\n+        taskStorage = new DbTaskStorage(\n+            getJsonMapper(),\n+            dbConnectorConfig,\n+            new DbConnector(dbConnectorConfig).getDBI()\n+        );\n       } else {\n         throw new ISE(\"Invalid storage implementation: %s\", config.getStorageImpl());\n       }\n@@ -754,9 +768,12 @@ public IndexerCoordinatorNode build()\n         jsonMapper = new DefaultObjectMapper();\n         smileMapper = new DefaultObjectMapper(new SmileFactory());\n         smileMapper.getJsonFactory().setCodec(smileMapper);\n-      }\n-      else if (jsonMapper == null || smileMapper == null) {\n-        throw new ISE(\"Only jsonMapper[%s] or smileMapper[%s] was set, must set neither or both.\", jsonMapper, smileMapper);\n+      } else if (jsonMapper == null || smileMapper == null) {\n+        throw new ISE(\n+            \"Only jsonMapper[%s] or smileMapper[%s] was set, must set neither or both.\",\n+            jsonMapper,\n+            smileMapper\n+        );\n       }\n \n       if (lifecycle == null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/merger/src/main/java/com/metamx/druid/merger/coordinator/http/IndexerCoordinatorNode.java",
                "sha": "23263bcf31ee6e66e66ac2e6d03210baa69bf0a6",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/merger/src/main/java/com/metamx/druid/merger/worker/http/WorkerNode.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/worker/http/WorkerNode.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 5,
                "filename": "merger/src/main/java/com/metamx/druid/merger/worker/http/WorkerNode.java",
                "patch": "@@ -81,6 +81,7 @@\n import org.jets3t.service.S3ServiceException;\n import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n import org.jets3t.service.security.AWSCredentials;\n+import org.joda.time.Duration;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.servlet.Context;\n import org.mortbay.jetty.servlet.DefaultServlet;\n@@ -317,7 +318,9 @@ private void initializeHttpClient()\n   {\n     if (httpClient == null) {\n       httpClient = HttpClientInit.createClient(\n-          HttpClientConfig.builder().withNumConnections(1).build(), lifecycle\n+          HttpClientConfig.builder().withNumConnections(1)\n+                          .withReadTimeout(new Duration(PropUtils.getProperty(props, \"druid.emitter.timeOut\")))\n+                          .build(), lifecycle\n       );\n     }\n   }\n@@ -336,7 +339,7 @@ private void initializeEmitter()\n \n   private void initializeS3Service() throws S3ServiceException\n   {\n-    if(s3Service == null) {\n+    if (s3Service == null) {\n       s3Service = new RestS3Service(\n           new AWSCredentials(\n               PropUtils.getProperty(props, \"com.metamx.aws.accessKey\"),\n@@ -527,9 +530,12 @@ public WorkerNode build()\n         jsonMapper = new DefaultObjectMapper();\n         smileMapper = new DefaultObjectMapper(new SmileFactory());\n         smileMapper.getJsonFactory().setCodec(smileMapper);\n-      }\n-      else if (jsonMapper == null || smileMapper == null) {\n-        throw new ISE(\"Only jsonMapper[%s] or smileMapper[%s] was set, must set neither or both.\", jsonMapper, smileMapper);\n+      } else if (jsonMapper == null || smileMapper == null) {\n+        throw new ISE(\n+            \"Only jsonMapper[%s] or smileMapper[%s] was set, must set neither or both.\",\n+            jsonMapper,\n+            smileMapper\n+        );\n       }\n \n       if (lifecycle == null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/merger/src/main/java/com/metamx/druid/merger/worker/http/WorkerNode.java",
                "sha": "2eaddc085b5f1ac82a8b5d3994fbcd60739dccc6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -64,7 +64,7 @@\n             <dependency>\n                 <groupId>com.metamx</groupId>\n                 <artifactId>http-client</artifactId>\n-                <version>0.7.0</version>\n+                <version>0.7.1</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.metamx</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/pom.xml",
                "sha": "811b7ab8f3c36ebb6846f8cc7ac2bc47912abddd",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/coordination/ZkCoordinator.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/coordination/ZkCoordinator.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 0,
                "filename": "server/src/main/java/com/metamx/druid/coordination/ZkCoordinator.java",
                "patch": "@@ -218,6 +218,13 @@ private void loadCache()\n         DataSegment segment = jsonMapper.readValue(file, DataSegment.class);\n         if (serverManager.isSegmentCached(segment)) {\n           addSegment(segment);\n+        } else {\n+          log.warn(\"Unable to find cache file for %s. Deleting lookup entry\", segment.getIdentifier());\n+\n+          File segmentInfoCacheFile = new File(config.getSegmentInfoCacheDirectory(), segment.getIdentifier());\n+          if (!segmentInfoCacheFile.delete()) {\n+            log.warn(\"Unable to delete segmentInfoCacheFile[%s]\", segmentInfoCacheFile);\n+          }\n         }\n       }\n       catch (Exception e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/coordination/ZkCoordinator.java",
                "sha": "2068ee2b8394ec7a6287f0db6f17f042cfb1fefc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/http/ComputeNode.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/http/ComputeNode.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 4,
                "filename": "server/src/main/java/com/metamx/druid/http/ComputeNode.java",
                "patch": "@@ -39,22 +39,19 @@\n import com.metamx.druid.initialization.Initialization;\n import com.metamx.druid.initialization.ServerInit;\n import com.metamx.druid.jackson.DefaultObjectMapper;\n-import com.metamx.druid.loading.SegmentLoaderConfig;\n import com.metamx.druid.loading.SegmentLoader;\n+import com.metamx.druid.loading.SegmentLoaderConfig;\n import com.metamx.druid.metrics.ServerMonitor;\n import com.metamx.druid.query.MetricsEmittingExecutorService;\n import com.metamx.druid.query.QueryRunnerFactoryConglomerate;\n import com.metamx.druid.utils.PropUtils;\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n import com.metamx.metrics.Monitor;\n-\n-\n import org.jets3t.service.S3ServiceException;\n import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n import org.jets3t.service.security.AWSCredentials;\n import org.mortbay.jetty.servlet.Context;\n-import org.mortbay.jetty.servlet.DefaultServlet;\n import org.mortbay.jetty.servlet.ServletHolder;\n import org.skife.config.ConfigurationObjectFactory;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/http/ComputeNode.java",
                "sha": "c5c7046485780d2c1c5bd34d62ce5a25d3acfe74",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/http/MasterMain.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/http/MasterMain.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 1,
                "filename": "server/src/main/java/com/metamx/druid/http/MasterMain.java",
                "patch": "@@ -73,6 +73,7 @@\n import com.netflix.curator.x.discovery.ServiceProvider;\n import org.I0Itec.zkclient.ZkClient;\n \n+import org.joda.time.Duration;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.servlet.Context;\n import org.mortbay.jetty.servlet.DefaultServlet;\n@@ -102,7 +103,14 @@ public static void main(String[] args) throws Exception\n     final Lifecycle lifecycle = new Lifecycle();\n \n     final HttpClient httpClient = HttpClientInit.createClient(\n-        HttpClientConfig.builder().withNumConnections(1).build(), lifecycle\n+        HttpClientConfig.builder().withNumConnections(1).withReadTimeout(\n+            new Duration(\n+                PropUtils.getProperty(\n+                    props,\n+                    \"druid.emitter.timeOut\"\n+                )\n+            )\n+        ).build(), lifecycle\n     );\n \n     final ServiceEmitter emitter = new ServiceEmitter(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/http/MasterMain.java",
                "sha": "19b64b42247862d9b3f7b605b5773ae56c17830e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/index/brita/DimensionPredicateFilter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/brita/DimensionPredicateFilter.java?ref=925c104dd21e1c0c4ebfeac755ac3f020ce421e1",
                "deletions": 1,
                "filename": "server/src/main/java/com/metamx/druid/index/brita/DimensionPredicateFilter.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Function;\n import com.google.common.base.Predicate;\n import com.metamx.common.guava.FunctionalIterable;\n+import com.metamx.druid.kv.Indexed;\n import it.uniroma3.mat.extendedset.intset.ImmutableConciseSet;\n \n import javax.annotation.Nullable;\n@@ -45,8 +46,13 @@ public DimensionPredicateFilter(\n   @Override\n   public ImmutableConciseSet goConcise(final BitmapIndexSelector selector)\n   {\n+    Indexed<String> dimValues = selector.getDimensionValues(dimension);\n+    if (dimValues == null || dimValues.size() == 0 || predicate == null) {\n+      return new ImmutableConciseSet();\n+    }\n+\n     return ImmutableConciseSet.union(\n-        FunctionalIterable.create(selector.getDimensionValues(dimension))\n+        FunctionalIterable.create(dimValues)\n                           .filter(predicate)\n                           .transform(\n                               new Function<String, ImmutableConciseSet>()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/925c104dd21e1c0c4ebfeac755ac3f020ce421e1/server/src/main/java/com/metamx/druid/index/brita/DimensionPredicateFilter.java",
                "sha": "d88dd06d3dcb2a59d20a0256ef9257867fc84956",
                "status": "modified"
            }
        ],
        "message": "update emitter version and fix some NPEs",
        "parent": "https://github.com/apache/incubator-druid/commit/8b100248a8780bcf7b7cc73a2e4ae6ecad570ab5",
        "repo": "incubator-druid",
        "unit_tests": [
            "ZkCoordinatorTest.java"
        ]
    },
    "incubator-druid_92cce04": {
        "bug_id": "incubator-druid_92cce04",
        "commit": "https://github.com/apache/incubator-druid/commit/92cce041654f48daffc0deb76d68e6afae3de8ee",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/incubator-druid/blob/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java?ref=92cce041654f48daffc0deb76d68e6afae3de8ee",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java",
                "patch": "@@ -21,11 +21,15 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableList;\n+import org.apache.druid.common.config.JacksonConfigManager;\n \n+import javax.annotation.Nonnull;\n import javax.annotation.Nullable;\n import java.util.List;\n import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicReference;\n \n public class CoordinatorCompactionConfig\n {\n@@ -73,6 +77,21 @@ public static CoordinatorCompactionConfig empty()\n     return new CoordinatorCompactionConfig(ImmutableList.of(), null, null);\n   }\n \n+  public static AtomicReference<CoordinatorCompactionConfig> watch(final JacksonConfigManager configManager)\n+  {\n+    return configManager.watch(\n+        CoordinatorCompactionConfig.CONFIG_KEY,\n+        CoordinatorCompactionConfig.class,\n+        CoordinatorCompactionConfig.empty()\n+    );\n+  }\n+\n+  @Nonnull\n+  public static CoordinatorCompactionConfig current(final JacksonConfigManager configManager)\n+  {\n+    return Preconditions.checkNotNull(watch(configManager).get(), \"Got null config from watcher?!\");\n+  }\n+\n   @JsonCreator\n   public CoordinatorCompactionConfig(\n       @JsonProperty(\"compactionConfigs\") List<DataSourceCompactionConfig> compactionConfigs,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java",
                "sha": "cc1fdf61e34f31a5921b63534bebc532f5acf01d",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java?ref=92cce041654f48daffc0deb76d68e6afae3de8ee",
                "deletions": 3,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "patch": "@@ -21,21 +21,25 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.druid.common.config.JacksonConfigManager;\n import org.apache.druid.java.util.common.IAE;\n \n+import javax.annotation.Nonnull;\n import javax.annotation.Nullable;\n import java.util.Collection;\n import java.util.HashSet;\n import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n \n /**\n  * This class is for users to change their configurations while their Druid cluster is running.\n  * These configurations are designed to allow only simple values rather than complicated JSON objects.\n  *\n- * @see org.apache.druid.common.config.JacksonConfigManager\n+ * @see JacksonConfigManager\n  * @see org.apache.druid.common.config.ConfigManager\n  */\n public class CoordinatorDynamicConfig\n@@ -121,6 +125,21 @@ public CoordinatorDynamicConfig(\n     }\n   }\n \n+  public static AtomicReference<CoordinatorDynamicConfig> watch(final JacksonConfigManager configManager)\n+  {\n+    return configManager.watch(\n+        CoordinatorDynamicConfig.CONFIG_KEY,\n+        CoordinatorDynamicConfig.class,\n+        CoordinatorDynamicConfig.builder().build()\n+    );\n+  }\n+\n+  @Nonnull\n+  public static CoordinatorDynamicConfig current(final JacksonConfigManager configManager)\n+  {\n+    return Preconditions.checkNotNull(watch(configManager).get(), \"Got null config from watcher?!\");\n+  }\n+\n   @JsonProperty\n   public long getMillisToWaitBeforeDeleting()\n   {\n@@ -424,7 +443,9 @@ public CoordinatorDynamicConfig build()\n           killDataSourceWhitelist,\n           killAllDataSources == null ? DEFAULT_KILL_ALL_DATA_SOURCES : killAllDataSources,\n           killPendingSegmentsSkipList,\n-          maxSegmentsInNodeLoadingQueue == null ? DEFAULT_MAX_SEGMENTS_IN_NODE_LOADING_QUEUE : maxSegmentsInNodeLoadingQueue\n+          maxSegmentsInNodeLoadingQueue == null\n+          ? DEFAULT_MAX_SEGMENTS_IN_NODE_LOADING_QUEUE\n+          : maxSegmentsInNodeLoadingQueue\n       );\n     }\n \n@@ -442,7 +463,9 @@ public CoordinatorDynamicConfig build(CoordinatorDynamicConfig defaults)\n           killDataSourceWhitelist == null ? defaults.getKillDataSourceWhitelist() : killDataSourceWhitelist,\n           killAllDataSources == null ? defaults.isKillAllDataSources() : killAllDataSources,\n           killPendingSegmentsSkipList == null ? defaults.getKillPendingSegmentsSkipList() : killPendingSegmentsSkipList,\n-          maxSegmentsInNodeLoadingQueue == null ? defaults.getMaxSegmentsInNodeLoadingQueue() : maxSegmentsInNodeLoadingQueue\n+          maxSegmentsInNodeLoadingQueue == null\n+          ? defaults.getMaxSegmentsInNodeLoadingQueue()\n+          : maxSegmentsInNodeLoadingQueue\n       );\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java",
                "sha": "c51a04a8deaa12943e36a639823599f41a7aa54b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java?ref=92cce041654f48daffc0deb76d68e6afae3de8ee",
                "deletions": 10,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java",
                "patch": "@@ -324,20 +324,12 @@ public long remainingSegmentSizeBytesForCompaction(String dataSource)\n \n   public CoordinatorDynamicConfig getDynamicConfigs()\n   {\n-    return configManager.watch(\n-        CoordinatorDynamicConfig.CONFIG_KEY,\n-        CoordinatorDynamicConfig.class,\n-        CoordinatorDynamicConfig.builder().build()\n-    ).get();\n+    return CoordinatorDynamicConfig.current(configManager);\n   }\n \n   public CoordinatorCompactionConfig getCompactionConfig()\n   {\n-    return configManager.watch(\n-        CoordinatorCompactionConfig.CONFIG_KEY,\n-        CoordinatorCompactionConfig.class,\n-        CoordinatorCompactionConfig.empty()\n-    ).get();\n+    return CoordinatorCompactionConfig.current(configManager);\n   }\n \n   public void removeSegment(DataSegment segment)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java",
                "sha": "852068342c002843489ffe939cac55cf6981b828",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java?ref=92cce041654f48daffc0deb76d68e6afae3de8ee",
                "deletions": 49,
                "filename": "server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java",
                "patch": "@@ -65,12 +65,7 @@ public CoordinatorCompactionConfigsResource(JacksonConfigManager manager)\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getCompactConfig()\n   {\n-    return Response.ok(\n-        manager.watch(\n-            CoordinatorCompactionConfig.CONFIG_KEY,\n-            CoordinatorCompactionConfig.class\n-        ).get()\n-    ).build();\n+    return Response.ok(CoordinatorCompactionConfig.current(manager)).build();\n   }\n \n   @POST\n@@ -84,17 +79,13 @@ public Response setCompactionTaskLimit(\n       @Context HttpServletRequest req\n   )\n   {\n-    CoordinatorCompactionConfig current = manager.watch(\n-        CoordinatorCompactionConfig.CONFIG_KEY,\n-        CoordinatorCompactionConfig.class\n-    ).get();\n+    final CoordinatorCompactionConfig current = CoordinatorCompactionConfig.current(manager);\n \n-    final CoordinatorCompactionConfig newCompactionConfig;\n-    if (current != null) {\n-      newCompactionConfig = CoordinatorCompactionConfig.from(current, compactionTaskSlotRatio, maxCompactionTaskSlots);\n-    } else {\n-      newCompactionConfig = new CoordinatorCompactionConfig(null, compactionTaskSlotRatio, maxCompactionTaskSlots);\n-    }\n+    final CoordinatorCompactionConfig newCompactionConfig = CoordinatorCompactionConfig.from(\n+        current,\n+        compactionTaskSlotRatio,\n+        maxCompactionTaskSlots\n+    );\n \n     final SetResult setResult = manager.set(\n         CoordinatorCompactionConfig.CONFIG_KEY,\n@@ -120,22 +111,14 @@ public Response addOrUpdateCompactionConfig(\n       @Context HttpServletRequest req\n   )\n   {\n-    CoordinatorCompactionConfig current = manager.watch(\n-        CoordinatorCompactionConfig.CONFIG_KEY,\n-        CoordinatorCompactionConfig.class\n-    ).get();\n-\n+    final CoordinatorCompactionConfig current = CoordinatorCompactionConfig.current(manager);\n     final CoordinatorCompactionConfig newCompactionConfig;\n-    if (current != null) {\n-      final Map<String, DataSourceCompactionConfig> newConfigs = current\n-          .getCompactionConfigs()\n-          .stream()\n-          .collect(Collectors.toMap(DataSourceCompactionConfig::getDataSource, Function.identity()));\n-      newConfigs.put(newConfig.getDataSource(), newConfig);\n-      newCompactionConfig = CoordinatorCompactionConfig.from(current, ImmutableList.copyOf(newConfigs.values()));\n-    } else {\n-      newCompactionConfig = CoordinatorCompactionConfig.from(ImmutableList.of(newConfig));\n-    }\n+    final Map<String, DataSourceCompactionConfig> newConfigs = current\n+        .getCompactionConfigs()\n+        .stream()\n+        .collect(Collectors.toMap(DataSourceCompactionConfig::getDataSource, Function.identity()));\n+    newConfigs.put(newConfig.getDataSource(), newConfig);\n+    newCompactionConfig = CoordinatorCompactionConfig.from(current, ImmutableList.copyOf(newConfigs.values()));\n \n     final SetResult setResult = manager.set(\n         CoordinatorCompactionConfig.CONFIG_KEY,\n@@ -155,15 +138,7 @@ public Response addOrUpdateCompactionConfig(\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getCompactionConfig(@PathParam(\"dataSource\") String dataSource)\n   {\n-    CoordinatorCompactionConfig current = manager.watch(\n-        CoordinatorCompactionConfig.CONFIG_KEY,\n-        CoordinatorCompactionConfig.class\n-    ).get();\n-\n-    if (current == null) {\n-      return Response.status(Response.Status.NOT_FOUND).build();\n-    }\n-\n+    final CoordinatorCompactionConfig current = CoordinatorCompactionConfig.current(manager);\n     final Map<String, DataSourceCompactionConfig> configs = current\n         .getCompactionConfigs()\n         .stream()\n@@ -187,15 +162,7 @@ public Response deleteCompactionConfig(\n       @Context HttpServletRequest req\n   )\n   {\n-    CoordinatorCompactionConfig current = manager.watch(\n-        CoordinatorCompactionConfig.CONFIG_KEY,\n-        CoordinatorCompactionConfig.class\n-    ).get();\n-\n-    if (current == null) {\n-      return Response.status(Response.Status.NOT_FOUND).build();\n-    }\n-\n+    final CoordinatorCompactionConfig current = CoordinatorCompactionConfig.current(manager);\n     final Map<String, DataSourceCompactionConfig> configs = current\n         .getCompactionConfigs()\n         .stream()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java",
                "sha": "8f7bf26721b82cc409935377db9b005d5d252779",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java?ref=92cce041654f48daffc0deb76d68e6afae3de8ee",
                "deletions": 15,
                "filename": "server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java",
                "patch": "@@ -67,31 +67,24 @@ public CoordinatorDynamicConfigsResource(\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getDynamicConfigs()\n   {\n-    return Response.ok(\n-        manager.watch(\n-            CoordinatorDynamicConfig.CONFIG_KEY,\n-            CoordinatorDynamicConfig.class\n-        ).get()\n-    ).build();\n+    return Response.ok(CoordinatorDynamicConfig.current(manager)).build();\n   }\n \n   // default value is used for backwards compatibility\n   @POST\n   @Consumes(MediaType.APPLICATION_JSON)\n-  public Response setDynamicConfigs(final CoordinatorDynamicConfig.Builder dynamicConfigBuilder,\n-                                    @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue(\"\") final String author,\n-                                    @HeaderParam(AuditManager.X_DRUID_COMMENT) @DefaultValue(\"\") final String comment,\n-                                    @Context HttpServletRequest req\n+  public Response setDynamicConfigs(\n+      final CoordinatorDynamicConfig.Builder dynamicConfigBuilder,\n+      @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue(\"\") final String author,\n+      @HeaderParam(AuditManager.X_DRUID_COMMENT) @DefaultValue(\"\") final String comment,\n+      @Context HttpServletRequest req\n   )\n   {\n-    CoordinatorDynamicConfig current = manager.watch(\n-        CoordinatorDynamicConfig.CONFIG_KEY,\n-        CoordinatorDynamicConfig.class\n-    ).get();\n+    CoordinatorDynamicConfig current = CoordinatorDynamicConfig.current(manager);\n \n     final SetResult setResult = manager.set(\n         CoordinatorDynamicConfig.CONFIG_KEY,\n-        current == null ? dynamicConfigBuilder.build() : dynamicConfigBuilder.build(current),\n+        dynamicConfigBuilder.build(current),\n         new AuditInfo(author, comment, req.getRemoteAddr())\n     );\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/92cce041654f48daffc0deb76d68e6afae3de8ee/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java",
                "sha": "85bdd3320bceffe11eedb706008ee52c1009f8ee",
                "status": "modified"
            }
        ],
        "message": "Fix missing default config in some calls to coordinator dynamic configs. (#6652)\n\n* Fix missing default config in some calls to coordinator dynamic configs.\r\n\r\nThe lack of a default config meant that if someone called an API\r\n_without_ a default config before one _with_ a default config, then\r\nthe default value would get stuck at null instead of the intended\r\ndefault value. I noticed this in a cluster where calling /druid/coordinator/v1/config\r\nbefore a coordinator had fully started up would lead to NPEs during\r\nDruidCoordinatorRuleRunner.\r\n\r\nThis patch makes the default configs consistent across all calls.\r\n\r\n* Remove unnecessary null check.",
        "parent": "https://github.com/apache/incubator-druid/commit/e285b1103d894acea2f20098e08881f044d00e30",
        "repo": "incubator-druid",
        "unit_tests": [
            "CoordinatorDynamicConfigTest.java",
            "DruidCoordinatorTest.java"
        ]
    },
    "incubator-druid_934c83b": {
        "bug_id": "incubator-druid_934c83b",
        "commit": "https://github.com/apache/incubator-druid/commit/934c83bca6df5db913c130af039d8893a9fcb16a",
        "file": [
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/incubator-druid/blob/934c83bca6df5db913c130af039d8893a9fcb16a/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskLockbox.java",
                "changes": 139,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskLockbox.java?ref=934c83bca6df5db913c130af039d8893a9fcb16a",
                "deletions": 51,
                "filename": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskLockbox.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.guava.Comparators;\n import org.apache.druid.java.util.emitter.EmittingLogger;\n+import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n import javax.annotation.Nullable;\n@@ -51,6 +52,7 @@\n import java.util.NavigableMap;\n import java.util.NavigableSet;\n import java.util.Set;\n+import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.locks.Condition;\n@@ -66,11 +68,14 @@\n  */\n public class TaskLockbox\n {\n-  // Datasource -> Interval -> list of (Tasks + TaskLock)\n+  // Datasource -> startTime -> Interval -> list of (Tasks + TaskLock)\n   // Multiple shared locks can be acquired for the same dataSource and interval.\n   // Note that revoked locks are also maintained in this map to notify that those locks are revoked to the callers when\n   // they acquire the same locks again.\n-  private final Map<String, NavigableMap<Interval, List<TaskLockPosse>>> running = new HashMap<>();\n+  // Also, the key of the second inner map is the start time to find all intervals properly starting with the same\n+  // startTime.\n+  private final Map<String, NavigableMap<DateTime, SortedMap<Interval, List<TaskLockPosse>>>> running = new HashMap<>();\n+\n   private final TaskStorage taskStorage;\n   private final ReentrantLock giant = new ReentrantLock(true);\n   private final Condition lockReleaseCondition = giant.newCondition();\n@@ -326,7 +331,14 @@ private TaskLockPosse createOrFindLockPosse(\n       final TaskLockType lockType\n   )\n   {\n-    return createOrFindLockPosse(task, interval, null, lockType);\n+    giant.lock();\n+\n+    try {\n+      return createOrFindLockPosse(task, interval, null, lockType);\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n   }\n \n   /**\n@@ -584,7 +596,8 @@ private TaskLockPosse createNewTaskLockPosse(\n       final TaskLockPosse posseToUse = new TaskLockPosse(\n           new TaskLock(lockType, groupId, dataSource, interval, version, priority, revoked)\n       );\n-      running.computeIfAbsent(dataSource, k -> new TreeMap<>(Comparators.intervalsByStartThenEnd()))\n+      running.computeIfAbsent(dataSource, k -> new TreeMap<>())\n+             .computeIfAbsent(interval.getStart(), k -> new TreeMap<>(Comparators.intervalsByStartThenEnd()))\n              .computeIfAbsent(interval, k -> new ArrayList<>())\n              .add(posseToUse);\n \n@@ -612,7 +625,7 @@ private TaskLockPosse createNewTaskLockPosse(\n       CriticalAction<T> action\n   ) throws Exception\n   {\n-    giant.lockInterruptibly();\n+    giant.lock();\n \n     try {\n       return action.perform(isTaskLocksValid(task, intervals));\n@@ -624,13 +637,19 @@ private TaskLockPosse createNewTaskLockPosse(\n \n   private boolean isTaskLocksValid(Task task, List<Interval> intervals)\n   {\n-    return intervals\n-        .stream()\n-        .allMatch(interval -> {\n-          final TaskLock lock = getOnlyTaskLockPosseContainingInterval(task, interval).getTaskLock();\n-          // Tasks cannot enter the critical section with a shared lock\n-          return !lock.isRevoked() && lock.getType() != TaskLockType.SHARED;\n-        });\n+    giant.lock();\n+    try {\n+      return intervals\n+          .stream()\n+          .allMatch(interval -> {\n+            final TaskLock lock = getOnlyTaskLockPosseContainingInterval(task, interval).getTaskLock();\n+            // Tasks cannot enter the critical section with a shared lock\n+            return !lock.isRevoked() && lock.getType() != TaskLockType.SHARED;\n+          });\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n   }\n \n   private void revokeLock(TaskLockPosse lockPosse)\n@@ -676,7 +695,7 @@ private void revokeLock(String taskId, TaskLock lock)\n         final TaskLock revokedLock = lock.revokedCopy();\n         taskStorage.replaceLock(taskId, lock, revokedLock);\n \n-        final List<TaskLockPosse> possesHolder = running.get(task.getDataSource()).get(lock.getInterval());\n+        final List<TaskLockPosse> possesHolder = running.get(task.getDataSource()).get(lock.getInterval().getStart()).get(lock.getInterval());\n         final TaskLockPosse foundPosse = possesHolder.stream()\n                                                      .filter(posse -> posse.getTaskLock().equals(lock))\n                                                      .findFirst()\n@@ -733,13 +752,19 @@ public void unlock(final Task task, final Interval interval)\n \n     try {\n       final String dataSource = task.getDataSource();\n-      final NavigableMap<Interval, List<TaskLockPosse>> dsRunning = running.get(task.getDataSource());\n+      final NavigableMap<DateTime, SortedMap<Interval, List<TaskLockPosse>>> dsRunning = running.get(task.getDataSource());\n \n       if (dsRunning == null || dsRunning.isEmpty()) {\n         return;\n       }\n \n-      final List<TaskLockPosse> possesHolder = dsRunning.get(interval);\n+      final SortedMap<Interval, List<TaskLockPosse>> intervalToPosses = dsRunning.get(interval.getStart());\n+\n+      if (intervalToPosses == null || intervalToPosses.isEmpty()) {\n+        return;\n+      }\n+\n+      final List<TaskLockPosse> possesHolder = intervalToPosses.get(interval);\n       if (possesHolder == null || possesHolder.isEmpty()) {\n         return;\n       }\n@@ -760,8 +785,12 @@ public void unlock(final Task task, final Interval interval)\n           possesHolder.remove(taskLockPosse);\n         }\n \n-        if (possesHolder.size() == 0) {\n-          dsRunning.remove(interval);\n+        if (possesHolder.isEmpty()) {\n+          intervalToPosses.remove(interval);\n+        }\n+\n+        if (intervalToPosses.isEmpty()) {\n+          dsRunning.remove(interval.getStart());\n         }\n \n         if (running.get(dataSource).size() == 0) {\n@@ -797,6 +826,18 @@ public void unlock(final Task task, final Interval interval)\n     }\n   }\n \n+  public void add(Task task)\n+  {\n+    giant.lock();\n+    try {\n+      log.info(\"Adding task[%s] to activeTasks\", task.getId());\n+      activeTasks.add(task.getId());\n+    }\n+    finally {\n+      giant.unlock();\n+    }\n+  }\n+\n   /**\n    * Release all locks for a task and remove task from set of active tasks. Does nothing if the task is not currently locked or not an active task.\n    *\n@@ -832,11 +873,12 @@ public void remove(final Task task)\n \n     try {\n       // Scan through all locks for this datasource\n-      final NavigableMap<Interval, List<TaskLockPosse>> dsRunning = running.get(task.getDataSource());\n+      final NavigableMap<DateTime, SortedMap<Interval, List<TaskLockPosse>>> dsRunning = running.get(task.getDataSource());\n       if (dsRunning == null) {\n         return ImmutableList.of();\n       } else {\n         return dsRunning.values().stream()\n+                        .flatMap(map -> map.values().stream())\n                         .flatMap(Collection::stream)\n                         .filter(taskLockPosse -> taskLockPosse.containsTask(task))\n                         .collect(Collectors.toList());\n@@ -870,29 +912,28 @@ public void remove(final Task task)\n     giant.lock();\n \n     try {\n-      final NavigableMap<Interval, List<TaskLockPosse>> dsRunning = running.get(dataSource);\n+      final NavigableMap<DateTime, SortedMap<Interval, List<TaskLockPosse>>> dsRunning = running.get(dataSource);\n       if (dsRunning == null) {\n         // No locks at all\n         return Collections.emptyList();\n       } else {\n         // Tasks are indexed by locked interval, which are sorted by interval start. Intervals are non-overlapping, so:\n-        final NavigableSet<Interval> dsLockbox = dsRunning.navigableKeySet();\n-        final Iterable<Interval> searchIntervals = Iterables.concat(\n+        final NavigableSet<DateTime> dsLockbox = dsRunning.navigableKeySet();\n+        final Iterable<DateTime> searchStartTimes = Iterables.concat(\n             // Single interval that starts at or before ours\n-            Collections.singletonList(dsLockbox.floor(new Interval(interval.getStart(), DateTimes.MAX))),\n+            Collections.singletonList(dsLockbox.floor(interval.getStart())),\n \n             // All intervals that start somewhere between our start instant (exclusive) and end instant (exclusive)\n-            dsLockbox.subSet(\n-                new Interval(interval.getStart(), DateTimes.MAX),\n-                false,\n-                new Interval(interval.getEnd(), interval.getEnd()),\n-                false\n-            )\n+            dsLockbox.subSet(interval.getStart(), false, interval.getEnd(), false)\n         );\n \n-        return StreamSupport.stream(searchIntervals.spliterator(), false)\n-                            .filter(searchInterval -> searchInterval != null && searchInterval.overlaps(interval))\n-                            .flatMap(searchInterval -> dsRunning.get(searchInterval).stream())\n+        return StreamSupport.stream(searchStartTimes.spliterator(), false)\n+                            .filter(java.util.Objects::nonNull)\n+                            .map(dsRunning::get)\n+                            .filter(java.util.Objects::nonNull)\n+                            .flatMap(sortedMap -> sortedMap.entrySet().stream())\n+                            .filter(entry -> entry.getKey().overlaps(interval))\n+                            .flatMap(entry -> entry.getValue().stream())\n                             .collect(Collectors.toList());\n       }\n     }\n@@ -901,12 +942,24 @@ public void remove(final Task task)\n     }\n   }\n \n-  public void add(Task task)\n+  @VisibleForTesting\n+  TaskLockPosse getOnlyTaskLockPosseContainingInterval(Task task, Interval interval)\n   {\n     giant.lock();\n+\n     try {\n-      log.info(\"Adding task[%s] to activeTasks\", task.getId());\n-      activeTasks.add(task.getId());\n+      final List<TaskLockPosse> filteredPosses = findLockPossesContainingInterval(task.getDataSource(), interval)\n+          .stream()\n+          .filter(lockPosse -> lockPosse.containsTask(task))\n+          .collect(Collectors.toList());\n+\n+      if (filteredPosses.isEmpty()) {\n+        throw new ISE(\"Cannot find locks for task[%s] and interval[%s]\", task.getId(), interval);\n+      } else if (filteredPosses.size() > 1) {\n+        throw new ISE(\"There are multiple lockPosses for task[%s] and interval[%s]?\", task.getId(), interval);\n+      } else {\n+        return filteredPosses.get(0);\n+      }\n     }\n     finally {\n       giant.unlock();\n@@ -936,30 +989,14 @@ private static boolean isRevocable(TaskLockPosse lockPosse, int tryLockPriority)\n     return existingLock.isRevoked() || existingLock.getNonNullPriority() < tryLockPriority;\n   }\n \n-  private TaskLockPosse getOnlyTaskLockPosseContainingInterval(Task task, Interval interval)\n-  {\n-    final List<TaskLockPosse> filteredPosses = findLockPossesContainingInterval(task.getDataSource(), interval)\n-        .stream()\n-        .filter(lockPosse -> lockPosse.containsTask(task))\n-        .collect(Collectors.toList());\n-\n-    if (filteredPosses.isEmpty()) {\n-      throw new ISE(\"Cannot find locks for task[%s] and interval[%s]\", task.getId(), interval);\n-    } else if (filteredPosses.size() > 1) {\n-      throw new ISE(\"There are multiple lockPosses for task[%s] and interval[%s]?\", task.getId(), interval);\n-    } else {\n-      return filteredPosses.get(0);\n-    }\n-  }\n-\n   @VisibleForTesting\n   Set<String> getActiveTasks()\n   {\n     return activeTasks;\n   }\n \n   @VisibleForTesting\n-  Map<String, NavigableMap<Interval, List<TaskLockPosse>>> getAllLocks()\n+  Map<String, NavigableMap<DateTime, SortedMap<Interval, List<TaskLockPosse>>>> getAllLocks()\n   {\n     return running;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/934c83bca6df5db913c130af039d8893a9fcb16a/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskLockbox.java",
                "sha": "626f4e333443b5c21fa717851dfe0ed296ac96af",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/934c83bca6df5db913c130af039d8893a9fcb16a/indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLockboxTest.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLockboxTest.java?ref=934c83bca6df5db913c130af039d8893a9fcb16a",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLockboxTest.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.druid.indexing.common.task.AbstractTask;\n import org.apache.druid.indexing.common.task.NoopTask;\n import org.apache.druid.indexing.common.task.Task;\n+import org.apache.druid.indexing.overlord.TaskLockbox.TaskLockPosse;\n import org.apache.druid.jackson.DefaultObjectMapper;\n import org.apache.druid.java.util.common.ISE;\n import org.apache.druid.java.util.common.Intervals;\n@@ -624,6 +625,48 @@ public void testUnlock() throws EntryExistsException\n     Assert.assertTrue(lockbox.getAllLocks().isEmpty());\n   }\n \n+  @Test\n+  public void testFindLockPosseAfterRevokeWithDifferentLockIntervals() throws EntryExistsException\n+  {\n+    final Task lowPriorityTask = NoopTask.create(0);\n+    final Task highPriorityTask = NoopTask.create(10);\n+\n+    taskStorage.insert(lowPriorityTask, TaskStatus.running(lowPriorityTask.getId()));\n+    taskStorage.insert(highPriorityTask, TaskStatus.running(highPriorityTask.getId()));\n+    lockbox.add(lowPriorityTask);\n+    lockbox.add(highPriorityTask);\n+\n+    Assert.assertTrue(\n+        lockbox.tryLock(\n+            TaskLockType.EXCLUSIVE,\n+            lowPriorityTask, Intervals.of(\"2018-12-16T09:00:00/2018-12-16T10:00:00\")\n+        ).isOk()\n+    );\n+\n+    Assert.assertTrue(\n+        lockbox.tryLock(\n+            TaskLockType.EXCLUSIVE,\n+            highPriorityTask, Intervals.of(\"2018-12-16T09:00:00/2018-12-16T09:30:00\")\n+        ).isOk()\n+    );\n+\n+    final TaskLockPosse highLockPosse = lockbox.getOnlyTaskLockPosseContainingInterval(\n+        highPriorityTask,\n+        Intervals.of(\"2018-12-16T09:00:00/2018-12-16T09:30:00\")\n+    );\n+\n+    Assert.assertTrue(highLockPosse.containsTask(highPriorityTask));\n+    Assert.assertFalse(highLockPosse.getTaskLock().isRevoked());\n+\n+    final TaskLockPosse lowLockPosse = lockbox.getOnlyTaskLockPosseContainingInterval(\n+        lowPriorityTask,\n+        Intervals.of(\"2018-12-16T09:00:00/2018-12-16T10:00:00\")\n+    );\n+\n+    Assert.assertTrue(lowLockPosse.containsTask(lowPriorityTask));\n+    Assert.assertTrue(lowLockPosse.getTaskLock().isRevoked());\n+  }\n+\n   private Set<TaskLock> getAllLocks(List<Task> tasks)\n   {\n     return tasks.stream()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/934c83bca6df5db913c130af039d8893a9fcb16a/indexing-service/src/test/java/org/apache/druid/indexing/overlord/TaskLockboxTest.java",
                "sha": "560425512513e01eaaaa8be6b4ec7334c568bc4a",
                "status": "modified"
            }
        ],
        "message": "Fix TaskLockbox when there are multiple intervals of the same start but differerent end (#6822)\n\n* Fix TaskLockbox when there are multiple intervals of the same start but differernt end\r\n\r\n* fix build\r\n\r\n* fix npe",
        "parent": "https://github.com/apache/incubator-druid/commit/ea973fee6b24f1e769ba50e942e68950a07c50d4",
        "repo": "incubator-druid",
        "unit_tests": [
            "TaskLockboxTest.java"
        ]
    },
    "incubator-druid_94da2b6": {
        "bug_id": "incubator-druid_94da2b6",
        "commit": "https://github.com/apache/incubator-druid/commit/94da2b6ad3e794e573afebf46abc99441e2425e4",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/94da2b6ad3e794e573afebf46abc99441e2425e4/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=94da2b6ad3e794e573afebf46abc99441e2425e4",
                "deletions": 10,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -47,14 +47,14 @@\n \n /**\n  * A QueryRunner that combines a list of other QueryRunners and executes them in parallel on an executor.\n- *\n+ * <p/>\n  * When using this, it is important to make sure that the list of QueryRunners provided is fully flattened.\n  * If, for example, you were to pass a list of a Chained QueryRunner (A) and a non-chained QueryRunner (B).  Imagine\n  * A has 2 QueryRunner chained together (Aa and Ab), the fact that the Queryables are run in parallel on an\n  * executor would mean that the Queryables are actually processed in the order\n- *\n+ * <p/>\n  * <pre>A -&gt; B -&gt; Aa -&gt; Ab</pre>\n- *\n+ * <p/>\n  * That is, the two sub queryables for A would run *after* B is run, effectively meaning that the results for B\n  * must be fully cached in memory before the results for Aa and Ab are computed.\n  */\n@@ -113,17 +113,17 @@ public ChainedExecutionQueryRunner(\n                           @Override\n                           public ListenableFuture<Iterable<T>> apply(final QueryRunner<T> input)\n                           {\n+                            if (input == null) {\n+                              throw new ISE(\"Null queryRunner! Looks to be some segment unmapping action happening\");\n+                            }\n+\n                             return exec.submit(\n                                 new AbstractPrioritizedCallable<Iterable<T>>(priority)\n                                 {\n                                   @Override\n                                   public Iterable<T> call() throws Exception\n                                   {\n                                     try {\n-                                      if (input == null) {\n-                                        throw new ISE(\"Input is null?! How is this possible?!\");\n-                                      }\n-\n                                       Sequence<T> result = input.run(query);\n                                       if (result == null) {\n                                         throw new ISE(\"Got a null result! Segments are missing!\");\n@@ -155,7 +155,7 @@ public ChainedExecutionQueryRunner(\n             queryWatcher.registerQuery(query, futures);\n \n             try {\n-              final Number timeout = query.getContextValue(\"timeout\", (Number)null);\n+              final Number timeout = query.getContextValue(\"timeout\", (Number) null);\n               return new MergeIterable<>(\n                   ordering.nullsFirst(),\n                   timeout == null ?\n@@ -168,10 +168,10 @@ public ChainedExecutionQueryRunner(\n               futures.cancel(true);\n               throw new QueryInterruptedException(\"Query interrupted\");\n             }\n-            catch(CancellationException e) {\n+            catch (CancellationException e) {\n               throw new QueryInterruptedException(\"Query cancelled\");\n             }\n-            catch(TimeoutException e) {\n+            catch (TimeoutException e) {\n               log.info(\"Query timeout, cancelling pending results for query id [%s]\", query.getId());\n               futures.cancel(true);\n               throw new QueryInterruptedException(\"Query timeout\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/94da2b6ad3e794e573afebf46abc99441e2425e4/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "6b474799de0cd56c196c617977e02e6893d7e29d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/94da2b6ad3e794e573afebf46abc99441e2425e4/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java?ref=94da2b6ad3e794e573afebf46abc99441e2425e4",
                "deletions": 3,
                "filename": "processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "patch": "@@ -29,6 +29,7 @@\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.ListeningExecutorService;\n import com.google.common.util.concurrent.MoreExecutors;\n+import com.metamx.common.ISE;\n import com.metamx.common.Pair;\n import com.metamx.common.guava.Accumulator;\n import com.metamx.common.guava.Sequence;\n@@ -81,9 +82,6 @@ public GroupByParallelQueryRunner(\n     final boolean bySegment = query.getContextBySegment(false);\n     final int priority = query.getContextPriority(0);\n \n-    if (Iterables.isEmpty(queryables)) {\n-      log.warn(\"No queryables found.\");\n-    }\n     ListenableFuture<List<Void>> futures = Futures.allAsList(\n         Lists.newArrayList(\n             Iterables.transform(\n@@ -93,6 +91,10 @@ public GroupByParallelQueryRunner(\n                   @Override\n                   public ListenableFuture<Void> apply(final QueryRunner<T> input)\n                   {\n+                    if (input == null) {\n+                      throw new ISE(\"Null queryRunner! Looks to be some segment unmapping action happening\");\n+                    }\n+\n                     return exec.submit(\n                         new AbstractPrioritizedCallable<Void>(priority)\n                         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/94da2b6ad3e794e573afebf46abc99441e2425e4/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "sha": "7e97ce03099b7a9469ac3ff006d68bce9bb71a2f",
                "status": "modified"
            }
        ],
        "message": "more logging for potential NPE because of segment unmapping",
        "parent": "https://github.com/apache/incubator-druid/commit/a0baf7e8f4f86d77ec6d0ca9fd75d7e15f809c49",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java"
        ]
    },
    "incubator-druid_976492c": {
        "bug_id": "incubator-druid_976492c",
        "commit": "https://github.com/apache/incubator-druid/commit/976492c18644614fa7d4cf0cd1ad508929579e6c",
        "file": [
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/api/src/main/java/io/druid/guice/PolyBind.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/guice/PolyBind.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 13,
                "filename": "api/src/main/java/io/druid/guice/PolyBind.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package io.druid.guice;\n \n+import com.google.common.base.Preconditions;\n import com.google.inject.Binder;\n import com.google.inject.Inject;\n import com.google.inject.Injector;\n@@ -30,6 +31,7 @@\n import com.google.inject.multibindings.MapBinder;\n import com.google.inject.util.Types;\n \n+import javax.annotation.Nullable;\n import java.lang.reflect.ParameterizedType;\n import java.util.Map;\n import java.util.Properties;\n@@ -58,10 +60,27 @@\n       Binder binder,\n       String property,\n       Key<T> interfaceKey,\n-      Key<? extends T> defaultKey\n+      @Nullable Key<? extends T> defaultKey\n   )\n   {\n-    return createChoiceWithDefault(binder, property, interfaceKey, defaultKey, null);\n+    ConfiggedProvider<T> provider = new ConfiggedProvider<>(interfaceKey, property, defaultKey, null);\n+    return binder.bind(interfaceKey).toProvider(provider);\n+  }\n+\n+  /**\n+   * @deprecated use {@link #createChoiceWithDefault(com.google.inject.Binder, String, com.google.inject.Key, String)}\n+   * instead. {@code defaultKey} argument is ignored.\n+   */\n+  @Deprecated\n+  public static <T> ScopedBindingBuilder createChoiceWithDefault(\n+      Binder binder,\n+      String property,\n+      Key<T> interfaceKey,\n+      Key<? extends T> defaultKey,\n+      String defaultPropertyValue\n+  )\n+  {\n+    return createChoiceWithDefault(binder, property, interfaceKey, defaultPropertyValue);\n   }\n \n   /**\n@@ -70,7 +89,6 @@\n    * @param binder the binder for the injector that is being configured\n    * @param property the property that will be checked to determine the implementation choice\n    * @param interfaceKey the interface that will be injected using this choice\n-   * @param defaultKey the default instance to be injected if the property doesn't match a choice.  Can be null\n    * @param defaultPropertyValue the default property value to use if the property is not set.\n    * @param <T> interface type\n    * @return A ScopedBindingBuilder so that scopes can be added to the binding, if required.\n@@ -79,11 +97,12 @@\n       Binder binder,\n       String property,\n       Key<T> interfaceKey,\n-      Key<? extends T> defaultKey,\n       String defaultPropertyValue\n   )\n   {\n-    return binder.bind(interfaceKey).toProvider(new ConfiggedProvider<T>(interfaceKey, property, defaultKey, defaultPropertyValue));\n+    Preconditions.checkNotNull(defaultPropertyValue);\n+    ConfiggedProvider<T> provider = new ConfiggedProvider<>(interfaceKey, property, null, defaultPropertyValue);\n+    return binder.bind(interfaceKey).toProvider(provider);\n   }\n \n   /**\n@@ -118,7 +137,9 @@ else if (interfaceKey.getAnnotationType() != null) {\n   {\n     private final Key<T> key;\n     private final String property;\n+    @Nullable\n     private final Key<? extends T> defaultKey;\n+    @Nullable\n     private final String defaultPropertyValue;\n \n     private Injector injector;\n@@ -127,8 +148,8 @@ else if (interfaceKey.getAnnotationType() != null) {\n     ConfiggedProvider(\n         Key<T> key,\n         String property,\n-        Key<? extends T> defaultKey,\n-        String defaultPropertyValue\n+        @Nullable Key<? extends T> defaultKey,\n+        @Nullable String defaultPropertyValue\n     )\n     {\n       this.key = key;\n@@ -165,17 +186,20 @@ else if (key.getAnnotationType() != null) {\n \n       String implName = props.getProperty(property);\n       if (implName == null) {\n+        if (defaultPropertyValue == null) {\n+          if (defaultKey == null) {\n+            throw new ProvisionException(String.format(\"Some value must be configured for [%s]\", key));\n+          }\n+          return injector.getInstance(defaultKey);\n+        }\n         implName = defaultPropertyValue;\n       }\n       final Provider<T> provider = implsMap.get(implName);\n \n       if (provider == null) {\n-        if (defaultKey == null) {\n-          throw new ProvisionException(\n-              String.format(\"Unknown provider[%s] of %s, known options[%s]\", implName, key, implsMap.keySet())\n-          );\n-        }\n-        return injector.getInstance(defaultKey);\n+        throw new ProvisionException(\n+            String.format(\"Unknown provider[%s] of %s, known options[%s]\", implName, key, implsMap.keySet())\n+        );\n       }\n \n       return provider.get();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/api/src/main/java/io/druid/guice/PolyBind.java",
                "sha": "a76decabc38b0b763f70e761d9e3eb101873edab",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/api/src/test/java/io/druid/guice/PolyBindTest.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/test/java/io/druid/guice/PolyBindTest.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 5,
                "filename": "api/src/test/java/io/druid/guice/PolyBindTest.java",
                "patch": "@@ -55,7 +55,7 @@ public void configure(Binder binder)\n                   {\n                     binder.bind(Properties.class).toInstance(props);\n                     PolyBind.createChoice(binder, \"billy\", Key.get(Gogo.class), Key.get(GoA.class));\n-                    PolyBind.createChoiceWithDefault(binder, \"sally\", Key.get(GogoSally.class), null, \"b\");\n+                    PolyBind.createChoiceWithDefault(binder, \"sally\", Key.get(GogoSally.class), \"b\");\n \n                   }\n                 }\n@@ -107,9 +107,23 @@ public void configure(Binder binder)\n     Assert.assertEquals(\"B\", injector.getInstance(Gogo.class).go());\n     Assert.assertEquals(\"A\", injector.getInstance(Key.get(Gogo.class, Names.named(\"reverse\"))).go());\n     props.setProperty(\"billy\", \"c\");\n-    Assert.assertEquals(\"A\", injector.getInstance(Gogo.class).go());\n-    Assert.assertEquals(\"B\", injector.getInstance(Key.get(Gogo.class, Names.named(\"reverse\"))).go());\n-\n+    try {\n+      Assert.assertEquals(\"A\", injector.getInstance(Gogo.class).go());\n+      Assert.fail(); // should never be reached\n+    }\n+    catch (Exception e) {\n+      Assert.assertTrue(e instanceof ProvisionException);\n+      Assert.assertTrue(e.getMessage().contains(\"Unknown provider[c] of Key[type=io.druid.guice.PolyBindTest$Gogo\"));\n+    }\n+    try {\n+      Assert.assertEquals(\"B\", injector.getInstance(Key.get(Gogo.class, Names.named(\"reverse\"))).go());\n+      Assert.fail(); // should never be reached\n+    }\n+    catch (Exception e) {\n+      Assert.assertTrue(e instanceof ProvisionException);\n+      Assert.assertTrue(e.getMessage().contains(\"Unknown provider[c] of Key[type=io.druid.guice.PolyBindTest$Gogo\"));\n+    }\n+    \n     // test default property value\n     Assert.assertEquals(\"B\", injector.getInstance(GogoSally.class).go());\n     props.setProperty(\"sally\", \"a\");\n@@ -120,7 +134,8 @@ public void configure(Binder binder)\n     try {\n       injector.getInstance(GogoSally.class).go();\n       Assert.fail(); // should never be reached\n-    } catch(Exception e) {\n+    }\n+    catch (Exception e) {\n       Assert.assertTrue(e instanceof ProvisionException);\n       Assert.assertTrue(e.getMessage().contains(\"Unknown provider[c] of Key[type=io.druid.guice.PolyBindTest$GogoSally\"));\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/api/src/test/java/io/druid/guice/PolyBindTest.java",
                "sha": "af090e62cbde191b1dad616502b5a4e6eeabf068",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 2,
                "filename": "extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "patch": "@@ -26,6 +26,8 @@\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n import io.druid.metadata.MetadataStorageConnector;\n+import io.druid.metadata.MetadataStorageProvider;\n+import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n \n import java.util.List;\n@@ -52,12 +54,20 @@ public void configure(Binder binder)\n   {\n     super.configure(binder);\n \n-    PolyBind.optionBinder(binder, Key.get(MetadataStorageConnector.class))\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageProvider.class))\n+        .addBinding(TYPE)\n+        .to(NoopMetadataStorageProvider.class)\n+        .in(LazySingleton.class);\n+\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageConnector.class))\n         .addBinding(TYPE)\n         .to(SQLServerConnector.class)\n         .in(LazySingleton.class);\n \n-    PolyBind.optionBinder(binder, Key.get(SQLMetadataConnector.class))\n+    PolyBind\n+        .optionBinder(binder, Key.get(SQLMetadataConnector.class))\n         .addBinding(TYPE)\n         .to(SQLServerConnector.class)\n         .in(LazySingleton.class);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-contrib/sqlserver-metadata-storage/src/main/java/io/druid/metadata/storage/sqlserver/SQLServerMetadataStorageModule.java",
                "sha": "0f27099dccc7dcb9504e3217190f119b67e9e346",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 7,
                "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "patch": "@@ -77,13 +77,15 @@\n   @Override\n   public void configure(Binder binder)\n   {\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        TYPE_PREFIX,\n-        Key.get(NamespaceExtractionCacheManager.class),\n-        Key.get(OnHeapNamespaceExtractionCacheManager.class),\n-        \"onHeap\"\n-    ).in(LazySingleton.class);\n+    PolyBind\n+        .createChoiceWithDefault(binder, TYPE_PREFIX, Key.get(NamespaceExtractionCacheManager.class), \"onHeap\")\n+        .in(LazySingleton.class);\n+\n+    PolyBind\n+        .optionBinder(binder, Key.get(NamespaceExtractionCacheManager.class))\n+        .addBinding(\"onHeap\")\n+        .to(OnHeapNamespaceExtractionCacheManager.class)\n+        .in(LazySingleton.class);\n \n     PolyBind\n         .optionBinder(binder, Key.get(NamespaceExtractionCacheManager.class))",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/NamespaceExtractionModule.java",
                "sha": "130d4c780b312cf0ab6f724783e1607373b7ee08",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 8,
                "filename": "extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "patch": "@@ -27,6 +27,8 @@\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n import io.druid.metadata.MetadataStorageConnector;\n+import io.druid.metadata.MetadataStorageProvider;\n+import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n \n import java.util.List;\n@@ -51,14 +53,22 @@ public void configure(Binder binder)\n   {\n     super.configure(binder);\n \n-    PolyBind.optionBinder(binder, Key.get(MetadataStorageConnector.class))\n-            .addBinding(TYPE)\n-            .to(MySQLConnector.class)\n-            .in(LazySingleton.class);\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageProvider.class))\n+        .addBinding(TYPE)\n+        .to(NoopMetadataStorageProvider.class)\n+        .in(LazySingleton.class);\n \n-    PolyBind.optionBinder(binder, Key.get(SQLMetadataConnector.class))\n-            .addBinding(TYPE)\n-            .to(MySQLConnector.class)\n-            .in(LazySingleton.class);\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageConnector.class))\n+        .addBinding(TYPE)\n+        .to(MySQLConnector.class)\n+        .in(LazySingleton.class);\n+\n+    PolyBind\n+        .optionBinder(binder, Key.get(SQLMetadataConnector.class))\n+        .addBinding(TYPE)\n+        .to(MySQLConnector.class)\n+        .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/mysql-metadata-storage/src/main/java/io/druid/metadata/storage/mysql/MySQLMetadataStorageModule.java",
                "sha": "4a86dd627bd1f288f2b95e67305c12a30329f536",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 8,
                "filename": "extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "patch": "@@ -27,6 +27,8 @@\n import io.druid.guice.SQLMetadataStorageDruidModule;\n import io.druid.initialization.DruidModule;\n import io.druid.metadata.MetadataStorageConnector;\n+import io.druid.metadata.MetadataStorageProvider;\n+import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n \n import java.util.List;\n@@ -52,14 +54,22 @@ public void configure(Binder binder)\n   {\n     super.configure(binder);\n \n-    PolyBind.optionBinder(binder, Key.get(MetadataStorageConnector.class))\n-            .addBinding(TYPE)\n-            .to(PostgreSQLConnector.class)\n-            .in(LazySingleton.class);\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageProvider.class))\n+        .addBinding(TYPE)\n+        .to(NoopMetadataStorageProvider.class)\n+        .in(LazySingleton.class);\n \n-    PolyBind.optionBinder(binder, Key.get(SQLMetadataConnector.class))\n-            .addBinding(TYPE)\n-            .to(PostgreSQLConnector.class)\n-            .in(LazySingleton.class);\n+    PolyBind\n+        .optionBinder(binder, Key.get(MetadataStorageConnector.class))\n+        .addBinding(TYPE)\n+        .to(PostgreSQLConnector.class)\n+        .in(LazySingleton.class);\n+\n+    PolyBind\n+        .optionBinder(binder, Key.get(SQLMetadataConnector.class))\n+        .addBinding(TYPE)\n+        .to(PostgreSQLConnector.class)\n+        .in(LazySingleton.class);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/extensions-core/postgresql-metadata-storage/src/main/java/io/druid/metadata/storage/postgresql/PostgreSQLMetadataStorageModule.java",
                "sha": "e815682d4d02781010158ce3d9ad7916953385f5",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 99,
                "filename": "server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "patch": "@@ -37,7 +37,6 @@\n import io.druid.metadata.MetadataStorageConnector;\n import io.druid.metadata.MetadataStorageProvider;\n import io.druid.metadata.MetadataSupervisorManager;\n-import io.druid.metadata.NoopMetadataStorageProvider;\n import io.druid.metadata.SQLMetadataConnector;\n import io.druid.metadata.SQLMetadataRuleManager;\n import io.druid.metadata.SQLMetadataRuleManagerProvider;\n@@ -65,106 +64,28 @@ public SQLMetadataStorageDruidModule(String type)\n   /**\n    * This function only needs to be called by the default SQL metadata storage module\n    * Other modules should default to calling super.configure(...) alone\n+   *\n+   * @param defaultValue default property value\n    */\n-  public void createBindingChoices(Binder binder, String defaultPropertyValue)\n+  public void createBindingChoices(Binder binder, String defaultValue)\n   {\n-    PolyBind.createChoiceWithDefault(\n-        binder, PROPERTY, Key.get(MetadataStorageConnector.class), null, defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataStorageProvider.class),\n-        Key.get(NoopMetadataStorageProvider.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder, PROPERTY, Key.get(SQLMetadataConnector.class), null, defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataSegmentManager.class),\n-        Key.get(SQLMetadataSegmentManager.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataSegmentManagerProvider.class),\n-        Key.get(SQLMetadataSegmentManagerProvider.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataRuleManager.class),\n-        Key.get(SQLMetadataRuleManager.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataRuleManagerProvider.class),\n-        Key.get(SQLMetadataRuleManagerProvider.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataSegmentPublisher.class),\n-        Key.get(SQLMetadataSegmentPublisher.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataSegmentPublisherProvider.class),\n-        Key.get(SQLMetadataSegmentPublisherProvider.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(IndexerMetadataStorageCoordinator.class),\n-        Key.get(IndexerSQLMetadataStorageCoordinator.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataStorageActionHandlerFactory.class),\n-        Key.get(SQLMetadataStorageActionHandlerFactory.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataStorageUpdaterJobHandler.class),\n-        Key.get(SQLMetadataStorageUpdaterJobHandler.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(AuditManager.class),\n-        Key.get(SQLAuditManager.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(AuditManagerProvider.class),\n-        Key.get(SQLAuditManagerProvider.class),\n-        defaultPropertyValue\n-    );\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        PROPERTY,\n-        Key.get(MetadataSupervisorManager.class),\n-        Key.get(SQLMetadataSupervisorManager.class),\n-        defaultPropertyValue\n-    );\n+    String prop = PROPERTY;\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataStorageConnector.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataStorageProvider.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(SQLMetadataConnector.class), defaultValue);\n+\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataSegmentManager.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataSegmentManagerProvider.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataRuleManager.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataRuleManagerProvider.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataSegmentPublisher.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataSegmentPublisherProvider.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(IndexerMetadataStorageCoordinator.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataStorageActionHandlerFactory.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataStorageUpdaterJobHandler.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(AuditManager.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(AuditManagerProvider.class), defaultValue);\n+    PolyBind.createChoiceWithDefault(binder, prop, Key.get(MetadataSupervisorManager.class), defaultValue);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/server/src/main/java/io/druid/guice/SQLMetadataStorageDruidModule.java",
                "sha": "78b9dc1ed5ebb5ef7c2f227c040beda8fc8753a2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/976492c18644614fa7d4cf0cd1ad508929579e6c/services/src/main/java/io/druid/guice/RealtimeModule.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/io/druid/guice/RealtimeModule.java?ref=976492c18644614fa7d4cf0cd1ad508929579e6c",
                "deletions": 7,
                "filename": "services/src/main/java/io/druid/guice/RealtimeModule.java",
                "patch": "@@ -58,13 +58,7 @@\n   @Override\n   public void configure(Binder binder)\n   {\n-    PolyBind.createChoiceWithDefault(\n-        binder,\n-        \"druid.publish.type\",\n-        Key.get(SegmentPublisher.class),\n-        null,\n-        \"metadata\"\n-    );\n+    PolyBind.createChoiceWithDefault(binder, \"druid.publish.type\", Key.get(SegmentPublisher.class), \"metadata\");\n     final MapBinder<String, SegmentPublisher> publisherBinder = PolyBind.optionBinder(\n         binder,\n         Key.get(SegmentPublisher.class)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/976492c18644614fa7d4cf0cd1ad508929579e6c/services/src/main/java/io/druid/guice/RealtimeModule.java",
                "sha": "827b94956b07f1222dbe9085f3a63c5bb1a32127",
                "status": "modified"
            }
        ],
        "message": "Make PolyBind to fail if property value is not found (fixes #4369) (#4374)\n\n* Make PolyBind to fail if property value is not found\r\n\r\n* Fix test\r\n\r\n* Add onHeap option in NamespaceExtractionModule\r\n\r\n* Add PolyBind.createChoiceWithDefaultNoScope()\r\n\r\n* Fix NPE\r\n\r\n* Fix\r\n\r\n* Configure MetadataStorageProvider option for MySQL, PostgreSQL and SQLServer\r\n\r\n* Deprecate PolyBind.createChoiceWithDefault form with unused defaultKey\r\n\r\n* Fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/073695d311492ce5aa0a8f2d9a2ec4a0744a93f5",
        "repo": "incubator-druid",
        "unit_tests": [
            "PolyBindTest.java"
        ]
    },
    "incubator-druid_9ac5eee": {
        "bug_id": "incubator-druid_9ac5eee",
        "commit": "https://github.com/apache/incubator-druid/commit/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java?ref=9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc",
                "deletions": 1,
                "filename": "indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -146,7 +146,7 @@ public boolean run()\n         log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n \n         if(!groupByJob.waitForCompletion(true)) {\n-          log.error(\"Job failed: %s\", groupByJob.getJobID().toString());\n+          log.error(\"Job failed: %s\", groupByJob.getJobID());\n           return false;\n         }\n       } else {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9ac5eeebb3925ca3155f8255a7f9bbd1723c5ddc/indexer/src/main/java/com/metamx/druid/indexer/DeterminePartitionsJob.java",
                "sha": "13490e3ef00c344912636faae2c4d3e0a17f663f",
                "status": "modified"
            }
        ],
        "message": "1) Fix NPE in DeterminePartitionsJob when it fails",
        "parent": "https://github.com/apache/incubator-druid/commit/ec2b906fada9255f779d9d3ebfb13ab9f0ef9129",
        "repo": "incubator-druid",
        "unit_tests": [
            "DeterminePartitionsJobTest.java"
        ]
    },
    "incubator-druid_9b5889b": {
        "bug_id": "incubator-druid_9b5889b",
        "commit": "https://github.com/apache/incubator-druid/commit/9b5889b39333127b538e55571d46d79df8b13bef",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9b5889b39333127b538e55571d46d79df8b13bef/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java?ref=9b5889b39333127b538e55571d46d79df8b13bef",
                "deletions": 9,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "patch": "@@ -215,15 +215,18 @@ public TaskStatus call()\n                               }\n \n                               // Override task specific properties\n-                              for (String propName : task.getContext().keySet()) {\n-                                if (propName.startsWith(CHILD_PROPERTY_PREFIX)) {\n-                                  command.add(\n-                                      String.format(\n-                                          \"-D%s=%s\",\n-                                          propName.substring(CHILD_PROPERTY_PREFIX.length()),\n-                                          task.getContextValue(propName)\n-                                      )\n-                                  );\n+                              final Map<String, Object> context = task.getContext();\n+                              if (context != null) {\n+                                for (String propName : context.keySet()) {\n+                                  if (propName.startsWith(CHILD_PROPERTY_PREFIX)) {\n+                                    command.add(\n+                                        String.format(\n+                                            \"-D%s=%s\",\n+                                            propName.substring(CHILD_PROPERTY_PREFIX.length()),\n+                                            task.getContextValue(propName)\n+                                        )\n+                                    );\n+                                  }\n                                 }\n                               }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9b5889b39333127b538e55571d46d79df8b13bef/indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
                "sha": "6d249f4284d03e6cc2f7e6ffbe340910a828e75d",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1703 from metamx/fix-npe\n\nadd null check for task context.",
        "parent": "https://github.com/apache/incubator-druid/commit/75a582974b8d3305981af30e691c67dee35acb63",
        "repo": "incubator-druid",
        "unit_tests": [
            "ForkingTaskRunnerTest.java"
        ]
    },
    "incubator-druid_9bece8c": {
        "bug_id": "incubator-druid_9bece8c",
        "commit": "https://github.com/apache/incubator-druid/commit/9bece8ce1e422257ab9a38ef8a56a9d91e728bfb",
        "file": [
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9bece8ce1e422257ab9a38ef8a56a9d91e728bfb/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=9bece8ce1e422257ab9a38ef8a56a9d91e728bfb",
                "deletions": 30,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -143,7 +143,8 @@\n    * time, there should only be up to a maximum of [taskCount] actively-reading task groups (tracked in the [taskGroups]\n    * map) + zero or more pending-completion task groups (tracked in [pendingCompletionTaskGroups]).\n    */\n-  private static class TaskGroup\n+  @VisibleForTesting\n+  static class TaskGroup\n   {\n     // This specifies the partitions and starting offsets for this task group. It is set on group creation from the data\n     // in [partitionGroups] and never changes during the lifetime of this task group, which will live until a task in\n@@ -777,8 +778,8 @@ void resetInternal(DataSourceMetadata dataSourceMetadata)\n           resetKafkaMetadata.getKafkaPartitions().getPartitionOffsetMap().keySet().forEach(partition -> {\n             final int groupId = getTaskGroupIdForPartition(partition);\n             killTaskGroupForPartitions(ImmutableSet.of(partition));\n-            sequenceTaskGroup.remove(generateSequenceName(groupId));\n-            taskGroups.remove(groupId);\n+            final TaskGroup removedGroup = taskGroups.remove(groupId);\n+            sequenceTaskGroup.remove(generateSequenceName(removedGroup));\n             partitionGroups.get(groupId).replaceAll((partitionId, offset) -> NOT_SET);\n           });\n         } else {\n@@ -886,12 +887,13 @@ String generateSequenceName(\n   }\n \n   @VisibleForTesting\n-  String generateSequenceName(int groupId)\n+  String generateSequenceName(TaskGroup taskGroup)\n   {\n+    Preconditions.checkNotNull(taskGroup, \"taskGroup cannot be null\");\n     return generateSequenceName(\n-        taskGroups.get(groupId).partitionOffsets,\n-        taskGroups.get(groupId).minimumMessageTime,\n-        taskGroups.get(groupId).maximumMessageTime\n+        taskGroup.partitionOffsets,\n+        taskGroup.minimumMessageTime,\n+        taskGroup.maximumMessageTime\n     );\n   }\n \n@@ -1085,18 +1087,19 @@ public Boolean apply(KafkaIndexTask.Status status)\n                             }\n                             return false;\n                           } else {\n+                            final TaskGroup taskGroup = new TaskGroup(\n+                                ImmutableMap.copyOf(\n+                                    kafkaTask.getIOConfig()\n+                                             .getStartPartitions()\n+                                             .getPartitionOffsetMap()\n+                                ), kafkaTask.getIOConfig().getMinimumMessageTime(),\n+                                kafkaTask.getIOConfig().getMaximumMessageTime()\n+                            );\n                             if (taskGroups.putIfAbsent(\n                                 taskGroupId,\n-                                new TaskGroup(\n-                                    ImmutableMap.copyOf(\n-                                        kafkaTask.getIOConfig()\n-                                                 .getStartPartitions()\n-                                                 .getPartitionOffsetMap()\n-                                    ), kafkaTask.getIOConfig().getMinimumMessageTime(),\n-                                    kafkaTask.getIOConfig().getMaximumMessageTime()\n-                                )\n+                                taskGroup\n                             ) == null) {\n-                              sequenceTaskGroup.put(generateSequenceName(taskGroupId), taskGroups.get(taskGroupId));\n+                              sequenceTaskGroup.put(generateSequenceName(taskGroup), taskGroups.get(taskGroupId));\n                               log.info(\"Created new task group [%d]\", taskGroupId);\n                             }\n                             taskGroupsToVerify.add(taskGroupId);\n@@ -1253,7 +1256,7 @@ public void onFailure(Throwable t)\n       // killing all tasks or no task left in the group ?\n       // clear state about the taskgroup so that get latest offset information is fetched from metadata store\n       log.warn(\"Clearing task group [%d] information as no valid tasks left the group\", groupId);\n-      sequenceTaskGroup.remove(generateSequenceName(groupId));\n+      sequenceTaskGroup.remove(generateSequenceName(taskGroup));\n       taskGroups.remove(groupId);\n       partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n     }\n@@ -1429,7 +1432,7 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n         partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n       }\n \n-      sequenceTaskGroup.remove(generateSequenceName(groupId));\n+      sequenceTaskGroup.remove(generateSequenceName(group));\n       // remove this task group from the list of current task groups now that it has been handled\n       taskGroups.remove(groupId);\n     }\n@@ -1630,8 +1633,7 @@ private void checkPendingCompletionTasks() throws ExecutionException, Interrupte\n \n           // reset partitions offsets for this task group so that they will be re-read from metadata storage\n           partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n-          sequenceTaskGroup.remove(generateSequenceName(groupId));\n-\n+          sequenceTaskGroup.remove(generateSequenceName(group));\n           // kill all the tasks in this pending completion group\n           killTasksInGroup(group);\n           // set a flag so the other pending completion groups for this set of partitions will also stop\n@@ -1691,7 +1693,7 @@ private void checkCurrentTaskState() throws ExecutionException, InterruptedExcep\n         // be recreated with the next set of offsets\n         if (taskData.status.isSuccess()) {\n           futures.add(stopTasksInGroup(taskGroup));\n-          sequenceTaskGroup.remove(generateSequenceName(groupId));\n+          sequenceTaskGroup.remove(generateSequenceName(taskGroup));\n           iTaskGroups.remove();\n           break;\n         }\n@@ -1724,15 +1726,16 @@ void createNewTasks() throws JsonProcessingException\n             DateTimes.nowUtc().plus(ioConfig.getTaskDuration()).plus(ioConfig.getEarlyMessageRejectionPeriod().get())\n         ) : Optional.<DateTime>absent());\n \n+        final TaskGroup taskGroup = new TaskGroup(\n+            generateStartingOffsetsForPartitionGroup(groupId),\n+            minimumMessageTime,\n+            maximumMessageTime\n+        );\n         taskGroups.put(\n             groupId,\n-            new TaskGroup(\n-                generateStartingOffsetsForPartitionGroup(groupId),\n-                minimumMessageTime,\n-                maximumMessageTime\n-            )\n+            taskGroup\n         );\n-        sequenceTaskGroup.put(generateSequenceName(groupId), taskGroups.get(groupId));\n+        sequenceTaskGroup.put(generateSequenceName(taskGroup), taskGroups.get(groupId));\n       }\n     }\n \n@@ -1767,8 +1770,8 @@ private void createKafkaTasksForGroup(int groupId, int replicas) throws JsonProc\n     for (Integer partition : startPartitions.keySet()) {\n       endPartitions.put(partition, Long.MAX_VALUE);\n     }\n-\n-    String sequenceName = generateSequenceName(groupId);\n+    TaskGroup group = taskGroups.get(groupId);\n+    String sequenceName = generateSequenceName(group);\n \n     Map<String, String> consumerProperties = Maps.newHashMap(ioConfig.getConsumerProperties());\n     DateTime minimumMessageTime = taskGroups.get(groupId).minimumMessageTime.orNull();\n@@ -1929,7 +1932,7 @@ private boolean isTaskCurrent(int taskGroupId, String taskId)\n \n     String taskSequenceName = ((KafkaIndexTask) taskOptional.get()).getIOConfig().getBaseSequenceName();\n     if (taskGroups.get(taskGroupId) != null) {\n-      return generateSequenceName(taskGroupId).equals(taskSequenceName);\n+      return generateSequenceName(taskGroups.get(taskGroupId)).equals(taskSequenceName);\n     } else {\n       return generateSequenceName(\n           ((KafkaIndexTask) taskOptional.get()).getIOConfig()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9bece8ce1e422257ab9a38ef8a56a9d91e728bfb/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "39b19ee73719356627c95c3888b346b90cc6ec2b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/9bece8ce1e422257ab9a38ef8a56a9d91e728bfb/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java?ref=9bece8ce1e422257ab9a38ef8a56a9d91e728bfb",
                "deletions": 7,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "patch": "@@ -2192,20 +2192,15 @@ public TestableKafkaSupervisor(\n       );\n     }\n \n-    @Override\n-    protected String generateSequenceName(int groupId)\n-    {\n-      return StringUtils.format(\"sequenceName-%d\", groupId);\n-    }\n-\n     @Override\n     protected String generateSequenceName(\n         Map<Integer, Long> startPartitions,\n         Optional<DateTime> minimumMessageTime,\n         Optional<DateTime> maximumMessageTime\n     )\n     {\n-      return generateSequenceName(getTaskGroupIdForPartition(startPartitions.keySet().iterator().next()));\n+      final int groupId = getTaskGroupIdForPartition(startPartitions.keySet().iterator().next());\n+      return StringUtils.format(\"sequenceName-%d\", groupId);\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/9bece8ce1e422257ab9a38ef8a56a9d91e728bfb/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "sha": "1d951de9d0b1b70d4c035c381c6a2ae7ba22e076",
                "status": "modified"
            }
        ],
        "message": "Prevent KafkaSupervisor NPE in generateSequenceName (#5900) (#5902)\n\n* Prevent KafkaSupervisor NPE in checkPendingCompletionTasks (#5900)\r\n\r\n* throw IAE in generateSequenceName if groupId not found in taskGroups\r\n* add null check in checkPendingCompletionTasks\r\n\r\n* Add warn log in checkPendingCompletionTasks\r\n\r\n* Address PR comments\r\n\r\nReplace warn with error log\r\n\r\n* Address PR comments\r\n\r\n* change signature of generateSequenceName to take a TaskGroup object instead of int\r\n\r\n* Address comments\r\n\r\n* Remove unnecessary method from KafkaSupervisorTest",
        "parent": "https://github.com/apache/incubator-druid/commit/4cd14e8158153ffc6432f7f09b148dca83517f5b",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaSupervisorTest.java"
        ]
    },
    "incubator-druid_a0c2ae7": {
        "bug_id": "incubator-druid_a0c2ae7",
        "commit": "https://github.com/apache/incubator-druid/commit/a0c2ae7a388920a6118d4461018247390aea603a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "patch": "@@ -440,7 +440,7 @@ public void run(Context context)\n     public int getPartition(LongWritable interval, BytesWritable text, int numPartitions)\n     {\n \n-      if (\"local\".equals(config.get(\"mapred.job.tracker\")) || determineIntervals) {\n+      if (\"local\".equals(JobHelper.getJobTrackerAddress(config)) || determineIntervals) {\n         return 0;\n       } else {\n         return reducerLookup.get(interval);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DetermineHashedPartitionsJob.java",
                "sha": "a85f56602607dd1d5ec6cdd6840ea3ef07a6c127",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "patch": "@@ -485,7 +485,8 @@ public int getPartition(BytesWritable bytesWritable, Text text, int numPartition\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       final int index = bytes.getInt();\n-      if (config.get(\"mapred.job.tracker\").equals(\"local\")) {\n+      String jobTrackerAddress = JobHelper.getJobTrackerAddress(config);\n+      if (\"local\".equals(jobTrackerAddress)) {\n         return index % numPartitions;\n       } else {\n         if (index >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/DeterminePartitionsJob.java",
                "sha": "0064a4363d550689f9b972c86e49583aa0502309",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 1,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "patch": "@@ -535,7 +535,7 @@ public int getPartition(BytesWritable bytesWritable, Writable value, int numPart\n       final ByteBuffer bytes = ByteBuffer.wrap(bytesWritable.getBytes());\n       bytes.position(4); // Skip length added by SortableBytes\n       int shardNum = bytes.getInt();\n-      if (\"local\".equals(config.get(\"mapreduce.jobtracker.address\")) || \"local\".equals(config.get(\"mapred.job.tracker\"))) {\n+      if (\"local\".equals(JobHelper.getJobTrackerAddress(config))) {\n         return shardNum % numPartitions;\n       } else {\n         if (shardNum >= numPartitions) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
                "sha": "a848714c8eb9cdeebd81169f5108e1e543030f45",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java?ref=a0c2ae7a388920a6118d4461018247390aea603a",
                "deletions": 0,
                "filename": "indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "patch": "@@ -831,4 +831,14 @@ public static boolean deleteWithRetry(final FileSystem fs, final Path path, fina\n       throw Throwables.propagate(e);\n     }\n   }\n+\n+  public static String getJobTrackerAddress(Configuration config)\n+  {\n+    String jobTrackerAddress = config.get(\"mapred.job.tracker\");\n+    if (jobTrackerAddress == null) {\n+      // New Property name for Hadoop 3.0 and later versions\n+      jobTrackerAddress = config.get(\"mapreduce.jobtracker.address\");\n+    }\n+    return jobTrackerAddress;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a0c2ae7a388920a6118d4461018247390aea603a/indexing-hadoop/src/main/java/io/druid/indexer/JobHelper.java",
                "sha": "bbe4f2bc37e307c18cf04b427c9634fc8b5c67f2",
                "status": "modified"
            }
        ],
        "message": "Fix NullPointerException when in DeterminePartitionsJob for Hadoop 3.0 and later versions (#5724)\n\nIn DeterminePartitonsJob -\r\nconfig.get(\"mapred.job.tracker\").equals(\"local\") throws NPE as the\r\nproperty name is changed in hadoop 3.0 to mapreduce.jobtracker.address\r\n\r\nThis patch extracts the logic to fetch jobTrackerAddress in JobHelper\r\nand reuses it when needed.",
        "parent": "https://github.com/apache/incubator-druid/commit/754c80e74a2dc725d79569e89f95682b209a518c",
        "repo": "incubator-druid",
        "unit_tests": [
            "DetermineHashedPartitionsJobTest.java",
            "DeterminePartitionsJobTest.java",
            "IndexGeneratorJobTest.java",
            "JobHelperTest.java"
        ]
    },
    "incubator-druid_a4777ed": {
        "bug_id": "incubator-druid_a4777ed",
        "commit": "https://github.com/apache/incubator-druid/commit/a4777ede94a4798c19859e57ed82bd3a3ecedf67",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a4777ede94a4798c19859e57ed82bd3a3ecedf67/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java?ref=a4777ede94a4798c19859e57ed82bd3a3ecedf67",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "patch": "@@ -255,13 +255,18 @@ public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> d\n                       final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n                       for (String dim : dims) {\n                         final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim);\n-                        dimSelectors.put(dim, dimSelector);\n+                        // dimSelector is null if the dimension is not present\n+                        if (dimSelector != null) {\n+                          dimSelectors.put(dim, dimSelector);\n+                        }\n                       }\n \n                       final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n                       for (String metric : metrics) {\n                         final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n-                        metSelectors.put(metric, metricSelector);\n+                        if (metricSelector != null) {\n+                          metSelectors.put(metric, metricSelector);\n+                        }\n                       }\n \n                       return Sequences.simple(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a4777ede94a4798c19859e57ed82bd3a3ecedf67/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "sha": "5744fa7f006f3e46227093b2be78e6da7545fba2",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #747 from metamx/ingest-segment-npe-fix\n\nfix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/a17794a51668cb83d07a4c7220960bfc29ad309e",
        "repo": "incubator-druid",
        "unit_tests": [
            "IngestSegmentFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_a72c442": {
        "bug_id": "incubator-druid_a72c442",
        "commit": "https://github.com/apache/incubator-druid/commit/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java?ref=a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719",
                "deletions": 8,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "patch": "@@ -61,6 +61,7 @@\n import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;\n import org.apache.curator.utils.ZKPaths;\n import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n import org.jboss.netty.handler.codec.http.HttpResponseStatus;\n import org.joda.time.DateTime;\n \n@@ -335,7 +336,7 @@ public void shutdown(final String taskId)\n       pendingTaskPayloads.remove(taskId);\n       log.info(\"Removed task from pending queue: %s\", taskId);\n     } else if (completeTasks.containsKey(taskId)) {\n-      cleanup(completeTasks.get(taskId).getWorker().getHost(), taskId);\n+      cleanup(taskId);\n     } else {\n       final ZkWorker zkWorker = findWorkerRunningTask(taskId);\n \n@@ -469,28 +470,32 @@ public Void call() throws Exception\n   /**\n    * Removes a task from the complete queue and clears out the ZK status path of the task.\n    *\n-   * @param workerId - the worker that was previously running the task\n    * @param taskId   - the task to cleanup\n    */\n-  private void cleanup(final String workerId, final String taskId)\n+  private void cleanup(final String taskId)\n   {\n     if (!started) {\n       return;\n     }\n-    if (completeTasks.remove(taskId) == null) {\n+    final RemoteTaskRunnerWorkItem removed = completeTasks.remove(taskId);\n+    final Worker worker = removed.getWorker();\n+    if (removed == null || worker == null) {\n       log.makeAlert(\"WTF?! Asked to cleanup nonexistent task\")\n-         .addData(\"workerId\", workerId)\n          .addData(\"taskId\", taskId)\n          .emit();\n     } else {\n+      final String workerId = worker.getHost();\n       log.info(\"Cleaning up task[%s] on worker[%s]\", taskId, workerId);\n       final String statusPath = JOINER.join(zkPaths.getIndexerStatusPath(), workerId, taskId);\n       try {\n         cf.delete().guaranteed().forPath(statusPath);\n       }\n-      catch (Exception e) {\n+      catch (KeeperException.NoNodeException e) {\n         log.info(\"Tried to delete status path[%s] that didn't exist! Must've gone away already?\", statusPath);\n       }\n+      catch (Exception e) {\n+        throw Throwables.propagate(e);\n+      }\n     }\n   }\n \n@@ -593,7 +598,6 @@ private void announceTask(\n               elapsed,\n               config.getTaskAssignmentTimeout()\n           );\n-\n           taskComplete(taskRunnerWorkItem, theZkWorker, TaskStatus.failure(task.getId()));\n           break;\n         }\n@@ -666,7 +670,7 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) th\n                             SettableFuture.<TaskStatus>create(),\n                             zkWorker.getWorker()\n                         );\n-                        runningTasks.put(taskId, taskRunnerWorkItem);\n+                        runningTasks.put(taskId, taskRunnerWorkItem.withWorker(zkWorker.getWorker()));\n                       }\n \n                       if (taskStatus.isComplete()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a72c4429f7c2d50fb64b9b3f0e98e4e7aeaae719/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "sha": "0dbc9031200fa1ac43d9ff376e914fc1153f79ba",
                "status": "modified"
            }
        ],
        "message": "RemoteTaskRunner: Fix NPE on cleanup due to missing withWorker",
        "parent": "https://github.com/apache/incubator-druid/commit/35ed3f74bf4963670061cd05881af93f4d50b27c",
        "repo": "incubator-druid",
        "unit_tests": [
            "RemoteTaskRunnerTest.java"
        ]
    },
    "incubator-druid_a7b09e8": {
        "bug_id": "incubator-druid_a7b09e8",
        "commit": "https://github.com/apache/incubator-druid/commit/a7b09e857c4c44964b74b3d1582fc6f2cd425622",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java?ref=a7b09e857c4c44964b74b3d1582fc6f2cd425622",
                "deletions": 6,
                "filename": "processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "patch": "@@ -38,7 +38,11 @@ public int compare(String str1, String str2)\n     {\n       int[] pos = {0, 0};\n \n-      if (str1.length() == 0) {\n+      if (str1 == null) {\n+        return -1;\n+      } else if (str2 == null) {\n+        return 1;\n+      } else if (str1.length() == 0) {\n         return str2.length() == 0 ? 0 : -1;\n       } else if (str2.length() == 0) {\n         return 1;\n@@ -179,15 +183,12 @@ private int compareNonNumeric(String str0, String str1, int[] pos)\n     }\n   };\n \n-  private final String previousStop;\n-\n   @JsonCreator\n   public AlphaNumericTopNMetricSpec(\n       @JsonProperty(\"previousStop\") String previousStop\n   )\n   {\n     super(previousStop);\n-    this.previousStop = (previousStop == null) ? \"\" : previousStop;\n   }\n \n   @Override\n@@ -199,7 +200,7 @@ public Comparator getComparator(List<AggregatorFactory> aggregatorSpecs, List<Po\n   @Override\n   public byte[] getCacheKey()\n   {\n-    byte[] previousStopBytes = StringUtils.toUtf8(previousStop);\n+    byte[] previousStopBytes = getPreviousStop() == null ? new byte[]{} : StringUtils.toUtf8(getPreviousStop());\n \n     return ByteBuffer.allocate(1 + previousStopBytes.length)\n                      .put(CACHE_TYPE_ID)\n@@ -217,7 +218,7 @@ public Comparator getComparator(List<AggregatorFactory> aggregatorSpecs, List<Po\n   public String toString()\n   {\n     return \"AlphaNumericTopNMetricSpec{\" +\n-           \"previousStop='\" + previousStop + '\\'' +\n+           \"previousStop='\" + getPreviousStop() + '\\'' +\n            '}';\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/main/java/io/druid/query/topn/AlphaNumericTopNMetricSpec.java",
                "sha": "4daadbe8137d8744a9dfc1e1f74ba7542310b15a",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java?ref=a7b09e857c4c44964b74b3d1582fc6f2cd425622",
                "deletions": 1,
                "filename": "processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "patch": "@@ -17,11 +17,15 @@\n \n package io.druid.query.topn;\n \n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import io.druid.jackson.DefaultObjectMapper;\n import org.junit.Test;\n \n+import java.io.IOException;\n import java.util.Comparator;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n public class AlphaNumericTopNMetricSpecTest\n {\n@@ -60,4 +64,22 @@ public void testComparator() throws Exception\n     assertTrue(comparator.compare(\"1.3\", \"1.15\") < 0);\n \n   }\n+\n+  @Test\n+  public void testSerdeAlphaNumericTopNMetricSpec() throws IOException{\n+    AlphaNumericTopNMetricSpec expectedMetricSpec = new AlphaNumericTopNMetricSpec(null);\n+    AlphaNumericTopNMetricSpec expectedMetricSpec1 = new AlphaNumericTopNMetricSpec(\"test\");\n+    String jsonSpec = \"{\\n\"\n+                      + \"    \\\"type\\\": \\\"alphaNumeric\\\"\\n\"\n+                      + \"}\";\n+    String jsonSpec1 = \"{\\n\"\n+                       + \"    \\\"type\\\": \\\"alphaNumeric\\\",\\n\"\n+                       + \"    \\\"previousStop\\\": \\\"test\\\"\\n\"\n+                       + \"}\";\n+    ObjectMapper jsonMapper = new DefaultObjectMapper();\n+    TopNMetricSpec actualMetricSpec = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n+    TopNMetricSpec actualMetricSpec1 = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonSpec1, TopNMetricSpec.class)), AlphaNumericTopNMetricSpec.class);\n+    assertEquals(expectedMetricSpec, actualMetricSpec);\n+    assertEquals(expectedMetricSpec1, actualMetricSpec1);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/AlphaNumericTopNMetricSpecTest.java",
                "sha": "18c37614b2e925b56fe8a0aa0e746e86906981a1",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java?ref=a7b09e857c4c44964b74b3d1582fc6f2cd425622",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "patch": "@@ -2524,4 +2524,34 @@ public void testTopNOverPartialNullDimensionWithFilterOnNOTNullValue()\n     );\n     TestHelper.assertExpectedResults(expectedResults, runner.run(query, new HashMap<String, Object>()));\n   }\n+  @Test\n+  public void testAlphaNumericTopNWithNullPreviousStop(){\n+    TopNQuery query = new TopNQueryBuilder()\n+        .dataSource(QueryRunnerTestHelper.dataSource)\n+        .granularity(QueryGranularity.ALL)\n+        .dimension(QueryRunnerTestHelper.marketDimension)\n+        .metric(new AlphaNumericTopNMetricSpec(null))\n+        .threshold(2)\n+        .intervals(QueryRunnerTestHelper.secondOnly)\n+        .aggregators(Lists.<AggregatorFactory>newArrayList(QueryRunnerTestHelper.rowsCount))\n+        .build();\n+    List<Result<TopNResultValue>> expectedResults = Arrays.asList(\n+        new Result<>(\n+            new DateTime(\"2011-04-02T00:00:00.000Z\"),\n+            new TopNResultValue(\n+                Arrays.asList(\n+                    ImmutableMap.<String, Object>of(\n+                        \"market\", \"spot\",\n+                        \"rows\", 9L\n+                    ),\n+                    ImmutableMap.<String, Object>of(\n+                        \"market\", \"total_market\",\n+                        \"rows\", 2L\n+                    )\n+                )\n+            )\n+        )\n+    );\n+    TestHelper.assertExpectedResults(expectedResults, runner.run(query, new HashMap<String, Object>()));\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "sha": "4d678823ce0fd3c88e659dea6a316dfe1ef4e4a4",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java?ref=a7b09e857c4c44964b74b3d1582fc6f2cd425622",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import io.druid.query.aggregation.DoubleMaxAggregatorFactory;\n import io.druid.query.aggregation.DoubleMinAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n+import io.druid.query.dimension.LegacyDimensionSpec;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -39,6 +40,7 @@\n import static io.druid.query.QueryRunnerTestHelper.fullOnInterval;\n import static io.druid.query.QueryRunnerTestHelper.indexMetric;\n import static io.druid.query.QueryRunnerTestHelper.marketDimension;\n+import static io.druid.query.QueryRunnerTestHelper.rowsCount;\n \n public class TopNQueryTest\n {\n@@ -74,4 +76,38 @@ public void testQuerySerialization() throws IOException\n     Assert.assertEquals(query, serdeQuery);\n   }\n \n+  @Test\n+  public void testQuerySerdeWithAlphaNumericTopNMetricSpec() throws IOException{\n+    TopNQuery expectedQuery = new TopNQueryBuilder()\n+        .dataSource(dataSource)\n+        .granularity(allGran)\n+        .dimension(new LegacyDimensionSpec(marketDimension))\n+        .metric(new AlphaNumericTopNMetricSpec(null))\n+        .threshold(2)\n+        .intervals(fullOnInterval.getIntervals())\n+        .aggregators(Lists.<AggregatorFactory>newArrayList(rowsCount))\n+        .build();\n+    String jsonQuery = \"{\\n\"\n+                       + \"  \\\"queryType\\\": \\\"topN\\\",\\n\"\n+                       + \"  \\\"dataSource\\\": \\\"testing\\\",\\n\"\n+                       + \"  \\\"dimension\\\": \\\"market\\\",\\n\"\n+                       + \"  \\\"threshold\\\": 2,\\n\"\n+                       + \"  \\\"metric\\\": {\\n\"\n+                       + \"    \\\"type\\\": \\\"alphaNumeric\\\"\\n\"\n+                       + \"   },\\n\"\n+                       + \"  \\\"granularity\\\": \\\"all\\\",\\n\"\n+                       + \"  \\\"aggregations\\\": [\\n\"\n+                       + \"    {\\n\"\n+                       + \"      \\\"type\\\": \\\"count\\\",\\n\"\n+                       + \"      \\\"name\\\": \\\"rows\\\"\\n\"\n+                       + \"    }\\n\"\n+                       + \"  ],\\n\"\n+                       + \"  \\\"intervals\\\": [\\n\"\n+                       + \"    \\\"1970-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\\\"\\n\"\n+                       + \"  ]\\n\"\n+                       + \"}\";\n+    TopNQuery actualQuery = jsonMapper.readValue(jsonMapper.writeValueAsString(jsonMapper.readValue(jsonQuery, TopNQuery.class)), TopNQuery.class);\n+    Assert.assertEquals(expectedQuery, actualQuery);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a7b09e857c4c44964b74b3d1582fc6f2cd425622/processing/src/test/java/io/druid/query/topn/TopNQueryTest.java",
                "sha": "bb2c37b2c24c6f6e6ffdc2fbbe89d16986fd235b",
                "status": "modified"
            }
        ],
        "message": "NPE fix for alphaNumericTopN when pervious stop is not specified",
        "parent": "https://github.com/apache/incubator-druid/commit/600b30566cd94bed06876fd8a474a289ab0c3c4d",
        "repo": "incubator-druid",
        "unit_tests": [
            "AlphaNumericTopNMetricSpecTest.java"
        ]
    },
    "incubator-druid_a933438": {
        "bug_id": "incubator-druid_a933438",
        "commit": "https://github.com/apache/incubator-druid/commit/a933438e4e266eb1aebf308ccc860188060d8dd6",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/client/src/main/java/com/metamx/druid/client/indexing/IndexingServiceClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/client/indexing/IndexingServiceClient.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "client/src/main/java/com/metamx/druid/client/indexing/IndexingServiceClient.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableMap;\n import com.metamx.common.IAE;\n+import com.metamx.common.ISE;\n import com.metamx.druid.client.DataSegment;\n import com.metamx.http.client.HttpClient;\n import com.metamx.http.client.response.InputStreamResponseHandler;\n@@ -106,6 +107,10 @@ private String baseUrl()\n   {\n     try {\n       final ServiceInstance instance = serviceProvider.getInstance();\n+      if (instance == null) {\n+        throw new ISE(\"Cannot find instance of indexingService\");\n+      }\n+\n       return String.format(\"http://%s:%s/mmx/merger/v1\", instance.getAddress(), instance.getPort());\n     }\n     catch (Exception e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/client/src/main/java/com/metamx/druid/client/indexing/IndexingServiceClient.java",
                "sha": "34277797cbaf1bdf2849a43f2270fe0ae6b273ea",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/pom.xml?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/pom.xml",
                "patch": "@@ -71,6 +71,10 @@\n             <groupId>com.google.guava</groupId>\n             <artifactId>guava</artifactId>\n         </dependency>\n+        <dependency>\n+            <groupId>commons-io</groupId>\n+            <artifactId>commons-io</artifactId>\n+        </dependency>\n \n         <!-- Tests -->\n         <dependency>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/pom.xml",
                "sha": "125abb066c7c3b1970be3f803048fd063101cc9e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "sha": "d09581c1c5b71b4c6c07a8897902bbcc42dcf32c",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "sha": "20ec5a4d30d310ba78fd4c1afba23235c08338b6",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndexAdapter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndexAdapter.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndexAdapter.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/IncrementalIndexAdapter.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndexAdapter.java",
                "sha": "3da50e3eaea6458e5b9cfab104239abe78d2b8ce",
                "status": "renamed"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "patch": "@@ -204,10 +204,31 @@ public static boolean convertSegment(File toConvert, File converted) throws IOEx\n     final int version = getVersionFromDir(toConvert);\n \n     switch (version) {\n+      case 1:\n+      case 2:\n+      case 3:\n+        final String mappableDirName = \"mappable\";\n+        if (toConvert.getName().equals(mappableDirName)) {\n+          throw new ISE(\"Infinite recursion at play!  OMFG quit it, please, it hurts!\");\n+        }\n+\n+        File mappable = new File(toConvert, mappableDirName);\n+        final Index index = readIndex(toConvert);\n+        storeLatest(index, mappable);\n+\n+        return convertSegment(mappable, converted);\n+      case 4:\n+      case 5:\n+      case 6:\n+      case 7:\n+        log.info(\"Old version, re-persisting.\");\n+        IndexMerger.append(Arrays.<IndexableAdapter>asList(new QueryableIndexIndexableAdapter(loadIndex(toConvert))), converted);\n+        return true;\n       case 8:\n         DefaultIndexIOHandler.convertV8toV9(toConvert, converted);\n         return true;\n       default:\n+        log.info(\"Version[%s], skipping.\", version);\n         return false;\n     }\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "sha": "aa10fa8b2ecb3d1dff284ea913030408127823a0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IndexMerger.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "patch": "@@ -310,9 +310,11 @@ public static File append(\n       throw new ISE(\"Couldn't make outdir[%s].\", outDir);\n     }\n \n+/*\n     if (indexes.size() < 2) {\n       throw new ISE(\"Too few indexes provided for append [%d].\", indexes.size());\n     }\n+*/\n \n     final List<String> mergedDimensions = mergeIndexed(\n         Lists.transform(",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "sha": "dc03b8866dd4d08b7036b4ac6e97eccbdbc7dc21",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexableAdapter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IndexableAdapter.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IndexableAdapter.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/IndexableAdapter.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/IndexableAdapter.java",
                "sha": "5c65a96995755aad6b1082db2de3541bdb505195",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/MMappedIndexAdapter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/MMappedIndexAdapter.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/MMappedIndexAdapter.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/MMappedIndexAdapter.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/MMappedIndexAdapter.java",
                "sha": "396fb0e2de0810ea4537b2436eb3d829f6d23044",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/MetricColumnSerializer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/MetricColumnSerializer.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/MetricColumnSerializer.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/MetricColumnSerializer.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/MetricColumnSerializer.java",
                "sha": "253ed3cf5a4694065c867c9d17ccd9cd66f52e0b",
                "status": "renamed"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 3,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "patch": "@@ -19,12 +19,12 @@\n \n package com.metamx.druid.index.v1;\n \n-import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.google.common.io.Closeables;\n import com.metamx.common.ISE;\n+import com.metamx.common.logger.Logger;\n import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.index.column.BitmapIndex;\n import com.metamx.druid.index.column.Column;\n@@ -44,6 +44,7 @@\n import java.io.Closeable;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Map;\n import java.util.NoSuchElementException;\n import java.util.Set;\n@@ -52,13 +53,35 @@\n */\n public class QueryableIndexIndexableAdapter implements IndexableAdapter\n {\n+  private static final Logger log = new Logger(QueryableIndexIndexableAdapter.class);\n+\n   private final int numRows;\n   private final QueryableIndex input;\n \n+  private final List<String> availableDimensions;\n+\n   public QueryableIndexIndexableAdapter(QueryableIndex input)\n   {\n     this.input = input;\n     numRows = input.getNumRows();\n+\n+    // It appears possible that the dimensions have some columns listed which do not have a DictionaryEncodedColumn\n+    // This breaks current logic, but should be fine going forward.  This is a work-around to make things work\n+    // in the current state.  This code shouldn't be needed once github tracker issue #55 is finished.\n+    this.availableDimensions = Lists.newArrayList();\n+    for (String dim : input.getAvailableDimensions()) {\n+      final Column col = input.getColumn(dim);\n+\n+      if (col == null) {\n+        log.warn(\"Wtf!? column[%s] didn't exist!?!?!?\", dim);\n+      }\n+      else if (col.getDictionaryEncoding() != null) {\n+        availableDimensions.add(dim);\n+      }\n+      else {\n+        log.info(\"No dictionary on dimension[%s]\", dim);\n+      }\n+    }\n   }\n \n   @Override\n@@ -76,7 +99,7 @@ public int getNumRows()\n   @Override\n   public Indexed<String> getAvailableDimensions()\n   {\n-    return input.getAvailableDimensions();\n+    return new ListIndexed<String>(availableDimensions, String.class);\n   }\n \n   @Override\n@@ -161,7 +184,7 @@ public int indexOf(String value)\n \n           {\n             dimensions = Maps.newLinkedHashMap();\n-            for (String dim : input.getAvailableDimensions()) {\n+            for (String dim : getAvailableDimensions()) {\n               dimensions.put(dim, input.getColumn(dim).getDictionaryEncoding());\n             }\n ",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "sha": "26a9b63add7824247c1584d2761f24e498a50c3d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/Rowboat.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/Rowboat.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/Rowboat.java",
                "previous_filename": "server/src/main/java/com/metamx/druid/index/v1/Rowboat.java",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/index-common/src/main/java/com/metamx/druid/index/v1/Rowboat.java",
                "sha": "551bf210bb5719e6b1807bec6d2983c444ba1185",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/pom.xml",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/pom.xml?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 21,
                "filename": "merger/pom.xml",
                "patch": "@@ -182,25 +182,4 @@\n             <artifactId>curator-test</artifactId>\n         </dependency>\n     </dependencies>\n-\n-    <build>\n-        <plugins>\n-            <plugin>\n-                <artifactId>maven-shade-plugin</artifactId>\n-                <executions>\n-                    <execution>\n-                        <phase>package</phase>\n-                        <goals>\n-                            <goal>shade</goal>\n-                        </goals>\n-                        <configuration>\n-                            <outputFile>\n-                                ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n-                            </outputFile>\n-                        </configuration>\n-                    </execution>\n-                </executions>\n-            </plugin>\n-        </plugins>\n-    </build>\n </project>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/pom.xml",
                "sha": "cc5826a2f742ccbdb1abf4ee5bb3f16a4ab72c58",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/src/main/java/com/metamx/druid/merger/common/task/VersionConverterTask.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/common/task/VersionConverterTask.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 9,
                "filename": "merger/src/main/java/com/metamx/druid/merger/common/task/VersionConverterTask.java",
                "patch": "@@ -34,6 +34,7 @@\n import com.metamx.druid.merger.common.TaskStatus;\n import com.metamx.druid.merger.common.TaskToolbox;\n import com.metamx.druid.merger.common.actions.SegmentInsertAction;\n+import com.metamx.druid.merger.common.actions.SegmentListUsedAction;\n import com.metamx.druid.merger.common.actions.SpawnTasksAction;\n import com.metamx.druid.merger.common.actions.TaskActionClient;\n import org.joda.time.DateTime;\n@@ -77,20 +78,34 @@ private static String makeId(String dataSource, Interval interval)\n   }\n \n   @JsonCreator\n-  private VersionConverterTask(\n+  private static VersionConverterTask createFromJson(\n       @JsonProperty(\"id\") String id,\n       @JsonProperty(\"groupId\") String groupId,\n       @JsonProperty(\"dataSource\") String dataSource,\n       @JsonProperty(\"interval\") Interval interval,\n       @JsonProperty(\"segment\") DataSegment segment\n   )\n   {\n-    super(\n-        id,\n-        groupId,\n-        dataSource,\n-        interval\n-    );\n+    if (id == null) {\n+      if (segment == null) {\n+        return create(dataSource, interval);\n+      }\n+      else {\n+        return create(segment);\n+      }\n+    }\n+    return new VersionConverterTask(id, groupId, dataSource, interval, segment);\n+  }\n+\n+  private VersionConverterTask(\n+      String id,\n+      String groupId,\n+      String dataSource,\n+      Interval interval,\n+      DataSegment segment\n+  )\n+  {\n+    super(id, groupId, dataSource, interval);\n \n     this.segment = segment;\n   }\n@@ -122,6 +137,7 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n   @Override\n   public TaskStatus preflight(TaskToolbox toolbox) throws Exception\n   {\n+    log.info(\"HLKFJDSLKFJDSKLJFLKDSF -- Preflight for segment[%s]\", segment);\n     if (segment != null) {\n       return super.preflight(toolbox);\n     }\n@@ -224,6 +240,21 @@ private static void convertSegment(TaskToolbox toolbox, final DataSegment segmen\n       throws SegmentLoadingException, IOException\n   {\n     log.info(\"Converting segment[%s]\", segment);\n+    final TaskActionClient actionClient = toolbox.getTaskActionClient();\n+    final List<DataSegment> currentSegments = actionClient.submit(\n+        new SegmentListUsedAction(segment.getDataSource(), segment.getInterval())\n+    );\n+\n+    for (DataSegment currentSegment : currentSegments) {\n+      final String version = currentSegment.getVersion();\n+      final Integer binaryVersion = currentSegment.getBinaryVersion();\n+\n+      if (version.startsWith(segment.getVersion()) && CURR_VERSION_INTEGER.equals(binaryVersion)) {\n+        log.info(\"Skipping already updated segment[%s].\", segment);\n+        return;\n+      }\n+    }\n+\n     final Map<DataSegment, File> localSegments = toolbox.getSegments(Arrays.asList(segment));\n \n     final File location = localSegments.get(segment);\n@@ -236,8 +267,7 @@ private static void convertSegment(TaskToolbox toolbox, final DataSegment segmen\n       DataSegment updatedSegment = segment.withVersion(String.format(\"%s_v%s\", segment.getVersion(), outVersion));\n       updatedSegment = toolbox.getSegmentPusher().push(outLocation, updatedSegment);\n \n-      toolbox.getTaskActionClient()\n-             .submit(new SegmentInsertAction(Sets.newHashSet(updatedSegment)).withAllowOlderVersions(true));\n+      actionClient.submit(new SegmentInsertAction(Sets.newHashSet(updatedSegment)).withAllowOlderVersions(true));\n     } else {\n       log.info(\"Conversion failed.\");\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/src/main/java/com/metamx/druid/merger/common/task/VersionConverterTask.java",
                "sha": "853207f0cf0da67d9caa83983a991ac8ad5e2f82",
                "status": "modified"
            },
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/src/main/java/com/metamx/druid/merger/coordinator/LocalTaskRunner.java",
                "changes": 159,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/coordinator/LocalTaskRunner.java?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 49,
                "filename": "merger/src/main/java/com/metamx/druid/merger/coordinator/LocalTaskRunner.java",
                "patch": "@@ -19,20 +19,33 @@\n \n package com.metamx.druid.merger.coordinator;\n \n+import com.google.common.base.Function;\n import com.google.common.base.Throwables;\n+import com.google.common.collect.Collections2;\n import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import com.metamx.common.guava.FunctionalIterable;\n import com.metamx.common.lifecycle.LifecycleStop;\n import com.metamx.common.logger.Logger;\n+import com.metamx.druid.merger.common.RetryPolicy;\n import com.metamx.druid.merger.common.TaskCallback;\n import com.metamx.druid.merger.common.TaskStatus;\n import com.metamx.druid.merger.common.TaskToolbox;\n import com.metamx.druid.merger.common.TaskToolboxFactory;\n import com.metamx.druid.merger.common.task.Task;\n import org.apache.commons.io.FileUtils;\n+import org.joda.time.DateTime;\n+import org.mortbay.thread.ThreadPool;\n \n+import javax.annotation.Nullable;\n import java.io.File;\n import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentSkipListSet;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n \n /**\n  * Runs tasks in a JVM thread using an ExecutorService.\n@@ -42,6 +55,8 @@\n   private final TaskToolboxFactory toolboxFactory;\n   private final ExecutorService exec;\n \n+  private final Set<TaskRunnerWorkItem> runningItems = new ConcurrentSkipListSet<TaskRunnerWorkItem>();\n+\n   private static final Logger log = new Logger(LocalTaskRunner.class);\n \n   public LocalTaskRunner(\n@@ -64,65 +79,39 @@ public void run(final Task task, final TaskCallback callback)\n   {\n     final TaskToolbox toolbox = toolboxFactory.build(task);\n \n-    exec.submit(\n-        new Runnable()\n-        {\n-          @Override\n-          public void run()\n-          {\n-            final long startTime = System.currentTimeMillis();\n-\n-            TaskStatus status;\n-\n-            try {\n-              log.info(\"Running task: %s\", task.getId());\n-              status = task.run(toolbox);\n-            }\n-            catch (InterruptedException e) {\n-              log.error(e, \"Interrupted while running task[%s]\", task);\n-              throw Throwables.propagate(e);\n-            }\n-            catch (Exception e) {\n-              log.error(e, \"Exception while running task[%s]\", task);\n-              status = TaskStatus.failure(task.getId());\n-            }\n-            catch (Throwable t) {\n-              log.error(t, \"Uncaught Throwable while running task[%s]\", task);\n-              throw Throwables.propagate(t);\n-            }\n-\n-            try {\n-              final File taskDir = toolbox.getTaskDir();\n-\n-              if (taskDir.exists()) {\n-                log.info(\"Removing task directory: %s\", taskDir);\n-                FileUtils.deleteDirectory(taskDir);\n-              }\n-            }\n-            catch (Exception e) {\n-              log.error(e, \"Failed to delete task directory: %s\", task.getId());\n-            }\n-\n-            try {\n-              callback.notify(status.withDuration(System.currentTimeMillis() - startTime));\n-            } catch(Exception e) {\n-              log.error(e, \"Uncaught Exception during callback for task[%s]\", task);\n-              throw Throwables.propagate(e);\n-            }\n-          }\n-        }\n-    );\n+    exec.submit(new LocalTaskRunnerRunnable(task, toolbox, callback));\n   }\n \n   @Override\n   public Collection<TaskRunnerWorkItem> getRunningTasks()\n   {\n-    return Lists.newArrayList();\n+    return runningItems;\n   }\n \n   @Override\n   public Collection<TaskRunnerWorkItem> getPendingTasks()\n   {\n+    if (exec instanceof ThreadPoolExecutor) {\n+      ThreadPoolExecutor tpe = (ThreadPoolExecutor) exec;\n+\n+      return Lists.newArrayList(\n+          FunctionalIterable.create(tpe.getQueue())\n+                            .keep(\n+                                new Function<Runnable, TaskRunnerWorkItem>()\n+                                {\n+                                  @Override\n+                                  public TaskRunnerWorkItem apply(Runnable input)\n+                                  {\n+                                    if (input instanceof LocalTaskRunnerRunnable) {\n+                                      return ((LocalTaskRunnerRunnable) input).getTaskRunnerWorkItem();\n+                                    }\n+                                    return null;\n+                                  }\n+                                }\n+                            )\n+      );\n+    }\n+\n     return Lists.newArrayList();\n   }\n \n@@ -131,4 +120,76 @@ public void run()\n   {\n     return Lists.newArrayList();\n   }\n+\n+  private static class LocalTaskRunnerRunnable implements Runnable\n+  {\n+    private final Task task;\n+    private final TaskToolbox toolbox;\n+    private final TaskCallback callback;\n+\n+    private final DateTime createdTime;\n+\n+    public LocalTaskRunnerRunnable(Task task, TaskToolbox toolbox, TaskCallback callback)\n+    {\n+      this.task = task;\n+      this.toolbox = toolbox;\n+      this.callback = callback;\n+\n+      this.createdTime = new DateTime();\n+    }\n+\n+    @Override\n+    public void run()\n+    {\n+      final long startTime = System.currentTimeMillis();\n+\n+      TaskStatus status;\n+\n+      try {\n+        log.info(\"Running task: %s\", task.getId());\n+        status = task.run(toolbox);\n+      }\n+      catch (InterruptedException e) {\n+        log.error(e, \"Interrupted while running task[%s]\", task);\n+        throw Throwables.propagate(e);\n+      }\n+      catch (Exception e) {\n+        log.error(e, \"Exception while running task[%s]\", task);\n+        status = TaskStatus.failure(task.getId());\n+      }\n+      catch (Throwable t) {\n+        log.error(t, \"Uncaught Throwable while running task[%s]\", task);\n+        throw Throwables.propagate(t);\n+      }\n+\n+      try {\n+        final File taskDir = toolbox.getTaskDir();\n+\n+        if (taskDir.exists()) {\n+          log.info(\"Removing task directory: %s\", taskDir);\n+          FileUtils.deleteDirectory(taskDir);\n+        }\n+      }\n+      catch (Exception e) {\n+        log.error(e, \"Failed to delete task directory: %s\", task.getId());\n+      }\n+\n+      try {\n+        callback.notify(status.withDuration(System.currentTimeMillis() - startTime));\n+      } catch(Exception e) {\n+        log.error(e, \"Uncaught Exception during callback for task[%s]\", task);\n+        throw Throwables.propagate(e);\n+      }\n+    }\n+\n+    public TaskRunnerWorkItem getTaskRunnerWorkItem()\n+    {\n+      return new TaskRunnerWorkItem(\n+          task,\n+          callback,\n+          null,\n+          createdTime\n+      );\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/merger/src/main/java/com/metamx/druid/merger/coordinator/LocalTaskRunner.java",
                "sha": "b8a40cdb0b4b8285e835a23d1477dd83485988e5",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/pom.xml",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 0,
                "filename": "pom.xml",
                "patch": "@@ -102,6 +102,11 @@\n                 <artifactId>commons-logging</artifactId>\n                 <version>1.1.1</version>\n             </dependency>\n+            <dependency>\n+                <groupId>commons-lang</groupId>\n+                <artifactId>commons-lang</artifactId>\n+                <version>2.6</version>\n+            </dependency>\n             <dependency>\n                 <groupId>com.ning</groupId>\n                 <artifactId>compress-lzf</artifactId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/pom.xml",
                "sha": "1f66f1e4c391b44e30f527c3235112e18e9307b0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/realtime/pom.xml",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/realtime/pom.xml?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 37,
                "filename": "realtime/pom.xml",
                "patch": "@@ -182,41 +182,4 @@\n         </dependency>\n \n     </dependencies>\n-\n-    <build>\n-        <plugins>\n-            <plugin>\n-                <groupId>org.scala-tools</groupId>\n-                <artifactId>maven-scala-plugin</artifactId>\n-                <configuration>\n-                    <args>\n-                        <arg>-unchecked</arg>\n-                        <arg>-deprecation</arg>\n-                    </args>\n-                </configuration>\n-                <executions>\n-                    <execution>\n-                        <id>compile</id>\n-                        <goals>\n-                            <goal>compile</goal>\n-                        </goals>\n-                        <phase>compile</phase>\n-                    </execution>\n-                    <execution>\n-                        <id>test-compile</id>\n-                        <goals>\n-                            <goal>testCompile</goal>\n-                        </goals>\n-                        <phase>test-compile</phase>\n-                    </execution>\n-                    <execution>\n-                        <goals>\n-                            <goal>compile</goal>\n-                        </goals>\n-                        <phase>process-resources</phase>\n-                    </execution>\n-                </executions>\n-            </plugin>\n-        </plugins>\n-    </build>\n </project>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/realtime/pom.xml",
                "sha": "4d2b084505989fa296d224252e54bb28d0b21da9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/a933438e4e266eb1aebf308ccc860188060d8dd6/server/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/pom.xml?ref=a933438e4e266eb1aebf308ccc860188060d8dd6",
                "deletions": 2,
                "filename": "server/pom.xml",
                "patch": "@@ -63,12 +63,10 @@\n         <dependency>\n             <groupId>commons-cli</groupId>\n             <artifactId>commons-cli</artifactId>\n-            <version>1.2</version>\n         </dependency>\n         <dependency>\n             <groupId>commons-lang</groupId>\n             <artifactId>commons-lang</artifactId>\n-            <version>2.6</version>\n         </dependency>\n         <dependency>\n             <groupId>commons-io</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/a933438e4e266eb1aebf308ccc860188060d8dd6/server/pom.xml",
                "sha": "3fdc65458b00473cd6dfefc7e15a6bebee103f75",
                "status": "modified"
            }
        ],
        "message": "1) Fix bugs with VersionConverterTask\n2) Fix bugs with NPEs on indexing",
        "parent": "https://github.com/apache/incubator-druid/commit/39449e6a3e95ed27231c4463291ea481ffdc4765",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexAdapterTest.java",
            "IndexIOTest.java",
            "QueryableIndexIndexableAdapterTest.java"
        ]
    },
    "incubator-druid_afbcb9c": {
        "bug_id": "incubator-druid_afbcb9c",
        "commit": "https://github.com/apache/incubator-druid/commit/afbcb9c07f21c12f241f1dc6575589ef14cda836",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/docs/content/configuration/index.md",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/configuration/index.md?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 2,
                "filename": "docs/content/configuration/index.md",
                "patch": "@@ -1254,8 +1254,8 @@ These Historical configurations can be defined in the `historical/runtime.proper\n |`druid.segmentCache.dropSegmentDelayMillis`|How long a process delays before completely dropping segment.|30000 (30 seconds)|\n |`druid.segmentCache.infoDir`|Historical processes keep track of the segments they are serving so that when the process is restarted they can reload the same segments without waiting for the Coordinator to reassign. This path defines where this metadata is kept. Directory will be created if needed.|${first_location}/info_dir|\n |`druid.segmentCache.announceIntervalMillis`|How frequently to announce segments while segments are loading from cache. Set this value to zero to wait for all segments to be loaded before announcing.|5000 (5 seconds)|\n-|`druid.segmentCache.numLoadingThreads`|How many segments to drop or load concurrently from from deep storage.|10|\n-|`druid.segmentCache.numBootstrapThreads`|How many segments to load concurrently from local storage at startup.|Same as numLoadingThreads|\n+|`druid.segmentCache.numLoadingThreads`|How many segments to drop or load concurrently from deep storage. Note that the work of loading segments involves downloading segments from deep storage, decompressing them and loading them to a memory mapped location. So the work is not all I/O Bound. Depending on CPU and network load, one could possibly increase this config to a higher value.|Number of cores|\n+|`druid.coordinator.loadqueuepeon.curator.numCallbackThreads`|Number of threads for executing callback actions associated with loading or dropping of segments. One might want to increase this number when noticing clusters are lagging behind w.r.t. balancing segments across historical nodes.|2|\n \n In `druid.segmentCache.locations`, *freeSpacePercent* was added because *maxSize* setting is only a theoretical limit and assumes that much space will always be available for storing segments. In case of any druid bug leading to unaccounted segment files left alone on disk or some other process writing stuff to disk, This check can start failing segment loading early before filling up the disk completely and leaving the host usable otherwise.\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/docs/content/configuration/index.md",
                "sha": "29c11fa05c56bec558b09d201ade5fc0bf7d49cb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.collect.Lists;\n import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.utils.JvmUtils;\n import org.hibernate.validator.constraints.NotEmpty;\n \n import java.io.File;\n@@ -46,7 +47,7 @@\n   private int announceIntervalMillis = 0; // do not background announce\n \n   @JsonProperty(\"numLoadingThreads\")\n-  private int numLoadingThreads = 10;\n+  private int numLoadingThreads = JvmUtils.getRuntimeInfo().getAvailableProcessors();\n \n   @JsonProperty(\"numBootstrapThreads\")\n   private Integer numBootstrapThreads = null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java",
                "sha": "80f0fbc9fdf44e9b78c3fd9e43f0293503ea2ac3",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordination/ZkCoordinator.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordination/ZkCoordinator.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 54,
                "filename": "server/src/main/java/org/apache/druid/server/coordination/ZkCoordinator.java",
                "patch": "@@ -32,9 +32,11 @@\n import org.apache.druid.java.util.common.lifecycle.LifecycleStart;\n import org.apache.druid.java.util.common.lifecycle.LifecycleStop;\n import org.apache.druid.java.util.emitter.EmittingLogger;\n+import org.apache.druid.segment.loading.SegmentLoaderConfig;\n import org.apache.druid.server.initialization.ZkPathsConfig;\n \n import java.io.IOException;\n+import java.util.concurrent.ExecutorService;\n \n /**\n  * Use {@link org.apache.druid.server.coordinator.HttpLoadQueuePeon} for segment load/drops.\n@@ -54,21 +56,27 @@\n \n   private volatile PathChildrenCache loadQueueCache;\n   private volatile boolean started = false;\n+  private final ExecutorService segmentLoadUnloadService;\n \n   @Inject\n   public ZkCoordinator(\n       SegmentLoadDropHandler loadDropHandler,\n       ObjectMapper jsonMapper,\n       ZkPathsConfig zkPaths,\n       DruidServerMetadata me,\n-      CuratorFramework curator\n+      CuratorFramework curator,\n+      SegmentLoaderConfig config\n   )\n   {\n     this.dataSegmentChangeHandler = loadDropHandler;\n     this.jsonMapper = jsonMapper;\n     this.zkPaths = zkPaths;\n     this.me = me;\n     this.curator = curator;\n+    this.segmentLoadUnloadService = Execs.multiThreaded(\n+        config.getNumLoadingThreads(),\n+        \"ZKCoordinator--%d\"\n+    );\n   }\n \n   @LifecycleStart\n@@ -102,63 +110,12 @@ public void start() throws IOException\n             new PathChildrenCacheListener()\n             {\n               @Override\n-              public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception\n+              public void childEvent(CuratorFramework client, PathChildrenCacheEvent event)\n               {\n                 final ChildData child = event.getData();\n                 switch (event.getType()) {\n                   case CHILD_ADDED:\n-                    final String path = child.getPath();\n-                    final DataSegmentChangeRequest request = jsonMapper.readValue(\n-                        child.getData(), DataSegmentChangeRequest.class\n-                    );\n-\n-                    log.info(\"New request[%s] with zNode[%s].\", request.asString(), path);\n-\n-                    try {\n-                      request.go(\n-                          dataSegmentChangeHandler,\n-                          new DataSegmentChangeCallback()\n-                          {\n-                            boolean hasRun = false;\n-\n-                            @Override\n-                            public void execute()\n-                            {\n-                              try {\n-                                if (!hasRun) {\n-                                  curator.delete().guaranteed().forPath(path);\n-                                  log.info(\"Completed request [%s]\", request.asString());\n-                                  hasRun = true;\n-                                }\n-                              }\n-                              catch (Exception e) {\n-                                try {\n-                                  curator.delete().guaranteed().forPath(path);\n-                                }\n-                                catch (Exception e1) {\n-                                  log.error(e1, \"Failed to delete zNode[%s], but ignoring exception.\", path);\n-                                }\n-                                log.error(e, \"Exception while removing zNode[%s]\", path);\n-                                throw new RuntimeException(e);\n-                              }\n-                            }\n-                          }\n-                      );\n-                    }\n-                    catch (Exception e) {\n-                      try {\n-                        curator.delete().guaranteed().forPath(path);\n-                      }\n-                      catch (Exception e1) {\n-                        log.error(e1, \"Failed to delete zNode[%s], but ignoring exception.\", path);\n-                      }\n-\n-                      log.makeAlert(e, \"Segment load/unload: uncaught exception.\")\n-                         .addData(\"node\", path)\n-                         .addData(\"nodeProperties\", request)\n-                         .emit();\n-                    }\n-\n+                    childAdded(child);\n                     break;\n                   case CHILD_REMOVED:\n                     log.info(\"zNode[%s] was removed\", event.getData().getPath());\n@@ -168,6 +125,7 @@ public void execute()\n                 }\n               }\n             }\n+\n         );\n         loadQueueCache.start();\n       }\n@@ -180,6 +138,59 @@ public void execute()\n     }\n   }\n \n+  private void childAdded(ChildData child)\n+  {\n+    segmentLoadUnloadService.submit(() -> {\n+      final String path = child.getPath();\n+      DataSegmentChangeRequest request = new SegmentChangeRequestNoop();\n+      try {\n+        final DataSegmentChangeRequest finalRequest = jsonMapper.readValue(\n+            child.getData(),\n+            DataSegmentChangeRequest.class\n+        );\n+\n+        finalRequest.go(\n+            dataSegmentChangeHandler,\n+            new DataSegmentChangeCallback()\n+            {\n+              @Override\n+              public void execute()\n+              {\n+                try {\n+                  curator.delete().guaranteed().forPath(path);\n+                  log.info(\"Completed request [%s]\", finalRequest.asString());\n+                }\n+                catch (Exception e) {\n+                  try {\n+                    curator.delete().guaranteed().forPath(path);\n+                  }\n+                  catch (Exception e1) {\n+                    log.error(e1, \"Failed to delete zNode[%s], but ignoring exception.\", path);\n+                  }\n+                  log.error(e, \"Exception while removing zNode[%s]\", path);\n+                  throw new RuntimeException(e);\n+                }\n+              }\n+            }\n+        );\n+      }\n+      catch (Exception e) {\n+        // Something went wrong in either deserializing the request using jsonMapper or when invoking it\n+        try {\n+          curator.delete().guaranteed().forPath(path);\n+        }\n+        catch (Exception e1) {\n+          log.error(e1, \"Failed to delete zNode[%s], but ignoring exception.\", path);\n+        }\n+\n+        log.makeAlert(e, \"Segment load/unload: uncaught exception.\")\n+           .addData(\"node\", path)\n+           .addData(\"nodeProperties\", request)\n+           .emit();\n+      }\n+    });\n+  }\n+\n   @LifecycleStop\n   public void stop()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordination/ZkCoordinator.java",
                "sha": "ca56b10024e713afe0f9efed0817c9728aec3cec",
                "status": "modified"
            },
            {
                "additions": 201,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java",
                "changes": 420,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 219,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java",
                "patch": "@@ -21,36 +21,46 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n import org.apache.curator.framework.CuratorFramework;\n import org.apache.curator.framework.api.CuratorWatcher;\n import org.apache.curator.utils.ZKPaths;\n import org.apache.druid.java.util.common.ISE;\n-import org.apache.druid.java.util.common.concurrent.ScheduledExecutors;\n import org.apache.druid.java.util.emitter.EmittingLogger;\n import org.apache.druid.server.coordination.DataSegmentChangeRequest;\n import org.apache.druid.server.coordination.SegmentChangeRequestDrop;\n import org.apache.druid.server.coordination.SegmentChangeRequestLoad;\n import org.apache.druid.server.coordination.SegmentChangeRequestNoop;\n import org.apache.druid.timeline.DataSegment;\n-import org.apache.druid.timeline.SegmentId;\n import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.data.Stat;\n \n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Set;\n import java.util.concurrent.ConcurrentSkipListMap;\n import java.util.concurrent.ConcurrentSkipListSet;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicLong;\n \n /**\n  * Use {@link HttpLoadQueuePeon} instead.\n+ * <p>\n+ * Objects of this class can be accessed by multiple threads. State wise, this class\n+ * is thread safe and callers of the public methods can expect thread safe behavior.\n+ * Though, like a typical object being accessed by multiple threads,\n+ * callers shouldn't expect strict consistency in results between two calls\n+ * of the same or different methods.\n  */\n @Deprecated\n public class CuratorLoadQueuePeon extends LoadQueuePeon\n@@ -59,40 +69,48 @@\n   private static final int DROP = 0;\n   private static final int LOAD = 1;\n \n-  private static void executeCallbacks(List<LoadPeonCallback> callbacks)\n-  {\n-    for (LoadPeonCallback callback : callbacks) {\n-      if (callback != null) {\n-        callback.execute();\n-      }\n-    }\n-  }\n-\n   private final CuratorFramework curator;\n   private final String basePath;\n   private final ObjectMapper jsonMapper;\n   private final ScheduledExecutorService processingExecutor;\n+\n+  /**\n+   * Threadpool with daemon threads that execute callback actions associated\n+   * with loading or dropping segments.\n+   */\n   private final ExecutorService callBackExecutor;\n   private final DruidCoordinatorConfig config;\n \n   private final AtomicLong queuedSize = new AtomicLong(0);\n   private final AtomicInteger failedAssignCount = new AtomicInteger(0);\n \n+  /**\n+   * Needs to be thread safe since it can be concurrently accessed via\n+   * {@link #loadSegment(DataSegment, LoadPeonCallback)}, {@link #actionCompleted(SegmentHolder)},\n+   * {@link #getSegmentsToLoad()} and {@link #stop()}\n+   */\n   private final ConcurrentSkipListMap<DataSegment, SegmentHolder> segmentsToLoad = new ConcurrentSkipListMap<>(\n       DruidCoordinator.SEGMENT_COMPARATOR_RECENT_FIRST\n   );\n+\n+  /**\n+   * Needs to be thread safe since it can be concurrently accessed via\n+   * {@link #dropSegment(DataSegment, LoadPeonCallback)}, {@link #actionCompleted(SegmentHolder)},\n+   * {@link #getSegmentsToDrop()} and {@link #stop()}\n+   */\n   private final ConcurrentSkipListMap<DataSegment, SegmentHolder> segmentsToDrop = new ConcurrentSkipListMap<>(\n       DruidCoordinator.SEGMENT_COMPARATOR_RECENT_FIRST\n   );\n+\n+  /**\n+   * Needs to be thread safe since it can be concurrently accessed via\n+   * {@link #markSegmentToDrop(DataSegment)}}, {@link #unmarkSegmentToDrop(DataSegment)}}\n+   * and {@link #getSegmentsToDrop()}\n+   */\n   private final ConcurrentSkipListSet<DataSegment> segmentsMarkedToDrop = new ConcurrentSkipListSet<>(\n       DruidCoordinator.SEGMENT_COMPARATOR_RECENT_FIRST\n   );\n \n-  private final Object lock = new Object();\n-\n-  private volatile SegmentHolder currentlyProcessing = null;\n-  private boolean stopped = false;\n-\n   CuratorLoadQueuePeon(\n       CuratorFramework curator,\n       String basePath,\n@@ -150,61 +168,30 @@ public int getNumberOfSegmentsInQueue()\n   }\n \n   @Override\n-  public void loadSegment(final DataSegment segment, final LoadPeonCallback callback)\n+  public void loadSegment(final DataSegment segment, @Nullable final LoadPeonCallback callback)\n   {\n-    synchronized (lock) {\n-      if ((currentlyProcessing != null) &&\n-          currentlyProcessing.getSegmentId().equals(segment.getId())) {\n-        if (callback != null) {\n-          currentlyProcessing.addCallback(callback);\n-        }\n-        return;\n-      }\n-    }\n-\n-    synchronized (lock) {\n-      final SegmentHolder existingHolder = segmentsToLoad.get(segment);\n-      if (existingHolder != null) {\n-        if ((callback != null)) {\n-          existingHolder.addCallback(callback);\n-        }\n-        return;\n-      }\n+    SegmentHolder segmentHolder = new SegmentHolder(segment, LOAD, Collections.singletonList(callback));\n+    final SegmentHolder existingHolder = segmentsToLoad.putIfAbsent(segment, segmentHolder);\n+    if (existingHolder != null) {\n+      existingHolder.addCallback(callback);\n+      return;\n     }\n-\n     log.debug(\"Asking server peon[%s] to load segment[%s]\", basePath, segment.getId());\n     queuedSize.addAndGet(segment.getSize());\n-    segmentsToLoad.put(segment, new SegmentHolder(segment, LOAD, Collections.singletonList(callback)));\n+    processingExecutor.submit(new SegmentChangeProcessor(segmentHolder));\n   }\n \n   @Override\n-  public void dropSegment(\n-      final DataSegment segment,\n-      final LoadPeonCallback callback\n-  )\n+  public void dropSegment(final DataSegment segment, @Nullable final LoadPeonCallback callback)\n   {\n-    synchronized (lock) {\n-      if ((currentlyProcessing != null) &&\n-          currentlyProcessing.getSegmentId().equals(segment.getId())) {\n-        if (callback != null) {\n-          currentlyProcessing.addCallback(callback);\n-        }\n-        return;\n-      }\n-    }\n-\n-    synchronized (lock) {\n-      final SegmentHolder existingHolder = segmentsToDrop.get(segment);\n-      if (existingHolder != null) {\n-        if (callback != null) {\n-          existingHolder.addCallback(callback);\n-        }\n-        return;\n-      }\n+    SegmentHolder segmentHolder = new SegmentHolder(segment, DROP, Collections.singletonList(callback));\n+    final SegmentHolder existingHolder = segmentsToDrop.putIfAbsent(segment, segmentHolder);\n+    if (existingHolder != null) {\n+      existingHolder.addCallback(callback);\n+      return;\n     }\n-\n     log.debug(\"Asking server peon[%s] to drop segment[%s]\", basePath, segment.getId());\n-    segmentsToDrop.put(segment, new SegmentHolder(segment, DROP, Collections.singletonList(callback)));\n+    processingExecutor.submit(new SegmentChangeProcessor(segmentHolder));\n   }\n \n   @Override\n@@ -219,206 +206,198 @@ public void unmarkSegmentToDrop(DataSegment dataSegment)\n     segmentsMarkedToDrop.remove(dataSegment);\n   }\n \n-  private void processSegmentChangeRequest()\n+  private class SegmentChangeProcessor implements Runnable\n   {\n-    if (currentlyProcessing != null) {\n-      log.debug(\n-          \"Server[%s] skipping processSegmentChangeRequest because something is currently loading[%s].\",\n-          basePath,\n-          currentlyProcessing.getSegmentId()\n-      );\n+    private final SegmentHolder segmentHolder;\n \n-      return;\n+    private SegmentChangeProcessor(SegmentHolder segmentHolder)\n+    {\n+      this.segmentHolder = segmentHolder;\n     }\n \n-    if (!segmentsToDrop.isEmpty()) {\n-      currentlyProcessing = segmentsToDrop.firstEntry().getValue();\n-      log.debug(\"Server[%s] dropping [%s]\", basePath, currentlyProcessing.getSegmentId());\n-    } else if (!segmentsToLoad.isEmpty()) {\n-      currentlyProcessing = segmentsToLoad.firstEntry().getValue();\n-      log.debug(\"Server[%s] loading [%s]\", basePath, currentlyProcessing.getSegmentId());\n-    } else {\n-      return;\n+    @Override\n+    public void run()\n+    {\n+      try {\n+        final String path = ZKPaths.makePath(basePath, segmentHolder.getSegmentIdentifier());\n+        final byte[] payload = jsonMapper.writeValueAsBytes(segmentHolder.getChangeRequest());\n+        curator.create().withMode(CreateMode.EPHEMERAL).forPath(path, payload);\n+        log.debug(\n+            \"ZKNode created for server to [%s] %s [%s]\",\n+            basePath,\n+            segmentHolder.getType() == LOAD ? \"load\" : \"drop\",\n+            segmentHolder.getSegmentIdentifier()\n+        );\n+        final ScheduledFuture<?> nodeDeletedCheck = scheduleNodeDeletedCheck(path);\n+        final Stat stat = curator.checkExists().usingWatcher(\n+            (CuratorWatcher) watchedEvent -> {\n+              switch (watchedEvent.getType()) {\n+                case NodeDeleted:\n+                  // Cancel the check node deleted task since we have already\n+                  // been notified by the zk watcher\n+                  nodeDeletedCheck.cancel(true);\n+                  entryRemoved(segmentHolder, watchedEvent.getPath());\n+                  break;\n+                default:\n+                  // do nothing\n+              }\n+            }\n+        ).forPath(path);\n+\n+        if (stat == null) {\n+          final byte[] noopPayload = jsonMapper.writeValueAsBytes(new SegmentChangeRequestNoop());\n+\n+          // Create a node and then delete it to remove the registered watcher.  This is a work-around for\n+          // a zookeeper race condition.  Specifically, when you set a watcher, it fires on the next event\n+          // that happens for that node.  If no events happen, the watcher stays registered foreverz.\n+          // Couple that with the fact that you cannot set a watcher when you create a node, but what we\n+          // want is to create a node and then watch for it to get deleted.  The solution is that you *can*\n+          // set a watcher when you check to see if it exists so, we first create the node and then set a\n+          // watcher on its existence.  However, if already does not exist by the time the existence check\n+          // returns, then the watcher that was set will never fire (nobody will ever create the node\n+          // again) and thus lead to a slow, but real, memory leak.  So, we create another node to cause\n+          // that watcher to fire and delete it right away.\n+          //\n+          // We do not create the existence watcher first, because then it will fire when we create the\n+          // node and we'll have the same race when trying to refresh that watcher.\n+          curator.create().withMode(CreateMode.EPHEMERAL).forPath(path, noopPayload);\n+          entryRemoved(segmentHolder, path);\n+        }\n+      }\n+      catch (KeeperException.NodeExistsException ne) {\n+        // This is expected when historicals haven't yet picked up processing this segment and coordinator\n+        // tries reassigning it to the same node.\n+        log.warn(ne, \"ZK node already exists because segment change request hasn't yet been processed\");\n+        failAssign(segmentHolder);\n+      }\n+      catch (Exception e) {\n+        failAssign(segmentHolder, e);\n+      }\n     }\n \n-    try {\n-      final String path = ZKPaths.makePath(basePath, currentlyProcessing.getSegmentId().toString());\n-      final byte[] payload = jsonMapper.writeValueAsBytes(currentlyProcessing.getChangeRequest());\n-      curator.create().withMode(CreateMode.EPHEMERAL).forPath(path, payload);\n-\n-      processingExecutor.schedule(\n+    @Nonnull\n+    private ScheduledFuture<?> scheduleNodeDeletedCheck(String path)\n+    {\n+      return processingExecutor.schedule(\n           () -> {\n             try {\n               if (curator.checkExists().forPath(path) != null) {\n-                failAssign(new ISE(\"%s was never removed! Failing this operation!\", path));\n+                failAssign(segmentHolder, new ISE(\"%s was never removed! Failing this operation!\", path));\n+              } else {\n+                log.debug(\"%s detected to be removed. \", path);\n               }\n             }\n             catch (Exception e) {\n-              failAssign(e);\n+              log.error(e, \"Exception caught and ignored when checking whether zk node was deleted\");\n+              failAssign(segmentHolder, e);\n             }\n           },\n           config.getLoadTimeoutDelay().getMillis(),\n           TimeUnit.MILLISECONDS\n       );\n-\n-      final Stat stat = curator.checkExists().usingWatcher(\n-          (CuratorWatcher) watchedEvent -> {\n-            switch (watchedEvent.getType()) {\n-              case NodeDeleted:\n-                entryRemoved(watchedEvent.getPath());\n-                break;\n-              default:\n-                // do nothing\n-            }\n-          }\n-      ).forPath(path);\n-\n-      if (stat == null) {\n-        final byte[] noopPayload = jsonMapper.writeValueAsBytes(new SegmentChangeRequestNoop());\n-\n-        // Create a node and then delete it to remove the registered watcher.  This is a work-around for\n-        // a zookeeper race condition.  Specifically, when you set a watcher, it fires on the next event\n-        // that happens for that node.  If no events happen, the watcher stays registered foreverz.\n-        // Couple that with the fact that you cannot set a watcher when you create a node, but what we\n-        // want is to create a node and then watch for it to get deleted.  The solution is that you *can*\n-        // set a watcher when you check to see if it exists so, we first create the node and then set a\n-        // watcher on its existence.  However, if already does not exist by the time the existence check\n-        // returns, then the watcher that was set will never fire (nobody will ever create the node\n-        // again) and thus lead to a slow, but real, memory leak.  So, we create another node to cause\n-        // that watcher to fire and delete it right away.\n-        //\n-        // We do not create the existence watcher first, because then it will fire when we create the\n-        // node and we'll have the same race when trying to refresh that watcher.\n-        curator.create().withMode(CreateMode.EPHEMERAL).forPath(path, noopPayload);\n-\n-        entryRemoved(path);\n-      }\n-    }\n-    catch (Exception e) {\n-      failAssign(e);\n     }\n   }\n \n-  private void actionCompleted()\n+  private void actionCompleted(SegmentHolder segmentHolder)\n   {\n-    if (currentlyProcessing != null) {\n-      switch (currentlyProcessing.getType()) {\n-        case LOAD:\n-          segmentsToLoad.remove(currentlyProcessing.getSegment());\n-          queuedSize.addAndGet(-currentlyProcessing.getSegmentSize());\n-          break;\n-        case DROP:\n-          segmentsToDrop.remove(currentlyProcessing.getSegment());\n-          break;\n-        default:\n-          throw new UnsupportedOperationException();\n-      }\n-\n-      final List<LoadPeonCallback> callbacks = currentlyProcessing.getCallbacks();\n-      currentlyProcessing = null;\n-      callBackExecutor.execute(\n-          () -> executeCallbacks(callbacks)\n-      );\n+    switch (segmentHolder.getType()) {\n+      case LOAD:\n+        segmentsToLoad.remove(segmentHolder.getSegment());\n+        queuedSize.addAndGet(-segmentHolder.getSegmentSize());\n+        break;\n+      case DROP:\n+        segmentsToDrop.remove(segmentHolder.getSegment());\n+        break;\n+      default:\n+        throw new UnsupportedOperationException();\n     }\n+    executeCallbacks(segmentHolder);\n   }\n \n+\n   @Override\n   public void start()\n-  {\n-    ScheduledExecutors.scheduleAtFixedRate(\n-        processingExecutor,\n-        config.getLoadQueuePeonRepeatDelay(),\n-        config.getLoadQueuePeonRepeatDelay(),\n-        () -> {\n-          processSegmentChangeRequest();\n-\n-          if (stopped) {\n-            return ScheduledExecutors.Signal.STOP;\n-          } else {\n-            return ScheduledExecutors.Signal.REPEAT;\n-          }\n-        }\n-    );\n-  }\n+  { }\n \n   @Override\n   public void stop()\n   {\n-    synchronized (lock) {\n-      if (currentlyProcessing != null) {\n-        executeCallbacks(currentlyProcessing.getCallbacks());\n-        currentlyProcessing = null;\n-      }\n-\n-      if (!segmentsToDrop.isEmpty()) {\n-        for (SegmentHolder holder : segmentsToDrop.values()) {\n-          executeCallbacks(holder.getCallbacks());\n-        }\n-      }\n-      segmentsToDrop.clear();\n-\n-      if (!segmentsToLoad.isEmpty()) {\n-        for (SegmentHolder holder : segmentsToLoad.values()) {\n-          executeCallbacks(holder.getCallbacks());\n-        }\n-      }\n-      segmentsToLoad.clear();\n+    for (SegmentHolder holder : segmentsToDrop.values()) {\n+      executeCallbacks(holder);\n+    }\n+    segmentsToDrop.clear();\n \n-      queuedSize.set(0L);\n-      failedAssignCount.set(0);\n-      stopped = true;\n+    for (SegmentHolder holder : segmentsToLoad.values()) {\n+      executeCallbacks(holder);\n     }\n+    segmentsToLoad.clear();\n+\n+    queuedSize.set(0L);\n+    failedAssignCount.set(0);\n+    processingExecutor.shutdown();\n+    callBackExecutor.shutdown();\n   }\n \n-  private void entryRemoved(String path)\n+  private void entryRemoved(SegmentHolder segmentHolder, String path)\n   {\n-    synchronized (lock) {\n-      if (currentlyProcessing == null) {\n-        log.warn(\"Server[%s] an entry[%s] was removed even though it wasn't loading!?\", basePath, path);\n-        return;\n-      }\n-      if (!ZKPaths.getNodeFromPath(path).equals(currentlyProcessing.getSegmentId().toString())) {\n-        log.warn(\n-            \"Server[%s] entry [%s] was removed even though it's not what is currently loading[%s]\",\n-            basePath, path, currentlyProcessing\n-        );\n-        return;\n-      }\n-      log.debug(\n-          \"Server[%s] done processing %s of segment [%s]\",\n-          basePath,\n-          currentlyProcessing.getType() == LOAD ? \"load\" : \"drop\",\n-          path\n+    if (!ZKPaths.getNodeFromPath(path).equals(segmentHolder.getSegmentIdentifier())) {\n+      log.warn(\n+          \"Server[%s] entry [%s] was removed even though it's not what is currently loading[%s]\",\n+          basePath, path, segmentHolder\n       );\n-      actionCompleted();\n+      return;\n     }\n+    actionCompleted(segmentHolder);\n+    log.debug(\n+        \"Server[%s] done processing %s of segment [%s]\",\n+        basePath,\n+        segmentHolder.getType() == LOAD ? \"load\" : \"drop\",\n+        path\n+    );\n   }\n \n-  private void failAssign(Exception e)\n+  private void failAssign(SegmentHolder segmentHolder)\n   {\n-    synchronized (lock) {\n-      log.error(e, \"Server[%s], throwable caught when submitting [%s].\", basePath, currentlyProcessing);\n-      failedAssignCount.getAndIncrement();\n-      // Act like it was completed so that the coordinator gives it to someone else\n-      actionCompleted();\n+    failAssign(segmentHolder, null);\n+  }\n+\n+  private void failAssign(SegmentHolder segmentHolder, Exception e)\n+  {\n+    if (e != null) {\n+      log.error(e, \"Server[%s], throwable caught when submitting [%s].\", basePath, segmentHolder);\n     }\n+    failedAssignCount.getAndIncrement();\n+    // Act like it was completed so that the coordinator gives it to someone else\n+    actionCompleted(segmentHolder);\n   }\n \n+\n   private static class SegmentHolder\n   {\n     private final DataSegment segment;\n     private final DataSegmentChangeRequest changeRequest;\n     private final int type;\n+    // Guaranteed to store only non-null elements\n     private final List<LoadPeonCallback> callbacks = new ArrayList<>();\n \n-    private SegmentHolder(DataSegment segment, int type, Collection<LoadPeonCallback> callbacks)\n+    private SegmentHolder(\n+        DataSegment segment,\n+        int type,\n+        Collection<LoadPeonCallback> callbacksParam\n+    )\n     {\n       this.segment = segment;\n       this.type = type;\n       this.changeRequest = (type == LOAD)\n                            ? new SegmentChangeRequestLoad(segment)\n                            : new SegmentChangeRequestDrop(segment);\n-      this.callbacks.addAll(callbacks);\n+      Iterator<LoadPeonCallback> itr = callbacksParam.iterator();\n+      while (itr.hasNext()) {\n+        LoadPeonCallback c = itr.next();\n+        if (c != null) {\n+          callbacks.add(c);\n+        }\n+      }\n     }\n \n     public DataSegment getSegment()\n@@ -431,34 +410,30 @@ public int getType()\n       return type;\n     }\n \n-    public SegmentId getSegmentId()\n+    public String getSegmentIdentifier()\n     {\n-      return segment.getId();\n+      return segment.getId().toString();\n     }\n \n     public long getSegmentSize()\n     {\n       return segment.getSize();\n     }\n \n-    public void addCallbacks(Collection<LoadPeonCallback> newCallbacks)\n+    public void addCallback(@Nullable LoadPeonCallback newCallback)\n     {\n-      synchronized (callbacks) {\n-        callbacks.addAll(newCallbacks);\n-      }\n-    }\n-\n-    public void addCallback(LoadPeonCallback newCallback)\n-    {\n-      synchronized (callbacks) {\n-        callbacks.add(newCallback);\n+      if (newCallback != null) {\n+        synchronized (callbacks) {\n+          callbacks.add(newCallback);\n+        }\n       }\n     }\n \n-    public List<LoadPeonCallback> getCallbacks()\n+    List<LoadPeonCallback> snapshotCallbacks()\n     {\n       synchronized (callbacks) {\n-        return callbacks;\n+        // Return an immutable copy so that callers don't have to worry about concurrent modification\n+        return ImmutableList.copyOf(callbacks);\n       }\n     }\n \n@@ -473,4 +448,11 @@ public String toString()\n       return changeRequest.toString();\n     }\n   }\n+\n+  private void executeCallbacks(SegmentHolder holder)\n+  {\n+    for (LoadPeonCallback callback : holder.snapshotCallbacks()) {\n+      callBackExecutor.submit(() -> callback.execute());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java",
                "sha": "a4d5d948cf07f24645b2b05bbe42318d0434b219",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorConfig.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorConfig.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorConfig.java",
                "patch": "@@ -75,6 +75,12 @@ public String getLoadQueuePeonType()\n     return \"curator\";\n   }\n \n+  @Config(\"druid.coordinator.curator.loadqueuepeon.numCallbackThreads\")\n+  public int getNumCuratorCallBackThreads()\n+  {\n+    return 2;\n+  }\n+\n   @Config(\"druid.coordinator.loadqueuepeon.http.repeatDelay\")\n   public Duration getHttpLoadQueuePeonRepeatDelay()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorConfig.java",
                "sha": "b4adebe851c96e170c6701382aa6148d2c83cf02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java",
                "patch": "@@ -133,7 +133,8 @@ public void removeSegment(DataSegment s, DataSegmentChangeCallback callback)\n         jsonMapper,\n         zkPaths,\n         me,\n-        curator\n+        curator,\n+        new SegmentLoaderConfig()\n     );\n     zkCoordinator.start();\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java",
                "sha": "9bdd84ed70fb59e756efc4006b1fe6f70ce54a31",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "patch": "@@ -162,7 +162,8 @@ public void setUp() throws Exception\n         null,\n         false,\n         false,\n-        new Duration(\"PT0s\")\n+        new Duration(\"PT0s\"),\n+        Duration.millis(10)\n     );\n     sourceLoadQueueChildrenCache = new PathChildrenCache(\n         curator,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java",
                "sha": "3b2223514f5943aa164f0cccf2e521ca891373b4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java",
                "patch": "@@ -139,7 +139,8 @@ public void setUp() throws Exception\n         null,\n         false,\n         false,\n-        new Duration(\"PT0s\")\n+        new Duration(\"PT0s\"),\n+        Duration.millis(10)\n     );\n     pathChildrenCache = new PathChildrenCache(\n         curator,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java",
                "sha": "793bd287b9501590ee132d13f5fb340026c707ee",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/HttpLoadQueuePeonTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/HttpLoadQueuePeonTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/HttpLoadQueuePeonTest.java",
                "patch": "@@ -84,7 +84,8 @@\n       null,\n       false,\n       false,\n-      Duration.ZERO\n+      Duration.ZERO,\n+      Duration.millis(10)\n   )\n   {\n     @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/HttpLoadQueuePeonTest.java",
                "sha": "894472a2cbccd4751f13fb020194342b408f2daa",
                "status": "modified"
            },
            {
                "additions": 104,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java",
                "changes": 203,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 99,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.druid.server.coordination.SegmentChangeRequestDrop;\n import org.apache.druid.server.coordination.SegmentChangeRequestLoad;\n import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n import org.apache.druid.timeline.partition.NoneShardSpec;\n import org.joda.time.Duration;\n import org.junit.After;\n@@ -47,8 +48,10 @@\n import org.junit.Test;\n \n import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.TimeUnit;\n \n public class LoadQueuePeonTest extends CuratorTestBase\n {\n@@ -79,46 +82,34 @@ public void setUp() throws Exception\n   @Test\n   public void testMultipleLoadDropSegments() throws Exception\n   {\n-    final AtomicInteger requestSignalIdx = new AtomicInteger(0);\n-    final AtomicInteger segmentSignalIdx = new AtomicInteger(0);\n-\n     loadQueuePeon = new CuratorLoadQueuePeon(\n         curator,\n         LOAD_QUEUE_PATH,\n         jsonMapper,\n         Execs.scheduledSingleThreaded(\"test_load_queue_peon_scheduled-%d\"),\n         Execs.singleThreaded(\"test_load_queue_peon-%d\"),\n-        new TestDruidCoordinatorConfig(null, null, null, null, null, null, 10, null, false, false, Duration.ZERO)\n+        new TestDruidCoordinatorConfig(\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            null,\n+            10,\n+            null,\n+            false,\n+            false,\n+            Duration.millis(0),\n+            Duration.millis(10)\n+        )\n     );\n \n     loadQueuePeon.start();\n \n-    final CountDownLatch[] loadRequestSignal = new CountDownLatch[5];\n-    final CountDownLatch[] dropRequestSignal = new CountDownLatch[5];\n-    final CountDownLatch[] segmentLoadedSignal = new CountDownLatch[5];\n-    final CountDownLatch[] segmentDroppedSignal = new CountDownLatch[5];\n-\n-    for (int i = 0; i < 5; ++i) {\n-      loadRequestSignal[i] = new CountDownLatch(1);\n-      dropRequestSignal[i] = new CountDownLatch(1);\n-      segmentLoadedSignal[i] = new CountDownLatch(1);\n-      segmentDroppedSignal[i] = new CountDownLatch(1);\n-    }\n-\n-    final DataSegmentChangeHandler handler = new DataSegmentChangeHandler()\n-    {\n-      @Override\n-      public void addSegment(DataSegment segment, DataSegmentChangeCallback callback)\n-      {\n-        loadRequestSignal[requestSignalIdx.get()].countDown();\n-      }\n-\n-      @Override\n-      public void removeSegment(DataSegment segment, DataSegmentChangeCallback callback)\n-      {\n-        dropRequestSignal[requestSignalIdx.get()].countDown();\n-      }\n-    };\n+    ConcurrentMap<SegmentId, CountDownLatch> loadRequestSignals = new ConcurrentHashMap<>(5);\n+    ConcurrentMap<SegmentId, CountDownLatch> dropRequestSignals = new ConcurrentHashMap<>(5);\n+    ConcurrentMap<SegmentId, CountDownLatch> segmentLoadedSignals = new ConcurrentHashMap<>(5);\n+    ConcurrentMap<SegmentId, CountDownLatch> segmentDroppedSignals = new ConcurrentHashMap<>(5);\n \n     final List<DataSegment> segmentToDrop = Lists.transform(\n         ImmutableList.of(\n@@ -132,11 +123,24 @@ public void removeSegment(DataSegment segment, DataSegmentChangeCallback callbac\n           @Override\n           public DataSegment apply(String intervalStr)\n           {\n-            return dataSegmentWithInterval(intervalStr);\n+            DataSegment dataSegment = dataSegmentWithInterval(intervalStr);\n+            return dataSegment;\n           }\n         }\n     );\n \n+    final CountDownLatch[] dropRequestLatches = new CountDownLatch[5];\n+    final CountDownLatch[] dropSegmentLatches = new CountDownLatch[5];\n+    for (int i = 0; i < 5; i++) {\n+      dropRequestLatches[i] = new CountDownLatch(1);\n+      dropSegmentLatches[i] = new CountDownLatch(1);\n+    }\n+    int i = 0;\n+    for (DataSegment s : segmentToDrop) {\n+      dropRequestSignals.put(s.getId(), dropRequestLatches[i]);\n+      segmentDroppedSignals.put(s.getId(), dropSegmentLatches[i++]);\n+    }\n+\n     final List<DataSegment> segmentToLoad = Lists.transform(\n         ImmutableList.of(\n             \"2014-10-27T00:00:00Z/P1D\",\n@@ -149,11 +153,26 @@ public DataSegment apply(String intervalStr)\n           @Override\n           public DataSegment apply(String intervalStr)\n           {\n-            return dataSegmentWithInterval(intervalStr);\n+            DataSegment dataSegment = dataSegmentWithInterval(intervalStr);\n+            loadRequestSignals.put(dataSegment.getId(), new CountDownLatch(1));\n+            segmentLoadedSignals.put(dataSegment.getId(), new CountDownLatch(1));\n+            return dataSegment;\n           }\n         }\n     );\n \n+    final CountDownLatch[] loadRequestLatches = new CountDownLatch[5];\n+    final CountDownLatch[] segmentLoadedLatches = new CountDownLatch[5];\n+    for (i = 0; i < 5; i++) {\n+      loadRequestLatches[i] = new CountDownLatch(1);\n+      segmentLoadedLatches[i] = new CountDownLatch(1);\n+    }\n+    i = 0;\n+    for (DataSegment s : segmentToDrop) {\n+      loadRequestSignals.put(s.getId(), loadRequestLatches[i]);\n+      segmentLoadedSignals.put(s.getId(), segmentLoadedLatches[i++]);\n+    }\n+\n     // segment with latest interval should be loaded first\n     final List<DataSegment> expectedLoadOrder = Lists.transform(\n         ImmutableList.of(\n@@ -162,59 +181,48 @@ public DataSegment apply(String intervalStr)\n             \"2014-10-30T00:00:00Z/P1D\",\n             \"2014-10-28T00:00:00Z/P1D\",\n             \"2014-10-27T00:00:00Z/P1D\"\n-        ), new Function<String, DataSegment>()\n-        {\n-          @Override\n-          public DataSegment apply(String intervalStr)\n-          {\n-            return dataSegmentWithInterval(intervalStr);\n-          }\n-        }\n+        ), intervalStr -> dataSegmentWithInterval(intervalStr)\n     );\n \n+    final DataSegmentChangeHandler handler = new DataSegmentChangeHandler()\n+    {\n+      @Override\n+      public void addSegment(DataSegment segment, DataSegmentChangeCallback callback)\n+      {\n+        loadRequestSignals.get(segment.getId()).countDown();\n+      }\n+\n+      @Override\n+      public void removeSegment(DataSegment segment, DataSegmentChangeCallback callback)\n+      {\n+        dropRequestSignals.get(segment.getId()).countDown();\n+      }\n+    };\n+\n     loadQueueCache.getListenable().addListener(\n-        new PathChildrenCacheListener()\n-        {\n-          @Override\n-          public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception\n-          {\n-            if (event.getType() == PathChildrenCacheEvent.Type.CHILD_ADDED) {\n-              DataSegmentChangeRequest request = jsonMapper.readValue(\n-                  event.getData().getData(),\n-                  DataSegmentChangeRequest.class\n-              );\n-              request.go(handler, null);\n-            }\n+        (client, event) -> {\n+          if (event.getType() == PathChildrenCacheEvent.Type.CHILD_ADDED) {\n+            DataSegmentChangeRequest request = jsonMapper.readValue(\n+                event.getData().getData(),\n+                DataSegmentChangeRequest.class\n+            );\n+            request.go(handler, null);\n           }\n         }\n     );\n     loadQueueCache.start();\n \n-    for (DataSegment segment : segmentToDrop) {\n+    for (final DataSegment segment : segmentToDrop) {\n       loadQueuePeon.dropSegment(\n           segment,\n-          new LoadPeonCallback()\n-          {\n-            @Override\n-            public void execute()\n-            {\n-              segmentDroppedSignal[segmentSignalIdx.get()].countDown();\n-            }\n-          }\n+          () -> segmentDroppedSignals.get(segment.getId()).countDown()\n       );\n     }\n \n-    for (DataSegment segment : segmentToLoad) {\n+    for (final DataSegment segment : segmentToLoad) {\n       loadQueuePeon.loadSegment(\n           segment,\n-          new LoadPeonCallback()\n-          {\n-            @Override\n-            public void execute()\n-            {\n-              segmentLoadedSignal[segmentSignalIdx.get()].countDown();\n-            }\n-          }\n+          () -> segmentLoadedSignals.get(segment.getId()).countDown()\n       );\n     }\n \n@@ -224,8 +232,14 @@ public void execute()\n \n     for (DataSegment segment : segmentToDrop) {\n       String dropRequestPath = ZKPaths.makePath(LOAD_QUEUE_PATH, segment.getId().toString());\n-      Assert.assertTrue(timing.forWaiting().awaitLatch(dropRequestSignal[requestSignalIdx.get()]));\n-      Assert.assertNotNull(curator.checkExists().forPath(dropRequestPath));\n+      Assert.assertTrue(\n+          \"Latch not counted down for \" + dropRequestSignals.get(segment.getId()),\n+          dropRequestSignals.get(segment.getId()).await(10, TimeUnit.SECONDS)\n+      );\n+      Assert.assertNotNull(\n+          \"Path \" + dropRequestPath + \" doesn't exist\",\n+          curator.checkExists().forPath(dropRequestPath)\n+      );\n       Assert.assertEquals(\n           segment,\n           ((SegmentChangeRequestDrop) jsonMapper.readValue(\n@@ -235,29 +249,14 @@ public void execute()\n           )).getSegment()\n       );\n \n-      if (requestSignalIdx.get() == 4) {\n-        requestSignalIdx.set(0);\n-      } else {\n-        requestSignalIdx.incrementAndGet();\n-      }\n-\n       // simulate completion of drop request by historical\n       curator.delete().guaranteed().forPath(dropRequestPath);\n-      Assert.assertTrue(timing.forWaiting().awaitLatch(segmentDroppedSignal[segmentSignalIdx.get()]));\n-\n-      int expectedNumSegmentToDrop = 5 - segmentSignalIdx.get() - 1;\n-      Assert.assertEquals(expectedNumSegmentToDrop, loadQueuePeon.getSegmentsToDrop().size());\n-\n-      if (segmentSignalIdx.get() == 4) {\n-        segmentSignalIdx.set(0);\n-      } else {\n-        segmentSignalIdx.incrementAndGet();\n-      }\n+      Assert.assertTrue(timing.forWaiting().awaitLatch(segmentDroppedSignals.get(segment.getId())));\n     }\n \n     for (DataSegment segment : expectedLoadOrder) {\n       String loadRequestPath = ZKPaths.makePath(LOAD_QUEUE_PATH, segment.getId().toString());\n-      Assert.assertTrue(timing.forWaiting().awaitLatch(loadRequestSignal[requestSignalIdx.get()]));\n+      Assert.assertTrue(timing.forWaiting().awaitLatch(loadRequestSignals.get(segment.getId())));\n       Assert.assertNotNull(curator.checkExists().forPath(loadRequestPath));\n       Assert.assertEquals(\n           segment,\n@@ -266,16 +265,9 @@ public void execute()\n               .getSegment()\n       );\n \n-      requestSignalIdx.incrementAndGet();\n-\n       // simulate completion of load request by historical\n       curator.delete().guaranteed().forPath(loadRequestPath);\n-      Assert.assertTrue(timing.forWaiting().awaitLatch(segmentLoadedSignal[segmentSignalIdx.get()]));\n-\n-      int expectedNumSegmentToLoad = 5 - segmentSignalIdx.get() - 1;\n-      Assert.assertEquals(1200 * expectedNumSegmentToLoad, loadQueuePeon.getLoadQueueSize());\n-      Assert.assertEquals(expectedNumSegmentToLoad, loadQueuePeon.getSegmentsToLoad().size());\n-      segmentSignalIdx.incrementAndGet();\n+      Assert.assertTrue(timing.forWaiting().awaitLatch(segmentLoadedSignals.get(segment.getId())));\n     }\n   }\n \n@@ -294,7 +286,20 @@ public void testFailAssign() throws Exception\n         Execs.scheduledSingleThreaded(\"test_load_queue_peon_scheduled-%d\"),\n         Execs.singleThreaded(\"test_load_queue_peon-%d\"),\n         // set time-out to 1 ms so that LoadQueuePeon will fail the assignment quickly\n-        new TestDruidCoordinatorConfig(null, null, null, new Duration(1), null, null, 10, null, false, false, new Duration(\"PT1s\"))\n+        new TestDruidCoordinatorConfig(\n+            null,\n+            null,\n+            null,\n+            new Duration(1),\n+            null,\n+            null,\n+            10,\n+            null,\n+            false,\n+            false,\n+            new Duration(\"PT1s\"),\n+            Duration.millis(10)\n+        )\n     );\n \n     loadQueuePeon.start();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java",
                "sha": "8d8271dab1dcf1ff5adf4524263b72d6b5d8f742",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTester.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTester.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTester.java",
                "patch": "@@ -19,7 +19,9 @@\n \n package org.apache.druid.server.coordinator;\n \n+import org.apache.druid.java.util.common.concurrent.Execs;\n import org.apache.druid.timeline.DataSegment;\n+import org.joda.time.Duration;\n \n import java.util.concurrent.ConcurrentSkipListSet;\n \n@@ -29,7 +31,27 @@\n \n   public LoadQueuePeonTester()\n   {\n-    super(null, null, null, null, null, null);\n+    super(\n+        null,\n+        null,\n+        null,\n+        Execs.scheduledSingleThreaded(\"LoadQueuePeonTester--%d\"),\n+        null,\n+        new TestDruidCoordinatorConfig(\n+            null,\n+            null,\n+            null,\n+            new Duration(1),\n+            null,\n+            null,\n+            10,\n+            null,\n+            false,\n+            false,\n+            new Duration(\"PT1s\"),\n+            Duration.millis(10)\n+        )\n+    );\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTester.java",
                "sha": "c979671ff847594e59c58f57c8a1557ad8c0d822",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/TestDruidCoordinatorConfig.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/TestDruidCoordinatorConfig.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 2,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/TestDruidCoordinatorConfig.java",
                "patch": "@@ -46,7 +46,8 @@ public TestDruidCoordinatorConfig(\n       String consoleStatic,\n       boolean mergeSegments,\n       boolean convertSegments,\n-      Duration getLoadQueuePeonRepeatDelay\n+      Duration getLoadQueuePeonRepeatDelay,\n+      Duration CuratorCreateZkNodesRepeatDelay\n   )\n   {\n     this.coordinatorStartDelay = coordinatorStartDelay;\n@@ -108,8 +109,10 @@ public String getConsoleStatic()\n     return consoleStatic;\n   }\n \n-  @Override public Duration getLoadQueuePeonRepeatDelay()\n+  @Override\n+  public Duration getLoadQueuePeonRepeatDelay()\n   {\n     return getLoadQueuePeonRepeatDelay;\n   }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/TestDruidCoordinatorConfig.java",
                "sha": "e1b91354de70d6d6a71f9f18a8ba01d792bd0ec5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/helper/DruidCoordinatorSegmentKillerTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/server/coordinator/helper/DruidCoordinatorSegmentKillerTest.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/server/coordinator/helper/DruidCoordinatorSegmentKillerTest.java",
                "patch": "@@ -113,7 +113,8 @@ private void testFindIntervalForKillTask(List<Interval> segmentManagerResult, In\n             null,\n             false,\n             false,\n-            Duration.ZERO\n+            Duration.ZERO,\n+            Duration.millis(10)\n         )\n     );\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/server/src/test/java/org/apache/druid/server/coordinator/helper/DruidCoordinatorSegmentKillerTest.java",
                "sha": "7ee58a2a8f192013d0475b09e3e95b4ef0cf2bac",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/afbcb9c07f21c12f241f1dc6575589ef14cda836/services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/services/src/main/java/org/apache/druid/cli/CliCoordinator.java?ref=afbcb9c07f21c12f241f1dc6575589ef14cda836",
                "deletions": 2,
                "filename": "services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.druid.guice.annotations.CoordinatorIndexingServiceHelper;\n import org.apache.druid.guice.annotations.EscalatedGlobal;\n import org.apache.druid.guice.http.JettyHttpClientModule;\n+import org.apache.druid.java.util.common.concurrent.Execs;\n import org.apache.druid.java.util.common.concurrent.ScheduledExecutorFactory;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.java.util.http.client.HttpClient;\n@@ -90,7 +91,7 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Properties;\n-import java.util.concurrent.Executors;\n+import java.util.concurrent.ExecutorService;\n \n /**\n  */\n@@ -249,11 +250,19 @@ public LoadQueueTaskMaster getLoadQueueTaskMaster(\n               ZkPathsConfig zkPaths\n           )\n           {\n+            boolean useHttpLoadQueuePeon = \"http\".equalsIgnoreCase(config.getLoadQueuePeonType());\n+            ExecutorService callBackExec;\n+            if (useHttpLoadQueuePeon) {\n+              callBackExec = Execs.singleThreaded(\"LoadQueuePeon-callbackexec--%d\");\n+            } else {\n+              callBackExec = Execs.multiThreaded(config.getNumCuratorCallBackThreads(), \"LoadQueuePeon\"\n+                                                                                        + \"-callbackexec--%d\");\n+            }\n             return new LoadQueueTaskMaster(\n                 curator,\n                 jsonMapper,\n                 factory.create(1, \"Master-PeonExec--%d\"),\n-                Executors.newSingleThreadExecutor(),\n+                callBackExec,\n                 config,\n                 httpClient,\n                 zkPaths",
                "raw_url": "https://github.com/apache/incubator-druid/raw/afbcb9c07f21c12f241f1dc6575589ef14cda836/services/src/main/java/org/apache/druid/cli/CliCoordinator.java",
                "sha": "88245624093d775542f25d1432b5eb71e146b14c",
                "status": "modified"
            }
        ],
        "message": "Improve parallelism of zookeeper based segment change processing (#7088)\n\n* V1 - improve parallelism of zookeeper based segment change processing\r\n\r\n* Create zk nodes in batches. Address code review comments.\r\nIntroduce various configs.\r\n\r\n* Add documentation for the newly added configs\r\n\r\n* Fix test failures\r\n\r\n* Fix more test failures\r\n\r\n* Remove prinstacktrace statements\r\n\r\n* Address code review comments\r\n\r\n* Use a single queue\r\n\r\n* Address code review comments\r\n\r\nSince we have a separate load peon for every historical, just having a single SegmentChangeProcessor\r\ntask per historical is enough. This commit also gets rid of the associated config druid.coordinator.loadqueuepeon.curator.numCreateThreads\r\n\r\n* Resolve merge conflict\r\n\r\n* Fix compilation failure\r\n\r\n* Remove batching since we already have a dynamic config maxSegmentsInNodeLoadingQueue that provides that control\r\n\r\n* Fix NPE in test\r\n\r\n* Remove documentation for configs that are no longer needed\r\n\r\n* Address code review comments\r\n\r\n* Address more code review comments\r\n\r\n* Fix checkstyle issue\r\n\r\n* Address code review comments\r\n\r\n* Code review comments\r\n\r\n* Add back monitor node remove executor\r\n\r\n* Cleanup code to isolate null checks  and minor refactoring\r\n\r\n* Change param name since it conflicts with member variable name",
        "parent": "https://github.com/apache/incubator-druid/commit/a013350018d96cbb40944a0b5e060d510eeabdbc",
        "repo": "incubator-druid",
        "unit_tests": [
            "ZkCoordinatorTest.java",
            "DruidCoordinatorConfigTest.java",
            "TestDruidCoordinatorConfig.java"
        ]
    },
    "incubator-druid_b0e33ac": {
        "bug_id": "incubator-druid_b0e33ac",
        "commit": "https://github.com/apache/incubator-druid/commit/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java?ref=b0e33ac879ac3f619f09c4657b44dd9797fbe4bd",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "patch": "@@ -44,6 +44,7 @@\n import java.nio.ByteBuffer;\n import java.util.List;\n import java.util.Map;\n+import java.util.Queue;\n import java.util.concurrent.CancellationException;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n@@ -84,7 +85,7 @@ public GroupByParallelQueryRunner(\n         configSupplier.get(),\n         bufferPool\n     );\n-    final Pair<List, Accumulator<List, T>> bySegmentAccumulatorPair = GroupByQueryHelper.createBySegmentAccumulatorPair();\n+    final Pair<Queue, Accumulator<Queue, T>> bySegmentAccumulatorPair = GroupByQueryHelper.createBySegmentAccumulatorPair();\n     final boolean bySegment = query.getContextBySegment(false);\n     final int priority = query.getContextPriority(0);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/GroupByParallelQueryRunner.java",
                "sha": "552bea9085bd162627b5cbceaf7d7ce3afe0f8d1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java?ref=b0e33ac879ac3f619f09c4657b44dd9797fbe4bd",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java",
                "patch": "@@ -36,7 +36,10 @@\n import io.druid.segment.incremental.OnheapIncrementalIndex;\n \n import java.nio.ByteBuffer;\n+import java.util.Collections;\n import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n \n public class GroupByQueryHelper\n {\n@@ -130,18 +133,19 @@ public IncrementalIndex accumulate(IncrementalIndex accumulated, T in)\n     return new Pair<>(index, accumulator);\n   }\n \n-  public static <T> Pair<List, Accumulator<List, T>> createBySegmentAccumulatorPair()\n+  public static <T> Pair<Queue, Accumulator<Queue, T>> createBySegmentAccumulatorPair()\n   {\n-    List init = Lists.newArrayList();\n-    Accumulator<List, T> accumulator = new Accumulator<List, T>()\n+    // In parallel query runner multiple threads add to this queue concurrently\n+    Queue init = new ConcurrentLinkedQueue<>();\n+    Accumulator<Queue, T> accumulator = new Accumulator<Queue, T>()\n     {\n       @Override\n-      public List accumulate(List accumulated, T in)\n+      public Queue accumulate(Queue accumulated, T in)\n       {\n         if(in == null){\n           throw new ISE(\"Cannot have null result\");\n         }\n-        accumulated.add(in);\n+        accumulated.offer(in);\n         return accumulated;\n       }\n     };",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/groupby/GroupByQueryHelper.java",
                "sha": "5f07b6fcf39c80bf13383e41caed6893ad97e354",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/groupby/GroupByQueryRunnerFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/groupby/GroupByQueryRunnerFactory.java?ref=b0e33ac879ac3f619f09c4657b44dd9797fbe4bd",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/groupby/GroupByQueryRunnerFactory.java",
                "patch": "@@ -49,6 +49,7 @@\n import java.nio.ByteBuffer;\n import java.util.List;\n import java.util.Map;\n+import java.util.Queue;\n import java.util.concurrent.CancellationException;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n@@ -115,7 +116,7 @@ public GroupByQueryRunnerFactory(\n                               config.get(),\n                               computationBufferPool\n                           );\n-                      final Pair<List, Accumulator<List, Row>> bySegmentAccumulatorPair = GroupByQueryHelper.createBySegmentAccumulatorPair();\n+                      final Pair<Queue, Accumulator<Queue, Row>> bySegmentAccumulatorPair = GroupByQueryHelper.createBySegmentAccumulatorPair();\n                       final int priority = query.getContextPriority(0);\n                       final boolean bySegment = query.getContextBySegment(false);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/main/java/io/druid/query/groupby/GroupByQueryRunnerFactory.java",
                "sha": "58ccf98c083fabe3d37630b62fbad18a77598d78",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java?ref=b0e33ac879ac3f619f09c4657b44dd9797fbe4bd",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Supplier;\n import com.google.common.base.Suppliers;\n import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n@@ -33,9 +34,14 @@\n import io.druid.granularity.PeriodGranularity;\n import io.druid.granularity.QueryGranularity;\n import io.druid.jackson.DefaultObjectMapper;\n+import io.druid.query.BySegmentResultValue;\n+import io.druid.query.BySegmentResultValueClass;\n+import io.druid.query.FinalizeResultsQueryRunner;\n import io.druid.query.Query;\n import io.druid.query.QueryRunner;\n import io.druid.query.QueryRunnerTestHelper;\n+import io.druid.query.QueryToolChest;\n+import io.druid.query.Result;\n import io.druid.query.TestQueryRunners;\n import io.druid.query.aggregation.AggregatorFactory;\n import io.druid.query.aggregation.DoubleSumAggregatorFactory;\n@@ -88,6 +94,8 @@\n import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n \n @RunWith(Parameterized.class)\n public class GroupByQueryRunnerTest\n@@ -2337,4 +2345,59 @@ public void testGroupByTimeExtraction()\n     Iterable<Row> results = GroupByQueryRunnerTestHelper.runQuery(factory, runner, query);\n     TestHelper.assertExpectedObjects(expectedResults, results, \"\");\n   }\n+\n+  @Test\n+  public void testBySegmentResults()\n+  {\n+    int segmentCount = 32;\n+    Result<BySegmentResultValue> singleSegmentResult = new Result<BySegmentResultValue>(\n+        new DateTime(\"2011-01-12T00:00:00.000Z\"),\n+        new BySegmentResultValueClass(\n+            Arrays.asList(\n+                GroupByQueryRunnerTestHelper.createExpectedRow(\n+                    \"2011-04-01\",\n+                    \"alias\",\n+                    \"mezzanine\",\n+                    \"rows\",\n+                    6L,\n+                    \"idx\",\n+                    4420L\n+                )\n+            ), \"testSegment\", new Interval(\"2011-04-02T00:00:00.000Z/2011-04-04T00:00:00.000Z\")\n+        )\n+    );\n+    List<Result> bySegmentResults = Lists.newArrayList();\n+    for (int i = 0; i < segmentCount; i++) {\n+      bySegmentResults.add(singleSegmentResult);\n+    }\n+    GroupByQuery.Builder builder = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setInterval(\"2011-04-02/2011-04-04\")\n+        .setDimensions(Lists.<DimensionSpec>newArrayList(new DefaultDimensionSpec(\"quality\", \"alias\")))\n+        .setAggregatorSpecs(\n+            Arrays.asList(\n+                QueryRunnerTestHelper.rowsCount,\n+                new LongSumAggregatorFactory(\"idx\", \"index\")\n+            )\n+        )\n+        .setGranularity(new PeriodGranularity(new Period(\"P1M\"), null, null))\n+        .setDimFilter(new SelectorDimFilter(\"quality\", \"mezzanine\"))\n+        .setContext(ImmutableMap.<String, Object>of(\"bySegment\", true));\n+    final GroupByQuery fullQuery = builder.build();\n+    QueryToolChest toolChest = factory.getToolchest();\n+\n+    List<QueryRunner<Row>> singleSegmentRunners = Lists.newArrayList();\n+    for (int i = 0; i < segmentCount; i++) {\n+      singleSegmentRunners.add(toolChest.preMergeQueryDecoration(runner));\n+    }\n+    ExecutorService exec = Executors.newCachedThreadPool();\n+    QueryRunner theRunner = new FinalizeResultsQueryRunner<>(\n+        toolChest.mergeResults(factory.mergeRunners(Executors.newCachedThreadPool(), singleSegmentRunners)),\n+        toolChest\n+    );\n+\n+    TestHelper.assertExpectedObjects(bySegmentResults, theRunner.run(fullQuery, Maps.newHashMap()), \"\");\n+    exec.shutdownNow();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b0e33ac879ac3f619f09c4657b44dd9797fbe4bd/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "sha": "da31afa0ddef121dcd6e9b177519806858f550b2",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1206 from metamx/group-by-NPE-fix\n\nfix race in groupByParallelQueryRunner",
        "parent": "https://github.com/apache/incubator-druid/commit/9d6b728054061b3ca0eb452b5655e485a975a3e1",
        "repo": "incubator-druid",
        "unit_tests": [
            "GroupByQueryRunnerFactoryTest.java"
        ]
    },
    "incubator-druid_b31abd0": {
        "bug_id": "incubator-druid_b31abd0",
        "commit": "https://github.com/apache/incubator-druid/commit/b31abd03add31b8b0efdf0d96c9ad50c07156088",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java?ref=b31abd03add31b8b0efdf0d96c9ad50c07156088",
                "deletions": 4,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "patch": "@@ -481,10 +481,9 @@ public ZkWorker findWorkerRunningTask(String taskId)\n     return null;\n   }\n \n-  public boolean isWorkerRunningTask(Worker worker, String taskId)\n+  public boolean isWorkerRunningTask(ZkWorker worker, String taskId)\n   {\n-    ZkWorker zkWorker = zkWorkers.get(worker.getHost());\n-    return (zkWorker != null && zkWorker.isRunningTask(taskId));\n+    return Preconditions.checkNotNull(worker, \"worker\").isRunningTask(taskId);\n   }\n \n   /**\n@@ -869,7 +868,7 @@ private boolean announceTask(\n       // Syncing state with Zookeeper - don't assign new tasks until the task we just assigned is actually running\n       // on a worker - this avoids overflowing a worker with tasks\n       Stopwatch timeoutStopwatch = Stopwatch.createStarted();\n-      while (!isWorkerRunningTask(theZkWorker.getWorker(), task.getId())) {\n+      while (!isWorkerRunningTask(theZkWorker, task.getId())) {\n         final long waitMs = config.getTaskAssignmentTimeout().toStandardDuration().getMillis();\n         statusLock.wait(waitMs);\n         long elapsed = timeoutStopwatch.elapsed(TimeUnit.MILLISECONDS);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/main/java/io/druid/indexing/overlord/RemoteTaskRunner.java",
                "sha": "c3e26bd5fac46dcf05935f9846b0374978314bb2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java?ref=b31abd03add31b8b0efdf0d96c9ad50c07156088",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "patch": "@@ -27,16 +27,15 @@\n import com.google.common.util.concurrent.MoreExecutors;\n import com.google.inject.Inject;\n import com.metamx.emitter.EmittingLogger;\n-import io.druid.java.util.common.concurrent.Execs;\n import io.druid.indexer.TaskLocation;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.TaskRunner;\n import io.druid.indexing.overlord.TaskRunnerListener;\n import io.druid.java.util.common.Pair;\n+import io.druid.java.util.common.concurrent.Execs;\n import io.druid.java.util.common.lifecycle.LifecycleStart;\n import io.druid.java.util.common.lifecycle.LifecycleStop;\n-\n import org.apache.curator.framework.CuratorFramework;\n import org.apache.curator.framework.recipes.cache.PathChildrenCache;\n import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/main/java/io/druid/indexing/worker/WorkerTaskMonitor.java",
                "sha": "5d470102f133b1122343918ff5c9c6f42b148e3f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java?ref=b31abd03add31b8b0efdf0d96c9ad50c07156088",
                "deletions": 1,
                "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n import com.google.common.base.Supplier;\n import com.google.common.base.Throwables;\n import com.metamx.http.client.HttpClient;\n@@ -170,10 +171,17 @@ void mockWorkerRunningTask(final String workerId, final Task task) throws Except\n   {\n     cf.delete().forPath(joiner.join(tasksPath, workerId, task.getId()));\n \n+    final String taskStatusPath = joiner.join(statusPath, workerId, task.getId());\n     TaskAnnouncement taskAnnouncement = TaskAnnouncement.create(task, TaskStatus.running(task.getId()), DUMMY_LOCATION);\n     cf.create()\n       .creatingParentsIfNeeded()\n-      .forPath(joiner.join(statusPath, workerId, task.getId()), jsonMapper.writeValueAsBytes(taskAnnouncement));\n+      .forPath(taskStatusPath, jsonMapper.writeValueAsBytes(taskAnnouncement));\n+\n+    Preconditions.checkNotNull(\n+        cf.checkExists().forPath(taskStatusPath),\n+        \"Failed to write status on [%s]\",\n+        taskStatusPath\n+    );\n   }\n \n   void mockWorkerCompleteSuccessfulTask(final String workerId, final Task task) throws Exception",
                "raw_url": "https://github.com/apache/incubator-druid/raw/b31abd03add31b8b0efdf0d96c9ad50c07156088/indexing-service/src/test/java/io/druid/indexing/overlord/RemoteTaskRunnerTestUtils.java",
                "sha": "7bcdff1530ad400e497d061085beef9b08d23961",
                "status": "modified"
            }
        ],
        "message": "Fix timeout in RemoteTaskRunnerTest (#5191)\n\n* Fix timeout in RemoteTaskRunnerTest\r\n\r\n* add message for npe",
        "parent": "https://github.com/apache/incubator-druid/commit/ba873c614ba768ca76c4c99c32f859bd95bd638c",
        "repo": "incubator-druid",
        "unit_tests": [
            "RemoteTaskRunnerTest.java",
            "WorkerTaskMonitorTest.java"
        ]
    },
    "incubator-druid_ba3dbf2": {
        "bug_id": "incubator-druid_ba3dbf2",
        "commit": "https://github.com/apache/incubator-druid/commit/ba3dbf2a42300c78ff0d8718627fd25256c8a945",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/ba3dbf2a42300c78ff0d8718627fd25256c8a945/api/src/main/java/io/druid/data/input/impl/DimensionSchema.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/api/src/main/java/io/druid/data/input/impl/DimensionSchema.java?ref=ba3dbf2a42300c78ff0d8718627fd25256c8a945",
                "deletions": 2,
                "filename": "api/src/main/java/io/druid/data/input/impl/DimensionSchema.java",
                "patch": "@@ -25,9 +25,10 @@\n import com.fasterxml.jackson.annotation.JsonSubTypes;\n import com.fasterxml.jackson.annotation.JsonTypeInfo;\n import com.fasterxml.jackson.annotation.JsonValue;\n-import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n import io.druid.guice.annotations.PublicApi;\n import io.druid.java.util.common.StringUtils;\n+import io.druid.java.util.emitter.EmittingLogger;\n \n import java.util.Objects;\n \n@@ -49,6 +50,7 @@\n   public static final String FLOAT_TYPE_NAME = \"float\";\n   public static final String SPATIAL_TYPE_NAME = \"spatial\";\n   public static final String DOUBLE_TYPE_NAME = \"double\";\n+  private static final EmittingLogger log = new EmittingLogger(DimensionSchema.class);\n \n \n   // main druid and druid-api should really use the same ValueType enum.\n@@ -124,7 +126,10 @@ public static MultiValueHandling ofDefault()\n \n   protected DimensionSchema(String name, MultiValueHandling multiValueHandling, boolean createBitmapIndex)\n   {\n-    this.name = Preconditions.checkNotNull(name, \"Dimension name cannot be null.\");\n+    if (Strings.isNullOrEmpty(name)) {\n+      log.warn(\"Null or Empty Dimension found\");\n+    }\n+    this.name = name;\n     this.multiValueHandling = multiValueHandling == null ? MultiValueHandling.ofDefault() : multiValueHandling;\n     this.createBitmapIndex = createBitmapIndex;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/ba3dbf2a42300c78ff0d8718627fd25256c8a945/api/src/main/java/io/druid/data/input/impl/DimensionSchema.java",
                "sha": "f2d9f9111860e19465b948333a681b006c59bec9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/main/java/io/druid/segment/IndexIO.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/IndexIO.java?ref=ba3dbf2a42300c78ff0d8718627fd25256c8a945",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/IndexIO.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.databind.JsonMappingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n import com.google.common.base.Suppliers;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n@@ -601,6 +602,10 @@ public QueryableIndex load(File inDir, ObjectMapper mapper) throws IOException\n       Map<String, Column> columns = Maps.newHashMap();\n \n       for (String columnName : cols) {\n+        if (Strings.isNullOrEmpty(columnName)) {\n+          log.warn(\"Null or Empty Dimension found in the file : \" + inDir);\n+          continue;\n+        }\n         columns.put(columnName, deserializeColumn(mapper, smooshedFiles.mapFile(columnName), smooshedFiles));\n       }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/main/java/io/druid/segment/IndexIO.java",
                "sha": "88d90055943aeed29a072c8d0be19ed94d75ae72",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=ba3dbf2a42300c78ff0d8718627fd25256c8a945",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Function;\n+import com.google.common.base.Strings;\n import com.google.common.base.Supplier;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n@@ -551,6 +552,9 @@ TimeAndDims toTimeAndDims(InputRow row)\n     synchronized (dimensionDescs) {\n       dims = new Object[dimensionDescs.size()];\n       for (String dimension : rowDimensions) {\n+        if (Strings.isNullOrEmpty(dimension)) {\n+          continue;\n+        }\n         boolean wasNewDim = false;\n         ColumnCapabilitiesImpl capabilities;\n         DimensionDesc desc = dimensionDescs.get(dimension);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "332781b9beebb64dfb8c4f7a937bd202ea21a832",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/incubator-druid/blob/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java?ref=ba3dbf2a42300c78ff0d8718627fd25256c8a945",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "patch": "@@ -2484,6 +2484,51 @@ public void testMultiValueHandling() throws Exception\n     }\n   }\n \n+  @Test\n+  public void testDimensionWithEmptyName() throws Exception\n+  {\n+    final long timestamp = System.currentTimeMillis();\n+\n+    IncrementalIndex toPersist = IncrementalIndexTest.createIndex(null);\n+    IncrementalIndexTest.populateIndex(timestamp, toPersist);\n+    toPersist.add(new MapBasedInputRow(\n+        timestamp,\n+        Arrays.asList(\"\", \"dim2\"),\n+        ImmutableMap.<String, Object>of(\"\", \"1\", \"dim2\", \"2\")\n+    ));\n+\n+    final File tempDir = temporaryFolder.newFolder();\n+    QueryableIndex index = closer.closeLater(\n+        indexIO.loadIndex(\n+            indexMerger.persist(\n+                toPersist,\n+                tempDir,\n+                indexSpec,\n+                null\n+            )\n+        )\n+    );\n+\n+    Assert.assertEquals(3, index.getColumn(Column.TIME_COLUMN_NAME).getLength());\n+    Assert.assertEquals(\n+        Arrays.asList(\"dim1\", \"dim2\"),\n+        Lists.newArrayList(index.getAvailableDimensions())\n+    );\n+    Assert.assertEquals(3, index.getColumnNames().size());\n+\n+    assertDimCompression(index, indexSpec.getDimensionCompression());\n+\n+    Assert.assertArrayEquals(\n+        IncrementalIndexTest.getDefaultCombiningAggregatorFactories(),\n+        index.getMetadata().getAggregators()\n+    );\n+\n+    Assert.assertEquals(\n+        Granularities.NONE,\n+        index.getMetadata().getQueryGranularity()\n+    );\n+  }\n+\n   private QueryableIndex persistAndLoad(List<DimensionSchema> schema, InputRow... rows) throws IOException\n   {\n     IncrementalIndex toPersist = IncrementalIndexTest.createIndex(null, new DimensionsSpec(schema, null, null));",
                "raw_url": "https://github.com/apache/incubator-druid/raw/ba3dbf2a42300c78ff0d8718627fd25256c8a945/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "sha": "794371151e3c81324790fb95900fff77d664bf90",
                "status": "modified"
            }
        ],
        "message": "Fixed NPE when dimension is null or empty. https://github.com/druid-io/druid/issues/3007 (#5299)",
        "parent": "https://github.com/apache/incubator-druid/commit/7416d1d02d5e88b92b728c627922e6293066e03a",
        "repo": "incubator-druid",
        "unit_tests": [
            "DimensionSchemaTest.java",
            "IndexIOTest.java",
            "IncrementalIndexTest.java"
        ]
    },
    "incubator-druid_bbedde3": {
        "bug_id": "incubator-druid_bbedde3",
        "commit": "https://github.com/apache/incubator-druid/commit/bbedde3418ba0a83a1777681d549b3ff1295b000",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bbedde3418ba0a83a1777681d549b3ff1295b000/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java?ref=bbedde3418ba0a83a1777681d549b3ff1295b000",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "patch": "@@ -91,7 +91,19 @@ public TopNBinaryFn(\n \n     String dimension = dimensionSpec.getOutputName();\n     for (DimensionAndMetricValueExtractor arg1Val : arg1Vals) {\n-      retVals.put(arg1Val.getStringDimensionValue(dimension), arg1Val);\n+      final String dimensionValue = arg1Val.getStringDimensionValue(dimension);\n+      Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n+      retVal.put(dimension, dimensionValue);\n+\n+      for (AggregatorFactory factory : aggregations) {\n+        final String metricName = factory.getName();\n+        retVal.put(metricName, arg1Val.getMetric(metricName));\n+      }\n+      for (PostAggregator postAgg : postAggregations) {\n+        retVal.put(postAgg.getName(), postAgg.compute(retVal));\n+      }\n+\n+      retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n     }\n     for (DimensionAndMetricValueExtractor arg2Val : arg2Vals) {\n       final String dimensionValue = arg2Val.getStringDimensionValue(dimension);\n@@ -112,7 +124,18 @@ public TopNBinaryFn(\n \n         retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n       } else {\n-        retVals.put(dimensionValue, arg2Val);\n+        Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n+        retVal.put(dimension, dimensionValue);\n+\n+        for (AggregatorFactory factory : aggregations) {\n+          final String metricName = factory.getName();\n+          retVal.put(metricName, arg2Val.getMetric(metricName));\n+        }\n+        for (PostAggregator postAgg : postAggregations) {\n+          retVal.put(postAgg.getName(), postAgg.compute(retVal));\n+        }\n+\n+        retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bbedde3418ba0a83a1777681d549b3ff1295b000/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "sha": "fd2b7a2845729166681f917c840de8eaa4f4e369",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bbedde3418ba0a83a1777681d549b3ff1295b000/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java?ref=bbedde3418ba0a83a1777681d549b3ff1295b000",
                "deletions": 4,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "patch": "@@ -324,9 +324,9 @@ public void testMergeByPostAgg()\n                     \"testdim\", \"2\"\n                 ),\n                 ImmutableMap.<String, Object>of(\n-                    \"rows\", 0L,\n-                    \"index\", 1L,\n-                    \"testdim\", \"3\"\n+                    \"rows\", 4L,\n+                    \"index\", 5L,\n+                    \"testdim\", \"other\"\n                 )\n             )\n         )\n@@ -336,6 +336,12 @@ public void testMergeByPostAgg()\n         currTime,\n         new TopNResultValue(\n             ImmutableList.<Map<String, Object>>of(\n+                ImmutableMap.<String, Object>of(\n+                    \"testdim\", \"other\",\n+                    \"rows\", 4L,\n+                    \"index\", 5L,\n+                    \"addrowsindexconstant\", 10.0\n+                ),\n                 ImmutableMap.<String, Object>of(\n                     \"testdim\", \"1\",\n                     \"rows\", 3L,\n@@ -357,7 +363,7 @@ public void testMergeByPostAgg()\n         QueryGranularity.ALL,\n         new DefaultDimensionSpec(\"testdim\", null),\n         new NumericTopNMetricSpec(\"addrowsindexconstant\"),\n-        2,\n+        3,\n         aggregatorFactories,\n         postAggregators\n     ).apply(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bbedde3418ba0a83a1777681d549b3ff1295b000/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "sha": "1caf76156a1a1ae7115e0391f06c27cb5118377f",
                "status": "modified"
            }
        ],
        "message": "fix npe in topNBinaryFn with post aggs",
        "parent": "https://github.com/apache/incubator-druid/commit/310073685d862eb1980ea5053f87939efb49d1d2",
        "repo": "incubator-druid",
        "unit_tests": [
            "TopNBinaryFnTest.java"
        ]
    },
    "incubator-druid_bcfeac2": {
        "bug_id": "incubator-druid_bcfeac2",
        "commit": "https://github.com/apache/incubator-druid/commit/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223/server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/coordination/ServerManager.java?ref=bcfeac2d8c4d75589c1c74d52bb9c92cb1596223",
                "deletions": 0,
                "filename": "server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "patch": "@@ -20,6 +20,7 @@\n package com.metamx.druid.coordination;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Predicate;\n import com.google.common.collect.Ordering;\n import com.metamx.common.ISE;\n import com.metamx.common.guava.FunctionalIterable;\n@@ -257,6 +258,16 @@ public void dropSegment(final DataSegment segment) throws SegmentLoadingExceptio\n                     );\n               }\n             }\n+        )\n+        .filter(\n+            new Predicate<QueryRunner<T>>()\n+            {\n+              @Override\n+              public boolean apply(@Nullable QueryRunner<T> input)\n+              {\n+                return (input != null);\n+              }\n+            }\n         );\n \n     return new FinalizeResultsQueryRunner<T>(toolChest.mergeResults(factory.mergeRunners(exec, adapters)), toolChest);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bcfeac2d8c4d75589c1c74d52bb9c92cb1596223/server/src/main/java/com/metamx/druid/coordination/ServerManager.java",
                "sha": "00721d042ade6264029922e342d692a413cab197",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/8e38a85788e07852960b0b148e110a0ce2e33078",
        "repo": "incubator-druid",
        "unit_tests": [
            "ServerManagerTest.java"
        ]
    },
    "incubator-druid_bd605a0": {
        "bug_id": "incubator-druid_bd605a0",
        "commit": "https://github.com/apache/incubator-druid/commit/bd605a097e917d0d525d17138f6457b99fd5e90d",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bd605a097e917d0d525d17138f6457b99fd5e90d/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java?ref=bd605a097e917d0d525d17138f6457b99fd5e90d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "patch": "@@ -20,6 +20,7 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n import com.metamx.common.StringUtils;\n \n import java.nio.ByteBuffer;\n@@ -59,8 +60,11 @@ public RegexDimExtractionFn(\n   @Override\n   public String apply(String dimValue)\n   {\n+    if (dimValue == null) {\n+      return null;\n+    }\n     Matcher matcher = pattern.matcher(dimValue);\n-    return matcher.find() ? matcher.group(1) : dimValue;\n+    return Strings.emptyToNull(matcher.find() ? matcher.group(1) : dimValue);\n   }\n \n   @JsonProperty(\"expr\")",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bd605a097e917d0d525d17138f6457b99fd5e90d/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "sha": "b3c33b68e140b9419a8817e14d0329a83465ed78",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bd605a097e917d0d525d17138f6457b99fd5e90d/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java?ref=bd605a097e917d0d525d17138f6457b99fd5e90d",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "patch": "@@ -102,6 +102,20 @@ public void testStringExtraction()\n     Assert.assertTrue(extracted.contains(\"c\"));\n   }\n \n+\n+  @Test\n+  public void testNullAndEmpty()\n+  {\n+    String regex = \"(.*)/.*/.*\";\n+    ExtractionFn extractionFn = new RegexDimExtractionFn(regex);\n+    // no match, map empty input value to null\n+    Assert.assertEquals(null, extractionFn.apply(\"\"));\n+    // null value, returns null\n+    Assert.assertEquals(null, extractionFn.apply(null));\n+    // empty match, map empty result to null\n+    Assert.assertEquals(null, extractionFn.apply(\"/a/b\"));\n+  }\n+\n   @Test\n   public void testSerde() throws Exception\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bd605a097e917d0d525d17138f6457b99fd5e90d/processing/src/test/java/io/druid/query/extraction/RegexDimExtractionFnTest.java",
                "sha": "3da70ba3f533d3cce13b688388f6595356bb0ae7",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1731 from metamx/regex-extraction-npe\n\nfix NPE with regex extraction function",
        "parent": "https://github.com/apache/incubator-druid/commit/5f36e7a99273525b84a3d9343197d3139ca821e0",
        "repo": "incubator-druid",
        "unit_tests": [
            "RegexDimExtractionFnTest.java"
        ]
    },
    "incubator-druid_bda5a8a": {
        "bug_id": "incubator-druid_bda5a8a",
        "commit": "https://github.com/apache/incubator-druid/commit/bda5a8a95e9ccc68221e0d689d27e6c81c3924f4",
        "file": [
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bda5a8a95e9ccc68221e0d689d27e6c81c3924f4/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 154,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=bda5a8a95e9ccc68221e0d689d27e6c81c3924f4",
                "deletions": 55,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -146,6 +146,8 @@\n    */\n   private class TaskGroup\n   {\n+    final int groupId;\n+\n     // This specifies the partitions and starting offsets for this task group. It is set on group creation from the data\n     // in [partitionGroups] and never changes during the lifetime of this task group, which will live until a task in\n     // this task group has completed successfully, at which point this will be destroyed and a new task group will be\n@@ -161,11 +163,13 @@\n     final String baseSequenceName;\n \n     TaskGroup(\n+        int groupId,\n         ImmutableMap<Integer, Long> partitionOffsets,\n         Optional<DateTime> minimumMessageTime,\n         Optional<DateTime> maximumMessageTime\n     )\n     {\n+      this.groupId = groupId;\n       this.partitionOffsets = partitionOffsets;\n       this.minimumMessageTime = minimumMessageTime;\n       this.maximumMessageTime = maximumMessageTime;\n@@ -187,9 +191,21 @@ int addNewCheckpoint(Map<Integer, Long> checkpoint)\n \n   private static class TaskData\n   {\n+    @Nullable\n     volatile TaskStatus status;\n+    @Nullable\n     volatile DateTime startTime;\n     volatile Map<Integer, Long> currentOffsets = new HashMap<>();\n+\n+    @Override\n+    public String toString()\n+    {\n+      return \"TaskData{\" +\n+             \"status=\" + status +\n+             \", startTime=\" + startTime +\n+             \", currentOffsets=\" + currentOffsets +\n+             '}';\n+    }\n   }\n \n   // Map<{group ID}, {actively reading task group}>; see documentation for TaskGroup class\n@@ -718,8 +734,8 @@ public void handle() throws ExecutionException, InterruptedException\n           log.info(\"Already checkpointed with offsets [%s]\", checkpoints.lastEntry().getValue());\n           return;\n         }\n-        final Map<Integer, Long> newCheckpoint = checkpointTaskGroup(taskGroupId, false).get();\n-        taskGroups.get(taskGroupId).addNewCheckpoint(newCheckpoint);\n+        final Map<Integer, Long> newCheckpoint = checkpointTaskGroup(taskGroup, false).get();\n+        taskGroup.addNewCheckpoint(newCheckpoint);\n         log.info(\"Handled checkpoint notice, new checkpoint is [%s] for taskGroup [%s]\", newCheckpoint, taskGroupId);\n       }\n     }\n@@ -785,10 +801,13 @@ void resetInternal(DataSourceMetadata dataSourceMetadata)\n                                                       : currentMetadata.getKafkaPartitions()\n                                                                        .getPartitionOffsetMap()\n                                                                        .get(resetPartitionOffset.getKey());\n-          final TaskGroup partitionTaskGroup = taskGroups.get(getTaskGroupIdForPartition(resetPartitionOffset.getKey()));\n-          if (partitionOffsetInMetadataStore != null ||\n-              (partitionTaskGroup != null && partitionTaskGroup.partitionOffsets.get(resetPartitionOffset.getKey())\n-                                                                                .equals(resetPartitionOffset.getValue()))) {\n+          final TaskGroup partitionTaskGroup = taskGroups.get(\n+              getTaskGroupIdForPartition(resetPartitionOffset.getKey())\n+          );\n+          final boolean isSameOffset = partitionTaskGroup != null\n+                                       && partitionTaskGroup.partitionOffsets.get(resetPartitionOffset.getKey())\n+                                                                             .equals(resetPartitionOffset.getValue());\n+          if (partitionOffsetInMetadataStore != null || isSameOffset) {\n             doReset = true;\n             break;\n           }\n@@ -1012,7 +1031,7 @@ private void discoverTasks() throws ExecutionException, InterruptedException, Ti\n     List<String> futureTaskIds = Lists.newArrayList();\n     List<ListenableFuture<Boolean>> futures = Lists.newArrayList();\n     List<Task> tasks = taskStorage.getActiveTasks();\n-    final Set<Integer> taskGroupsToVerify = new HashSet<>();\n+    final Map<Integer, TaskGroup> taskGroupsToVerify = new HashMap<>();\n \n     for (Task task : tasks) {\n       if (!(task instanceof KafkaIndexTask) || !dataSource.equals(task.getDataSource())) {\n@@ -1119,6 +1138,7 @@ public Boolean apply(KafkaIndexTask.Status status)\n                                 k -> {\n                                   log.info(\"Creating a new task group for taskGroupId[%d]\", taskGroupId);\n                                   return new TaskGroup(\n+                                      taskGroupId,\n                                       ImmutableMap.copyOf(\n                                           kafkaTask.getIOConfig().getStartPartitions().getPartitionOffsetMap()\n                                       ),\n@@ -1127,8 +1147,15 @@ public Boolean apply(KafkaIndexTask.Status status)\n                                   );\n                                 }\n                             );\n-                            taskGroupsToVerify.add(taskGroupId);\n-                            taskGroup.tasks.putIfAbsent(taskId, new TaskData());\n+                            taskGroupsToVerify.put(taskGroupId, taskGroup);\n+                            final TaskData prevTaskGroup = taskGroup.tasks.putIfAbsent(taskId, new TaskData());\n+                            if (prevTaskGroup != null) {\n+                              throw new ISE(\n+                                  \"WTH? a taskGroup[%s] already exists for new task[%s]\",\n+                                  prevTaskGroup,\n+                                  taskId\n+                              );\n+                            }\n                           }\n                         }\n                         return true;\n@@ -1156,7 +1183,7 @@ public Boolean apply(KafkaIndexTask.Status status)\n     log.debug(\"Found [%d] Kafka indexing tasks for dataSource [%s]\", taskCount, dataSource);\n \n     // make sure the checkpoints are consistent with each other and with the metadata store\n-    taskGroupsToVerify.forEach(this::verifyAndMergeCheckpoints);\n+    taskGroupsToVerify.values().forEach(this::verifyAndMergeCheckpoints);\n   }\n \n   /**\n@@ -1166,10 +1193,9 @@ public Boolean apply(KafkaIndexTask.Status status)\n    * 2. truncates the checkpoints in the taskGroup corresponding to which segments have been published, so that any newly\n    * created tasks for the taskGroup start indexing from after the latest published offsets.\n    */\n-  private void verifyAndMergeCheckpoints(final Integer groupId)\n+  private void verifyAndMergeCheckpoints(final TaskGroup taskGroup)\n   {\n-    final TaskGroup taskGroup = taskGroups.get(groupId);\n-\n+    final int groupId = taskGroup.groupId;\n     // List<TaskId, Map -> {SequenceId, Checkpoints}>\n     final List<Pair<String, TreeMap<Integer, Map<Integer, Long>>>> taskSequences = new CopyOnWriteArrayList<>();\n     final List<ListenableFuture<TreeMap<Integer, Map<Integer, Long>>>> futures = new ArrayList<>();\n@@ -1330,6 +1356,7 @@ private void addDiscoveredTaskToPendingCompletionTaskGroups(\n     // reading the minimumMessageTime & maximumMessageTime from the publishing task and setting it here is not necessary as this task cannot\n     // change to a state where it will read any more events\n     TaskGroup newTaskGroup = new TaskGroup(\n+        groupId,\n         ImmutableMap.copyOf(startingPartitions),\n         Optional.absent(),\n         Optional.absent()\n@@ -1367,8 +1394,8 @@ public Boolean apply(@Nullable DateTime startTime)\n                       }\n \n                       taskData.startTime = startTime;\n-                      long millisRemaining = ioConfig.getTaskDuration().getMillis() - (System.currentTimeMillis()\n-                                                                                       - taskData.startTime.getMillis());\n+                      long millisRemaining = ioConfig.getTaskDuration().getMillis() -\n+                                             (System.currentTimeMillis() - taskData.startTime.getMillis());\n                       if (millisRemaining > 0) {\n                         scheduledExec.schedule(\n                             buildRunTask(),\n@@ -1421,7 +1448,8 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n       // find the longest running task from this group\n       DateTime earliestTaskStart = DateTimes.nowUtc();\n       for (TaskData taskData : group.tasks.values()) {\n-        if (earliestTaskStart.isAfter(taskData.startTime)) {\n+        // startTime can be null if kafkaSupervisor is stopped gracefully before processing any runNotice\n+        if (taskData.startTime != null && earliestTaskStart.isAfter(taskData.startTime)) {\n           earliestTaskStart = taskData.startTime;\n         }\n       }\n@@ -1430,7 +1458,7 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n       if (earliestTaskStart.plus(ioConfig.getTaskDuration()).isBeforeNow()) {\n         log.info(\"Task group [%d] has run for [%s]\", groupId, ioConfig.getTaskDuration());\n         futureGroupIds.add(groupId);\n-        futures.add(checkpointTaskGroup(groupId, true));\n+        futures.add(checkpointTaskGroup(group, true));\n       }\n     }\n \n@@ -1468,10 +1496,8 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n     }\n   }\n \n-  private ListenableFuture<Map<Integer, Long>> checkpointTaskGroup(final int groupId, final boolean finalize)\n+  private ListenableFuture<Map<Integer, Long>> checkpointTaskGroup(final TaskGroup taskGroup, final boolean finalize)\n   {\n-    final TaskGroup taskGroup = taskGroups.get(groupId);\n-\n     if (finalize) {\n       // 1) Check if any task completed (in which case we're done) and kill unassigned tasks\n       Iterator<Map.Entry<String, TaskData>> i = taskGroup.tasks.entrySet().iterator();\n@@ -1480,30 +1506,33 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n         String taskId = taskEntry.getKey();\n         TaskData task = taskEntry.getValue();\n \n-        if (task.status.isSuccess()) {\n-          // If any task in this group has already completed, stop the rest of the tasks in the group and return.\n-          // This will cause us to create a new set of tasks next cycle that will start from the offsets in\n-          // metadata store (which will have advanced if we succeeded in publishing and will remain the same if publishing\n-          // failed and we need to re-ingest)\n-          return Futures.transform(\n-              stopTasksInGroup(taskGroup),\n-              new Function<Object, Map<Integer, Long>>()\n-              {\n-                @Nullable\n-                @Override\n-                public Map<Integer, Long> apply(@Nullable Object input)\n+        // task.status can be null if kafkaSupervisor is stopped gracefully before processing any runNotice.\n+        if (task.status != null) {\n+          if (task.status.isSuccess()) {\n+            // If any task in this group has already completed, stop the rest of the tasks in the group and return.\n+            // This will cause us to create a new set of tasks next cycle that will start from the offsets in\n+            // metadata store (which will have advanced if we succeeded in publishing and will remain the same if\n+            // publishing failed and we need to re-ingest)\n+            return Futures.transform(\n+                stopTasksInGroup(taskGroup),\n+                new Function<Object, Map<Integer, Long>>()\n                 {\n-                  return null;\n+                  @Nullable\n+                  @Override\n+                  public Map<Integer, Long> apply(@Nullable Object input)\n+                  {\n+                    return null;\n+                  }\n                 }\n-              }\n-          );\n-        }\n+            );\n+          }\n \n-        if (task.status.isRunnable()) {\n-          if (taskInfoProvider.getTaskLocation(taskId).equals(TaskLocation.unknown())) {\n-            log.info(\"Killing task [%s] which hasn't been assigned to a worker\", taskId);\n-            killTask(taskId);\n-            i.remove();\n+          if (task.status.isRunnable()) {\n+            if (taskInfoProvider.getTaskLocation(taskId).equals(TaskLocation.unknown())) {\n+              log.info(\"Killing task [%s] which hasn't been assigned to a worker\", taskId);\n+              killTask(taskId);\n+              i.remove();\n+            }\n           }\n         }\n       }\n@@ -1550,7 +1579,7 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n             final List<String> setEndOffsetTaskIds = ImmutableList.copyOf(taskGroup.taskIds());\n \n             if (setEndOffsetTaskIds.isEmpty()) {\n-              log.info(\"All tasks in taskGroup [%d] have failed, tasks will be re-created\", groupId);\n+              log.info(\"All tasks in taskGroup [%d] have failed, tasks will be re-created\", taskGroup.groupId);\n               return null;\n             }\n \n@@ -1561,11 +1590,15 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n                     \"Checkpoint [%s] is same as the start offsets [%s] of latest sequence for the task group [%d]\",\n                     endOffsets,\n                     taskGroup.sequenceOffsets.lastEntry().getValue(),\n-                    groupId\n+                    taskGroup.groupId\n                 );\n               }\n \n-              log.info(\"Setting endOffsets for tasks in taskGroup [%d] to %s and resuming\", groupId, endOffsets);\n+              log.info(\n+                  \"Setting endOffsets for tasks in taskGroup [%d] to %s and resuming\",\n+                  taskGroup.groupId,\n+                  endOffsets\n+              );\n               for (final String taskId : setEndOffsetTaskIds) {\n                 setEndOffsetFutures.add(taskClient.setEndOffsetsAsync(taskId, endOffsets, finalize));\n               }\n@@ -1587,7 +1620,7 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n             }\n \n             if (taskGroup.tasks.isEmpty()) {\n-              log.info(\"All tasks in taskGroup [%d] have failed, tasks will be re-created\", groupId);\n+              log.info(\"All tasks in taskGroup [%d] have failed, tasks will be re-created\", taskGroup.groupId);\n               return null;\n             }\n \n@@ -1627,11 +1660,15 @@ private void checkPendingCompletionTasks() throws ExecutionException, Interrupte\n           continue;\n         }\n \n-        Iterator<Map.Entry<String, TaskData>> iTask = group.tasks.entrySet().iterator();\n+        Iterator<Entry<String, TaskData>> iTask = group.tasks.entrySet().iterator();\n         while (iTask.hasNext()) {\n-          Map.Entry<String, TaskData> task = iTask.next();\n+          final Entry<String, TaskData> entry = iTask.next();\n+          final String taskId = entry.getKey();\n+          final TaskData taskData = entry.getValue();\n+\n+          Preconditions.checkNotNull(taskData.status, \"WTH? task[%s] has a null status\", taskId);\n \n-          if (task.getValue().status.isFailure()) {\n+          if (taskData.status.isFailure()) {\n             iTask.remove(); // remove failed task\n             if (group.tasks.isEmpty()) {\n               // if all tasks in the group have failed, just nuke all task groups with this partition set and restart\n@@ -1640,10 +1677,10 @@ private void checkPendingCompletionTasks() throws ExecutionException, Interrupte\n             }\n           }\n \n-          if (task.getValue().status.isSuccess()) {\n+          if (taskData.status.isSuccess()) {\n             // If one of the pending completion tasks was successful, stop the rest of the tasks in the group as\n             // we no longer need them to publish their segment.\n-            log.info(\"Task [%s] completed successfully, stopping tasks %s\", task.getKey(), group.taskIds());\n+            log.info(\"Task [%s] completed successfully, stopping tasks %s\", taskId, group.taskIds());\n             futures.add(stopTasksInGroup(group));\n             foundSuccess = true;\n             toRemove.add(group); // remove the TaskGroup from the list of pending completion task groups\n@@ -1714,6 +1751,8 @@ private void checkCurrentTaskState() throws ExecutionException, InterruptedExcep\n           continue;\n         }\n \n+        Preconditions.checkNotNull(taskData.status, \"WTH? task[%s] has a null status\", taskId);\n+\n         // remove failed tasks\n         if (taskData.status.isFailure()) {\n           iTasks.remove();\n@@ -1741,7 +1780,7 @@ void createNewTasks() throws JsonProcessingException\n     taskGroups.entrySet()\n               .stream()\n               .filter(taskGroup -> taskGroup.getValue().tasks.size() < ioConfig.getReplicas())\n-              .forEach(taskGroup -> verifyAndMergeCheckpoints(taskGroup.getKey()));\n+              .forEach(taskGroup -> verifyAndMergeCheckpoints(taskGroup.getValue()));\n \n     // check that there is a current task group for each group of partitions in [partitionGroups]\n     for (Integer groupId : partitionGroups.keySet()) {\n@@ -1757,6 +1796,7 @@ void createNewTasks() throws JsonProcessingException\n         ) : Optional.absent());\n \n         final TaskGroup taskGroup = new TaskGroup(\n+            groupId,\n             generateStartingOffsetsForPartitionGroup(groupId),\n             minimumMessageTime,\n             maximumMessageTime\n@@ -1984,8 +2024,12 @@ private boolean isTaskCurrent(int taskGroupId, String taskId)\n \n     final List<ListenableFuture<Void>> futures = Lists.newArrayList();\n     for (Map.Entry<String, TaskData> entry : taskGroup.tasks.entrySet()) {\n-      if (!entry.getValue().status.isComplete()) {\n-        futures.add(stopTask(entry.getKey(), false));\n+      final String taskId = entry.getKey();\n+      final TaskData taskData = entry.getValue();\n+      if (taskData.status == null) {\n+        killTask(taskId);\n+      } else if (!taskData.status.isComplete()) {\n+        futures.add(stopTask(taskId, false));\n       }\n     }\n \n@@ -2066,7 +2110,7 @@ private boolean isTaskInPendingCompletionGroups(String taskId)\n       for (TaskGroup taskGroup : taskGroups.values()) {\n         for (Map.Entry<String, TaskData> entry : taskGroup.tasks.entrySet()) {\n           String taskId = entry.getKey();\n-          DateTime startTime = entry.getValue().startTime;\n+          @Nullable DateTime startTime = entry.getValue().startTime;\n           Map<Integer, Long> currentOffsets = entry.getValue().currentOffsets;\n           Long remainingSeconds = null;\n           if (startTime != null) {\n@@ -2093,7 +2137,7 @@ private boolean isTaskInPendingCompletionGroups(String taskId)\n         for (TaskGroup taskGroup : taskGroups) {\n           for (Map.Entry<String, TaskData> entry : taskGroup.tasks.entrySet()) {\n             String taskId = entry.getKey();\n-            DateTime startTime = entry.getValue().startTime;\n+            @Nullable DateTime startTime = entry.getValue().startTime;\n             Map<Integer, Long> currentOffsets = entry.getValue().currentOffsets;\n             Long remainingSeconds = null;\n             if (taskGroup.completionTimeout != null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bda5a8a95e9ccc68221e0d689d27e6c81c3924f4/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "8e12f591461aa728a6a58bbbc1ee1d3674e1b051",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bda5a8a95e9ccc68221e0d689d27e6c81c3924f4/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/TaskReportData.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/TaskReportData.java?ref=bda5a8a95e9ccc68221e0d689d27e6c81c3924f4",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/TaskReportData.java",
                "patch": "@@ -45,7 +45,7 @@ public TaskReportData(\n       String id,\n       @Nullable Map<Integer, Long> startingOffsets,\n       @Nullable Map<Integer, Long> currentOffsets,\n-      DateTime startTime,\n+      @Nullable DateTime startTime,\n       Long remainingSeconds,\n       TaskType type,\n       @Nullable Map<Integer, Long> lag",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bda5a8a95e9ccc68221e0d689d27e6c81c3924f4/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/TaskReportData.java",
                "sha": "6abc3b02f6a3ea277d67e354dade4163798aba65",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in KafkaSupervisor.checkpointTaskGroup (#6206)\n\n* Fix NPE in KafkaSupervisor.checkpointTaskGroup\r\n\r\n* address comments\r\n\r\n* address comment",
        "parent": "https://github.com/apache/incubator-druid/commit/0172326c62bed0a342d6c147af08ce598016ff91",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaSupervisorTest.java"
        ]
    },
    "incubator-druid_bf4b523": {
        "bug_id": "incubator-druid_bf4b523",
        "commit": "https://github.com/apache/incubator-druid/commit/bf4b52384cfa0ebe582d222004273762de674bda",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bf4b52384cfa0ebe582d222004273762de674bda/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=bf4b52384cfa0ebe582d222004273762de674bda",
                "deletions": 2,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -130,7 +130,6 @@ public Response doPost(\n   {\n     final long start = System.currentTimeMillis();\n     Query query = null;\n-    byte[] requestQuery = null;\n     String queryId = null;\n \n     final String reqContentType = req.getContentType();\n@@ -267,9 +266,10 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n       ).build();\n     }\n     catch (Exception e) {\n+      // Input stream has already been consumed by the json object mapper if query == null\n       final String queryString =\n           query == null\n-          ? (isSmile ? \"smile_unknown\" : new String(requestQuery, Charsets.UTF_8))\n+          ? \"unparsable query\"\n           : query.toString();\n \n       log.warn(e, \"Exception occurred on request [%s]\", queryString);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bf4b52384cfa0ebe582d222004273762de674bda/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "165904668935fc1f7379739ab147a424108c4d28",
                "status": "modified"
            },
            {
                "additions": 153,
                "blob_url": "https://github.com/apache/incubator-druid/blob/bf4b52384cfa0ebe582d222004273762de674bda/server/src/test/java/io/druid/server/QueryResourceTest.java",
                "changes": 153,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/server/QueryResourceTest.java?ref=bf4b52384cfa0ebe582d222004273762de674bda",
                "deletions": 0,
                "filename": "server/src/test/java/io/druid/server/QueryResourceTest.java",
                "patch": "@@ -0,0 +1,153 @@\n+/*\n+ * Druid - a distributed column store.\n+ * Copyright (C) 2012, 2013, 2014  Metamarkets Group Inc.\n+ *\n+ * This program is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU General Public License\n+ * as published by the Free Software Foundation; either version 2\n+ * of the License, or (at your option) any later version.\n+ *\n+ * This program is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU General Public License\n+ * along with this program; if not, write to the Free Software\n+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n+ */\n+\n+package io.druid.server;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.metamx.common.guava.Sequence;\n+import com.metamx.common.guava.Sequences;\n+import com.metamx.emitter.service.ServiceEmitter;\n+import io.druid.jackson.DefaultObjectMapper;\n+import io.druid.query.Query;\n+import io.druid.query.QueryRunner;\n+import io.druid.query.QuerySegmentWalker;\n+import io.druid.query.SegmentDescriptor;\n+import io.druid.server.initialization.ServerConfig;\n+import io.druid.server.log.NoopRequestLogger;\n+import io.druid.server.metrics.NoopServiceEmitter;\n+import org.easymock.EasyMock;\n+import org.joda.time.Interval;\n+import org.joda.time.Period;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import javax.servlet.http.HttpServletRequest;\n+import javax.ws.rs.core.MediaType;\n+import javax.ws.rs.core.Response;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.Map;\n+\n+/**\n+ *\n+ */\n+public class QueryResourceTest\n+{\n+  private static final ObjectMapper jsonMapper = new DefaultObjectMapper();\n+  public static final ServerConfig serverConfig = new ServerConfig(){\n+    @Override\n+    public int getNumThreads(){\n+      return 1;\n+    }\n+    @Override\n+    public Period getMaxIdleTime(){\n+      return Period.seconds(1);\n+    }\n+  };\n+  private final HttpServletRequest testServletRequest = EasyMock.createMock(HttpServletRequest.class);\n+  public static final QuerySegmentWalker testSegmentWalker = new QuerySegmentWalker()\n+  {\n+    @Override\n+    public <T> QueryRunner<T> getQueryRunnerForIntervals(\n+        Query<T> query, Iterable<Interval> intervals\n+    )\n+    {\n+      return new QueryRunner<T>()\n+      {\n+        @Override\n+        public Sequence<T> run(\n+            Query<T> query, Map<String, Object> responseContext\n+        )\n+        {\n+          return Sequences.<T>empty();\n+        }\n+      };\n+    }\n+\n+    @Override\n+    public <T> QueryRunner<T> getQueryRunnerForSegments(\n+        Query<T> query, Iterable<SegmentDescriptor> specs\n+    )\n+    {\n+      return getQueryRunnerForIntervals(null, null);\n+    }\n+  };\n+\n+  private static final ServiceEmitter noopServiceEmitter = new NoopServiceEmitter();\n+  @BeforeClass\n+  public static void staticSetup(){\n+    com.metamx.emitter.EmittingLogger.registerEmitter(noopServiceEmitter);\n+  }\n+  @Before\n+  public void setup()\n+  {\n+    EasyMock.expect(testServletRequest.getContentType()).andReturn(MediaType.APPLICATION_JSON);\n+    EasyMock.expect(testServletRequest.getRemoteAddr()).andReturn(\"localhost\").anyTimes();\n+    EasyMock.replay(testServletRequest);\n+  }\n+  private static final String simpleTimeSeriesQuery = \"{\\n\"\n+                                                      + \"    \\\"queryType\\\": \\\"timeseries\\\",\\n\"\n+                                                      + \"    \\\"dataSource\\\": \\\"mmx_metrics\\\",\\n\"\n+                                                      + \"    \\\"granularity\\\": \\\"hour\\\",\\n\"\n+                                                      + \"    \\\"intervals\\\": [\\n\"\n+                                                      + \"      \\\"2014-12-17/2015-12-30\\\"\\n\"\n+                                                      + \"    ],\\n\"\n+                                                      + \"    \\\"aggregations\\\": [\\n\"\n+                                                      + \"      {\\n\"\n+                                                      + \"        \\\"type\\\": \\\"count\\\",\\n\"\n+                                                      + \"        \\\"name\\\": \\\"rows\\\"\\n\"\n+                                                      + \"      }\\n\"\n+                                                      + \"    ]\\n\"\n+                                                      + \"}\";\n+  @Test\n+  public void testGoodQuery() throws IOException\n+  {\n+    QueryResource queryResource = new QueryResource(\n+        serverConfig,\n+        jsonMapper,\n+        jsonMapper,\n+        testSegmentWalker,\n+        new NoopServiceEmitter(),\n+        new NoopRequestLogger(),\n+        new QueryManager()\n+        );\n+    Response respone = queryResource.doPost(new ByteArrayInputStream(simpleTimeSeriesQuery.getBytes(\"UTF-8\")), null /*pretty*/, testServletRequest);\n+    Assert.assertNotNull(respone);\n+  }\n+  @Test\n+  public void testBadQuery() throws IOException\n+  {\n+\n+    QueryResource queryResource = new QueryResource(\n+        serverConfig,\n+        jsonMapper,\n+        jsonMapper,\n+        testSegmentWalker,\n+        new NoopServiceEmitter(),\n+        new NoopRequestLogger(),\n+        new QueryManager()\n+    );\n+    Response respone = queryResource.doPost(new ByteArrayInputStream(\"Meka Leka Hi Meka Hiney Ho\".getBytes(\"UTF-8\")), null /*pretty*/, testServletRequest);\n+    Assert.assertNotNull(respone);\n+    Assert.assertEquals(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), respone.getStatus());\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/bf4b52384cfa0ebe582d222004273762de674bda/server/src/test/java/io/druid/server/QueryResourceTest.java",
                "sha": "a8b6d52335e4d1f0219a5e5d3bb5e62fdc31642b",
                "status": "added"
            }
        ],
        "message": "Merge pull request #988 from drcrallen/queryErrorFixer\n\nFix NPE in QueryResource on bad query",
        "parent": "https://github.com/apache/incubator-druid/commit/2c533547c34c83d0ada9afe82e5eb4836ac01b0d",
        "repo": "incubator-druid",
        "unit_tests": [
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_c2a42e0": {
        "bug_id": "incubator-druid_c2a42e0",
        "commit": "https://github.com/apache/incubator-druid/commit/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/CacheStrategy.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/CacheStrategy.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/CacheStrategy.java",
                "patch": "@@ -51,6 +51,16 @@\n    */\n   byte[] computeCacheKey(QueryType query);\n \n+  /**\n+   * Computes the result level cache key for the given query.\n+   * Some implementations may include query parameters that might not be used in {@code computeCacheKey} for same query\n+   *\n+   * @param query the query to be cached\n+   *\n+   * @return the result level cache key\n+   */\n+  byte[] computeResultLevelCacheKey(QueryType query);\n+\n   /**\n    * Returns the class type of what is used in the cache\n    *",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/CacheStrategy.java",
                "sha": "8b106a6f2b7ac6ff949f1e38b9d3edbf8c0c383b",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChest.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChest.java",
                "patch": "@@ -481,6 +481,28 @@ public boolean isCacheable(GroupByQuery query, boolean willMergeRunners)\n             .build();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(GroupByQuery query)\n+      {\n+        final CacheKeyBuilder builder = new CacheKeyBuilder(GROUPBY_QUERY)\n+            .appendByte(CACHE_STRATEGY_VERSION)\n+            .appendCacheable(query.getGranularity())\n+            .appendCacheable(query.getDimFilter())\n+            .appendCacheables(query.getAggregatorSpecs())\n+            .appendCacheables(query.getDimensions())\n+            .appendCacheable(query.getVirtualColumns())\n+            .appendCacheable(query.getHavingSpec())\n+            .appendCacheable(query.getLimitSpec())\n+            .appendCacheables(query.getPostAggregatorSpecs());\n+\n+        if (query.getSubtotalsSpec() != null && !query.getSubtotalsSpec().isEmpty()) {\n+          for (List<String> subTotalSpec : query.getSubtotalsSpec()) {\n+            builder.appendStrings(subTotalSpec);\n+          }\n+        }\n+        return builder.build();\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChest.java",
                "sha": "c94d427b1bd24918cb365ae3a1183d70d886bb3c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/AlwaysHavingSpec.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/AlwaysHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/AlwaysHavingSpec.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.query.groupby.having;\n \n import org.apache.druid.data.input.Row;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n \n /**\n  * A \"having\" spec that always evaluates to true\n@@ -31,4 +32,10 @@ public boolean eval(Row row)\n   {\n     return true;\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_ALWAYS).build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/AlwaysHavingSpec.java",
                "sha": "8450589f814aaa6d668291ec81d90ef6c0558875",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/AndHavingSpec.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/AndHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/AndHavingSpec.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.segment.column.ValueType;\n \n import java.util.List;\n@@ -110,4 +111,11 @@ public String toString()\n     sb.append('}');\n     return sb.toString();\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_AND)\n+        .appendCacheables(havingSpecs).build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/AndHavingSpec.java",
                "sha": "f035db3dbde28e43bdbc927a1611af0830c590f5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/DimFilterHavingSpec.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/DimFilterHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/DimFilterHavingSpec.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.query.filter.DimFilter;\n import org.apache.druid.segment.column.ValueType;\n import org.apache.druid.segment.transform.RowFunction;\n@@ -170,6 +171,15 @@ public RowFunction getRowFunction()\n     return new TransformSpec(filter, transforms).toTransformer(rowSignature);\n   }\n \n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_DIM_FILTER)\n+        .appendCacheable(dimFilter)\n+        .appendByte((byte) (isFinalize() ? 1 : 0))\n+        .build();\n+  }\n+\n   private static class RowAsInputRow implements InputRow\n   {\n     private final Row row;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/DimFilterHavingSpec.java",
                "sha": "cf916bf0255649a4cbb82c250403a568794cc167",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/DimensionSelectorHavingSpec.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/DimensionSelectorHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/DimensionSelectorHavingSpec.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.base.Strings;\n import org.apache.druid.data.input.Row;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.query.extraction.ExtractionFn;\n import org.apache.druid.query.extraction.IdentityExtractionFn;\n \n@@ -117,4 +118,14 @@ public String toString()\n            \", extractionFn=\" + extractionFn +\n            '}';\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_DIM_SELECTOR)\n+        .appendString(dimension)\n+        .appendString(value)\n+        .appendByteArray(extractionFn == null ? new byte[0] : extractionFn.getCacheKey())\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/DimensionSelectorHavingSpec.java",
                "sha": "4dfc6e63c60753873e37dd2106cf51f2fce1b1e3",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/EqualToHavingSpec.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/EqualToHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/EqualToHavingSpec.java",
                "patch": "@@ -22,7 +22,9 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.druid.data.input.Row;\n+import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n \n import java.util.Map;\n \n@@ -123,4 +125,13 @@ public String toString()\n     sb.append('}');\n     return sb.toString();\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_EQUAL)\n+        .appendString(aggregationName)\n+        .appendByteArray(StringUtils.toUtf8(String.valueOf(value)))\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/EqualToHavingSpec.java",
                "sha": "00c471b200b9244c12711df7c34b82d6616c319c",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/GreaterThanHavingSpec.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/GreaterThanHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/GreaterThanHavingSpec.java",
                "patch": "@@ -22,7 +22,9 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.druid.data.input.Row;\n+import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n \n import java.util.Map;\n \n@@ -119,4 +121,13 @@ public String toString()\n     sb.append('}');\n     return sb.toString();\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_GREATER_THAN)\n+        .appendString(aggregationName)\n+        .appendByteArray(StringUtils.toUtf8(String.valueOf(value)))\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/GreaterThanHavingSpec.java",
                "sha": "aa276a9466cad487497a31aa0337be5f5633a680",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 1,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpec.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.fasterxml.jackson.annotation.JsonSubTypes;\n import com.fasterxml.jackson.annotation.JsonTypeInfo;\n import org.apache.druid.data.input.Row;\n+import org.apache.druid.java.util.common.Cacheable;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.segment.column.ValueType;\n \n@@ -44,7 +45,7 @@\n     @JsonSubTypes.Type(name = \"always\", value = AlwaysHavingSpec.class),\n     @JsonSubTypes.Type(name = \"filter\", value = DimFilterHavingSpec.class)\n })\n-public interface HavingSpec\n+public interface HavingSpec extends Cacheable\n {\n   // Atoms for easy combination, but for now they are mostly useful\n   // for testing.",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpec.java",
                "sha": "e75641f541997595d2ef68a0618062166a313334",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpecUtil.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpecUtil.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpecUtil.java",
                "patch": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query.groupby.having;\n+\n+public class HavingSpecUtil\n+{\n+  static final byte CACHE_TYPE_ID_ALWAYS = 0x0;\n+  static final byte CACHE_TYPE_ID_AND = 0x1;\n+  static final byte CACHE_TYPE_ID_DIM_SELECTOR = 0x2;\n+  static final byte CACHE_TYPE_ID_DIM_FILTER = 0x3;\n+  static final byte CACHE_TYPE_ID_EQUAL = 0x4;\n+  static final byte CACHE_TYPE_ID_GREATER_THAN = 0x5;\n+  static final byte CACHE_TYPE_ID_LESS_THAN = 0x6;\n+  static final byte CACHE_TYPE_ID_NEVER = 0x7;\n+  static final byte CACHE_TYPE_ID_NOT = 0x8;\n+  static final byte CACHE_TYPE_ID_OR = 0x9;\n+  static final byte CACHE_TYPE_ID_COUNTING = 0xA;\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/HavingSpecUtil.java",
                "sha": "e1227e7f04375ec8f96bbe8728d7c8cd95eafc18",
                "status": "added"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/LessThanHavingSpec.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/LessThanHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/LessThanHavingSpec.java",
                "patch": "@@ -21,7 +21,9 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.druid.data.input.Row;\n+import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n \n import java.util.Map;\n \n@@ -117,4 +119,13 @@ public String toString()\n     sb.append('}');\n     return sb.toString();\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_LESS_THAN)\n+        .appendString(aggregationName)\n+        .appendByteArray(StringUtils.toUtf8(String.valueOf(value)))\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/LessThanHavingSpec.java",
                "sha": "3c937cfba9fd90a04e2c37632de5092be5be00fd",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/NeverHavingSpec.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/NeverHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/NeverHavingSpec.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.druid.query.groupby.having;\n \n import org.apache.druid.data.input.Row;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n \n /**\n  * A \"having\" spec that always evaluates to false\n@@ -31,4 +32,10 @@ public boolean eval(Row row)\n   {\n     return false;\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_NEVER).build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/NeverHavingSpec.java",
                "sha": "fa2c15d4bc4850b39612e27034faaa6b45341623",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/NotHavingSpec.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/NotHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/NotHavingSpec.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.segment.column.ValueType;\n \n import java.util.Map;\n@@ -98,4 +99,12 @@ public int hashCode()\n   {\n     return havingSpec != null ? havingSpec.hashCode() : 0;\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_NOT)\n+        .appendCacheable(havingSpec)\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/NotHavingSpec.java",
                "sha": "81d7a63eaf8771c4719f8d04d9f65a041f084b5d",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/OrHavingSpec.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/groupby/having/OrHavingSpec.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/groupby/having/OrHavingSpec.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.query.aggregation.AggregatorFactory;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.segment.column.ValueType;\n \n import java.util.List;\n@@ -110,4 +111,12 @@ public String toString()\n     sb.append('}');\n     return sb.toString();\n   }\n+\n+  @Override\n+  public byte[] getCacheKey()\n+  {\n+    return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_OR)\n+        .appendCacheables(havingSpecs)\n+        .build();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/groupby/having/OrHavingSpec.java",
                "sha": "e6483490d7e0949953e4c8eeebb608107570c2db",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "patch": "@@ -50,6 +50,7 @@\n import org.apache.druid.query.aggregation.AggregatorFactory;\n import org.apache.druid.query.aggregation.AggregatorFactoryNotMergeableException;\n import org.apache.druid.query.aggregation.MetricManipulationFn;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.apache.druid.query.metadata.metadata.ColumnAnalysis;\n import org.apache.druid.query.metadata.metadata.SegmentAnalysis;\n import org.apache.druid.query.metadata.metadata.SegmentMetadataQuery;\n@@ -73,6 +74,7 @@\n   {\n   };\n   private static final byte[] SEGMENT_METADATA_CACHE_PREFIX = new byte[]{0x4};\n+  private static final byte SEGMENT_METADATA_QUERY = 0x16;\n   private static final Function<SegmentAnalysis, SegmentAnalysis> MERGE_TRANSFORM_FN = new Function<SegmentAnalysis, SegmentAnalysis>()\n   {\n     @Override\n@@ -194,6 +196,16 @@ public boolean isCacheable(SegmentMetadataQuery query, boolean willMergeRunners)\n                          .array();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(SegmentMetadataQuery query)\n+      {\n+        // need to include query \"merge\" and \"lenientAggregatorMerge\" for result level cache key\n+        return new CacheKeyBuilder(SEGMENT_METADATA_QUERY).appendByteArray(computeCacheKey(query))\n+                                                          .appendBoolean(query.isMerge())\n+                                                          .appendBoolean(query.isLenientAggregatorMerge())\n+                                                          .build();\n+      }\n+\n       @Override\n       public TypeReference<SegmentAnalysis> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/metadata/SegmentMetadataQueryQueryToolChest.java",
                "sha": "2813000ab7607f8d5245f62489da80d8fce25595",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "patch": "@@ -201,6 +201,12 @@ public boolean isCacheable(SearchQuery query, boolean willMergeRunners)\n         return queryCacheKey.array();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(SearchQuery query)\n+      {\n+        return computeCacheKey(query);\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/search/SearchQueryQueryToolChest.java",
                "sha": "8f35b2541bd8f564fd02c8be81cc073abfef573a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/select/SelectQueryQueryToolChest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/select/SelectQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/select/SelectQueryQueryToolChest.java",
                "patch": "@@ -236,6 +236,12 @@ public boolean isCacheable(SelectQuery query, boolean willMergeRunners)\n         return queryCacheKey.array();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(SelectQuery query)\n+      {\n+        return computeCacheKey(query);\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/select/SelectQueryQueryToolChest.java",
                "sha": "45ebb95a52dd1b35d16e8984b21043e2f322dd26",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java",
                "patch": "@@ -154,6 +154,12 @@ public boolean isCacheable(TimeBoundaryQuery query, boolean willMergeRunners)\n                          .array();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(TimeBoundaryQuery query)\n+      {\n+        return computeCacheKey(query);\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java",
                "sha": "9fc0e88e7e5b4a8388971fc0b23a5c61155fec1f",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "patch": "@@ -274,6 +274,22 @@ public boolean isCacheable(TimeseriesQuery query, boolean willMergeRunners)\n             .build();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(TimeseriesQuery query)\n+      {\n+        final CacheKeyBuilder builder = new CacheKeyBuilder(TIMESERIES_QUERY)\n+            .appendBoolean(query.isDescending())\n+            .appendBoolean(query.isSkipEmptyBuckets())\n+            .appendCacheable(query.getGranularity())\n+            .appendCacheable(query.getDimensionsFilter())\n+            .appendCacheables(query.getAggregatorSpecs())\n+            .appendCacheable(query.getVirtualColumns())\n+            .appendCacheables(query.getPostAggregatorSpecs())\n+            .appendInt(query.getLimit())\n+            .appendBoolean(query.isGrandTotal());\n+        return builder.build();\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java",
                "sha": "f8f5aa0c4c7ee335b55a9f207ef0b66b1bd69d00",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/topn/TopNQueryQueryToolChest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/org/apache/druid/query/topn/TopNQueryQueryToolChest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/main/java/org/apache/druid/query/topn/TopNQueryQueryToolChest.java",
                "patch": "@@ -326,6 +326,21 @@ public boolean isCacheable(TopNQuery query, boolean willMergeRunners)\n         return builder.build();\n       }\n \n+      @Override\n+      public byte[] computeResultLevelCacheKey(TopNQuery query)\n+      {\n+        final CacheKeyBuilder builder = new CacheKeyBuilder(TOPN_QUERY)\n+            .appendCacheable(query.getDimensionSpec())\n+            .appendCacheable(query.getTopNMetricSpec())\n+            .appendInt(query.getThreshold())\n+            .appendCacheable(query.getGranularity())\n+            .appendCacheable(query.getDimensionsFilter())\n+            .appendCacheables(query.getAggregatorSpecs())\n+            .appendCacheable(query.getVirtualColumns())\n+            .appendCacheables(query.getPostAggregatorSpecs());\n+        return builder.build();\n+      }\n+\n       @Override\n       public TypeReference<Object> getCacheObjectClazz()\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/main/java/org/apache/druid/query/topn/TopNQueryQueryToolChest.java",
                "sha": "2c3bd2b2f758491837783261291caeb1f08d34c0",
                "status": "modified"
            },
            {
                "additions": 486,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChestTest.java",
                "changes": 486,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChestTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChestTest.java",
                "patch": "@@ -0,0 +1,486 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query.groupby;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import org.apache.druid.data.input.Row;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.query.CacheStrategy;\n+import org.apache.druid.query.QueryRunnerTestHelper;\n+import org.apache.druid.query.aggregation.DoubleSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.FloatSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.LongSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.post.ExpressionPostAggregator;\n+import org.apache.druid.query.dimension.DefaultDimensionSpec;\n+import org.apache.druid.query.expression.TestExprMacroTable;\n+import org.apache.druid.query.filter.AndDimFilter;\n+import org.apache.druid.query.filter.BoundDimFilter;\n+import org.apache.druid.query.filter.OrDimFilter;\n+import org.apache.druid.query.filter.SelectorDimFilter;\n+import org.apache.druid.query.groupby.having.AndHavingSpec;\n+import org.apache.druid.query.groupby.having.DimFilterHavingSpec;\n+import org.apache.druid.query.groupby.having.EqualToHavingSpec;\n+import org.apache.druid.query.groupby.having.GreaterThanHavingSpec;\n+import org.apache.druid.query.groupby.having.HavingSpec;\n+import org.apache.druid.query.groupby.having.LessThanHavingSpec;\n+import org.apache.druid.query.groupby.having.NotHavingSpec;\n+import org.apache.druid.query.groupby.having.OrHavingSpec;\n+import org.apache.druid.query.groupby.orderby.DefaultLimitSpec;\n+import org.apache.druid.query.groupby.orderby.OrderByColumnSpec;\n+import org.apache.druid.query.ordering.StringComparators;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GroupByQueryQueryToolChestTest\n+{\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithPostAggregate()\n+  {\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias - 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithLimitSpec()\n+  {\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias - 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithHavingSpec()\n+  {\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(new GreaterThanHavingSpec(QueryRunnerTestHelper.uniqueMetric, 8))\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(new GreaterThanHavingSpec(QueryRunnerTestHelper.uniqueMetric, 10))\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithAndHavingSpec()\n+  {\n+    final List<HavingSpec> havings = Arrays.asList(\n+        new GreaterThanHavingSpec(\"agg\", Double.valueOf(1.3)),\n+        new OrHavingSpec(\n+            Arrays.asList(\n+                new LessThanHavingSpec(\"lessAgg\", Long.valueOf(1L)),\n+                new NotHavingSpec(new EqualToHavingSpec(\"equalAgg\", Double.valueOf(2)))\n+            )\n+        )\n+    );\n+    final HavingSpec andHavingSpec = new AndHavingSpec(havings);\n+\n+    final List<HavingSpec> havings2 = Arrays.asList(\n+        new GreaterThanHavingSpec(\"agg\", Double.valueOf(13.0)),\n+        new OrHavingSpec(\n+            Arrays.asList(\n+                new LessThanHavingSpec(\"lessAgg\", Long.valueOf(1L)),\n+                new NotHavingSpec(new EqualToHavingSpec(\"equalAgg\", Double.valueOf(22)))\n+            )\n+        )\n+    );\n+    final HavingSpec andHavingSpec2 = new AndHavingSpec(havings2);\n+\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(andHavingSpec)\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(andHavingSpec2)\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithHavingDimFilterHavingSpec()\n+  {\n+    final DimFilterHavingSpec havingSpec1 = new DimFilterHavingSpec(\n+        new AndDimFilter(\n+            ImmutableList.of(\n+                new OrDimFilter(\n+                    ImmutableList.of(\n+                        new BoundDimFilter(\"rows\", \"2\", null, true, false, null, null, StringComparators.NUMERIC),\n+                        new SelectorDimFilter(\"idx\", \"217\", null)\n+                    )\n+                ),\n+                new SelectorDimFilter(\"__time\", String.valueOf(DateTimes.of(\"2011-04-01\").getMillis()), null)\n+            )\n+        ),\n+        null\n+    );\n+\n+    final DimFilterHavingSpec havingSpec2 = new DimFilterHavingSpec(\n+        new AndDimFilter(\n+            ImmutableList.of(\n+                new OrDimFilter(\n+                    ImmutableList.of(\n+                        new BoundDimFilter(\"rows\", \"2\", null, true, false, null, null, StringComparators.NUMERIC),\n+                        new SelectorDimFilter(\"idx\", \"317\", null)\n+                    )\n+                ),\n+                new SelectorDimFilter(\"__time\", String.valueOf(DateTimes.of(\"2011-04-01\").getMillis()), null)\n+            )\n+        ),\n+        null\n+    );\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(havingSpec1)\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(new DefaultDimensionSpec(\"quality\", \"alias\"))\n+        .setAggregatorSpecs(QueryRunnerTestHelper.rowsCount, new LongSumAggregatorFactory(\"idx\", \"index\"))\n+        .setPostAggregatorSpecs(\n+            ImmutableList.of(\n+                new ExpressionPostAggregator(\"post\", \"alias + 'x'\", null, TestExprMacroTable.INSTANCE)\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setLimitSpec(\n+            new DefaultLimitSpec(\n+                ImmutableList.of(\n+                    new OrderByColumnSpec(\"post\", OrderByColumnSpec.Direction.DESCENDING)\n+                ),\n+                Integer.MAX_VALUE\n+            )\n+        )\n+        .setHavingSpec(havingSpec2)\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithSubTotalsSpec()\n+  {\n+    final GroupByQuery query1 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(Lists.newArrayList(\n+            new DefaultDimensionSpec(\"quality\", \"alias\"),\n+            new DefaultDimensionSpec(\"market\", \"market\")\n+        ))\n+        .setAggregatorSpecs(\n+            Arrays.asList(\n+                QueryRunnerTestHelper.rowsCount,\n+                new LongSumAggregatorFactory(\"idx\", \"index\"),\n+                new FloatSumAggregatorFactory(\"idxFloat\", \"indexFloat\"),\n+                new DoubleSumAggregatorFactory(\"idxDouble\", \"index\")\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setSubtotalsSpec(ImmutableList.of(\n+            ImmutableList.of(\"alias\"),\n+            ImmutableList.of(\"market\"),\n+            ImmutableList.of()\n+        ))\n+        .build();\n+\n+    final GroupByQuery query2 = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setDimensions(Lists.newArrayList(\n+            new DefaultDimensionSpec(\"quality\", \"alias\"),\n+            new DefaultDimensionSpec(\"market\", \"market\")\n+        ))\n+        .setAggregatorSpecs(\n+            Arrays.asList(\n+                QueryRunnerTestHelper.rowsCount,\n+                new LongSumAggregatorFactory(\"idx\", \"index\"),\n+                new FloatSumAggregatorFactory(\"idxFloat\", \"indexFloat\"),\n+                new DoubleSumAggregatorFactory(\"idxDouble\", \"index\")\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .setSubtotalsSpec(ImmutableList.of(\n+            ImmutableList.of(\"alias\"),\n+            ImmutableList.of()\n+        ))\n+        .build();\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy1 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Row, Object, GroupByQuery> strategy2 = new GroupByQueryQueryToolChest(\n+        null,\n+        QueryRunnerTestHelper.sameThreadIntervalChunkingQueryRunnerDecorator()\n+    ).getCacheStrategy(query2);\n+\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(\n+        strategy1.computeResultLevelCacheKey(query1),\n+        strategy2.computeResultLevelCacheKey(query2)\n+    ));\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryQueryToolChestTest.java",
                "sha": "2bad8f82f7f473ddce9ef846f3683f7d7132e84f",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java",
                "patch": "@@ -5368,6 +5368,12 @@ public void testSubqueryWithPostAggregatorsAndHaving()\n         .setHavingSpec(\n             new BaseHavingSpec()\n             {\n+              @Override\n+              public byte[] getCacheKey()\n+              {\n+                return new byte[0];\n+              }\n+\n               @Override\n               public boolean eval(Row row)\n               {\n@@ -5629,6 +5635,12 @@ public void testSubqueryWithMultiColumnAggregators()\n         .setHavingSpec(\n             new BaseHavingSpec()\n             {\n+              @Override\n+              public byte[] getCacheKey()\n+              {\n+                return new byte[0];\n+              }\n+\n               @Override\n               public boolean eval(Row row)\n               {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/GroupByQueryRunnerTest.java",
                "sha": "cc25be424a087539ad110800afae304e41bfcccb",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/having/HavingSpecTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/groupby/having/HavingSpecTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/groupby/having/HavingSpecTest.java",
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.druid.data.input.MapBasedInputRow;\n import org.apache.druid.data.input.Row;\n import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.query.cache.CacheKeyBuilder;\n import org.junit.Test;\n \n import java.util.ArrayList;\n@@ -233,6 +235,15 @@ public boolean eval(Row row)\n       counter.incrementAndGet();\n       return value;\n     }\n+\n+    @Override\n+    public byte[] getCacheKey()\n+    {\n+      return new CacheKeyBuilder(HavingSpecUtil.CACHE_TYPE_ID_COUNTING)\n+          .appendByte((byte) (value ? 1 : 0))\n+          .appendByteArray(StringUtils.toUtf8(String.valueOf(counter)))\n+          .build();\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/groupby/having/HavingSpecTest.java",
                "sha": "a0e1d7457117b96caea3c2a1969528701097624f",
                "status": "modified"
            },
            {
                "additions": 162,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChestTest.java",
                "changes": 162,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChestTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChestTest.java",
                "patch": "@@ -32,7 +32,9 @@\n import org.apache.druid.query.TableDataSource;\n import org.apache.druid.query.aggregation.CountAggregatorFactory;\n import org.apache.druid.query.aggregation.LongSumAggregatorFactory;\n+import org.apache.druid.query.aggregation.post.ArithmeticPostAggregator;\n import org.apache.druid.query.aggregation.post.ConstantPostAggregator;\n+import org.apache.druid.query.aggregation.post.FieldAccessPostAggregator;\n import org.apache.druid.query.spec.MultipleIntervalSegmentSpec;\n import org.apache.druid.segment.TestHelper;\n import org.apache.druid.segment.VirtualColumns;\n@@ -158,4 +160,164 @@ public void testCacheKey()\n         )\n     );\n   }\n+\n+  @Test\n+  public void testResultLevelCacheKey()\n+  {\n+    final TimeseriesQuery query1 = Druids.newTimeseriesQueryBuilder()\n+                                         .dataSource(\"dummy\")\n+                                         .intervals(\"2015-01-01/2015-01-02\")\n+                                         .descending(descending)\n+                                         .granularity(Granularities.ALL)\n+                                         .aggregators(\n+                                             ImmutableList.of(\n+                                                 new LongSumAggregatorFactory(\"metric0\", \"metric0\"),\n+                                                 new CountAggregatorFactory(\"metric1\")\n+                                             )\n+                                         )\n+                                         .postAggregators(\n+                                             ImmutableList.of(\n+                                                 new ArithmeticPostAggregator(\n+                                                     \"post\",\n+                                                     \"+\",\n+                                                     ImmutableList.of(\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric1\"\n+                                                         ),\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric0\"\n+                                                         )\n+                                                     )\n+                                                 )\n+                                             )\n+                                         )\n+                                         .build();\n+\n+    final TimeseriesQuery query2 = Druids.newTimeseriesQueryBuilder()\n+                                         .dataSource(\"dummy\")\n+                                         .intervals(\"2015-01-01/2015-01-02\")\n+                                         .descending(descending)\n+                                         .granularity(Granularities.ALL)\n+                                         .aggregators(\n+                                             ImmutableList.of(\n+                                                 new LongSumAggregatorFactory(\"metric0\", \"metric0\"),\n+                                                 new CountAggregatorFactory(\"metric1\")\n+                                             )\n+                                         )\n+                                         .postAggregators(\n+                                             ImmutableList.of(\n+                                                 new ArithmeticPostAggregator(\n+                                                     \"post\",\n+                                                     \"/\",\n+                                                     ImmutableList.of(\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric1\"\n+                                                         ),\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric0\"\n+                                                         )\n+                                                     )\n+                                                 )\n+                                             )\n+                                         )\n+                                         .build();\n+\n+    Assert.assertTrue(\n+        Arrays.equals(\n+            TOOL_CHEST.getCacheStrategy(query1).computeCacheKey(query1),\n+            TOOL_CHEST.getCacheStrategy(query2).computeCacheKey(query2)\n+        )\n+    );\n+    Assert.assertFalse(\n+        Arrays.equals(\n+            TOOL_CHEST.getCacheStrategy(query1).computeResultLevelCacheKey(query1),\n+            TOOL_CHEST.getCacheStrategy(query2).computeResultLevelCacheKey(query2)\n+        )\n+    );\n+  }\n+\n+  @Test\n+  public void testResultLevelCacheKeyWithGrandTotal()\n+  {\n+    final TimeseriesQuery query1 = Druids.newTimeseriesQueryBuilder()\n+                                         .dataSource(\"dummy\")\n+                                         .intervals(\"2015-01-01/2015-01-02\")\n+                                         .descending(descending)\n+                                         .granularity(Granularities.ALL)\n+                                         .aggregators(\n+                                             ImmutableList.of(\n+                                                 new LongSumAggregatorFactory(\"metric0\", \"metric0\"),\n+                                                 new CountAggregatorFactory(\"metric1\")\n+                                             )\n+                                         )\n+                                         .postAggregators(\n+                                             ImmutableList.of(\n+                                                 new ArithmeticPostAggregator(\n+                                                     \"post\",\n+                                                     \"+\",\n+                                                     ImmutableList.of(\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric1\"\n+                                                         ),\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric0\"\n+                                                         )\n+                                                     )\n+                                                 )\n+                                             )\n+                                         )\n+                                         .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, true))\n+                                         .build();\n+\n+    final TimeseriesQuery query2 = Druids.newTimeseriesQueryBuilder()\n+                                         .dataSource(\"dummy\")\n+                                         .intervals(\"2015-01-01/2015-01-02\")\n+                                         .descending(descending)\n+                                         .granularity(Granularities.ALL)\n+                                         .aggregators(\n+                                             ImmutableList.of(\n+                                                 new LongSumAggregatorFactory(\"metric0\", \"metric0\"),\n+                                                 new CountAggregatorFactory(\"metric1\")\n+                                             )\n+                                         )\n+                                         .postAggregators(\n+                                             ImmutableList.of(\n+                                                 new ArithmeticPostAggregator(\n+                                                     \"post\",\n+                                                     \"/\",\n+                                                     ImmutableList.of(\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric1\"\n+                                                         ),\n+                                                         new FieldAccessPostAggregator(\n+                                                             null,\n+                                                             \"metric0\"\n+                                                         )\n+                                                     )\n+                                                 )\n+                                             )\n+                                         )\n+                                         .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, true))\n+                                         .build();\n+\n+    Assert.assertTrue(\n+        Arrays.equals(\n+            TOOL_CHEST.getCacheStrategy(query1).computeCacheKey(query1),\n+            TOOL_CHEST.getCacheStrategy(query2).computeCacheKey(query2)\n+        )\n+    );\n+    Assert.assertFalse(\n+        Arrays.equals(\n+            TOOL_CHEST.getCacheStrategy(query1).computeResultLevelCacheKey(query1),\n+            TOOL_CHEST.getCacheStrategy(query2).computeResultLevelCacheKey(query2)\n+        )\n+    );\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChestTest.java",
                "sha": "6d07c59d269e876069343d511fe7a6c42df0e7a0",
                "status": "modified"
            },
            {
                "additions": 87,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/topn/TopNQueryQueryToolChestTest.java",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/org/apache/druid/query/topn/TopNQueryQueryToolChestTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "processing/src/test/java/org/apache/druid/query/topn/TopNQueryQueryToolChestTest.java",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.druid.query.TableDataSource;\n import org.apache.druid.query.TestQueryRunners;\n import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.aggregation.LongSumAggregatorFactory;\n import org.apache.druid.query.aggregation.post.ArithmeticPostAggregator;\n import org.apache.druid.query.aggregation.post.ConstantPostAggregator;\n import org.apache.druid.query.aggregation.post.FieldAccessPostAggregator;\n@@ -128,6 +129,92 @@ public void testComputeCacheKeyWithDifferentPostAgg()\n     ).getCacheStrategy(query2);\n \n     Assert.assertFalse(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(strategy1.computeResultLevelCacheKey(query1),\n+                                     strategy2.computeResultLevelCacheKey(query2)));\n+  }\n+\n+  @Test\n+  public void testComputeResultLevelCacheKeyWithDifferentPostAgg() throws IOException\n+  {\n+    final TopNQuery query1 = new TopNQuery(\n+        new TableDataSource(\"dummy\"),\n+        VirtualColumns.EMPTY,\n+        new DefaultDimensionSpec(\"test\", \"test\"),\n+        new LegacyTopNMetricSpec(\"metric1\"),\n+        3,\n+        new MultipleIntervalSegmentSpec(ImmutableList.of(Intervals.of(\"2015-01-01T18:00:00/2015-01-02T18:00:00\"))),\n+        null,\n+        Granularities.ALL,\n+        ImmutableList.of(\n+            new LongSumAggregatorFactory(\"metric1\", \"metric1\"),\n+            new LongSumAggregatorFactory(\"metric2\", \"metric2\")\n+        ),\n+        ImmutableList.of(\n+            new ArithmeticPostAggregator(\n+                \"post1\",\n+                \"/\",\n+                ImmutableList.of(\n+                    new FieldAccessPostAggregator(\n+                        \"metric1\",\n+                        \"metric1\"\n+                    ),\n+                    new FieldAccessPostAggregator(\n+                        \"metric2\",\n+                        \"metric2\"\n+                    )\n+                )\n+            )\n+        ),\n+        null\n+    );\n+\n+    final TopNQuery query2 = new TopNQuery(\n+        new TableDataSource(\"dummy\"),\n+        VirtualColumns.EMPTY,\n+        new DefaultDimensionSpec(\"test\", \"test\"),\n+        new LegacyTopNMetricSpec(\"metric1\"),\n+        3,\n+        new MultipleIntervalSegmentSpec(ImmutableList.of(Intervals.of(\"2015-01-01T18:00:00/2015-01-02T18:00:00\"))),\n+        null,\n+        Granularities.ALL,\n+        ImmutableList.of(\n+            new LongSumAggregatorFactory(\"metric1\", \"metric1\"),\n+            new LongSumAggregatorFactory(\"metric2\", \"metric2\")\n+        ),\n+        ImmutableList.of(\n+            new ArithmeticPostAggregator(\n+                \"post2\",\n+                \"+\",\n+                ImmutableList.of(\n+                    new FieldAccessPostAggregator(\n+                        \"metric1\",\n+                        \"metric1\"\n+                    ),\n+                    new FieldAccessPostAggregator(\n+                        \"metric2\",\n+                        \"metric2\"\n+                    )\n+                )\n+            )\n+        ),\n+        null\n+    );\n+\n+    final CacheStrategy<Result<TopNResultValue>, Object, TopNQuery> strategy1 = new TopNQueryQueryToolChest(\n+        null,\n+        null\n+    ).getCacheStrategy(query1);\n+\n+    final CacheStrategy<Result<TopNResultValue>, Object, TopNQuery> strategy2 = new TopNQueryQueryToolChest(\n+        null,\n+        null\n+    ).getCacheStrategy(query2);\n+\n+    //segment level cache key excludes postaggregates in topn\n+    Assert.assertTrue(Arrays.equals(strategy1.computeCacheKey(query1), strategy2.computeCacheKey(query2)));\n+    Assert.assertFalse(Arrays.equals(strategy1.computeCacheKey(query1), strategy1.computeResultLevelCacheKey(query1)));\n+    Assert.assertFalse(Arrays.equals(strategy1.computeResultLevelCacheKey(query1),\n+                                     strategy2.computeResultLevelCacheKey(query2)));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/processing/src/test/java/org/apache/druid/query/topn/TopNQueryQueryToolChestTest.java",
                "sha": "cede671e6a475ece53213f21ce124c3ebab1032a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java",
                "patch": "@@ -385,6 +385,10 @@ private String computeCurrentEtag(final Set<ServerToSegment> segments, @Nullable\n           break;\n         }\n         hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);\n+        // it is important to add the \"query interval\" as part ETag calculation\n+        // to have result level cache work correctly for queries with different\n+        // intervals covering the same set of segments\n+        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);\n       }\n \n       if (hasOnlyHistoricalSegments) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java",
                "sha": "1facccdf0fea9184ec1ab1dd5e5dca59af6800af",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 1,
                "filename": "server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java",
                "patch": "@@ -80,7 +80,7 @@ public ResultLevelCachingQueryRunner(\n   {\n     if (useResultCache || populateResultCache) {\n \n-      final String cacheKeyStr = StringUtils.fromUtf8(strategy.computeCacheKey(query));\n+      final String cacheKeyStr = StringUtils.fromUtf8(strategy.computeResultLevelCacheKey(query));\n       final byte[] cachedResultSet = fetchResultsFromResultLevelCache(cacheKeyStr);\n       String existingResultSetId = extractEtagFromResults(cachedResultSet);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java",
                "sha": "6a303b8a6df74180f1662e4be7daa042e5639e49",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java?ref=c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a",
                "deletions": 1,
                "filename": "server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "patch": "@@ -3129,7 +3129,56 @@ public void testIfNoneMatch()\n     Map<String, Object> responseContext = new HashMap<>();\n \n     getDefaultQueryRunner().run(QueryPlus.wrap(query), responseContext);\n-    Assert.assertEquals(\"Z/eS4rQz5v477iq7Aashr6JPZa0=\", responseContext.get(\"ETag\"));\n+    Assert.assertEquals(\"MDs2yIUvYLVzaG6zmwTH1plqaYE=\", responseContext.get(\"ETag\"));\n+  }\n+\n+  @Test\n+  public void testEtagforDifferentQueryInterval()\n+  {\n+    final Interval interval = Intervals.of(\"2016-01-01/2016-01-02\");\n+    final Interval queryInterval = Intervals.of(\"2016-01-01T14:00:00/2016-01-02T14:00:00\");\n+    final Interval queryInterval2 = Intervals.of(\"2016-01-01T18:00:00/2016-01-02T18:00:00\");\n+    final DataSegment dataSegment = new DataSegment(\n+        \"dataSource\",\n+        interval,\n+        \"ver\",\n+        ImmutableMap.of(\n+            \"type\", \"hdfs\",\n+            \"path\", \"/tmp\"\n+        ),\n+        ImmutableList.of(\"product\"),\n+        ImmutableList.of(\"visited_sum\"),\n+        NoneShardSpec.instance(),\n+        9,\n+        12334\n+    );\n+    final ServerSelector selector = new ServerSelector(\n+        dataSegment,\n+        new HighestPriorityTierSelectorStrategy(new RandomServerSelectorStrategy())\n+    );\n+    selector.addServerAndUpdateSegment(new QueryableDruidServer(servers[0], null), dataSegment);\n+    timeline.add(interval, \"ver\", new SingleElementPartitionChunk<>(selector));\n+\n+    final TimeBoundaryQuery query = Druids.newTimeBoundaryQueryBuilder()\n+                                    .dataSource(DATA_SOURCE)\n+                                    .intervals(new MultipleIntervalSegmentSpec(ImmutableList.of(queryInterval)))\n+                                    .context(ImmutableMap.of(\"If-None-Match\", \"aVJV29CJY93rszVW/QBy0arWZo0=\"))\n+                                    .build();\n+\n+    final TimeBoundaryQuery query2 = Druids.newTimeBoundaryQueryBuilder()\n+                                     .dataSource(DATA_SOURCE)\n+                                     .intervals(new MultipleIntervalSegmentSpec(ImmutableList.of(queryInterval2)))\n+                                     .context(ImmutableMap.of(\"If-None-Match\", \"aVJV29CJY93rszVW/QBy0arWZo0=\"))\n+                                     .build();\n+\n+\n+    final Map<String, Object> responseContext = new HashMap<>();\n+\n+    getDefaultQueryRunner().run(QueryPlus.wrap(query), responseContext);\n+    final Object etag1 = responseContext.get(\"ETag\");\n+    getDefaultQueryRunner().run(QueryPlus.wrap(query2), responseContext);\n+    final Object etag2 = responseContext.get(\"ETag\");\n+    Assert.assertNotEquals(etag1, etag2);\n   }\n \n   @SuppressWarnings(\"unchecked\")",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c2a42e05bb08aefa31f1d1ed09d568cdaa726b8a/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java",
                "sha": "ca3a309e687e2aa34be12fba038d11594edea2c9",
                "status": "modified"
            }
        ],
        "message": "Fix result-level cache for queries (#7325)\n\n* Add SegmentDescriptor interval in the hash while calculating Etag\r\n\r\n* Add computeResultLevelCacheKey to CacheStrategy\r\n\r\nMake HavingSpec cacheable and implement getCacheKey for subclasses\r\nAdd unit tests for computeResultLevelCacheKey\r\n\r\n* Add more tests\r\n\r\n* Use CacheKeyBuilder for HavingSpec's getCacheKey\r\n\r\n* Initialize aggregators map to avoid NPE\r\n\r\n* adjust cachekey builder for HavingSpec to ignore aggregators\r\n\r\n* unused import\r\n\r\n* PR comments",
        "parent": "https://github.com/apache/incubator-druid/commit/be65cca248d7b3dd5ebacc2a537aa3d25d420475",
        "repo": "incubator-druid",
        "unit_tests": [
            "GroupByQueryQueryToolChestTest.java",
            "DimFilterHavingSpecTest.java",
            "DimensionSelectorHavingSpecTest.java",
            "HavingSpecTest.java",
            "SegmentMetadataQueryQueryToolChestTest.java",
            "SearchQueryQueryToolChestTest.java",
            "TimeBoundaryQueryQueryToolChestTest.java",
            "TimeseriesQueryQueryToolChestTest.java",
            "TopNQueryQueryToolChestTest.java",
            "CachingClusteredClientTest.java"
        ]
    },
    "incubator-druid_c44cdba": {
        "bug_id": "incubator-druid_c44cdba",
        "commit": "https://github.com/apache/incubator-druid/commit/c44cdba65e5b62ebfcc54610a86ed4699281ee79",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c44cdba65e5b62ebfcc54610a86ed4699281ee79/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java?ref=c44cdba65e5b62ebfcc54610a86ed4699281ee79",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "patch": "@@ -90,7 +90,19 @@ public TopNBinaryFn(\n     TopNResultValue arg2Vals = arg2.getValue();\n \n     for (DimensionAndMetricValueExtractor arg1Val : arg1Vals) {\n-      retVals.put(arg1Val.getStringDimensionValue(dimension), arg1Val);\n+      final String dimensionValue = arg1Val.getStringDimensionValue(dimension);\n+      Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n+      retVal.put(dimension, dimensionValue);\n+\n+      for (AggregatorFactory factory : aggregations) {\n+        final String metricName = factory.getName();\n+        retVal.put(metricName, arg1Val.getMetric(metricName));\n+      }\n+      for (PostAggregator postAgg : postAggregations) {\n+        retVal.put(postAgg.getName(), postAgg.compute(retVal));\n+      }\n+\n+      retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n     }\n     for (DimensionAndMetricValueExtractor arg2Val : arg2Vals) {\n       final String dimensionValue = arg2Val.getStringDimensionValue(dimension);\n@@ -112,7 +124,18 @@ public TopNBinaryFn(\n \n         retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n       } else {\n-        retVals.put(dimensionValue, arg2Val);\n+        Map<String, Object> retVal = new LinkedHashMap<String, Object>(aggregations.size() + 2);\n+        retVal.put(dimension, dimensionValue);\n+\n+        for (AggregatorFactory factory : aggregations) {\n+          final String metricName = factory.getName();\n+          retVal.put(metricName, arg2Val.getMetric(metricName));\n+        }\n+        for (PostAggregator postAgg : postAggregations) {\n+          retVal.put(postAgg.getName(), postAgg.compute(retVal));\n+        }\n+\n+        retVals.put(dimensionValue, new DimensionAndMetricValueExtractor(retVal));\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c44cdba65e5b62ebfcc54610a86ed4699281ee79/processing/src/main/java/io/druid/query/topn/TopNBinaryFn.java",
                "sha": "e2a928465178b67a8fe5add0954d5f18b8b594b7",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c44cdba65e5b62ebfcc54610a86ed4699281ee79/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java?ref=c44cdba65e5b62ebfcc54610a86ed4699281ee79",
                "deletions": 4,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "patch": "@@ -324,9 +324,9 @@ public void testMergeByPostAgg()\n                     \"testdim\", \"2\"\n                 ),\n                 ImmutableMap.<String, Object>of(\n-                    \"rows\", 0L,\n-                    \"index\", 1L,\n-                    \"testdim\", \"3\"\n+                    \"rows\", 4L,\n+                    \"index\", 5L,\n+                    \"testdim\", \"other\"\n                 )\n             )\n         )\n@@ -336,6 +336,12 @@ public void testMergeByPostAgg()\n         currTime,\n         new TopNResultValue(\n             ImmutableList.<Map<String, Object>>of(\n+                ImmutableMap.<String, Object>of(\n+                    \"testdim\", \"other\",\n+                    \"rows\", 4L,\n+                    \"index\", 5L,\n+                    \"addrowsindexconstant\", 10.0\n+                ),\n                 ImmutableMap.<String, Object>of(\n                     \"testdim\", \"1\",\n                     \"rows\", 3L,\n@@ -357,7 +363,7 @@ public void testMergeByPostAgg()\n         QueryGranularity.ALL,\n         new DefaultDimensionSpec(\"testdim\", null),\n         new NumericTopNMetricSpec(\"addrowsindexconstant\"),\n-        2,\n+        3,\n         aggregatorFactories,\n         postAggregators\n     ).apply(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c44cdba65e5b62ebfcc54610a86ed4699281ee79/processing/src/test/java/io/druid/query/topn/TopNBinaryFnTest.java",
                "sha": "1caf76156a1a1ae7115e0391f06c27cb5118377f",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #496 from metamx/fix-topN-postagg-npe\n\nfix npe in topNBinaryFn with post aggs",
        "parent": "https://github.com/apache/incubator-druid/commit/07aab2832533d1a17d143cc61e24b9097279927b",
        "repo": "incubator-druid",
        "unit_tests": [
            "TopNBinaryFnTest.java"
        ]
    },
    "incubator-druid_c48aa74": {
        "bug_id": "incubator-druid_c48aa74",
        "commit": "https://github.com/apache/incubator-druid/commit/c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 6,
                "filename": "extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "patch": "@@ -53,7 +53,6 @@\n import org.joda.time.Duration;\n import org.joda.time.Interval;\n \n-import javax.annotation.Nullable;\n import java.io.IOException;\n import java.util.List;\n import java.util.Map;\n@@ -240,11 +239,7 @@ public void reset(DataSourceMetadata dataSourceMetadata)\n   }\n \n   @Override\n-  public void checkpoint(\n-      @Nullable String sequenceName,\n-      @Nullable DataSourceMetadata previousCheckPoint,\n-      @Nullable DataSourceMetadata currentCheckPoint\n-  )\n+  public void checkpoint(int taskGroupId, DataSourceMetadata previousCheckPoint, DataSourceMetadata currentCheckPoint)\n   {\n     // do nothing\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-contrib/materialized-view-maintenance/src/main/java/io/druid/indexing/materializedview/MaterializedViewSupervisor.java",
                "sha": "fedda092c4d65304763f079b92200ff664fb1a46",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 3,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "patch": "@@ -600,12 +600,13 @@ public void onFailure(Throwable t)\n                 sequences\n             );\n             requestPause();\n-            if (!toolbox.getTaskActionClient().submit(new CheckPointDataSourceMetadataAction(\n+            final CheckPointDataSourceMetadataAction checkpointAction = new CheckPointDataSourceMetadataAction(\n                 task.getDataSource(),\n-                ioConfig.getBaseSequenceName(),\n+                ioConfig.getTaskGroupId(),\n                 new KafkaDataSourceMetadata(new KafkaPartitions(topic, sequenceToCheckpoint.getStartOffsets())),\n                 new KafkaDataSourceMetadata(new KafkaPartitions(topic, nextOffsets))\n-            ))) {\n+            );\n+            if (!toolbox.getTaskActionClient().submit(checkpointAction)) {\n               throw new ISE(\"Checkpoint request with offsets [%s] failed, dying\", nextOffsets);\n             }\n           }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/IncrementalPublishingKafkaIndexTaskRunner.java",
                "sha": "a93fde611c5cf9872e9e49ba60c2663c35e15802",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIOConfig.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIOConfig.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIOConfig.java",
                "patch": "@@ -26,13 +26,16 @@\n import io.druid.segment.indexing.IOConfig;\n import org.joda.time.DateTime;\n \n+import javax.annotation.Nullable;\n import java.util.Map;\n \n public class KafkaIOConfig implements IOConfig\n {\n   private static final boolean DEFAULT_USE_TRANSACTION = true;\n   private static final boolean DEFAULT_SKIP_OFFSET_GAPS = false;\n \n+  @Nullable\n+  private final Integer taskGroupId;\n   private final String baseSequenceName;\n   private final KafkaPartitions startPartitions;\n   private final KafkaPartitions endPartitions;\n@@ -44,6 +47,7 @@\n \n   @JsonCreator\n   public KafkaIOConfig(\n+      @JsonProperty(\"taskGroupId\") @Nullable Integer taskGroupId, // can be null for backward compabitility\n       @JsonProperty(\"baseSequenceName\") String baseSequenceName,\n       @JsonProperty(\"startPartitions\") KafkaPartitions startPartitions,\n       @JsonProperty(\"endPartitions\") KafkaPartitions endPartitions,\n@@ -54,6 +58,7 @@ public KafkaIOConfig(\n       @JsonProperty(\"skipOffsetGaps\") Boolean skipOffsetGaps\n   )\n   {\n+    this.taskGroupId = taskGroupId;\n     this.baseSequenceName = Preconditions.checkNotNull(baseSequenceName, \"baseSequenceName\");\n     this.startPartitions = Preconditions.checkNotNull(startPartitions, \"startPartitions\");\n     this.endPartitions = Preconditions.checkNotNull(endPartitions, \"endPartitions\");\n@@ -83,6 +88,13 @@ public KafkaIOConfig(\n     }\n   }\n \n+  @Nullable\n+  @JsonProperty\n+  public Integer getTaskGroupId()\n+  {\n+    return taskGroupId;\n+  }\n+\n   @JsonProperty\n   public String getBaseSequenceName()\n   {\n@@ -135,7 +147,8 @@ public boolean isSkipOffsetGaps()\n   public String toString()\n   {\n     return \"KafkaIOConfig{\" +\n-           \"baseSequenceName='\" + baseSequenceName + '\\'' +\n+           \"taskGroupId=\" + taskGroupId +\n+           \", baseSequenceName='\" + baseSequenceName + '\\'' +\n            \", startPartitions=\" + startPartitions +\n            \", endPartitions=\" + endPartitions +\n            \", consumerProperties=\" + consumerProperties +",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIOConfig.java",
                "sha": "b6c1d765c95852203e32bd39bb06e74359b49349",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 185,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 83,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -143,8 +143,7 @@\n    * time, there should only be up to a maximum of [taskCount] actively-reading task groups (tracked in the [taskGroups]\n    * map) + zero or more pending-completion task groups (tracked in [pendingCompletionTaskGroups]).\n    */\n-  @VisibleForTesting\n-  static class TaskGroup\n+  private static class TaskGroup\n   {\n     // This specifies the partitions and starting offsets for this task group. It is set on group creation from the data\n     // in [partitionGroups] and never changes during the lifetime of this task group, which will live until a task in\n@@ -159,7 +158,7 @@\n     DateTime completionTimeout; // is set after signalTasksToFinish(); if not done by timeout, take corrective action\n     final TreeMap<Integer, Map<Integer, Long>> sequenceOffsets = new TreeMap<>();\n \n-    public TaskGroup(\n+    TaskGroup(\n         ImmutableMap<Integer, Long> partitionOffsets,\n         Optional<DateTime> minimumMessageTime,\n         Optional<DateTime> maximumMessageTime\n@@ -171,7 +170,7 @@ public TaskGroup(\n       this.sequenceOffsets.put(0, partitionOffsets);\n     }\n \n-    public int addNewCheckpoint(Map<Integer, Long> checkpoint)\n+    int addNewCheckpoint(Map<Integer, Long> checkpoint)\n     {\n       sequenceOffsets.put(sequenceOffsets.lastKey() + 1, checkpoint);\n       return sequenceOffsets.lastKey();\n@@ -212,9 +211,6 @@ public int addNewCheckpoint(Map<Integer, Long> checkpoint)\n   private final ConcurrentHashMap<Integer, ConcurrentHashMap<Integer, Long>> partitionGroups = new ConcurrentHashMap<>();\n   // --------------------------------------------------------\n \n-  // BaseSequenceName -> TaskGroup\n-  private final ConcurrentHashMap<String, TaskGroup> sequenceTaskGroup = new ConcurrentHashMap<>();\n-\n   private final TaskStorage taskStorage;\n   private final TaskMaster taskMaster;\n   private final IndexerMetadataStorageCoordinator indexerMetadataStorageCoordinator;\n@@ -513,13 +509,9 @@ public void reset(DataSourceMetadata dataSourceMetadata)\n   }\n \n   @Override\n-  public void checkpoint(\n-      String sequenceName,\n-      DataSourceMetadata previousCheckpoint,\n-      DataSourceMetadata currentCheckpoint\n-  )\n+  public void checkpoint(int taskGroupId, DataSourceMetadata previousCheckpoint, DataSourceMetadata currentCheckpoint)\n   {\n-    Preconditions.checkNotNull(sequenceName, \"Cannot checkpoint without a sequence name\");\n+    Preconditions.checkNotNull(previousCheckpoint, \"previousCheckpoint\");\n     Preconditions.checkNotNull(currentCheckpoint, \"current checkpoint cannot be null\");\n     Preconditions.checkArgument(\n         ioConfig.getTopic()\n@@ -530,12 +522,14 @@ public void checkpoint(\n         ((KafkaDataSourceMetadata) currentCheckpoint).getKafkaPartitions().getTopic()\n     );\n \n-    log.info(\"Checkpointing [%s] for sequence [%s]\", currentCheckpoint, sequenceName);\n-    notices.add(new CheckpointNotice(\n-        sequenceName,\n-        (KafkaDataSourceMetadata) previousCheckpoint,\n-        (KafkaDataSourceMetadata) currentCheckpoint\n-    ));\n+    log.info(\"Checkpointing [%s] for taskGroup [%s]\", currentCheckpoint, taskGroupId);\n+    notices.add(\n+        new CheckpointNotice(\n+            taskGroupId,\n+            (KafkaDataSourceMetadata) previousCheckpoint,\n+            (KafkaDataSourceMetadata) currentCheckpoint\n+        )\n+    );\n   }\n \n   public void possiblyRegisterListener()\n@@ -637,17 +631,17 @@ public void handle()\n \n   private class CheckpointNotice implements Notice\n   {\n-    final String sequenceName;\n+    final int taskGroupId;\n     final KafkaDataSourceMetadata previousCheckpoint;\n     final KafkaDataSourceMetadata currentCheckpoint;\n \n     CheckpointNotice(\n-        String sequenceName,\n+        int taskGroupId,\n         KafkaDataSourceMetadata previousCheckpoint,\n         KafkaDataSourceMetadata currentCheckpoint\n     )\n     {\n-      this.sequenceName = sequenceName;\n+      this.taskGroupId = taskGroupId;\n       this.previousCheckpoint = previousCheckpoint;\n       this.currentCheckpoint = currentCheckpoint;\n     }\n@@ -658,17 +652,12 @@ public void handle() throws ExecutionException, InterruptedException\n       // check for consistency\n       // if already received request for this sequenceName and dataSourceMetadata combination then return\n \n-      Preconditions.checkNotNull(\n-          sequenceTaskGroup.get(sequenceName),\n-          \"WTH?! cannot find task group for this sequence [%s], sequencesTaskGroup map [%s], taskGroups [%s]\",\n-          sequenceName,\n-          sequenceTaskGroup,\n-          taskGroups\n-      );\n-      final TreeMap<Integer, Map<Integer, Long>> checkpoints = sequenceTaskGroup.get(sequenceName).sequenceOffsets;\n+      final TaskGroup taskGroup = taskGroups.get(taskGroupId);\n+\n+      if (isValidTaskGroup(taskGroup)) {\n+        final TreeMap<Integer, Map<Integer, Long>> checkpoints = taskGroup.sequenceOffsets;\n \n-      // check validity of previousCheckpoint if it is not null\n-      if (previousCheckpoint != null) {\n+        // check validity of previousCheckpoint\n         int index = checkpoints.size();\n         for (int sequenceId : checkpoints.descendingKeySet()) {\n           Map<Integer, Long> checkpoint = checkpoints.get(sequenceId);\n@@ -685,26 +674,39 @@ public void handle() throws ExecutionException, InterruptedException\n           log.info(\"Already checkpointed with offsets [%s]\", checkpoints.lastEntry().getValue());\n           return;\n         }\n-      } else {\n-        // There cannot be more than one checkpoint when previous checkpoint is null\n-        // as when the task starts they are sent existing checkpoints\n-        Preconditions.checkState(\n-            checkpoints.size() <= 1,\n-            \"Got checkpoint request with null as previous check point, however found more than one checkpoints\"\n+        final int taskGroupId = getTaskGroupIdForPartition(\n+            currentCheckpoint.getKafkaPartitions()\n+                             .getPartitionOffsetMap()\n+                             .keySet()\n+                             .iterator()\n+                             .next()\n         );\n-        if (checkpoints.size() == 1) {\n-          log.info(\"Already checkpointed with dataSourceMetadata [%s]\", checkpoints.get(0));\n-          return;\n+        final Map<Integer, Long> newCheckpoint = checkpointTaskGroup(taskGroupId, false).get();\n+        taskGroups.get(taskGroupId).addNewCheckpoint(newCheckpoint);\n+        log.info(\"Handled checkpoint notice, new checkpoint is [%s] for taskGroup [%s]\", newCheckpoint, taskGroupId);\n+      }\n+    }\n+\n+    private boolean isValidTaskGroup(@Nullable TaskGroup taskGroup)\n+    {\n+      if (taskGroup == null) {\n+        // taskGroup might be in pendingCompletionTaskGroups or partitionGroups\n+        if (pendingCompletionTaskGroups.containsKey(taskGroupId)) {\n+          log.warn(\n+              \"Ignoring checkpoint request because taskGroup[%d] has already stopped indexing and is waiting for \"\n+              + \"publishing segments\",\n+              taskGroupId\n+          );\n+          return false;\n+        } else if (partitionGroups.containsKey(taskGroupId)) {\n+          log.warn(\"Ignoring checkpoint request because taskGroup[%d] is inactive\", taskGroupId);\n+          return false;\n+        } else {\n+          throw new ISE(\"WTH?! cannot find taskGroup [%s] among all taskGroups [%s]\", taskGroupId, taskGroups);\n         }\n       }\n-      final int taskGroupId = getTaskGroupIdForPartition(currentCheckpoint.getKafkaPartitions()\n-                                                                          .getPartitionOffsetMap()\n-                                                                          .keySet()\n-                                                                          .iterator()\n-                                                                          .next());\n-      final Map<Integer, Long> newCheckpoint = checkpointTaskGroup(taskGroupId, false).get();\n-      sequenceTaskGroup.get(sequenceName).addNewCheckpoint(newCheckpoint);\n-      log.info(\"Handled checkpoint notice, new checkpoint is [%s] for sequence [%s]\", newCheckpoint, sequenceName);\n+\n+      return true;\n     }\n   }\n \n@@ -718,7 +720,6 @@ void resetInternal(DataSourceMetadata dataSourceMetadata)\n       taskGroups.values().forEach(this::killTasksInGroup);\n       taskGroups.clear();\n       partitionGroups.clear();\n-      sequenceTaskGroup.clear();\n     } else if (!(dataSourceMetadata instanceof KafkaDataSourceMetadata)) {\n       throw new IAE(\"Expected KafkaDataSourceMetadata but found instance of [%s]\", dataSourceMetadata.getClass());\n     } else {\n@@ -778,8 +779,7 @@ void resetInternal(DataSourceMetadata dataSourceMetadata)\n           resetKafkaMetadata.getKafkaPartitions().getPartitionOffsetMap().keySet().forEach(partition -> {\n             final int groupId = getTaskGroupIdForPartition(partition);\n             killTaskGroupForPartitions(ImmutableSet.of(partition));\n-            final TaskGroup removedGroup = taskGroups.remove(groupId);\n-            sequenceTaskGroup.remove(generateSequenceName(removedGroup));\n+            taskGroups.remove(groupId);\n             partitionGroups.get(groupId).replaceAll((partitionId, offset) -> NOT_SET);\n           });\n         } else {\n@@ -955,9 +955,10 @@ private void updatePartitionDataFromKafka()\n     for (int partition = 0; partition < numPartitions; partition++) {\n       int taskGroupId = getTaskGroupIdForPartition(partition);\n \n-      partitionGroups.putIfAbsent(taskGroupId, new ConcurrentHashMap<Integer, Long>());\n-\n-      ConcurrentHashMap<Integer, Long> partitionMap = partitionGroups.get(taskGroupId);\n+      ConcurrentHashMap<Integer, Long> partitionMap = partitionGroups.computeIfAbsent(\n+          taskGroupId,\n+          k -> new ConcurrentHashMap<>()\n+      );\n \n       // The starting offset for a new partition in [partitionGroups] is initially set to NOT_SET; when a new task group\n       // is created and is assigned partitions, if the offset in [partitionGroups] is NOT_SET it will take the starting\n@@ -1087,23 +1088,21 @@ public Boolean apply(KafkaIndexTask.Status status)\n                             }\n                             return false;\n                           } else {\n-                            final TaskGroup taskGroup = new TaskGroup(\n-                                ImmutableMap.copyOf(\n-                                    kafkaTask.getIOConfig()\n-                                             .getStartPartitions()\n-                                             .getPartitionOffsetMap()\n-                                ), kafkaTask.getIOConfig().getMinimumMessageTime(),\n-                                kafkaTask.getIOConfig().getMaximumMessageTime()\n-                            );\n-                            if (taskGroups.putIfAbsent(\n+                            final TaskGroup taskGroup = taskGroups.computeIfAbsent(\n                                 taskGroupId,\n-                                taskGroup\n-                            ) == null) {\n-                              sequenceTaskGroup.put(generateSequenceName(taskGroup), taskGroups.get(taskGroupId));\n-                              log.info(\"Created new task group [%d]\", taskGroupId);\n-                            }\n+                                k -> {\n+                                  log.info(\"Creating a new task group for taskGroupId[%d]\", taskGroupId);\n+                                  return new TaskGroup(\n+                                      ImmutableMap.copyOf(\n+                                          kafkaTask.getIOConfig().getStartPartitions().getPartitionOffsetMap()\n+                                      ),\n+                                      kafkaTask.getIOConfig().getMinimumMessageTime(),\n+                                      kafkaTask.getIOConfig().getMaximumMessageTime()\n+                                  );\n+                                }\n+                            );\n                             taskGroupsToVerify.add(taskGroupId);\n-                            taskGroups.get(taskGroupId).tasks.putIfAbsent(taskId, new TaskData());\n+                            taskGroup.tasks.putIfAbsent(taskId, new TaskData());\n                           }\n                         }\n                         return true;\n@@ -1256,7 +1255,6 @@ public void onFailure(Throwable t)\n       // killing all tasks or no task left in the group ?\n       // clear state about the taskgroup so that get latest offset information is fetched from metadata store\n       log.warn(\"Clearing task group [%d] information as no valid tasks left the group\", groupId);\n-      sequenceTaskGroup.remove(generateSequenceName(taskGroup));\n       taskGroups.remove(groupId);\n       partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n     }\n@@ -1281,9 +1279,10 @@ private void addDiscoveredTaskToPendingCompletionTaskGroups(\n       Map<Integer, Long> startingPartitions\n   )\n   {\n-    pendingCompletionTaskGroups.putIfAbsent(groupId, Lists.<TaskGroup>newCopyOnWriteArrayList());\n-\n-    CopyOnWriteArrayList<TaskGroup> taskGroupList = pendingCompletionTaskGroups.get(groupId);\n+    final CopyOnWriteArrayList<TaskGroup> taskGroupList = pendingCompletionTaskGroups.computeIfAbsent(\n+        groupId,\n+        k -> new CopyOnWriteArrayList<>()\n+    );\n     for (TaskGroup taskGroup : taskGroupList) {\n       if (taskGroup.partitionOffsets.equals(startingPartitions)) {\n         if (taskGroup.tasks.putIfAbsent(taskId, new TaskData()) == null) {\n@@ -1411,8 +1410,7 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n       if (endOffsets != null) {\n         // set a timeout and put this group in pendingCompletionTaskGroups so that it can be monitored for completion\n         group.completionTimeout = DateTimes.nowUtc().plus(ioConfig.getCompletionTimeout());\n-        pendingCompletionTaskGroups.putIfAbsent(groupId, Lists.<TaskGroup>newCopyOnWriteArrayList());\n-        pendingCompletionTaskGroups.get(groupId).add(group);\n+        pendingCompletionTaskGroups.computeIfAbsent(groupId, k -> new CopyOnWriteArrayList<>()).add(group);\n \n         // set endOffsets as the next startOffsets\n         for (Map.Entry<Integer, Long> entry : endOffsets.entrySet()) {\n@@ -1432,7 +1430,6 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n         partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n       }\n \n-      sequenceTaskGroup.remove(generateSequenceName(group));\n       // remove this task group from the list of current task groups now that it has been handled\n       taskGroups.remove(groupId);\n     }\n@@ -1456,7 +1453,8 @@ private void checkTaskDuration() throws InterruptedException, ExecutionException\n           // metadata store (which will have advanced if we succeeded in publishing and will remain the same if publishing\n           // failed and we need to re-ingest)\n           return Futures.transform(\n-              stopTasksInGroup(taskGroup), new Function<Object, Map<Integer, Long>>()\n+              stopTasksInGroup(taskGroup),\n+              new Function<Object, Map<Integer, Long>>()\n               {\n                 @Nullable\n                 @Override\n@@ -1625,15 +1623,15 @@ private void checkPendingCompletionTasks() throws ExecutionException, Interrupte\n             log.warn(\"All tasks in group [%d] failed to publish, killing all tasks for these partitions\", groupId);\n           } else {\n             log.makeAlert(\n-                \"No task in [%s] succeeded before the completion timeout elapsed [%s]!\",\n+                \"No task in [%s] for taskGroup [%d] succeeded before the completion timeout elapsed [%s]!\",\n                 group.taskIds(),\n+                groupId,\n                 ioConfig.getCompletionTimeout()\n             ).emit();\n           }\n \n           // reset partitions offsets for this task group so that they will be re-read from metadata storage\n           partitionGroups.get(groupId).replaceAll((partition, offset) -> NOT_SET);\n-          sequenceTaskGroup.remove(generateSequenceName(group));\n           // kill all the tasks in this pending completion group\n           killTasksInGroup(group);\n           // set a flag so the other pending completion groups for this set of partitions will also stop\n@@ -1693,7 +1691,6 @@ private void checkCurrentTaskState() throws ExecutionException, InterruptedExcep\n         // be recreated with the next set of offsets\n         if (taskData.status.isSuccess()) {\n           futures.add(stopTasksInGroup(taskGroup));\n-          sequenceTaskGroup.remove(generateSequenceName(taskGroup));\n           iTaskGroups.remove();\n           break;\n         }\n@@ -1735,7 +1732,6 @@ void createNewTasks() throws JsonProcessingException\n             groupId,\n             taskGroup\n         );\n-        sequenceTaskGroup.put(generateSequenceName(taskGroup), taskGroups.get(groupId));\n       }\n     }\n \n@@ -1778,6 +1774,7 @@ private void createKafkaTasksForGroup(int groupId, int replicas) throws JsonProc\n     DateTime maximumMessageTime = taskGroups.get(groupId).maximumMessageTime.orNull();\n \n     KafkaIOConfig kafkaIOConfig = new KafkaIOConfig(\n+        groupId,\n         sequenceName,\n         new KafkaPartitions(ioConfig.getTopic(), startPartitions),\n         new KafkaPartitions(ioConfig.getTopic(), endPartitions),\n@@ -1944,7 +1941,7 @@ private boolean isTaskCurrent(int taskGroupId, String taskId)\n     }\n   }\n \n-  private ListenableFuture<?> stopTasksInGroup(TaskGroup taskGroup)\n+  private ListenableFuture<?> stopTasksInGroup(@Nullable TaskGroup taskGroup)\n   {\n     if (taskGroup == null) {\n       return Futures.immediateFuture(null);\n@@ -2289,6 +2286,28 @@ Runnable updateCurrentAndLatestOffsets()\n     return allStats;\n   }\n \n+  @VisibleForTesting\n+  @Nullable\n+  TaskGroup removeTaskGroup(int taskGroupId)\n+  {\n+    return taskGroups.remove(taskGroupId);\n+  }\n+\n+  @VisibleForTesting\n+  void moveTaskGroupToPendingCompletion(int taskGroupId)\n+  {\n+    final TaskGroup taskGroup = taskGroups.remove(taskGroupId);\n+    if (taskGroup != null) {\n+      pendingCompletionTaskGroups.computeIfAbsent(taskGroupId, k -> new CopyOnWriteArrayList<>()).add(taskGroup);\n+    }\n+  }\n+\n+  @VisibleForTesting\n+  int getNoticesQueueSize()\n+  {\n+    return notices.size();\n+  }\n+\n   private static class StatsFromTaskResult\n   {\n     private final String groupId;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "ed287fa05919b4038e7ee1614f40ada648c8e801",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIOConfigTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIOConfigTest.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 0,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIOConfigTest.java",
                "patch": "@@ -50,6 +50,7 @@ public void testSerdeWithDefaults() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n@@ -82,6 +83,7 @@ public void testSerdeWithNonDefaults() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n@@ -118,6 +120,7 @@ public void testBaseSequenceNameRequired() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n                      + \"  \\\"consumerProperties\\\": {\\\"bootstrap.servers\\\":\\\"localhost:9092\\\"},\\n\"\n@@ -137,6 +140,7 @@ public void testStartPartitionsRequired() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n                      + \"  \\\"consumerProperties\\\": {\\\"bootstrap.servers\\\":\\\"localhost:9092\\\"},\\n\"\n@@ -156,6 +160,7 @@ public void testEndPartitionsRequired() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"consumerProperties\\\": {\\\"bootstrap.servers\\\":\\\"localhost:9092\\\"},\\n\"\n@@ -175,6 +180,7 @@ public void testConsumerPropertiesRequired() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n@@ -194,6 +200,7 @@ public void testStartAndEndTopicMatch() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"other\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":200}},\\n\"\n@@ -214,6 +221,7 @@ public void testStartAndEndPartitionSetMatch() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15}},\\n\"\n@@ -234,6 +242,7 @@ public void testEndOffsetGreaterThanStart() throws Exception\n   {\n     String jsonStr = \"{\\n\"\n                      + \"  \\\"type\\\": \\\"kafka\\\",\\n\"\n+                     + \"  \\\"taskGroupId\\\": 0,\\n\"\n                      + \"  \\\"baseSequenceName\\\": \\\"my-sequence-name\\\",\\n\"\n                      + \"  \\\"startPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":1, \\\"1\\\":10}},\\n\"\n                      + \"  \\\"endPartitions\\\": {\\\"topic\\\":\\\"mytopic\\\", \\\"partitionOffsetMap\\\" : {\\\"0\\\":15, \\\"1\\\":2}},\\n\"",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIOConfigTest.java",
                "sha": "050dba753b8edac8c58c263ef333681513663d2d",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "changes": 106,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 36,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "patch": "@@ -262,21 +262,21 @@ public KafkaIndexTaskTest(boolean isIncrementalHandoffSupported)\n   private static List<ProducerRecord<byte[], byte[]>> generateRecords(String topic)\n   {\n     return ImmutableList.of(\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2008\", \"a\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2009\", \"b\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2010\", \"c\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2011\", \"d\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2011\", \"e\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"246140482-04-24T15:36:27.903Z\", \"x\", \"z\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, StringUtils.toUtf8(\"unparseable\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, StringUtils.toUtf8(\"unparseable2\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, null),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2013\", \"f\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"notanumber\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"10\", \"notanumber\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"10\", \"20.0\", \"notanumber\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 1, null, JB(\"2012\", \"g\", \"y\", \"10\", \"20.0\", \"1.0\")),\n-        new ProducerRecord<byte[], byte[]>(topic, 1, null, JB(\"2011\", \"h\", \"y\", \"10\", \"20.0\", \"1.0\"))\n+        new ProducerRecord<>(topic, 0, null, JB(\"2008\", \"a\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2009\", \"b\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2010\", \"c\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2011\", \"d\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2011\", \"e\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"246140482-04-24T15:36:27.903Z\", \"x\", \"z\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(\"unparseable\")),\n+        new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(\"unparseable2\")),\n+        new ProducerRecord<>(topic, 0, null, null),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2013\", \"f\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"notanumber\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"10\", \"notanumber\", \"1.0\")),\n+        new ProducerRecord<>(topic, 0, null, JB(\"2049\", \"f\", \"y\", \"10\", \"20.0\", \"notanumber\")),\n+        new ProducerRecord<>(topic, 1, null, JB(\"2012\", \"g\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+        new ProducerRecord<>(topic, 1, null, JB(\"2011\", \"h\", \"y\", \"10\", \"20.0\", \"1.0\"))\n     );\n   }\n \n@@ -377,6 +377,7 @@ public void testRunAfterDataInserted() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -418,6 +419,7 @@ public void testRunBeforeDataInserted() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -493,6 +495,7 @@ public void testIncrementalHandOff() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             baseSequenceName,\n             startPartitions,\n             endPartitions,\n@@ -514,14 +517,16 @@ public void testIncrementalHandOff() throws Exception\n     Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     Assert.assertEquals(1, checkpointRequestsHash.size());\n-    Assert.assertTrue(checkpointRequestsHash.contains(\n-        Objects.hash(\n-            DATA_SCHEMA.getDataSource(),\n-            baseSequenceName,\n-            new KafkaDataSourceMetadata(startPartitions),\n-            new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n+    Assert.assertTrue(\n+        checkpointRequestsHash.contains(\n+            Objects.hash(\n+                DATA_SCHEMA.getDataSource(),\n+                0,\n+                new KafkaDataSourceMetadata(startPartitions),\n+                new KafkaDataSourceMetadata(new KafkaPartitions(topic, currentOffsets))\n+            )\n         )\n-    ));\n+    );\n \n     // Check metrics\n     Assert.assertEquals(8, task.getRunner().getRowIngestionMeters().getProcessed());\n@@ -581,6 +586,7 @@ public void testTimeBasedIncrementalHandOff() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             baseSequenceName,\n             startPartitions,\n             endPartitions,\n@@ -603,14 +609,16 @@ public void testTimeBasedIncrementalHandOff() throws Exception\n     Assert.assertEquals(TaskState.SUCCESS, future.get().getStatusCode());\n \n     Assert.assertEquals(1, checkpointRequestsHash.size());\n-    Assert.assertTrue(checkpointRequestsHash.contains(\n-        Objects.hash(\n-            DATA_SCHEMA.getDataSource(),\n-            baseSequenceName,\n-            new KafkaDataSourceMetadata(startPartitions),\n-            new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n+    Assert.assertTrue(\n+        checkpointRequestsHash.contains(\n+            Objects.hash(\n+                DATA_SCHEMA.getDataSource(),\n+                0,\n+                new KafkaDataSourceMetadata(startPartitions),\n+                new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoint.getPartitionOffsetMap()))\n+            )\n         )\n-    ));\n+    );\n \n     // Check metrics\n     Assert.assertEquals(2, task.getRunner().getRowIngestionMeters().getProcessed());\n@@ -637,6 +645,7 @@ public void testRunWithMinimumMessageTime() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -690,6 +699,7 @@ public void testRunWithMaximumMessageTime() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -753,6 +763,7 @@ public void testRunWithTransformSpec() throws Exception\n             )\n         ),\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -812,6 +823,7 @@ public void testRunOnNothing() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n@@ -852,6 +864,7 @@ public void testHandoffConditionTimeoutWhenHandoffOccurs() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -903,6 +916,7 @@ public void testHandoffConditionTimeoutWhenHandoffDoesNotOccur() throws Exceptio\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -957,6 +971,7 @@ public void testReportParseExceptions() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 7L)),\n@@ -1000,6 +1015,7 @@ public void testMultipleParseExceptionsSuccess() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 13L)),\n@@ -1081,6 +1097,7 @@ public void testMultipleParseExceptionsFailure() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 10L)),\n@@ -1140,6 +1157,7 @@ public void testRunReplicas() throws Exception\n     final KafkaIndexTask task1 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1153,6 +1171,7 @@ public void testRunReplicas() throws Exception\n     final KafkaIndexTask task2 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1206,6 +1225,7 @@ public void testRunConflicting() throws Exception\n     final KafkaIndexTask task1 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1219,6 +1239,7 @@ public void testRunConflicting() throws Exception\n     final KafkaIndexTask task2 = createTask(\n         null,\n         new KafkaIOConfig(\n+            1,\n             \"sequence1\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 3L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 10L)),\n@@ -1273,6 +1294,7 @@ public void testRunConflictingWithoutTransactions() throws Exception\n     final KafkaIndexTask task1 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1286,6 +1308,7 @@ public void testRunConflictingWithoutTransactions() throws Exception\n     final KafkaIndexTask task2 = createTask(\n         null,\n         new KafkaIOConfig(\n+            1,\n             \"sequence1\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 3L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 10L)),\n@@ -1345,6 +1368,7 @@ public void testRunOneTaskTwoPartitions() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L, 1, 0L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 2L)),\n@@ -1409,6 +1433,7 @@ public void testRunTwoTasksTwoPartitions() throws Exception\n     final KafkaIndexTask task1 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1422,6 +1447,7 @@ public void testRunTwoTasksTwoPartitions() throws Exception\n     final KafkaIndexTask task2 = createTask(\n         null,\n         new KafkaIOConfig(\n+            1,\n             \"sequence1\",\n             new KafkaPartitions(topic, ImmutableMap.of(1, 0L)),\n             new KafkaPartitions(topic, ImmutableMap.of(1, 1L)),\n@@ -1477,6 +1503,7 @@ public void testRestore() throws Exception\n     final KafkaIndexTask task1 = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1513,6 +1540,7 @@ public void testRestore() throws Exception\n     final KafkaIndexTask task2 = createTask(\n         task1.getId(),\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1564,6 +1592,7 @@ public void testRunWithPauseAndResume() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1647,6 +1676,7 @@ public void testRunWithOffsetOutOfRangeExceptionAndPause() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 2L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 5L)),\n@@ -1685,6 +1715,7 @@ public void testRunWithOffsetOutOfRangeExceptionAndNextOffsetGreaterThanLeastAva\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             new KafkaPartitions(topic, ImmutableMap.of(0, 200L)),\n             new KafkaPartitions(topic, ImmutableMap.of(0, 500L)),\n@@ -1737,6 +1768,7 @@ public void testRunContextSequenceAheadOfStartingOffsets() throws Exception\n     final KafkaIndexTask task = createTask(\n         null,\n         new KafkaIOConfig(\n+            0,\n             \"sequence0\",\n             // task should ignore these and use sequence info sent in the context\n             new KafkaPartitions(topic, ImmutableMap.of(0, 0L)),\n@@ -2026,18 +2058,20 @@ private void makeToolboxFactory() throws IOException\n           @Override\n           public boolean checkPointDataSourceMetadata(\n               String supervisorId,\n-              @Nullable String sequenceName,\n+              int taskGroupId,\n               @Nullable DataSourceMetadata previousDataSourceMetadata,\n               @Nullable DataSourceMetadata currentDataSourceMetadata\n           )\n           {\n             log.info(\"Adding checkpoint hash to the set\");\n-            checkpointRequestsHash.add(Objects.hash(\n-                supervisorId,\n-                sequenceName,\n-                previousDataSourceMetadata,\n-                currentDataSourceMetadata\n-            ));\n+            checkpointRequestsHash.add(\n+                Objects.hash(\n+                    supervisorId,\n+                    taskGroupId,\n+                    previousDataSourceMetadata,\n+                    currentDataSourceMetadata\n+                )\n+            );\n             return true;\n           }\n         }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java",
                "sha": "411fff9168e9d88d9ef8da471702e8bf6faf91be",
                "status": "modified"
            },
            {
                "additions": 279,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "changes": 344,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 65,
                "filename": "extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package io.druid.indexing.kafka.supervisor;\n \n+import com.fasterxml.jackson.core.JsonProcessingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Optional;\n import com.google.common.collect.ImmutableList;\n@@ -61,6 +62,7 @@\n import io.druid.java.util.common.granularity.Granularities;\n import io.druid.java.util.common.parsers.JSONPathFieldSpec;\n import io.druid.java.util.common.parsers.JSONPathSpec;\n+import io.druid.java.util.emitter.EmittingLogger;\n import io.druid.query.aggregation.AggregatorFactory;\n import io.druid.query.aggregation.CountAggregatorFactory;\n import io.druid.segment.TestHelper;\n@@ -70,6 +72,7 @@\n import io.druid.segment.realtime.FireDepartment;\n import io.druid.server.metrics.DruidMonitorSchedulerConfig;\n import io.druid.server.metrics.NoopServiceEmitter;\n+import io.druid.server.metrics.ExceptionCapturingServiceEmitter;\n import org.apache.curator.test.TestingCluster;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n@@ -99,7 +102,9 @@\n import java.util.List;\n import java.util.Map;\n import java.util.TreeMap;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executor;\n+import java.util.concurrent.TimeoutException;\n \n import static org.easymock.EasyMock.anyBoolean;\n import static org.easymock.EasyMock.anyObject;\n@@ -141,6 +146,7 @@\n   private TaskQueue taskQueue;\n   private String topic;\n   private RowIngestionMetersFactory rowIngestionMetersFactory;\n+  private ExceptionCapturingServiceEmitter serviceEmitter;\n \n   private static String getTopic()\n   {\n@@ -213,6 +219,8 @@ public void setupTest()\n \n     topic = getTopic();\n     rowIngestionMetersFactory = new TestUtils().getRowIngestionMetersFactory();\n+    serviceEmitter = new ExceptionCapturingServiceEmitter();\n+    EmittingLogger.registerEmitter(serviceEmitter);\n   }\n \n   @After\n@@ -553,7 +561,7 @@ public void testKillIncompatibleTasks() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"index_kafka_testDS__some_other_sequenceName\",\n+        1,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L)),\n         null,\n@@ -564,7 +572,7 @@ public void testKillIncompatibleTasks() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 333L, 1, 333L, 2, 333L)),\n         null,\n@@ -575,7 +583,7 @@ public void testKillIncompatibleTasks() throws Exception\n     Task id3 = createKafkaIndexTask(\n         \"id3\",\n         DATASOURCE,\n-        \"index_kafka_testDS__some_other_sequenceName\",\n+        1,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 1L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 333L, 1, 333L, 2, 330L)),\n         null,\n@@ -586,7 +594,7 @@ public void testKillIncompatibleTasks() throws Exception\n     Task id4 = createKafkaIndexTask(\n         \"id4\",\n         \"other-datasource\",\n-        \"index_kafka_testDS_d927edff33c4b3f\",\n+        2,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L)),\n         null,\n@@ -634,7 +642,9 @@ public void testKillIncompatibleTasks() throws Exception\n \n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(2);\n \n     replayAll();\n \n@@ -652,7 +662,7 @@ public void testKillBadPartitionAssignment() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -661,7 +671,7 @@ public void testKillBadPartitionAssignment() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-1\",\n+        1,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(1, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(1, Long.MAX_VALUE)),\n         null,\n@@ -670,7 +680,7 @@ public void testKillBadPartitionAssignment() throws Exception\n     Task id3 = createKafkaIndexTask(\n         \"id3\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -679,7 +689,7 @@ public void testKillBadPartitionAssignment() throws Exception\n     Task id4 = createKafkaIndexTask(\n         \"id4\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE)),\n         null,\n@@ -688,7 +698,7 @@ public void testKillBadPartitionAssignment() throws Exception\n     Task id5 = createKafkaIndexTask(\n         \"id5\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -727,8 +737,12 @@ public void testKillBadPartitionAssignment() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1)).times(1);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(1);\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     taskQueue.shutdown(\"id4\");\n@@ -765,10 +779,12 @@ public void testRequeueTaskWhenFailed() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .anyTimes();\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .anyTimes();\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .anyTimes();\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .anyTimes();\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     replayAll();\n@@ -830,7 +846,7 @@ public void testRequeueAdoptedTaskWhenFailed() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         now,\n@@ -857,7 +873,9 @@ public void testRequeueAdoptedTaskWhenFailed() throws Exception\n \n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(2);\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     replayAll();\n@@ -878,9 +896,12 @@ public void testRequeueAdoptedTaskWhenFailed() throws Exception\n     reset(taskClient);\n \n     // for the newly created replica task\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n \n     expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(captured.getValue())).anyTimes();\n     expect(taskStorage.getStatus(iHaveFailed.getId())).andReturn(Optional.of(TaskStatus.failure(iHaveFailed.getId())));\n@@ -953,10 +974,12 @@ public void testQueueNextTasksOnSuccess() throws Exception\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n     // there would be 4 tasks, 2 for each task group\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(2);\n \n     expect(taskStorage.getActiveTasks()).andReturn(tasks).anyTimes();\n     for (Task task : tasks) {\n@@ -1063,10 +1086,12 @@ public void testBeginPublishAndQueueNextTasks() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(2);\n \n     replay(taskStorage, taskRunner, taskClient, taskQueue);\n \n@@ -1100,7 +1125,7 @@ public void testDiscoverExistingPublishingTask() throws Exception\n     Task task = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1192,7 +1217,7 @@ public void testDiscoverExistingPublishingTaskWithDifferentPartitionAllocation()\n     Task task = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1282,7 +1307,7 @@ public void testDiscoverExistingPublishingAndReadingTask() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1292,7 +1317,7 @@ public void testDiscoverExistingPublishingAndReadingTask() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 1L, 1, 2L, 2, 3L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1330,7 +1355,9 @@ public void testDiscoverExistingPublishingAndReadingTask() throws Exception\n     // since id1 is publishing, so getCheckpoints wouldn't be called for it\n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 1L, 1, 2L, 2, 3L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n \n     replayAll();\n \n@@ -1404,10 +1431,12 @@ public void testKillUnresponsiveTasksWhileGettingStartTime() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(2);\n \n     expect(taskStorage.getActiveTasks()).andReturn(tasks).anyTimes();\n     for (Task task : tasks) {\n@@ -1463,10 +1492,12 @@ public void testKillUnresponsiveTasksWhilePausing() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(2);\n \n     captured = Capture.newInstance(CaptureType.ALL);\n     expect(taskStorage.getActiveTasks()).andReturn(tasks).anyTimes();\n@@ -1540,10 +1571,12 @@ public void testKillUnresponsiveTasksWhileSettingEndOffsets() throws Exception\n     checkpoints1.put(0, ImmutableMap.of(0, 0L, 2, 0L));\n     TreeMap<Integer, Map<Integer, Long>> checkpoints2 = new TreeMap<>();\n     checkpoints2.put(0, ImmutableMap.of(1, 0L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints1))\n-                                                                                        .times(2);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints2))\n-                                                                                        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-0\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints1))\n+        .times(2);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"sequenceName-1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints2))\n+        .times(2);\n \n     captured = Capture.newInstance(CaptureType.ALL);\n     expect(taskStorage.getActiveTasks()).andReturn(tasks).anyTimes();\n@@ -1622,7 +1655,7 @@ public void testStopGracefully() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1632,7 +1665,7 @@ public void testStopGracefully() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1642,7 +1675,7 @@ public void testStopGracefully() throws Exception\n     Task id3 = createKafkaIndexTask(\n         \"id3\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1678,8 +1711,12 @@ public void testStopGracefully() throws Exception\n     // getCheckpoints will not be called for id1 as it is in publishing state\n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     replayAll();\n@@ -1824,7 +1861,7 @@ public void testResetRunningTasks() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1834,7 +1871,7 @@ public void testResetRunningTasks() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1844,7 +1881,7 @@ public void testResetRunningTasks() throws Exception\n     Task id3 = createKafkaIndexTask(\n         \"id3\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1879,8 +1916,12 @@ public void testResetRunningTasks() throws Exception\n \n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     replayAll();\n@@ -1908,7 +1949,7 @@ public void testNoDataIngestionTasks() throws Exception\n     Task id1 = createKafkaIndexTask(\n         \"id1\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1918,7 +1959,7 @@ public void testNoDataIngestionTasks() throws Exception\n     Task id2 = createKafkaIndexTask(\n         \"id2\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1928,7 +1969,7 @@ public void testNoDataIngestionTasks() throws Exception\n     Task id3 = createKafkaIndexTask(\n         \"id3\",\n         DATASOURCE,\n-        \"sequenceName-0\",\n+        0,\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n         new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n         null,\n@@ -1958,9 +1999,15 @@ public void testNoDataIngestionTasks() throws Exception\n \n     TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n     checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n-    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean())).andReturn(Futures.immediateFuture(checkpoints)).times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n \n     taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n     replayAll();\n@@ -1980,6 +2027,172 @@ public void testNoDataIngestionTasks() throws Exception\n     verifyAll();\n   }\n \n+  @Test(timeout = 60_000L)\n+  public void testCheckpointForInactiveTaskGroup()\n+      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n+  {\n+    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n+    //not adding any events\n+    final Task id1 = createKafkaIndexTask(\n+        \"id1\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id2 = createKafkaIndexTask(\n+        \"id2\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id3 = createKafkaIndexTask(\n+        \"id3\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n+    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n+    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n+    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n+    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n+    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n+    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n+    expect(\n+        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n+    ).anyTimes();\n+    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n+    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n+    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n+\n+    final DateTime startTime = DateTimes.nowUtc();\n+    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n+    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n+    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n+\n+    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n+    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n+        .andReturn(Futures.immediateFuture(checkpoints))\n+        .times(1);\n+\n+    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n+    replayAll();\n+\n+    supervisor.start();\n+    supervisor.runInternal();\n+\n+    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n+    supervisor.moveTaskGroupToPendingCompletion(0);\n+    supervisor.checkpoint(\n+        0,\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n+    );\n+\n+    while (supervisor.getNoticesQueueSize() > 0) {\n+      Thread.sleep(100);\n+    }\n+\n+    verifyAll();\n+\n+    Assert.assertNull(serviceEmitter.getStackTrace());\n+    Assert.assertNull(serviceEmitter.getExceptionMessage());\n+    Assert.assertNull(serviceEmitter.getExceptionClass());\n+  }\n+\n+  @Test(timeout = 60_000L)\n+  public void testCheckpointForUnknownTaskGroup() throws InterruptedException\n+  {\n+    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n+    //not adding any events\n+    final Task id1 = createKafkaIndexTask(\n+        \"id1\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id2 = createKafkaIndexTask(\n+        \"id2\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    final Task id3 = createKafkaIndexTask(\n+        \"id3\",\n+        DATASOURCE,\n+        0,\n+        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n+        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n+        null,\n+        null\n+    );\n+\n+    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n+    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n+    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n+    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n+    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n+    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n+    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n+    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n+    expect(\n+        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n+    ).anyTimes();\n+\n+    replayAll();\n+\n+    supervisor.start();\n+\n+    supervisor.checkpoint(\n+        0,\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, Collections.emptyMap())),\n+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, Collections.emptyMap()))\n+    );\n+\n+    while (supervisor.getNoticesQueueSize() > 0) {\n+      Thread.sleep(100);\n+    }\n+\n+    verifyAll();\n+\n+    Assert.assertNotNull(serviceEmitter.getStackTrace());\n+    Assert.assertEquals(\n+        \"WTH?! cannot find taskGroup [0] among all taskGroups [{}]\",\n+        serviceEmitter.getExceptionMessage()\n+    );\n+    Assert.assertEquals(ISE.class, serviceEmitter.getExceptionClass());\n+  }\n+\n   private void addSomeEvents(int numEventsPerPartition) throws Exception\n   {\n     try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n@@ -2106,7 +2319,7 @@ private static DataSchema getDataSchema(String dataSource)\n   private KafkaIndexTask createKafkaIndexTask(\n       String id,\n       String dataSource,\n-      String sequenceName,\n+      int taskGroupId,\n       KafkaPartitions startPartitions,\n       KafkaPartitions endPartitions,\n       DateTime minimumMessageTime,\n@@ -2119,7 +2332,8 @@ private KafkaIndexTask createKafkaIndexTask(\n         getDataSchema(dataSource),\n         tuningConfig,\n         new KafkaIOConfig(\n-            sequenceName,\n+            taskGroupId,\n+            \"sequenceName-\" + taskGroupId,\n             startPartitions,\n             endPartitions,\n             ImmutableMap.<String, String>of(),\n@@ -2128,7 +2342,7 @@ private KafkaIndexTask createKafkaIndexTask(\n             maximumMessageTime,\n             false\n         ),\n-        ImmutableMap.<String, Object>of(),\n+        Collections.emptyMap(),\n         null,\n         null,\n         rowIngestionMetersFactory",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java",
                "sha": "f0f6033eea38bd847e517b06b4ae208e7b2b0274",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 11,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "patch": "@@ -21,27 +21,28 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.core.type.TypeReference;\n+import com.google.common.base.Preconditions;\n import io.druid.indexing.common.task.Task;\n import io.druid.indexing.overlord.DataSourceMetadata;\n \n public class CheckPointDataSourceMetadataAction implements TaskAction<Boolean>\n {\n   private final String supervisorId;\n-  private final String sequenceName;\n+  private final int taskGroupId;\n   private final DataSourceMetadata previousCheckPoint;\n   private final DataSourceMetadata currentCheckPoint;\n \n   public CheckPointDataSourceMetadataAction(\n       @JsonProperty(\"supervisorId\") String supervisorId,\n-      @JsonProperty(\"sequenceName\") String sequenceName,\n+      @JsonProperty(\"taskGroupId\") Integer taskGroupId,\n       @JsonProperty(\"previousCheckPoint\") DataSourceMetadata previousCheckPoint,\n       @JsonProperty(\"currentCheckPoint\") DataSourceMetadata currentCheckPoint\n   )\n   {\n-    this.supervisorId = supervisorId;\n-    this.sequenceName = sequenceName;\n-    this.previousCheckPoint = previousCheckPoint;\n-    this.currentCheckPoint = currentCheckPoint;\n+    this.supervisorId = Preconditions.checkNotNull(supervisorId, \"supervisorId\");\n+    this.taskGroupId = Preconditions.checkNotNull(taskGroupId, \"taskGroupId\");\n+    this.previousCheckPoint = Preconditions.checkNotNull(previousCheckPoint, \"previousCheckPoint\");\n+    this.currentCheckPoint = Preconditions.checkNotNull(currentCheckPoint, \"currentCheckPoint\");\n   }\n \n   @JsonProperty\n@@ -51,9 +52,9 @@ public String getSupervisorId()\n   }\n \n   @JsonProperty\n-  public String getSequenceName()\n+  public int getTaskGroupId()\n   {\n-    return sequenceName;\n+    return taskGroupId;\n   }\n \n   @JsonProperty\n@@ -81,8 +82,12 @@ public Boolean perform(\n       Task task, TaskActionToolbox toolbox\n   )\n   {\n-    return toolbox.getSupervisorManager()\n-                  .checkPointDataSourceMetadata(supervisorId, sequenceName, previousCheckPoint, currentCheckPoint);\n+    return toolbox.getSupervisorManager().checkPointDataSourceMetadata(\n+        supervisorId,\n+        taskGroupId,\n+        previousCheckPoint,\n+        currentCheckPoint\n+    );\n   }\n \n   @Override\n@@ -96,7 +101,7 @@ public String toString()\n   {\n     return \"CheckPointDataSourceMetadataAction{\" +\n            \"supervisorId='\" + supervisorId + '\\'' +\n-           \", sequenceName='\" + sequenceName + '\\'' +\n+           \", taskGroupId='\" + taskGroupId + '\\'' +\n            \", previousCheckPoint=\" + previousCheckPoint +\n            \", currentCheckPoint=\" + currentCheckPoint +\n            '}';",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/indexing-service/src/main/java/io/druid/indexing/common/actions/CheckPointDataSourceMetadataAction.java",
                "sha": "f1d11deb4eafa61c0f0856654869b984e2afa267",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 4,
                "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "patch": "@@ -165,9 +165,9 @@ public boolean resetSupervisor(String id, @Nullable DataSourceMetadata dataSourc\n \n   public boolean checkPointDataSourceMetadata(\n       String supervisorId,\n-      @Nullable String sequenceName,\n-      @Nullable DataSourceMetadata previousDataSourceMetadata,\n-      @Nullable DataSourceMetadata currentDataSourceMetadata\n+      int taskGroupId,\n+      DataSourceMetadata previousDataSourceMetadata,\n+      DataSourceMetadata currentDataSourceMetadata\n   )\n   {\n     try {\n@@ -178,7 +178,7 @@ public boolean checkPointDataSourceMetadata(\n \n       Preconditions.checkNotNull(supervisor, \"supervisor could not be found\");\n \n-      supervisor.lhs.checkpoint(sequenceName, previousDataSourceMetadata, currentDataSourceMetadata);\n+      supervisor.lhs.checkpoint(taskGroupId, previousDataSourceMetadata, currentDataSourceMetadata);\n       return true;\n     }\n     catch (Exception e) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/indexing-service/src/main/java/io/druid/indexing/overlord/supervisor/SupervisorManager.java",
                "sha": "f9a55644432741cfb4b1cb695669d12358ed208d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/java-util/src/main/java/io/druid/java/util/emitter/EmittingLogger.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/java-util/src/main/java/io/druid/java/util/emitter/EmittingLogger.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 0,
                "filename": "java-util/src/main/java/io/druid/java/util/emitter/EmittingLogger.java",
                "patch": "@@ -35,6 +35,10 @@\n  */\n public class EmittingLogger extends Logger\n {\n+  public static final String EXCEPTION_TYPE_KEY = \"exceptionType\";\n+  public static final String EXCEPTION_MESSAGE_KEY = \"exceptionMessage\";\n+  public static final String EXCEPTION_STACK_TRACE_KEY = \"exceptionStackTrace\";\n+\n   private static volatile ServiceEmitter emitter = null;\n \n   private final String className;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/java-util/src/main/java/io/druid/java/util/emitter/EmittingLogger.java",
                "sha": "661ed17d3b94f566426288cb797a8c7a5390d2d5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 3,
                "filename": "server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "patch": "@@ -83,9 +83,9 @@ public void reset(DataSourceMetadata dataSourceMetadata) {}\n \n       @Override\n       public void checkpoint(\n-          @Nullable String sequenceName,\n-          @Nullable DataSourceMetadata previousCheckPoint,\n-          @Nullable DataSourceMetadata currentCheckPoint\n+          int taskGroupId,\n+          DataSourceMetadata previousCheckPoint,\n+          DataSourceMetadata currentCheckPoint\n       )\n       {\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/main/java/io/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java",
                "sha": "0408104cde83e7e5daddcdd534767b07f3e1174d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 7,
                "filename": "server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "patch": "@@ -22,7 +22,6 @@\n import com.google.common.collect.ImmutableMap;\n import io.druid.indexing.overlord.DataSourceMetadata;\n \n-import javax.annotation.Nullable;\n import java.util.Map;\n \n public interface Supervisor\n@@ -52,13 +51,9 @@\n    * for example - Kafka Supervisor uses this to merge and handoff segments containing at least the data\n    * represented by {@param currentCheckpoint} DataSourceMetadata\n    *\n-   * @param sequenceName       unique Identifier to figure out for which sequence to do checkpointing\n+   * @param taskGroupId        unique Identifier to figure out for which sequence to do checkpointing\n    * @param previousCheckPoint DataSourceMetadata checkpointed in previous call\n    * @param currentCheckPoint  current DataSourceMetadata to be checkpointed\n    */\n-  void checkpoint(\n-      @Nullable String sequenceName,\n-      @Nullable DataSourceMetadata previousCheckPoint,\n-      @Nullable DataSourceMetadata currentCheckPoint\n-  );\n+  void checkpoint(int taskGroupId, DataSourceMetadata previousCheckPoint, DataSourceMetadata currentCheckPoint);\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/main/java/io/druid/indexing/overlord/supervisor/Supervisor.java",
                "sha": "04afac7aea6b53b68b92ade85df6b565a5fb6187",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/test/java/io/druid/server/metrics/ExceptionCapturingServiceEmitter.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/server/metrics/ExceptionCapturingServiceEmitter.java?ref=c48aa74a301a11f49b0d6ba6bde4283bdab7f699",
                "deletions": 0,
                "filename": "server/src/test/java/io/druid/server/metrics/ExceptionCapturingServiceEmitter.java",
                "patch": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.druid.server.metrics;\n+\n+import io.druid.java.util.emitter.EmittingLogger;\n+import io.druid.java.util.emitter.core.Event;\n+import io.druid.java.util.emitter.service.ServiceEmitter;\n+\n+import javax.annotation.Nullable;\n+import java.util.Map;\n+\n+public class ExceptionCapturingServiceEmitter extends ServiceEmitter\n+{\n+  private volatile Class exceptionClass;\n+  private volatile String exceptionMessage;\n+  private volatile String stackTrace;\n+\n+  public ExceptionCapturingServiceEmitter()\n+  {\n+    super(\"\", \"\", null);\n+  }\n+\n+  @Override\n+  public void emit(Event event)\n+  {\n+    //noinspection unchecked\n+    final Map<String, Object> dataMap = (Map<String, Object>) event.toMap().get(\"data\");\n+    final Class exceptionClass = (Class) dataMap.get(EmittingLogger.EXCEPTION_TYPE_KEY);\n+    if (exceptionClass != null) {\n+      final String exceptionMessage = (String) dataMap.get(EmittingLogger.EXCEPTION_MESSAGE_KEY);\n+      final String stackTrace = (String) dataMap.get(EmittingLogger.EXCEPTION_STACK_TRACE_KEY);\n+      this.exceptionClass = exceptionClass;\n+      this.exceptionMessage = exceptionMessage;\n+      this.stackTrace = stackTrace;\n+    }\n+  }\n+\n+  @Nullable\n+  public Class getExceptionClass()\n+  {\n+    return exceptionClass;\n+  }\n+\n+  @Nullable\n+  public String getExceptionMessage()\n+  {\n+    return exceptionMessage;\n+  }\n+\n+  @Nullable\n+  public String getStackTrace()\n+  {\n+    return stackTrace;\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c48aa74a301a11f49b0d6ba6bde4283bdab7f699/server/src/test/java/io/druid/server/metrics/ExceptionCapturingServiceEmitter.java",
                "sha": "cc217c6f5b054eea8639ecb9eb5dcae14eb4c1fa",
                "status": "added"
            }
        ],
        "message": "Fix NPE while handling CheckpointNotice in KafkaSupervisor (#5996)\n\n* Fix NPE while handling CheckpointNotice\r\n\r\n* fix code style\r\n\r\n* Fix test\r\n\r\n* fix test\r\n\r\n* add a log for creating a new taskGroup\r\n\r\n* fix backward compatibility in KafkaIOConfig",
        "parent": "https://github.com/apache/incubator-druid/commit/31c2179fe1daa564e918f4ba64937743a45132a1",
        "repo": "incubator-druid",
        "unit_tests": [
            "MaterializedViewSupervisorTest.java",
            "KafkaIOConfigTest.java",
            "KafkaSupervisorTest.java",
            "SupervisorManagerTest.java"
        ]
    },
    "incubator-druid_c56a980": {
        "bug_id": "incubator-druid_c56a980",
        "commit": "https://github.com/apache/incubator-druid/commit/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java?ref=c56a9807d4bb95e0e7280476e8f4607bc62b1e4f",
                "deletions": 1,
                "filename": "extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "patch": "@@ -2117,7 +2117,7 @@ private void updateLatestOffsetsFromKafka()\n                      && latestOffsetsFromKafka.get(e.getKey()) != null\n                      && e.getValue() != null\n                      ? latestOffsetsFromKafka.get(e.getKey()) - e.getValue()\n-                     : null\n+                     : Integer.MIN_VALUE\n             )\n         );\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c56a9807d4bb95e0e7280476e8f4607bc62b1e4f/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/supervisor/KafkaSupervisor.java",
                "sha": "e9165ffbe3472dcdf98265e1c4d4832845f085b9",
                "status": "modified"
            }
        ],
        "message": "prevent npe on mismatch between number of kafka partitions and task count (#5139)",
        "parent": "https://github.com/apache/incubator-druid/commit/11814119019232e7c17d68b665526f6462c3e521",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaSupervisorTest.java"
        ]
    },
    "incubator-druid_c88cb2a": {
        "bug_id": "incubator-druid_c88cb2a",
        "commit": "https://github.com/apache/incubator-druid/commit/c88cb2af16db60ae9cbc0d89929bfc4610da5fce",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java?ref=c88cb2af16db60ae9cbc0d89929bfc4610da5fce",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java",
                "patch": "@@ -126,8 +126,13 @@ public Capabilities getCapabilities()\n   @Override\n   public Iterable<Cursor> makeCursors(final Filter filter, final Interval interval, final QueryGranularity gran)\n   {\n+    if (index.isEmpty()) {\n+      return ImmutableList.of();\n+    }\n+\n     Interval actualIntervalTmp = interval;\n \n+\n     final Interval dataInterval = new Interval(getMinTime().getMillis(), gran.next(getMaxTime().getMillis()));\n     if (!actualIntervalTmp.overlaps(dataInterval)) {\n       return ImmutableList.of();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/main/java/io/druid/segment/incremental/IncrementalIndexStorageAdapter.java",
                "sha": "8843879f91b2f622ea259760d023c3a795ed572d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java?ref=c88cb2af16db60ae9cbc0d89929bfc4610da5fce",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "patch": "@@ -103,6 +103,9 @@\n   public static final QuerySegmentSpec fullOnInterval = new MultipleIntervalSegmentSpec(\n       Arrays.asList(new Interval(\"1970-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"))\n   );\n+  public static final QuerySegmentSpec emptyInterval = new MultipleIntervalSegmentSpec(\n+      Arrays.asList(new Interval(\"2020-04-02T00:00:00.000Z/P1D\"))\n+  );\n \n   @SuppressWarnings(\"unchecked\")\n   public static Collection<?> makeQueryRunners(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/test/java/io/druid/query/QueryRunnerTestHelper.java",
                "sha": "7434b7301f3e0112ec4b81f21c32a74c72b42e18",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java?ref=c88cb2af16db60ae9cbc0d89929bfc4610da5fce",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "patch": "@@ -58,7 +58,9 @@\n import org.joda.time.DateTimeZone;\n import org.joda.time.Interval;\n import org.joda.time.Period;\n+import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Ignore;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -910,6 +912,39 @@ public void testDifferentIntervalSubquery()\n     TestHelper.assertExpectedObjects(expectedResults, results, \"\");\n   }\n \n+  @Test\n+  public void testEmptySubquery()\n+  {\n+    GroupByQuery subquery = GroupByQuery\n+        .builder()\n+        .setDataSource(QueryRunnerTestHelper.dataSource)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.emptyInterval)\n+        .setDimensions(Lists.<DimensionSpec>newArrayList(new DefaultDimensionSpec(\"quality\", \"alias\")))\n+        .setAggregatorSpecs(\n+            Arrays.<AggregatorFactory>asList(\n+                QueryRunnerTestHelper.rowsCount,\n+                new LongSumAggregatorFactory(\"idx\", \"index\")\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .build();\n+\n+    GroupByQuery query = GroupByQuery\n+        .builder()\n+        .setDataSource(subquery)\n+        .setQuerySegmentSpec(QueryRunnerTestHelper.firstToThird)\n+        .setAggregatorSpecs(\n+            Arrays.<AggregatorFactory>asList(\n+                new MaxAggregatorFactory(\"idx\", \"idx\")\n+            )\n+        )\n+        .setGranularity(QueryRunnerTestHelper.dayGran)\n+        .build();\n+\n+    Iterable<Row> results = runQuery(query);\n+    Assert.assertFalse(results.iterator().hasNext());\n+  }\n+\n   private Iterable<Row> runQuery(GroupByQuery query)\n   {\n     QueryToolChest<Row, GroupByQuery> toolChest = factory.getToolchest();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c88cb2af16db60ae9cbc0d89929bfc4610da5fce/processing/src/test/java/io/druid/query/groupby/GroupByQueryRunnerTest.java",
                "sha": "4fffcf67c015777c80eb063a2d4890b6ca59af0b",
                "status": "modified"
            }
        ],
        "message": "Fix NPE when subquery returns empty result",
        "parent": "https://github.com/apache/incubator-druid/commit/f2c1f798b9858cb96b3094529a8ec479e19dd01b",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexStorageAdapterTest.java"
        ]
    },
    "incubator-druid_c8cb96b": {
        "bug_id": "incubator-druid_c8cb96b",
        "commit": "https://github.com/apache/incubator-druid/commit/c8cb96b00654d9de38e9bf4e28675984b20ae57f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/group_by_query.body",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/examples/twitter/group_by_query.body?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 12,
                "filename": "examples/twitter/group_by_query.body",
                "patch": "@@ -2,19 +2,11 @@\n     \"queryType\": \"groupBy\",\n     \"dataSource\": \"twitterstream\",\n     \"granularity\": \"all\",\n-    \"dimensions\": [\"lang\"],\n+    \"dimensions\": [\"lang\", \"utc_offset\"],\n     \"aggregations\":[\n-    { \"type\": \"count\", \"name\": \"rows\"},\n-    { \"type\": \"doubleSum\", \"fieldName\": \"tweets\", \"name\": \"tweets\"},\n-\n-    { \"type\": \"max\", \"fieldName\": \"max_statuses_count\", \"name\": \"theMaxStatusesCount\"},\n-    { \"type\": \"max\", \"fieldName\": \"max_retweet_count\", \"name\": \"theMaxRetweetCount\"},\n-\n-    { \"type\": \"max\", \"fieldName\": \"max_friends_count\", \"name\": \"theMaxFriendsCount\"},\n-    { \"type\": \"max\", \"fieldName\": \"max_follower_count\", \"name\": \"theMaxFollowerCount\"},\n-\n-    { \"type\": \"doubleSum\", \"fieldName\": \"total_statuses_count\", \"name\": \"total_tweets_all_time\"}\n-\n+      { \"type\": \"count\", \"name\": \"rows\"},\n+      { \"type\": \"doubleSum\", \"fieldName\": \"tweets\", \"name\": \"tweets\"}\n     ],\n+    \"filter\": { \"type\": \"selector\", \"dimension\": \"lang\", \"value\": \"en\" },\n     \"intervals\":[\"2012-10-01T00:00/2020-01-01T00\"]\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/group_by_query.body",
                "sha": "e0607aa15543b1253936208e2918342cd3bbcd75",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/src/main/java/druid/examples/twitter/TwitterSpritzerFirehoseFactory.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/examples/twitter/src/main/java/druid/examples/twitter/TwitterSpritzerFirehoseFactory.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 21,
                "filename": "examples/twitter/src/main/java/druid/examples/twitter/TwitterSpritzerFirehoseFactory.java",
                "patch": "@@ -1,25 +1,34 @@\n package druid.examples.twitter;\n \n+import com.google.common.collect.Lists;\n import com.metamx.common.logger.Logger;\n import com.metamx.druid.input.InputRow;\n import com.metamx.druid.input.MapBasedInputRow;\n import com.metamx.druid.realtime.Firehose;\n import com.metamx.druid.realtime.FirehoseFactory;\n import org.codehaus.jackson.annotate.JsonCreator;\n-import org.codehaus.jackson.annotate.JsonTypeName;\n import org.codehaus.jackson.annotate.JsonProperty;\n-import org.codehaus.jackson.map.ObjectMapper;\n-import twitter4j.*;\n+import org.codehaus.jackson.annotate.JsonTypeName;\n+import twitter4j.ConnectionLifeCycleListener;\n+import twitter4j.HashtagEntity;\n+import twitter4j.Status;\n+import twitter4j.StatusDeletionNotice;\n+import twitter4j.StatusListener;\n+import twitter4j.TwitterStream;\n+import twitter4j.TwitterStreamFactory;\n+import twitter4j.User;\n \n import java.io.IOException;\n+import java.util.Arrays;\n import java.util.HashMap;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.TimeUnit;\n \n-import static java.lang.Thread.*;\n+import static java.lang.Thread.sleep;\n \n \n /**\n@@ -241,30 +250,26 @@ public InputRow nextRow()\n         } catch (InterruptedException e) {\n           throw new RuntimeException(\"InterruptedException\", e);\n         }\n-        //log.info(\"twitterStatus: \"+ status.getCreatedAt() + \" @\" + status.getUser().getScreenName() + \" - \" + status.getText());//DEBUG\n-\n-        // theMap.put(\"twid\", status.getUser().getScreenName());\n-        // theMap.put(\"msg\", status.getText());  // ToDo:  verify encoding\n \n         HashtagEntity[] hts = status.getHashtagEntities();\n         if (hts != null && hts.length > 0) {\n-          // ToDo: get all the hash tags instead of just the first one\n-          theMap.put(\"htags\", hts[0].getText());\n-        } else {\n-          theMap.put(\"htags\", null);\n+          List<String> hashTags = Lists.newArrayListWithExpectedSize(hts.length);\n+          for (HashtagEntity ht : hts) {\n+            hashTags.add(ht.getText());\n+          }\n+\n+          theMap.put(\"htags\", Arrays.asList(hashTags.get(0)));\n         }\n \n         long retweetCount = status.getRetweetCount();\n         theMap.put(\"retweet_count\", retweetCount);\n-        User u = status.getUser();\n-        if (u != null) {\n-          theMap.put(\"follower_count\", u.getFollowersCount());\n-          theMap.put(\"friends_count\", u.getFriendsCount());\n-          theMap.put(\"lang\", u.getLang());\n-          theMap.put(\"utc_offset\", u.getUtcOffset());  // resolution in seconds, -1 if not available?\n-          theMap.put(\"statuses_count\", u.getStatusesCount());\n-        } else {\n-          log.error(\"status.getUser() is null\");\n+        User user = status.getUser();\n+        if (user != null) {\n+          theMap.put(\"follower_count\", user.getFollowersCount());\n+          theMap.put(\"friends_count\", user.getFriendsCount());\n+          theMap.put(\"lang\", user.getLang());\n+          theMap.put(\"utc_offset\", user.getUtcOffset());  // resolution in seconds, -1 if not available?\n+          theMap.put(\"statuses_count\", user.getStatusesCount());\n         }\n \n         return new MapBasedInputRow(status.getCreatedAt().getTime(), dimensions, theMap);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/src/main/java/druid/examples/twitter/TwitterSpritzerFirehoseFactory.java",
                "sha": "992cd239487c7e5ea82b8b5bbd836b8d90dc2a32",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/twitter_realtime.spec",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/examples/twitter/twitter_realtime.spec?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 2,
                "filename": "examples/twitter/twitter_realtime.spec",
                "patch": "@@ -31,8 +31,8 @@\n \n     \"firehose\": {\n         \"type\": \"twitzer\",\n-        \"maxEventCount\": 50000,\n-        \"maxRunMinutes\": 10\n+        \"maxEventCount\": 500000,\n+        \"maxRunMinutes\": 120\n     },\n \n     \"plumber\": {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/examples/twitter/twitter_realtime.spec",
                "sha": "00b1707028d125bd642d46a4f655db39907ee699",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/QueryableIndex.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/QueryableIndex.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/QueryableIndex.java",
                "patch": "@@ -28,6 +28,7 @@\n public interface QueryableIndex extends ColumnSelector\n {\n   public Interval getDataInterval();\n+  public int getNumRows();\n   public Indexed<String> getColumnNames();\n   public Indexed<String> getAvailableDimensions();\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/QueryableIndex.java",
                "sha": "82cee9e54dd0ae2c8860c5516a5a0f2c9502c4f7",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/SimpleQueryableIndex.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/SimpleQueryableIndex.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/SimpleQueryableIndex.java",
                "patch": "@@ -56,6 +56,12 @@ public Interval getDataInterval()\n     return dataInterval;\n   }\n \n+  @Override\n+  public int getNumRows()\n+  {\n+    return timeColumn.getLength();\n+  }\n+\n   @Override\n   public Indexed<String> getColumnNames()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/SimpleQueryableIndex.java",
                "sha": "2f60b73adc63aeb34f3bc1960028ef7677bfc6b3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/Column.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/Column.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/Column.java",
                "patch": "@@ -25,6 +25,7 @@\n {\n   public ColumnCapabilities getCapabilities();\n \n+  public int getLength();\n   public DictionaryEncodedColumn getDictionaryEncoding();\n   public RunLengthColumn getRunLengthColumn();\n   public GenericColumn getGenericColumn();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/Column.java",
                "sha": "fa418a3398a126e6e8cc10301c44ab776260019e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumn.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/ComplexColumn.java",
                "patch": "@@ -26,5 +26,6 @@\n public interface ComplexColumn extends Closeable\n {\n   public Class<?> getClazz();\n+  public String getTypeName();\n   public Object getRowValue(int rowNum);\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumn.java",
                "sha": "f7cfb706e44108aa3bad290645afb6e4a05e031d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumnImpl.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumnImpl.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 2,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/ComplexColumnImpl.java",
                "patch": "@@ -29,10 +29,12 @@\n       .setType(ValueType.COMPLEX);\n \n   private final Indexed column;\n+  private final String typeName;\n \n-  public ComplexColumnImpl(Indexed column)\n+  public ComplexColumnImpl(String typeName, Indexed column)\n   {\n     this.column = column;\n+    this.typeName = typeName;\n   }\n \n   @Override\n@@ -41,9 +43,15 @@ public ColumnCapabilities getCapabilities()\n     return CAPABILITIES;\n   }\n \n+  @Override\n+  public int getLength()\n+  {\n+    return column.size();\n+  }\n+\n   @Override\n   public ComplexColumn getComplexColumn()\n   {\n-    return new IndexedComplexColumn(column);\n+    return new IndexedComplexColumn(typeName, column);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/ComplexColumnImpl.java",
                "sha": "46f665c57c665b97e1b8374bdc6ff2699b6f9c2c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/DictionaryEncodedColumn.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/DictionaryEncodedColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/DictionaryEncodedColumn.java",
                "patch": "@@ -26,6 +26,7 @@\n public interface DictionaryEncodedColumn\n {\n   public int size();\n+  public boolean hasMultipleValues();\n   public int getSingleValueRow(int rowNum);\n   public IndexedInts getMultiValueRow(int rowNum);\n   public String lookupName(int id);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/DictionaryEncodedColumn.java",
                "sha": "9301734f97034c75f34d2b3d2bbe8ae8d24ec603",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/FloatColumn.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/FloatColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 4,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/FloatColumn.java",
                "patch": "@@ -19,8 +19,7 @@\n \n package com.metamx.druid.index.column;\n \n-import com.google.common.base.Supplier;\n-import com.metamx.druid.kv.IndexedFloats;\n+import com.metamx.druid.index.v1.CompressedFloatsIndexedSupplier;\n \n /**\n  */\n@@ -29,9 +28,9 @@\n   private static final ColumnCapabilitiesImpl CAPABILITIES = new ColumnCapabilitiesImpl()\n       .setType(ValueType.FLOAT);\n \n-  private final Supplier<IndexedFloats> column;\n+  private final CompressedFloatsIndexedSupplier column;\n \n-  public FloatColumn(Supplier<IndexedFloats> column)\n+  public FloatColumn(CompressedFloatsIndexedSupplier column)\n   {\n     this.column = column;\n   }\n@@ -42,6 +41,12 @@ public ColumnCapabilities getCapabilities()\n     return CAPABILITIES;\n   }\n \n+  @Override\n+  public int getLength()\n+  {\n+    return column.size();\n+  }\n+\n   @Override\n   public GenericColumn getGenericColumn()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/FloatColumn.java",
                "sha": "44ffd7e970ab77536b1dc31862c06a5156d31aa7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/GenericColumn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/GenericColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/GenericColumn.java",
                "patch": "@@ -30,6 +30,8 @@\n public interface GenericColumn extends Closeable\n {\n   public int size();\n+  public ValueType getType();\n+  public boolean hasMultipleValues();\n \n   public String getStringSingleValueRow(int rowNum);\n   public Indexed<String> getStringMultiValueRow(int rowNum);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/GenericColumn.java",
                "sha": "c41b4906bd10308ab9157d43ac7bca04b9177434",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedComplexColumn.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/IndexedComplexColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 1,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/IndexedComplexColumn.java",
                "patch": "@@ -28,19 +28,27 @@\n public class IndexedComplexColumn implements ComplexColumn\n {\n   private final Indexed column;\n+  private final String typeName;\n \n   public IndexedComplexColumn(\n-      Indexed column\n+      String typeName, Indexed column\n   )\n   {\n     this.column = column;\n+    this.typeName = typeName;\n   }\n   @Override\n   public Class<?> getClazz()\n   {\n     return column.getClazz();\n   }\n \n+  @Override\n+  public String getTypeName()\n+  {\n+    return typeName;\n+  }\n+\n   @Override\n   public Object getRowValue(int rowNum)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedComplexColumn.java",
                "sha": "bafb6977dd0066748b9cb7bacd0fd3a32ab08e8a",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedFloatsGenericColumn.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/IndexedFloatsGenericColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/IndexedFloatsGenericColumn.java",
                "patch": "@@ -43,6 +43,18 @@ public int size()\n     return column.size();\n   }\n \n+  @Override\n+  public ValueType getType()\n+  {\n+    return ValueType.FLOAT;\n+  }\n+\n+  @Override\n+  public boolean hasMultipleValues()\n+  {\n+    return false;\n+  }\n+\n   @Override\n   public String getStringSingleValueRow(int rowNum)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedFloatsGenericColumn.java",
                "sha": "423b046e106a19329d550e89e6a0dc6d5e500cd1",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedLongsGenericColumn.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/IndexedLongsGenericColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/IndexedLongsGenericColumn.java",
                "patch": "@@ -43,6 +43,18 @@ public int size()\n     return column.size();\n   }\n \n+  @Override\n+  public ValueType getType()\n+  {\n+    return ValueType.LONG;\n+  }\n+\n+  @Override\n+  public boolean hasMultipleValues()\n+  {\n+    return false;\n+  }\n+\n   @Override\n   public String getStringSingleValueRow(int rowNum)\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/IndexedLongsGenericColumn.java",
                "sha": "0e96a63924b5a018d67156e99fa32a087ef84064",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/LongColumn.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/LongColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 4,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/LongColumn.java",
                "patch": "@@ -19,8 +19,7 @@\n \n package com.metamx.druid.index.column;\n \n-import com.google.common.base.Supplier;\n-import com.metamx.druid.kv.IndexedLongs;\n+import com.metamx.druid.index.v1.CompressedLongsIndexedSupplier;\n \n /**\n  */\n@@ -29,9 +28,9 @@\n   private static final ColumnCapabilitiesImpl CAPABILITIES = new ColumnCapabilitiesImpl()\n       .setType(ValueType.LONG);\n \n-  private final Supplier<IndexedLongs> column;\n+  private final CompressedLongsIndexedSupplier column;\n \n-  public LongColumn(Supplier<IndexedLongs> column)\n+  public LongColumn(CompressedLongsIndexedSupplier column)\n   {\n     this.column = column;\n   }\n@@ -42,6 +41,12 @@ public ColumnCapabilities getCapabilities()\n     return CAPABILITIES;\n   }\n \n+  @Override\n+  public int getLength()\n+  {\n+    return column.size();\n+  }\n+\n   @Override\n   public GenericColumn getGenericColumn()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/LongColumn.java",
                "sha": "1ec297ea61c7f4ebe7aa94864e3fbda95824935a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/SimpleColumn.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/SimpleColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/SimpleColumn.java",
                "patch": "@@ -20,6 +20,7 @@\n package com.metamx.druid.index.column;\n \n import com.google.common.base.Supplier;\n+import com.google.common.io.Closeables;\n \n /**\n  */\n@@ -55,6 +56,19 @@ public ColumnCapabilities getCapabilities()\n     return capabilities;\n   }\n \n+  @Override\n+  public int getLength()\n+  {\n+    GenericColumn column = null;\n+    try {\n+      column = genericColumn.get();\n+      return column.size();\n+    }\n+    finally {\n+      Closeables.closeQuietly(column);\n+    }\n+  }\n+\n   @Override\n   public DictionaryEncodedColumn getDictionaryEncoding()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/SimpleColumn.java",
                "sha": "a3884203227a72d57d9507973264c7ddba90d0ea",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/SimpleDictionaryEncodedColumn.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/SimpleDictionaryEncodedColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 1,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/SimpleDictionaryEncodedColumn.java",
                "patch": "@@ -46,7 +46,13 @@ public SimpleDictionaryEncodedColumn(\n   @Override\n   public int size()\n   {\n-    return column == null ? multiValueColumn.size() : column.size();\n+    return hasMultipleValues() ? multiValueColumn.size() : column.size();\n+  }\n+\n+  @Override\n+  public boolean hasMultipleValues()\n+  {\n+    return column == null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/SimpleDictionaryEncodedColumn.java",
                "sha": "7a28a53b0afd7145ca71c729a10b1801840e08b4",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/StringMultiValueColumn.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/column/StringMultiValueColumn.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/column/StringMultiValueColumn.java",
                "patch": "@@ -55,6 +55,12 @@ public ColumnCapabilities getCapabilities()\n     return CAPABILITIES;\n   }\n \n+  @Override\n+  public int getLength()\n+  {\n+    return column.size();\n+  }\n+\n   @Override\n   public DictionaryEncodedColumn getDictionaryEncoding()\n   {\n@@ -66,6 +72,12 @@ public int size()\n         return column.size();\n       }\n \n+      @Override\n+      public boolean hasMultipleValues()\n+      {\n+        return true;\n+      }\n+\n       @Override\n       public int getSingleValueRow(int rowNum)\n       {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/column/StringMultiValueColumn.java",
                "sha": "053bcee19564432e810d2dfc4003353bdd05ffc8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/serde/ComplexColumnPartSupplier.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/serde/ComplexColumnPartSupplier.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 3,
                "filename": "index-common/src/main/java/com/metamx/druid/index/serde/ComplexColumnPartSupplier.java",
                "patch": "@@ -32,8 +32,7 @@\n   private final String typeName;\n \n   public ComplexColumnPartSupplier(\n-      final GenericIndexed complexType,\n-      final String typeName\n+      final String typeName, final GenericIndexed complexType\n   ) {\n     this.complexType = complexType;\n     this.typeName = typeName;\n@@ -42,6 +41,6 @@ public ComplexColumnPartSupplier(\n   @Override\n   public ComplexColumn get()\n   {\n-    return new IndexedComplexColumn(complexType);\n+    return new IndexedComplexColumn(typeName, complexType);\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/serde/ComplexColumnPartSupplier.java",
                "sha": "4a4cfce4f77c1ba486c3e11b3989c19dc00cc026",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/CompressedFloatsIndexedSupplier.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/CompressedFloatsIndexedSupplier.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/CompressedFloatsIndexedSupplier.java",
                "patch": "@@ -59,6 +59,11 @@\n     this.baseFloatBuffers = baseFloatBuffers;\n   }\n \n+  public int size()\n+  {\n+    return totalSize;\n+  }\n+\n   @Override\n   public IndexedFloats get()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/CompressedFloatsIndexedSupplier.java",
                "sha": "1def2af031f289b580ddacbca39f6f51263a59f3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndex.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndex.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 1,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndex.java",
                "patch": "@@ -524,7 +524,7 @@ public DimDim()\n \n     public String get(String value)\n     {\n-      return poorMansInterning.get(value);\n+      return value == null ? null : poorMansInterning.get(value);\n     }\n \n     public int getId(String value)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/IncrementalIndex.java",
                "sha": "624d6d4b3755895ea3473001a4abe86a4a7c6674",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 6,
                "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "patch": "@@ -21,6 +21,7 @@\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n@@ -58,6 +59,7 @@\n import com.metamx.druid.kv.VSizeIndexed;\n import com.metamx.druid.kv.VSizeIndexedInts;\n import com.metamx.druid.utils.SerializerUtils;\n+import it.uniroma3.mat.extendedset.intset.ConciseSet;\n import it.uniroma3.mat.extendedset.intset.ImmutableConciseSet;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.joda.time.Interval;\n@@ -70,6 +72,7 @@\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n import java.util.AbstractList;\n+import java.util.Arrays;\n import java.util.LinkedHashSet;\n import java.util.List;\n import java.util.Map;\n@@ -99,7 +102,7 @@\n \n   private static final Logger log = new Logger(IndexIO.class);\n   private static final SerializerUtils serializerUtils = new SerializerUtils();\n-  private static final ByteOrder BYTE_ORDER = ByteOrder.nativeOrder();\n+  public static final ByteOrder BYTE_ORDER = ByteOrder.nativeOrder();\n \n   // This should really be provided by DI, should be changed once we switch around to using a DI framework\n   private static final ObjectMapper mapper = new DefaultObjectMapper();\n@@ -120,6 +123,7 @@ public static boolean canBeMapped(final File inDir) throws IOException\n     return handler.canBeMapped(inDir);\n   }\n \n+  @Deprecated\n   public static MMappedIndex mapDir(final File inDir) throws IOException\n   {\n     init();\n@@ -383,30 +387,79 @@ public static void convertV8toV9(File v8Dir, File v9Dir) throws IOException\n           serializerUtils.writeString(nameBAOS, dimension);\n           outParts.add(ByteBuffer.wrap(nameBAOS.toByteArray()));\n \n-          final GenericIndexed<String> dictionary = GenericIndexed.read(\n+          GenericIndexed<String> dictionary = GenericIndexed.read(\n               dimBuffer, GenericIndexed.stringStrategy\n           );\n+\n           VSizeIndexedInts singleValCol = null;\n           VSizeIndexed multiValCol = VSizeIndexed.readFromByteBuffer(dimBuffer.asReadOnlyBuffer());\n+          GenericIndexed<ImmutableConciseSet> bitmaps = bitmapIndexes.get(dimension);\n+\n           boolean onlyOneValue = true;\n-          for (VSizeIndexedInts rowValue : multiValCol) {\n+          ConciseSet nullsSet = null;\n+          for (int i = 0; i < multiValCol.size(); ++i) {\n+            VSizeIndexedInts rowValue = multiValCol.get(i);\n             if (!onlyOneValue) {\n               break;\n             }\n             if (rowValue.size() > 1) {\n               onlyOneValue = false;\n             }\n+            if (rowValue.size() == 0) {\n+              if (nullsSet == null) {\n+                nullsSet = new ConciseSet();\n+              }\n+              nullsSet.add(i);\n+            }\n           }\n \n           if (onlyOneValue) {\n+            log.info(\"Dimension[%s] is single value, converting...\", dimension);\n+            final boolean bumpedDictionary;\n+            if (nullsSet != null) {\n+              log.info(\"Dimension[%s] has null rows.\", dimension);\n+              final ImmutableConciseSet theNullSet = ImmutableConciseSet.newImmutableFromMutable(nullsSet);\n+\n+              if (dictionary.get(0) != null) {\n+                log.info(\"Dimension[%s] has no null value in the dictionary, expanding...\", dimension);\n+                bumpedDictionary = true;\n+                final List<String> nullList = Lists.newArrayList();\n+                nullList.add(null);\n+\n+                dictionary = GenericIndexed.fromIterable(\n+                    Iterables.concat(nullList, dictionary),\n+                    GenericIndexed.stringStrategy\n+                );\n+\n+                bitmaps = GenericIndexed.fromIterable(\n+                    Iterables.concat(Arrays.asList(theNullSet), bitmaps),\n+                    ConciseCompressedIndexedInts.objectStrategy\n+                );\n+              }\n+              else {\n+                bumpedDictionary = false;\n+                bitmaps = GenericIndexed.fromIterable(\n+                    Iterables.concat(\n+                        Arrays.asList(ImmutableConciseSet.union(theNullSet, bitmaps.get(0))),\n+                        Iterables.skip(bitmaps, 1)\n+                    ),\n+                    ConciseCompressedIndexedInts.objectStrategy\n+                );\n+              }\n+            }\n+            else {\n+              bumpedDictionary = false;\n+            }\n+\n             final VSizeIndexed finalMultiValCol = multiValCol;\n             singleValCol = VSizeIndexedInts.fromList(\n                 new AbstractList<Integer>()\n                 {\n                   @Override\n                   public Integer get(int index)\n                   {\n-                    return finalMultiValCol.get(index).get(0);\n+                    final VSizeIndexedInts ints = finalMultiValCol.get(index);\n+                    return ints.size() == 0 ? 0 : ints.get(0) + (bumpedDictionary ? 1 : 0);\n                   }\n \n                   @Override\n@@ -423,7 +476,7 @@ public int size()\n           }\n \n           builder.addSerde(\n-              new DictionaryEncodedColumnPartSerde(dictionary, singleValCol, multiValCol, bitmapIndexes.get(dimension))\n+              new DictionaryEncodedColumnPartSerde(dictionary, singleValCol, multiValCol, bitmaps)\n           );\n \n           final ColumnDescriptor serdeficator = builder.build();\n@@ -587,7 +640,7 @@ public QueryableIndex load(File inDir) throws IOException\n                   .setType(ValueType.COMPLEX)\n                   .setComplexColumn(\n                       new ComplexColumnPartSupplier(\n-                          (GenericIndexed) metricHolder.complexType, metricHolder.getTypeName()\n+                          metricHolder.getTypeName(), (GenericIndexed) metricHolder.complexType\n                       )\n                   )\n                   .build()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
                "sha": "afedbb6a7423cf2abcbb6cf715e2f07588e7a525",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/indexer/src/main/java/com/metamx/druid/indexer/IndexGeneratorJob.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexer/src/main/java/com/metamx/druid/indexer/IndexGeneratorJob.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 4,
                "filename": "indexer/src/main/java/com/metamx/druid/indexer/IndexGeneratorJob.java",
                "patch": "@@ -38,10 +38,10 @@\n import com.metamx.common.parsers.ParserUtils;\n import com.metamx.druid.aggregation.AggregatorFactory;\n import com.metamx.druid.client.DataSegment;\n+import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.index.v1.IncrementalIndex;\n import com.metamx.druid.index.v1.IndexIO;\n import com.metamx.druid.index.v1.IndexMerger;\n-import com.metamx.druid.index.v1.MMappedIndex;\n import com.metamx.druid.indexer.rollup.DataRollupSpec;\n import com.metamx.druid.input.MapBasedInputRow;\n import org.apache.commons.io.FileUtils;\n@@ -359,7 +359,7 @@ public void progress()\n \n       log.info(\"%,d lines completed.\", lineCount);\n \n-      List<MMappedIndex> indexes = Lists.newArrayListWithCapacity(indexCount);\n+      List<QueryableIndex> indexes = Lists.newArrayListWithCapacity(indexCount);\n       final File mergedBase;\n \n       if (toMerge.size() == 0) {\n@@ -389,9 +389,9 @@ public void progress()\n         toMerge.add(finalFile);\n \n         for (File file : toMerge) {\n-          indexes.add(IndexIO.mapDir(file));\n+          indexes.add(IndexIO.loadIndex(file));\n         }\n-        mergedBase = IndexMerger.mergeMMapped(\n+        mergedBase = IndexMerger.mergeQueryableIndex(\n             indexes, aggs, new File(baseFlushFile, \"merged\"), new IndexMerger.ProgressIndicator()\n         {\n           @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/indexer/src/main/java/com/metamx/druid/indexer/IndexGeneratorJob.java",
                "sha": "28dacd1ca9a941a10a59ecadc879e18d580a9e7f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/index/YeOldePlumberSchool.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/common/index/YeOldePlumberSchool.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 5,
                "filename": "merger/src/main/java/com/metamx/druid/merger/common/index/YeOldePlumberSchool.java",
                "patch": "@@ -27,16 +27,16 @@\n import com.metamx.common.logger.Logger;\n import com.metamx.druid.Query;\n import com.metamx.druid.client.DataSegment;\n+import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.index.v1.IndexIO;\n import com.metamx.druid.index.v1.IndexMerger;\n-import com.metamx.druid.index.v1.MMappedIndex;\n+import com.metamx.druid.loading.SegmentPusher;\n import com.metamx.druid.query.QueryRunner;\n import com.metamx.druid.realtime.FireDepartmentMetrics;\n import com.metamx.druid.realtime.FireHydrant;\n import com.metamx.druid.realtime.Plumber;\n import com.metamx.druid.realtime.PlumberSchool;\n import com.metamx.druid.realtime.Schema;\n-import com.metamx.druid.loading.SegmentPusher;\n import com.metamx.druid.realtime.Sink;\n import org.codehaus.jackson.annotate.JsonCreator;\n import org.codehaus.jackson.annotate.JsonProperty;\n@@ -129,13 +129,13 @@ public void finishJob()\n           } else if(spilled.size() == 1) {\n             fileToUpload = Iterables.getOnlyElement(spilled);\n           } else {\n-            List<MMappedIndex> indexes = Lists.newArrayList();\n+            List<QueryableIndex> indexes = Lists.newArrayList();\n             for (final File oneSpill : spilled) {\n-              indexes.add(IndexIO.mapDir(oneSpill));\n+              indexes.add(IndexIO.loadIndex(oneSpill));\n             }\n \n             fileToUpload = new File(tmpSegmentDir, \"merged\");\n-            IndexMerger.mergeMMapped(indexes, schema.getAggregators(), fileToUpload);\n+            IndexMerger.mergeQueryableIndex(indexes, schema.getAggregators(), fileToUpload);\n           }\n \n           final DataSegment segmentToUpload = theSink.getSegment().withVersion(version);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/index/YeOldePlumberSchool.java",
                "sha": "345af0207baee7333e68f33a616d2f8d34903213",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/task/AppendTask.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/common/task/AppendTask.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 3,
                "filename": "merger/src/main/java/com/metamx/druid/merger/common/task/AppendTask.java",
                "patch": "@@ -30,7 +30,7 @@\n import com.metamx.druid.index.v1.IndexIO;\n import com.metamx.druid.index.v1.IndexMerger;\n import com.metamx.druid.index.v1.IndexableAdapter;\n-import com.metamx.druid.index.v1.MMappedIndexAdapter;\n+import com.metamx.druid.index.v1.QueryableIndexIndexableAdapter;\n import com.metamx.druid.index.v1.Rowboat;\n import com.metamx.druid.index.v1.RowboatFilteringIndexAdapter;\n import org.codehaus.jackson.annotate.JsonCreator;\n@@ -90,8 +90,8 @@ public SegmentToMergeHolder apply(TimelineObjectHolder<String, DataSegment> inpu\n     for (final SegmentToMergeHolder holder : segmentsToMerge) {\n       adapters.add(\n           new RowboatFilteringIndexAdapter(\n-              new MMappedIndexAdapter(\n-                  IndexIO.mapDir(holder.getFile())\n+              new QueryableIndexIndexableAdapter(\n+                  IndexIO.loadIndex(holder.getFile())\n               ),\n               new Predicate<Rowboat>()\n               {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/task/AppendTask.java",
                "sha": "f1153e5c43c1d431e19ae3bea21eab27c404d07d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/task/DefaultMergeTask.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/merger/src/main/java/com/metamx/druid/merger/common/task/DefaultMergeTask.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 5,
                "filename": "merger/src/main/java/com/metamx/druid/merger/common/task/DefaultMergeTask.java",
                "patch": "@@ -25,9 +25,9 @@\n import com.google.common.collect.Lists;\n import com.metamx.druid.aggregation.AggregatorFactory;\n import com.metamx.druid.client.DataSegment;\n+import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.index.v1.IndexIO;\n import com.metamx.druid.index.v1.IndexMerger;\n-import com.metamx.druid.index.v1.MMappedIndex;\n import org.codehaus.jackson.annotate.JsonCreator;\n import org.codehaus.jackson.annotate.JsonProperty;\n \n@@ -57,16 +57,16 @@ public DefaultMergeTask(\n   public File merge(final Map<DataSegment, File> segments, final File outDir)\n       throws Exception\n   {\n-    return IndexMerger.mergeMMapped(\n+    return IndexMerger.mergeQueryableIndex(\n         Lists.transform(\n             ImmutableList.copyOf(segments.values()),\n-            new Function<File, MMappedIndex>()\n+            new Function<File, QueryableIndex>()\n             {\n               @Override\n-              public MMappedIndex apply(@Nullable File input)\n+              public QueryableIndex apply(@Nullable File input)\n               {\n                 try {\n-                  return IndexIO.mapDir(input);\n+                  return IndexIO.loadIndex(input);\n                 }\n                 catch (Exception e) {\n                   throw Throwables.propagate(e);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/merger/src/main/java/com/metamx/druid/merger/common/task/DefaultMergeTask.java",
                "sha": "e17db4b980e8085fbdb2007c970bd262272699d7",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/realtime/src/main/java/com/metamx/druid/realtime/RealtimePlumberSchool.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/realtime/src/main/java/com/metamx/druid/realtime/RealtimePlumberSchool.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 17,
                "filename": "realtime/src/main/java/com/metamx/druid/realtime/RealtimePlumberSchool.java",
                "patch": "@@ -308,30 +308,21 @@ public void doRun()\n \n                           final File mergedFile;\n                           try {\n-                            List<MMappedIndex> indexes = Lists.newArrayList();\n+                            List<QueryableIndex> indexes = Lists.newArrayList();\n                             for (FireHydrant fireHydrant : sink) {\n                               Segment segment = fireHydrant.getSegment();\n                               final QueryableIndex queryableIndex = segment.asQueryableIndex();\n-                              if (queryableIndex instanceof MMappedIndexQueryableIndex) {\n-                                log.info(\"Adding hydrant[%s]\", fireHydrant);\n-                                indexes.add(((MMappedIndexQueryableIndex) queryableIndex).getIndex());\n-                              }\n-                              else {\n-                                log.makeAlert(\"[%s] Failure to merge-n-push\", schema.getDataSource())\n-                                   .addData(\"type\", \"Unknown segment type\")\n-                                   .addData(\"adapterClass\", segment.getClass().toString())\n-                                   .emit();\n-                                return;\n-                              }\n+                              log.info(\"Adding hydrant[%s]\", fireHydrant);\n+                              indexes.add(queryableIndex);\n                             }\n \n-                            mergedFile = IndexMerger.mergeMMapped(\n+                            mergedFile = IndexMerger.mergeQueryableIndex(\n                                 indexes,\n                                 schema.getAggregators(),\n                                 new File(computePersistDir(schema, interval), \"merged\")\n                             );\n \n-                            MMappedIndex index = IndexIO.mapDir(mergedFile);\n+                            QueryableIndex index = IndexIO.loadIndex(mergedFile);\n \n                             DataSegment segment = segmentPusher.push(\n                                 mergedFile,\n@@ -503,9 +494,7 @@ private int persistHydrant(FireHydrant indexToPersist, Schema schema, Interval i\n           new File(computePersistDir(schema, interval), String.valueOf(indexToPersist.getCount()))\n       );\n \n-      indexToPersist.swapSegment(\n-          new QueryableIndexSegment(null, new MMappedIndexQueryableIndex(IndexIO.mapDir(persistedFile)))\n-      );\n+      indexToPersist.swapSegment(new QueryableIndexSegment(null, IndexIO.loadIndex(persistedFile)));\n \n       return numRows;\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/realtime/src/main/java/com/metamx/druid/realtime/RealtimePlumberSchool.java",
                "sha": "02bd6acbb4c72235d5bda188dfb98d64685a6e3d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 12,
                "filename": "server/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "patch": "@@ -19,15 +19,13 @@\n \n package com.metamx.druid.index.v1;\n \n-import com.google.common.io.ByteStreams;\n import com.google.common.io.Files;\n import com.metamx.druid.index.v1.serde.ComplexMetricSerde;\n import com.metamx.druid.kv.FlattenedArrayWriter;\n import com.metamx.druid.kv.IOPeon;\n \n import java.io.File;\n import java.io.IOException;\n-import java.nio.ByteOrder;\n \n /**\n  */\n@@ -75,18 +73,12 @@ public void close() throws IOException\n   {\n     writer.close();\n \n-    final File littleEndianFile = IndexIO.makeMetricFile(outDir, metricName, ByteOrder.LITTLE_ENDIAN);\n-    littleEndianFile.delete();\n+    final File outFile = IndexIO.makeMetricFile(outDir, metricName, IndexIO.BYTE_ORDER);\n+    outFile.delete();\n     MetricHolder.writeComplexMetric(\n-        Files.newOutputStreamSupplier(littleEndianFile, true), metricName, serde.getTypeName(), writer\n-    );\n-    IndexIO.checkFileSize(littleEndianFile);\n-\n-    final File bigEndianFile = IndexIO.makeMetricFile(outDir, metricName, ByteOrder.BIG_ENDIAN);\n-    ByteStreams.copy(\n-        Files.newInputStreamSupplier(littleEndianFile),\n-        Files.newOutputStreamSupplier(bigEndianFile, false)\n+        Files.newOutputStreamSupplier(outFile, true), metricName, serde.getTypeName(), writer\n     );\n+    IndexIO.checkFileSize(outFile);\n \n     writer = null;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/ComplexMetricColumnSerializer.java",
                "sha": "d09581c1c5b71b4c6c07a8897902bbcc42dcf32c",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 25,
                "filename": "server/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "patch": "@@ -24,7 +24,6 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.nio.ByteOrder;\n \n /**\n  */\n@@ -34,8 +33,7 @@\n   private final IOPeon ioPeon;\n   private final File outDir;\n \n-  private CompressedFloatsSupplierSerializer littleMetricsWriter;\n-  private CompressedFloatsSupplierSerializer bigEndianMetricsWriter;\n+  private CompressedFloatsSupplierSerializer writer;\n \n   public FloatMetricColumnSerializer(\n       String metricName,\n@@ -51,43 +49,30 @@ public FloatMetricColumnSerializer(\n   @Override\n   public void open() throws IOException\n   {\n-    littleMetricsWriter = CompressedFloatsSupplierSerializer.create(\n-        ioPeon, String.format(\"%s_little\", metricName), ByteOrder.LITTLE_ENDIAN\n-    );\n-    bigEndianMetricsWriter = CompressedFloatsSupplierSerializer.create(\n-        ioPeon, String.format(\"%s_big\", metricName), ByteOrder.BIG_ENDIAN\n+    writer = CompressedFloatsSupplierSerializer.create(\n+        ioPeon, String.format(\"%s_little\", metricName), IndexIO.BYTE_ORDER\n     );\n \n-    littleMetricsWriter.open();\n-    bigEndianMetricsWriter.open();\n+    writer.open();\n   }\n \n   @Override\n   public void serialize(Object obj) throws IOException\n   {\n     float val = (obj == null) ? 0 : ((Number) obj).floatValue();\n-    littleMetricsWriter.add(val);\n-    bigEndianMetricsWriter.add(val);\n+    writer.add(val);\n   }\n \n   @Override\n   public void close() throws IOException\n   {\n-    final File littleEndianFile = IndexIO.makeMetricFile(outDir, metricName, ByteOrder.LITTLE_ENDIAN);\n-    littleEndianFile.delete();\n-    MetricHolder.writeFloatMetric(\n-        Files.newOutputStreamSupplier(littleEndianFile, true), metricName, littleMetricsWriter\n-    );\n-    IndexIO.checkFileSize(littleEndianFile);\n-\n-    final File bigEndianFile = IndexIO.makeMetricFile(outDir, metricName, ByteOrder.BIG_ENDIAN);\n-    bigEndianFile.delete();\n+    final File outFile = IndexIO.makeMetricFile(outDir, metricName, IndexIO.BYTE_ORDER);\n+    outFile.delete();\n     MetricHolder.writeFloatMetric(\n-        Files.newOutputStreamSupplier(bigEndianFile, true), metricName, bigEndianMetricsWriter\n+        Files.newOutputStreamSupplier(outFile, true), metricName, writer\n     );\n-    IndexIO.checkFileSize(bigEndianFile);\n+    IndexIO.checkFileSize(outFile);\n \n-    littleMetricsWriter = null;\n-    bigEndianMetricsWriter = null;\n+    writer = null;\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/FloatMetricColumnSerializer.java",
                "sha": "20ec5a4d30d310ba78fd4c1afba23235c08338b6",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/v1/IndexMerger.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 33,
                "filename": "server/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "patch": "@@ -44,6 +44,7 @@\n import com.metamx.druid.aggregation.ToLowerCaseAggregatorFactory;\n import com.metamx.druid.guava.FileOutputSupplier;\n import com.metamx.druid.guava.GuavaUtils;\n+import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.index.v1.serde.ComplexMetricSerde;\n import com.metamx.druid.index.v1.serde.ComplexMetrics;\n import com.metamx.druid.kv.ConciseCompressedIndexedInts;\n@@ -75,6 +76,7 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Iterator;\n+import java.util.LinkedHashSet;\n import java.util.List;\n import java.util.Map;\n import java.util.TreeSet;\n@@ -139,26 +141,26 @@ public static File persist(\n     );\n   }\n \n-  public static File mergeMMapped(\n-      List<MMappedIndex> indexes, final AggregatorFactory[] metricAggs, File outDir\n+  public static File mergeQueryableIndex(\n+      List<QueryableIndex> indexes, final AggregatorFactory[] metricAggs, File outDir\n   ) throws IOException\n   {\n-    return mergeMMapped(indexes, metricAggs, outDir, new NoopProgressIndicator());\n+    return mergeQueryableIndex(indexes, metricAggs, outDir, new NoopProgressIndicator());\n   }\n \n-  public static File mergeMMapped(\n-      List<MMappedIndex> indexes, final AggregatorFactory[] metricAggs, File outDir, ProgressIndicator progress\n+  public static File mergeQueryableIndex(\n+      List<QueryableIndex> indexes, final AggregatorFactory[] metricAggs, File outDir, ProgressIndicator progress\n   ) throws IOException\n   {\n     return merge(\n         Lists.transform(\n             indexes,\n-            new Function<MMappedIndex, IndexableAdapter>()\n+            new Function<QueryableIndex, IndexableAdapter>()\n             {\n               @Override\n-              public IndexableAdapter apply(@Nullable final MMappedIndex input)\n+              public IndexableAdapter apply(final QueryableIndex input)\n               {\n-                return new MMappedIndexAdapter(input);\n+                return new QueryableIndexIndexableAdapter(input);\n               }\n             }\n         ),\n@@ -392,6 +394,7 @@ private static File makeIndexFiles(\n     }\n     final Interval dataInterval;\n     File v8OutDir = new File(outDir, \"v8-tmp\");\n+    v8OutDir.mkdirs();\n \n     /*************  Main index.drd file **************/\n     progress.progress();\n@@ -573,15 +576,11 @@ public Rowboat apply(@Nullable Rowboat input)\n \n     Iterable<Rowboat> theRows = rowMergerFn.apply(boats);\n \n-    CompressedLongsSupplierSerializer littleEndianTimeWriter = CompressedLongsSupplierSerializer.create(\n-        ioPeon, \"little_end_time\", ByteOrder.LITTLE_ENDIAN\n-    );\n-    CompressedLongsSupplierSerializer bigEndianTimeWriter = CompressedLongsSupplierSerializer.create(\n-        ioPeon, \"big_end_time\", ByteOrder.BIG_ENDIAN\n+    CompressedLongsSupplierSerializer timeWriter = CompressedLongsSupplierSerializer.create(\n+        ioPeon, \"little_end_time\", IndexIO.BYTE_ORDER\n     );\n \n-    littleEndianTimeWriter.open();\n-    bigEndianTimeWriter.open();\n+    timeWriter.open();\n \n     ArrayList<VSizeIndexedWriter> forwardDimWriters = Lists.newArrayListWithCapacity(mergedDimensions.size());\n     for (String dimension : mergedDimensions) {\n@@ -621,8 +620,7 @@ public Rowboat apply(@Nullable Rowboat input)\n \n     for (Rowboat theRow : theRows) {\n       progress.progress();\n-      littleEndianTimeWriter.add(theRow.getTimestamp());\n-      bigEndianTimeWriter.add(theRow.getTimestamp());\n+      timeWriter.add(theRow.getTimestamp());\n \n       final Object[] metrics = theRow.getMetrics();\n       for (int i = 0; i < metrics.length; ++i) {\n@@ -660,17 +658,11 @@ public Rowboat apply(@Nullable Rowboat input)\n       rowNumConversion.rewind();\n     }\n \n-    final File littleEndianFile = IndexIO.makeTimeFile(v8OutDir, ByteOrder.LITTLE_ENDIAN);\n-    littleEndianFile.delete();\n-    OutputSupplier<FileOutputStream> out = Files.newOutputStreamSupplier(littleEndianFile, true);\n-    littleEndianTimeWriter.closeAndConsolidate(out);\n-    IndexIO.checkFileSize(littleEndianFile);\n-\n-    final File bigEndianFile = IndexIO.makeTimeFile(v8OutDir, ByteOrder.BIG_ENDIAN);\n-    bigEndianFile.delete();\n-    out = Files.newOutputStreamSupplier(bigEndianFile, true);\n-    bigEndianTimeWriter.closeAndConsolidate(out);\n-    IndexIO.checkFileSize(bigEndianFile);\n+    final File timeFile = IndexIO.makeTimeFile(v8OutDir, IndexIO.BYTE_ORDER);\n+    timeFile.delete();\n+    OutputSupplier<FileOutputStream> out = Files.newOutputStreamSupplier(timeFile, true);\n+    timeWriter.closeAndConsolidate(out);\n+    IndexIO.checkFileSize(timeFile);\n \n     for (int i = 0; i < mergedDimensions.size(); ++i) {\n       forwardDimWriters.get(i).close();\n@@ -746,11 +738,12 @@ public Rowboat apply(@Nullable Rowboat input)\n     final ArrayList<String> expectedFiles = Lists.newArrayList(\n         Iterables.concat(\n             Arrays.asList(\n-                \"index.drd\", \"inverted.drd\", \"time_BIG_ENDIAN.drd\", \"time_LITTLE_ENDIAN.drd\"\n+                \"index.drd\", \"inverted.drd\", String.format(\"time_%s.drd\", IndexIO.BYTE_ORDER)\n             ),\n             Iterables.transform(mergedDimensions, GuavaUtils.formatFunction(\"dim_%s.drd\")),\n-            Iterables.transform(mergedMetrics, GuavaUtils.formatFunction(\"met_%s_LITTLE_ENDIAN.drd\")),\n-            Iterables.transform(mergedMetrics, GuavaUtils.formatFunction(\"met_%s_BIG_ENDIAN.drd\"))\n+            Iterables.transform(\n+                mergedMetrics, GuavaUtils.formatFunction(String.format(\"met_%%s_%s.drd\", IndexIO.BYTE_ORDER))\n+            )\n         )\n     );\n \n@@ -791,11 +784,13 @@ public Rowboat apply(@Nullable Rowboat input)\n \n   private static <T extends Comparable> ArrayList<T> mergeIndexed(final List<Iterable<T>> indexedLists)\n   {\n-    TreeSet<T> retVal = Sets.newTreeSet(Ordering.<T>natural().nullsFirst());\n+    LinkedHashSet<T> retVal = Sets.newLinkedHashSet();\n \n     for (Iterable<T> indexedList : indexedLists) {\n       for (T val : indexedList) {\n-        retVal.add(val);\n+        if (val != null) {\n+          retVal.add(val);\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/IndexMerger.java",
                "sha": "4eb8db5fcc7c915767eca591b29570b53afc8c0c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/MMappedIndexQueryableIndex.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/v1/MMappedIndexQueryableIndex.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 1,
                "filename": "server/src/main/java/com/metamx/druid/index/v1/MMappedIndexQueryableIndex.java",
                "patch": "@@ -53,6 +53,12 @@ public Interval getDataInterval()\n     return index.getDataInterval();\n   }\n \n+  @Override\n+  public int getNumRows()\n+  {\n+    return index.getTimestamps().size();\n+  }\n+\n   @Override\n   public Indexed<String> getColumnNames()\n   {\n@@ -91,7 +97,7 @@ else if (metricHolder.getType() == MetricHolder.MetricType.FLOAT) {\n       return new FloatColumn(metricHolder.floatType);\n     }\n     else {\n-      return new ComplexColumnImpl(metricHolder.getComplexType());\n+      return new ComplexColumnImpl(metricHolder.getTypeName(), metricHolder.getComplexType());\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/MMappedIndexQueryableIndex.java",
                "sha": "3c9d62d677604a1db9bf8c6cb7487d217f647ed8",
                "status": "modified"
            },
            {
                "additions": 267,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "changes": 267,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 0,
                "filename": "server/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "patch": "@@ -0,0 +1,267 @@\n+/*\n+ * Druid - a distributed column store.\n+ * Copyright (C) 2012  Metamarkets Group Inc.\n+ *\n+ * This program is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU General Public License\n+ * as published by the Free Software Foundation; either version 2\n+ * of the License, or (at your option) any later version.\n+ *\n+ * This program is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU General Public License\n+ * along with this program; if not, write to the Free Software\n+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n+ */\n+\n+package com.metamx.druid.index.v1;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.common.io.Closeables;\n+import com.metamx.common.ISE;\n+import com.metamx.druid.index.QueryableIndex;\n+import com.metamx.druid.index.column.Column;\n+import com.metamx.druid.index.column.ComplexColumn;\n+import com.metamx.druid.index.column.DictionaryEncodedColumn;\n+import com.metamx.druid.index.column.GenericColumn;\n+import com.metamx.druid.index.column.ValueType;\n+import com.metamx.druid.kv.ArrayBasedIndexedInts;\n+import com.metamx.druid.kv.ConciseCompressedIndexedInts;\n+import com.metamx.druid.kv.Indexed;\n+import com.metamx.druid.kv.IndexedInts;\n+import com.metamx.druid.kv.IndexedIterable;\n+import com.metamx.druid.kv.ListIndexed;\n+import org.joda.time.Interval;\n+\n+import java.io.Closeable;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Set;\n+\n+/**\n+*/\n+public class QueryableIndexIndexableAdapter implements IndexableAdapter\n+{\n+  private final int numRows;\n+  private final QueryableIndex input;\n+\n+  public QueryableIndexIndexableAdapter(QueryableIndex input)\n+  {\n+    this.input = input;\n+    numRows = input.getNumRows();\n+  }\n+\n+  @Override\n+  public Interval getDataInterval()\n+  {\n+    return input.getDataInterval();\n+  }\n+\n+  @Override\n+  public int getNumRows()\n+  {\n+    return numRows;\n+  }\n+\n+  @Override\n+  public Indexed<String> getAvailableDimensions()\n+  {\n+    return input.getAvailableDimensions();\n+  }\n+\n+  @Override\n+  public Indexed<String> getAvailableMetrics()\n+  {\n+    final Set<String> columns = Sets.newLinkedHashSet(input.getColumnNames());\n+    final HashSet<String> dimensions = Sets.newHashSet(getAvailableDimensions());\n+\n+    return new ListIndexed<String>(\n+        Lists.newArrayList(Sets.difference(columns, dimensions)),\n+        String.class\n+    );\n+  }\n+\n+  @Override\n+  public Indexed<String> getDimValueLookup(String dimension)\n+  {\n+    final DictionaryEncodedColumn dict = input.getColumn(dimension).getDictionaryEncoding();\n+    return new Indexed<String>()\n+    {\n+      @Override\n+      public Class<? extends String> getClazz()\n+      {\n+        return String.class;\n+      }\n+\n+      @Override\n+      public int size()\n+      {\n+        return dict.getCardinality();\n+      }\n+\n+      @Override\n+      public String get(int index)\n+      {\n+        return dict.lookupName(index);\n+      }\n+\n+      @Override\n+      public int indexOf(String value)\n+      {\n+        return dict.lookupId(value);\n+      }\n+\n+      @Override\n+      public Iterator<String> iterator()\n+      {\n+        return IndexedIterable.create(this).iterator();\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public Iterable<Rowboat> getRows()\n+  {\n+    return new Iterable<Rowboat>()\n+    {\n+      @Override\n+      public Iterator<Rowboat> iterator()\n+      {\n+        return new Iterator<Rowboat>()\n+        {\n+          final GenericColumn timestamps = input.getTimeColumn().getGenericColumn();\n+          final Object[] metrics;\n+          final Map<String, DictionaryEncodedColumn> dimensions;\n+\n+          final int numMetrics = getAvailableMetrics().size();\n+\n+          int currRow = 0;\n+          boolean done = false;\n+\n+          {\n+            dimensions = Maps.newLinkedHashMap();\n+            for (String dim : input.getAvailableDimensions()) {\n+              dimensions.put(dim, input.getColumn(dim).getDictionaryEncoding());\n+            }\n+\n+            final Indexed<String> availableMetrics = getAvailableMetrics();\n+            metrics = new Object[availableMetrics.size()];\n+            for (int i = 0; i < metrics.length; ++i) {\n+              final Column column = input.getColumn(availableMetrics.get(i));\n+              final ValueType type = column.getCapabilities().getType();\n+              switch (type) {\n+                case FLOAT:\n+                  metrics[i] = column.getGenericColumn();\n+                  break;\n+                case COMPLEX:\n+                  metrics[i] = column.getComplexColumn();\n+                  break;\n+                default:\n+                  throw new ISE(\"Cannot handle type[%s]\", type);\n+              }\n+            }\n+          }\n+\n+          @Override\n+          public boolean hasNext()\n+          {\n+            final boolean hasNext = currRow < numRows;\n+            if (!hasNext && !done) {\n+              Closeables.closeQuietly(timestamps);\n+              for (Object metric : metrics) {\n+                if (metric instanceof Closeable) {\n+                  Closeables.closeQuietly((Closeable) metric);\n+                }\n+              }\n+              done = true;\n+            }\n+            return hasNext;\n+          }\n+\n+          @Override\n+          public Rowboat next()\n+          {\n+            if (!hasNext()) {\n+              throw new NoSuchElementException();\n+            }\n+\n+            int[][] dims = new int[dimensions.size()][];\n+            int dimIndex = 0;\n+            for (String dim : dimensions.keySet()) {\n+              final DictionaryEncodedColumn dict = dimensions.get(dim);\n+              final IndexedInts dimVals;\n+              if (dict.hasMultipleValues()) {\n+                dimVals = dict.getMultiValueRow(currRow);\n+              }\n+              else {\n+                dimVals = new ArrayBasedIndexedInts(new int[]{dict.getSingleValueRow(currRow)});\n+              }\n+\n+              int[] theVals = new int[dimVals.size()];\n+              for (int j = 0; j < theVals.length; ++j) {\n+                theVals[j] = dimVals.get(j);\n+              }\n+\n+              dims[dimIndex++] = theVals;\n+            }\n+\n+            Object[] metricArray = new Object[numMetrics];\n+            for (int i = 0; i < metricArray.length; ++i) {\n+              if (metrics[i] instanceof GenericColumn) {\n+                metricArray[i] = ((GenericColumn) metrics[i]).getFloatSingleValueRow(currRow);\n+              }\n+              else if (metrics[i] instanceof ComplexColumn) {\n+                metricArray[i] = ((ComplexColumn) metrics[i]).getRowValue(currRow);\n+              }\n+            }\n+\n+            final Rowboat retVal = new Rowboat(\n+                timestamps.getLongSingleValueRow(currRow), dims, metricArray, currRow\n+            );\n+\n+            ++currRow;\n+\n+            return retVal;\n+          }\n+\n+          @Override\n+          public void remove()\n+          {\n+            throw new UnsupportedOperationException();\n+          }\n+        };\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public IndexedInts getInverteds(String dimension, String value)\n+  {\n+    return new ConciseCompressedIndexedInts(\n+        input.getColumn(dimension).getBitmapIndex().getConciseSet(value)\n+    );\n+  }\n+\n+  @Override\n+  public String getMetricType(String metric)\n+  {\n+    final Column column = input.getColumn(metric);\n+\n+    final ValueType type = column.getCapabilities().getType();\n+    switch (type) {\n+      case FLOAT:\n+        return \"float\";\n+      case COMPLEX:\n+        return column.getComplexColumn().getTypeName();\n+      default:\n+        throw new ISE(\"Unknown type[%s]\", type);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/index/v1/QueryableIndexIndexableAdapter.java",
                "sha": "72eddb9141ca7716a57fb2cb03e08241445dcf70",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/query/group/GroupByQueryEngine.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/com/metamx/druid/query/group/GroupByQueryEngine.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 1,
                "filename": "server/src/main/java/com/metamx/druid/query/group/GroupByQueryEngine.java",
                "patch": "@@ -277,7 +277,10 @@ public RowIterator(GroupByQuery query, Cursor cursor, ByteBuffer metricsBuffer)\n       dimNames = new String[dimensionSpecs.size()];\n       for (int i = 0; i < dimensionSpecs.size(); ++i) {\n         final DimensionSpec dimSpec = dimensionSpecs.get(i);\n-        dimensions.add(cursor.makeDimensionSelector(dimSpec.getDimension()));\n+        final DimensionSelector selector = cursor.makeDimensionSelector(dimSpec.getDimension());\n+        if (selector != null) {\n+          dimensions.add(selector);\n+        }\n         dimNames[i] = dimSpec.getOutputName();\n       }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/main/java/com/metamx/druid/query/group/GroupByQueryEngine.java",
                "sha": "788e1adb02a3cfe856001a8e9fdd4c54bb8b0f1f",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/EmptyIndexTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/com/metamx/druid/index/v1/EmptyIndexTest.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 6,
                "filename": "server/src/test/java/com/metamx/druid/index/v1/EmptyIndexTest.java",
                "patch": "@@ -23,12 +23,12 @@\n import com.google.common.collect.Lists;\n import com.metamx.druid.QueryGranularity;\n import com.metamx.druid.aggregation.AggregatorFactory;\n+import com.metamx.druid.index.QueryableIndex;\n import org.joda.time.Interval;\n import org.junit.Assert;\n import org.junit.Test;\n \n import java.io.File;\n-import java.util.ArrayList;\n \n public class EmptyIndexTest\n {\n@@ -48,11 +48,11 @@ public void testEmptyIndex() throws Exception\n     IncrementalIndexAdapter emptyIndexAdapter = new IncrementalIndexAdapter(new Interval(\"2012-08-01/P3D\"), emptyIndex);\n     IndexMerger.merge(Lists.<IndexableAdapter>newArrayList(emptyIndexAdapter), new AggregatorFactory[0], tmpDir);\n \n-    MMappedIndex emptyIndexMMapped = IndexIO.mapDir(tmpDir);\n+    QueryableIndex emptyQueryableIndex = IndexIO.loadIndex(tmpDir);\n \n-    Assert.assertEquals(\"getAvailableDimensions\", 0, Iterables.size(emptyIndexMMapped.getAvailableDimensions()));\n-    Assert.assertEquals(\"getAvailableMetrics\", 0, Iterables.size(emptyIndexMMapped.getAvailableMetrics()));\n-    Assert.assertEquals(\"getDataInterval\", new Interval(\"2012-08-01/P3D\"), emptyIndexMMapped.getDataInterval());\n-    Assert.assertEquals(\"getReadOnlyTimestamps\", 0, emptyIndexMMapped.getReadOnlyTimestamps().size());\n+    Assert.assertEquals(\"getAvailableDimensions\", 0, Iterables.size(emptyQueryableIndex.getAvailableDimensions()));\n+    Assert.assertEquals(\"getAvailableMetrics\", 0, Iterables.size(emptyQueryableIndex.getColumnNames()));\n+    Assert.assertEquals(\"getDataInterval\", new Interval(\"2012-08-01/P3D\"), emptyQueryableIndex.getDataInterval());\n+    Assert.assertEquals(\"getReadOnlyTimestamps\", 0, emptyQueryableIndex.getTimeColumn().getLength());\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/EmptyIndexTest.java",
                "sha": "55c4b7be6e548765fce3c518210ece21db7d1686",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/IndexMergerTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/com/metamx/druid/index/v1/IndexMergerTest.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 13,
                "filename": "server/src/test/java/com/metamx/druid/index/v1/IndexMergerTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.io.Files;\n import com.metamx.druid.QueryGranularity;\n import com.metamx.druid.aggregation.AggregatorFactory;\n+import com.metamx.druid.index.QueryableIndex;\n import com.metamx.druid.input.MapBasedInputRow;\n import junit.framework.Assert;\n import org.apache.commons.io.FileUtils;\n@@ -44,11 +45,11 @@ public void testPersistCaseInsensitive() throws Exception\n \n     final File tempDir = Files.createTempDir();\n     try {\n-      MMappedIndex index = IndexIO.mapDir(IndexMerger.persist(toPersist, tempDir));\n+      QueryableIndex index = IndexIO.loadIndex(IndexMerger.persist(toPersist, tempDir));\n \n-      Assert.assertEquals(2, index.getTimestamps().size());\n+      Assert.assertEquals(2, index.getTimeColumn().getLength());\n       Assert.assertEquals(Arrays.asList(\"dim1\", \"dim2\"), Lists.newArrayList(index.getAvailableDimensions()));\n-      Assert.assertEquals(0, index.getAvailableMetrics().size());\n+      Assert.assertEquals(2, index.getColumnNames().size());\n     }\n     finally {\n       tempDir.delete();\n@@ -84,25 +85,25 @@ public void testPersistMergeCaseInsensitive() throws Exception\n     final File tempDir2 = Files.createTempDir();\n     final File mergedDir = Files.createTempDir();\n     try {\n-      MMappedIndex index1 = IndexIO.mapDir(IndexMerger.persist(toPersist1, tempDir1));\n+      QueryableIndex index1 = IndexIO.loadIndex(IndexMerger.persist(toPersist1, tempDir1));\n \n-      Assert.assertEquals(2, index1.getTimestamps().size());\n+      Assert.assertEquals(2, index1.getTimeColumn().getLength());\n       Assert.assertEquals(Arrays.asList(\"dim1\", \"dim2\"), Lists.newArrayList(index1.getAvailableDimensions()));\n-      Assert.assertEquals(0, index1.getAvailableMetrics().size());\n+      Assert.assertEquals(2, index1.getColumnNames().size());\n \n-      MMappedIndex index2 = IndexIO.mapDir(IndexMerger.persist(toPersist2, tempDir2));\n+      QueryableIndex index2 = IndexIO.loadIndex(IndexMerger.persist(toPersist2, tempDir2));\n \n-      Assert.assertEquals(2, index2.getTimestamps().size());\n+      Assert.assertEquals(2, index2.getTimeColumn().getLength());\n       Assert.assertEquals(Arrays.asList(\"dim1\", \"dim2\"), Lists.newArrayList(index2.getAvailableDimensions()));\n-      Assert.assertEquals(0, index2.getAvailableMetrics().size());\n+      Assert.assertEquals(2, index2.getColumnNames().size());\n \n-      MMappedIndex merged = IndexIO.mapDir(\n-          IndexMerger.mergeMMapped(Arrays.asList(index1, index2), new AggregatorFactory[]{}, mergedDir)\n+      QueryableIndex merged = IndexIO.loadIndex(\n+          IndexMerger.mergeQueryableIndex(Arrays.asList(index1, index2), new AggregatorFactory[]{}, mergedDir)\n       );\n \n-      Assert.assertEquals(3, merged.getTimestamps().size());\n+      Assert.assertEquals(3, merged.getTimeColumn().getLength());\n       Assert.assertEquals(Arrays.asList(\"dim1\", \"dim2\"), Lists.newArrayList(merged.getAvailableDimensions()));\n-      Assert.assertEquals(0, merged.getAvailableMetrics().size());\n+      Assert.assertEquals(2, merged.getColumnNames().size());\n     }\n     finally {\n       FileUtils.deleteQuietly(tempDir1);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/IndexMergerTest.java",
                "sha": "097762106a2adc017f8821b21b13822fe3795882",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/TestIndex.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/com/metamx/druid/index/v1/TestIndex.java?ref=c8cb96b00654d9de38e9bf4e28675984b20ae57f",
                "deletions": 2,
                "filename": "server/src/test/java/com/metamx/druid/index/v1/TestIndex.java",
                "patch": "@@ -119,8 +119,8 @@ public static QueryableIndex mergedRealtimeIndex()\n         IndexMerger.persist(bottom, DATA_INTERVAL, bottomFile);\n \n         mergedRealtime = IndexIO.loadIndex(\n-            IndexMerger.mergeMMapped(\n-                Arrays.asList(IndexIO.mapDir(topFile), IndexIO.mapDir(bottomFile)),\n+            IndexMerger.mergeQueryableIndex(\n+                Arrays.asList(IndexIO.loadIndex(topFile), IndexIO.loadIndex(bottomFile)),\n                 METRIC_AGGS,\n                 mergedFile\n             )",
                "raw_url": "https://github.com/apache/incubator-druid/raw/c8cb96b00654d9de38e9bf4e28675984b20ae57f/server/src/test/java/com/metamx/druid/index/v1/TestIndex.java",
                "sha": "164c18a13fc1690fe37e426b5b38e613cf6bad37",
                "status": "modified"
            }
        ],
        "message": "1) Remove vast majority of usages of IndexIO.mapDir() and deprecated it.  IndexIO.loadIndex() is the new IndexIO.mapDir()\n2) Fix bug with IndexMerger and null columns\n3) Add QueryableIndexIndexableAdapter so that QueryableIndexes can be merged\n4) Adjust twitter example to have multiple values for each hash tag\n5) Adjusted GroupByQueryEngine to just drop dimensions that don't exist instead of throwing an NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/538d00e75e73e776964b6df1e893c0d8a49ed50e",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexTest.java",
            "IndexIOTest.java",
            "IndexGeneratorJobTest.java",
            "RealtimePlumberSchoolTest.java",
            "QueryableIndexIndexableAdapterTest.java"
        ]
    },
    "incubator-druid_d02f152": {
        "bug_id": "incubator-druid_d02f152",
        "commit": "https://github.com/apache/incubator-druid/commit/d02f152498750f213c38e35a41e0d5e2d6a3f55a",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d02f152498750f213c38e35a41e0d5e2d6a3f55a/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java?ref=d02f152498750f213c38e35a41e0d5e2d6a3f55a",
                "deletions": 0,
                "filename": "client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "patch": "@@ -288,6 +288,10 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) th\n \n         final String inventoryKey = ZKPaths.getNodeFromPath(child.getPath());\n \n+        if (inventoryKey == null) {\n+          return;\n+        }\n+\n         switch (event.getType()) {\n           case CHILD_ADDED:\n           case CHILD_UPDATED:",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d02f152498750f213c38e35a41e0d5e2d6a3f55a/client/src/main/java/com/metamx/druid/curator/inventory/CuratorInventoryManager.java",
                "sha": "bfd75488be848bd06ffedd0c79c645f710b35bf9",
                "status": "modified"
            }
        ],
        "message": "fix NPE",
        "parent": "https://github.com/apache/incubator-druid/commit/5a57539736770af6e581622f48e0d2617f57dd1f",
        "repo": "incubator-druid",
        "unit_tests": [
            "CuratorInventoryManagerTest.java"
        ]
    },
    "incubator-druid_d63107f": {
        "bug_id": "incubator-druid_d63107f",
        "commit": "https://github.com/apache/incubator-druid/commit/d63107f8908f64360be620ec95000696811e033d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/pom.xml?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -74,7 +74,7 @@\n             <dependency>\n                 <groupId>com.metamx</groupId>\n                 <artifactId>emitter</artifactId>\n-                <version>0.2.10</version>\n+                <version>0.2.11</version>\n             </dependency>\n             <dependency>\n                 <groupId>com.metamx</groupId>",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/pom.xml",
                "sha": "c0c9f69a4b0d6a35510b353467ad9dfe14215e95",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "patch": "@@ -35,8 +35,10 @@\n import java.util.Arrays;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n /**\n@@ -110,7 +112,11 @@ public ChainedExecutionQueryRunner(\n                                   if (input == null) {\n                                     throw new ISE(\"Input is null?! How is this possible?!\");\n                                   }\n-                                  return Sequences.toList(input.run(query), Lists.<T>newArrayList());\n+                                  Sequence<T> result = input.run(query);\n+                                  if (result == null) {\n+                                    throw new ISE(\"Got a null result! Segments are missing!\");\n+                                  }\n+                                  return Sequences.toList(result, Lists.<T>newArrayList());\n                                 }\n                                 catch (Exception e) {\n                                   log.error(e, \"Exception with one of the sequences!\");",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/ChainedExecutionQueryRunner.java",
                "sha": "4dbb75b3b04064a4442977f3f05a661437a16586",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 10,
                "filename": "processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "patch": "@@ -70,7 +70,7 @@\n   private final StupidPool<ByteBuffer> intermediateResultsBufferPool;\n \n   @Inject\n-  public GroupByQueryEngine (\n+  public GroupByQueryEngine(\n       Supplier<GroupByQueryConfig> config,\n       @Global StupidPool<ByteBuffer> intermediateResultsBufferPool\n   )\n@@ -81,6 +81,12 @@ public GroupByQueryEngine (\n \n   public Sequence<Row> process(final GroupByQuery query, StorageAdapter storageAdapter)\n   {\n+    if (storageAdapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> intervals = query.getQuerySegmentSpec().getIntervals();\n     if (intervals.size() != 1) {\n       throw new IAE(\"Should only have one interval, got[%s]\", intervals);\n@@ -187,8 +193,7 @@ public int getNumRows()\n           ByteBuffer newKey = key.duplicate();\n           newKey.putInt(dimSelector.getValueCardinality());\n           unaggregatedBuffers = updateValues(newKey, dims.subList(1, dims.size()));\n-        }\n-        else {\n+        } else {\n           for (Integer dimValue : row) {\n             ByteBuffer newKey = key.duplicate();\n             newKey.putInt(dimValue);\n@@ -202,8 +207,7 @@ public int getNumRows()\n           retVal.addAll(unaggregatedBuffers);\n         }\n         return retVal;\n-      }\n-      else {\n+      } else {\n         key.clear();\n         Integer position = positions.get(key);\n         int[] increments = positionMaintainer.getIncrements();\n@@ -267,8 +271,7 @@ public Integer getNext()\n     {\n       if (nextVal > max) {\n         return null;\n-      }\n-      else {\n+      } else {\n         int retVal = (int) nextVal;\n         nextVal += increment;\n         return retVal;\n@@ -402,7 +405,7 @@ public Row apply(@Nullable Map.Entry<ByteBuffer, Integer> input)\n                     final DimExtractionFn fn = dimensionSpecs.get(i).getDimExtractionFn();\n                     final int dimVal = keyBuffer.getInt();\n                     if (dimSelector.getValueCardinality() != dimVal) {\n-                      if(fn != null) {\n+                      if (fn != null) {\n                         theEvent.put(dimNames.get(i), fn.apply(dimSelector.lookupName(dimVal)));\n                       } else {\n                         theEvent.put(dimNames.get(i), dimSelector.lookupName(dimVal));\n@@ -434,9 +437,10 @@ public void remove()\n       throw new UnsupportedOperationException();\n     }\n \n-    public void close() {\n+    public void close()\n+    {\n       // cleanup\n-      for(BufferAggregator agg : aggregators) {\n+      for (BufferAggregator agg : aggregators) {\n         agg.close();\n       }\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/groupby/GroupByQueryEngine.java",
                "sha": "1c75f1390f17561fc5df2d2a388dff3fce03b5ab",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 35,
                "filename": "processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "patch": "@@ -104,47 +104,47 @@\n   )\n   {\n     return new ConcatQueryRunner<SegmentAnalysis>(\n-            Sequences.map(\n-                Sequences.simple(queryRunners),\n-                new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+        Sequences.map(\n+            Sequences.simple(queryRunners),\n+            new Function<QueryRunner<SegmentAnalysis>, QueryRunner<SegmentAnalysis>>()\n+            {\n+              @Override\n+              public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+              {\n+                return new QueryRunner<SegmentAnalysis>()\n                 {\n                   @Override\n-                  public QueryRunner<SegmentAnalysis> apply(final QueryRunner<SegmentAnalysis> input)\n+                  public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n                   {\n-                    return new QueryRunner<SegmentAnalysis>()\n-                    {\n-                      @Override\n-                      public Sequence<SegmentAnalysis> run(final Query<SegmentAnalysis> query)\n-                      {\n \n-                        Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n-                            new Callable<Sequence<SegmentAnalysis>>()\n-                            {\n-                              @Override\n-                              public Sequence<SegmentAnalysis> call() throws Exception\n-                              {\n-                                return new ExecutorExecutingSequence<SegmentAnalysis>(\n-                                    input.run(query),\n-                                    queryExecutor\n-                                );\n-                              }\n-                            }\n-                        );\n-                        try {\n-                          return future.get();\n+                    Future<Sequence<SegmentAnalysis>> future = queryExecutor.submit(\n+                        new Callable<Sequence<SegmentAnalysis>>()\n+                        {\n+                          @Override\n+                          public Sequence<SegmentAnalysis> call() throws Exception\n+                          {\n+                            return new ExecutorExecutingSequence<SegmentAnalysis>(\n+                                input.run(query),\n+                                queryExecutor\n+                            );\n+                          }\n                         }\n-                        catch (InterruptedException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                        catch (ExecutionException e) {\n-                          throw Throwables.propagate(e);\n-                        }\n-                      }\n-                    };\n+                    );\n+                    try {\n+                      return future.get();\n+                    }\n+                    catch (InterruptedException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n+                    catch (ExecutionException e) {\n+                      throw Throwables.propagate(e);\n+                    }\n                   }\n-                }\n-            )\n-        );\n+                };\n+              }\n+            }\n+        )\n+    );\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/SegmentMetadataQueryRunnerFactory.java",
                "sha": "c5b64c2d46ac1af275c5a42451015e56b30b5ca5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "patch": "@@ -32,7 +32,6 @@\n \n public class SegmentMetadataQuery extends BaseQuery<SegmentAnalysis>\n {\n-\n   private final ColumnIncluderator toInclude;\n   private final boolean merge;\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/metadata/metadata/SegmentMetadataQuery.java",
                "sha": "c6d6ecca0bde4d22a13a50f3b2ebdaccb84a3ffa",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 38,
                "filename": "processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "patch": "@@ -55,7 +55,7 @@\n import java.util.TreeSet;\n \n /**\n-*/\n+ */\n public class SearchQueryRunner implements QueryRunner<Result<SearchResultValue>>\n {\n   private static final EmittingLogger log = new EmittingLogger(SearchQueryRunner.class);\n@@ -99,12 +99,10 @@ public SearchQueryRunner(Segment segment)\n           ConciseSet set = new ConciseSet();\n           set.add(0);\n           baseFilter = ImmutableConciseSet.newImmutableFromMutable(set);\n-        }\n-        else {\n+        } else {\n           baseFilter = ImmutableConciseSet.complement(new ImmutableConciseSet(), index.getNumRows());\n         }\n-      }\n-      else {\n+      } else {\n         baseFilter = filter.goConcise(new ColumnSelectorBitmapIndexSelector(index));\n       }\n \n@@ -133,49 +131,52 @@ public SearchQueryRunner(Segment segment)\n     }\n \n     final StorageAdapter adapter = segment.asStorageAdapter();\n-    if (adapter != null) {\n-      Iterable<String> dimsToSearch;\n-      if (dimensions == null || dimensions.isEmpty()) {\n-        dimsToSearch = adapter.getAvailableDimensions();\n-      } else {\n-        dimsToSearch = dimensions;\n-      }\n \n-      final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n+    if (adapter == null) {\n+      log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n+         .addData(\"segment\", segment.getIdentifier())\n+         .addData(\"query\", query).emit();\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n \n-      final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n-      for (Cursor cursor : cursors) {\n-        Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n-        for (String dim : dimsToSearch) {\n-          dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n-        }\n+    Iterable<String> dimsToSearch;\n+    if (dimensions == null || dimensions.isEmpty()) {\n+      dimsToSearch = adapter.getAvailableDimensions();\n+    } else {\n+      dimsToSearch = dimensions;\n+    }\n+\n+    final TreeSet<SearchHit> retVal = Sets.newTreeSet(query.getSort().getComparator());\n \n-        while (!cursor.isDone()) {\n-          for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n-            final DimensionSelector selector = entry.getValue();\n-            final IndexedInts vals = selector.getRow();\n-            for (int i = 0; i < vals.size(); ++i) {\n-              final String dimVal = selector.lookupName(vals.get(i));\n-              if (searchQuerySpec.accept(dimVal)) {\n-                retVal.add(new SearchHit(entry.getKey(), dimVal));\n-                if (retVal.size() >= limit) {\n-                  return makeReturnResult(limit, retVal);\n-                }\n+    final Iterable<Cursor> cursors = adapter.makeCursors(filter, segment.getDataInterval(), QueryGranularity.ALL);\n+    for (Cursor cursor : cursors) {\n+      Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n+      for (String dim : dimsToSearch) {\n+        dimSelectors.put(dim, cursor.makeDimensionSelector(dim));\n+      }\n+\n+      while (!cursor.isDone()) {\n+        for (Map.Entry<String, DimensionSelector> entry : dimSelectors.entrySet()) {\n+          final DimensionSelector selector = entry.getValue();\n+          final IndexedInts vals = selector.getRow();\n+          for (int i = 0; i < vals.size(); ++i) {\n+            final String dimVal = selector.lookupName(vals.get(i));\n+            if (searchQuerySpec.accept(dimVal)) {\n+              retVal.add(new SearchHit(entry.getKey(), dimVal));\n+              if (retVal.size() >= limit) {\n+                return makeReturnResult(limit, retVal);\n               }\n             }\n           }\n-\n-          cursor.advance();\n         }\n-      }\n \n-      return makeReturnResult(limit, retVal);\n+        cursor.advance();\n+      }\n     }\n \n-    log.makeAlert(\"WTF!? Unable to process search query on segment.\")\n-       .addData(\"segment\", segment.getIdentifier())\n-       .addData(\"query\", query);\n-    return Sequences.empty();\n+    return makeReturnResult(limit, retVal);\n   }\n \n   private Sequence<Result<SearchResultValue>> makeReturnResult(int limit, TreeSet<SearchHit> retVal)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/search/SearchQueryRunner.java",
                "sha": "4070a274b4fe374a6d2e582e0d8c4631beb43050",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Function;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -54,6 +55,12 @@\n           {\n             final StorageAdapter adapter = segment.asStorageAdapter();\n \n+            if (adapter == null) {\n+              throw new ISE(\n+                  \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+              );\n+            }\n+\n             final Iterable<String> dims;\n             if (query.getDimensions() == null || query.getDimensions().isEmpty()) {\n               dims = adapter.getAvailableDimensions();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/select/SelectQueryEngine.java",
                "sha": "f5bc7aba043c425bffb6d40885d082a07136ffcd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "patch": "@@ -87,6 +87,12 @@ public TimeBoundaryQueryRunner(Segment segment)\n             @Override\n             public Iterator<Result<TimeBoundaryResultValue>> make()\n             {\n+              if (adapter == null) {\n+                throw new ISE(\n+                    \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+                );\n+              }\n+\n               return legacyQuery.buildResult(\n                   adapter.getInterval().getStart(),\n                   adapter.getMinTime(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeboundary/TimeBoundaryQueryRunnerFactory.java",
                "sha": "16e9ae832fae74154c2ad1380350931455320343",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "patch": "@@ -20,6 +20,7 @@\n package io.druid.query.timeseries;\n \n import com.google.common.base.Function;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.BaseSequence;\n import com.metamx.common.guava.Sequence;\n import io.druid.query.QueryRunnerHelper;\n@@ -40,6 +41,12 @@\n {\n   public Sequence<Result<TimeseriesResultValue>> process(final TimeseriesQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     return new BaseSequence<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>(\n         new BaseSequence.IteratorMaker<Result<TimeseriesResultValue>, Iterator<Result<TimeseriesResultValue>>>()\n         {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
                "sha": "6ab42477890e9791a42cc60e4066be26f02d033e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 12,
                "filename": "processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "patch": "@@ -21,7 +21,7 @@\n \n import com.google.common.base.Function;\n import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n+import com.metamx.common.ISE;\n import com.metamx.common.guava.FunctionalIterable;\n import com.metamx.common.logger.Logger;\n import io.druid.collections.StupidPool;\n@@ -53,6 +53,12 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n \n   public Iterable<Result<TopNResultValue>> query(final TopNQuery query, final StorageAdapter adapter)\n   {\n+    if (adapter == null) {\n+      throw new ISE(\n+          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped.\"\n+      );\n+    }\n+\n     final List<Interval> queryIntervals = query.getQuerySegmentSpec().getIntervals();\n     final Filter filter = Filters.convertDimensionFilters(query.getDimensionsFilter());\n     final QueryGranularity granularity = query.getGranularity();\n@@ -62,10 +68,6 @@ public TopNQueryEngine(StupidPool<ByteBuffer> bufferPool)\n         queryIntervals.size() == 1, \"Can only handle a single interval, got[%s]\", queryIntervals\n     );\n \n-    if (mapFn == null) {\n-      return Lists.newArrayList();\n-    }\n-\n     return FunctionalIterable\n         .create(adapter.makeCursors(filter, queryIntervals.get(0), granularity))\n         .transform(\n@@ -84,13 +86,6 @@ public Cursor apply(Cursor input)\n \n   private Function<Cursor, Result<TopNResultValue>> getMapFn(TopNQuery query, final StorageAdapter adapter)\n   {\n-    if (adapter == null) {\n-      log.warn(\n-          \"Null storage adapter found. Probably trying to issue a query against a segment being memory unmapped. Returning empty results.\"\n-      );\n-      return null;\n-    }\n-\n     final Capabilities capabilities = adapter.getCapabilities();\n     final int cardinality = adapter.getDimensionCardinality(query.getDimensionSpec().getDimension());\n     int numBytesPerRecord = 0;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/processing/src/main/java/io/druid/query/topn/TopNQueryEngine.java",
                "sha": "1f3a8892733b2f3f2fc9a84a5eb92bade1224721",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d63107f8908f64360be620ec95000696811e033d/server/src/main/java/io/druid/server/QueryResource.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/QueryResource.java?ref=d63107f8908f64360be620ec95000696811e033d",
                "deletions": 13,
                "filename": "server/src/main/java/io/druid/server/QueryResource.java",
                "patch": "@@ -29,8 +29,7 @@\n import com.google.inject.Inject;\n import com.metamx.common.guava.Sequence;\n import com.metamx.common.guava.Sequences;\n-import com.metamx.common.logger.Logger;\n-import com.metamx.emitter.service.AlertEvent;\n+import com.metamx.emitter.EmittingLogger;\n import com.metamx.emitter.service.ServiceEmitter;\n import com.metamx.emitter.service.ServiceMetricEvent;\n import io.druid.guice.annotations.Json;\n@@ -57,7 +56,7 @@\n @Path(\"/druid/v2/\")\n public class QueryResource\n {\n-  private static final Logger log = new Logger(QueryResource.class);\n+  private static final EmittingLogger log = new EmittingLogger(QueryResource.class);\n   private static final Charset UTF8 = Charset.forName(\"UTF-8\");\n   private static final Joiner COMMA_JOIN = Joiner.on(\",\");\n \n@@ -192,16 +191,11 @@ public void doPost(\n         log.error(e2, \"Unable to log query [%s]!\", queryString);\n       }\n \n-      emitter.emit(\n-          new AlertEvent.Builder().build(\n-              \"Exception handling request\",\n-              ImmutableMap.<String, Object>builder()\n-                          .put(\"exception\", e.toString())\n-                          .put(\"query\", queryString)\n-                          .put(\"peer\", req.getRemoteAddr())\n-                          .build()\n-          )\n-      );\n+      log.makeAlert(e, \"Exception handling request\")\n+         .addData(\"exception\", e.toString())\n+         .addData(\"query\", queryString)\n+         .addData(\"peer\", req.getRemoteAddr())\n+         .emit();\n     }\n     finally {\n       resp.flushBuffer();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d63107f8908f64360be620ec95000696811e033d/server/src/main/java/io/druid/server/QueryResource.java",
                "sha": "77d0c1bf04c0eba991bf85579a1def6800eb9347",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #502 from metamx/fix-NPE-CQE\n\nBetter handle null adapters and NPEs in the CQE",
        "parent": "https://github.com/apache/incubator-druid/commit/560045ddd2a37ed96703d31623e059248c62b944",
        "repo": "incubator-druid",
        "unit_tests": [
            "ChainedExecutionQueryRunnerTest.java",
            "SegmentMetadataQueryTest.java",
            "SearchQueryRunnerTest.java",
            "QueryResourceTest.java"
        ]
    },
    "incubator-druid_d875d72": {
        "bug_id": "incubator-druid_d875d72",
        "commit": "https://github.com/apache/incubator-druid/commit/d875d7294b803ad1d4dd44b94c3c9f5ad1093822",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d875d7294b803ad1d4dd44b94c3c9f5ad1093822/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java?ref=d875d7294b803ad1d4dd44b94c3c9f5ad1093822",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "patch": "@@ -111,6 +111,12 @@ public Comparator getComparator()\n       @Override\n       public int compare(HyperLogLogCollector lhs, HyperLogLogCollector rhs)\n       {\n+        if(lhs == null) {\n+          return -1;\n+        }\n+        if(rhs == null) {\n+          return 1;\n+        }\n         return lhs.compareTo(rhs);\n       }\n     };",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d875d7294b803ad1d4dd44b94c3c9f5ad1093822/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "sha": "7b3d31189c5f4437d903953449a868386052b15d",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/d875d7294b803ad1d4dd44b94c3c9f5ad1093822/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java?ref=d875d7294b803ad1d4dd44b94c3c9f5ad1093822",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import io.druid.query.aggregation.MaxAggregatorFactory;\n import io.druid.query.aggregation.MinAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n+import io.druid.query.aggregation.hyperloglog.HyperUniquesAggregatorFactory;\n import io.druid.query.dimension.ExtractionDimensionSpec;\n import io.druid.query.extraction.RegexDimExtractionFn;\n import io.druid.query.filter.AndDimFilter;\n@@ -303,6 +304,46 @@ public void testFullOnTopNOverUniques()\n     TestHelper.assertExpectedResults(expectedResults, runner.run(query, context));\n   }\n \n+  @Test\n+  public void testTopNOverMissingUniques()\n+  {\n+    TopNQuery query = new TopNQueryBuilder()\n+        .dataSource(QueryRunnerTestHelper.dataSource)\n+        .granularity(QueryRunnerTestHelper.allGran)\n+        .dimension(marketDimension)\n+        .metric(QueryRunnerTestHelper.uniqueMetric)\n+        .threshold(3)\n+        .intervals(QueryRunnerTestHelper.fullOnInterval)\n+        .aggregators(\n+            Arrays.<AggregatorFactory>asList(new HyperUniquesAggregatorFactory(\"uniques\", \"missingUniques\"))\n+        )\n+        .build();\n+\n+    List<Result<TopNResultValue>> expectedResults = Arrays.asList(\n+        new Result<TopNResultValue>(\n+            new DateTime(\"2011-01-12T00:00:00.000Z\"),\n+            new TopNResultValue(\n+                Arrays.<Map<String, Object>>asList(\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"total_market\")\n+                                .put(\"uniques\", 0)\n+                                .build(),\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"spot\")\n+                                .put(\"uniques\", 0)\n+                                .build(),\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"upfront\")\n+                                .put(\"uniques\", 0)\n+                                .build()\n+                )\n+            )\n+        )\n+    );\n+    HashMap<String, Object> context = new HashMap<String, Object>();\n+    TestHelper.assertExpectedResults(expectedResults, runner.run(query, context));\n+  }\n+\n \n   @Test\n   public void testTopNBySegment()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/d875d7294b803ad1d4dd44b94c3c9f5ad1093822/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "sha": "c3a40424c3bb3c6da934b67012239f8c6ccc2ef2",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1070 from metamx/fix-topn-missing-hll-npe\n\nfix NPE for topN over missing hyperUniques column",
        "parent": "https://github.com/apache/incubator-druid/commit/05b2bf3fddf00915d2dcf46ebfedca7dbdd44919",
        "repo": "incubator-druid",
        "unit_tests": [
            "HyperUniquesAggregatorFactoryTest.java"
        ]
    },
    "incubator-druid_da43f68": {
        "bug_id": "incubator-druid_da43f68",
        "commit": "https://github.com/apache/incubator-druid/commit/da43f68e9517ed2cfd51eab79b36f6ec05798639",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/BucketExtractionFn.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/BucketExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/extraction/BucketExtractionFn.java",
                "patch": "@@ -56,8 +56,13 @@ public double getOffset()\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n+    if (value == null) {\n+      return null;\n+    }\n+\n     if (value instanceof Number) {\n       return bucket(((Number) value).doubleValue());\n     } else if (value instanceof String) {\n@@ -67,8 +72,13 @@ public String apply(Object value)\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n+    if (value == null) {\n+      return null;\n+    }\n+\n     try {\n       return bucket(Double.parseDouble(value));\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/BucketExtractionFn.java",
                "sha": "55fee2e237bbe0b83831f99fb222dddaeb9b846a",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/CascadeExtractionFn.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/CascadeExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 6,
                "filename": "processing/src/main/java/io/druid/query/extraction/CascadeExtractionFn.java",
                "patch": "@@ -25,6 +25,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.primitives.Bytes;\n \n+import javax.annotation.Nullable;\n import java.util.Arrays;\n \n public class CascadeExtractionFn implements ExtractionFn\n@@ -40,14 +41,16 @@\n           return new byte[0];\n         }\n \n+        @Nullable\n         @Override\n-        public String apply(Object value)\n+        public String apply(@Nullable Object value)\n         {\n           return null;\n         }\n \n+        @Nullable\n         @Override\n-        public String apply(String value)\n+        public String apply(@Nullable String value)\n         {\n           return null;\n         }\n@@ -113,13 +116,15 @@ public CascadeExtractionFn(\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n     return chainedExtractionFn.apply(value);\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return chainedExtractionFn.apply(value);\n   }\n@@ -195,12 +200,14 @@ public ChainedExtractionFn(ExtractionFn fn, ChainedExtractionFn child)\n       return (child != null) ? Bytes.concat(fnCacheKey, child.getCacheKey()) : fnCacheKey;\n     }\n \n-    public String apply(Object value)\n+    @Nullable\n+    public String apply(@Nullable Object value)\n     {\n       return fn.apply((child != null) ? child.apply(value) : value);\n     }\n \n-    public String apply(String value)\n+    @Nullable\n+    public String apply(@Nullable String value)\n     {\n       return fn.apply((child != null) ? child.apply(value) : value);\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/CascadeExtractionFn.java",
                "sha": "f9b5e604f401c860d516a38c2c8da50f4680ce6e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/DimExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/DimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/DimExtractionFn.java",
                "patch": "@@ -19,12 +19,14 @@\n \n package io.druid.query.extraction;\n \n+import javax.annotation.Nullable;\n import java.util.Objects;\n \n public abstract class DimExtractionFn implements ExtractionFn\n {\n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n     return apply(Objects.toString(value, null));\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/DimExtractionFn.java",
                "sha": "7cfbe32c593c2fb16e1c7c8e6101245fa779ce67",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/ExtractionFn.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/ExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/extraction/ExtractionFn.java",
                "patch": "@@ -24,6 +24,8 @@\n import io.druid.query.lookup.LookupExtractionFn;\n import io.druid.query.lookup.RegisteredLookupExtractionFn;\n \n+import javax.annotation.Nullable;\n+\n /**\n  */\n @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\")\n@@ -75,7 +77,8 @@\n    *\n    * @return a value that should be used instead of the original\n    */\n-  public String apply(Object value);\n+  @Nullable\n+  public String apply(@Nullable Object value);\n \n   /**\n    * The \"extraction\" function.  This should map a String value into some other String value.\n@@ -87,7 +90,8 @@\n    *\n    * @return a value that should be used instead of the original\n    */\n-  public String apply(String value);\n+  @Nullable\n+  public String apply(@Nullable String value);\n \n   /**\n    * The \"extraction\" function.  This should map a long value into some String value.",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/ExtractionFn.java",
                "sha": "0bfe2033fc713e42c20ec500f4a3d3177484c540",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/FunctionalExtraction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/FunctionalExtraction.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/FunctionalExtraction.java",
                "patch": "@@ -105,7 +105,8 @@ public boolean isInjective()\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return extractionFunction.apply(value);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/FunctionalExtraction.java",
                "sha": "bd762574efd151551543cab09e44963b2f24f63d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/IdentityExtractionFn.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/IdentityExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/extraction/IdentityExtractionFn.java",
                "patch": "@@ -21,6 +21,8 @@\n \n import com.google.common.base.Strings;\n \n+import javax.annotation.Nullable;\n+\n public class IdentityExtractionFn implements ExtractionFn\n {\n   private static final IdentityExtractionFn instance = new IdentityExtractionFn();\n@@ -37,13 +39,15 @@ private IdentityExtractionFn()\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n     return value == null ? null : Strings.emptyToNull(value.toString());\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return Strings.emptyToNull(value);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/IdentityExtractionFn.java",
                "sha": "d0158e7f27e8e70090d5ed951698001b9cbc117f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/JavaScriptExtractionFn.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/JavaScriptExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/extraction/JavaScriptExtractionFn.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.mozilla.javascript.ContextFactory;\n import org.mozilla.javascript.ScriptableObject;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n \n public class JavaScriptExtractionFn implements ExtractionFn\n@@ -111,7 +112,8 @@ public boolean isInjective()\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n     if (fn == null) {\n       throw new ISE(\"JavaScript is disabled\");\n@@ -121,7 +123,8 @@ public String apply(Object value)\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return this.apply((Object) Strings.emptyToNull(value));\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/JavaScriptExtractionFn.java",
                "sha": "51efd13684419d9d234a11ff4b1dfdcc4c0fc2ea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/LowerExtractionFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/LowerExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/LowerExtractionFn.java",
                "patch": "@@ -50,7 +50,7 @@ public LowerExtractionFn(@JsonProperty(\"locale\") String localeString)\n \n   @Nullable\n   @Override\n-  public String apply(String key)\n+  public String apply(@Nullable String key)\n   {\n     if (Strings.isNullOrEmpty(key)) {\n       return null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/LowerExtractionFn.java",
                "sha": "da2a8e3bc70214f58ca93e9c8cefb2d3a3ebdba0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/MatchingDimExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/MatchingDimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/MatchingDimExtractionFn.java",
                "patch": "@@ -25,6 +25,7 @@\n import com.google.common.base.Strings;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n@@ -57,8 +58,9 @@ public MatchingDimExtractionFn(\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String dimValue)\n+  public String apply(@Nullable String dimValue)\n   {\n     if (Strings.isNullOrEmpty(dimValue)) {\n       // We'd return null whether or not the pattern matched",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/MatchingDimExtractionFn.java",
                "sha": "4d121be031fc339a77d182f08c458eae2b63986a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.common.primitives.Ints;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.Objects;\n import java.util.regex.Matcher;\n@@ -100,8 +101,9 @@ public RegexDimExtractionFn(\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String dimValue)\n+  public String apply(@Nullable String dimValue)\n   {\n     final String retVal;\n     final Matcher matcher = pattern.matcher(Strings.nullToEmpty(dimValue));",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/RegexDimExtractionFn.java",
                "sha": "3a1b0a5cc293550b87432d20a46d208b77b37e19",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/SearchQuerySpecDimExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/SearchQuerySpecDimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/SearchQuerySpecDimExtractionFn.java",
                "patch": "@@ -25,6 +25,7 @@\n import com.google.common.base.Strings;\n import io.druid.query.search.search.SearchQuerySpec;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n \n /**\n@@ -59,8 +60,9 @@ public SearchQuerySpec getSearchQuerySpec()\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String dimValue)\n+  public String apply(@Nullable String dimValue)\n   {\n     return searchQuerySpec.accept(dimValue) ? Strings.emptyToNull(dimValue) : null;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/SearchQuerySpecDimExtractionFn.java",
                "sha": "b80971ec2dbbec9534aa123c911d057e08e06555",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/StringFormatExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/StringFormatExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/StringFormatExtractionFn.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.common.base.Strings;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n \n /**\n@@ -94,8 +95,9 @@ public NullHandling getNullHandling()\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String value)\n+  public String apply(@Nullable String value)\n   {\n     if (value == null) {\n       if (nullHandling == NullHandling.RETURNNULL) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/StringFormatExtractionFn.java",
                "sha": "1b22d28abfca98d3158b0be5287f1e9e07380520",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/StrlenExtractionFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/StrlenExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/StrlenExtractionFn.java",
                "patch": "@@ -21,6 +21,8 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n \n+import javax.annotation.Nullable;\n+\n public class StrlenExtractionFn extends DimExtractionFn\n {\n   private static final StrlenExtractionFn INSTANCE = new StrlenExtractionFn();\n@@ -36,7 +38,7 @@ public static StrlenExtractionFn instance()\n   }\n \n   @Override\n-  public String apply(String value)\n+  public String apply(@Nullable String value)\n   {\n     return String.valueOf(value == null ? 0 : value.length());\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/StrlenExtractionFn.java",
                "sha": "5ad88fa38fcee6da521f37deb20f0e2b86975624",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/SubstringDimExtractionFn.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/SubstringDimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/SubstringDimExtractionFn.java",
                "patch": "@@ -61,8 +61,9 @@ public SubstringDimExtractionFn(\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String dimValue)\n+  public String apply(@Nullable String dimValue)\n   {\n     if (Strings.isNullOrEmpty(dimValue)) {\n       return null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/SubstringDimExtractionFn.java",
                "sha": "2d38806ddd45a1e07836d18aec15e20edc08792b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/TimeDimExtractionFn.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/TimeDimExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/TimeDimExtractionFn.java",
                "patch": "@@ -22,9 +22,11 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n import com.ibm.icu.text.SimpleDateFormat;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.text.ParseException;\n import java.util.Date;\n@@ -71,9 +73,14 @@ public TimeDimExtractionFn(\n                      .array();\n   }\n \n+  @Nullable\n   @Override\n-  public String apply(String dimValue)\n+  public String apply(@Nullable String dimValue)\n   {\n+    if (Strings.isNullOrEmpty(dimValue)) {\n+      return null;\n+    }\n+\n     Date date;\n     try {\n       date = timeFormatter.get().parse(dimValue);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/TimeDimExtractionFn.java",
                "sha": "0aa7fdeb8acfb1ef3bcb1ba973e5daf325cc0f45",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/TimeFormatExtractionFn.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/TimeFormatExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/extraction/TimeFormatExtractionFn.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.joda.time.format.DateTimeFormatter;\n import org.joda.time.format.ISODateTimeFormat;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.Locale;\n \n@@ -128,8 +129,13 @@ public String apply(long value)\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n+    if (value == null) {\n+      return null;\n+    }\n+\n     if (asMillis && value instanceof String) {\n       final Long theLong = GuavaUtils.tryParseLong((String) value);\n       return theLong == null ? apply(new DateTime(value).getMillis()) : apply(theLong.longValue());\n@@ -139,7 +145,8 @@ public String apply(Object value)\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return apply((Object) value);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/TimeFormatExtractionFn.java",
                "sha": "fa66b5c2d75dff754109b3a9b4c6aa2b74d76471",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/UpperExtractionFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/extraction/UpperExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/extraction/UpperExtractionFn.java",
                "patch": "@@ -49,7 +49,7 @@ public UpperExtractionFn(@JsonProperty(\"locale\") String localeString)\n    */\n   @Nullable\n   @Override\n-  public String apply(String key)\n+  public String apply(@Nullable String key)\n   {\n     if (Strings.isNullOrEmpty(key)) {\n       return null;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/extraction/UpperExtractionFn.java",
                "sha": "34368f7f0fd6e84d0892c2fdf84e98ee5714a20c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "patch": "@@ -105,13 +105,15 @@ public boolean isOptimize()\n   }\n \n   @Override\n-  public String apply(Object value)\n+  @Nullable\n+  public String apply(@Nullable Object value)\n   {\n     return ensureDelegate().apply(value);\n   }\n \n   @Override\n-  public String apply(String value)\n+  @Nullable\n+  public String apply(@Nullable String value)\n   {\n     return ensureDelegate().apply(value);\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/lookup/RegisteredLookupExtractionFn.java",
                "sha": "525f72dcb78a8e79b261a0158d883158a788058b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/AllSearchQuerySpec.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/search/AllSearchQuerySpec.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/search/AllSearchQuerySpec.java",
                "patch": "@@ -19,14 +19,16 @@\n \n package io.druid.query.search.search;\n \n+import javax.annotation.Nullable;\n+\n /**\n  */\n public class AllSearchQuerySpec implements SearchQuerySpec\n {\n   private static final byte CACHE_TYPE_ID = 0x7f;\n \n   @Override\n-  public boolean accept(String dimVal)\n+  public boolean accept(@Nullable String dimVal)\n   {\n     return true;\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/AllSearchQuerySpec.java",
                "sha": "4f64561fe94959fa348c02b0689a56cda17544ba",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/ContainsSearchQuerySpec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/search/ContainsSearchQuerySpec.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/search/ContainsSearchQuerySpec.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.base.Objects;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n \n /**\n@@ -59,7 +60,7 @@ public boolean isCaseSensitive()\n   }\n \n   @Override\n-  public boolean accept(String dimVal)\n+  public boolean accept(@Nullable String dimVal)\n   {\n     if (dimVal == null || value == null) {\n       return false;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/ContainsSearchQuerySpec.java",
                "sha": "3ea9d0d0c70b051c7f81542de778b59fa3b4e20d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/FragmentSearchQuerySpec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/search/FragmentSearchQuerySpec.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/search/FragmentSearchQuerySpec.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.fasterxml.jackson.annotation.JsonProperty;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.Arrays;\n import java.util.List;\n@@ -77,7 +78,7 @@ public boolean isCaseSensitive()\n   }\n \n   @Override\n-  public boolean accept(String dimVal)\n+  public boolean accept(@Nullable String dimVal)\n   {\n     if (dimVal == null || values == null) {\n       return false;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/FragmentSearchQuerySpec.java",
                "sha": "9798bd305607045f77857dac910d53381e0090e4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/RegexSearchQuerySpec.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/search/RegexSearchQuerySpec.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/search/RegexSearchQuerySpec.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.base.Preconditions;\n import io.druid.java.util.common.StringUtils;\n \n+import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.regex.Pattern;\n \n@@ -74,7 +75,7 @@ public int hashCode()\n   }\n \n   @Override\n-  public boolean accept(String dimVal)\n+  public boolean accept(@Nullable String dimVal)\n   {\n     if (dimVal == null) {\n       return false;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/RegexSearchQuerySpec.java",
                "sha": "085a1216bcc6e80d33d15b5be128d33008ad8068",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/SearchQuerySpec.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/search/search/SearchQuerySpec.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/query/search/search/SearchQuerySpec.java",
                "patch": "@@ -22,6 +22,8 @@\n import com.fasterxml.jackson.annotation.JsonSubTypes;\n import com.fasterxml.jackson.annotation.JsonTypeInfo;\n \n+import javax.annotation.Nullable;\n+\n /**\n  */\n @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\")\n@@ -34,7 +36,7 @@\n })\n public interface SearchQuerySpec\n {\n-  public boolean accept(String dimVal);\n+  public boolean accept(@Nullable String dimVal);\n \n   public byte[] getCacheKey();\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/main/java/io/druid/query/search/search/SearchQuerySpec.java",
                "sha": "b3429a3c8cb03ed9e592c3d068723af912438ef0",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/test/java/io/druid/query/extraction/TimeDimExtractionFnTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/extraction/TimeDimExtractionFnTest.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/extraction/TimeDimExtractionFnTest.java",
                "patch": "@@ -40,6 +40,16 @@\n       \"12/21/2012\"\n   };\n \n+  @Test\n+  public void testEmptyAndNullExtraction()\n+  {\n+    Set<String> testPeriod = Sets.newHashSet();\n+    ExtractionFn extractionFn = new TimeDimExtractionFn(\"MM/dd/yyyy\", \"MM/yyyy\");\n+\n+    Assert.assertNull(extractionFn.apply(null));\n+    Assert.assertNull(extractionFn.apply(\"\"));\n+  }\n+\n   @Test\n   public void testMonthExtraction()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/test/java/io/druid/query/extraction/TimeDimExtractionFnTest.java",
                "sha": "e8424c0314c6fae7e74bfc77e5bfd002978ebec7",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/test/java/io/druid/segment/filter/SelectorFilterTest.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/filter/SelectorFilterTest.java?ref=da43f68e9517ed2cfd51eab79b36f6ec05798639",
                "deletions": 4,
                "filename": "processing/src/test/java/io/druid/segment/filter/SelectorFilterTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import io.druid.data.input.impl.TimestampSpec;\n import io.druid.java.util.common.Pair;\n import io.druid.query.extraction.MapLookupExtractor;\n+import io.druid.query.extraction.TimeDimExtractionFn;\n import io.druid.query.filter.ExtractionDimFilter;\n import io.druid.query.filter.InDimFilter;\n import io.druid.query.filter.SelectorDimFilter;\n@@ -58,17 +59,17 @@\n       new TimeAndDimsParseSpec(\n           new TimestampSpec(TIMESTAMP_COLUMN, \"iso\", new DateTime(\"2000\")),\n           new DimensionsSpec(\n-              DimensionsSpec.getDefaultSchemas(ImmutableList.of(\"dim0\", \"dim1\", \"dim2\", \"dim3\")),\n+              DimensionsSpec.getDefaultSchemas(ImmutableList.of(\"dim0\", \"dim1\", \"dim2\", \"dim3\", \"dim6\")),\n               null,\n               null\n           )\n       )\n   );\n \n   private static final List<InputRow> ROWS = ImmutableList.of(\n-      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"0\", \"dim1\", \"\", \"dim2\", ImmutableList.of(\"a\", \"b\"))),\n-      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"1\", \"dim1\", \"10\", \"dim2\", ImmutableList.of())),\n-      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"2\", \"dim1\", \"2\", \"dim2\", ImmutableList.of(\"\"))),\n+      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"0\", \"dim1\", \"\", \"dim2\", ImmutableList.of(\"a\", \"b\"), \"dim6\", \"2017-07-25\")),\n+      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"1\", \"dim1\", \"10\", \"dim2\", ImmutableList.of(), \"dim6\", \"2017-07-25\")),\n+      PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"2\", \"dim1\", \"2\", \"dim2\", ImmutableList.of(\"\"), \"dim6\", \"2017-05-25\")),\n       PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"3\", \"dim1\", \"1\", \"dim2\", ImmutableList.of(\"a\"))),\n       PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"4\", \"dim1\", \"def\", \"dim2\", ImmutableList.of(\"c\"))),\n       PARSER.parse(ImmutableMap.<String, Object>of(\"dim0\", \"5\", \"dim1\", \"abc\"))\n@@ -91,6 +92,15 @@ public static void tearDown() throws Exception\n     BaseFilterTest.tearDown(SelectorFilterTest.class.getName());\n   }\n \n+  @Test\n+  public void testWithTimeExtractionFnNull()\n+  {\n+    assertFilterMatches(new SelectorDimFilter(\"dim0\", null, new TimeDimExtractionFn(\"yyyy-mm-dd\", \"yyyy-mm\")), ImmutableList.<String>of());\n+    assertFilterMatches(new SelectorDimFilter(\"dim6\", null, new TimeDimExtractionFn(\"yyyy-mm-dd\", \"yyyy-mm\")), ImmutableList.<String>of(\"3\", \"4\", \"5\"));\n+    assertFilterMatches(new SelectorDimFilter(\"dim6\", \"2017-07\", new TimeDimExtractionFn(\"yyyy-mm-dd\", \"yyyy-mm\")), ImmutableList.<String>of(\"0\", \"1\"));\n+    assertFilterMatches(new SelectorDimFilter(\"dim6\", \"2017-05\", new TimeDimExtractionFn(\"yyyy-mm-dd\", \"yyyy-mm\")), ImmutableList.<String>of(\"2\"));\n+  }\n+\n   @Test\n   public void testSingleValueStringColumnWithoutNulls()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/da43f68e9517ed2cfd51eab79b36f6ec05798639/processing/src/test/java/io/druid/segment/filter/SelectorFilterTest.java",
                "sha": "e3bd81b472716ac9952882ce33fba511b3e50f55",
                "status": "modified"
            }
        ],
        "message": "NPE thrown when empty/null is passes to TimeDimExtractionFn (#4601)\n\n* NPE thrown when empty/null is passes to TimeDimExtractionFn\r\n\r\n* Add @Nullable where ever applicable\r\n\r\n* Add @Nullable to SearchQuerySpec.apply()\r\n\r\n* Remove unused",
        "parent": "https://github.com/apache/incubator-druid/commit/5929066dfba96c918f14e26b629534f53de0f9cd",
        "repo": "incubator-druid",
        "unit_tests": [
            "BucketExtractionFnTest.java",
            "CascadeExtractionFnTest.java",
            "FunctionalExtractionTest.java",
            "JavaScriptExtractionFnTest.java",
            "LowerExtractionFnTest.java",
            "MatchingDimExtractionFnTest.java",
            "RegexDimExtractionFnTest.java",
            "SearchQuerySpecDimExtractionFnTest.java",
            "StringFormatExtractionFnTest.java",
            "StrlenExtractionFnTest.java",
            "SubstringDimExtractionFnTest.java",
            "TimeDimExtractionFnTest.java",
            "TimeFormatExtractionFnTest.java",
            "UpperExtractionFnTest.java",
            "RegisteredLookupExtractionFnTest.java"
        ]
    },
    "incubator-druid_e2653a8": {
        "bug_id": "incubator-druid_e2653a8",
        "commit": "https://github.com/apache/incubator-druid/commit/e2653a8cf4a1d727c5a0e0406092002e68634052",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e2653a8cf4a1d727c5a0e0406092002e68634052/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java?ref=e2653a8cf4a1d727c5a0e0406092002e68634052",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "patch": "@@ -551,6 +551,10 @@ public void run()\n                 LOG.info(\"Not started. Returning\");\n                 return;\n               }\n+              if (allLookupTiers == null) {\n+                LOG.info(\"Not updating lookups because no data exists\");\n+                return;\n+              }\n               for (final String tier : allLookupTiers.keySet()) {\n                 try {\n                   final Map<String, Map<String, Object>> allLookups = allLookupTiers.get(tier);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e2653a8cf4a1d727c5a0e0406092002e68634052/server/src/main/java/io/druid/server/lookup/cache/LookupCoordinatorManager.java",
                "sha": "f9aaee2f64fb7ab6a0b27c5099a7e03a31c133a2",
                "status": "modified"
            }
        ],
        "message": "handle a NPE in LookupCoordinatorManager.start() (#3026)",
        "parent": "https://github.com/apache/incubator-druid/commit/ebb6831770e6da5d84b1c39c3e6d27554852e784",
        "repo": "incubator-druid",
        "unit_tests": [
            "LookupCoordinatorManagerTest.java"
        ]
    },
    "incubator-druid_e3260aa": {
        "bug_id": "incubator-druid_e3260aa",
        "commit": "https://github.com/apache/incubator-druid/commit/e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/docs/content/Aggregations.md",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/docs/content/Aggregations.md?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 2,
                "filename": "docs/content/Aggregations.md",
                "patch": "@@ -169,14 +169,13 @@ A filtered aggregator wraps any given aggregator, but only aggregates the values\n \n This makes it possible to compute the results of a filtered and an unfiltered aggregation simultaneously, without having to issue multiple queries, and use both results as part of post-aggregations.\n \n-*Limitations:* The filtered aggregator currently only supports selector and not filter with a single selector, i.e. matching a dimension against a single value.\n+*Limitations:* The filtered aggregator currently only supports 'or', 'and', 'selector' and 'not' filters, i.e. matching one or multiple dimensions against a single value.\n \n *Note:* If only the filtered results are required, consider putting the filter on the query itself, which will be much faster since it does not require scanning all the data.\n \n ```json\n {\n   \"type\" : \"filtered\",\n-  \"name\" : \"aggMatching\",\n   \"filter\" : {\n     \"type\" : \"selector\",\n     \"dimension\" : <dimension>,",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/docs/content/Aggregations.md",
                "sha": "ca10bd7a05c524945b805111bed7ce72c787e5c1",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredAggregator.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/FilteredAggregator.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 21,
                "filename": "processing/src/main/java/io/druid/query/aggregation/FilteredAggregator.java",
                "patch": "@@ -19,38 +19,24 @@\n \n package io.druid.query.aggregation;\n \n-import com.google.common.base.Predicate;\n-import com.google.common.collect.Iterables;\n-import io.druid.segment.DimensionSelector;\n-import io.druid.segment.data.IndexedInts;\n-\n-import javax.annotation.Nullable;\n+import io.druid.query.filter.ValueMatcher;\n \n public class FilteredAggregator implements Aggregator\n {\n-  private final String name;\n-  private final DimensionSelector dimSelector;\n+  private final ValueMatcher matcher;\n   private final Aggregator delegate;\n-  private final IntPredicate predicate;\n \n-  public FilteredAggregator(String name, DimensionSelector dimSelector, IntPredicate predicate, Aggregator delegate)\n+  public FilteredAggregator(ValueMatcher matcher, Aggregator delegate)\n   {\n-    this.name = name;\n-    this.dimSelector = dimSelector;\n+    this.matcher = matcher;\n     this.delegate = delegate;\n-    this.predicate = predicate;\n   }\n \n   @Override\n   public void aggregate()\n   {\n-    final IndexedInts row = dimSelector.getRow();\n-    final int size = row.size();\n-    for (int i = 0; i < size; ++i) {\n-      if (predicate.apply(row.get(i))) {\n-        delegate.aggregate();\n-        break;\n-      }\n+    if (matcher.matches()) {\n+      delegate.aggregate();\n     }\n   }\n \n@@ -75,7 +61,7 @@ public float getFloat()\n   @Override\n   public String getName()\n   {\n-    return name;\n+    return delegate.getName();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredAggregator.java",
                "sha": "1c8bf9c76a2f70d19f6d3f41af2b923c67ba13cd",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredAggregatorFactory.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/FilteredAggregatorFactory.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 61,
                "filename": "processing/src/main/java/io/druid/query/aggregation/FilteredAggregatorFactory.java",
                "patch": "@@ -21,13 +21,10 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n-import com.metamx.common.ISE;\n-import com.metamx.common.Pair;\n import io.druid.query.filter.DimFilter;\n-import io.druid.query.filter.NotDimFilter;\n-import io.druid.query.filter.SelectorDimFilter;\n+import io.druid.query.filter.ValueMatcher;\n import io.druid.segment.ColumnSelectorFactory;\n-import io.druid.segment.DimensionSelector;\n+import io.druid.segment.filter.Filters;\n \n import java.nio.ByteBuffer;\n import java.util.Comparator;\n@@ -37,49 +34,40 @@\n {\n   private static final byte CACHE_TYPE_ID = 0x9;\n \n-  private final String name;\n   private final AggregatorFactory delegate;\n   private final DimFilter filter;\n \n   public FilteredAggregatorFactory(\n-      @JsonProperty(\"name\") String name,\n       @JsonProperty(\"aggregator\") AggregatorFactory delegate,\n       @JsonProperty(\"filter\") DimFilter filter\n   )\n   {\n     Preconditions.checkNotNull(delegate);\n     Preconditions.checkNotNull(filter);\n-    Preconditions.checkArgument(\n-        filter instanceof SelectorDimFilter ||\n-        (filter instanceof NotDimFilter && ((NotDimFilter) filter).getField() instanceof SelectorDimFilter),\n-        \"FilteredAggregator currently only supports filters of type 'selector' and their negation\"\n-    );\n \n-    this.name = name;\n     this.delegate = delegate;\n     this.filter = filter;\n   }\n \n   @Override\n-  public Aggregator factorize(ColumnSelectorFactory metricFactory)\n+  public Aggregator factorize(ColumnSelectorFactory columnSelectorFactory)\n   {\n-    final Aggregator aggregator = delegate.factorize(metricFactory);\n-    final Pair<DimensionSelector, IntPredicate> selectorPredicatePair = makeFilterPredicate(\n-        filter,\n-        metricFactory\n-    );\n-    return new FilteredAggregator(name, selectorPredicatePair.lhs, selectorPredicatePair.rhs, aggregator);\n+    final ValueMatcher valueMatcher = Filters.convertDimensionFilters(filter).makeMatcher(columnSelectorFactory);\n+      return new FilteredAggregator(\n+          valueMatcher,\n+          delegate.factorize(columnSelectorFactory)\n+      );\n+\n   }\n \n   @Override\n-  public BufferAggregator factorizeBuffered(ColumnSelectorFactory metricFactory)\n+  public BufferAggregator factorizeBuffered(ColumnSelectorFactory columnSelectorFactory)\n   {\n-    final BufferAggregator aggregator = delegate.factorizeBuffered(metricFactory);\n-    final Pair<DimensionSelector, IntPredicate> selectorPredicatePair = makeFilterPredicate(\n-        filter,\n-        metricFactory\n+    final ValueMatcher valueMatcher = Filters.convertDimensionFilters(filter).makeMatcher(columnSelectorFactory);\n+    return new FilteredBufferAggregator(\n+        valueMatcher,\n+        delegate.factorizeBuffered(columnSelectorFactory)\n     );\n-    return new FilteredBufferAggregator(selectorPredicatePair.lhs, selectorPredicatePair.rhs, aggregator);\n   }\n \n   @Override\n@@ -116,7 +104,7 @@ public Object finalizeComputation(Object object)\n   @Override\n   public String getName()\n   {\n-    return name;\n+    return delegate.getName();\n   }\n \n   @Override\n@@ -173,44 +161,42 @@ public DimFilter getFilter()\n     return delegate.getRequiredColumns();\n   }\n \n-  private static Pair<DimensionSelector, IntPredicate> makeFilterPredicate(\n-      final DimFilter dimFilter,\n-      final ColumnSelectorFactory metricFactory\n-  )\n+  @Override\n+  public String toString()\n+  {\n+    return \"FilteredAggregatorFactory{\" +\n+           \", delegate=\" + delegate +\n+           \", filter=\" + filter +\n+           '}';\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n   {\n-    final SelectorDimFilter selector;\n-    if (dimFilter instanceof NotDimFilter) {\n-      // we only support NotDimFilter with Selector filter\n-      selector = (SelectorDimFilter) ((NotDimFilter) dimFilter).getField();\n-    } else if (dimFilter instanceof SelectorDimFilter) {\n-      selector = (SelectorDimFilter) dimFilter;\n-    } else {\n-      throw new ISE(\"Unsupported DimFilter type [%d]\", dimFilter.getClass());\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n     }\n \n-    final DimensionSelector dimSelector = metricFactory.makeDimensionSelector(selector.getDimension());\n-    final int lookupId = dimSelector.lookupId(selector.getValue());\n-    final IntPredicate predicate;\n-    if (dimFilter instanceof NotDimFilter) {\n-      predicate = new IntPredicate()\n-      {\n-        @Override\n-        public boolean apply(int value)\n-        {\n-          return lookupId != value;\n-        }\n-      };\n-    } else {\n-      predicate = new IntPredicate()\n-      {\n-        @Override\n-        public boolean apply(int value)\n-        {\n-          return lookupId == value;\n-        }\n-      };\n+    FilteredAggregatorFactory that = (FilteredAggregatorFactory) o;\n+\n+    if (delegate != null ? !delegate.equals(that.delegate) : that.delegate != null) {\n+      return false;\n+    }\n+    if (filter != null ? !filter.equals(that.filter) : that.filter != null) {\n+      return false;\n     }\n-    return Pair.of(dimSelector, predicate);\n+\n+    return true;\n   }\n \n+  @Override\n+  public int hashCode()\n+  {\n+    int result = delegate != null ? delegate.hashCode() : 0;\n+    result = 31 * result + (filter != null ? filter.hashCode() : 0);\n+    return result;\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredAggregatorFactory.java",
                "sha": "d0109fb9e04bbf43fef71c5dbc5075a687369730",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredBufferAggregator.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/FilteredBufferAggregator.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 12,
                "filename": "processing/src/main/java/io/druid/query/aggregation/FilteredBufferAggregator.java",
                "patch": "@@ -19,21 +19,20 @@\n \n package io.druid.query.aggregation;\n \n+import io.druid.query.filter.ValueMatcher;\n import io.druid.segment.DimensionSelector;\n import io.druid.segment.data.IndexedInts;\n \n import java.nio.ByteBuffer;\n \n public class FilteredBufferAggregator implements BufferAggregator\n {\n-  private final DimensionSelector dimSelector;\n-  private final IntPredicate predicate;\n+  private final ValueMatcher matcher;\n   private final BufferAggregator delegate;\n \n-  public FilteredBufferAggregator(DimensionSelector dimSelector, IntPredicate predicate, BufferAggregator delegate)\n+  public FilteredBufferAggregator(ValueMatcher matcher, BufferAggregator delegate)\n   {\n-    this.dimSelector = dimSelector;\n-    this.predicate = predicate;\n+    this.matcher = matcher;\n     this.delegate = delegate;\n   }\n \n@@ -46,13 +45,8 @@ public void init(ByteBuffer buf, int position)\n   @Override\n   public void aggregate(ByteBuffer buf, int position)\n   {\n-    final IndexedInts row = dimSelector.getRow();\n-    final int size = row.size();\n-    for (int i = 0; i < size; ++i) {\n-      if (predicate.apply(row.get(i))) {\n-        delegate.aggregate(buf, position);\n-        break;\n-      }\n+    if (matcher.matches()) {\n+      delegate.aggregate(buf, position);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/aggregation/FilteredBufferAggregator.java",
                "sha": "5c1a66e4e3438351cbde479fa424dc4d482fe419",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/filter/Filter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/filter/Filter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/filter/Filter.java",
                "patch": "@@ -20,11 +20,13 @@\n package io.druid.query.filter;\n \n import com.metamx.collections.bitmap.ImmutableBitmap;\n+import io.druid.segment.ColumnSelectorFactory;\n \n /**\n  */\n public interface Filter\n {\n   public ImmutableBitmap getBitmapIndex(BitmapIndexSelector selector);\n   public ValueMatcher makeMatcher(ValueMatcherFactory factory);\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory columnSelectorFactory);\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/query/filter/Filter.java",
                "sha": "be1ff8d1845a06eb5cdc78a80bce9cddbaf081eb",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/AndFilter.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/AndFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 4,
                "filename": "processing/src/main/java/io/druid/segment/filter/AndFilter.java",
                "patch": "@@ -25,6 +25,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n \n import java.util.List;\n \n@@ -68,17 +69,31 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n     for (int i = 0; i < filters.size(); i++) {\n       matchers[i] = filters.get(i).makeMatcher(factory);\n     }\n+    return makeMatcher(matchers);\n+  }\n+\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    final ValueMatcher[] matchers = new ValueMatcher[filters.size()];\n \n-    if (matchers.length == 1) {\n-      return matchers[0];\n+    for (int i = 0; i < filters.size(); i++) {\n+      matchers[i] = filters.get(i).makeMatcher(factory);\n+    }\n+    return makeMatcher(matchers);\n+  }\n+\n+  private ValueMatcher makeMatcher(final ValueMatcher[] baseMatchers)\n+  {\n+    if (baseMatchers.length == 1) {\n+      return baseMatchers[0];\n     }\n \n     return new ValueMatcher()\n     {\n       @Override\n       public boolean matches()\n       {\n-        for (ValueMatcher matcher : matchers) {\n+        for (ValueMatcher matcher : baseMatchers) {\n           if (!matcher.matches()) {\n             return false;\n           }\n@@ -87,5 +102,4 @@ public boolean matches()\n       }\n     };\n   }\n-\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/AndFilter.java",
                "sha": "ca1b7727b50f278299ac8c1eeb699b9115165908",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/DimensionPredicateFilter.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/DimensionPredicateFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/DimensionPredicateFilter.java",
                "patch": "@@ -27,6 +27,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n import io.druid.segment.data.Indexed;\n \n import javax.annotation.Nullable;\n@@ -76,4 +77,10 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n   {\n     return factory.makeValueMatcher(dimension, predicate);\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/DimensionPredicateFilter.java",
                "sha": "1f4e8de51dc8efce0ce3f6a2ead14a91b1cc4856",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/ExtractionFilter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/ExtractionFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/ExtractionFilter.java",
                "patch": "@@ -26,6 +26,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n import io.druid.segment.data.Indexed;\n \n import java.util.List;\n@@ -75,4 +76,11 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n   {\n     throw new UnsupportedOperationException();\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/ExtractionFilter.java",
                "sha": "28302277f403bdffe71ff415d7b0be1f3c3c3097",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/JavaScriptFilter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/JavaScriptFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/JavaScriptFilter.java",
                "patch": "@@ -27,6 +27,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n import io.druid.segment.data.Indexed;\n import org.mozilla.javascript.Context;\n import org.mozilla.javascript.Function;\n@@ -160,4 +161,11 @@ public int hashCode()\n       return script.hashCode();\n     }\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/JavaScriptFilter.java",
                "sha": "3c7efc5fb9aca38f29fbaf6b1cf11f9e4be08275",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/NotFilter.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/NotFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/NotFilter.java",
                "patch": "@@ -24,6 +24,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n \n /**\n  */\n@@ -61,4 +62,19 @@ public boolean matches()\n       }\n     };\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    final ValueMatcher baseMatcher = baseFilter.makeMatcher(factory);\n+\n+    return new ValueMatcher()\n+    {\n+      @Override\n+      public boolean matches()\n+      {\n+        return !baseMatcher.matches();\n+      }\n+    };\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/NotFilter.java",
                "sha": "9303d240e5ec32a976613f612722d1161ef0e6ae",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/OrFilter.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/OrFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 3,
                "filename": "processing/src/main/java/io/druid/segment/filter/OrFilter.java",
                "patch": "@@ -25,6 +25,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n \n import java.util.List;\n \n@@ -68,17 +69,30 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n     for (int i = 0; i < filters.size(); i++) {\n       matchers[i] = filters.get(i).makeMatcher(factory);\n     }\n+    return makeMatcher(matchers);\n+  }\n+\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    final ValueMatcher[] matchers = new ValueMatcher[filters.size()];\n+\n+    for (int i = 0; i < filters.size(); i++) {\n+      matchers[i] = filters.get(i).makeMatcher(factory);\n+    }\n+    return makeMatcher(matchers);\n+  }\n \n-    if (matchers.length == 1) {\n-      return matchers[0];\n+  private ValueMatcher makeMatcher(final ValueMatcher[] baseMatchers){\n+    if (baseMatchers.length == 1) {\n+      return baseMatchers[0];\n     }\n \n     return new ValueMatcher()\n     {\n       @Override\n       public boolean matches()\n       {\n-        for (ValueMatcher matcher : matchers) {\n+        for (ValueMatcher matcher : baseMatchers) {\n           if (matcher.matches()) {\n             return true;\n           }\n@@ -87,4 +101,5 @@ public boolean matches()\n       }\n     };\n   }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/OrFilter.java",
                "sha": "73fb3448b089a173d538767e95211c13e6f25e04",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/SelectorFilter.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/SelectorFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/SelectorFilter.java",
                "patch": "@@ -20,10 +20,14 @@\n package io.druid.segment.filter;\n \n import com.metamx.collections.bitmap.ImmutableBitmap;\n+import io.druid.query.aggregation.Aggregators;\n import io.druid.query.filter.BitmapIndexSelector;\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n+import io.druid.segment.DimensionSelector;\n+import io.druid.segment.data.IndexedInts;\n \n /**\n  */\n@@ -52,4 +56,34 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n   {\n     return factory.makeValueMatcher(dimension, value);\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory columnSelectorFactory)\n+  {\n+    final DimensionSelector dimensionSelector = columnSelectorFactory.makeDimensionSelector(dimension);\n+\n+    // Missing columns are treated the same way as selector.getBitmapIndex, always returning false\n+    if (dimensionSelector == null) {\n+      return new BooleanValueMatcher(false);\n+    } else {\n+      final int valueId = dimensionSelector.lookupId(value);\n+      return new ValueMatcher()\n+      {\n+        @Override\n+        public boolean matches()\n+        {\n+          final IndexedInts row = dimensionSelector.getRow();\n+          final int size = row.size();\n+          for (int i = 0; i < size; ++i) {\n+            if (row.get(i) == valueId) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        }\n+      };\n+    }\n+  }\n+\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/SelectorFilter.java",
                "sha": "a03ed506ed5b46c5777b88cf950e33760ca8c134",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/SpatialFilter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/filter/SpatialFilter.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/filter/SpatialFilter.java",
                "patch": "@@ -24,6 +24,7 @@\n import io.druid.query.filter.Filter;\n import io.druid.query.filter.ValueMatcher;\n import io.druid.query.filter.ValueMatcherFactory;\n+import io.druid.segment.ColumnSelectorFactory;\n \n /**\n  */\n@@ -59,4 +60,11 @@ public ValueMatcher makeMatcher(ValueMatcherFactory factory)\n         bound\n     );\n   }\n+\n+  @Override\n+  public ValueMatcher makeMatcher(ColumnSelectorFactory factory)\n+  {\n+    throw new UnsupportedOperationException();\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/filter/SpatialFilter.java",
                "sha": "f1517c51311425ce68d2e835d67aeee5a9fbbf9c",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -837,7 +837,11 @@ public String get(String str)\n \n     public int getId(String value)\n     {\n-      return falseIds.get(value);\n+      if (value == null) {\n+        value = \"\";\n+      }\n+      final Integer id = falseIds.get(value);\n+      return id == null ? -1 : id;\n     }\n \n     public String getValue(int id)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "cb2cc7e3b0c27611f7fcfcc95357bfafca8fe31f",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/test/java/io/druid/query/aggregation/FilteredAggregatorTest.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/aggregation/FilteredAggregatorTest.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 4,
                "filename": "processing/src/test/java/io/druid/query/aggregation/FilteredAggregatorTest.java",
                "patch": "@@ -19,7 +19,11 @@\n \n package io.druid.query.aggregation;\n \n+import com.google.common.collect.Lists;\n+import io.druid.query.filter.AndDimFilter;\n+import io.druid.query.filter.DimFilter;\n import io.druid.query.filter.NotDimFilter;\n+import io.druid.query.filter.OrDimFilter;\n import io.druid.query.filter.SelectorDimFilter;\n import io.druid.segment.ColumnSelectorFactory;\n import io.druid.segment.DimensionSelector;\n@@ -46,7 +50,6 @@ public void testAggregate()\n     final TestFloatColumnSelector selector = new TestFloatColumnSelector(values);\n \n     FilteredAggregatorFactory factory = new FilteredAggregatorFactory(\n-        \"test\",\n         new DoubleSumAggregatorFactory(\"billy\", \"value\"),\n         new SelectorDimFilter(\"dim\", \"a\")\n     );\n@@ -55,7 +58,7 @@ public void testAggregate()\n      makeColumnSelector(selector)\n     );\n \n-    Assert.assertEquals(\"test\", agg.getName());\n+    Assert.assertEquals(\"billy\", agg.getName());\n \n     double expectedFirst = new Float(values[0]).doubleValue();\n     double expectedSecond = new Float(values[1]).doubleValue() + expectedFirst;\n@@ -164,7 +167,6 @@ public void testAggregateWithNotFilter()\n     final TestFloatColumnSelector selector = new TestFloatColumnSelector(values);\n \n     FilteredAggregatorFactory factory = new FilteredAggregatorFactory(\n-        \"test\",\n         new DoubleSumAggregatorFactory(\"billy\", \"value\"),\n         new NotDimFilter(new SelectorDimFilter(\"dim\", \"b\"))\n     );\n@@ -173,7 +175,52 @@ public void testAggregateWithNotFilter()\n         makeColumnSelector(selector)\n     );\n \n-    Assert.assertEquals(\"test\", agg.getName());\n+    Assert.assertEquals(\"billy\", agg.getName());\n+\n+    double expectedFirst = new Float(values[0]).doubleValue();\n+    double expectedSecond = new Float(values[1]).doubleValue() + expectedFirst;\n+    double expectedThird = expectedSecond;\n+    assertValues(agg, selector, expectedFirst, expectedSecond, expectedThird);\n+  }\n+\n+  @Test\n+  public void testAggregateWithOrFilter()\n+  {\n+    final float[] values = {0.15f, 0.27f, 0.14f};\n+    final TestFloatColumnSelector selector = new TestFloatColumnSelector(values);\n+\n+    FilteredAggregatorFactory factory = new FilteredAggregatorFactory(\n+        new DoubleSumAggregatorFactory(\"billy\", \"value\"),\n+        new OrDimFilter(Lists.<DimFilter>newArrayList(new SelectorDimFilter(\"dim\", \"a\"), new SelectorDimFilter(\"dim\", \"b\")))\n+    );\n+\n+    FilteredAggregator agg = (FilteredAggregator) factory.factorize(\n+        makeColumnSelector(selector)\n+    );\n+\n+    Assert.assertEquals(\"billy\", agg.getName());\n+\n+    double expectedFirst = new Float(values[0]).doubleValue();\n+    double expectedSecond = new Float(values[1]).doubleValue() + expectedFirst;\n+    double expectedThird = expectedSecond + new Float(values[2]).doubleValue();\n+    assertValues(agg, selector, expectedFirst, expectedSecond, expectedThird);\n+  }\n+\n+  @Test\n+  public void testAggregateWithAndFilter()\n+  {\n+    final float[] values = {0.15f, 0.27f};\n+    final TestFloatColumnSelector selector = new TestFloatColumnSelector(values);\n+\n+    FilteredAggregatorFactory factory = new FilteredAggregatorFactory(\n+        new DoubleSumAggregatorFactory(\"billy\", \"value\"),\n+        new AndDimFilter(Lists.<DimFilter>newArrayList(new NotDimFilter(new SelectorDimFilter(\"dim\", \"b\")), new SelectorDimFilter(\"dim\", \"a\"))));\n+\n+    FilteredAggregator agg = (FilteredAggregator) factory.factorize(\n+        makeColumnSelector(selector)\n+    );\n+\n+    Assert.assertEquals(\"billy\", agg.getName());\n \n     double expectedFirst = new Float(values[0]).doubleValue();\n     double expectedSecond = new Float(values[1]).doubleValue() + expectedFirst;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/test/java/io/druid/query/aggregation/FilteredAggregatorTest.java",
                "sha": "4f5df77fdbae5c28ec2afab06d75404b91a11433",
                "status": "modified"
            },
            {
                "additions": 250,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerTest.java",
                "changes": 251,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerTest.java?ref=e3260aa1779b9bf08309a7f0959cd46d4c3279e9",
                "deletions": 1,
                "filename": "processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerTest.java",
                "patch": "@@ -32,12 +32,15 @@\n import io.druid.query.QueryRunnerTestHelper;\n import io.druid.query.Result;\n import io.druid.query.aggregation.AggregatorFactory;\n+import io.druid.query.aggregation.CountAggregatorFactory;\n+import io.druid.query.aggregation.FilteredAggregatorFactory;\n import io.druid.query.aggregation.LongSumAggregatorFactory;\n import io.druid.query.aggregation.MaxAggregatorFactory;\n import io.druid.query.aggregation.MinAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n import io.druid.query.filter.AndDimFilter;\n import io.druid.query.filter.DimFilter;\n+import io.druid.query.filter.NotDimFilter;\n import io.druid.query.filter.RegexDimFilter;\n import io.druid.query.spec.MultipleIntervalSegmentSpec;\n import io.druid.segment.TestHelper;\n@@ -53,8 +56,8 @@\n import java.io.IOException;\n import java.util.Arrays;\n import java.util.Collection;\n-import java.util.Map;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  */\n@@ -1658,4 +1661,250 @@ public void testTimeseriesWithMultiValueDimFilterAndOr2()\n     );\n     TestHelper.assertExpectedResults(expectedResults, actualResults);\n   }\n+\n+  @Test\n+  public void testTimeSeriesWithFilteredAgg()\n+  {\n+    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.allGran)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n+                                  .aggregators(\n+                                      Lists.newArrayList(\n+                                          Iterables.concat(\n+                                              QueryRunnerTestHelper.commonAggregators,\n+                                              Lists.newArrayList(\n+                                                  new FilteredAggregatorFactory(\n+                                                      new CountAggregatorFactory(\"filteredAgg\"),\n+                                                      Druids.newSelectorDimFilterBuilder()\n+                                                            .dimension(QueryRunnerTestHelper.marketDimension)\n+                                                            .value(\"spot\")\n+                                                            .build()\n+                                                  )\n+                                              )\n+                                          )\n+                                      )\n+                                  )\n+                                  .postAggregators(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\n+                                  .build();\n+\n+    Iterable<Result<TimeseriesResultValue>> actualResults = Sequences.toList(\n+        runner.run(query, CONTEXT),\n+        Lists.<Result<TimeseriesResultValue>>newArrayList()\n+    );\n+    List<Result<TimeseriesResultValue>> expectedResults = Arrays.asList(\n+        new Result<TimeseriesResultValue>(\n+            new DateTime(\"2011-04-01\"),\n+            new TimeseriesResultValue(\n+                ImmutableMap.<String, Object>of(\n+                    \"filteredAgg\", 18L,\n+                    \"addRowsIndexConstant\", 12486.361190795898d,\n+                    \"index\", 12459.361190795898d,\n+                    \"uniques\", 9.019833517963864d,\n+                    \"rows\", 26L\n+                )\n+            )\n+        )\n+    );\n+\n+    TestHelper.assertExpectedResults(expectedResults, actualResults);\n+  }\n+\n+  @Test\n+  public void testTimeSeriesWithFilteredAggDimensionNotPresentNotNullValue()\n+  {\n+    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.allGran)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n+                                  .aggregators(\n+                                      Lists.newArrayList(\n+                                          Iterables.concat(\n+                                              QueryRunnerTestHelper.commonAggregators,\n+                                              Lists.newArrayList(\n+                                                  new FilteredAggregatorFactory(\n+                                                      new CountAggregatorFactory(\"filteredAgg\"),\n+                                                      Druids.newSelectorDimFilterBuilder()\n+                                                            .dimension(\"abraKaDabra\")\n+                                                            .value(\"Lol\")\n+                                                            .build()\n+                                                  )\n+                                              )\n+                                          )\n+                                      )\n+                                  )\n+                                  .postAggregators(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\n+                                  .build();\n+\n+    Iterable<Result<TimeseriesResultValue>> actualResults = Sequences.toList(\n+        runner.run(query, CONTEXT),\n+        Lists.<Result<TimeseriesResultValue>>newArrayList()\n+    );\n+\n+    List<Result<TimeseriesResultValue>> expectedResults = Arrays.asList(\n+        new Result<TimeseriesResultValue>(\n+            new DateTime(\"2011-04-01\"),\n+            new TimeseriesResultValue(\n+                ImmutableMap.<String, Object>of(\n+                    \"filteredAgg\", 0L,\n+                    \"addRowsIndexConstant\", 12486.361190795898d,\n+                    \"index\", 12459.361190795898d,\n+                    \"uniques\", 9.019833517963864d,\n+                    \"rows\", 26L\n+                )\n+            )\n+        )\n+    );\n+\n+    TestHelper.assertExpectedResults(expectedResults, actualResults);\n+  }\n+\n+  @Test\n+  public void testTimeSeriesWithFilteredAggDimensionNotPresentNullValue()\n+  {\n+    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.allGran)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n+                                  .aggregators(\n+                                      Lists.newArrayList(\n+                                          Iterables.concat(\n+                                              QueryRunnerTestHelper.commonAggregators,\n+                                              Lists.newArrayList(\n+                                                  new FilteredAggregatorFactory(\n+                                                      new CountAggregatorFactory(\"filteredAgg\"),\n+                                                      Druids.newSelectorDimFilterBuilder()\n+                                                            .dimension(\"abraKaDabra\")\n+                                                            .value(null)\n+                                                            .build()\n+                                                  )\n+                                              )\n+                                          )\n+                                      )\n+                                  )\n+                                  .postAggregators(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\n+                                  .build();\n+\n+    Iterable<Result<TimeseriesResultValue>> actualResults = Sequences.toList(\n+        runner.run(query, CONTEXT),\n+        Lists.<Result<TimeseriesResultValue>>newArrayList()\n+    );\n+\n+    List<Result<TimeseriesResultValue>> expectedResults = Arrays.asList(\n+        new Result<TimeseriesResultValue>(\n+            new DateTime(\"2011-04-01\"),\n+            new TimeseriesResultValue(\n+                ImmutableMap.<String, Object>of(\n+                    \"filteredAgg\", 0L,\n+                    \"addRowsIndexConstant\", 12486.361190795898d,\n+                    \"index\", 12459.361190795898d,\n+                    \"uniques\", 9.019833517963864d,\n+                    \"rows\", 26L\n+                )\n+            )\n+        )\n+    );\n+\n+    TestHelper.assertExpectedResults(expectedResults, actualResults);\n+  }\n+\n+  @Test\n+  public void testTimeSeriesWithFilteredAggValueNotPresent()\n+  {\n+    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.allGran)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n+                                  .aggregators(\n+                                      Lists.newArrayList(\n+                                          Iterables.concat(\n+                                              QueryRunnerTestHelper.commonAggregators,\n+                                              Lists.newArrayList(\n+                                                  new FilteredAggregatorFactory(\n+                                                      new CountAggregatorFactory(\"filteredAgg\"),\n+                                                      new NotDimFilter(\n+                                                          Druids.newSelectorDimFilterBuilder()\n+                                                                .dimension(QueryRunnerTestHelper.marketDimension)\n+                                                                .value(\"LolLol\")\n+                                                                .build()\n+                                                      )\n+                                                  )\n+                                              )\n+                                          )\n+                                      )\n+                                  )\n+                                  .postAggregators(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\n+                                  .build();\n+\n+    Iterable<Result<TimeseriesResultValue>> actualResults = Sequences.toList(\n+        runner.run(query, CONTEXT),\n+        Lists.<Result<TimeseriesResultValue>>newArrayList()\n+    );\n+    List<Result<TimeseriesResultValue>> expectedResults = Arrays.asList(\n+        new Result<TimeseriesResultValue>(\n+            new DateTime(\"2011-04-01\"),\n+            new TimeseriesResultValue(\n+                ImmutableMap.<String, Object>of(\n+                    \"filteredAgg\", 26L,\n+                    \"addRowsIndexConstant\", 12486.361190795898d,\n+                    \"index\", 12459.361190795898d,\n+                    \"uniques\", 9.019833517963864d,\n+                    \"rows\", 26L\n+                )\n+            )\n+        )\n+    );\n+\n+    TestHelper.assertExpectedResults(expectedResults, actualResults);\n+  }\n+\n+  @Test\n+  public void testTimeSeriesWithFilteredAggNullValue()\n+  {\n+    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n+                                  .dataSource(QueryRunnerTestHelper.dataSource)\n+                                  .granularity(QueryRunnerTestHelper.allGran)\n+                                  .intervals(QueryRunnerTestHelper.firstToThird)\n+                                  .aggregators(\n+                                      Lists.newArrayList(\n+                                          Iterables.concat(\n+                                              QueryRunnerTestHelper.commonAggregators,\n+                                              Lists.newArrayList(\n+                                                  new FilteredAggregatorFactory(\n+                                                      new CountAggregatorFactory(\"filteredAgg\"),\n+                                                      new NotDimFilter(\n+                                                          Druids.newSelectorDimFilterBuilder()\n+                                                                .dimension(QueryRunnerTestHelper.marketDimension)\n+                                                                .value(null)\n+                                                                .build()\n+                                                      )\n+                                                  )\n+                                              )\n+                                          )\n+                                      )\n+                                  )\n+                                  .postAggregators(Arrays.<PostAggregator>asList(QueryRunnerTestHelper.addRowsIndexConstant))\n+                                  .build();\n+\n+    Iterable<Result<TimeseriesResultValue>> actualResults = Sequences.toList(\n+        runner.run(query, CONTEXT),\n+        Lists.<Result<TimeseriesResultValue>>newArrayList()\n+    );\n+    List<Result<TimeseriesResultValue>> expectedResults = Arrays.asList(\n+        new Result<TimeseriesResultValue>(\n+            new DateTime(\"2011-04-01\"),\n+            new TimeseriesResultValue(\n+                ImmutableMap.<String, Object>of(\n+                    \"filteredAgg\", 26L,\n+                    \"addRowsIndexConstant\", 12486.361190795898d,\n+                    \"index\", 12459.361190795898d,\n+                    \"uniques\", 9.019833517963864d,\n+                    \"rows\", 26L\n+                )\n+            )\n+        )\n+    );\n+\n+    TestHelper.assertExpectedResults(expectedResults, actualResults);\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e3260aa1779b9bf08309a7f0959cd46d4c3279e9/processing/src/test/java/io/druid/query/timeseries/TimeseriesQueryRunnerTest.java",
                "sha": "de8886c7d9e150f859c0e36394d0caa0773196bd",
                "status": "modified"
            }
        ],
        "message": "Filtered Aggregator fixes + enhancements\n\n- fix NPE on IncrementIndex\n- refactor code to support AND, OR filter\n- tests for AND & OR filter\n- handling for missing column / null values",
        "parent": "https://github.com/apache/incubator-druid/commit/2f08ab85fc3673fea9e7afc99ce8339f1c9ed1bd",
        "repo": "incubator-druid",
        "unit_tests": [
            "FilteredAggregatorTest.java",
            "FilteredAggregatorFactoryTest.java",
            "AndFilterTest.java",
            "JavaScriptFilterTest.java",
            "NotFilterTest.java",
            "SelectorFilterTest.java",
            "SpatialFilterTest.java",
            "IncrementalIndexTest.java"
        ]
    },
    "incubator-druid_e4e18b1": {
        "bug_id": "incubator-druid_e4e18b1",
        "commit": "https://github.com/apache/incubator-druid/commit/e4e18b17b5232816e150eb2e18159f807d80b178",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e4e18b17b5232816e150eb2e18159f807d80b178/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java?ref=e4e18b17b5232816e150eb2e18159f807d80b178",
                "deletions": 18,
                "filename": "server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "patch": "@@ -165,16 +165,15 @@ public Boolean withHandle(Handle handle) throws Exception\n           @Override\n           public Optional<EntryType> withHandle(Handle handle) throws Exception\n           {\n+            byte[] res = handle.createQuery(\n+                String.format(\"SELECT payload FROM %s WHERE id = :id\", entryTable)\n+            )\n+                               .bind(\"id\", entryId)\n+                               .map(ByteArrayMapper.FIRST)\n+                               .first();\n+\n             return Optional.fromNullable(\n-                jsonMapper.<EntryType>readValue(\n-                    handle.createQuery(\n-                        String.format(\"SELECT payload FROM %s WHERE id = :id\", entryTable)\n-                    )\n-                          .bind(\"id\", entryId)\n-                          .map(ByteArrayMapper.FIRST)\n-                          .first(),\n-                    entryType\n-                )\n+                res == null ? null : jsonMapper.<EntryType>readValue(res, entryType)\n             );\n           }\n         }\n@@ -190,16 +189,15 @@ public Boolean withHandle(Handle handle) throws Exception\n           @Override\n           public Optional<StatusType> withHandle(Handle handle) throws Exception\n           {\n+            byte[] res = handle.createQuery(\n+                String.format(\"SELECT status_payload FROM %s WHERE id = :id\", entryTable)\n+            )\n+                               .bind(\"id\", entryId)\n+                               .map(ByteArrayMapper.FIRST)\n+                               .first();\n+\n             return Optional.fromNullable(\n-                jsonMapper.<StatusType>readValue(\n-                    handle.createQuery(\n-                        String.format(\"SELECT status_payload FROM %s WHERE id = :id\", entryTable)\n-                    )\n-                          .bind(\"id\", entryId)\n-                          .map(ByteArrayMapper.FIRST)\n-                          .first(),\n-                    statusType\n-                )\n+                res == null ? null : jsonMapper.<StatusType>readValue(res, statusType)\n             );\n           }\n         }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e4e18b17b5232816e150eb2e18159f807d80b178/server/src/main/java/io/druid/metadata/SQLMetadataStorageActionHandler.java",
                "sha": "271495790c6d9022c93332db1fb0c61924fdfe19",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e4e18b17b5232816e150eb2e18159f807d80b178/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java?ref=e4e18b17b5232816e150eb2e18159f807d80b178",
                "deletions": 0,
                "filename": "server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "patch": "@@ -118,8 +118,12 @@ public void testEntryAndStatus() throws Exception\n         handler.getEntry(entryId)\n     );\n \n+    Assert.assertEquals(Optional.absent(), handler.getEntry(\"non_exist_entry\"));\n+\n     Assert.assertEquals(Optional.absent(), handler.getStatus(entryId));\n \n+    Assert.assertEquals(Optional.absent(), handler.getStatus(\"non_exist_entry\"));\n+\n     Assert.assertTrue(handler.setStatus(entryId, true, status1));\n \n     Assert.assertEquals(\n@@ -179,6 +183,11 @@ public void testLogs() throws Exception\n \n     handler.insert(entryId, new DateTime(\"2014-01-01\"), \"test\", entry, true, status);\n \n+    Assert.assertEquals(\n+        ImmutableList.of(),\n+        handler.getLogs(\"non_exist_entry\")\n+    );\n+\n     Assert.assertEquals(\n         ImmutableMap.of(),\n         handler.getLocks(entryId)\n@@ -206,6 +215,11 @@ public void testLocks() throws Exception\n \n     handler.insert(entryId, new DateTime(\"2014-01-01\"), \"test\", entry, true, status);\n \n+    Assert.assertEquals(\n+        ImmutableMap.<Long, Map<String, Integer>>of(),\n+        handler.getLocks(\"non_exist_entry\")\n+    );\n+\n     Assert.assertEquals(\n         ImmutableMap.<Long, Map<String, Integer>>of(),\n         handler.getLocks(entryId)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e4e18b17b5232816e150eb2e18159f807d80b178/server/src/test/java/io/druid/metadata/SQLMetadataStorageActionHandlerTest.java",
                "sha": "e9f0788184990e8909bc74f8d07cadd48220888c",
                "status": "modified"
            }
        ],
        "message": "fix npe thrown from getEntry() and getStatus() SQLMetadataStorageActionHandler due to a non-exist entryId",
        "parent": "https://github.com/apache/incubator-druid/commit/0c85c8c60a31863f8bdac2d46a92e87d86dd5d9a",
        "repo": "incubator-druid",
        "unit_tests": [
            "SQLMetadataStorageActionHandlerTest.java"
        ]
    },
    "incubator-druid_e5653f0": {
        "bug_id": "incubator-druid_e5653f0",
        "commit": "https://github.com/apache/incubator-druid/commit/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
        "file": [
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/main/java/io/druid/indexing/common/task/AppendTask.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/task/AppendTask.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 16,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/AppendTask.java",
                "patch": "@@ -22,6 +22,7 @@\n import com.google.common.base.Function;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Predicate;\n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Ordering;\n import io.druid.segment.IndexIO;\n@@ -33,8 +34,10 @@\n import io.druid.timeline.DataSegment;\n import io.druid.timeline.TimelineObjectHolder;\n import io.druid.timeline.VersionedIntervalTimeline;\n+import io.druid.timeline.partition.PartitionChunk;\n import org.joda.time.Interval;\n \n+import javax.annotation.Nullable;\n import java.io.File;\n import java.util.List;\n import java.util.Map;\n@@ -65,22 +68,36 @@ public File merge(final Map<DataSegment, File> segments, final File outDir)\n       timeline.add(segment.getInterval(), segment.getVersion(), segment.getShardSpec().createChunk(segment));\n     }\n \n-    final List<SegmentToMergeHolder> segmentsToMerge = Lists.transform(\n-        timeline.lookup(new Interval(\"1000-01-01/3000-01-01\")),\n-        new Function<TimelineObjectHolder<String, DataSegment>, SegmentToMergeHolder>()\n-        {\n-          @Override\n-          public SegmentToMergeHolder apply(TimelineObjectHolder<String, DataSegment> input)\n-          {\n-            final DataSegment segment = input.getObject().getChunk(0).getObject();\n-            final File file = Preconditions.checkNotNull(\n-                segments.get(segment),\n-                \"File for segment %s\", segment.getIdentifier()\n-            );\n-\n-            return new SegmentToMergeHolder(segment, input.getInterval(), file);\n-          }\n-        }\n+    final Iterable<SegmentToMergeHolder> segmentsToMerge = Iterables.concat(\n+        Iterables.transform(\n+            timeline.lookup(new Interval(\"1000-01-01/3000-01-01\")),\n+            new Function<TimelineObjectHolder<String, DataSegment>, Iterable<SegmentToMergeHolder>>()\n+            {\n+              @Override\n+              public Iterable<SegmentToMergeHolder> apply(final TimelineObjectHolder<String, DataSegment> input)\n+              {\n+                return Iterables.transform(\n+                    input.getObject(),\n+                    new Function<PartitionChunk<DataSegment>, SegmentToMergeHolder>()\n+                    {\n+                      @Nullable\n+                      @Override\n+                      public SegmentToMergeHolder apply(PartitionChunk<DataSegment> chunkInput)\n+                      {\n+                        DataSegment segment = chunkInput.getObject();\n+                        return new SegmentToMergeHolder(\n+                            segment, input.getInterval(),\n+                            Preconditions.checkNotNull(\n+                                segments.get(segment),\n+                                \"File for segment %s\", segment.getIdentifier()\n+                            )\n+                        );\n+                      }\n+                    }\n+                );\n+              }\n+            }\n+        )\n     );\n \n     List<IndexableAdapter> adapters = Lists.newArrayList();",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/main/java/io/druid/indexing/common/task/AppendTask.java",
                "sha": "ddf2a8c59296db86b5dc0d22bd31c3f8769d110f",
                "status": "modified"
            },
            {
                "additions": 196,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "changes": 319,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 123,
                "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "patch": "@@ -29,6 +29,7 @@\n import com.google.common.collect.Ordering;\n import com.google.common.collect.Sets;\n import com.google.inject.Injector;\n+import com.google.inject.Provider;\n import com.metamx.common.guava.Sequence;\n import com.metamx.common.guava.Sequences;\n import com.metamx.common.guava.Yielder;\n@@ -61,14 +62,14 @@\n import io.druid.timeline.DataSegment;\n import io.druid.timeline.TimelineObjectHolder;\n import io.druid.timeline.VersionedIntervalTimeline;\n+import io.druid.timeline.partition.PartitionChunk;\n import io.druid.utils.Runnables;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n import javax.annotation.Nullable;\n import java.io.File;\n import java.io.IOException;\n-import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -166,15 +167,43 @@ public Firehose connect(InputRowParser inputRowParser) throws IOException, Parse\n       } else if (inputRowParser.getParseSpec().getDimensionsSpec().hasCustomDimensions()) {\n         dims = inputRowParser.getParseSpec().getDimensionsSpec().getDimensions();\n       } else {\n-        Set<String> dimSet = new HashSet<>();\n-        for (TimelineObjectHolder<String, DataSegment> timelineObjectHolder : timeLineSegments) {\n-          dimSet.addAll(timelineObjectHolder.getObject().getChunk(0).getObject().getDimensions());\n-        }\n+        Set<String> dimSet = Sets.newHashSet(\n+            Iterables.concat(\n+                Iterables.transform(\n+                    timeLineSegments,\n+                    new Function<TimelineObjectHolder<String, DataSegment>, Iterable<String>>()\n+                    {\n+                      @Override\n+                      public Iterable<String> apply(\n+                          TimelineObjectHolder<String, DataSegment> timelineObjectHolder\n+                      )\n+                      {\n+                        return Iterables.concat(\n+                            Iterables.transform(\n+                                timelineObjectHolder.getObject(),\n+                                new Function<PartitionChunk<DataSegment>, Iterable<String>>()\n+                                {\n+                                  @Override\n+                                  public Iterable<String> apply(PartitionChunk<DataSegment> input)\n+                                  {\n+                                    return input.getObject().getDimensions();\n+                                  }\n+                                }\n+                            )\n+                        );\n+                      }\n+                    }\n+\n+                )\n+            )\n+        );\n         dims = Lists.newArrayList(\n             Sets.difference(\n                 dimSet,\n-                inputRowParser.getParseSpec().getDimensionsSpec()\n-                              .getDimensionExclusions()\n+                inputRowParser\n+                    .getParseSpec()\n+                    .getDimensionsSpec()\n+                    .getDimensionExclusions()\n             )\n         );\n       }\n@@ -183,35 +212,79 @@ public Firehose connect(InputRowParser inputRowParser) throws IOException, Parse\n       if (metrics != null) {\n         metricsList = metrics;\n       } else {\n-        Set<String> metricsSet = new HashSet<>();\n-        for (TimelineObjectHolder<String, DataSegment> timelineObjectHolder : timeLineSegments) {\n-          metricsSet.addAll(timelineObjectHolder.getObject().getChunk(0).getObject().getMetrics());\n-        }\n+        Set<String> metricsSet = Sets.newHashSet(\n+            Iterables.concat(\n+                Iterables.transform(\n+                    timeLineSegments,\n+                    new Function<TimelineObjectHolder<String, DataSegment>, Iterable<String>>()\n+                    {\n+                      @Override\n+                      public Iterable<String> apply(\n+                          TimelineObjectHolder<String, DataSegment> input\n+                      )\n+                      {\n+                        return Iterables.concat(\n+                            Iterables.transform(\n+                                input.getObject(),\n+                                new Function<PartitionChunk<DataSegment>, Iterable<String>>()\n+                                {\n+                                  @Override\n+                                  public Iterable<String> apply(PartitionChunk<DataSegment> input)\n+                                  {\n+                                    return input.getObject().getMetrics();\n+                                  }\n+                                }\n+                            )\n+                        );\n+                      }\n+                    }\n+                )\n+            )\n+        );\n         metricsList = Lists.newArrayList(metricsSet);\n       }\n \n \n-      final List<StorageAdapter> adapters = Lists.transform(\n-          timeLineSegments,\n-          new Function<TimelineObjectHolder<String, DataSegment>, StorageAdapter>()\n-          {\n-            @Override\n-            public StorageAdapter apply(TimelineObjectHolder<String, DataSegment> input)\n-            {\n-              final DataSegment segment = input.getObject().getChunk(0).getObject();\n-              final File file = Preconditions.checkNotNull(\n-                  segmentFileMap.get(segment),\n-                  \"File for segment %s\", segment.getIdentifier()\n-              );\n-\n-              try {\n-                return new QueryableIndexStorageAdapter((IndexIO.loadIndex(file)));\n-              }\n-              catch (IOException e) {\n-                throw Throwables.propagate(e);\n-              }\n-            }\n-          }\n+      final List<StorageAdapter> adapters = Lists.newArrayList(\n+          Iterables.concat(\n+              Iterables.transform(\n+                  timeLineSegments,\n+                  new Function<TimelineObjectHolder<String, DataSegment>, Iterable<StorageAdapter>>()\n+                  {\n+                    @Override\n+                    public Iterable<StorageAdapter> apply(\n+                        TimelineObjectHolder<String, DataSegment> input\n+                    )\n+                    {\n+                      return\n+                          Iterables.transform(\n+                              input.getObject(),\n+                              new Function<PartitionChunk<DataSegment>, StorageAdapter>()\n+                              {\n+                                @Override\n+                                public StorageAdapter apply(PartitionChunk<DataSegment> input)\n+                                {\n+                                  final DataSegment segment = input.getObject();\n+                                  try {\n+                                    return new QueryableIndexStorageAdapter(\n+                                        IndexIO.loadIndex(\n+                                            Preconditions.checkNotNull(\n+                                                segmentFileMap.get(segment),\n+                                                \"File for segment %s\", segment.getIdentifier()\n+                                            )\n+                                        )\n+                                    );\n+                                  }\n+                                  catch (IOException e) {\n+                                    throw Throwables.propagate(e);\n+                                  }\n+                                }\n+                              }\n+                          );\n+                    }\n+                  }\n+              )\n+          )\n       );\n \n       return new IngestSegmentFirehose(adapters, dims, metricsList);\n@@ -235,112 +308,112 @@ public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> d\n       Sequence<InputRow> rows = Sequences.concat(\n           Iterables.transform(\n               adapters, new Function<StorageAdapter, Sequence<InputRow>>()\n-          {\n-            @Nullable\n-            @Override\n-            public Sequence<InputRow> apply(StorageAdapter adapter)\n-            {\n-              return Sequences.concat(\n-                  Sequences.map(\n-                      adapter.makeCursors(\n-                          Filters.convertDimensionFilters(dimFilter),\n-                          interval,\n-                          QueryGranularity.ALL\n-                      ), new Function<Cursor, Sequence<InputRow>>()\n-                  {\n-                    @Nullable\n-                    @Override\n-                    public Sequence<InputRow> apply(final Cursor cursor)\n-                    {\n-                      final LongColumnSelector timestampColumnSelector = cursor.makeLongColumnSelector(Column.TIME_COLUMN_NAME);\n-\n-                      final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n-                      for (String dim : dims) {\n-                        final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim, null);\n-                        // dimSelector is null if the dimension is not present\n-                        if (dimSelector != null) {\n-                          dimSelectors.put(dim, dimSelector);\n-                        }\n-                      }\n-\n-                      final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n-                      for (String metric : metrics) {\n-                        final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n-                        if (metricSelector != null) {\n-                          metSelectors.put(metric, metricSelector);\n-                        }\n-                      }\n-\n-                      return Sequences.simple(\n-                          new Iterable<InputRow>()\n+              {\n+                @Nullable\n+                @Override\n+                public Sequence<InputRow> apply(StorageAdapter adapter)\n+                {\n+                  return Sequences.concat(\n+                      Sequences.map(\n+                          adapter.makeCursors(\n+                              Filters.convertDimensionFilters(dimFilter),\n+                              interval,\n+                              QueryGranularity.ALL\n+                          ), new Function<Cursor, Sequence<InputRow>>()\n                           {\n+                            @Nullable\n                             @Override\n-                            public Iterator<InputRow> iterator()\n+                            public Sequence<InputRow> apply(final Cursor cursor)\n                             {\n-                              return new Iterator<InputRow>()\n-                              {\n-                                @Override\n-                                public boolean hasNext()\n-                                {\n-                                  return !cursor.isDone();\n+                              final LongColumnSelector timestampColumnSelector = cursor.makeLongColumnSelector(Column.TIME_COLUMN_NAME);\n+\n+                              final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n+                              for (String dim : dims) {\n+                                final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim, null);\n+                                // dimSelector is null if the dimension is not present\n+                                if (dimSelector != null) {\n+                                  dimSelectors.put(dim, dimSelector);\n                                 }\n+                              }\n \n-                                @Override\n-                                public InputRow next()\n-                                {\n-                                  final Map<String, Object> theEvent = Maps.newLinkedHashMap();\n-                                  final long timestamp = timestampColumnSelector.get();\n-                                  theEvent.put(EventHolder.timestampKey, new DateTime(timestamp));\n-\n-                                  for (Map.Entry<String, DimensionSelector> dimSelector : dimSelectors.entrySet()) {\n-                                    final String dim = dimSelector.getKey();\n-                                    final DimensionSelector selector = dimSelector.getValue();\n-                                    final IndexedInts vals = selector.getRow();\n-\n-                                    if (vals.size() == 1) {\n-                                      final String dimVal = selector.lookupName(vals.get(0));\n-                                      theEvent.put(dim, dimVal);\n-                                    } else {\n-                                      List<String> dimVals = Lists.newArrayList();\n-                                      for (int i = 0; i < vals.size(); ++i) {\n-                                        dimVals.add(selector.lookupName(vals.get(i)));\n-                                      }\n-                                      theEvent.put(dim, dimVals);\n+                              final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n+                              for (String metric : metrics) {\n+                                final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n+                                if (metricSelector != null) {\n+                                  metSelectors.put(metric, metricSelector);\n+                                }\n+                              }\n+\n+                              return Sequences.simple(\n+                                  new Iterable<InputRow>()\n+                                  {\n+                                    @Override\n+                                    public Iterator<InputRow> iterator()\n+                                    {\n+                                      return new Iterator<InputRow>()\n+                                      {\n+                                        @Override\n+                                        public boolean hasNext()\n+                                        {\n+                                          return !cursor.isDone();\n+                                        }\n+\n+                                        @Override\n+                                        public InputRow next()\n+                                        {\n+                                          final Map<String, Object> theEvent = Maps.newLinkedHashMap();\n+                                          final long timestamp = timestampColumnSelector.get();\n+                                          theEvent.put(EventHolder.timestampKey, new DateTime(timestamp));\n+\n+                                          for (Map.Entry<String, DimensionSelector> dimSelector : dimSelectors.entrySet()) {\n+                                            final String dim = dimSelector.getKey();\n+                                            final DimensionSelector selector = dimSelector.getValue();\n+                                            final IndexedInts vals = selector.getRow();\n+\n+                                            if (vals.size() == 1) {\n+                                              final String dimVal = selector.lookupName(vals.get(0));\n+                                              theEvent.put(dim, dimVal);\n+                                            } else {\n+                                              List<String> dimVals = Lists.newArrayList();\n+                                              for (int i = 0; i < vals.size(); ++i) {\n+                                                dimVals.add(selector.lookupName(vals.get(i)));\n+                                              }\n+                                              theEvent.put(dim, dimVals);\n+                                            }\n+                                          }\n+\n+                                          for (Map.Entry<String, ObjectColumnSelector> metSelector : metSelectors.entrySet()) {\n+                                            final String metric = metSelector.getKey();\n+                                            final ObjectColumnSelector selector = metSelector.getValue();\n+                                            theEvent.put(metric, selector.get());\n+                                          }\n+                                          cursor.advance();\n+                                          return new MapBasedInputRow(timestamp, dims, theEvent);\n+                                        }\n+\n+                                        @Override\n+                                        public void remove()\n+                                        {\n+                                          throw new UnsupportedOperationException(\"Remove Not Supported\");\n+                                        }\n+                                      };\n                                     }\n                                   }\n-\n-                                  for (Map.Entry<String, ObjectColumnSelector> metSelector : metSelectors.entrySet()) {\n-                                    final String metric = metSelector.getKey();\n-                                    final ObjectColumnSelector selector = metSelector.getValue();\n-                                    theEvent.put(metric, selector.get());\n-                                  }\n-                                  cursor.advance();\n-                                  return new MapBasedInputRow(timestamp, dims, theEvent);\n-                                }\n-\n-                                @Override\n-                                public void remove()\n-                                {\n-                                  throw new UnsupportedOperationException(\"Remove Not Supported\");\n-                                }\n-                              };\n+                              );\n                             }\n                           }\n-                      );\n-                    }\n-                  }\n-                  )\n-              );\n-            }\n-          }\n+                      )\n+                  );\n+                }\n+              }\n           )\n       );\n       rowYielder = rows.toYielder(\n           null,\n-          new YieldingAccumulator()\n+          new YieldingAccumulator<InputRow, InputRow>()\n           {\n             @Override\n-            public Object accumulate(Object accumulated, Object in)\n+            public InputRow accumulate(InputRow accumulated, InputRow in)\n             {\n               yield();\n               return in;",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "sha": "98f48800f846a033fcbb379123b20113599e8d15",
                "status": "modified"
            },
            {
                "additions": 460,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "changes": 460,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 0,
                "filename": "indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "patch": "@@ -0,0 +1,460 @@\n+/*\n+ * Druid - a distributed column store.\n+ * Copyright 2012 - 2015 Metamarkets Group Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.druid.indexing.firehose;\n+\n+import com.google.api.client.repackaged.com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import com.google.common.io.Files;\n+import com.google.inject.Binder;\n+import com.google.inject.Guice;\n+import com.google.inject.Module;\n+import com.metamx.common.logger.Logger;\n+import com.metamx.emitter.core.Event;\n+import com.metamx.emitter.service.ServiceEmitter;\n+import com.metamx.emitter.service.ServiceEventBuilder;\n+import io.druid.common.utils.JodaUtils;\n+import io.druid.data.input.InputRow;\n+import io.druid.data.input.impl.DimensionsSpec;\n+import io.druid.data.input.impl.InputRowParser;\n+import io.druid.data.input.impl.JSONParseSpec;\n+import io.druid.data.input.impl.MapInputRowParser;\n+import io.druid.data.input.impl.SpatialDimensionSchema;\n+import io.druid.data.input.impl.TimestampSpec;\n+import io.druid.granularity.QueryGranularity;\n+import io.druid.indexing.common.SegmentLoaderFactory;\n+import io.druid.indexing.common.TaskToolboxFactory;\n+import io.druid.indexing.common.actions.LocalTaskActionClientFactory;\n+import io.druid.indexing.common.actions.TaskActionToolbox;\n+import io.druid.indexing.common.config.TaskConfig;\n+import io.druid.indexing.common.config.TaskStorageConfig;\n+import io.druid.indexing.overlord.HeapMemoryTaskStorage;\n+import io.druid.indexing.overlord.TaskLockbox;\n+import io.druid.jackson.DefaultObjectMapper;\n+import io.druid.metadata.IndexerSQLMetadataStorageCoordinator;\n+import io.druid.query.aggregation.AggregatorFactory;\n+import io.druid.query.aggregation.DoubleSumAggregatorFactory;\n+import io.druid.query.aggregation.LongSumAggregatorFactory;\n+import io.druid.query.filter.SelectorDimFilter;\n+import io.druid.segment.IndexMerger;\n+import io.druid.segment.incremental.IncrementalIndexSchema;\n+import io.druid.segment.incremental.OnheapIncrementalIndex;\n+import io.druid.segment.loading.DataSegmentArchiver;\n+import io.druid.segment.loading.DataSegmentKiller;\n+import io.druid.segment.loading.DataSegmentMover;\n+import io.druid.segment.loading.DataSegmentPuller;\n+import io.druid.segment.loading.DataSegmentPusher;\n+import io.druid.segment.loading.LocalDataSegmentPuller;\n+import io.druid.segment.loading.OmniSegmentLoader;\n+import io.druid.segment.loading.SegmentLoaderConfig;\n+import io.druid.segment.loading.SegmentLoadingException;\n+import io.druid.segment.loading.StorageLocationConfig;\n+import io.druid.timeline.DataSegment;\n+import io.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ *\n+ */\n+@RunWith(Parameterized.class)\n+public class IngestSegmentFirehoseFactoryTest\n+{\n+  @Parameterized.Parameters(name = \"{1}\")\n+  public static Collection<Object[]> constructorFeeder() throws IOException\n+  {\n+\n+    final HeapMemoryTaskStorage ts = new HeapMemoryTaskStorage(\n+        new TaskStorageConfig(null)\n+        {\n+        }\n+    );\n+    final IncrementalIndexSchema schema = new IncrementalIndexSchema.Builder()\n+        .withQueryGranularity(QueryGranularity.NONE)\n+        .withMinTimestamp(JodaUtils.MIN_INSTANT)\n+        .withDimensionsSpec(ROW_PARSER)\n+        .withMetrics(\n+            new AggregatorFactory[]{\n+                new LongSumAggregatorFactory(METRIC_LONG_NAME, DIM_LONG_NAME),\n+                new DoubleSumAggregatorFactory(METRIC_FLOAT_NAME, DIM_FLOAT_NAME)\n+            }\n+        )\n+        .build();\n+    final OnheapIncrementalIndex index = new OnheapIncrementalIndex(\n+        schema,\n+        MAX_ROWS * MAX_SHARD_NUMBER\n+    );\n+\n+    for (Integer i = 0; i < MAX_ROWS; ++i) {\n+      index.add(ROW_PARSER.parse(buildRow(i.longValue())));\n+    }\n+\n+    if (!persistDir.mkdirs() && !persistDir.exists()) {\n+      throw new IOException(String.format(\"Could not create directory at [%s]\", persistDir.getAbsolutePath()));\n+    }\n+    IndexMerger.persist(index, persistDir);\n+\n+    final TaskLockbox tl = new TaskLockbox(ts);\n+    final IndexerSQLMetadataStorageCoordinator mdc = new IndexerSQLMetadataStorageCoordinator(null, null, null)\n+    {\n+      final private Set<DataSegment> published = Sets.newHashSet();\n+      final private Set<DataSegment> nuked = Sets.newHashSet();\n+\n+      @Override\n+      public List<DataSegment> getUsedSegmentsForInterval(String dataSource, Interval interval) throws IOException\n+      {\n+        return ImmutableList.copyOf(segmentSet);\n+      }\n+\n+      @Override\n+      public List<DataSegment> getUnusedSegmentsForInterval(String dataSource, Interval interval)\n+      {\n+        return ImmutableList.of();\n+      }\n+\n+      @Override\n+      public Set<DataSegment> announceHistoricalSegments(Set<DataSegment> segments)\n+      {\n+        Set<DataSegment> added = Sets.newHashSet();\n+        for (final DataSegment segment : segments) {\n+          if (published.add(segment)) {\n+            added.add(segment);\n+          }\n+        }\n+\n+        return ImmutableSet.copyOf(added);\n+      }\n+\n+      @Override\n+      public void deleteSegments(Set<DataSegment> segments)\n+      {\n+        nuked.addAll(segments);\n+      }\n+    };\n+    final LocalTaskActionClientFactory tac = new LocalTaskActionClientFactory(\n+        ts,\n+        new TaskActionToolbox(tl, mdc, newMockEmitter())\n+    );\n+    final TaskToolboxFactory taskToolboxFactory = new TaskToolboxFactory(\n+        new TaskConfig(tmpDir.getAbsolutePath(), null, null, 50000, null),\n+        tac,\n+        newMockEmitter(),\n+        new DataSegmentPusher()\n+        {\n+          @Override\n+          public String getPathForHadoop(String dataSource)\n+          {\n+            throw new UnsupportedOperationException();\n+          }\n+\n+          @Override\n+          public DataSegment push(File file, DataSegment segment) throws IOException\n+          {\n+            return segment;\n+          }\n+        },\n+        new DataSegmentKiller()\n+        {\n+          @Override\n+          public void kill(DataSegment segments) throws SegmentLoadingException\n+          {\n+\n+          }\n+        },\n+        new DataSegmentMover()\n+        {\n+          @Override\n+          public DataSegment move(DataSegment dataSegment, Map<String, Object> targetLoadSpec)\n+              throws SegmentLoadingException\n+          {\n+            return dataSegment;\n+          }\n+        },\n+        new DataSegmentArchiver()\n+        {\n+          @Override\n+          public DataSegment archive(DataSegment segment) throws SegmentLoadingException\n+          {\n+            return segment;\n+          }\n+\n+          @Override\n+          public DataSegment restore(DataSegment segment) throws SegmentLoadingException\n+          {\n+            return segment;\n+          }\n+        },\n+        null, // segment announcer\n+        null, // new segment server view\n+        null, // query runner factory conglomerate corporation unionized collective\n+        null, // query executor service\n+        null, // monitor scheduler\n+        new SegmentLoaderFactory(\n+            new OmniSegmentLoader(\n+                ImmutableMap.<String, DataSegmentPuller>of(\n+                    \"local\",\n+                    new LocalDataSegmentPuller()\n+                ),\n+                null,\n+                new SegmentLoaderConfig()\n+                {\n+                  @Override\n+                  public List<StorageLocationConfig> getLocations()\n+                  {\n+                    return Lists.newArrayList();\n+                  }\n+                }\n+            )\n+        ),\n+        new DefaultObjectMapper()\n+    );\n+    Collection<Object[]> values = new LinkedList<>();\n+    for (InputRowParser parser : Arrays.<InputRowParser>asList(\n+        ROW_PARSER,\n+        new MapInputRowParser(\n+            new JSONParseSpec(\n+                new TimestampSpec(TIME_COLUMN, \"auto\"),\n+                new DimensionsSpec(\n+                    ImmutableList.<String>of(),\n+                    ImmutableList.of(DIM_FLOAT_NAME, DIM_LONG_NAME),\n+                    ImmutableList.<SpatialDimensionSchema>of()\n+                )\n+            )\n+        )\n+    )) {\n+      for (List<String> dim_names : Arrays.<List<String>>asList(null, ImmutableList.of(DIM_NAME))) {\n+        for (List<String> metric_names : Arrays.<List<String>>asList(\n+            null,\n+            ImmutableList.of(METRIC_LONG_NAME, METRIC_FLOAT_NAME)\n+        )) {\n+          values.add(\n+              new Object[]{\n+                  new IngestSegmentFirehoseFactory(\n+                      DATA_SOURCE_NAME,\n+                      FOREVER,\n+                      new SelectorDimFilter(DIM_NAME, DIM_VALUE),\n+                      dim_names,\n+                      metric_names,\n+                      Guice.createInjector(\n+                          new Module()\n+                          {\n+                            @Override\n+                            public void configure(Binder binder)\n+                            {\n+                              binder.bind(TaskToolboxFactory.class).toInstance(taskToolboxFactory);\n+                            }\n+                          }\n+                      )\n+                  ),\n+                  String.format(\n+                      \"DimNames[%s]MetricNames[%s]ParserDimNames[%s]\",\n+                      dim_names == null ? \"null\" : \"dims\",\n+                      metric_names == null ? \"null\" : \"metrics\",\n+                      parser == ROW_PARSER ? \"dims\" : \"null\"\n+                  ),\n+                  parser\n+              }\n+          );\n+        }\n+      }\n+    }\n+    return values;\n+  }\n+\n+  public IngestSegmentFirehoseFactoryTest(\n+      IngestSegmentFirehoseFactory factory,\n+      String testName,\n+      InputRowParser rowParser\n+  )\n+  {\n+    this.factory = factory;\n+    this.rowParser = rowParser;\n+  }\n+\n+  private static final Logger log = new Logger(IngestSegmentFirehoseFactoryTest.class);\n+  private static final Interval FOREVER = new Interval(JodaUtils.MIN_INSTANT, JodaUtils.MAX_INSTANT);\n+  private static final String DATA_SOURCE_NAME = \"testDataSource\";\n+  private static final String DATA_SOURCE_VERSION = \"version\";\n+  private static final Integer BINARY_VERSION = -1;\n+  private static final String DIM_NAME = \"testDimName\";\n+  private static final String DIM_VALUE = \"testDimValue\";\n+  private static final String DIM_LONG_NAME = \"testDimLongName\";\n+  private static final String DIM_FLOAT_NAME = \"testDimFloatName\";\n+  private static final String METRIC_LONG_NAME = \"testLongMetric\";\n+  private static final String METRIC_FLOAT_NAME = \"testFloatMetric\";\n+  private static final Long METRIC_LONG_VALUE = 1l;\n+  private static final Float METRIC_FLOAT_VALUE = 1.0f;\n+  private static final String TIME_COLUMN = \"ts\";\n+  private static final Integer MAX_SHARD_NUMBER = 10;\n+  private static final Integer MAX_ROWS = 10;\n+  private static final File tmpDir = Files.createTempDir();\n+  private static final File persistDir = Paths.get(tmpDir.getAbsolutePath(), \"indexTestMerger\").toFile();\n+  private static final List<DataSegment> segmentSet = new ArrayList<>(MAX_SHARD_NUMBER);\n+\n+  private final IngestSegmentFirehoseFactory factory;\n+  private final InputRowParser rowParser;\n+\n+  private static final InputRowParser<Map<String, Object>> ROW_PARSER = new MapInputRowParser(\n+      new JSONParseSpec(\n+          new TimestampSpec(TIME_COLUMN, \"auto\"),\n+          new DimensionsSpec(\n+              ImmutableList.of(DIM_NAME),\n+              ImmutableList.of(DIM_FLOAT_NAME, DIM_LONG_NAME),\n+              ImmutableList.<SpatialDimensionSchema>of()\n+          )\n+      )\n+  );\n+\n+  private static Map<String, Object> buildRow(Long ts)\n+  {\n+    return ImmutableMap.<String, Object>of(\n+        TIME_COLUMN, ts,\n+        DIM_NAME, DIM_VALUE,\n+        DIM_FLOAT_NAME, METRIC_FLOAT_VALUE,\n+        DIM_LONG_NAME, METRIC_LONG_VALUE\n+    );\n+  }\n+\n+  private static DataSegment buildSegment(Integer shardNumber)\n+  {\n+    Preconditions.checkArgument(shardNumber < MAX_SHARD_NUMBER);\n+    Preconditions.checkArgument(shardNumber >= 0);\n+    return new DataSegment(\n+        DATA_SOURCE_NAME,\n+        FOREVER,\n+        DATA_SOURCE_VERSION,\n+        ImmutableMap.<String, Object>of(\n+            \"type\", \"local\",\n+            \"path\", persistDir.getAbsolutePath()\n+        ),\n+        ImmutableList.of(DIM_NAME),\n+        ImmutableList.of(METRIC_LONG_NAME, METRIC_FLOAT_NAME),\n+        new NumberedShardSpec(\n+            shardNumber,\n+            MAX_SHARD_NUMBER\n+        ),\n+        BINARY_VERSION,\n+        0l\n+    );\n+  }\n+\n+  @BeforeClass\n+  public static void setUpStatic() throws IOException\n+  {\n+    for (int i = 0; i < MAX_SHARD_NUMBER; ++i) {\n+      segmentSet.add(buildSegment(i));\n+    }\n+  }\n+\n+  @AfterClass\n+  public static void tearDownStatic()\n+  {\n+    recursivelyDelete(tmpDir);\n+  }\n+\n+  private static void recursivelyDelete(final File dir)\n+  {\n+    if (dir != null) {\n+      if (dir.isDirectory()) {\n+        final File[] files = dir.listFiles();\n+        if (files != null) {\n+          for (File file : files) {\n+            recursivelyDelete(file);\n+          }\n+        }\n+      } else {\n+        if (!dir.delete()) {\n+          log.warn(\"Could not delete file at [%s]\", dir.getAbsolutePath());\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void sanityTest()\n+  {\n+    Assert.assertEquals(DATA_SOURCE_NAME, factory.getDataSource());\n+    if (factory.getDimensions() != null) {\n+      Assert.assertArrayEquals(new String[]{DIM_NAME}, factory.getDimensions().toArray());\n+    }\n+    Assert.assertEquals(FOREVER, factory.getInterval());\n+    if (factory.getMetrics() != null) {\n+      Assert.assertEquals(\n+          ImmutableSet.of(METRIC_LONG_NAME, METRIC_FLOAT_NAME),\n+          ImmutableSet.copyOf(factory.getMetrics())\n+      );\n+    }\n+  }\n+\n+  @Test\n+  public void simpleFirehoseReadingTest() throws IOException\n+  {\n+    Assert.assertEquals(MAX_SHARD_NUMBER.longValue(), segmentSet.size());\n+    Integer rowcount = 0;\n+    try (final IngestSegmentFirehoseFactory.IngestSegmentFirehose firehose =\n+             (IngestSegmentFirehoseFactory.IngestSegmentFirehose)\n+                 factory.connect(rowParser)) {\n+      while (firehose.hasMore()) {\n+        InputRow row = firehose.nextRow();\n+        Assert.assertArrayEquals(new String[]{DIM_NAME}, row.getDimensions().toArray());\n+        Assert.assertArrayEquals(new String[]{DIM_VALUE}, row.getDimension(DIM_NAME).toArray());\n+        Assert.assertEquals(METRIC_LONG_VALUE.longValue(), row.getLongMetric(METRIC_LONG_NAME));\n+        Assert.assertEquals(METRIC_FLOAT_VALUE, row.getFloatMetric(METRIC_FLOAT_NAME), METRIC_FLOAT_VALUE * 0.0001);\n+        ++rowcount;\n+      }\n+    }\n+    Assert.assertEquals((int) MAX_SHARD_NUMBER * MAX_ROWS, (int) rowcount);\n+  }\n+\n+  private static ServiceEmitter newMockEmitter()\n+  {\n+    return new ServiceEmitter(null, null, null)\n+    {\n+      @Override\n+      public void emit(Event event)\n+      {\n+\n+      }\n+\n+      @Override\n+      public void emit(ServiceEventBuilder builder)\n+      {\n+\n+      }\n+    };\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/indexing-service/src/test/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactoryTest.java",
                "sha": "1650929b15643ec2b4d16384726a57dcf333c941",
                "status": "added"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/processing/src/test/java/io/druid/segment/SchemalessIndex.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/SchemalessIndex.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 27,
                "filename": "processing/src/test/java/io/druid/segment/SchemalessIndex.java",
                "patch": "@@ -27,6 +27,7 @@\n import com.google.common.collect.Ordering;\n import com.google.common.hash.Hashing;\n import com.metamx.common.Pair;\n+import com.metamx.common.guava.FunctionalIterable;\n import com.metamx.common.logger.Logger;\n import io.druid.data.input.MapBasedInputRow;\n import io.druid.granularity.QueryGranularity;\n@@ -43,6 +44,7 @@\n import io.druid.timeline.TimelineObjectHolder;\n import io.druid.timeline.VersionedIntervalTimeline;\n import io.druid.timeline.partition.NoneShardSpec;\n+import io.druid.timeline.partition.PartitionChunk;\n import io.druid.timeline.partition.ShardSpec;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n@@ -426,8 +428,6 @@ private static QueryableIndex makeAppendedMMappedIndex(\n \n       List<File> filesToMap = makeFilesToMap(tmpFile, files);\n \n-      List<IndexableAdapter> adapters = Lists.newArrayList();\n-\n       VersionedIntervalTimeline<Integer, File> timeline = new VersionedIntervalTimeline<Integer, File>(\n           Ordering.natural().nullsFirst()\n       );\n@@ -438,33 +438,49 @@ private static QueryableIndex makeAppendedMMappedIndex(\n         timeline.add(intervals.get(i), i, noneShardSpec.createChunk(filesToMap.get(i)));\n       }\n \n-      List<Pair<File, Interval>> intervalsToMerge = Lists.transform(\n-          timeline.lookup(new Interval(\"1000-01-01/3000-01-01\")),\n-          new Function<TimelineObjectHolder<Integer, File>, Pair<File, Interval>>()\n-          {\n-            @Override\n-            public Pair<File, Interval> apply(@Nullable TimelineObjectHolder<Integer, File> input)\n-            {\n-              return new Pair<File, Interval>(input.getObject().getChunk(0).getObject(), input.getInterval());\n-            }\n-          }\n-      );\n-\n-      for (final Pair<File, Interval> pair : intervalsToMerge) {\n-        adapters.add(\n-            new RowboatFilteringIndexAdapter(\n-                new QueryableIndexIndexableAdapter(IndexIO.loadIndex(pair.lhs)),\n-                new Predicate<Rowboat>()\n-                {\n-                  @Override\n-                  public boolean apply(@Nullable Rowboat input)\n+      final List<IndexableAdapter> adapters = Lists.newArrayList(\n+          Iterables.concat(\n+              // TimelineObjectHolder is actually an iterable of iterable of indexable adapters\n+              Iterables.transform(\n+                  timeline.lookup(new Interval(\"1000-01-01/3000-01-01\")),\n+                  new Function<TimelineObjectHolder<Integer, File>, Iterable<IndexableAdapter>>()\n                   {\n-                    return pair.rhs.contains(input.getTimestamp());\n+                    @Override\n+                    public Iterable<IndexableAdapter> apply(final TimelineObjectHolder<Integer, File> timelineObjectHolder)\n+                    {\n+                      return Iterables.transform(\n+                          timelineObjectHolder.getObject(),\n+\n+                          // Each chunk can be used to build the actual IndexableAdapter\n+                          new Function<PartitionChunk<File>, IndexableAdapter>()\n+                          {\n+                            @Override\n+                            public IndexableAdapter apply(PartitionChunk<File> chunk)\n+                            {\n+                              try {\n+                                return new RowboatFilteringIndexAdapter(\n+                                    new QueryableIndexIndexableAdapter(IndexIO.loadIndex(chunk.getObject())),\n+                                    new Predicate<Rowboat>()\n+                                    {\n+                                      @Override\n+                                      public boolean apply(Rowboat input)\n+                                      {\n+                                        return timelineObjectHolder.getInterval().contains(input.getTimestamp());\n+                                      }\n+                                    }\n+                                );\n+                              }\n+                              catch (IOException e) {\n+                                throw Throwables.propagate(e);\n+                              }\n+                            }\n+                          }\n+                      );\n+                    }\n                   }\n-                }\n-            )\n-        );\n-      }\n+              )\n+          )\n+      );\n \n       return IndexIO.loadIndex(IndexMerger.append(adapters, mergedFile));\n     }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/processing/src/test/java/io/druid/segment/SchemalessIndex.java",
                "sha": "7e7fb3d31e805191046a99f0b52cb0c0d1da2775",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 12,
                "filename": "server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "patch": "@@ -21,6 +21,7 @@\n import com.google.common.base.Function;\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Ordering;\n import com.google.common.collect.Sets;\n@@ -115,19 +116,21 @@ public IndexerSQLMetadataStorageCoordinator(\n         }\n     );\n \n-    final List<DataSegment> segments = Lists.transform(\n-        timeline.lookup(interval),\n-        new Function<TimelineObjectHolder<String, DataSegment>, DataSegment>()\n-        {\n-          @Override\n-          public DataSegment apply(TimelineObjectHolder<String, DataSegment> input)\n-          {\n-            return input.getObject().getChunk(0).getObject();\n-          }\n-        }\n+    return Lists.newArrayList(\n+        Iterables.concat(\n+            Iterables.transform(\n+                timeline.lookup(interval),\n+                new Function<TimelineObjectHolder<String, DataSegment>, Iterable<DataSegment>>()\n+                {\n+                  @Override\n+                  public Iterable<DataSegment> apply(TimelineObjectHolder<String, DataSegment> input)\n+                  {\n+                    return input.getObject().payloads();\n+                  }\n+                }\n+            )\n+        )\n     );\n-\n-    return segments;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
                "sha": "df208a050937e9195c6d7a36c252541b760e3176",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java?ref=e5653f07521e8d5a5850f9ddebf75325a5fdc1ba",
                "deletions": 0,
                "filename": "server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java",
                "patch": "@@ -267,6 +267,7 @@ public Sink getSink(long timestamp)\n                           throw new ISE(\"No timeline entry at all!\");\n                         }\n \n+                        // The realtime plumber always uses SingleElementPartitionChunk\n                         final Sink theSink = holder.getObject().getChunk(0).getObject();\n \n                         if (theSink == null) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e5653f07521e8d5a5850f9ddebf75325a5fdc1ba/server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java",
                "sha": "02c6786853e3d6d0d254010ab2d3a64f33eed338",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #1190 from vigiglobe/master\n\nFix NPE when partionNumber 0 does not exist.",
        "parent": "https://github.com/apache/incubator-druid/commit/2c58e355fd48724a0be52debc0e561de446965f3",
        "repo": "incubator-druid",
        "unit_tests": [
            "IngestSegmentFirehoseFactoryTest.java",
            "SchemalessIndexTest.java",
            "IndexerSQLMetadataStorageCoordinatorTest.java"
        ]
    },
    "incubator-druid_e6d93a3": {
        "bug_id": "incubator-druid_e6d93a3",
        "commit": "https://github.com/apache/incubator-druid/commit/e6d93a307019406c9e4ec26d9d865fd3717feeb7",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/e6d93a307019406c9e4ec26d9d865fd3717feeb7/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java?ref=e6d93a307019406c9e4ec26d9d865fd3717feeb7",
                "deletions": 2,
                "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "patch": "@@ -255,13 +255,18 @@ public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> d\n                       final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n                       for (String dim : dims) {\n                         final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim);\n-                        dimSelectors.put(dim, dimSelector);\n+                        // dimSelector is null if the dimension is not present\n+                        if (dimSelector != null) {\n+                          dimSelectors.put(dim, dimSelector);\n+                        }\n                       }\n \n                       final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n                       for (String metric : metrics) {\n                         final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n-                        metSelectors.put(metric, metricSelector);\n+                        if (metricSelector != null) {\n+                          metSelectors.put(metric, metricSelector);\n+                        }\n                       }\n \n                       return Sequences.simple(",
                "raw_url": "https://github.com/apache/incubator-druid/raw/e6d93a307019406c9e4ec26d9d865fd3717feeb7/indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
                "sha": "5744fa7f006f3e46227093b2be78e6da7545fba2",
                "status": "modified"
            }
        ],
        "message": "fix NPE\n\nfix NPE when the dimension of metric is not present one of the segments\nto be reIndexed.",
        "parent": "https://github.com/apache/incubator-druid/commit/a17794a51668cb83d07a4c7220960bfc29ad309e",
        "repo": "incubator-druid",
        "unit_tests": [
            "IngestSegmentFirehoseFactoryTest.java"
        ]
    },
    "incubator-druid_f0871ea": {
        "bug_id": "incubator-druid_f0871ea",
        "commit": "https://github.com/apache/incubator-druid/commit/f0871ea40b73c188196660b35c3d9af8133137b4",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/main/java/io/druid/segment/incremental/OffheapIncrementalIndex.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/OffheapIncrementalIndex.java?ref=f0871ea40b73c188196660b35c3d9af8133137b4",
                "deletions": 4,
                "filename": "processing/src/main/java/io/druid/segment/incremental/OffheapIncrementalIndex.java",
                "patch": "@@ -151,8 +151,6 @@ protected DimDim makeDimDim(String dimension, Object lock)\n     selectors = Maps.newHashMap();\n     aggOffsetInBuffer = new int[metrics.length];\n \n-    BufferAggregator[] aggregators = new BufferAggregator[metrics.length];\n-\n     for (int i = 0; i < metrics.length; i++) {\n       AggregatorFactory agg = metrics[i];\n \n@@ -167,7 +165,6 @@ protected DimDim makeDimDim(String dimension, Object lock)\n           new OnheapIncrementalIndex.ObjectCachingColumnSelectorFactory(columnSelectorFactory)\n       );\n \n-      aggregators[i] = agg.factorizeBuffered(columnSelectorFactory);\n       if (i == 0) {\n         aggOffsetInBuffer[i] = 0;\n       } else {\n@@ -177,7 +174,7 @@ protected DimDim makeDimDim(String dimension, Object lock)\n \n     aggsTotalSize = aggOffsetInBuffer[metrics.length - 1] + metrics[metrics.length - 1].getMaxIntermediateSize();\n \n-    return aggregators;\n+    return new BufferAggregator[metrics.length];\n   }\n \n   @Override\n@@ -203,6 +200,15 @@ protected Integer addToFacts(\n         bufferOffset = indexAndOffset[1];\n         aggBuffer = aggBuffers.get(bufferIndex).get();\n       } else {\n+        rowContainer.set(row);\n+        for (int i = 0; i < metrics.length; i++) {\n+          final AggregatorFactory agg = metrics[i];\n+          getAggs()[i] = agg.factorizeBuffered(\n+              makeColumnSelectorFactory(agg, rowSupplier, deserializeComplexMetrics)\n+          );\n+        }\n+        rowContainer.set(null);\n+\n         bufferIndex = aggBuffers.size() - 1;\n         ByteBuffer lastBuffer = aggBuffers.isEmpty() ? null : aggBuffers.get(aggBuffers.size() - 1).get();\n         int[] lastAggregatorsIndexAndOffset = indexAndOffsets.isEmpty()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/main/java/io/druid/segment/incremental/OffheapIncrementalIndex.java",
                "sha": "f5d4d8248cdad455c3d14ccf40da0d5b924f703d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/main/java/io/druid/segment/incremental/OnheapIncrementalIndex.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/OnheapIncrementalIndex.java?ref=f0871ea40b73c188196660b35c3d9af8133137b4",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/segment/incremental/OnheapIncrementalIndex.java",
                "patch": "@@ -155,12 +155,15 @@ protected Integer addToFacts(\n     } else {\n       aggs = new Aggregator[metrics.length];\n \n+      rowContainer.set(row);\n       for (int i = 0; i < metrics.length; i++) {\n         final AggregatorFactory agg = metrics[i];\n         aggs[i] = agg.factorize(\n             selectors.get(agg.getName())\n         );\n       }\n+      rowContainer.set(null);\n+\n       final Integer rowIndex = indexIncrement.getAndIncrement();\n \n       concurrentSet(rowIndex, aggs);",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/main/java/io/druid/segment/incremental/OnheapIncrementalIndex.java",
                "sha": "dcf6c0d2946780443b5ddbce766d154bc4e147d2",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java?ref=f0871ea40b73c188196660b35c3d9af8133137b4",
                "deletions": 3,
                "filename": "processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java",
                "patch": "@@ -29,6 +29,8 @@\n import io.druid.query.aggregation.AggregatorFactory;\n import io.druid.query.aggregation.CountAggregatorFactory;\n import io.druid.segment.CloserRule;\n+import io.druid.query.aggregation.FilteredAggregatorFactory;\n+import io.druid.query.filter.SelectorDimFilter;\n import org.joda.time.DateTime;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -73,7 +75,15 @@ public IncrementalIndexTest(IndexCreator IndexCreator)\n                   public IncrementalIndex createIndex()\n                   {\n                     return new OnheapIncrementalIndex(\n-                        0, QueryGranularity.MINUTE, new AggregatorFactory[]{new CountAggregatorFactory(\"cnt\")}, 1000\n+                        0,\n+                        QueryGranularity.MINUTE,\n+                        new AggregatorFactory[]{\n+                            new FilteredAggregatorFactory(\n+                                new CountAggregatorFactory(\"cnt\"),\n+                                new SelectorDimFilter(\"billy\", \"A\")\n+                            )\n+                        },\n+                        1000\n                     );\n                   }\n                 }\n@@ -88,7 +98,12 @@ public IncrementalIndex createIndex()\n                     return new OffheapIncrementalIndex(\n                         0L,\n                         QueryGranularity.NONE,\n-                        new AggregatorFactory[]{new CountAggregatorFactory(\"cnt\")},\n+                        new AggregatorFactory[]{\n+                            new FilteredAggregatorFactory(\n+                                new CountAggregatorFactory(\"cnt\"),\n+                                new SelectorDimFilter(\"billy\", \"A\")\n+                            )\n+                        },\n                         1000000,\n                         new StupidPool<ByteBuffer>(\n                             new Supplier<ByteBuffer>()\n@@ -104,7 +119,6 @@ public ByteBuffer get()\n                   }\n                 }\n             }\n-\n         }\n     );\n   }\n@@ -153,6 +167,13 @@ public void controlTest() throws IndexSizeExceededException\n             ImmutableMap.<String, Object>of(\"billy\", \"A\", \"joe\", \"B\")\n         )\n     );\n+    index.add(\n+        new MapBasedInputRow(\n+            new DateTime().minus(1).getMillis(),\n+            Lists.newArrayList(\"billy\", \"joe\"),\n+            ImmutableMap.<String, Object>of(\"billy\", \"C\", \"joe\", \"B\")\n+        )\n+    );\n     index.add(\n         new MapBasedInputRow(\n             new DateTime().minus(1).getMillis(),",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f0871ea40b73c188196660b35c3d9af8133137b4/processing/src/test/java/io/druid/segment/incremental/IncrementalIndexTest.java",
                "sha": "0fd115b45c60338d5e604ca6bdd8e795eeec33b4",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #2239 from sirpkt/filtered-aggregators-at-ingestion\n\nfix NPE with filtered aggregator at ingestion time",
        "parent": "https://github.com/apache/incubator-druid/commit/7a0bfa693beb44de6709848fe226d6093d5e29ae",
        "repo": "incubator-druid",
        "unit_tests": [
            "OnheapIncrementalIndexTest.java"
        ]
    },
    "incubator-druid_f24a89a": {
        "bug_id": "incubator-druid_f24a89a",
        "commit": "https://github.com/apache/incubator-druid/commit/f24a89a22a3b92940606fb0ec74c4d70feb77b1d",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f24a89a22a3b92940606fb0ec74c4d70feb77b1d/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java?ref=f24a89a22a3b92940606fb0ec74c4d70feb77b1d",
                "deletions": 0,
                "filename": "processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "patch": "@@ -111,6 +111,12 @@ public Comparator getComparator()\n       @Override\n       public int compare(HyperLogLogCollector lhs, HyperLogLogCollector rhs)\n       {\n+        if(lhs == null) {\n+          return -1;\n+        }\n+        if(rhs == null) {\n+          return 1;\n+        }\n         return lhs.compareTo(rhs);\n       }\n     };",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f24a89a22a3b92940606fb0ec74c4d70feb77b1d/processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperUniquesAggregatorFactory.java",
                "sha": "7b3d31189c5f4437d903953449a868386052b15d",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f24a89a22a3b92940606fb0ec74c4d70feb77b1d/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java?ref=f24a89a22a3b92940606fb0ec74c4d70feb77b1d",
                "deletions": 0,
                "filename": "processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "patch": "@@ -37,6 +37,7 @@\n import io.druid.query.aggregation.MaxAggregatorFactory;\n import io.druid.query.aggregation.MinAggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n+import io.druid.query.aggregation.hyperloglog.HyperUniquesAggregatorFactory;\n import io.druid.query.dimension.ExtractionDimensionSpec;\n import io.druid.query.extraction.RegexDimExtractionFn;\n import io.druid.query.filter.AndDimFilter;\n@@ -303,6 +304,46 @@ public void testFullOnTopNOverUniques()\n     TestHelper.assertExpectedResults(expectedResults, runner.run(query, context));\n   }\n \n+  @Test\n+  public void testTopNOverMissingUniques()\n+  {\n+    TopNQuery query = new TopNQueryBuilder()\n+        .dataSource(QueryRunnerTestHelper.dataSource)\n+        .granularity(QueryRunnerTestHelper.allGran)\n+        .dimension(marketDimension)\n+        .metric(QueryRunnerTestHelper.uniqueMetric)\n+        .threshold(3)\n+        .intervals(QueryRunnerTestHelper.fullOnInterval)\n+        .aggregators(\n+            Arrays.<AggregatorFactory>asList(new HyperUniquesAggregatorFactory(\"uniques\", \"missingUniques\"))\n+        )\n+        .build();\n+\n+    List<Result<TopNResultValue>> expectedResults = Arrays.asList(\n+        new Result<TopNResultValue>(\n+            new DateTime(\"2011-01-12T00:00:00.000Z\"),\n+            new TopNResultValue(\n+                Arrays.<Map<String, Object>>asList(\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"total_market\")\n+                                .put(\"uniques\", 0)\n+                                .build(),\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"spot\")\n+                                .put(\"uniques\", 0)\n+                                .build(),\n+                    ImmutableMap.<String, Object>builder()\n+                                .put(\"market\", \"upfront\")\n+                                .put(\"uniques\", 0)\n+                                .build()\n+                )\n+            )\n+        )\n+    );\n+    HashMap<String, Object> context = new HashMap<String, Object>();\n+    TestHelper.assertExpectedResults(expectedResults, runner.run(query, context));\n+  }\n+\n \n   @Test\n   public void testTopNBySegment()",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f24a89a22a3b92940606fb0ec74c4d70feb77b1d/processing/src/test/java/io/druid/query/topn/TopNQueryRunnerTest.java",
                "sha": "c3a40424c3bb3c6da934b67012239f8c6ccc2ef2",
                "status": "modified"
            }
        ],
        "message": "fix NPE for topN over missing hyperUniques column",
        "parent": "https://github.com/apache/incubator-druid/commit/6993d84f02c5a8d19813f6d59930f53b430ce361",
        "repo": "incubator-druid",
        "unit_tests": [
            "HyperUniquesAggregatorFactoryTest.java"
        ]
    },
    "incubator-druid_f24e2f1": {
        "bug_id": "incubator-druid_f24e2f1",
        "commit": "https://github.com/apache/incubator-druid/commit/f24e2f16af38352056e713347de4e63d38c785ad",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f24e2f16af38352056e713347de4e63d38c785ad/server/src/main/java/org/apache/druid/metadata/SQLMetadataSegmentManager.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/main/java/org/apache/druid/metadata/SQLMetadataSegmentManager.java?ref=f24e2f16af38352056e713347de4e63d38c785ad",
                "deletions": 8,
                "filename": "server/src/main/java/org/apache/druid/metadata/SQLMetadataSegmentManager.java",
                "patch": "@@ -950,14 +950,10 @@ public DataSegment map(int index, ResultSet r, StatementContext ctx) throws SQLE\n         }\n     );\n \n-    if (segments == null || segments.isEmpty()) {\n-      log.info(\"No segments found in the database!\");\n-      return;\n-    }\n-\n-    log.info(\"Polled and found %,d segments in the database\", segments.size());\n-\n-    ImmutableMap<String, String> dataSourceProperties = createDefaultDataSourceProperties();\n+    Preconditions.checkNotNull(\n+        segments,\n+        \"Unexpected 'null' when polling segments from the db, aborting snapshot update.\"\n+    );\n \n     // dataSourcesSnapshot is updated only here and the DataSourcesSnapshot object is immutable. If data sources or\n     // segments are marked as used or unused directly (via markAs...() methods in MetadataSegmentManager), the\n@@ -967,6 +963,13 @@ public DataSegment map(int index, ResultSet r, StatementContext ctx) throws SQLE\n     // segment mark calls in rapid succession. So the snapshot update is not done outside of database poll at this time.\n     // Updates outside of database polls were primarily for the user experience, so users would immediately see the\n     // effect of a segment mark call reflected in MetadataResource API calls.\n+\n+    ImmutableMap<String, String> dataSourceProperties = createDefaultDataSourceProperties();\n+    if (segments.isEmpty()) {\n+      log.info(\"No segments found in the database!\");\n+    } else {\n+      log.info(\"Polled and found %,d segments in the database\", segments.size());\n+    }\n     dataSourcesSnapshot = DataSourcesSnapshot.fromUsedSegments(\n         Iterables.filter(segments, Objects::nonNull), // Filter corrupted entries (see above in this method).\n         dataSourceProperties",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f24e2f16af38352056e713347de4e63d38c785ad/server/src/main/java/org/apache/druid/metadata/SQLMetadataSegmentManager.java",
                "sha": "2f37fd6ed71939b6077573974599b1d7bbdcc549",
                "status": "modified"
            },
            {
                "additions": 113,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f24e2f16af38352056e713347de4e63d38c785ad/server/src/test/java/org/apache/druid/metadata/SQLMetadataSegmentManagerEmptyTest.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/server/src/test/java/org/apache/druid/metadata/SQLMetadataSegmentManagerEmptyTest.java?ref=f24e2f16af38352056e713347de4e63d38c785ad",
                "deletions": 0,
                "filename": "server/src/test/java/org/apache/druid/metadata/SQLMetadataSegmentManagerEmptyTest.java",
                "patch": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Suppliers;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import org.apache.druid.client.ImmutableDruidDataSource;\n+import org.apache.druid.segment.TestHelper;\n+import org.joda.time.Period;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import java.util.stream.Collectors;\n+\n+\n+/**\n+ * Like {@link SQLMetadataRuleManagerTest} except with no segments to make sure it behaves when it's empty\n+ */\n+public class SQLMetadataSegmentManagerEmptyTest\n+{\n+\n+  @Rule\n+  public final TestDerbyConnector.DerbyConnectorRule derbyConnectorRule = new TestDerbyConnector.DerbyConnectorRule();\n+\n+  private SQLMetadataSegmentManager sqlSegmentsMetadata;\n+  private final ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n+\n+  @Before\n+  public void setUp() throws Exception\n+  {\n+    TestDerbyConnector connector = derbyConnectorRule.getConnector();\n+    MetadataSegmentManagerConfig config = new MetadataSegmentManagerConfig();\n+    config.setPollDuration(Period.seconds(1));\n+    sqlSegmentsMetadata = new SQLMetadataSegmentManager(\n+        jsonMapper,\n+        Suppliers.ofInstance(config),\n+        derbyConnectorRule.metadataTablesConfigSupplier(),\n+        connector\n+    );\n+    sqlSegmentsMetadata.start();\n+\n+    connector.createSegmentTable();\n+  }\n+\n+  @After\n+  public void teardown()\n+  {\n+    if (sqlSegmentsMetadata.isPollingDatabasePeriodically()) {\n+      sqlSegmentsMetadata.stopPollingDatabasePeriodically();\n+    }\n+    sqlSegmentsMetadata.stop();\n+  }\n+\n+  @Test\n+  public void testPollEmpty()\n+  {\n+    sqlSegmentsMetadata.startPollingDatabasePeriodically();\n+    sqlSegmentsMetadata.poll();\n+    Assert.assertTrue(sqlSegmentsMetadata.isPollingDatabasePeriodically());\n+    Assert.assertEquals(\n+        ImmutableList.of(),\n+        sqlSegmentsMetadata.retrieveAllDataSourceNames()\n+    );\n+    Assert.assertEquals(\n+        ImmutableList.of(),\n+        sqlSegmentsMetadata\n+            .getImmutableDataSourcesWithAllUsedSegments()\n+            .stream()\n+            .map(ImmutableDruidDataSource::getName)\n+            .collect(Collectors.toList())\n+    );\n+    Assert.assertEquals(\n+        null,\n+        sqlSegmentsMetadata.getImmutableDataSourceWithUsedSegments(\"wikipedia\")\n+    );\n+    Assert.assertEquals(\n+        ImmutableSet.of(),\n+        ImmutableSet.copyOf(sqlSegmentsMetadata.iterateAllUsedSegments())\n+    );\n+  }\n+\n+  @Test\n+  public void testStopAndStart()\n+  {\n+    // Simulate successive losing and getting the coordinator leadership\n+    sqlSegmentsMetadata.startPollingDatabasePeriodically();\n+    sqlSegmentsMetadata.stopPollingDatabasePeriodically();\n+    sqlSegmentsMetadata.startPollingDatabasePeriodically();\n+    sqlSegmentsMetadata.stopPollingDatabasePeriodically();\n+  }\n+}",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f24e2f16af38352056e713347de4e63d38c785ad/server/src/test/java/org/apache/druid/metadata/SQLMetadataSegmentManagerEmptyTest.java",
                "sha": "1e3f5d8e25bc135814bcd61586c65bae246d8ef3",
                "status": "added"
            }
        ],
        "message": "fix npe with sql metadata manager polling and empty database (#8106)\n\n* fix npe with sql metadata manager polling and empty database\r\n\r\n* treat null segments separately\r\n\r\n* use preconditions check\r\n\r\n* add test",
        "parent": "https://github.com/apache/incubator-druid/commit/2b2fcc0371a8d7002d58f1bd997ea03f383dee14",
        "repo": "incubator-druid",
        "unit_tests": [
            "SQLMetadataSegmentManagerTest.java"
        ]
    },
    "incubator-druid_f2e0dc9": {
        "bug_id": "incubator-druid_f2e0dc9",
        "commit": "https://github.com/apache/incubator-druid/commit/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=f2e0dc9327aaacf8ca479b712eb40c04cbde7cab",
                "deletions": 15,
                "filename": "s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n import com.google.inject.Inject;\n+import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n@@ -120,23 +121,30 @@ public Void call() throws Exception\n               if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n                 if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) {\n                   log.info(\"No need to move file[s3://%s/%s] onto itself\", s3Bucket, s3Path);\n-                } else if (s3Client.getObjectDetails(s3Bucket, s3Path)\n-                                   .getStorageClass()\n-                                   .equals(S3Object.STORAGE_CLASS_GLACIER)) {\n-                  log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier.\");\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n-                      s3Path,\n-                      targetS3Bucket,\n-                      targetS3Path\n-                  );\n-                  final S3Object target = new S3Object(targetS3Path);\n-                  if(!config.getDisableAcl()) {\n-                    target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                  final S3Object[] list = s3Client.listObjects(s3Bucket, s3Path, \"\");\n+                  if (list.length == 0) {\n+                    // should never happen\n+                    throw new ISE(\"Unable to list object [s3://%s/%s]\", s3Bucket, s3Path);\n+                  }\n+                  final S3Object s3Object = list[0];\n+                  if (s3Object.getStorageClass() != null &&\n+                      s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n+                    log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\");\n+                  } else {\n+                    log.info(\n+                        \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n+                        s3Bucket,\n+                        s3Path,\n+                        targetS3Bucket,\n+                        targetS3Path\n+                    );\n+                    final S3Object target = new S3Object(targetS3Path);\n+                    if (!config.getDisableAcl()) {\n+                      target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                    }\n+                    s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f2e0dc9327aaacf8ca479b712eb40c04cbde7cab/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "379dd8374fab0d5c67b1601469d9302226318c93",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #637 from metamx/fix-s3-storageclass-check\n\nfix storage class npe",
        "parent": "https://github.com/apache/incubator-druid/commit/7a7386be74247dbfc42963dac491e8fcddad7eeb",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    },
    "incubator-druid_f349e03": {
        "bug_id": "incubator-druid_f349e03",
        "commit": "https://github.com/apache/incubator-druid/commit/f349e03091546c2b55ca91750a44c107403b84d3",
        "file": [
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/indexing-service/src/main/java/io/druid/indexing/common/task/CompactionTask.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/main/java/io/druid/indexing/common/task/CompactionTask.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 31,
                "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/CompactionTask.java",
                "patch": "@@ -51,6 +51,7 @@\n import io.druid.java.util.common.ISE;\n import io.druid.java.util.common.JodaUtils;\n import io.druid.java.util.common.Pair;\n+import io.druid.java.util.common.RE;\n import io.druid.java.util.common.granularity.NoneGranularity;\n import io.druid.java.util.common.guava.Comparators;\n import io.druid.java.util.common.jackson.JacksonUtils;\n@@ -79,6 +80,7 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -195,28 +197,37 @@ public TaskStatus run(final TaskToolbox toolbox) throws Exception\n           jsonMapper\n       );\n \n-      indexTaskSpec = new IndexTask(\n-          getId(),\n-          getGroupId(),\n-          getTaskResource(),\n-          getDataSource(),\n-          ingestionSpec,\n-          getContext(),\n-          authorizerMapper,\n-          null\n-      );\n-    }\n-\n-    if (indexTaskSpec.getIngestionSchema() == null) {\n-      log.info(\"Cannot find segments for interval\");\n+      if (ingestionSpec != null) {\n+        indexTaskSpec = new IndexTask(\n+            getId(),\n+            getGroupId(),\n+            getTaskResource(),\n+            getDataSource(),\n+            ingestionSpec,\n+            getContext(),\n+            authorizerMapper,\n+            null\n+        );\n+      }\n     }\n \n-    final String json = jsonMapper.writerWithDefaultPrettyPrinter().writeValueAsString(indexTaskSpec);\n-    log.info(\"Generated compaction task details: \" + json);\n+    if (indexTaskSpec == null) {\n+      log.warn(\"Failed to generate compaction spec\");\n+      return TaskStatus.failure(getId());\n+    } else {\n+      final String json = jsonMapper.writerWithDefaultPrettyPrinter().writeValueAsString(indexTaskSpec);\n+      log.info(\"Generated compaction task details: \" + json);\n \n-    return indexTaskSpec.run(toolbox);\n+      return indexTaskSpec.run(toolbox);\n+    }\n   }\n \n+  /**\n+   * Generate {@link IndexIngestionSpec} from input segments.\n+\n+   * @return null if input segments don't exist. Otherwise, a generated ingestionSpec.\n+   */\n+  @Nullable\n   @VisibleForTesting\n   static IndexIngestionSpec createIngestionSchema(\n       TaskToolbox toolbox,\n@@ -289,12 +300,22 @@ private static DataSchema createDataSchema(\n       throws IOException\n   {\n     // find metadata for interval\n-    final List<QueryableIndex> queryableIndices = loadSegments(timelineSegments, segmentFileMap, indexIO);\n+    final List<Pair<QueryableIndex, DataSegment>> queryableIndexAndSegments = loadSegments(\n+        timelineSegments,\n+        segmentFileMap,\n+        indexIO\n+    );\n \n     // find merged aggregators\n-    final List<AggregatorFactory[]> aggregatorFactories = queryableIndices\n+    for (Pair<QueryableIndex, DataSegment> pair : queryableIndexAndSegments) {\n+      final QueryableIndex index = pair.lhs;\n+      if (index.getMetadata() == null) {\n+        throw new RE(\"Index metadata doesn't exist for segment[%s]\", pair.rhs.getIdentifier());\n+      }\n+    }\n+    final List<AggregatorFactory[]> aggregatorFactories = queryableIndexAndSegments\n         .stream()\n-        .map(index -> index.getMetadata().getAggregators())\n+        .map(pair -> pair.lhs.getMetadata().getAggregators()) // We have already done null check on index.getMetadata()\n         .collect(Collectors.toList());\n     final AggregatorFactory[] mergedAggregators = AggregatorFactory.mergeAggregators(aggregatorFactories);\n \n@@ -304,7 +325,11 @@ private static DataSchema createDataSchema(\n \n     // find granularity spec\n     // set rollup only if rollup is set for all segments\n-    final boolean rollup = queryableIndices.stream().allMatch(index -> index.getMetadata().isRollup());\n+    final boolean rollup = queryableIndexAndSegments.stream().allMatch(pair -> {\n+      // We have already checked getMetadata() doesn't return null\n+      final Boolean isRollup = pair.lhs.getMetadata().isRollup();\n+      return isRollup != null && isRollup;\n+    });\n     final GranularitySpec granularitySpec = new ArbitraryGranularitySpec(\n         new NoneGranularity(),\n         rollup,\n@@ -313,7 +338,7 @@ private static DataSchema createDataSchema(\n \n     // find unique dimensions\n     final DimensionsSpec finalDimensionsSpec = dimensionsSpec == null ?\n-                                               createDimensionsSpec(queryableIndices) :\n+                                               createDimensionsSpec(queryableIndexAndSegments) :\n                                                dimensionsSpec;\n     final InputRowParser parser = new NoopInputRowParser(new TimeAndDimsParseSpec(null, finalDimensionsSpec));\n \n@@ -327,7 +352,7 @@ private static DataSchema createDataSchema(\n     );\n   }\n \n-  private static DimensionsSpec createDimensionsSpec(List<QueryableIndex> queryableIndices)\n+  private static DimensionsSpec createDimensionsSpec(List<Pair<QueryableIndex, DataSegment>> queryableIndices)\n   {\n     final BiMap<String, Integer> uniqueDims = HashBiMap.create();\n     final Map<String, DimensionSchema> dimensionSchemaMap = new HashMap<>();\n@@ -337,9 +362,24 @@ private static DimensionsSpec createDimensionsSpec(List<QueryableIndex> queryabl\n     // Dimensions are extracted from the recent segments to olders because recent segments are likely to be queried more\n     // frequently, and thus the performance should be optimized for recent ones rather than old ones.\n \n-    // timelineSegments are sorted in order of interval\n+    // timelineSegments are sorted in order of interval, but we do a sanity check here.\n+    final Comparator<Interval> intervalComparator = Comparators.intervalsByStartThenEnd();\n+    for (int i = 0; i < queryableIndices.size() - 1; i++) {\n+      final Interval shouldBeSmaller = queryableIndices.get(i).lhs.getDataInterval();\n+      final Interval shouldBeLarger = queryableIndices.get(i + 1).lhs.getDataInterval();\n+      Preconditions.checkState(\n+          intervalComparator.compare(shouldBeSmaller, shouldBeLarger) <= 0,\n+          \"QueryableIndexes are not sorted! Interval[%s] of segment[%s] is laster than interval[%s] of segment[%s]\",\n+          shouldBeSmaller,\n+          queryableIndices.get(i).rhs.getIdentifier(),\n+          shouldBeLarger,\n+          queryableIndices.get(i + 1).rhs.getIdentifier()\n+      );\n+    }\n+\n     int index = 0;\n-    for (QueryableIndex queryableIndex : Lists.reverse(queryableIndices)) {\n+    for (Pair<QueryableIndex, DataSegment> pair : Lists.reverse(queryableIndices)) {\n+      final QueryableIndex queryableIndex = pair.lhs;\n       final Map<String, DimensionHandler> dimensionHandlerMap = queryableIndex.getDimensionHandlers();\n \n       for (String dimension : queryableIndex.getAvailableDimensions()) {\n@@ -385,23 +425,22 @@ private static DimensionsSpec createDimensionsSpec(List<QueryableIndex> queryabl\n     return new DimensionsSpec(dimensionSchemas, null, null);\n   }\n \n-  private static List<QueryableIndex> loadSegments(\n+  private static List<Pair<QueryableIndex, DataSegment>> loadSegments(\n       List<TimelineObjectHolder<String, DataSegment>> timelineSegments,\n       Map<DataSegment, File> segmentFileMap,\n       IndexIO indexIO\n   ) throws IOException\n   {\n-    final List<QueryableIndex> segments = new ArrayList<>();\n+    final List<Pair<QueryableIndex, DataSegment>> segments = new ArrayList<>();\n \n     for (TimelineObjectHolder<String, DataSegment> timelineSegment : timelineSegments) {\n       final PartitionHolder<DataSegment> partitionHolder = timelineSegment.getObject();\n       for (PartitionChunk<DataSegment> chunk : partitionHolder) {\n         final DataSegment segment = chunk.getObject();\n-        segments.add(\n-            indexIO.loadIndex(\n-                Preconditions.checkNotNull(segmentFileMap.get(segment), \"File for segment %s\", segment.getIdentifier())\n-            )\n+        final QueryableIndex queryableIndex = indexIO.loadIndex(\n+            Preconditions.checkNotNull(segmentFileMap.get(segment), \"File for segment %s\", segment.getIdentifier())\n         );\n+        segments.add(Pair.of(queryableIndex, segment));\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/indexing-service/src/main/java/io/druid/indexing/common/task/CompactionTask.java",
                "sha": "85d2e333cf320727d6f88b74301353577d3f2ab5",
                "status": "modified"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/indexing-service/src/test/java/io/druid/indexing/common/task/CompactionTaskTest.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/indexing-service/src/test/java/io/druid/indexing/common/task/CompactionTaskTest.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 12,
                "filename": "indexing-service/src/test/java/io/druid/indexing/common/task/CompactionTaskTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.inject.Binder;\n import com.google.inject.Module;\n@@ -88,6 +89,7 @@\n import org.hamcrest.CoreMatchers;\n import org.joda.time.Interval;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -129,10 +131,12 @@\n   private static Map<String, AggregatorFactory> AGGREGATORS;\n   private static List<DataSegment> SEGMENTS;\n   private static ObjectMapper objectMapper = setupInjectablesInObjectMapper(new DefaultObjectMapper());\n-  private static TaskToolbox toolbox;\n+  private static Map<DataSegment, File> segmentMap;\n+\n+  private TaskToolbox toolbox;\n \n   @BeforeClass\n-  public static void setup()\n+  public static void setupClass()\n   {\n     DIMENSIONS = new HashMap<>();\n     AGGREGATORS = new HashMap<>();\n@@ -166,7 +170,7 @@ public static void setup()\n     AGGREGATORS.put(\"agg_3\", new FloatFirstAggregatorFactory(\"agg_3\", \"float_dim_3\"));\n     AGGREGATORS.put(\"agg_4\", new DoubleLastAggregatorFactory(\"agg_4\", \"double_dim_4\"));\n \n-    final Map<DataSegment, File> segmentMap = new HashMap<>(5);\n+    segmentMap = new HashMap<>(5);\n     for (int i = 0; i < 5; i++) {\n       final Interval segmentInterval = Intervals.of(StringUtils.format(\"2017-0%d-01/2017-0%d-01\", (i + 1), (i + 2)));\n       segmentMap.put(\n@@ -185,12 +189,6 @@ public static void setup()\n       );\n     }\n     SEGMENTS = new ArrayList<>(segmentMap.keySet());\n-\n-    toolbox = new TestTaskToolbox(\n-        new TestTaskActionClient(new ArrayList<>(segmentMap.keySet())),\n-        new TestIndexIO(objectMapper, segmentMap),\n-        segmentMap\n-    );\n   }\n \n   private static ObjectMapper setupInjectablesInObjectMapper(ObjectMapper objectMapper)\n@@ -272,6 +270,16 @@ private static IndexTuningConfig createTuningConfig()\n   @Rule\n   public ExpectedException expectedException = ExpectedException.none();\n \n+  @Before\n+  public void setup()\n+  {\n+    toolbox = new TestTaskToolbox(\n+        new TestTaskActionClient(new ArrayList<>(segmentMap.keySet())),\n+        new TestIndexIO(objectMapper, segmentMap),\n+        segmentMap\n+    );\n+  }\n+\n   @Test\n   public void testSerdeWithInterval() throws IOException\n   {\n@@ -415,6 +423,24 @@ public void testCreateIngestionSchemaWithDifferentSegmentSet() throws IOExceptio\n     );\n   }\n \n+  @Test\n+  public void testMissingMetadata() throws IOException, SegmentLoadingException\n+  {\n+    expectedException.expect(RuntimeException.class);\n+    expectedException.expectMessage(CoreMatchers.startsWith(\"Index metadata doesn't exist for segment\"));\n+\n+    final TestIndexIO indexIO = (TestIndexIO) toolbox.getIndexIO();\n+    indexIO.removeMetadata(Iterables.getFirst(indexIO.getQueryableIndexMap().keySet(), null));\n+    final List<DataSegment> segments = new ArrayList<>(SEGMENTS);\n+    CompactionTask.createIngestionSchema(\n+        toolbox,\n+        new SegmentProvider(segments),\n+        null,\n+        TUNING_CONFIG,\n+        objectMapper\n+    );\n+  }\n+\n   private static DimensionsSpec getExpectedDimensionsSpecForAutoGeneration()\n   {\n     return new DimensionsSpec(\n@@ -599,9 +625,13 @@ private static void assertIngestionSchema(\n           }\n         }\n \n-        final Metadata metadata = new Metadata();\n-        metadata.setAggregators(aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]));\n-        metadata.setRollup(false);\n+        final Metadata metadata = new Metadata(\n+            null,\n+            aggregatorFactories.toArray(new AggregatorFactory[0]),\n+            null,\n+            null,\n+            null\n+        );\n \n         queryableIndexMap.put(\n             entry.getValue(),\n@@ -622,6 +652,31 @@ public QueryableIndex loadIndex(File file)\n     {\n       return queryableIndexMap.get(file);\n     }\n+\n+    void removeMetadata(File file)\n+    {\n+      final SimpleQueryableIndex index = (SimpleQueryableIndex) queryableIndexMap.get(file);\n+      if (index != null) {\n+        queryableIndexMap.put(\n+            file,\n+            new SimpleQueryableIndex(\n+                index.getDataInterval(),\n+                index.getColumnNames(),\n+                index.getAvailableDimensions(),\n+                index.getBitmapFactoryForDimensions(),\n+                index.getColumns(),\n+                index.getFileMapper(),\n+                null,\n+                index.getDimensionHandlers()\n+            )\n+        );\n+      }\n+    }\n+\n+    Map<File, QueryableIndex> getQueryableIndexMap()\n+    {\n+      return queryableIndexMap;\n+    }\n   }\n \n   private static Column createColumn(DimensionSchema dimensionSchema)",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/indexing-service/src/test/java/io/druid/indexing/common/task/CompactionTaskTest.java",
                "sha": "cefe501552952553ece0aa4cd934558421ec5775",
                "status": "modified"
            },
            {
                "additions": 64,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/Metadata.java",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/Metadata.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 85,
                "filename": "processing/src/main/java/io/druid/segment/Metadata.java",
                "patch": "@@ -25,11 +25,13 @@\n import io.druid.java.util.common.granularity.Granularity;\n import io.druid.query.aggregation.AggregatorFactory;\n \n+import javax.annotation.Nullable;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n \n /**\n@@ -39,71 +41,66 @@\n {\n   // container is used for arbitrary key-value pairs in segment metadata e.g.\n   // kafka firehose uses it to store commit offset\n-  @JsonProperty\n   private final Map<String, Object> container;\n+  @Nullable\n+  private final AggregatorFactory[] aggregators;\n+  @Nullable\n+  private final TimestampSpec timestampSpec;\n+  @Nullable\n+  private final Granularity queryGranularity;\n+  @Nullable\n+  private final Boolean rollup;\n+\n+  public Metadata(\n+      @JsonProperty(\"container\") @Nullable Map<String, Object> container,\n+      @JsonProperty(\"aggregators\") @Nullable AggregatorFactory[] aggregators,\n+      @JsonProperty(\"timestampSpec\") @Nullable TimestampSpec timestampSpec,\n+      @JsonProperty(\"queryGranularity\") @Nullable Granularity queryGranularity,\n+      @JsonProperty(\"rollup\") @Nullable Boolean rollup\n+  )\n+  {\n+    this.container = container == null ? new ConcurrentHashMap<>() : container;\n+    this.aggregators = aggregators;\n+    this.timestampSpec = timestampSpec;\n+    this.queryGranularity = queryGranularity;\n+    this.rollup = rollup;\n+  }\n \n   @JsonProperty\n-  private AggregatorFactory[] aggregators;\n-\n-  @JsonProperty\n-  private TimestampSpec timestampSpec;\n-\n-  @JsonProperty\n-  private Granularity queryGranularity;\n-\n-  @JsonProperty\n-  private Boolean rollup;\n-\n-  public Metadata()\n+  public Map<String, Object> getContainer()\n   {\n-    container = new ConcurrentHashMap<>();\n+    return container;\n   }\n \n+  @JsonProperty\n+  @Nullable\n   public AggregatorFactory[] getAggregators()\n   {\n     return aggregators;\n   }\n \n-  public Metadata setAggregators(AggregatorFactory[] aggregators)\n-  {\n-    this.aggregators = aggregators;\n-    return this;\n-  }\n-\n+  @JsonProperty\n+  @Nullable\n   public TimestampSpec getTimestampSpec()\n   {\n     return timestampSpec;\n   }\n \n-  public Metadata setTimestampSpec(TimestampSpec timestampSpec)\n-  {\n-    this.timestampSpec = timestampSpec;\n-    return this;\n-  }\n-\n+  @JsonProperty\n+  @Nullable\n   public Granularity getQueryGranularity()\n   {\n     return queryGranularity;\n   }\n \n-  public Metadata setQueryGranularity(Granularity queryGranularity)\n-  {\n-    this.queryGranularity = queryGranularity;\n-    return this;\n-  }\n-\n+  @JsonProperty\n+  @Nullable\n   public Boolean isRollup()\n   {\n     return rollup;\n   }\n \n-  public Metadata setRollup(Boolean rollup)\n-  {\n-    this.rollup = rollup;\n-    return this;\n-  }\n-\n-  public Metadata putAll(Map<String, Object> other)\n+  public Metadata putAll(@Nullable Map<String, Object> other)\n   {\n     if (other != null) {\n       container.putAll(other);\n@@ -116,7 +113,7 @@ public Object get(String key)\n     return container.get(key);\n   }\n \n-  public Metadata put(String key, Object value)\n+  public Metadata put(String key, @Nullable Object value)\n   {\n     if (value != null) {\n       container.put(key, value);\n@@ -127,9 +124,10 @@ public Metadata put(String key, Object value)\n   // arbitrary key-value pairs from the metadata just follow the semantics of last one wins if same\n   // key exists in multiple input Metadata containers\n   // for others e.g. Aggregators, appropriate merging is done\n+  @Nullable\n   public static Metadata merge(\n-      List<Metadata> toBeMerged,\n-      AggregatorFactory[] overrideMergedAggregators\n+      @Nullable List<Metadata> toBeMerged,\n+      @Nullable AggregatorFactory[] overrideMergedAggregators\n   )\n   {\n     if (toBeMerged == null || toBeMerged.size() == 0) {\n@@ -139,7 +137,7 @@ public static Metadata merge(\n     boolean foundSomeMetadata = false;\n     Map<String, Object> mergedContainer = new HashMap<>();\n     List<AggregatorFactory[]> aggregatorsToMerge = overrideMergedAggregators == null\n-                                                   ? new ArrayList<AggregatorFactory[]>()\n+                                                   ? new ArrayList<>()\n                                                    : null;\n \n     List<TimestampSpec> timestampSpecsToMerge = new ArrayList<>();\n@@ -179,20 +177,17 @@ public static Metadata merge(\n       return null;\n     }\n \n-    Metadata result = new Metadata();\n-    if (aggregatorsToMerge != null) {\n-      result.setAggregators(AggregatorFactory.mergeAggregators(aggregatorsToMerge));\n-    } else {\n-      result.setAggregators(overrideMergedAggregators);\n-    }\n+    final AggregatorFactory[] mergedAggregators = aggregatorsToMerge == null ?\n+                                                  overrideMergedAggregators :\n+                                                  AggregatorFactory.mergeAggregators(aggregatorsToMerge);\n \n-    if (timestampSpecsToMerge != null) {\n-      result.setTimestampSpec(TimestampSpec.mergeTimestampSpec(timestampSpecsToMerge));\n-    }\n+    final TimestampSpec mergedTimestampSpec = timestampSpecsToMerge == null ?\n+                                              null :\n+                                              TimestampSpec.mergeTimestampSpec(timestampSpecsToMerge);\n \n-    if (gransToMerge != null) {\n-      result.setQueryGranularity(Granularity.mergeGranularities(gransToMerge));\n-    }\n+    final Granularity mergedGranularity = gransToMerge == null ?\n+                                          null :\n+                                          Granularity.mergeGranularities(gransToMerge);\n \n     Boolean rollup = null;\n     if (rollupToMerge != null && !rollupToMerge.isEmpty()) {\n@@ -210,10 +205,13 @@ public static Metadata merge(\n       }\n     }\n \n-    result.setRollup(rollup);\n-    result.container.putAll(mergedContainer);\n-    return result;\n-\n+    return new Metadata(\n+        mergedContainer,\n+        mergedAggregators,\n+        mergedTimestampSpec,\n+        mergedGranularity,\n+        rollup\n+    );\n   }\n \n   @Override\n@@ -225,37 +223,18 @@ public boolean equals(Object o)\n     if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-\n-    Metadata metadata = (Metadata) o;\n-\n-    if (!container.equals(metadata.container)) {\n-      return false;\n-    }\n-    // Probably incorrect - comparing Object[] arrays with Arrays.equals\n-    if (!Arrays.equals(aggregators, metadata.aggregators)) {\n-      return false;\n-    }\n-    if (timestampSpec != null ? !timestampSpec.equals(metadata.timestampSpec) : metadata.timestampSpec != null) {\n-      return false;\n-    }\n-    if (rollup != null ? !rollup.equals(metadata.rollup) : metadata.rollup != null) {\n-      return false;\n-    }\n-    return queryGranularity != null\n-           ? queryGranularity.equals(metadata.queryGranularity)\n-           : metadata.queryGranularity == null;\n-\n+    final Metadata metadata = (Metadata) o;\n+    return Objects.equals(container, metadata.container) &&\n+           Arrays.equals(aggregators, metadata.aggregators) &&\n+           Objects.equals(timestampSpec, metadata.timestampSpec) &&\n+           Objects.equals(queryGranularity, metadata.queryGranularity) &&\n+           Objects.equals(rollup, metadata.rollup);\n   }\n \n   @Override\n   public int hashCode()\n   {\n-    int result = container.hashCode();\n-    result = 31 * result + Arrays.hashCode(aggregators);\n-    result = 31 * result + (timestampSpec != null ? timestampSpec.hashCode() : 0);\n-    result = 31 * result + (queryGranularity != null ? queryGranularity.hashCode() : 0);\n-    result = 31 * result + (rollup != null ? rollup.hashCode() : 0);\n-    return result;\n+    return Objects.hash(container, Arrays.hashCode(aggregators), timestampSpec, queryGranularity, rollup);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/Metadata.java",
                "sha": "3a7c94d294cd772a64149cc48aaa49946ca7d3a5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/QueryableIndex.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/QueryableIndex.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 1,
                "filename": "processing/src/main/java/io/druid/segment/QueryableIndex.java",
                "patch": "@@ -23,6 +23,7 @@\n import io.druid.segment.data.Indexed;\n import org.joda.time.Interval;\n \n+import javax.annotation.Nullable;\n import java.io.Closeable;\n import java.io.IOException;\n import java.util.Map;\n@@ -40,7 +41,7 @@\n   int getNumRows();\n   Indexed<String> getAvailableDimensions();\n   BitmapFactory getBitmapFactoryForDimensions();\n-  Metadata getMetadata();\n+  @Nullable Metadata getMetadata();\n   Map<String, DimensionHandler> getDimensionHandlers();\n \n   /**",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/QueryableIndex.java",
                "sha": "9a21891b73e17fd32fe4c4a6a4e8b80e64a036d4",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/SimpleQueryableIndex.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/SimpleQueryableIndex.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 2,
                "filename": "processing/src/main/java/io/druid/segment/SimpleQueryableIndex.java",
                "patch": "@@ -19,6 +19,7 @@\n \n package io.druid.segment;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Maps;\n@@ -43,6 +44,7 @@\n   private final BitmapFactory bitmapFactory;\n   private final Map<String, Column> columns;\n   private final SmooshedFileMapper fileMapper;\n+  @Nullable\n   private final Metadata metadata;\n   private final Map<String, DimensionHandler> dimensionHandlers;\n \n@@ -51,11 +53,11 @@ public SimpleQueryableIndex(\n       BitmapFactory bitmapFactory,\n       Map<String, Column> columns,\n       SmooshedFileMapper fileMapper,\n-      Metadata metadata\n+      @Nullable Metadata metadata\n   )\n   {\n     Preconditions.checkNotNull(columns.get(Column.TIME_COLUMN_NAME));\n-    this.dataInterval = dataInterval;\n+    this.dataInterval = Preconditions.checkNotNull(dataInterval, \"dataInterval\");\n     ImmutableList.Builder<String> columnNamesBuilder = ImmutableList.builder();\n     for (String column : columns.keySet()) {\n       if (!Column.TIME_COLUMN_NAME.equals(column)) {\n@@ -72,6 +74,28 @@ public SimpleQueryableIndex(\n     initDimensionHandlers();\n   }\n \n+  @VisibleForTesting\n+  public SimpleQueryableIndex(\n+      Interval interval,\n+      List<String> columnNames,\n+      Indexed<String> availableDimensions,\n+      BitmapFactory bitmapFactory,\n+      Map<String, Column> columns,\n+      SmooshedFileMapper fileMapper,\n+      @Nullable Metadata metadata,\n+      Map<String, DimensionHandler> dimensionHandlers\n+  )\n+  {\n+    this.dataInterval = interval;\n+    this.columnNames = columnNames;\n+    this.availableDimensions = availableDimensions;\n+    this.bitmapFactory = bitmapFactory;\n+    this.columns = columns;\n+    this.fileMapper = fileMapper;\n+    this.metadata = metadata;\n+    this.dimensionHandlers = dimensionHandlers;\n+  }\n+\n   @Override\n   public Interval getDataInterval()\n   {\n@@ -115,6 +139,18 @@ public Column getColumn(String columnName)\n     return columns.get(columnName);\n   }\n \n+  @VisibleForTesting\n+  public Map<String, Column> getColumns()\n+  {\n+    return columns;\n+  }\n+\n+  @VisibleForTesting\n+  public SmooshedFileMapper getFileMapper()\n+  {\n+    return fileMapper;\n+  }\n+\n   @Override\n   public void close()\n   {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/SimpleQueryableIndex.java",
                "sha": "74ef74e0606762d2e4f516028154732024b65ea2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -276,11 +276,13 @@ protected IncrementalIndex(\n     this.reportParseExceptions = reportParseExceptions;\n \n     this.columnCapabilities = Maps.newHashMap();\n-    this.metadata = new Metadata()\n-        .setAggregators(getCombiningAggregators(metrics))\n-        .setTimestampSpec(incrementalIndexSchema.getTimestampSpec())\n-        .setQueryGranularity(this.gran)\n-        .setRollup(this.rollup);\n+    this.metadata = new Metadata(\n+        null,\n+        getCombiningAggregators(metrics),\n+        incrementalIndexSchema.getTimestampSpec(),\n+        this.gran,\n+        this.rollup\n+    );\n \n     this.aggs = initAggs(metrics, rowSupplier, deserializeComplexMetrics, concurrentEventAdd);\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "91e8c1dfe05840634b92293d0cbcc372f8b443b5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 7,
                "filename": "processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "patch": "@@ -274,13 +274,13 @@ public void testPersistWithSegmentMetadata() throws Exception\n     assertDimCompression(index, indexSpec.getDimensionCompression());\n \n     Assert.assertEquals(\n-        new Metadata()\n-            .setAggregators(\n-                IncrementalIndexTest.getDefaultCombiningAggregatorFactories()\n-            )\n-            .setQueryGranularity(Granularities.NONE)\n-            .setRollup(Boolean.TRUE)\n-            .putAll(metadataElems),\n+        new Metadata(\n+            metadataElems,\n+            IncrementalIndexTest.getDefaultCombiningAggregatorFactories(),\n+            null,\n+            Granularities.NONE,\n+            Boolean.TRUE\n+        ),\n         index.getMetadata()\n     );\n   }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/test/java/io/druid/segment/IndexMergerTestBase.java",
                "sha": "bb7c6246c601d709ff390c60962c113a15d01f63",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/test/java/io/druid/segment/MetadataTest.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/test/java/io/druid/segment/MetadataTest.java?ref=f349e03091546c2b55ca91750a44c107403b84d3",
                "deletions": 39,
                "filename": "processing/src/test/java/io/druid/segment/MetadataTest.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n \n /**\n@@ -42,15 +43,17 @@ public void testSerde() throws Exception\n   {\n     ObjectMapper jsonMapper = TestHelper.makeJsonMapper();\n \n-    Metadata metadata = new Metadata();\n-    metadata.put(\"k\", \"v\");\n-\n     AggregatorFactory[] aggregators = new AggregatorFactory[] {\n         new LongSumAggregatorFactory(\"out\", \"in\")\n     };\n-    metadata.setAggregators(aggregators);\n-    metadata.setQueryGranularity(Granularities.ALL);\n-    metadata.setRollup(Boolean.FALSE);\n+\n+    Metadata metadata = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n+        aggregators,\n+        null,\n+        Granularities.ALL,\n+        Boolean.FALSE\n+    );\n \n     Metadata other = jsonMapper.readValue(\n         jsonMapper.writeValueAsString(metadata),\n@@ -75,30 +78,39 @@ public void testMerge()\n     AggregatorFactory[] aggs = new AggregatorFactory[] {\n         new LongMaxAggregatorFactory(\"n\", \"f\")\n     };\n-    Metadata m1 = new Metadata();\n-    m1.put(\"k\", \"v\");\n-    m1.setAggregators(aggs);\n-    m1.setTimestampSpec(new TimestampSpec(\"ds\", \"auto\", null));\n-    m1.setQueryGranularity(Granularities.ALL);\n-    m1.setRollup(Boolean.FALSE);\n-\n-    Metadata m2 = new Metadata();\n-    m2.put(\"k\", \"v\");\n-    m2.setAggregators(aggs);\n-    m2.setTimestampSpec(new TimestampSpec(\"ds\", \"auto\", null));\n-    m2.setQueryGranularity(Granularities.ALL);\n-    m2.setRollup(Boolean.FALSE);\n-\n-    Metadata merged = new Metadata();\n-    merged.put(\"k\", \"v\");\n-    merged.setAggregators(\n+    final Metadata m1 = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n+        aggs,\n+        new TimestampSpec(\"ds\", \"auto\", null),\n+        Granularities.ALL,\n+        Boolean.FALSE\n+    );\n+\n+    final Metadata m2 = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n+        aggs,\n+        new TimestampSpec(\"ds\", \"auto\", null),\n+        Granularities.ALL,\n+        Boolean.FALSE\n+    );\n+\n+    final Metadata m3 = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n+        aggs,\n+        new TimestampSpec(\"ds\", \"auto\", null),\n+        Granularities.ALL,\n+        Boolean.TRUE\n+    );\n+\n+    final Metadata merged = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n         new AggregatorFactory[]{\n             new LongMaxAggregatorFactory(\"n\", \"n\")\n-        }\n+        },\n+        new TimestampSpec(\"ds\", \"auto\", null),\n+        Granularities.ALL,\n+        Boolean.FALSE\n     );\n-    merged.setTimestampSpec(new TimestampSpec(\"ds\", \"auto\", null));\n-    merged.setRollup(Boolean.FALSE);\n-    merged.setQueryGranularity(Granularities.ALL);\n     Assert.assertEquals(merged, Metadata.merge(ImmutableList.of(m1, m2), null));\n \n     //merge check with one metadata being null\n@@ -107,29 +119,32 @@ public void testMerge()\n     metadataToBeMerged.add(m2);\n     metadataToBeMerged.add(null);\n \n-    merged.setAggregators(null);\n-    merged.setTimestampSpec(null);\n-    merged.setQueryGranularity(null);\n-    merged.setRollup(null);\n-    Assert.assertEquals(merged, Metadata.merge(metadataToBeMerged, null));\n+    final Metadata merged2 = new Metadata(Collections.singletonMap(\"k\", \"v\"), null, null, null, null);\n+\n+    Assert.assertEquals(merged2, Metadata.merge(metadataToBeMerged, null));\n \n     //merge check with client explicitly providing merged aggregators\n     AggregatorFactory[] explicitAggs = new AggregatorFactory[] {\n         new DoubleMaxAggregatorFactory(\"x\", \"y\")\n     };\n-    merged.setAggregators(explicitAggs);\n+\n+    final Metadata merged3 = new Metadata(Collections.singletonMap(\"k\", \"v\"), explicitAggs, null, null, null);\n \n     Assert.assertEquals(\n-        merged,\n+        merged3,\n         Metadata.merge(metadataToBeMerged, explicitAggs)\n     );\n \n-    merged.setTimestampSpec(new TimestampSpec(\"ds\", \"auto\", null));\n-    merged.setQueryGranularity(Granularities.ALL);\n-    m1.setRollup(Boolean.TRUE);\n+    final Metadata merged4 = new Metadata(\n+        Collections.singletonMap(\"k\", \"v\"),\n+        explicitAggs,\n+        new TimestampSpec(\"ds\", \"auto\", null),\n+        Granularities.ALL,\n+        null\n+    );\n     Assert.assertEquals(\n-        merged,\n-        Metadata.merge(ImmutableList.of(m1, m2), explicitAggs)\n+        merged4,\n+        Metadata.merge(ImmutableList.of(m3, m2), explicitAggs)\n     );\n   }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f349e03091546c2b55ca91750a44c107403b84d3/processing/src/test/java/io/druid/segment/MetadataTest.java",
                "sha": "b1ca2f288be0fe4c2ffa5aec6885730a21b2a82c",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in compactionTask (#5613)\n\n* Fix NPE in compactionTask\r\n\r\n* more annotations for metadata\r\n\r\n* better error message for empty input\r\n\r\n* fix build\r\n\r\n* revert some null checks\r\n\r\n* address comments",
        "parent": "https://github.com/apache/incubator-druid/commit/124c89e435ee3e368f2dcc6cb4ec305a65cf9e65",
        "repo": "incubator-druid",
        "unit_tests": [
            "CompactionTaskTest.java",
            "MetadataTest.java",
            "IncrementalIndexTest.java"
        ]
    },
    "incubator-druid_f36d030": {
        "bug_id": "incubator-druid_f36d030",
        "commit": "https://github.com/apache/incubator-druid/commit/f36d030ef20e9cf810ae229fa673b61ee6f69271",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f36d030ef20e9cf810ae229fa673b61ee6f69271/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java?ref=f36d030ef20e9cf810ae229fa673b61ee6f69271",
                "deletions": 5,
                "filename": "processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "patch": "@@ -286,11 +286,6 @@ public int lookupId(String name)\n \n     this.dimensionOrder = Maps.newLinkedHashMap();\n     this.dimensions = new CopyOnWriteArrayList<>();\n-    int index = 0;\n-    for (String dim : incrementalIndexSchema.getDimensionsSpec().getDimensions()) {\n-      dimensionOrder.put(dim, index++);\n-      dimensions.add(dim);\n-    }\n     // This should really be more generic\n     List<SpatialDimensionSchema> spatialDimensions = incrementalIndexSchema.getDimensionsSpec().getSpatialDimensions();\n     if (!spatialDimensions.isEmpty()) {",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f36d030ef20e9cf810ae229fa673b61ee6f69271/processing/src/main/java/io/druid/segment/incremental/IncrementalIndex.java",
                "sha": "26985edc024b7e154d53d222b29fb0f0e4b6b61d",
                "status": "modified"
            }
        ],
        "message": "fix NPE while indexing",
        "parent": "https://github.com/apache/incubator-druid/commit/c6712739dc499fbae8cdfd063ef9931626675626",
        "repo": "incubator-druid",
        "unit_tests": [
            "IncrementalIndexTest.java"
        ]
    },
    "incubator-druid_f56d60b": {
        "bug_id": "incubator-druid_f56d60b",
        "commit": "https://github.com/apache/incubator-druid/commit/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java?ref=f56d60b4511d15c3caaeda1dee56b8deb3f14c4b",
                "deletions": 15,
                "filename": "s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "patch": "@@ -24,6 +24,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Maps;\n import com.google.inject.Inject;\n+import com.metamx.common.ISE;\n import com.metamx.common.MapUtils;\n import com.metamx.common.logger.Logger;\n import io.druid.segment.loading.DataSegmentMover;\n@@ -120,23 +121,30 @@ public Void call() throws Exception\n               if (s3Client.isObjectInBucket(s3Bucket, s3Path)) {\n                 if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) {\n                   log.info(\"No need to move file[s3://%s/%s] onto itself\", s3Bucket, s3Path);\n-                } else if (s3Client.getObjectDetails(s3Bucket, s3Path)\n-                                   .getStorageClass()\n-                                   .equals(S3Object.STORAGE_CLASS_GLACIER)) {\n-                  log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier.\");\n                 } else {\n-                  log.info(\n-                      \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n-                      s3Bucket,\n-                      s3Path,\n-                      targetS3Bucket,\n-                      targetS3Path\n-                  );\n-                  final S3Object target = new S3Object(targetS3Path);\n-                  if(!config.getDisableAcl()) {\n-                    target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                  final S3Object[] list = s3Client.listObjects(s3Bucket, s3Path, \"\");\n+                  if (list.length == 0) {\n+                    // should never happen\n+                    throw new ISE(\"Unable to list object [s3://%s/%s]\", s3Bucket, s3Path);\n+                  }\n+                  final S3Object s3Object = list[0];\n+                  if (s3Object.getStorageClass() != null &&\n+                      s3Object.getStorageClass().equals(S3Object.STORAGE_CLASS_GLACIER)) {\n+                    log.warn(\"Cannot move file[s3://%s/%s] of storage class glacier, skipping.\");\n+                  } else {\n+                    log.info(\n+                        \"Moving file[s3://%s/%s] to [s3://%s/%s]\",\n+                        s3Bucket,\n+                        s3Path,\n+                        targetS3Bucket,\n+                        targetS3Path\n+                    );\n+                    final S3Object target = new S3Object(targetS3Path);\n+                    if (!config.getDisableAcl()) {\n+                      target.setAcl(GSAccessControlList.REST_CANNED_BUCKET_OWNER_FULL_CONTROL);\n+                    }\n+                    s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                   }\n-                  s3Client.moveObject(s3Bucket, s3Path, targetS3Bucket, target, false);\n                 }\n               } else {\n                 // ensure object exists in target location",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f56d60b4511d15c3caaeda1dee56b8deb3f14c4b/s3-extensions/src/main/java/io/druid/storage/s3/S3DataSegmentMover.java",
                "sha": "379dd8374fab0d5c67b1601469d9302226318c93",
                "status": "modified"
            }
        ],
        "message": "fix storage class npe",
        "parent": "https://github.com/apache/incubator-druid/commit/b8347cf4af247b259cda138a40cf45236528b85f",
        "repo": "incubator-druid",
        "unit_tests": [
            "S3DataSegmentMoverTest.java"
        ]
    },
    "incubator-druid_f8dcb05": {
        "bug_id": "incubator-druid_f8dcb05",
        "commit": "https://github.com/apache/incubator-druid/commit/f8dcb05fd19164f711797f60a9ed463f03ff129a",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f8dcb05fd19164f711797f60a9ed463f03ff129a/extensions-contrib/kafka-emitter/src/main/java/io/druid/emitter/kafka/KafkaEmitterConfig.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/kafka-emitter/src/main/java/io/druid/emitter/kafka/KafkaEmitterConfig.java?ref=f8dcb05fd19164f711797f60a9ed463f03ff129a",
                "deletions": 6,
                "filename": "extensions-contrib/kafka-emitter/src/main/java/io/druid/emitter/kafka/KafkaEmitterConfig.java",
                "patch": "@@ -22,8 +22,10 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n import org.apache.kafka.clients.producer.ProducerConfig;\n \n+import javax.annotation.Nullable;\n import java.util.Map;\n \n public class KafkaEmitterConfig\n@@ -46,14 +48,14 @@ public KafkaEmitterConfig(\n       @JsonProperty(\"metric.topic\") String metricTopic,\n       @JsonProperty(\"alert.topic\") String alertTopic,\n       @JsonProperty(\"clusterName\") String clusterName,\n-      @JsonProperty(\"producer.config\") Map<String, String> kafkaProducerConfig\n+      @JsonProperty(\"producer.config\") @Nullable Map<String, String> kafkaProducerConfig\n   )\n   {\n     this.bootstrapServers = Preconditions.checkNotNull(bootstrapServers, \"bootstrap.servers can not be null\");\n     this.metricTopic = Preconditions.checkNotNull(metricTopic, \"metric.topic can not be null\");\n     this.alertTopic = Preconditions.checkNotNull(alertTopic, \"alert.topic can not be null\");\n     this.clusterName = clusterName;\n-    this.kafkaProducerConfig = kafkaProducerConfig;\n+    this.kafkaProducerConfig = kafkaProducerConfig == null? ImmutableMap.<String,String>of() : kafkaProducerConfig;\n   }\n \n   @JsonProperty\n@@ -110,9 +112,7 @@ public boolean equals(Object o)\n     if (getClusterName() != null ? !getClusterName().equals(that.getClusterName()) : that.getClusterName() != null) {\n       return false;\n     }\n-    return getKafkaProducerConfig() != null\n-           ? getKafkaProducerConfig().equals(that.getKafkaProducerConfig())\n-           : that.getKafkaProducerConfig() == null;\n+    return getKafkaProducerConfig().equals(that.getKafkaProducerConfig());\n   }\n \n   @Override\n@@ -122,7 +122,7 @@ public int hashCode()\n     result = 31 * result + getMetricTopic().hashCode();\n     result = 31 * result + getAlertTopic().hashCode();\n     result = 31 * result + (getClusterName() != null ? getClusterName().hashCode() : 0);\n-    result = 31 * result + (getKafkaProducerConfig() != null ? getKafkaProducerConfig().hashCode() : 0);\n+    result = 31 * result + getKafkaProducerConfig().hashCode();\n     return result;\n   }\n ",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f8dcb05fd19164f711797f60a9ed463f03ff129a/extensions-contrib/kafka-emitter/src/main/java/io/druid/emitter/kafka/KafkaEmitterConfig.java",
                "sha": "2b73f59b6dcc6293e63ee6682f88bf283b5d4e87",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/incubator-druid/blob/f8dcb05fd19164f711797f60a9ed463f03ff129a/extensions-contrib/kafka-emitter/src/test/java/io/druid/emitter/kafka/KafkaEmitterConfigTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/incubator-druid/contents/extensions-contrib/kafka-emitter/src/test/java/io/druid/emitter/kafka/KafkaEmitterConfigTest.java?ref=f8dcb05fd19164f711797f60a9ed463f03ff129a",
                "deletions": 0,
                "filename": "extensions-contrib/kafka-emitter/src/test/java/io/druid/emitter/kafka/KafkaEmitterConfigTest.java",
                "patch": "@@ -52,4 +52,19 @@ public void testSerDeserKafkaEmitterConfig() throws IOException\n                                                           .readValue(kafkaEmitterConfigString);\n     Assert.assertEquals(kafkaEmitterConfigExpected, kafkaEmitterConfig);\n   }\n+\n+  @Test\n+  public void testSerDeNotRequiredKafkaProducerConfig() throws IOException\n+  {\n+    KafkaEmitterConfig kafkaEmitterConfig = new KafkaEmitterConfig(\"localhost:9092\", \"metricTest\",\n+                                                                   \"alertTest\", \"clusterNameTest\",\n+                                                                   null\n+    );\n+    try {\n+      KafkaEmitter emitter = new KafkaEmitter(kafkaEmitterConfig, mapper);\n+    }\n+    catch (NullPointerException e) {\n+      Assert.fail();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/incubator-druid/raw/f8dcb05fd19164f711797f60a9ed463f03ff129a/extensions-contrib/kafka-emitter/src/test/java/io/druid/emitter/kafka/KafkaEmitterConfigTest.java",
                "sha": "3b91706fbf029274dfbf4377c12016606c8d764f",
                "status": "modified"
            }
        ],
        "message": "Fix a NPE using kafka emitter extension (#4608)\n\n* Fix a NPE using kafka emitter extension\r\n\r\n* fix format\r\n\r\n* Add @Nullable annotation on kafkaProducerConfig",
        "parent": "https://github.com/apache/incubator-druid/commit/486b7a234771f377078565ed353e1179dc416b75",
        "repo": "incubator-druid",
        "unit_tests": [
            "KafkaEmitterConfigTest.java"
        ]
    }
}